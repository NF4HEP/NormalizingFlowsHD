2023-10-25 16:08:04.490930: Importing os...
2023-10-25 16:08:04.490978: Importing sys...
2023-10-25 16:08:04.490989: Importing and initializing argparse...
Visible devices: [0]
2023-10-25 16:08:04.502405: Importing timer from timeit...
2023-10-25 16:08:04.502823: Setting env variables for tf import (only device [0] will be available)...
2023-10-25 16:08:04.502855: Importing numpy...
2023-10-25 16:08:04.664026: Importing pandas...
2023-10-25 16:08:04.890548: Importing shutil...
2023-10-25 16:08:04.890576: Importing subprocess...
2023-10-25 16:08:04.890585: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-25 16:08:07.029985: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-25 16:08:07.386470: Importing textwrap...
2023-10-25 16:08:07.386497: Importing timeit...
2023-10-25 16:08:07.386508: Importing traceback...
2023-10-25 16:08:07.386515: Importing typing...
2023-10-25 16:08:07.386524: Setting tf configs...
2023-10-25 16:08:07.521510: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-25 16:08:08.616950: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

===========
Generating train data for run 360.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  4278720   
 r)                                                              
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7ff3f0513e80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff3f0268880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff3f0268880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4a05ddb70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff3a06d6f20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff3a06d7490>, <keras.callbacks.ModelCheckpoint object at 0x7ff3a06d75e0>, <keras.callbacks.EarlyStopping object at 0x7ff3a06d77f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff3a06d7820>, <keras.callbacks.TerminateOnNaN object at 0x7ff3a06d7550>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_360/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:08:18.902016
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:11:02.450 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6225.8818, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 6225.8818 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 163s/epoch - 833ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 360.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7ff77c5bbf70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0645e0040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0645e0040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff10868a320>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff77bf69ab0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff77bf6a020>, <keras.callbacks.ModelCheckpoint object at 0x7ff77bf6a0e0>, <keras.callbacks.EarlyStopping object at 0x7ff77bf6a350>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff77bf6a380>, <keras.callbacks.TerminateOnNaN object at 0x7ff77bf69fc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_360/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:11:11.655268
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 107: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:14:32.496 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3525.3457, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 201s - loss: nan - MinusLogProbMetric: 3525.3457 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 201s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 360.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7ff2645a3b20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff25c5e3b20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff25c5e3b20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff1906845e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff26451a740>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff26451acb0>, <keras.callbacks.ModelCheckpoint object at 0x7ff26451ad70>, <keras.callbacks.EarlyStopping object at 0x7ff26451afe0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff26451b010>, <keras.callbacks.TerminateOnNaN object at 0x7ff26451ac50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_360/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:14:42.670730
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 16:18:28.809 
Epoch 1/1000 
	 loss: 3537.7837, MinusLogProbMetric: 3537.7837, val_loss: 1285.5569, val_MinusLogProbMetric: 1285.5569

Epoch 1: val_loss improved from inf to 1285.55688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 227s - loss: 3537.7837 - MinusLogProbMetric: 3537.7837 - val_loss: 1285.5569 - val_MinusLogProbMetric: 1285.5569 - lr: 1.1111e-04 - 227s/epoch - 1s/step
Epoch 2/1000
2023-10-25 16:19:40.212 
Epoch 2/1000 
	 loss: 1460.1931, MinusLogProbMetric: 1460.1931, val_loss: 1768.2308, val_MinusLogProbMetric: 1768.2308

Epoch 2: val_loss did not improve from 1285.55688
196/196 - 70s - loss: 1460.1931 - MinusLogProbMetric: 1460.1931 - val_loss: 1768.2308 - val_MinusLogProbMetric: 1768.2308 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 3/1000
2023-10-25 16:20:44.922 
Epoch 3/1000 
	 loss: 1420.3933, MinusLogProbMetric: 1420.3933, val_loss: 1269.6218, val_MinusLogProbMetric: 1269.6218

Epoch 3: val_loss improved from 1285.55688 to 1269.62183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 66s - loss: 1420.3933 - MinusLogProbMetric: 1420.3933 - val_loss: 1269.6218 - val_MinusLogProbMetric: 1269.6218 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 4/1000
2023-10-25 16:21:58.111 
Epoch 4/1000 
	 loss: 1075.7118, MinusLogProbMetric: 1075.7118, val_loss: 863.7883, val_MinusLogProbMetric: 863.7883

Epoch 4: val_loss improved from 1269.62183 to 863.78827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 73s - loss: 1075.7118 - MinusLogProbMetric: 1075.7118 - val_loss: 863.7883 - val_MinusLogProbMetric: 863.7883 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 37: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:22:15.984 
Epoch 5/1000 
	 loss: nan, MinusLogProbMetric: 1002.2673, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 5: val_loss did not improve from 863.78827
196/196 - 17s - loss: nan - MinusLogProbMetric: 1002.2673 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 17s/epoch - 86ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 360.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7ff0bbb5f070>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff1bc2c5090>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff1bc2c5090>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0e8827ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0e8884d30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0e88852a0>, <keras.callbacks.ModelCheckpoint object at 0x7ff0e8885360>, <keras.callbacks.EarlyStopping object at 0x7ff0e88855d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0e8885600>, <keras.callbacks.TerminateOnNaN object at 0x7ff0e8885240>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:22:27.329513
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-25 16:26:22.631 
Epoch 1/1000 
	 loss: 930.3226, MinusLogProbMetric: 930.3226, val_loss: 975.0052, val_MinusLogProbMetric: 975.0052

Epoch 1: val_loss improved from inf to 975.00525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 236s - loss: 930.3226 - MinusLogProbMetric: 930.3226 - val_loss: 975.0052 - val_MinusLogProbMetric: 975.0052 - lr: 3.7037e-05 - 236s/epoch - 1s/step
Epoch 2/1000
2023-10-25 16:27:34.902 
Epoch 2/1000 
	 loss: 814.0610, MinusLogProbMetric: 814.0610, val_loss: 767.1041, val_MinusLogProbMetric: 767.1041

Epoch 2: val_loss improved from 975.00525 to 767.10413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 72s - loss: 814.0610 - MinusLogProbMetric: 814.0610 - val_loss: 767.1041 - val_MinusLogProbMetric: 767.1041 - lr: 3.7037e-05 - 72s/epoch - 367ms/step
Epoch 3/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-25 16:28:49.975 
Epoch 3/1000 
	 loss: 743.9305, MinusLogProbMetric: 743.9305, val_loss: 658.0429, val_MinusLogProbMetric: 658.0430

Epoch 3: val_loss improved from 767.10413 to 658.04291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 75s - loss: 743.9305 - MinusLogProbMetric: 743.9305 - val_loss: 658.0429 - val_MinusLogProbMetric: 658.0430 - lr: 3.7037e-05 - 75s/epoch - 383ms/step
Epoch 4/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 85: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:29:24.740 
Epoch 4/1000 
	 loss: nan, MinusLogProbMetric: 642.3105, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 4: val_loss did not improve from 658.04291
196/196 - 34s - loss: nan - MinusLogProbMetric: 642.3105 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 34s/epoch - 172ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 360.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7ff0e8e2fb20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff168497a90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff168497a90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0b83e14e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff2641b9a50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff2641b9fc0>, <keras.callbacks.ModelCheckpoint object at 0x7ff2641ba080>, <keras.callbacks.EarlyStopping object at 0x7ff2641ba2f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff2641ba320>, <keras.callbacks.TerminateOnNaN object at 0x7ff2641b9f60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:29:35.998337
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 103: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:33:09.332 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 680.8549, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 213s - loss: nan - MinusLogProbMetric: 680.8549 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 213s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 360.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7ff2c86d23b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff064741270>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff064741270>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0647a16f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0647c38e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0647c31f0>, <keras.callbacks.ModelCheckpoint object at 0x7ff0647c32b0>, <keras.callbacks.EarlyStopping object at 0x7ff0647c2c20>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0647c2e00>, <keras.callbacks.TerminateOnNaN object at 0x7ff0647c3700>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:33:23.169686
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-25 16:37:04.784 
Epoch 1/1000 
	 loss: 604.4929, MinusLogProbMetric: 604.4929, val_loss: 594.6322, val_MinusLogProbMetric: 594.6324

Epoch 1: val_loss improved from inf to 594.63220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 222s - loss: 604.4929 - MinusLogProbMetric: 604.4929 - val_loss: 594.6322 - val_MinusLogProbMetric: 594.6324 - lr: 4.1152e-06 - 222s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 29: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:37:19.946 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 588.9185, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 594.63220
196/196 - 14s - loss: nan - MinusLogProbMetric: 588.9185 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 14s/epoch - 70ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 360.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7ff128719870>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff276d084c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff276d084c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0642636a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff308419a50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff308419fc0>, <keras.callbacks.ModelCheckpoint object at 0x7ff30841a080>, <keras.callbacks.EarlyStopping object at 0x7ff30841a2f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff30841a320>, <keras.callbacks.TerminateOnNaN object at 0x7ff308419f60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:37:29.792873
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 157: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:41:08.348 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 585.9322, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 218s - loss: nan - MinusLogProbMetric: 585.9322 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 218s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 360.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7ff0a89d3b80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0c3843bb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0c3843bb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff108582590>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0a89b6bc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0a89b7130>, <keras.callbacks.ModelCheckpoint object at 0x7ff0a89b71f0>, <keras.callbacks.EarlyStopping object at 0x7ff0a89b7460>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0a89b7490>, <keras.callbacks.TerminateOnNaN object at 0x7ff0a89b70d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:41:19.685576
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 16:45:27.478 
Epoch 1/1000 
	 loss: 584.0458, MinusLogProbMetric: 584.0458, val_loss: 579.0933, val_MinusLogProbMetric: 579.0933

Epoch 1: val_loss improved from inf to 579.09326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 249s - loss: 584.0458 - MinusLogProbMetric: 584.0458 - val_loss: 579.0933 - val_MinusLogProbMetric: 579.0933 - lr: 4.5725e-07 - 249s/epoch - 1s/step
Epoch 2/1000
2023-10-25 16:46:51.744 
Epoch 2/1000 
	 loss: 577.2944, MinusLogProbMetric: 577.2944, val_loss: 575.5005, val_MinusLogProbMetric: 575.5005

Epoch 2: val_loss improved from 579.09326 to 575.50049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 84s - loss: 577.2944 - MinusLogProbMetric: 577.2944 - val_loss: 575.5005 - val_MinusLogProbMetric: 575.5005 - lr: 4.5725e-07 - 84s/epoch - 428ms/step
Epoch 3/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-25 16:48:15.866 
Epoch 3/1000 
	 loss: 571.6929, MinusLogProbMetric: 571.6929, val_loss: 567.1539, val_MinusLogProbMetric: 567.1537

Epoch 3: val_loss improved from 575.50049 to 567.15387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 84s - loss: 571.6929 - MinusLogProbMetric: 571.6929 - val_loss: 567.1539 - val_MinusLogProbMetric: 567.1537 - lr: 4.5725e-07 - 84s/epoch - 428ms/step
Epoch 4/1000
2023-10-25 16:49:40.916 
Epoch 4/1000 
	 loss: 566.0093, MinusLogProbMetric: 566.0093, val_loss: 562.8373, val_MinusLogProbMetric: 562.8373

Epoch 4: val_loss improved from 567.15387 to 562.83728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 86s - loss: 566.0093 - MinusLogProbMetric: 566.0093 - val_loss: 562.8373 - val_MinusLogProbMetric: 562.8373 - lr: 4.5725e-07 - 86s/epoch - 436ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 16: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:49:54.866 
Epoch 5/1000 
	 loss: nan, MinusLogProbMetric: 562.5181, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 5: val_loss did not improve from 562.83728
196/196 - 12s - loss: nan - MinusLogProbMetric: 562.5181 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 12s/epoch - 63ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 360.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7ff0ba939240>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff348621cf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff348621cf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff128323640>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1682336a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff168233c10>, <keras.callbacks.ModelCheckpoint object at 0x7ff168233cd0>, <keras.callbacks.EarlyStopping object at 0x7ff168233f40>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff168233f70>, <keras.callbacks.TerminateOnNaN object at 0x7ff168233bb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:50:08.886745
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 16:54:50.138 
Epoch 1/1000 
	 loss: 560.0368, MinusLogProbMetric: 560.0368, val_loss: 558.4725, val_MinusLogProbMetric: 558.4725

Epoch 1: val_loss improved from inf to 558.47247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 282s - loss: 560.0368 - MinusLogProbMetric: 560.0368 - val_loss: 558.4725 - val_MinusLogProbMetric: 558.4725 - lr: 1.5242e-07 - 282s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:55:00.249 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 557.9379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 558.47247
196/196 - 8s - loss: nan - MinusLogProbMetric: 557.9379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 8s/epoch - 42ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 360.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  4278720   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7ff0b9285d50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff1085f5360>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff1085f5360>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff79d9062f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0a9c0d540>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0a9c0dab0>, <keras.callbacks.ModelCheckpoint object at 0x7ff0a9c0db70>, <keras.callbacks.EarlyStopping object at 0x7ff0a9c0dde0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0a9c0de10>, <keras.callbacks.TerminateOnNaN object at 0x7ff0a9c0da50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 16:55:14.042636
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 17:00:10.861 
Epoch 1/1000 
	 loss: 557.7808, MinusLogProbMetric: 557.7808, val_loss: 557.5689, val_MinusLogProbMetric: 557.5689

Epoch 1: val_loss improved from inf to 557.56891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 298s - loss: 557.7808 - MinusLogProbMetric: 557.7808 - val_loss: 557.5689 - val_MinusLogProbMetric: 557.5689 - lr: 5.0805e-08 - 298s/epoch - 2s/step
Epoch 2/1000
2023-10-25 17:01:37.005 
Epoch 2/1000 
	 loss: 556.9767, MinusLogProbMetric: 556.9767, val_loss: 556.7213, val_MinusLogProbMetric: 556.7213

Epoch 2: val_loss improved from 557.56891 to 556.72131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 85s - loss: 556.9767 - MinusLogProbMetric: 556.9767 - val_loss: 556.7213 - val_MinusLogProbMetric: 556.7213 - lr: 5.0805e-08 - 85s/epoch - 436ms/step
Epoch 3/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 30: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 17:01:56.654 
Epoch 3/1000 
	 loss: nan, MinusLogProbMetric: 556.8821, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 3: val_loss did not improve from 556.72131
196/196 - 18s - loss: nan - MinusLogProbMetric: 556.8821 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 18s/epoch - 93ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 360.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_360/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_360
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7ff0c0a53370>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff2c85710c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff2c85710c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff30822bca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0c0a35ab0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0c0a36020>, <keras.callbacks.ModelCheckpoint object at 0x7ff0c0a360e0>, <keras.callbacks.EarlyStopping object at 0x7ff0c0a36350>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0c0a36380>, <keras.callbacks.TerminateOnNaN object at 0x7ff0c0a35fc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 360/720 with hyperparameters:
timestamp = 2023-10-25 17:02:10.899393
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 17:07:18.090 
Epoch 1/1000 
	 loss: 556.3326, MinusLogProbMetric: 556.3326, val_loss: 556.4495, val_MinusLogProbMetric: 556.4495

Epoch 1: val_loss improved from inf to 556.44946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_360/weights/best_weights.h5
196/196 - 309s - loss: 556.3326 - MinusLogProbMetric: 556.3326 - val_loss: 556.4495 - val_MinusLogProbMetric: 556.4495 - lr: 1.6935e-08 - 309s/epoch - 2s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 110: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 17:08:09.896 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 555.8557, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 556.44946
196/196 - 50s - loss: nan - MinusLogProbMetric: 555.8557 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 50s/epoch - 253ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 360/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

===========
Generating train data for run 364.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_364/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_364/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_364/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_364
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7ff067843fd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0e8168790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0e8168790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0b8196f80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0678d77f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0678d7d60>, <keras.callbacks.ModelCheckpoint object at 0x7ff0678d7e20>, <keras.callbacks.EarlyStopping object at 0x7ff0678d7fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0678d7d30>, <keras.callbacks.TerminateOnNaN object at 0x7ff0678d7f10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_364/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 364/720 with hyperparameters:
timestamp = 2023-10-25 17:08:17.355901
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 30: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 17:10:04.270 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3012.5410, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 107s - loss: nan - MinusLogProbMetric: 3012.5410 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 107s/epoch - 544ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 364.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_364/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_364/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_364/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_364
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_132"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_133 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7ff7732b3b20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff77360e920>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff77360e920>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff77bf19a80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff773312740>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff773312cb0>, <keras.callbacks.ModelCheckpoint object at 0x7ff773312d70>, <keras.callbacks.EarlyStopping object at 0x7ff773312fe0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff773313010>, <keras.callbacks.TerminateOnNaN object at 0x7ff773312c50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_364/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 364/720 with hyperparameters:
timestamp = 2023-10-25 17:10:11.346518
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 17:12:17.234 
Epoch 1/1000 
	 loss: 519.6808, MinusLogProbMetric: 519.6808, val_loss: 138.5068, val_MinusLogProbMetric: 138.5068

Epoch 1: val_loss improved from inf to 138.50676, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 127s - loss: 519.6808 - MinusLogProbMetric: 519.6808 - val_loss: 138.5068 - val_MinusLogProbMetric: 138.5068 - lr: 3.3333e-04 - 127s/epoch - 646ms/step
Epoch 2/1000
2023-10-25 17:13:02.646 
Epoch 2/1000 
	 loss: 112.0094, MinusLogProbMetric: 112.0094, val_loss: 91.0375, val_MinusLogProbMetric: 91.0375

Epoch 2: val_loss improved from 138.50676 to 91.03748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 112.0094 - MinusLogProbMetric: 112.0094 - val_loss: 91.0375 - val_MinusLogProbMetric: 91.0375 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 3/1000
2023-10-25 17:13:47.839 
Epoch 3/1000 
	 loss: 81.5643, MinusLogProbMetric: 81.5643, val_loss: 73.7731, val_MinusLogProbMetric: 73.7731

Epoch 3: val_loss improved from 91.03748 to 73.77310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 81.5643 - MinusLogProbMetric: 81.5643 - val_loss: 73.7731 - val_MinusLogProbMetric: 73.7731 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 4/1000
2023-10-25 17:14:32.709 
Epoch 4/1000 
	 loss: 66.7785, MinusLogProbMetric: 66.7785, val_loss: 61.5708, val_MinusLogProbMetric: 61.5708

Epoch 4: val_loss improved from 73.77310 to 61.57077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 66.7785 - MinusLogProbMetric: 66.7785 - val_loss: 61.5708 - val_MinusLogProbMetric: 61.5708 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 5/1000
2023-10-25 17:15:17.635 
Epoch 5/1000 
	 loss: 58.6078, MinusLogProbMetric: 58.6078, val_loss: 55.2425, val_MinusLogProbMetric: 55.2425

Epoch 5: val_loss improved from 61.57077 to 55.24245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 58.6078 - MinusLogProbMetric: 58.6078 - val_loss: 55.2425 - val_MinusLogProbMetric: 55.2425 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 6/1000
2023-10-25 17:16:03.246 
Epoch 6/1000 
	 loss: 53.2568, MinusLogProbMetric: 53.2568, val_loss: 51.5395, val_MinusLogProbMetric: 51.5395

Epoch 6: val_loss improved from 55.24245 to 51.53948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 53.2568 - MinusLogProbMetric: 53.2568 - val_loss: 51.5395 - val_MinusLogProbMetric: 51.5395 - lr: 3.3333e-04 - 45s/epoch - 232ms/step
Epoch 7/1000
2023-10-25 17:16:48.694 
Epoch 7/1000 
	 loss: 49.6518, MinusLogProbMetric: 49.6518, val_loss: 48.4680, val_MinusLogProbMetric: 48.4680

Epoch 7: val_loss improved from 51.53948 to 48.46796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 49.6518 - MinusLogProbMetric: 49.6518 - val_loss: 48.4680 - val_MinusLogProbMetric: 48.4680 - lr: 3.3333e-04 - 45s/epoch - 232ms/step
Epoch 8/1000
2023-10-25 17:17:33.289 
Epoch 8/1000 
	 loss: 47.1652, MinusLogProbMetric: 47.1652, val_loss: 46.4598, val_MinusLogProbMetric: 46.4598

Epoch 8: val_loss improved from 48.46796 to 46.45984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 47.1652 - MinusLogProbMetric: 47.1652 - val_loss: 46.4598 - val_MinusLogProbMetric: 46.4598 - lr: 3.3333e-04 - 45s/epoch - 228ms/step
Epoch 9/1000
2023-10-25 17:18:18.541 
Epoch 9/1000 
	 loss: 45.4593, MinusLogProbMetric: 45.4593, val_loss: 43.8206, val_MinusLogProbMetric: 43.8206

Epoch 9: val_loss improved from 46.45984 to 43.82060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 45.4593 - MinusLogProbMetric: 45.4593 - val_loss: 43.8206 - val_MinusLogProbMetric: 43.8206 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 10/1000
2023-10-25 17:19:03.512 
Epoch 10/1000 
	 loss: 43.8752, MinusLogProbMetric: 43.8752, val_loss: 43.6207, val_MinusLogProbMetric: 43.6207

Epoch 10: val_loss improved from 43.82060 to 43.62066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 43.8752 - MinusLogProbMetric: 43.8752 - val_loss: 43.6207 - val_MinusLogProbMetric: 43.6207 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 11/1000
2023-10-25 17:19:47.468 
Epoch 11/1000 
	 loss: 42.1443, MinusLogProbMetric: 42.1443, val_loss: 42.5985, val_MinusLogProbMetric: 42.5985

Epoch 11: val_loss improved from 43.62066 to 42.59846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 42.1443 - MinusLogProbMetric: 42.1443 - val_loss: 42.5985 - val_MinusLogProbMetric: 42.5985 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 12/1000
2023-10-25 17:20:31.844 
Epoch 12/1000 
	 loss: 41.5766, MinusLogProbMetric: 41.5766, val_loss: 41.7646, val_MinusLogProbMetric: 41.7646

Epoch 12: val_loss improved from 42.59846 to 41.76462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 41.5766 - MinusLogProbMetric: 41.5766 - val_loss: 41.7646 - val_MinusLogProbMetric: 41.7646 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 13/1000
2023-10-25 17:21:16.294 
Epoch 13/1000 
	 loss: 40.6290, MinusLogProbMetric: 40.6290, val_loss: 40.4066, val_MinusLogProbMetric: 40.4066

Epoch 13: val_loss improved from 41.76462 to 40.40657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 40.6290 - MinusLogProbMetric: 40.6290 - val_loss: 40.4066 - val_MinusLogProbMetric: 40.4066 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 14/1000
2023-10-25 17:22:01.689 
Epoch 14/1000 
	 loss: 39.8021, MinusLogProbMetric: 39.8021, val_loss: 40.0237, val_MinusLogProbMetric: 40.0237

Epoch 14: val_loss improved from 40.40657 to 40.02371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 46s - loss: 39.8021 - MinusLogProbMetric: 39.8021 - val_loss: 40.0237 - val_MinusLogProbMetric: 40.0237 - lr: 3.3333e-04 - 46s/epoch - 232ms/step
Epoch 15/1000
2023-10-25 17:22:47.364 
Epoch 15/1000 
	 loss: 39.1951, MinusLogProbMetric: 39.1951, val_loss: 39.8429, val_MinusLogProbMetric: 39.8429

Epoch 15: val_loss improved from 40.02371 to 39.84295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 46s - loss: 39.1951 - MinusLogProbMetric: 39.1951 - val_loss: 39.8429 - val_MinusLogProbMetric: 39.8429 - lr: 3.3333e-04 - 46s/epoch - 233ms/step
Epoch 16/1000
2023-10-25 17:23:32.059 
Epoch 16/1000 
	 loss: 38.5651, MinusLogProbMetric: 38.5651, val_loss: 38.9082, val_MinusLogProbMetric: 38.9082

Epoch 16: val_loss improved from 39.84295 to 38.90824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 38.5651 - MinusLogProbMetric: 38.5651 - val_loss: 38.9082 - val_MinusLogProbMetric: 38.9082 - lr: 3.3333e-04 - 45s/epoch - 227ms/step
Epoch 17/1000
2023-10-25 17:24:16.513 
Epoch 17/1000 
	 loss: 38.4235, MinusLogProbMetric: 38.4235, val_loss: 38.3610, val_MinusLogProbMetric: 38.3610

Epoch 17: val_loss improved from 38.90824 to 38.36095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 38.4235 - MinusLogProbMetric: 38.4235 - val_loss: 38.3610 - val_MinusLogProbMetric: 38.3610 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 18/1000
2023-10-25 17:25:01.345 
Epoch 18/1000 
	 loss: 37.8801, MinusLogProbMetric: 37.8801, val_loss: 37.2014, val_MinusLogProbMetric: 37.2014

Epoch 18: val_loss improved from 38.36095 to 37.20138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 37.8801 - MinusLogProbMetric: 37.8801 - val_loss: 37.2014 - val_MinusLogProbMetric: 37.2014 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 19/1000
2023-10-25 17:25:46.279 
Epoch 19/1000 
	 loss: 37.3495, MinusLogProbMetric: 37.3495, val_loss: 37.5548, val_MinusLogProbMetric: 37.5548

Epoch 19: val_loss did not improve from 37.20138
196/196 - 44s - loss: 37.3495 - MinusLogProbMetric: 37.3495 - val_loss: 37.5548 - val_MinusLogProbMetric: 37.5548 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 20/1000
2023-10-25 17:26:29.946 
Epoch 20/1000 
	 loss: 37.0850, MinusLogProbMetric: 37.0850, val_loss: 37.0894, val_MinusLogProbMetric: 37.0894

Epoch 20: val_loss improved from 37.20138 to 37.08943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 37.0850 - MinusLogProbMetric: 37.0850 - val_loss: 37.0894 - val_MinusLogProbMetric: 37.0894 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 21/1000
2023-10-25 17:27:14.829 
Epoch 21/1000 
	 loss: 36.6932, MinusLogProbMetric: 36.6932, val_loss: 36.2732, val_MinusLogProbMetric: 36.2732

Epoch 21: val_loss improved from 37.08943 to 36.27319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 36.6932 - MinusLogProbMetric: 36.6932 - val_loss: 36.2732 - val_MinusLogProbMetric: 36.2732 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 22/1000
2023-10-25 17:27:59.162 
Epoch 22/1000 
	 loss: 36.4806, MinusLogProbMetric: 36.4806, val_loss: 37.2701, val_MinusLogProbMetric: 37.2701

Epoch 22: val_loss did not improve from 36.27319
196/196 - 43s - loss: 36.4806 - MinusLogProbMetric: 36.4806 - val_loss: 37.2701 - val_MinusLogProbMetric: 37.2701 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 23/1000
2023-10-25 17:28:43.217 
Epoch 23/1000 
	 loss: 36.2431, MinusLogProbMetric: 36.2431, val_loss: 35.8362, val_MinusLogProbMetric: 35.8362

Epoch 23: val_loss improved from 36.27319 to 35.83620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 36.2431 - MinusLogProbMetric: 36.2431 - val_loss: 35.8362 - val_MinusLogProbMetric: 35.8362 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 24/1000
2023-10-25 17:29:28.298 
Epoch 24/1000 
	 loss: 36.0313, MinusLogProbMetric: 36.0313, val_loss: 42.0199, val_MinusLogProbMetric: 42.0199

Epoch 24: val_loss did not improve from 35.83620
196/196 - 44s - loss: 36.0313 - MinusLogProbMetric: 36.0313 - val_loss: 42.0199 - val_MinusLogProbMetric: 42.0199 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 25/1000
2023-10-25 17:30:12.479 
Epoch 25/1000 
	 loss: 35.9634, MinusLogProbMetric: 35.9634, val_loss: 35.6564, val_MinusLogProbMetric: 35.6564

Epoch 25: val_loss improved from 35.83620 to 35.65636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 35.9634 - MinusLogProbMetric: 35.9634 - val_loss: 35.6564 - val_MinusLogProbMetric: 35.6564 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 26/1000
2023-10-25 17:30:57.057 
Epoch 26/1000 
	 loss: 35.5343, MinusLogProbMetric: 35.5343, val_loss: 36.8615, val_MinusLogProbMetric: 36.8615

Epoch 26: val_loss did not improve from 35.65636
196/196 - 44s - loss: 35.5343 - MinusLogProbMetric: 35.5343 - val_loss: 36.8615 - val_MinusLogProbMetric: 36.8615 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 27/1000
2023-10-25 17:31:40.727 
Epoch 27/1000 
	 loss: 35.4641, MinusLogProbMetric: 35.4641, val_loss: 34.8736, val_MinusLogProbMetric: 34.8736

Epoch 27: val_loss improved from 35.65636 to 34.87356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 35.4641 - MinusLogProbMetric: 35.4641 - val_loss: 34.8736 - val_MinusLogProbMetric: 34.8736 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 28/1000
2023-10-25 17:32:25.416 
Epoch 28/1000 
	 loss: 35.0873, MinusLogProbMetric: 35.0873, val_loss: 35.6977, val_MinusLogProbMetric: 35.6977

Epoch 28: val_loss did not improve from 34.87356
196/196 - 44s - loss: 35.0873 - MinusLogProbMetric: 35.0873 - val_loss: 35.6977 - val_MinusLogProbMetric: 35.6977 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 29/1000
2023-10-25 17:33:09.400 
Epoch 29/1000 
	 loss: 34.9128, MinusLogProbMetric: 34.9128, val_loss: 36.5360, val_MinusLogProbMetric: 36.5360

Epoch 29: val_loss did not improve from 34.87356
196/196 - 44s - loss: 34.9128 - MinusLogProbMetric: 34.9128 - val_loss: 36.5360 - val_MinusLogProbMetric: 36.5360 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 30/1000
2023-10-25 17:33:53.532 
Epoch 30/1000 
	 loss: 34.8870, MinusLogProbMetric: 34.8870, val_loss: 34.6842, val_MinusLogProbMetric: 34.6842

Epoch 30: val_loss improved from 34.87356 to 34.68425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 34.8870 - MinusLogProbMetric: 34.8870 - val_loss: 34.6842 - val_MinusLogProbMetric: 34.6842 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 31/1000
2023-10-25 17:34:38.498 
Epoch 31/1000 
	 loss: 34.5447, MinusLogProbMetric: 34.5447, val_loss: 34.1531, val_MinusLogProbMetric: 34.1531

Epoch 31: val_loss improved from 34.68425 to 34.15305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 34.5447 - MinusLogProbMetric: 34.5447 - val_loss: 34.1531 - val_MinusLogProbMetric: 34.1531 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 32/1000
2023-10-25 17:35:23.556 
Epoch 32/1000 
	 loss: 34.3184, MinusLogProbMetric: 34.3184, val_loss: 34.4224, val_MinusLogProbMetric: 34.4224

Epoch 32: val_loss did not improve from 34.15305
196/196 - 44s - loss: 34.3184 - MinusLogProbMetric: 34.3184 - val_loss: 34.4224 - val_MinusLogProbMetric: 34.4224 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 33/1000
2023-10-25 17:36:07.113 
Epoch 33/1000 
	 loss: 34.2173, MinusLogProbMetric: 34.2173, val_loss: 35.2671, val_MinusLogProbMetric: 35.2671

Epoch 33: val_loss did not improve from 34.15305
196/196 - 44s - loss: 34.2173 - MinusLogProbMetric: 34.2173 - val_loss: 35.2671 - val_MinusLogProbMetric: 35.2671 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 34/1000
2023-10-25 17:36:50.638 
Epoch 34/1000 
	 loss: 34.2252, MinusLogProbMetric: 34.2252, val_loss: 34.1051, val_MinusLogProbMetric: 34.1051

Epoch 34: val_loss improved from 34.15305 to 34.10514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 34.2252 - MinusLogProbMetric: 34.2252 - val_loss: 34.1051 - val_MinusLogProbMetric: 34.1051 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 35/1000
2023-10-25 17:37:35.873 
Epoch 35/1000 
	 loss: 33.9718, MinusLogProbMetric: 33.9718, val_loss: 34.1583, val_MinusLogProbMetric: 34.1583

Epoch 35: val_loss did not improve from 34.10514
196/196 - 44s - loss: 33.9718 - MinusLogProbMetric: 33.9718 - val_loss: 34.1583 - val_MinusLogProbMetric: 34.1583 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 36/1000
2023-10-25 17:38:20.117 
Epoch 36/1000 
	 loss: 33.8160, MinusLogProbMetric: 33.8160, val_loss: 33.9250, val_MinusLogProbMetric: 33.9250

Epoch 36: val_loss improved from 34.10514 to 33.92505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 33.8160 - MinusLogProbMetric: 33.8160 - val_loss: 33.9250 - val_MinusLogProbMetric: 33.9250 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 37/1000
2023-10-25 17:39:04.689 
Epoch 37/1000 
	 loss: 33.6727, MinusLogProbMetric: 33.6727, val_loss: 34.1472, val_MinusLogProbMetric: 34.1472

Epoch 37: val_loss did not improve from 33.92505
196/196 - 44s - loss: 33.6727 - MinusLogProbMetric: 33.6727 - val_loss: 34.1472 - val_MinusLogProbMetric: 34.1472 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 38/1000
2023-10-25 17:39:48.857 
Epoch 38/1000 
	 loss: 33.5611, MinusLogProbMetric: 33.5611, val_loss: 34.5551, val_MinusLogProbMetric: 34.5551

Epoch 38: val_loss did not improve from 33.92505
196/196 - 44s - loss: 33.5611 - MinusLogProbMetric: 33.5611 - val_loss: 34.5551 - val_MinusLogProbMetric: 34.5551 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 39/1000
2023-10-25 17:40:32.077 
Epoch 39/1000 
	 loss: 33.6392, MinusLogProbMetric: 33.6392, val_loss: 33.2249, val_MinusLogProbMetric: 33.2249

Epoch 39: val_loss improved from 33.92505 to 33.22492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 33.6392 - MinusLogProbMetric: 33.6392 - val_loss: 33.2249 - val_MinusLogProbMetric: 33.2249 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 40/1000
2023-10-25 17:41:17.175 
Epoch 40/1000 
	 loss: 33.5348, MinusLogProbMetric: 33.5348, val_loss: 34.3026, val_MinusLogProbMetric: 34.3026

Epoch 40: val_loss did not improve from 33.22492
196/196 - 44s - loss: 33.5348 - MinusLogProbMetric: 33.5348 - val_loss: 34.3026 - val_MinusLogProbMetric: 34.3026 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 41/1000
2023-10-25 17:42:01.427 
Epoch 41/1000 
	 loss: 33.4125, MinusLogProbMetric: 33.4125, val_loss: 34.3108, val_MinusLogProbMetric: 34.3108

Epoch 41: val_loss did not improve from 33.22492
196/196 - 44s - loss: 33.4125 - MinusLogProbMetric: 33.4125 - val_loss: 34.3108 - val_MinusLogProbMetric: 34.3108 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 42/1000
2023-10-25 17:42:45.715 
Epoch 42/1000 
	 loss: 33.3215, MinusLogProbMetric: 33.3215, val_loss: 33.4779, val_MinusLogProbMetric: 33.4779

Epoch 42: val_loss did not improve from 33.22492
196/196 - 44s - loss: 33.3215 - MinusLogProbMetric: 33.3215 - val_loss: 33.4779 - val_MinusLogProbMetric: 33.4779 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 43/1000
2023-10-25 17:43:29.864 
Epoch 43/1000 
	 loss: 33.0674, MinusLogProbMetric: 33.0674, val_loss: 33.0641, val_MinusLogProbMetric: 33.0641

Epoch 43: val_loss improved from 33.22492 to 33.06408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 33.0674 - MinusLogProbMetric: 33.0674 - val_loss: 33.0641 - val_MinusLogProbMetric: 33.0641 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 44/1000
2023-10-25 17:44:15.277 
Epoch 44/1000 
	 loss: 33.1032, MinusLogProbMetric: 33.1032, val_loss: 33.4374, val_MinusLogProbMetric: 33.4374

Epoch 44: val_loss did not improve from 33.06408
196/196 - 44s - loss: 33.1032 - MinusLogProbMetric: 33.1032 - val_loss: 33.4374 - val_MinusLogProbMetric: 33.4374 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 45/1000
2023-10-25 17:44:59.710 
Epoch 45/1000 
	 loss: 33.1318, MinusLogProbMetric: 33.1318, val_loss: 33.4552, val_MinusLogProbMetric: 33.4552

Epoch 45: val_loss did not improve from 33.06408
196/196 - 44s - loss: 33.1318 - MinusLogProbMetric: 33.1318 - val_loss: 33.4552 - val_MinusLogProbMetric: 33.4552 - lr: 3.3333e-04 - 44s/epoch - 227ms/step
Epoch 46/1000
2023-10-25 17:45:44.519 
Epoch 46/1000 
	 loss: 33.1150, MinusLogProbMetric: 33.1150, val_loss: 32.9957, val_MinusLogProbMetric: 32.9957

Epoch 46: val_loss improved from 33.06408 to 32.99573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 46s - loss: 33.1150 - MinusLogProbMetric: 33.1150 - val_loss: 32.9957 - val_MinusLogProbMetric: 32.9957 - lr: 3.3333e-04 - 46s/epoch - 232ms/step
Epoch 47/1000
2023-10-25 17:46:28.818 
Epoch 47/1000 
	 loss: 32.7461, MinusLogProbMetric: 32.7461, val_loss: 33.6897, val_MinusLogProbMetric: 33.6897

Epoch 47: val_loss did not improve from 32.99573
196/196 - 44s - loss: 32.7461 - MinusLogProbMetric: 32.7461 - val_loss: 33.6897 - val_MinusLogProbMetric: 33.6897 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 48/1000
2023-10-25 17:47:13.008 
Epoch 48/1000 
	 loss: 32.6661, MinusLogProbMetric: 32.6661, val_loss: 33.6686, val_MinusLogProbMetric: 33.6686

Epoch 48: val_loss did not improve from 32.99573
196/196 - 44s - loss: 32.6661 - MinusLogProbMetric: 32.6661 - val_loss: 33.6686 - val_MinusLogProbMetric: 33.6686 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 49/1000
2023-10-25 17:47:57.543 
Epoch 49/1000 
	 loss: 32.7128, MinusLogProbMetric: 32.7128, val_loss: 33.1448, val_MinusLogProbMetric: 33.1448

Epoch 49: val_loss did not improve from 32.99573
196/196 - 45s - loss: 32.7128 - MinusLogProbMetric: 32.7128 - val_loss: 33.1448 - val_MinusLogProbMetric: 33.1448 - lr: 3.3333e-04 - 45s/epoch - 227ms/step
Epoch 50/1000
2023-10-25 17:48:41.872 
Epoch 50/1000 
	 loss: 32.6322, MinusLogProbMetric: 32.6322, val_loss: 33.6401, val_MinusLogProbMetric: 33.6401

Epoch 50: val_loss did not improve from 32.99573
196/196 - 44s - loss: 32.6322 - MinusLogProbMetric: 32.6322 - val_loss: 33.6401 - val_MinusLogProbMetric: 33.6401 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 51/1000
2023-10-25 17:49:26.381 
Epoch 51/1000 
	 loss: 32.5677, MinusLogProbMetric: 32.5677, val_loss: 32.3555, val_MinusLogProbMetric: 32.3555

Epoch 51: val_loss improved from 32.99573 to 32.35548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 32.5677 - MinusLogProbMetric: 32.5677 - val_loss: 32.3555 - val_MinusLogProbMetric: 32.3555 - lr: 3.3333e-04 - 45s/epoch - 231ms/step
Epoch 52/1000
2023-10-25 17:50:11.309 
Epoch 52/1000 
	 loss: 32.4839, MinusLogProbMetric: 32.4839, val_loss: 32.3845, val_MinusLogProbMetric: 32.3845

Epoch 52: val_loss did not improve from 32.35548
196/196 - 44s - loss: 32.4839 - MinusLogProbMetric: 32.4839 - val_loss: 32.3845 - val_MinusLogProbMetric: 32.3845 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 53/1000
2023-10-25 17:50:56.116 
Epoch 53/1000 
	 loss: 32.4128, MinusLogProbMetric: 32.4128, val_loss: 32.5219, val_MinusLogProbMetric: 32.5219

Epoch 53: val_loss did not improve from 32.35548
196/196 - 45s - loss: 32.4128 - MinusLogProbMetric: 32.4128 - val_loss: 32.5219 - val_MinusLogProbMetric: 32.5219 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 54/1000
2023-10-25 17:51:39.998 
Epoch 54/1000 
	 loss: 32.4747, MinusLogProbMetric: 32.4747, val_loss: 32.3307, val_MinusLogProbMetric: 32.3307

Epoch 54: val_loss improved from 32.35548 to 32.33067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 45s - loss: 32.4747 - MinusLogProbMetric: 32.4747 - val_loss: 32.3307 - val_MinusLogProbMetric: 32.3307 - lr: 3.3333e-04 - 45s/epoch - 228ms/step
Epoch 55/1000
2023-10-25 17:52:25.133 
Epoch 55/1000 
	 loss: 32.3226, MinusLogProbMetric: 32.3226, val_loss: 32.3756, val_MinusLogProbMetric: 32.3756

Epoch 55: val_loss did not improve from 32.33067
196/196 - 44s - loss: 32.3226 - MinusLogProbMetric: 32.3226 - val_loss: 32.3756 - val_MinusLogProbMetric: 32.3756 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 56/1000
2023-10-25 17:53:09.442 
Epoch 56/1000 
	 loss: 32.2499, MinusLogProbMetric: 32.2499, val_loss: 34.7328, val_MinusLogProbMetric: 34.7328

Epoch 56: val_loss did not improve from 32.33067
196/196 - 44s - loss: 32.2499 - MinusLogProbMetric: 32.2499 - val_loss: 34.7328 - val_MinusLogProbMetric: 34.7328 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 57/1000
2023-10-25 17:53:53.491 
Epoch 57/1000 
	 loss: 32.7587, MinusLogProbMetric: 32.7587, val_loss: 32.9516, val_MinusLogProbMetric: 32.9516

Epoch 57: val_loss did not improve from 32.33067
196/196 - 44s - loss: 32.7587 - MinusLogProbMetric: 32.7587 - val_loss: 32.9516 - val_MinusLogProbMetric: 32.9516 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 58/1000
2023-10-25 17:54:37.149 
Epoch 58/1000 
	 loss: 32.1137, MinusLogProbMetric: 32.1137, val_loss: 33.0735, val_MinusLogProbMetric: 33.0735

Epoch 58: val_loss did not improve from 32.33067
196/196 - 44s - loss: 32.1137 - MinusLogProbMetric: 32.1137 - val_loss: 33.0735 - val_MinusLogProbMetric: 33.0735 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 59/1000
2023-10-25 17:55:19.613 
Epoch 59/1000 
	 loss: 32.2902, MinusLogProbMetric: 32.2902, val_loss: 32.2109, val_MinusLogProbMetric: 32.2109

Epoch 59: val_loss improved from 32.33067 to 32.21092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 32.2902 - MinusLogProbMetric: 32.2902 - val_loss: 32.2109 - val_MinusLogProbMetric: 32.2109 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 60/1000
2023-10-25 17:55:54.899 
Epoch 60/1000 
	 loss: 32.0773, MinusLogProbMetric: 32.0773, val_loss: 32.3425, val_MinusLogProbMetric: 32.3425

Epoch 60: val_loss did not improve from 32.21092
196/196 - 35s - loss: 32.0773 - MinusLogProbMetric: 32.0773 - val_loss: 32.3425 - val_MinusLogProbMetric: 32.3425 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 61/1000
2023-10-25 17:56:31.428 
Epoch 61/1000 
	 loss: 32.3730, MinusLogProbMetric: 32.3730, val_loss: 32.1248, val_MinusLogProbMetric: 32.1248

Epoch 61: val_loss improved from 32.21092 to 32.12479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 37s - loss: 32.3730 - MinusLogProbMetric: 32.3730 - val_loss: 32.1248 - val_MinusLogProbMetric: 32.1248 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 62/1000
2023-10-25 17:57:12.823 
Epoch 62/1000 
	 loss: 32.0260, MinusLogProbMetric: 32.0260, val_loss: 31.8683, val_MinusLogProbMetric: 31.8683

Epoch 62: val_loss improved from 32.12479 to 31.86834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 41s - loss: 32.0260 - MinusLogProbMetric: 32.0260 - val_loss: 31.8683 - val_MinusLogProbMetric: 31.8683 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 63/1000
2023-10-25 17:57:54.999 
Epoch 63/1000 
	 loss: 31.8554, MinusLogProbMetric: 31.8554, val_loss: 32.9472, val_MinusLogProbMetric: 32.9472

Epoch 63: val_loss did not improve from 31.86834
196/196 - 41s - loss: 31.8554 - MinusLogProbMetric: 31.8554 - val_loss: 32.9472 - val_MinusLogProbMetric: 32.9472 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 64/1000
2023-10-25 17:58:32.769 
Epoch 64/1000 
	 loss: 32.0114, MinusLogProbMetric: 32.0114, val_loss: 31.9189, val_MinusLogProbMetric: 31.9189

Epoch 64: val_loss did not improve from 31.86834
196/196 - 38s - loss: 32.0114 - MinusLogProbMetric: 32.0114 - val_loss: 31.9189 - val_MinusLogProbMetric: 31.9189 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 65/1000
2023-10-25 17:59:14.522 
Epoch 65/1000 
	 loss: 31.7770, MinusLogProbMetric: 31.7770, val_loss: 31.4451, val_MinusLogProbMetric: 31.4451

Epoch 65: val_loss improved from 31.86834 to 31.44510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 42s - loss: 31.7770 - MinusLogProbMetric: 31.7770 - val_loss: 31.4451 - val_MinusLogProbMetric: 31.4451 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 66/1000
2023-10-25 17:59:54.111 
Epoch 66/1000 
	 loss: 31.6881, MinusLogProbMetric: 31.6881, val_loss: 34.1977, val_MinusLogProbMetric: 34.1977

Epoch 66: val_loss did not improve from 31.44510
196/196 - 39s - loss: 31.6881 - MinusLogProbMetric: 31.6881 - val_loss: 34.1977 - val_MinusLogProbMetric: 34.1977 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 67/1000
2023-10-25 18:00:36.044 
Epoch 67/1000 
	 loss: 31.7980, MinusLogProbMetric: 31.7980, val_loss: 32.6777, val_MinusLogProbMetric: 32.6777

Epoch 67: val_loss did not improve from 31.44510
196/196 - 42s - loss: 31.7980 - MinusLogProbMetric: 31.7980 - val_loss: 32.6777 - val_MinusLogProbMetric: 32.6777 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 68/1000
2023-10-25 18:01:15.351 
Epoch 68/1000 
	 loss: 31.8516, MinusLogProbMetric: 31.8516, val_loss: 35.2652, val_MinusLogProbMetric: 35.2652

Epoch 68: val_loss did not improve from 31.44510
196/196 - 39s - loss: 31.8516 - MinusLogProbMetric: 31.8516 - val_loss: 35.2652 - val_MinusLogProbMetric: 35.2652 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 69/1000
2023-10-25 18:01:52.207 
Epoch 69/1000 
	 loss: 31.8979, MinusLogProbMetric: 31.8979, val_loss: 32.3294, val_MinusLogProbMetric: 32.3294

Epoch 69: val_loss did not improve from 31.44510
196/196 - 37s - loss: 31.8979 - MinusLogProbMetric: 31.8979 - val_loss: 32.3294 - val_MinusLogProbMetric: 32.3294 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 70/1000
2023-10-25 18:02:27.396 
Epoch 70/1000 
	 loss: 31.6693, MinusLogProbMetric: 31.6693, val_loss: 33.6350, val_MinusLogProbMetric: 33.6350

Epoch 70: val_loss did not improve from 31.44510
196/196 - 35s - loss: 31.6693 - MinusLogProbMetric: 31.6693 - val_loss: 33.6350 - val_MinusLogProbMetric: 33.6350 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 71/1000
2023-10-25 18:03:09.062 
Epoch 71/1000 
	 loss: 31.6087, MinusLogProbMetric: 31.6087, val_loss: 31.5820, val_MinusLogProbMetric: 31.5820

Epoch 71: val_loss did not improve from 31.44510
196/196 - 42s - loss: 31.6087 - MinusLogProbMetric: 31.6087 - val_loss: 31.5820 - val_MinusLogProbMetric: 31.5820 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 72/1000
2023-10-25 18:03:48.164 
Epoch 72/1000 
	 loss: 31.5918, MinusLogProbMetric: 31.5918, val_loss: 31.6595, val_MinusLogProbMetric: 31.6595

Epoch 72: val_loss did not improve from 31.44510
196/196 - 39s - loss: 31.5918 - MinusLogProbMetric: 31.5918 - val_loss: 31.6595 - val_MinusLogProbMetric: 31.6595 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 73/1000
2023-10-25 18:04:26.140 
Epoch 73/1000 
	 loss: 31.5285, MinusLogProbMetric: 31.5285, val_loss: 32.2952, val_MinusLogProbMetric: 32.2952

Epoch 73: val_loss did not improve from 31.44510
196/196 - 38s - loss: 31.5285 - MinusLogProbMetric: 31.5285 - val_loss: 32.2952 - val_MinusLogProbMetric: 32.2952 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 74/1000
2023-10-25 18:05:04.603 
Epoch 74/1000 
	 loss: 31.4734, MinusLogProbMetric: 31.4734, val_loss: 31.5289, val_MinusLogProbMetric: 31.5289

Epoch 74: val_loss did not improve from 31.44510
196/196 - 38s - loss: 31.4734 - MinusLogProbMetric: 31.4734 - val_loss: 31.5289 - val_MinusLogProbMetric: 31.5289 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 75/1000
2023-10-25 18:05:46.432 
Epoch 75/1000 
	 loss: 31.6518, MinusLogProbMetric: 31.6518, val_loss: 32.0058, val_MinusLogProbMetric: 32.0058

Epoch 75: val_loss did not improve from 31.44510
196/196 - 42s - loss: 31.6518 - MinusLogProbMetric: 31.6518 - val_loss: 32.0058 - val_MinusLogProbMetric: 32.0058 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 76/1000
2023-10-25 18:06:28.449 
Epoch 76/1000 
	 loss: 31.3829, MinusLogProbMetric: 31.3829, val_loss: 31.6291, val_MinusLogProbMetric: 31.6291

Epoch 76: val_loss did not improve from 31.44510
196/196 - 42s - loss: 31.3829 - MinusLogProbMetric: 31.3829 - val_loss: 31.6291 - val_MinusLogProbMetric: 31.6291 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 77/1000
2023-10-25 18:07:09.057 
Epoch 77/1000 
	 loss: 31.4008, MinusLogProbMetric: 31.4008, val_loss: 31.6071, val_MinusLogProbMetric: 31.6071

Epoch 77: val_loss did not improve from 31.44510
196/196 - 41s - loss: 31.4008 - MinusLogProbMetric: 31.4008 - val_loss: 31.6071 - val_MinusLogProbMetric: 31.6071 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 78/1000
2023-10-25 18:07:49.342 
Epoch 78/1000 
	 loss: 31.5498, MinusLogProbMetric: 31.5498, val_loss: 31.2506, val_MinusLogProbMetric: 31.2506

Epoch 78: val_loss improved from 31.44510 to 31.25060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 41s - loss: 31.5498 - MinusLogProbMetric: 31.5498 - val_loss: 31.2506 - val_MinusLogProbMetric: 31.2506 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 79/1000
2023-10-25 18:08:29.655 
Epoch 79/1000 
	 loss: 31.3216, MinusLogProbMetric: 31.3216, val_loss: 30.8277, val_MinusLogProbMetric: 30.8277

Epoch 79: val_loss improved from 31.25060 to 30.82767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 40s - loss: 31.3216 - MinusLogProbMetric: 31.3216 - val_loss: 30.8277 - val_MinusLogProbMetric: 30.8277 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 80/1000
2023-10-25 18:09:11.242 
Epoch 80/1000 
	 loss: 31.2834, MinusLogProbMetric: 31.2834, val_loss: 31.8880, val_MinusLogProbMetric: 31.8880

Epoch 80: val_loss did not improve from 30.82767
196/196 - 41s - loss: 31.2834 - MinusLogProbMetric: 31.2834 - val_loss: 31.8880 - val_MinusLogProbMetric: 31.8880 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 81/1000
2023-10-25 18:09:53.075 
Epoch 81/1000 
	 loss: 31.7226, MinusLogProbMetric: 31.7226, val_loss: 32.4030, val_MinusLogProbMetric: 32.4030

Epoch 81: val_loss did not improve from 30.82767
196/196 - 42s - loss: 31.7226 - MinusLogProbMetric: 31.7226 - val_loss: 32.4030 - val_MinusLogProbMetric: 32.4030 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 82/1000
2023-10-25 18:10:31.713 
Epoch 82/1000 
	 loss: 31.1936, MinusLogProbMetric: 31.1936, val_loss: 31.1498, val_MinusLogProbMetric: 31.1498

Epoch 82: val_loss did not improve from 30.82767
196/196 - 39s - loss: 31.1936 - MinusLogProbMetric: 31.1936 - val_loss: 31.1498 - val_MinusLogProbMetric: 31.1498 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 83/1000
2023-10-25 18:11:08.129 
Epoch 83/1000 
	 loss: 31.2293, MinusLogProbMetric: 31.2293, val_loss: 31.3324, val_MinusLogProbMetric: 31.3324

Epoch 83: val_loss did not improve from 30.82767
196/196 - 36s - loss: 31.2293 - MinusLogProbMetric: 31.2293 - val_loss: 31.3324 - val_MinusLogProbMetric: 31.3324 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 84/1000
2023-10-25 18:11:48.369 
Epoch 84/1000 
	 loss: 31.1475, MinusLogProbMetric: 31.1475, val_loss: 31.4006, val_MinusLogProbMetric: 31.4006

Epoch 84: val_loss did not improve from 30.82767
196/196 - 40s - loss: 31.1475 - MinusLogProbMetric: 31.1475 - val_loss: 31.4006 - val_MinusLogProbMetric: 31.4006 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 85/1000
2023-10-25 18:12:29.346 
Epoch 85/1000 
	 loss: 31.2793, MinusLogProbMetric: 31.2793, val_loss: 31.3384, val_MinusLogProbMetric: 31.3384

Epoch 85: val_loss did not improve from 30.82767
196/196 - 41s - loss: 31.2793 - MinusLogProbMetric: 31.2793 - val_loss: 31.3384 - val_MinusLogProbMetric: 31.3384 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 86/1000
2023-10-25 18:13:04.510 
Epoch 86/1000 
	 loss: 31.3184, MinusLogProbMetric: 31.3184, val_loss: 31.6683, val_MinusLogProbMetric: 31.6683

Epoch 86: val_loss did not improve from 30.82767
196/196 - 35s - loss: 31.3184 - MinusLogProbMetric: 31.3184 - val_loss: 31.6683 - val_MinusLogProbMetric: 31.6683 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 87/1000
2023-10-25 18:13:42.059 
Epoch 87/1000 
	 loss: 31.1932, MinusLogProbMetric: 31.1932, val_loss: 31.4077, val_MinusLogProbMetric: 31.4077

Epoch 87: val_loss did not improve from 30.82767
196/196 - 38s - loss: 31.1932 - MinusLogProbMetric: 31.1932 - val_loss: 31.4077 - val_MinusLogProbMetric: 31.4077 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 88/1000
2023-10-25 18:14:21.293 
Epoch 88/1000 
	 loss: 31.1844, MinusLogProbMetric: 31.1844, val_loss: 31.0450, val_MinusLogProbMetric: 31.0450

Epoch 88: val_loss did not improve from 30.82767
196/196 - 39s - loss: 31.1844 - MinusLogProbMetric: 31.1844 - val_loss: 31.0450 - val_MinusLogProbMetric: 31.0450 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 89/1000
2023-10-25 18:15:03.835 
Epoch 89/1000 
	 loss: 31.0446, MinusLogProbMetric: 31.0446, val_loss: 31.1725, val_MinusLogProbMetric: 31.1725

Epoch 89: val_loss did not improve from 30.82767
196/196 - 43s - loss: 31.0446 - MinusLogProbMetric: 31.0446 - val_loss: 31.1725 - val_MinusLogProbMetric: 31.1725 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 90/1000
2023-10-25 18:15:44.045 
Epoch 90/1000 
	 loss: 30.9003, MinusLogProbMetric: 30.9003, val_loss: 31.6883, val_MinusLogProbMetric: 31.6883

Epoch 90: val_loss did not improve from 30.82767
196/196 - 40s - loss: 30.9003 - MinusLogProbMetric: 30.9003 - val_loss: 31.6883 - val_MinusLogProbMetric: 31.6883 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 91/1000
2023-10-25 18:16:21.861 
Epoch 91/1000 
	 loss: 31.1009, MinusLogProbMetric: 31.1009, val_loss: 31.2778, val_MinusLogProbMetric: 31.2778

Epoch 91: val_loss did not improve from 30.82767
196/196 - 38s - loss: 31.1009 - MinusLogProbMetric: 31.1009 - val_loss: 31.2778 - val_MinusLogProbMetric: 31.2778 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 92/1000
2023-10-25 18:16:59.372 
Epoch 92/1000 
	 loss: 30.9458, MinusLogProbMetric: 30.9458, val_loss: 33.6115, val_MinusLogProbMetric: 33.6115

Epoch 92: val_loss did not improve from 30.82767
196/196 - 38s - loss: 30.9458 - MinusLogProbMetric: 30.9458 - val_loss: 33.6115 - val_MinusLogProbMetric: 33.6115 - lr: 3.3333e-04 - 38s/epoch - 191ms/step
Epoch 93/1000
2023-10-25 18:17:40.740 
Epoch 93/1000 
	 loss: 31.1747, MinusLogProbMetric: 31.1747, val_loss: 31.8795, val_MinusLogProbMetric: 31.8795

Epoch 93: val_loss did not improve from 30.82767
196/196 - 41s - loss: 31.1747 - MinusLogProbMetric: 31.1747 - val_loss: 31.8795 - val_MinusLogProbMetric: 31.8795 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 94/1000
2023-10-25 18:18:22.530 
Epoch 94/1000 
	 loss: 31.0385, MinusLogProbMetric: 31.0385, val_loss: 31.1978, val_MinusLogProbMetric: 31.1978

Epoch 94: val_loss did not improve from 30.82767
196/196 - 42s - loss: 31.0385 - MinusLogProbMetric: 31.0385 - val_loss: 31.1978 - val_MinusLogProbMetric: 31.1978 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 95/1000
2023-10-25 18:19:00.802 
Epoch 95/1000 
	 loss: 30.8946, MinusLogProbMetric: 30.8946, val_loss: 32.4853, val_MinusLogProbMetric: 32.4853

Epoch 95: val_loss did not improve from 30.82767
196/196 - 38s - loss: 30.8946 - MinusLogProbMetric: 30.8946 - val_loss: 32.4853 - val_MinusLogProbMetric: 32.4853 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 96/1000
2023-10-25 18:19:42.720 
Epoch 96/1000 
	 loss: 30.9529, MinusLogProbMetric: 30.9529, val_loss: 31.6648, val_MinusLogProbMetric: 31.6648

Epoch 96: val_loss did not improve from 30.82767
196/196 - 42s - loss: 30.9529 - MinusLogProbMetric: 30.9529 - val_loss: 31.6648 - val_MinusLogProbMetric: 31.6648 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 97/1000
2023-10-25 18:20:21.866 
Epoch 97/1000 
	 loss: 30.9688, MinusLogProbMetric: 30.9688, val_loss: 32.1091, val_MinusLogProbMetric: 32.1091

Epoch 97: val_loss did not improve from 30.82767
196/196 - 39s - loss: 30.9688 - MinusLogProbMetric: 30.9688 - val_loss: 32.1091 - val_MinusLogProbMetric: 32.1091 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 98/1000
2023-10-25 18:21:04.401 
Epoch 98/1000 
	 loss: 31.0920, MinusLogProbMetric: 31.0920, val_loss: 31.2891, val_MinusLogProbMetric: 31.2891

Epoch 98: val_loss did not improve from 30.82767
196/196 - 43s - loss: 31.0920 - MinusLogProbMetric: 31.0920 - val_loss: 31.2891 - val_MinusLogProbMetric: 31.2891 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 99/1000
2023-10-25 18:21:44.974 
Epoch 99/1000 
	 loss: 30.8581, MinusLogProbMetric: 30.8581, val_loss: 33.1823, val_MinusLogProbMetric: 33.1823

Epoch 99: val_loss did not improve from 30.82767
196/196 - 41s - loss: 30.8581 - MinusLogProbMetric: 30.8581 - val_loss: 33.1823 - val_MinusLogProbMetric: 33.1823 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 100/1000
2023-10-25 18:22:24.389 
Epoch 100/1000 
	 loss: 30.9079, MinusLogProbMetric: 30.9079, val_loss: 31.3746, val_MinusLogProbMetric: 31.3746

Epoch 100: val_loss did not improve from 30.82767
196/196 - 39s - loss: 30.9079 - MinusLogProbMetric: 30.9079 - val_loss: 31.3746 - val_MinusLogProbMetric: 31.3746 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 101/1000
2023-10-25 18:22:58.910 
Epoch 101/1000 
	 loss: 30.7143, MinusLogProbMetric: 30.7143, val_loss: 31.7604, val_MinusLogProbMetric: 31.7604

Epoch 101: val_loss did not improve from 30.82767
196/196 - 35s - loss: 30.7143 - MinusLogProbMetric: 30.7143 - val_loss: 31.7604 - val_MinusLogProbMetric: 31.7604 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 102/1000
2023-10-25 18:23:40.547 
Epoch 102/1000 
	 loss: 30.7797, MinusLogProbMetric: 30.7797, val_loss: 30.8061, val_MinusLogProbMetric: 30.8061

Epoch 102: val_loss improved from 30.82767 to 30.80609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 42s - loss: 30.7797 - MinusLogProbMetric: 30.7797 - val_loss: 30.8061 - val_MinusLogProbMetric: 30.8061 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 103/1000
2023-10-25 18:24:22.588 
Epoch 103/1000 
	 loss: 30.7941, MinusLogProbMetric: 30.7941, val_loss: 32.4719, val_MinusLogProbMetric: 32.4719

Epoch 103: val_loss did not improve from 30.80609
196/196 - 41s - loss: 30.7941 - MinusLogProbMetric: 30.7941 - val_loss: 32.4719 - val_MinusLogProbMetric: 32.4719 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 104/1000
2023-10-25 18:25:02.073 
Epoch 104/1000 
	 loss: 30.7664, MinusLogProbMetric: 30.7664, val_loss: 30.4192, val_MinusLogProbMetric: 30.4192

Epoch 104: val_loss improved from 30.80609 to 30.41923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 40s - loss: 30.7664 - MinusLogProbMetric: 30.7664 - val_loss: 30.4192 - val_MinusLogProbMetric: 30.4192 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 105/1000
2023-10-25 18:25:40.225 
Epoch 105/1000 
	 loss: 31.1078, MinusLogProbMetric: 31.1078, val_loss: 31.0032, val_MinusLogProbMetric: 31.0032

Epoch 105: val_loss did not improve from 30.41923
196/196 - 38s - loss: 31.1078 - MinusLogProbMetric: 31.1078 - val_loss: 31.0032 - val_MinusLogProbMetric: 31.0032 - lr: 3.3333e-04 - 38s/epoch - 191ms/step
Epoch 106/1000
2023-10-25 18:26:20.871 
Epoch 106/1000 
	 loss: 30.6741, MinusLogProbMetric: 30.6741, val_loss: 31.0679, val_MinusLogProbMetric: 31.0679

Epoch 106: val_loss did not improve from 30.41923
196/196 - 41s - loss: 30.6741 - MinusLogProbMetric: 30.6741 - val_loss: 31.0679 - val_MinusLogProbMetric: 31.0679 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 107/1000
2023-10-25 18:27:03.470 
Epoch 107/1000 
	 loss: 30.6729, MinusLogProbMetric: 30.6729, val_loss: 30.7552, val_MinusLogProbMetric: 30.7552

Epoch 107: val_loss did not improve from 30.41923
196/196 - 43s - loss: 30.6729 - MinusLogProbMetric: 30.6729 - val_loss: 30.7552 - val_MinusLogProbMetric: 30.7552 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 108/1000
2023-10-25 18:27:40.958 
Epoch 108/1000 
	 loss: 30.7558, MinusLogProbMetric: 30.7558, val_loss: 32.7822, val_MinusLogProbMetric: 32.7822

Epoch 108: val_loss did not improve from 30.41923
196/196 - 37s - loss: 30.7558 - MinusLogProbMetric: 30.7558 - val_loss: 32.7822 - val_MinusLogProbMetric: 32.7822 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 109/1000
2023-10-25 18:28:17.896 
Epoch 109/1000 
	 loss: 30.7890, MinusLogProbMetric: 30.7890, val_loss: 31.3408, val_MinusLogProbMetric: 31.3408

Epoch 109: val_loss did not improve from 30.41923
196/196 - 37s - loss: 30.7890 - MinusLogProbMetric: 30.7890 - val_loss: 31.3408 - val_MinusLogProbMetric: 31.3408 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 110/1000
2023-10-25 18:28:57.845 
Epoch 110/1000 
	 loss: 30.7721, MinusLogProbMetric: 30.7721, val_loss: 31.8179, val_MinusLogProbMetric: 31.8179

Epoch 110: val_loss did not improve from 30.41923
196/196 - 40s - loss: 30.7721 - MinusLogProbMetric: 30.7721 - val_loss: 31.8179 - val_MinusLogProbMetric: 31.8179 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 111/1000
2023-10-25 18:29:40.016 
Epoch 111/1000 
	 loss: 30.7293, MinusLogProbMetric: 30.7293, val_loss: 31.5446, val_MinusLogProbMetric: 31.5446

Epoch 111: val_loss did not improve from 30.41923
196/196 - 42s - loss: 30.7293 - MinusLogProbMetric: 30.7293 - val_loss: 31.5446 - val_MinusLogProbMetric: 31.5446 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 112/1000
2023-10-25 18:30:18.062 
Epoch 112/1000 
	 loss: 30.6789, MinusLogProbMetric: 30.6789, val_loss: 31.1846, val_MinusLogProbMetric: 31.1846

Epoch 112: val_loss did not improve from 30.41923
196/196 - 38s - loss: 30.6789 - MinusLogProbMetric: 30.6789 - val_loss: 31.1846 - val_MinusLogProbMetric: 31.1846 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 113/1000
2023-10-25 18:30:56.151 
Epoch 113/1000 
	 loss: 30.7573, MinusLogProbMetric: 30.7573, val_loss: 30.2343, val_MinusLogProbMetric: 30.2343

Epoch 113: val_loss improved from 30.41923 to 30.23427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 39s - loss: 30.7573 - MinusLogProbMetric: 30.7573 - val_loss: 30.2343 - val_MinusLogProbMetric: 30.2343 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 114/1000
2023-10-25 18:31:32.044 
Epoch 114/1000 
	 loss: 30.6833, MinusLogProbMetric: 30.6833, val_loss: 32.1011, val_MinusLogProbMetric: 32.1011

Epoch 114: val_loss did not improve from 30.23427
196/196 - 35s - loss: 30.6833 - MinusLogProbMetric: 30.6833 - val_loss: 32.1011 - val_MinusLogProbMetric: 32.1011 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 115/1000
2023-10-25 18:32:14.538 
Epoch 115/1000 
	 loss: 30.4825, MinusLogProbMetric: 30.4825, val_loss: 33.1844, val_MinusLogProbMetric: 33.1844

Epoch 115: val_loss did not improve from 30.23427
196/196 - 42s - loss: 30.4825 - MinusLogProbMetric: 30.4825 - val_loss: 33.1844 - val_MinusLogProbMetric: 33.1844 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 116/1000
2023-10-25 18:32:56.860 
Epoch 116/1000 
	 loss: 31.3116, MinusLogProbMetric: 31.3116, val_loss: 32.3229, val_MinusLogProbMetric: 32.3229

Epoch 116: val_loss did not improve from 30.23427
196/196 - 42s - loss: 31.3116 - MinusLogProbMetric: 31.3116 - val_loss: 32.3229 - val_MinusLogProbMetric: 32.3229 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 117/1000
2023-10-25 18:33:38.061 
Epoch 117/1000 
	 loss: 30.6030, MinusLogProbMetric: 30.6030, val_loss: 31.0627, val_MinusLogProbMetric: 31.0627

Epoch 117: val_loss did not improve from 30.23427
196/196 - 41s - loss: 30.6030 - MinusLogProbMetric: 30.6030 - val_loss: 31.0627 - val_MinusLogProbMetric: 31.0627 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 118/1000
2023-10-25 18:34:17.677 
Epoch 118/1000 
	 loss: 30.4962, MinusLogProbMetric: 30.4962, val_loss: 30.5863, val_MinusLogProbMetric: 30.5863

Epoch 118: val_loss did not improve from 30.23427
196/196 - 40s - loss: 30.4962 - MinusLogProbMetric: 30.4962 - val_loss: 30.5863 - val_MinusLogProbMetric: 30.5863 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 119/1000
2023-10-25 18:34:56.920 
Epoch 119/1000 
	 loss: 30.6183, MinusLogProbMetric: 30.6183, val_loss: 30.7186, val_MinusLogProbMetric: 30.7186

Epoch 119: val_loss did not improve from 30.23427
196/196 - 39s - loss: 30.6183 - MinusLogProbMetric: 30.6183 - val_loss: 30.7186 - val_MinusLogProbMetric: 30.7186 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 120/1000
2023-10-25 18:35:39.391 
Epoch 120/1000 
	 loss: 30.6374, MinusLogProbMetric: 30.6374, val_loss: 31.0181, val_MinusLogProbMetric: 31.0181

Epoch 120: val_loss did not improve from 30.23427
196/196 - 42s - loss: 30.6374 - MinusLogProbMetric: 30.6374 - val_loss: 31.0181 - val_MinusLogProbMetric: 31.0181 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 121/1000
2023-10-25 18:36:21.814 
Epoch 121/1000 
	 loss: 30.4645, MinusLogProbMetric: 30.4645, val_loss: 30.3675, val_MinusLogProbMetric: 30.3675

Epoch 121: val_loss did not improve from 30.23427
196/196 - 42s - loss: 30.4645 - MinusLogProbMetric: 30.4645 - val_loss: 30.3675 - val_MinusLogProbMetric: 30.3675 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 122/1000
2023-10-25 18:36:58.577 
Epoch 122/1000 
	 loss: 30.6248, MinusLogProbMetric: 30.6248, val_loss: 30.3699, val_MinusLogProbMetric: 30.3699

Epoch 122: val_loss did not improve from 30.23427
196/196 - 37s - loss: 30.6248 - MinusLogProbMetric: 30.6248 - val_loss: 30.3699 - val_MinusLogProbMetric: 30.3699 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 123/1000
2023-10-25 18:37:36.312 
Epoch 123/1000 
	 loss: 30.4268, MinusLogProbMetric: 30.4268, val_loss: 30.6709, val_MinusLogProbMetric: 30.6709

Epoch 123: val_loss did not improve from 30.23427
196/196 - 38s - loss: 30.4268 - MinusLogProbMetric: 30.4268 - val_loss: 30.6709 - val_MinusLogProbMetric: 30.6709 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 124/1000
2023-10-25 18:38:16.960 
Epoch 124/1000 
	 loss: 30.4016, MinusLogProbMetric: 30.4016, val_loss: 31.6272, val_MinusLogProbMetric: 31.6272

Epoch 124: val_loss did not improve from 30.23427
196/196 - 41s - loss: 30.4016 - MinusLogProbMetric: 30.4016 - val_loss: 31.6272 - val_MinusLogProbMetric: 31.6272 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 125/1000
2023-10-25 18:38:58.086 
Epoch 125/1000 
	 loss: 30.3597, MinusLogProbMetric: 30.3597, val_loss: 30.7604, val_MinusLogProbMetric: 30.7604

Epoch 125: val_loss did not improve from 30.23427
196/196 - 41s - loss: 30.3597 - MinusLogProbMetric: 30.3597 - val_loss: 30.7604 - val_MinusLogProbMetric: 30.7604 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 126/1000
2023-10-25 18:39:37.745 
Epoch 126/1000 
	 loss: 30.4312, MinusLogProbMetric: 30.4312, val_loss: 30.6108, val_MinusLogProbMetric: 30.6108

Epoch 126: val_loss did not improve from 30.23427
196/196 - 40s - loss: 30.4312 - MinusLogProbMetric: 30.4312 - val_loss: 30.6108 - val_MinusLogProbMetric: 30.6108 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 127/1000
2023-10-25 18:40:17.748 
Epoch 127/1000 
	 loss: 30.3956, MinusLogProbMetric: 30.3956, val_loss: 32.2020, val_MinusLogProbMetric: 32.2020

Epoch 127: val_loss did not improve from 30.23427
196/196 - 40s - loss: 30.3956 - MinusLogProbMetric: 30.3956 - val_loss: 32.2020 - val_MinusLogProbMetric: 32.2020 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 128/1000
2023-10-25 18:40:58.338 
Epoch 128/1000 
	 loss: 30.3961, MinusLogProbMetric: 30.3961, val_loss: 31.0971, val_MinusLogProbMetric: 31.0971

Epoch 128: val_loss did not improve from 30.23427
196/196 - 41s - loss: 30.3961 - MinusLogProbMetric: 30.3961 - val_loss: 31.0971 - val_MinusLogProbMetric: 31.0971 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 129/1000
2023-10-25 18:41:40.202 
Epoch 129/1000 
	 loss: 30.3186, MinusLogProbMetric: 30.3186, val_loss: 30.5377, val_MinusLogProbMetric: 30.5377

Epoch 129: val_loss did not improve from 30.23427
196/196 - 42s - loss: 30.3186 - MinusLogProbMetric: 30.3186 - val_loss: 30.5377 - val_MinusLogProbMetric: 30.5377 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 130/1000
2023-10-25 18:42:18.617 
Epoch 130/1000 
	 loss: 30.4465, MinusLogProbMetric: 30.4465, val_loss: 31.4531, val_MinusLogProbMetric: 31.4531

Epoch 130: val_loss did not improve from 30.23427
196/196 - 38s - loss: 30.4465 - MinusLogProbMetric: 30.4465 - val_loss: 31.4531 - val_MinusLogProbMetric: 31.4531 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 131/1000
2023-10-25 18:42:57.267 
Epoch 131/1000 
	 loss: 30.3836, MinusLogProbMetric: 30.3836, val_loss: 30.7393, val_MinusLogProbMetric: 30.7393

Epoch 131: val_loss did not improve from 30.23427
196/196 - 39s - loss: 30.3836 - MinusLogProbMetric: 30.3836 - val_loss: 30.7393 - val_MinusLogProbMetric: 30.7393 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 132/1000
2023-10-25 18:43:36.884 
Epoch 132/1000 
	 loss: 30.3275, MinusLogProbMetric: 30.3275, val_loss: 30.5566, val_MinusLogProbMetric: 30.5566

Epoch 132: val_loss did not improve from 30.23427
196/196 - 40s - loss: 30.3275 - MinusLogProbMetric: 30.3275 - val_loss: 30.5566 - val_MinusLogProbMetric: 30.5566 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 133/1000
2023-10-25 18:44:14.571 
Epoch 133/1000 
	 loss: 30.2826, MinusLogProbMetric: 30.2826, val_loss: 31.9475, val_MinusLogProbMetric: 31.9475

Epoch 133: val_loss did not improve from 30.23427
196/196 - 38s - loss: 30.2826 - MinusLogProbMetric: 30.2826 - val_loss: 31.9475 - val_MinusLogProbMetric: 31.9475 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 134/1000
2023-10-25 18:44:57.503 
Epoch 134/1000 
	 loss: 30.3609, MinusLogProbMetric: 30.3609, val_loss: 30.6123, val_MinusLogProbMetric: 30.6123

Epoch 134: val_loss did not improve from 30.23427
196/196 - 43s - loss: 30.3609 - MinusLogProbMetric: 30.3609 - val_loss: 30.6123 - val_MinusLogProbMetric: 30.6123 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 135/1000
2023-10-25 18:45:40.208 
Epoch 135/1000 
	 loss: 30.2976, MinusLogProbMetric: 30.2976, val_loss: 32.0011, val_MinusLogProbMetric: 32.0011

Epoch 135: val_loss did not improve from 30.23427
196/196 - 43s - loss: 30.2976 - MinusLogProbMetric: 30.2976 - val_loss: 32.0011 - val_MinusLogProbMetric: 32.0011 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 136/1000
2023-10-25 18:46:20.780 
Epoch 136/1000 
	 loss: 30.5968, MinusLogProbMetric: 30.5968, val_loss: 30.5492, val_MinusLogProbMetric: 30.5492

Epoch 136: val_loss did not improve from 30.23427
196/196 - 41s - loss: 30.5968 - MinusLogProbMetric: 30.5968 - val_loss: 30.5492 - val_MinusLogProbMetric: 30.5492 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 137/1000
2023-10-25 18:46:56.404 
Epoch 137/1000 
	 loss: 30.2411, MinusLogProbMetric: 30.2411, val_loss: 30.3149, val_MinusLogProbMetric: 30.3149

Epoch 137: val_loss did not improve from 30.23427
196/196 - 36s - loss: 30.2411 - MinusLogProbMetric: 30.2411 - val_loss: 30.3149 - val_MinusLogProbMetric: 30.3149 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 138/1000
2023-10-25 18:47:30.985 
Epoch 138/1000 
	 loss: 30.1927, MinusLogProbMetric: 30.1927, val_loss: 30.1224, val_MinusLogProbMetric: 30.1224

Epoch 138: val_loss improved from 30.23427 to 30.12237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 35s - loss: 30.1927 - MinusLogProbMetric: 30.1927 - val_loss: 30.1224 - val_MinusLogProbMetric: 30.1224 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 139/1000
2023-10-25 18:48:08.143 
Epoch 139/1000 
	 loss: 30.0592, MinusLogProbMetric: 30.0592, val_loss: 31.0712, val_MinusLogProbMetric: 31.0712

Epoch 139: val_loss did not improve from 30.12237
196/196 - 37s - loss: 30.0592 - MinusLogProbMetric: 30.0592 - val_loss: 31.0712 - val_MinusLogProbMetric: 31.0712 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 140/1000
2023-10-25 18:48:48.199 
Epoch 140/1000 
	 loss: 30.3015, MinusLogProbMetric: 30.3015, val_loss: 30.8496, val_MinusLogProbMetric: 30.8496

Epoch 140: val_loss did not improve from 30.12237
196/196 - 40s - loss: 30.3015 - MinusLogProbMetric: 30.3015 - val_loss: 30.8496 - val_MinusLogProbMetric: 30.8496 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 141/1000
2023-10-25 18:49:29.448 
Epoch 141/1000 
	 loss: 30.1603, MinusLogProbMetric: 30.1603, val_loss: 30.5555, val_MinusLogProbMetric: 30.5555

Epoch 141: val_loss did not improve from 30.12237
196/196 - 41s - loss: 30.1603 - MinusLogProbMetric: 30.1603 - val_loss: 30.5555 - val_MinusLogProbMetric: 30.5555 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 142/1000
2023-10-25 18:50:07.453 
Epoch 142/1000 
	 loss: 30.1309, MinusLogProbMetric: 30.1309, val_loss: 30.6203, val_MinusLogProbMetric: 30.6203

Epoch 142: val_loss did not improve from 30.12237
196/196 - 38s - loss: 30.1309 - MinusLogProbMetric: 30.1309 - val_loss: 30.6203 - val_MinusLogProbMetric: 30.6203 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 143/1000
2023-10-25 18:50:47.026 
Epoch 143/1000 
	 loss: 30.2629, MinusLogProbMetric: 30.2629, val_loss: 31.3335, val_MinusLogProbMetric: 31.3335

Epoch 143: val_loss did not improve from 30.12237
196/196 - 40s - loss: 30.2629 - MinusLogProbMetric: 30.2629 - val_loss: 31.3335 - val_MinusLogProbMetric: 31.3335 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 144/1000
2023-10-25 18:51:26.115 
Epoch 144/1000 
	 loss: 30.0960, MinusLogProbMetric: 30.0960, val_loss: 32.1248, val_MinusLogProbMetric: 32.1248

Epoch 144: val_loss did not improve from 30.12237
196/196 - 39s - loss: 30.0960 - MinusLogProbMetric: 30.0960 - val_loss: 32.1248 - val_MinusLogProbMetric: 32.1248 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 145/1000
2023-10-25 18:52:05.803 
Epoch 145/1000 
	 loss: 30.2324, MinusLogProbMetric: 30.2324, val_loss: 30.2394, val_MinusLogProbMetric: 30.2394

Epoch 145: val_loss did not improve from 30.12237
196/196 - 40s - loss: 30.2324 - MinusLogProbMetric: 30.2324 - val_loss: 30.2394 - val_MinusLogProbMetric: 30.2394 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 146/1000
2023-10-25 18:52:46.556 
Epoch 146/1000 
	 loss: 30.1280, MinusLogProbMetric: 30.1280, val_loss: 32.8090, val_MinusLogProbMetric: 32.8090

Epoch 146: val_loss did not improve from 30.12237
196/196 - 41s - loss: 30.1280 - MinusLogProbMetric: 30.1280 - val_loss: 32.8090 - val_MinusLogProbMetric: 32.8090 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 147/1000
2023-10-25 18:53:27.789 
Epoch 147/1000 
	 loss: 30.2686, MinusLogProbMetric: 30.2686, val_loss: 31.9995, val_MinusLogProbMetric: 31.9995

Epoch 147: val_loss did not improve from 30.12237
196/196 - 41s - loss: 30.2686 - MinusLogProbMetric: 30.2686 - val_loss: 31.9995 - val_MinusLogProbMetric: 31.9995 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 148/1000
2023-10-25 18:54:04.674 
Epoch 148/1000 
	 loss: 30.1801, MinusLogProbMetric: 30.1801, val_loss: 30.0719, val_MinusLogProbMetric: 30.0719

Epoch 148: val_loss improved from 30.12237 to 30.07194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 38s - loss: 30.1801 - MinusLogProbMetric: 30.1801 - val_loss: 30.0719 - val_MinusLogProbMetric: 30.0719 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 149/1000
2023-10-25 18:54:42.421 
Epoch 149/1000 
	 loss: 30.1493, MinusLogProbMetric: 30.1493, val_loss: 32.0442, val_MinusLogProbMetric: 32.0442

Epoch 149: val_loss did not improve from 30.07194
196/196 - 37s - loss: 30.1493 - MinusLogProbMetric: 30.1493 - val_loss: 32.0442 - val_MinusLogProbMetric: 32.0442 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 150/1000
2023-10-25 18:55:20.413 
Epoch 150/1000 
	 loss: 30.0216, MinusLogProbMetric: 30.0216, val_loss: 29.9376, val_MinusLogProbMetric: 29.9376

Epoch 150: val_loss improved from 30.07194 to 29.93761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 39s - loss: 30.0216 - MinusLogProbMetric: 30.0216 - val_loss: 29.9376 - val_MinusLogProbMetric: 29.9376 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 151/1000
2023-10-25 18:55:58.523 
Epoch 151/1000 
	 loss: 30.0819, MinusLogProbMetric: 30.0819, val_loss: 31.7608, val_MinusLogProbMetric: 31.7608

Epoch 151: val_loss did not improve from 29.93761
196/196 - 37s - loss: 30.0819 - MinusLogProbMetric: 30.0819 - val_loss: 31.7608 - val_MinusLogProbMetric: 31.7608 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 152/1000
2023-10-25 18:56:39.322 
Epoch 152/1000 
	 loss: 30.1360, MinusLogProbMetric: 30.1360, val_loss: 30.6832, val_MinusLogProbMetric: 30.6832

Epoch 152: val_loss did not improve from 29.93761
196/196 - 41s - loss: 30.1360 - MinusLogProbMetric: 30.1360 - val_loss: 30.6832 - val_MinusLogProbMetric: 30.6832 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 153/1000
2023-10-25 18:57:20.381 
Epoch 153/1000 
	 loss: 30.0436, MinusLogProbMetric: 30.0436, val_loss: 30.7718, val_MinusLogProbMetric: 30.7718

Epoch 153: val_loss did not improve from 29.93761
196/196 - 41s - loss: 30.0436 - MinusLogProbMetric: 30.0436 - val_loss: 30.7718 - val_MinusLogProbMetric: 30.7718 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 154/1000
2023-10-25 18:57:58.931 
Epoch 154/1000 
	 loss: 30.0427, MinusLogProbMetric: 30.0427, val_loss: 30.4998, val_MinusLogProbMetric: 30.4998

Epoch 154: val_loss did not improve from 29.93761
196/196 - 39s - loss: 30.0427 - MinusLogProbMetric: 30.0427 - val_loss: 30.4998 - val_MinusLogProbMetric: 30.4998 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 155/1000
2023-10-25 18:58:33.935 
Epoch 155/1000 
	 loss: 30.3016, MinusLogProbMetric: 30.3016, val_loss: 30.3291, val_MinusLogProbMetric: 30.3291

Epoch 155: val_loss did not improve from 29.93761
196/196 - 35s - loss: 30.3016 - MinusLogProbMetric: 30.3016 - val_loss: 30.3291 - val_MinusLogProbMetric: 30.3291 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 156/1000
2023-10-25 18:59:08.772 
Epoch 156/1000 
	 loss: 29.8843, MinusLogProbMetric: 29.8843, val_loss: 30.1343, val_MinusLogProbMetric: 30.1343

Epoch 156: val_loss did not improve from 29.93761
196/196 - 35s - loss: 29.8843 - MinusLogProbMetric: 29.8843 - val_loss: 30.1343 - val_MinusLogProbMetric: 30.1343 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 157/1000
2023-10-25 18:59:49.008 
Epoch 157/1000 
	 loss: 30.0281, MinusLogProbMetric: 30.0281, val_loss: 29.8480, val_MinusLogProbMetric: 29.8480

Epoch 157: val_loss improved from 29.93761 to 29.84797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 41s - loss: 30.0281 - MinusLogProbMetric: 30.0281 - val_loss: 29.8480 - val_MinusLogProbMetric: 29.8480 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 158/1000
2023-10-25 19:00:31.270 
Epoch 158/1000 
	 loss: 29.9014, MinusLogProbMetric: 29.9014, val_loss: 30.1873, val_MinusLogProbMetric: 30.1873

Epoch 158: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.9014 - MinusLogProbMetric: 29.9014 - val_loss: 30.1873 - val_MinusLogProbMetric: 30.1873 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 159/1000
2023-10-25 19:01:09.866 
Epoch 159/1000 
	 loss: 30.1092, MinusLogProbMetric: 30.1092, val_loss: 30.3776, val_MinusLogProbMetric: 30.3776

Epoch 159: val_loss did not improve from 29.84797
196/196 - 39s - loss: 30.1092 - MinusLogProbMetric: 30.1092 - val_loss: 30.3776 - val_MinusLogProbMetric: 30.3776 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 160/1000
2023-10-25 19:01:46.353 
Epoch 160/1000 
	 loss: 29.9361, MinusLogProbMetric: 29.9361, val_loss: 30.5909, val_MinusLogProbMetric: 30.5909

Epoch 160: val_loss did not improve from 29.84797
196/196 - 36s - loss: 29.9361 - MinusLogProbMetric: 29.9361 - val_loss: 30.5909 - val_MinusLogProbMetric: 30.5909 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 161/1000
2023-10-25 19:02:22.366 
Epoch 161/1000 
	 loss: 29.8722, MinusLogProbMetric: 29.8722, val_loss: 30.2844, val_MinusLogProbMetric: 30.2844

Epoch 161: val_loss did not improve from 29.84797
196/196 - 36s - loss: 29.8722 - MinusLogProbMetric: 29.8722 - val_loss: 30.2844 - val_MinusLogProbMetric: 30.2844 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 162/1000
2023-10-25 19:03:04.101 
Epoch 162/1000 
	 loss: 30.1892, MinusLogProbMetric: 30.1892, val_loss: 29.9981, val_MinusLogProbMetric: 29.9981

Epoch 162: val_loss did not improve from 29.84797
196/196 - 42s - loss: 30.1892 - MinusLogProbMetric: 30.1892 - val_loss: 29.9981 - val_MinusLogProbMetric: 29.9981 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 163/1000
2023-10-25 19:03:45.759 
Epoch 163/1000 
	 loss: 29.7968, MinusLogProbMetric: 29.7968, val_loss: 29.9998, val_MinusLogProbMetric: 29.9998

Epoch 163: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.7968 - MinusLogProbMetric: 29.7968 - val_loss: 29.9998 - val_MinusLogProbMetric: 29.9998 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 164/1000
2023-10-25 19:04:24.471 
Epoch 164/1000 
	 loss: 29.8465, MinusLogProbMetric: 29.8465, val_loss: 30.2281, val_MinusLogProbMetric: 30.2281

Epoch 164: val_loss did not improve from 29.84797
196/196 - 39s - loss: 29.8465 - MinusLogProbMetric: 29.8465 - val_loss: 30.2281 - val_MinusLogProbMetric: 30.2281 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 165/1000
2023-10-25 19:05:01.793 
Epoch 165/1000 
	 loss: 30.0806, MinusLogProbMetric: 30.0806, val_loss: 30.2214, val_MinusLogProbMetric: 30.2214

Epoch 165: val_loss did not improve from 29.84797
196/196 - 37s - loss: 30.0806 - MinusLogProbMetric: 30.0806 - val_loss: 30.2214 - val_MinusLogProbMetric: 30.2214 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 166/1000
2023-10-25 19:05:39.746 
Epoch 166/1000 
	 loss: 29.8860, MinusLogProbMetric: 29.8860, val_loss: 31.4339, val_MinusLogProbMetric: 31.4339

Epoch 166: val_loss did not improve from 29.84797
196/196 - 38s - loss: 29.8860 - MinusLogProbMetric: 29.8860 - val_loss: 31.4339 - val_MinusLogProbMetric: 31.4339 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 167/1000
2023-10-25 19:06:22.130 
Epoch 167/1000 
	 loss: 29.8922, MinusLogProbMetric: 29.8922, val_loss: 30.1887, val_MinusLogProbMetric: 30.1887

Epoch 167: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.8922 - MinusLogProbMetric: 29.8922 - val_loss: 30.1887 - val_MinusLogProbMetric: 30.1887 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 168/1000
2023-10-25 19:07:01.341 
Epoch 168/1000 
	 loss: 29.8622, MinusLogProbMetric: 29.8622, val_loss: 30.3770, val_MinusLogProbMetric: 30.3770

Epoch 168: val_loss did not improve from 29.84797
196/196 - 39s - loss: 29.8622 - MinusLogProbMetric: 29.8622 - val_loss: 30.3770 - val_MinusLogProbMetric: 30.3770 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 169/1000
2023-10-25 19:07:36.809 
Epoch 169/1000 
	 loss: 30.0017, MinusLogProbMetric: 30.0017, val_loss: 31.2222, val_MinusLogProbMetric: 31.2222

Epoch 169: val_loss did not improve from 29.84797
196/196 - 35s - loss: 30.0017 - MinusLogProbMetric: 30.0017 - val_loss: 31.2222 - val_MinusLogProbMetric: 31.2222 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 170/1000
2023-10-25 19:08:18.366 
Epoch 170/1000 
	 loss: 29.8235, MinusLogProbMetric: 29.8235, val_loss: 29.9115, val_MinusLogProbMetric: 29.9115

Epoch 170: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.8235 - MinusLogProbMetric: 29.8235 - val_loss: 29.9115 - val_MinusLogProbMetric: 29.9115 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 171/1000
2023-10-25 19:08:57.190 
Epoch 171/1000 
	 loss: 29.8743, MinusLogProbMetric: 29.8743, val_loss: 30.0893, val_MinusLogProbMetric: 30.0893

Epoch 171: val_loss did not improve from 29.84797
196/196 - 39s - loss: 29.8743 - MinusLogProbMetric: 29.8743 - val_loss: 30.0893 - val_MinusLogProbMetric: 30.0893 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 172/1000
2023-10-25 19:09:35.305 
Epoch 172/1000 
	 loss: 29.8481, MinusLogProbMetric: 29.8481, val_loss: 33.0731, val_MinusLogProbMetric: 33.0731

Epoch 172: val_loss did not improve from 29.84797
196/196 - 38s - loss: 29.8481 - MinusLogProbMetric: 29.8481 - val_loss: 33.0731 - val_MinusLogProbMetric: 33.0731 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 173/1000
2023-10-25 19:10:15.247 
Epoch 173/1000 
	 loss: 30.0500, MinusLogProbMetric: 30.0500, val_loss: 30.1274, val_MinusLogProbMetric: 30.1274

Epoch 173: val_loss did not improve from 29.84797
196/196 - 40s - loss: 30.0500 - MinusLogProbMetric: 30.0500 - val_loss: 30.1274 - val_MinusLogProbMetric: 30.1274 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 174/1000
2023-10-25 19:10:53.843 
Epoch 174/1000 
	 loss: 29.8470, MinusLogProbMetric: 29.8470, val_loss: 29.8816, val_MinusLogProbMetric: 29.8816

Epoch 174: val_loss did not improve from 29.84797
196/196 - 39s - loss: 29.8470 - MinusLogProbMetric: 29.8470 - val_loss: 29.8816 - val_MinusLogProbMetric: 29.8816 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 175/1000
2023-10-25 19:11:32.749 
Epoch 175/1000 
	 loss: 29.8039, MinusLogProbMetric: 29.8039, val_loss: 30.4984, val_MinusLogProbMetric: 30.4984

Epoch 175: val_loss did not improve from 29.84797
196/196 - 39s - loss: 29.8039 - MinusLogProbMetric: 29.8039 - val_loss: 30.4984 - val_MinusLogProbMetric: 30.4984 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 176/1000
2023-10-25 19:12:11.769 
Epoch 176/1000 
	 loss: 29.6764, MinusLogProbMetric: 29.6764, val_loss: 30.2993, val_MinusLogProbMetric: 30.2993

Epoch 176: val_loss did not improve from 29.84797
196/196 - 39s - loss: 29.6764 - MinusLogProbMetric: 29.6764 - val_loss: 30.2993 - val_MinusLogProbMetric: 30.2993 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 177/1000
2023-10-25 19:12:49.001 
Epoch 177/1000 
	 loss: 29.7596, MinusLogProbMetric: 29.7596, val_loss: 30.2514, val_MinusLogProbMetric: 30.2514

Epoch 177: val_loss did not improve from 29.84797
196/196 - 37s - loss: 29.7596 - MinusLogProbMetric: 29.7596 - val_loss: 30.2514 - val_MinusLogProbMetric: 30.2514 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 178/1000
2023-10-25 19:13:30.792 
Epoch 178/1000 
	 loss: 30.0543, MinusLogProbMetric: 30.0543, val_loss: 30.1366, val_MinusLogProbMetric: 30.1366

Epoch 178: val_loss did not improve from 29.84797
196/196 - 42s - loss: 30.0543 - MinusLogProbMetric: 30.0543 - val_loss: 30.1366 - val_MinusLogProbMetric: 30.1366 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 179/1000
2023-10-25 19:14:11.782 
Epoch 179/1000 
	 loss: 29.6595, MinusLogProbMetric: 29.6595, val_loss: 30.7241, val_MinusLogProbMetric: 30.7241

Epoch 179: val_loss did not improve from 29.84797
196/196 - 41s - loss: 29.6595 - MinusLogProbMetric: 29.6595 - val_loss: 30.7241 - val_MinusLogProbMetric: 30.7241 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 180/1000
2023-10-25 19:14:53.643 
Epoch 180/1000 
	 loss: 29.8291, MinusLogProbMetric: 29.8291, val_loss: 30.8188, val_MinusLogProbMetric: 30.8188

Epoch 180: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.8291 - MinusLogProbMetric: 29.8291 - val_loss: 30.8188 - val_MinusLogProbMetric: 30.8188 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 181/1000
2023-10-25 19:15:35.571 
Epoch 181/1000 
	 loss: 29.9188, MinusLogProbMetric: 29.9188, val_loss: 30.3437, val_MinusLogProbMetric: 30.3437

Epoch 181: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.9188 - MinusLogProbMetric: 29.9188 - val_loss: 30.3437 - val_MinusLogProbMetric: 30.3437 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 182/1000
2023-10-25 19:16:14.802 
Epoch 182/1000 
	 loss: 29.5617, MinusLogProbMetric: 29.5617, val_loss: 30.6805, val_MinusLogProbMetric: 30.6805

Epoch 182: val_loss did not improve from 29.84797
196/196 - 39s - loss: 29.5617 - MinusLogProbMetric: 29.5617 - val_loss: 30.6805 - val_MinusLogProbMetric: 30.6805 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 183/1000
2023-10-25 19:16:56.938 
Epoch 183/1000 
	 loss: 29.9632, MinusLogProbMetric: 29.9632, val_loss: 30.2326, val_MinusLogProbMetric: 30.2326

Epoch 183: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.9632 - MinusLogProbMetric: 29.9632 - val_loss: 30.2326 - val_MinusLogProbMetric: 30.2326 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 184/1000
2023-10-25 19:17:38.797 
Epoch 184/1000 
	 loss: 29.6608, MinusLogProbMetric: 29.6608, val_loss: 31.0161, val_MinusLogProbMetric: 31.0161

Epoch 184: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.6608 - MinusLogProbMetric: 29.6608 - val_loss: 31.0161 - val_MinusLogProbMetric: 31.0161 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 185/1000
2023-10-25 19:18:21.677 
Epoch 185/1000 
	 loss: 29.6694, MinusLogProbMetric: 29.6694, val_loss: 30.2577, val_MinusLogProbMetric: 30.2577

Epoch 185: val_loss did not improve from 29.84797
196/196 - 43s - loss: 29.6694 - MinusLogProbMetric: 29.6694 - val_loss: 30.2577 - val_MinusLogProbMetric: 30.2577 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 186/1000
2023-10-25 19:19:03.731 
Epoch 186/1000 
	 loss: 29.8414, MinusLogProbMetric: 29.8414, val_loss: 29.8631, val_MinusLogProbMetric: 29.8631

Epoch 186: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.8414 - MinusLogProbMetric: 29.8414 - val_loss: 29.8631 - val_MinusLogProbMetric: 29.8631 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 187/1000
2023-10-25 19:19:46.453 
Epoch 187/1000 
	 loss: 29.5084, MinusLogProbMetric: 29.5084, val_loss: 30.0570, val_MinusLogProbMetric: 30.0570

Epoch 187: val_loss did not improve from 29.84797
196/196 - 43s - loss: 29.5084 - MinusLogProbMetric: 29.5084 - val_loss: 30.0570 - val_MinusLogProbMetric: 30.0570 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 188/1000
2023-10-25 19:20:28.830 
Epoch 188/1000 
	 loss: 29.6234, MinusLogProbMetric: 29.6234, val_loss: 30.1872, val_MinusLogProbMetric: 30.1872

Epoch 188: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.6234 - MinusLogProbMetric: 29.6234 - val_loss: 30.1872 - val_MinusLogProbMetric: 30.1872 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 189/1000
2023-10-25 19:21:11.540 
Epoch 189/1000 
	 loss: 29.8375, MinusLogProbMetric: 29.8375, val_loss: 30.0376, val_MinusLogProbMetric: 30.0376

Epoch 189: val_loss did not improve from 29.84797
196/196 - 43s - loss: 29.8375 - MinusLogProbMetric: 29.8375 - val_loss: 30.0376 - val_MinusLogProbMetric: 30.0376 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 190/1000
2023-10-25 19:21:54.028 
Epoch 190/1000 
	 loss: 29.6819, MinusLogProbMetric: 29.6819, val_loss: 29.8792, val_MinusLogProbMetric: 29.8792

Epoch 190: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.6819 - MinusLogProbMetric: 29.6819 - val_loss: 29.8792 - val_MinusLogProbMetric: 29.8792 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 191/1000
2023-10-25 19:22:36.416 
Epoch 191/1000 
	 loss: 29.7429, MinusLogProbMetric: 29.7429, val_loss: 30.1128, val_MinusLogProbMetric: 30.1128

Epoch 191: val_loss did not improve from 29.84797
196/196 - 42s - loss: 29.7429 - MinusLogProbMetric: 29.7429 - val_loss: 30.1128 - val_MinusLogProbMetric: 30.1128 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 192/1000
2023-10-25 19:23:19.083 
Epoch 192/1000 
	 loss: 29.5130, MinusLogProbMetric: 29.5130, val_loss: 30.7619, val_MinusLogProbMetric: 30.7619

Epoch 192: val_loss did not improve from 29.84797
196/196 - 43s - loss: 29.5130 - MinusLogProbMetric: 29.5130 - val_loss: 30.7619 - val_MinusLogProbMetric: 30.7619 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 193/1000
2023-10-25 19:24:01.620 
Epoch 193/1000 
	 loss: 29.6564, MinusLogProbMetric: 29.6564, val_loss: 29.9182, val_MinusLogProbMetric: 29.9182

Epoch 193: val_loss did not improve from 29.84797
196/196 - 43s - loss: 29.6564 - MinusLogProbMetric: 29.6564 - val_loss: 29.9182 - val_MinusLogProbMetric: 29.9182 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 194/1000
2023-10-25 19:24:44.373 
Epoch 194/1000 
	 loss: 29.8085, MinusLogProbMetric: 29.8085, val_loss: 30.1361, val_MinusLogProbMetric: 30.1361

Epoch 194: val_loss did not improve from 29.84797
196/196 - 43s - loss: 29.8085 - MinusLogProbMetric: 29.8085 - val_loss: 30.1361 - val_MinusLogProbMetric: 30.1361 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 195/1000
2023-10-25 19:25:26.899 
Epoch 195/1000 
	 loss: 29.5576, MinusLogProbMetric: 29.5576, val_loss: 29.4482, val_MinusLogProbMetric: 29.4482

Epoch 195: val_loss improved from 29.84797 to 29.44820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 29.5576 - MinusLogProbMetric: 29.5576 - val_loss: 29.4482 - val_MinusLogProbMetric: 29.4482 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 196/1000
2023-10-25 19:26:10.482 
Epoch 196/1000 
	 loss: 29.5186, MinusLogProbMetric: 29.5186, val_loss: 30.7399, val_MinusLogProbMetric: 30.7399

Epoch 196: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5186 - MinusLogProbMetric: 29.5186 - val_loss: 30.7399 - val_MinusLogProbMetric: 30.7399 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 197/1000
2023-10-25 19:26:53.183 
Epoch 197/1000 
	 loss: 29.7873, MinusLogProbMetric: 29.7873, val_loss: 29.6145, val_MinusLogProbMetric: 29.6145

Epoch 197: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.7873 - MinusLogProbMetric: 29.7873 - val_loss: 29.6145 - val_MinusLogProbMetric: 29.6145 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 198/1000
2023-10-25 19:27:35.459 
Epoch 198/1000 
	 loss: 29.5322, MinusLogProbMetric: 29.5322, val_loss: 30.0271, val_MinusLogProbMetric: 30.0271

Epoch 198: val_loss did not improve from 29.44820
196/196 - 42s - loss: 29.5322 - MinusLogProbMetric: 29.5322 - val_loss: 30.0271 - val_MinusLogProbMetric: 30.0271 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 199/1000
2023-10-25 19:28:18.088 
Epoch 199/1000 
	 loss: 29.9234, MinusLogProbMetric: 29.9234, val_loss: 30.2826, val_MinusLogProbMetric: 30.2826

Epoch 199: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.9234 - MinusLogProbMetric: 29.9234 - val_loss: 30.2826 - val_MinusLogProbMetric: 30.2826 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 200/1000
2023-10-25 19:29:00.672 
Epoch 200/1000 
	 loss: 29.5944, MinusLogProbMetric: 29.5944, val_loss: 29.8227, val_MinusLogProbMetric: 29.8227

Epoch 200: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5944 - MinusLogProbMetric: 29.5944 - val_loss: 29.8227 - val_MinusLogProbMetric: 29.8227 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 201/1000
2023-10-25 19:29:43.279 
Epoch 201/1000 
	 loss: 29.5277, MinusLogProbMetric: 29.5277, val_loss: 29.9797, val_MinusLogProbMetric: 29.9797

Epoch 201: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5277 - MinusLogProbMetric: 29.5277 - val_loss: 29.9797 - val_MinusLogProbMetric: 29.9797 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 202/1000
2023-10-25 19:30:25.935 
Epoch 202/1000 
	 loss: 29.4811, MinusLogProbMetric: 29.4811, val_loss: 29.8792, val_MinusLogProbMetric: 29.8792

Epoch 202: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.4811 - MinusLogProbMetric: 29.4811 - val_loss: 29.8792 - val_MinusLogProbMetric: 29.8792 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 203/1000
2023-10-25 19:31:08.292 
Epoch 203/1000 
	 loss: 29.4579, MinusLogProbMetric: 29.4579, val_loss: 30.5922, val_MinusLogProbMetric: 30.5922

Epoch 203: val_loss did not improve from 29.44820
196/196 - 42s - loss: 29.4579 - MinusLogProbMetric: 29.4579 - val_loss: 30.5922 - val_MinusLogProbMetric: 30.5922 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 204/1000
2023-10-25 19:31:50.674 
Epoch 204/1000 
	 loss: 29.6374, MinusLogProbMetric: 29.6374, val_loss: 29.8418, val_MinusLogProbMetric: 29.8418

Epoch 204: val_loss did not improve from 29.44820
196/196 - 42s - loss: 29.6374 - MinusLogProbMetric: 29.6374 - val_loss: 29.8418 - val_MinusLogProbMetric: 29.8418 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 205/1000
2023-10-25 19:32:33.254 
Epoch 205/1000 
	 loss: 29.5887, MinusLogProbMetric: 29.5887, val_loss: 30.7547, val_MinusLogProbMetric: 30.7547

Epoch 205: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5887 - MinusLogProbMetric: 29.5887 - val_loss: 30.7547 - val_MinusLogProbMetric: 30.7547 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 206/1000
2023-10-25 19:33:16.373 
Epoch 206/1000 
	 loss: 29.4682, MinusLogProbMetric: 29.4682, val_loss: 30.5907, val_MinusLogProbMetric: 30.5907

Epoch 206: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.4682 - MinusLogProbMetric: 29.4682 - val_loss: 30.5907 - val_MinusLogProbMetric: 30.5907 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 207/1000
2023-10-25 19:33:58.737 
Epoch 207/1000 
	 loss: 29.5682, MinusLogProbMetric: 29.5682, val_loss: 30.1389, val_MinusLogProbMetric: 30.1389

Epoch 207: val_loss did not improve from 29.44820
196/196 - 42s - loss: 29.5682 - MinusLogProbMetric: 29.5682 - val_loss: 30.1389 - val_MinusLogProbMetric: 30.1389 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 208/1000
2023-10-25 19:34:41.589 
Epoch 208/1000 
	 loss: 29.4422, MinusLogProbMetric: 29.4422, val_loss: 30.5506, val_MinusLogProbMetric: 30.5506

Epoch 208: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.4422 - MinusLogProbMetric: 29.4422 - val_loss: 30.5506 - val_MinusLogProbMetric: 30.5506 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 209/1000
2023-10-25 19:35:24.149 
Epoch 209/1000 
	 loss: 29.8235, MinusLogProbMetric: 29.8235, val_loss: 30.3191, val_MinusLogProbMetric: 30.3191

Epoch 209: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.8235 - MinusLogProbMetric: 29.8235 - val_loss: 30.3191 - val_MinusLogProbMetric: 30.3191 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 210/1000
2023-10-25 19:36:07.171 
Epoch 210/1000 
	 loss: 29.5025, MinusLogProbMetric: 29.5025, val_loss: 29.9530, val_MinusLogProbMetric: 29.9530

Epoch 210: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5025 - MinusLogProbMetric: 29.5025 - val_loss: 29.9530 - val_MinusLogProbMetric: 29.9530 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 211/1000
2023-10-25 19:36:49.826 
Epoch 211/1000 
	 loss: 29.5445, MinusLogProbMetric: 29.5445, val_loss: 29.7051, val_MinusLogProbMetric: 29.7051

Epoch 211: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5445 - MinusLogProbMetric: 29.5445 - val_loss: 29.7051 - val_MinusLogProbMetric: 29.7051 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 212/1000
2023-10-25 19:37:32.337 
Epoch 212/1000 
	 loss: 29.4225, MinusLogProbMetric: 29.4225, val_loss: 29.9465, val_MinusLogProbMetric: 29.9465

Epoch 212: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.4225 - MinusLogProbMetric: 29.4225 - val_loss: 29.9465 - val_MinusLogProbMetric: 29.9465 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 213/1000
2023-10-25 19:38:14.869 
Epoch 213/1000 
	 loss: 29.5678, MinusLogProbMetric: 29.5678, val_loss: 30.8220, val_MinusLogProbMetric: 30.8220

Epoch 213: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5678 - MinusLogProbMetric: 29.5678 - val_loss: 30.8220 - val_MinusLogProbMetric: 30.8220 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 214/1000
2023-10-25 19:38:57.373 
Epoch 214/1000 
	 loss: 29.5241, MinusLogProbMetric: 29.5241, val_loss: 30.4982, val_MinusLogProbMetric: 30.4982

Epoch 214: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5241 - MinusLogProbMetric: 29.5241 - val_loss: 30.4982 - val_MinusLogProbMetric: 30.4982 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 215/1000
2023-10-25 19:39:40.023 
Epoch 215/1000 
	 loss: 29.5036, MinusLogProbMetric: 29.5036, val_loss: 30.5007, val_MinusLogProbMetric: 30.5007

Epoch 215: val_loss did not improve from 29.44820
196/196 - 43s - loss: 29.5036 - MinusLogProbMetric: 29.5036 - val_loss: 30.5007 - val_MinusLogProbMetric: 30.5007 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 216/1000
2023-10-25 19:40:22.486 
Epoch 216/1000 
	 loss: 29.5315, MinusLogProbMetric: 29.5315, val_loss: 29.8935, val_MinusLogProbMetric: 29.8935

Epoch 216: val_loss did not improve from 29.44820
196/196 - 42s - loss: 29.5315 - MinusLogProbMetric: 29.5315 - val_loss: 29.8935 - val_MinusLogProbMetric: 29.8935 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 217/1000
2023-10-25 19:41:01.574 
Epoch 217/1000 
	 loss: 29.3958, MinusLogProbMetric: 29.3958, val_loss: 30.6048, val_MinusLogProbMetric: 30.6048

Epoch 217: val_loss did not improve from 29.44820
196/196 - 39s - loss: 29.3958 - MinusLogProbMetric: 29.3958 - val_loss: 30.6048 - val_MinusLogProbMetric: 30.6048 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 218/1000
2023-10-25 19:41:43.874 
Epoch 218/1000 
	 loss: 29.3955, MinusLogProbMetric: 29.3955, val_loss: 29.9000, val_MinusLogProbMetric: 29.9000

Epoch 218: val_loss did not improve from 29.44820
196/196 - 42s - loss: 29.3955 - MinusLogProbMetric: 29.3955 - val_loss: 29.9000 - val_MinusLogProbMetric: 29.9000 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 219/1000
2023-10-25 19:42:25.279 
Epoch 219/1000 
	 loss: 29.4499, MinusLogProbMetric: 29.4499, val_loss: 29.9055, val_MinusLogProbMetric: 29.9055

Epoch 219: val_loss did not improve from 29.44820
196/196 - 41s - loss: 29.4499 - MinusLogProbMetric: 29.4499 - val_loss: 29.9055 - val_MinusLogProbMetric: 29.9055 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 220/1000
2023-10-25 19:43:07.009 
Epoch 220/1000 
	 loss: 29.4554, MinusLogProbMetric: 29.4554, val_loss: 30.4866, val_MinusLogProbMetric: 30.4866

Epoch 220: val_loss did not improve from 29.44820
196/196 - 42s - loss: 29.4554 - MinusLogProbMetric: 29.4554 - val_loss: 30.4866 - val_MinusLogProbMetric: 30.4866 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 221/1000
2023-10-25 19:43:49.053 
Epoch 221/1000 
	 loss: 29.4999, MinusLogProbMetric: 29.4999, val_loss: 29.3882, val_MinusLogProbMetric: 29.3882

Epoch 221: val_loss improved from 29.44820 to 29.38819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 29.4999 - MinusLogProbMetric: 29.4999 - val_loss: 29.3882 - val_MinusLogProbMetric: 29.3882 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 222/1000
2023-10-25 19:44:32.489 
Epoch 222/1000 
	 loss: 29.3403, MinusLogProbMetric: 29.3403, val_loss: 29.8093, val_MinusLogProbMetric: 29.8093

Epoch 222: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.3403 - MinusLogProbMetric: 29.3403 - val_loss: 29.8093 - val_MinusLogProbMetric: 29.8093 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 223/1000
2023-10-25 19:45:14.066 
Epoch 223/1000 
	 loss: 29.3927, MinusLogProbMetric: 29.3927, val_loss: 29.6002, val_MinusLogProbMetric: 29.6002

Epoch 223: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.3927 - MinusLogProbMetric: 29.3927 - val_loss: 29.6002 - val_MinusLogProbMetric: 29.6002 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 224/1000
2023-10-25 19:45:56.638 
Epoch 224/1000 
	 loss: 29.5085, MinusLogProbMetric: 29.5085, val_loss: 29.6756, val_MinusLogProbMetric: 29.6756

Epoch 224: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.5085 - MinusLogProbMetric: 29.5085 - val_loss: 29.6756 - val_MinusLogProbMetric: 29.6756 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 225/1000
2023-10-25 19:46:39.188 
Epoch 225/1000 
	 loss: 29.3592, MinusLogProbMetric: 29.3592, val_loss: 30.2398, val_MinusLogProbMetric: 30.2398

Epoch 225: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.3592 - MinusLogProbMetric: 29.3592 - val_loss: 30.2398 - val_MinusLogProbMetric: 30.2398 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 226/1000
2023-10-25 19:47:21.642 
Epoch 226/1000 
	 loss: 29.4865, MinusLogProbMetric: 29.4865, val_loss: 29.7678, val_MinusLogProbMetric: 29.7678

Epoch 226: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.4865 - MinusLogProbMetric: 29.4865 - val_loss: 29.7678 - val_MinusLogProbMetric: 29.7678 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 227/1000
2023-10-25 19:48:04.310 
Epoch 227/1000 
	 loss: 29.2460, MinusLogProbMetric: 29.2460, val_loss: 29.9160, val_MinusLogProbMetric: 29.9160

Epoch 227: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.2460 - MinusLogProbMetric: 29.2460 - val_loss: 29.9160 - val_MinusLogProbMetric: 29.9160 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 228/1000
2023-10-25 19:48:46.952 
Epoch 228/1000 
	 loss: 29.2603, MinusLogProbMetric: 29.2603, val_loss: 29.4685, val_MinusLogProbMetric: 29.4685

Epoch 228: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.2603 - MinusLogProbMetric: 29.2603 - val_loss: 29.4685 - val_MinusLogProbMetric: 29.4685 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 229/1000
2023-10-25 19:49:29.337 
Epoch 229/1000 
	 loss: 29.5647, MinusLogProbMetric: 29.5647, val_loss: 29.9699, val_MinusLogProbMetric: 29.9699

Epoch 229: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.5647 - MinusLogProbMetric: 29.5647 - val_loss: 29.9699 - val_MinusLogProbMetric: 29.9699 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 230/1000
2023-10-25 19:50:11.871 
Epoch 230/1000 
	 loss: 29.8119, MinusLogProbMetric: 29.8119, val_loss: 29.6021, val_MinusLogProbMetric: 29.6021

Epoch 230: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.8119 - MinusLogProbMetric: 29.8119 - val_loss: 29.6021 - val_MinusLogProbMetric: 29.6021 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 231/1000
2023-10-25 19:50:54.572 
Epoch 231/1000 
	 loss: 29.2924, MinusLogProbMetric: 29.2924, val_loss: 30.1225, val_MinusLogProbMetric: 30.1225

Epoch 231: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.2924 - MinusLogProbMetric: 29.2924 - val_loss: 30.1225 - val_MinusLogProbMetric: 30.1225 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 232/1000
2023-10-25 19:51:36.869 
Epoch 232/1000 
	 loss: 29.4120, MinusLogProbMetric: 29.4120, val_loss: 29.6733, val_MinusLogProbMetric: 29.6733

Epoch 232: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.4120 - MinusLogProbMetric: 29.4120 - val_loss: 29.6733 - val_MinusLogProbMetric: 29.6733 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 233/1000
2023-10-25 19:52:19.790 
Epoch 233/1000 
	 loss: 29.4048, MinusLogProbMetric: 29.4048, val_loss: 30.0579, val_MinusLogProbMetric: 30.0579

Epoch 233: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.4048 - MinusLogProbMetric: 29.4048 - val_loss: 30.0579 - val_MinusLogProbMetric: 30.0579 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 234/1000
2023-10-25 19:53:02.720 
Epoch 234/1000 
	 loss: 29.2876, MinusLogProbMetric: 29.2876, val_loss: 32.3275, val_MinusLogProbMetric: 32.3275

Epoch 234: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.2876 - MinusLogProbMetric: 29.2876 - val_loss: 32.3275 - val_MinusLogProbMetric: 32.3275 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 235/1000
2023-10-25 19:53:45.349 
Epoch 235/1000 
	 loss: 29.4365, MinusLogProbMetric: 29.4365, val_loss: 30.3888, val_MinusLogProbMetric: 30.3888

Epoch 235: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.4365 - MinusLogProbMetric: 29.4365 - val_loss: 30.3888 - val_MinusLogProbMetric: 30.3888 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 236/1000
2023-10-25 19:54:27.974 
Epoch 236/1000 
	 loss: 29.3604, MinusLogProbMetric: 29.3604, val_loss: 29.7304, val_MinusLogProbMetric: 29.7304

Epoch 236: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.3604 - MinusLogProbMetric: 29.3604 - val_loss: 29.7304 - val_MinusLogProbMetric: 29.7304 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 237/1000
2023-10-25 19:55:10.528 
Epoch 237/1000 
	 loss: 29.5726, MinusLogProbMetric: 29.5726, val_loss: 30.2307, val_MinusLogProbMetric: 30.2307

Epoch 237: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.5726 - MinusLogProbMetric: 29.5726 - val_loss: 30.2307 - val_MinusLogProbMetric: 30.2307 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 238/1000
2023-10-25 19:55:53.292 
Epoch 238/1000 
	 loss: 29.4004, MinusLogProbMetric: 29.4004, val_loss: 29.7789, val_MinusLogProbMetric: 29.7789

Epoch 238: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.4004 - MinusLogProbMetric: 29.4004 - val_loss: 29.7789 - val_MinusLogProbMetric: 29.7789 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 239/1000
2023-10-25 19:56:36.012 
Epoch 239/1000 
	 loss: 29.2383, MinusLogProbMetric: 29.2383, val_loss: 30.4437, val_MinusLogProbMetric: 30.4437

Epoch 239: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.2383 - MinusLogProbMetric: 29.2383 - val_loss: 30.4437 - val_MinusLogProbMetric: 30.4437 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 240/1000
2023-10-25 19:57:18.072 
Epoch 240/1000 
	 loss: 29.3067, MinusLogProbMetric: 29.3067, val_loss: 29.9086, val_MinusLogProbMetric: 29.9086

Epoch 240: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.3067 - MinusLogProbMetric: 29.3067 - val_loss: 29.9086 - val_MinusLogProbMetric: 29.9086 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 241/1000
2023-10-25 19:58:00.562 
Epoch 241/1000 
	 loss: 29.2828, MinusLogProbMetric: 29.2828, val_loss: 30.0170, val_MinusLogProbMetric: 30.0170

Epoch 241: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.2828 - MinusLogProbMetric: 29.2828 - val_loss: 30.0170 - val_MinusLogProbMetric: 30.0170 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 242/1000
2023-10-25 19:58:42.773 
Epoch 242/1000 
	 loss: 29.2104, MinusLogProbMetric: 29.2104, val_loss: 30.0852, val_MinusLogProbMetric: 30.0852

Epoch 242: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.2104 - MinusLogProbMetric: 29.2104 - val_loss: 30.0852 - val_MinusLogProbMetric: 30.0852 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 243/1000
2023-10-25 19:59:25.188 
Epoch 243/1000 
	 loss: 29.4055, MinusLogProbMetric: 29.4055, val_loss: 29.9688, val_MinusLogProbMetric: 29.9688

Epoch 243: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.4055 - MinusLogProbMetric: 29.4055 - val_loss: 29.9688 - val_MinusLogProbMetric: 29.9688 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 244/1000
2023-10-25 20:00:07.640 
Epoch 244/1000 
	 loss: 29.2531, MinusLogProbMetric: 29.2531, val_loss: 29.4591, val_MinusLogProbMetric: 29.4591

Epoch 244: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.2531 - MinusLogProbMetric: 29.2531 - val_loss: 29.4591 - val_MinusLogProbMetric: 29.4591 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 245/1000
2023-10-25 20:00:49.796 
Epoch 245/1000 
	 loss: 29.1459, MinusLogProbMetric: 29.1459, val_loss: 30.1571, val_MinusLogProbMetric: 30.1571

Epoch 245: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.1459 - MinusLogProbMetric: 29.1459 - val_loss: 30.1571 - val_MinusLogProbMetric: 30.1571 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 246/1000
2023-10-25 20:01:32.466 
Epoch 246/1000 
	 loss: 29.3188, MinusLogProbMetric: 29.3188, val_loss: 29.8648, val_MinusLogProbMetric: 29.8648

Epoch 246: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.3188 - MinusLogProbMetric: 29.3188 - val_loss: 29.8648 - val_MinusLogProbMetric: 29.8648 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 247/1000
2023-10-25 20:02:14.866 
Epoch 247/1000 
	 loss: 29.4071, MinusLogProbMetric: 29.4071, val_loss: 29.5365, val_MinusLogProbMetric: 29.5365

Epoch 247: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.4071 - MinusLogProbMetric: 29.4071 - val_loss: 29.5365 - val_MinusLogProbMetric: 29.5365 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 248/1000
2023-10-25 20:02:57.350 
Epoch 248/1000 
	 loss: 29.2023, MinusLogProbMetric: 29.2023, val_loss: 29.6337, val_MinusLogProbMetric: 29.6337

Epoch 248: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.2023 - MinusLogProbMetric: 29.2023 - val_loss: 29.6337 - val_MinusLogProbMetric: 29.6337 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 249/1000
2023-10-25 20:03:39.995 
Epoch 249/1000 
	 loss: 29.1656, MinusLogProbMetric: 29.1656, val_loss: 30.0309, val_MinusLogProbMetric: 30.0309

Epoch 249: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.1656 - MinusLogProbMetric: 29.1656 - val_loss: 30.0309 - val_MinusLogProbMetric: 30.0309 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 250/1000
2023-10-25 20:04:21.988 
Epoch 250/1000 
	 loss: 29.3476, MinusLogProbMetric: 29.3476, val_loss: 29.7249, val_MinusLogProbMetric: 29.7249

Epoch 250: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.3476 - MinusLogProbMetric: 29.3476 - val_loss: 29.7249 - val_MinusLogProbMetric: 29.7249 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 251/1000
2023-10-25 20:05:04.186 
Epoch 251/1000 
	 loss: 29.2052, MinusLogProbMetric: 29.2052, val_loss: 29.6789, val_MinusLogProbMetric: 29.6789

Epoch 251: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.2052 - MinusLogProbMetric: 29.2052 - val_loss: 29.6789 - val_MinusLogProbMetric: 29.6789 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 252/1000
2023-10-25 20:05:46.502 
Epoch 252/1000 
	 loss: 29.2429, MinusLogProbMetric: 29.2429, val_loss: 29.6514, val_MinusLogProbMetric: 29.6514

Epoch 252: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.2429 - MinusLogProbMetric: 29.2429 - val_loss: 29.6514 - val_MinusLogProbMetric: 29.6514 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 253/1000
2023-10-25 20:06:28.757 
Epoch 253/1000 
	 loss: 29.1185, MinusLogProbMetric: 29.1185, val_loss: 29.4955, val_MinusLogProbMetric: 29.4955

Epoch 253: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.1185 - MinusLogProbMetric: 29.1185 - val_loss: 29.4955 - val_MinusLogProbMetric: 29.4955 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 254/1000
2023-10-25 20:07:10.964 
Epoch 254/1000 
	 loss: 29.2908, MinusLogProbMetric: 29.2908, val_loss: 29.9656, val_MinusLogProbMetric: 29.9656

Epoch 254: val_loss did not improve from 29.38819
196/196 - 42s - loss: 29.2908 - MinusLogProbMetric: 29.2908 - val_loss: 29.9656 - val_MinusLogProbMetric: 29.9656 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 255/1000
2023-10-25 20:07:53.732 
Epoch 255/1000 
	 loss: 29.1567, MinusLogProbMetric: 29.1567, val_loss: 30.4519, val_MinusLogProbMetric: 30.4519

Epoch 255: val_loss did not improve from 29.38819
196/196 - 43s - loss: 29.1567 - MinusLogProbMetric: 29.1567 - val_loss: 30.4519 - val_MinusLogProbMetric: 30.4519 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 256/1000
2023-10-25 20:08:36.449 
Epoch 256/1000 
	 loss: 29.2855, MinusLogProbMetric: 29.2855, val_loss: 29.3786, val_MinusLogProbMetric: 29.3786

Epoch 256: val_loss improved from 29.38819 to 29.37864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 29.2855 - MinusLogProbMetric: 29.2855 - val_loss: 29.3786 - val_MinusLogProbMetric: 29.3786 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 257/1000
2023-10-25 20:09:19.302 
Epoch 257/1000 
	 loss: 29.0651, MinusLogProbMetric: 29.0651, val_loss: 30.0298, val_MinusLogProbMetric: 30.0298

Epoch 257: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.0651 - MinusLogProbMetric: 29.0651 - val_loss: 30.0298 - val_MinusLogProbMetric: 30.0298 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 258/1000
2023-10-25 20:10:01.526 
Epoch 258/1000 
	 loss: 29.2456, MinusLogProbMetric: 29.2456, val_loss: 29.8172, val_MinusLogProbMetric: 29.8172

Epoch 258: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.2456 - MinusLogProbMetric: 29.2456 - val_loss: 29.8172 - val_MinusLogProbMetric: 29.8172 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 259/1000
2023-10-25 20:10:43.976 
Epoch 259/1000 
	 loss: 29.1443, MinusLogProbMetric: 29.1443, val_loss: 29.6868, val_MinusLogProbMetric: 29.6868

Epoch 259: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.1443 - MinusLogProbMetric: 29.1443 - val_loss: 29.6868 - val_MinusLogProbMetric: 29.6868 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 260/1000
2023-10-25 20:11:26.332 
Epoch 260/1000 
	 loss: 29.2665, MinusLogProbMetric: 29.2665, val_loss: 30.0516, val_MinusLogProbMetric: 30.0516

Epoch 260: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.2665 - MinusLogProbMetric: 29.2665 - val_loss: 30.0516 - val_MinusLogProbMetric: 30.0516 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 261/1000
2023-10-25 20:12:08.599 
Epoch 261/1000 
	 loss: 29.4007, MinusLogProbMetric: 29.4007, val_loss: 29.5754, val_MinusLogProbMetric: 29.5754

Epoch 261: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.4007 - MinusLogProbMetric: 29.4007 - val_loss: 29.5754 - val_MinusLogProbMetric: 29.5754 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 262/1000
2023-10-25 20:12:50.746 
Epoch 262/1000 
	 loss: 29.1269, MinusLogProbMetric: 29.1269, val_loss: 29.4263, val_MinusLogProbMetric: 29.4263

Epoch 262: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.1269 - MinusLogProbMetric: 29.1269 - val_loss: 29.4263 - val_MinusLogProbMetric: 29.4263 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 263/1000
2023-10-25 20:13:31.931 
Epoch 263/1000 
	 loss: 29.2017, MinusLogProbMetric: 29.2017, val_loss: 29.5511, val_MinusLogProbMetric: 29.5511

Epoch 263: val_loss did not improve from 29.37864
196/196 - 41s - loss: 29.2017 - MinusLogProbMetric: 29.2017 - val_loss: 29.5511 - val_MinusLogProbMetric: 29.5511 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 264/1000
2023-10-25 20:14:13.237 
Epoch 264/1000 
	 loss: 29.1582, MinusLogProbMetric: 29.1582, val_loss: 30.4138, val_MinusLogProbMetric: 30.4138

Epoch 264: val_loss did not improve from 29.37864
196/196 - 41s - loss: 29.1582 - MinusLogProbMetric: 29.1582 - val_loss: 30.4138 - val_MinusLogProbMetric: 30.4138 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 265/1000
2023-10-25 20:14:55.197 
Epoch 265/1000 
	 loss: 29.0685, MinusLogProbMetric: 29.0685, val_loss: 29.8003, val_MinusLogProbMetric: 29.8003

Epoch 265: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.0685 - MinusLogProbMetric: 29.0685 - val_loss: 29.8003 - val_MinusLogProbMetric: 29.8003 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 266/1000
2023-10-25 20:15:37.472 
Epoch 266/1000 
	 loss: 29.3132, MinusLogProbMetric: 29.3132, val_loss: 29.4779, val_MinusLogProbMetric: 29.4779

Epoch 266: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.3132 - MinusLogProbMetric: 29.3132 - val_loss: 29.4779 - val_MinusLogProbMetric: 29.4779 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 267/1000
2023-10-25 20:16:19.585 
Epoch 267/1000 
	 loss: 29.0030, MinusLogProbMetric: 29.0030, val_loss: 29.3879, val_MinusLogProbMetric: 29.3879

Epoch 267: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.0030 - MinusLogProbMetric: 29.0030 - val_loss: 29.3879 - val_MinusLogProbMetric: 29.3879 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 268/1000
2023-10-25 20:17:02.258 
Epoch 268/1000 
	 loss: 29.1411, MinusLogProbMetric: 29.1411, val_loss: 29.7090, val_MinusLogProbMetric: 29.7090

Epoch 268: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.1411 - MinusLogProbMetric: 29.1411 - val_loss: 29.7090 - val_MinusLogProbMetric: 29.7090 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 269/1000
2023-10-25 20:17:45.092 
Epoch 269/1000 
	 loss: 29.1279, MinusLogProbMetric: 29.1279, val_loss: 30.1775, val_MinusLogProbMetric: 30.1775

Epoch 269: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.1279 - MinusLogProbMetric: 29.1279 - val_loss: 30.1775 - val_MinusLogProbMetric: 30.1775 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 270/1000
2023-10-25 20:18:27.273 
Epoch 270/1000 
	 loss: 29.0984, MinusLogProbMetric: 29.0984, val_loss: 29.6881, val_MinusLogProbMetric: 29.6881

Epoch 270: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.0984 - MinusLogProbMetric: 29.0984 - val_loss: 29.6881 - val_MinusLogProbMetric: 29.6881 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 271/1000
2023-10-25 20:19:10.118 
Epoch 271/1000 
	 loss: 29.0602, MinusLogProbMetric: 29.0602, val_loss: 29.6189, val_MinusLogProbMetric: 29.6189

Epoch 271: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.0602 - MinusLogProbMetric: 29.0602 - val_loss: 29.6189 - val_MinusLogProbMetric: 29.6189 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 272/1000
2023-10-25 20:19:52.839 
Epoch 272/1000 
	 loss: 29.5234, MinusLogProbMetric: 29.5234, val_loss: 29.7458, val_MinusLogProbMetric: 29.7458

Epoch 272: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.5234 - MinusLogProbMetric: 29.5234 - val_loss: 29.7458 - val_MinusLogProbMetric: 29.7458 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 273/1000
2023-10-25 20:20:35.667 
Epoch 273/1000 
	 loss: 28.9421, MinusLogProbMetric: 28.9421, val_loss: 30.3646, val_MinusLogProbMetric: 30.3646

Epoch 273: val_loss did not improve from 29.37864
196/196 - 43s - loss: 28.9421 - MinusLogProbMetric: 28.9421 - val_loss: 30.3646 - val_MinusLogProbMetric: 30.3646 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 274/1000
2023-10-25 20:21:18.485 
Epoch 274/1000 
	 loss: 29.0351, MinusLogProbMetric: 29.0351, val_loss: 29.5802, val_MinusLogProbMetric: 29.5802

Epoch 274: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.0351 - MinusLogProbMetric: 29.0351 - val_loss: 29.5802 - val_MinusLogProbMetric: 29.5802 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 275/1000
2023-10-25 20:22:01.505 
Epoch 275/1000 
	 loss: 29.1150, MinusLogProbMetric: 29.1150, val_loss: 29.5262, val_MinusLogProbMetric: 29.5262

Epoch 275: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.1150 - MinusLogProbMetric: 29.1150 - val_loss: 29.5262 - val_MinusLogProbMetric: 29.5262 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 276/1000
2023-10-25 20:22:44.496 
Epoch 276/1000 
	 loss: 29.1180, MinusLogProbMetric: 29.1180, val_loss: 29.5396, val_MinusLogProbMetric: 29.5396

Epoch 276: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.1180 - MinusLogProbMetric: 29.1180 - val_loss: 29.5396 - val_MinusLogProbMetric: 29.5396 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 277/1000
2023-10-25 20:23:27.117 
Epoch 277/1000 
	 loss: 29.0781, MinusLogProbMetric: 29.0781, val_loss: 29.5881, val_MinusLogProbMetric: 29.5881

Epoch 277: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.0781 - MinusLogProbMetric: 29.0781 - val_loss: 29.5881 - val_MinusLogProbMetric: 29.5881 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 278/1000
2023-10-25 20:24:09.519 
Epoch 278/1000 
	 loss: 29.2910, MinusLogProbMetric: 29.2910, val_loss: 29.3972, val_MinusLogProbMetric: 29.3972

Epoch 278: val_loss did not improve from 29.37864
196/196 - 42s - loss: 29.2910 - MinusLogProbMetric: 29.2910 - val_loss: 29.3972 - val_MinusLogProbMetric: 29.3972 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 279/1000
2023-10-25 20:24:52.773 
Epoch 279/1000 
	 loss: 29.0102, MinusLogProbMetric: 29.0102, val_loss: 29.4221, val_MinusLogProbMetric: 29.4221

Epoch 279: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.0102 - MinusLogProbMetric: 29.0102 - val_loss: 29.4221 - val_MinusLogProbMetric: 29.4221 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 280/1000
2023-10-25 20:25:35.940 
Epoch 280/1000 
	 loss: 29.1172, MinusLogProbMetric: 29.1172, val_loss: 29.5118, val_MinusLogProbMetric: 29.5118

Epoch 280: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.1172 - MinusLogProbMetric: 29.1172 - val_loss: 29.5118 - val_MinusLogProbMetric: 29.5118 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 281/1000
2023-10-25 20:26:18.980 
Epoch 281/1000 
	 loss: 29.0568, MinusLogProbMetric: 29.0568, val_loss: 29.6607, val_MinusLogProbMetric: 29.6607

Epoch 281: val_loss did not improve from 29.37864
196/196 - 43s - loss: 29.0568 - MinusLogProbMetric: 29.0568 - val_loss: 29.6607 - val_MinusLogProbMetric: 29.6607 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 282/1000
2023-10-25 20:27:01.614 
Epoch 282/1000 
	 loss: 29.0488, MinusLogProbMetric: 29.0488, val_loss: 29.2483, val_MinusLogProbMetric: 29.2483

Epoch 282: val_loss improved from 29.37864 to 29.24833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 29.0488 - MinusLogProbMetric: 29.0488 - val_loss: 29.2483 - val_MinusLogProbMetric: 29.2483 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 283/1000
2023-10-25 20:27:45.026 
Epoch 283/1000 
	 loss: 29.0921, MinusLogProbMetric: 29.0921, val_loss: 29.8616, val_MinusLogProbMetric: 29.8616

Epoch 283: val_loss did not improve from 29.24833
196/196 - 43s - loss: 29.0921 - MinusLogProbMetric: 29.0921 - val_loss: 29.8616 - val_MinusLogProbMetric: 29.8616 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 284/1000
2023-10-25 20:28:27.606 
Epoch 284/1000 
	 loss: 28.9228, MinusLogProbMetric: 28.9228, val_loss: 30.8705, val_MinusLogProbMetric: 30.8705

Epoch 284: val_loss did not improve from 29.24833
196/196 - 43s - loss: 28.9228 - MinusLogProbMetric: 28.9228 - val_loss: 30.8705 - val_MinusLogProbMetric: 30.8705 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 285/1000
2023-10-25 20:29:10.273 
Epoch 285/1000 
	 loss: 29.2295, MinusLogProbMetric: 29.2295, val_loss: 29.9635, val_MinusLogProbMetric: 29.9635

Epoch 285: val_loss did not improve from 29.24833
196/196 - 43s - loss: 29.2295 - MinusLogProbMetric: 29.2295 - val_loss: 29.9635 - val_MinusLogProbMetric: 29.9635 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 286/1000
2023-10-25 20:29:52.781 
Epoch 286/1000 
	 loss: 29.0079, MinusLogProbMetric: 29.0079, val_loss: 29.2429, val_MinusLogProbMetric: 29.2429

Epoch 286: val_loss improved from 29.24833 to 29.24291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 29.0079 - MinusLogProbMetric: 29.0079 - val_loss: 29.2429 - val_MinusLogProbMetric: 29.2429 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 287/1000
2023-10-25 20:30:36.347 
Epoch 287/1000 
	 loss: 29.0594, MinusLogProbMetric: 29.0594, val_loss: 29.3909, val_MinusLogProbMetric: 29.3909

Epoch 287: val_loss did not improve from 29.24291
196/196 - 43s - loss: 29.0594 - MinusLogProbMetric: 29.0594 - val_loss: 29.3909 - val_MinusLogProbMetric: 29.3909 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 288/1000
2023-10-25 20:31:18.613 
Epoch 288/1000 
	 loss: 28.9160, MinusLogProbMetric: 28.9160, val_loss: 29.9936, val_MinusLogProbMetric: 29.9936

Epoch 288: val_loss did not improve from 29.24291
196/196 - 42s - loss: 28.9160 - MinusLogProbMetric: 28.9160 - val_loss: 29.9936 - val_MinusLogProbMetric: 29.9936 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 289/1000
2023-10-25 20:32:01.119 
Epoch 289/1000 
	 loss: 29.0209, MinusLogProbMetric: 29.0209, val_loss: 30.3692, val_MinusLogProbMetric: 30.3692

Epoch 289: val_loss did not improve from 29.24291
196/196 - 43s - loss: 29.0209 - MinusLogProbMetric: 29.0209 - val_loss: 30.3692 - val_MinusLogProbMetric: 30.3692 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 290/1000
2023-10-25 20:32:43.524 
Epoch 290/1000 
	 loss: 28.8986, MinusLogProbMetric: 28.8986, val_loss: 29.6799, val_MinusLogProbMetric: 29.6799

Epoch 290: val_loss did not improve from 29.24291
196/196 - 42s - loss: 28.8986 - MinusLogProbMetric: 28.8986 - val_loss: 29.6799 - val_MinusLogProbMetric: 29.6799 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 291/1000
2023-10-25 20:33:26.410 
Epoch 291/1000 
	 loss: 29.2342, MinusLogProbMetric: 29.2342, val_loss: 30.9022, val_MinusLogProbMetric: 30.9022

Epoch 291: val_loss did not improve from 29.24291
196/196 - 43s - loss: 29.2342 - MinusLogProbMetric: 29.2342 - val_loss: 30.9022 - val_MinusLogProbMetric: 30.9022 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 292/1000
2023-10-25 20:34:09.510 
Epoch 292/1000 
	 loss: 28.9590, MinusLogProbMetric: 28.9590, val_loss: 30.1249, val_MinusLogProbMetric: 30.1249

Epoch 292: val_loss did not improve from 29.24291
196/196 - 43s - loss: 28.9590 - MinusLogProbMetric: 28.9590 - val_loss: 30.1249 - val_MinusLogProbMetric: 30.1249 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 293/1000
2023-10-25 20:34:52.182 
Epoch 293/1000 
	 loss: 28.9183, MinusLogProbMetric: 28.9183, val_loss: 29.3727, val_MinusLogProbMetric: 29.3727

Epoch 293: val_loss did not improve from 29.24291
196/196 - 43s - loss: 28.9183 - MinusLogProbMetric: 28.9183 - val_loss: 29.3727 - val_MinusLogProbMetric: 29.3727 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 294/1000
2023-10-25 20:35:34.752 
Epoch 294/1000 
	 loss: 28.8579, MinusLogProbMetric: 28.8579, val_loss: 29.5438, val_MinusLogProbMetric: 29.5438

Epoch 294: val_loss did not improve from 29.24291
196/196 - 43s - loss: 28.8579 - MinusLogProbMetric: 28.8579 - val_loss: 29.5438 - val_MinusLogProbMetric: 29.5438 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 295/1000
2023-10-25 20:36:17.573 
Epoch 295/1000 
	 loss: 28.9085, MinusLogProbMetric: 28.9085, val_loss: 29.1800, val_MinusLogProbMetric: 29.1800

Epoch 295: val_loss improved from 29.24291 to 29.18000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 28.9085 - MinusLogProbMetric: 28.9085 - val_loss: 29.1800 - val_MinusLogProbMetric: 29.1800 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 296/1000
2023-10-25 20:37:00.681 
Epoch 296/1000 
	 loss: 29.4172, MinusLogProbMetric: 29.4172, val_loss: 29.6637, val_MinusLogProbMetric: 29.6637

Epoch 296: val_loss did not improve from 29.18000
196/196 - 42s - loss: 29.4172 - MinusLogProbMetric: 29.4172 - val_loss: 29.6637 - val_MinusLogProbMetric: 29.6637 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 297/1000
2023-10-25 20:37:43.380 
Epoch 297/1000 
	 loss: 29.3065, MinusLogProbMetric: 29.3065, val_loss: 29.9703, val_MinusLogProbMetric: 29.9703

Epoch 297: val_loss did not improve from 29.18000
196/196 - 43s - loss: 29.3065 - MinusLogProbMetric: 29.3065 - val_loss: 29.9703 - val_MinusLogProbMetric: 29.9703 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 298/1000
2023-10-25 20:38:25.792 
Epoch 298/1000 
	 loss: 28.8927, MinusLogProbMetric: 28.8927, val_loss: 29.8894, val_MinusLogProbMetric: 29.8894

Epoch 298: val_loss did not improve from 29.18000
196/196 - 42s - loss: 28.8927 - MinusLogProbMetric: 28.8927 - val_loss: 29.8894 - val_MinusLogProbMetric: 29.8894 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 299/1000
2023-10-25 20:39:08.395 
Epoch 299/1000 
	 loss: 29.0260, MinusLogProbMetric: 29.0260, val_loss: 30.0755, val_MinusLogProbMetric: 30.0755

Epoch 299: val_loss did not improve from 29.18000
196/196 - 43s - loss: 29.0260 - MinusLogProbMetric: 29.0260 - val_loss: 30.0755 - val_MinusLogProbMetric: 30.0755 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 300/1000
2023-10-25 20:39:50.992 
Epoch 300/1000 
	 loss: 28.8946, MinusLogProbMetric: 28.8946, val_loss: 29.9668, val_MinusLogProbMetric: 29.9668

Epoch 300: val_loss did not improve from 29.18000
196/196 - 43s - loss: 28.8946 - MinusLogProbMetric: 28.8946 - val_loss: 29.9668 - val_MinusLogProbMetric: 29.9668 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 301/1000
2023-10-25 20:40:33.568 
Epoch 301/1000 
	 loss: 29.2303, MinusLogProbMetric: 29.2303, val_loss: 30.1978, val_MinusLogProbMetric: 30.1978

Epoch 301: val_loss did not improve from 29.18000
196/196 - 43s - loss: 29.2303 - MinusLogProbMetric: 29.2303 - val_loss: 30.1978 - val_MinusLogProbMetric: 30.1978 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 302/1000
2023-10-25 20:41:15.755 
Epoch 302/1000 
	 loss: 28.9529, MinusLogProbMetric: 28.9529, val_loss: 29.9669, val_MinusLogProbMetric: 29.9669

Epoch 302: val_loss did not improve from 29.18000
196/196 - 42s - loss: 28.9529 - MinusLogProbMetric: 28.9529 - val_loss: 29.9669 - val_MinusLogProbMetric: 29.9669 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 303/1000
2023-10-25 20:41:57.116 
Epoch 303/1000 
	 loss: 28.9755, MinusLogProbMetric: 28.9755, val_loss: 29.7515, val_MinusLogProbMetric: 29.7515

Epoch 303: val_loss did not improve from 29.18000
196/196 - 41s - loss: 28.9755 - MinusLogProbMetric: 28.9755 - val_loss: 29.7515 - val_MinusLogProbMetric: 29.7515 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 304/1000
2023-10-25 20:42:38.783 
Epoch 304/1000 
	 loss: 28.9106, MinusLogProbMetric: 28.9106, val_loss: 29.3576, val_MinusLogProbMetric: 29.3576

Epoch 304: val_loss did not improve from 29.18000
196/196 - 42s - loss: 28.9106 - MinusLogProbMetric: 28.9106 - val_loss: 29.3576 - val_MinusLogProbMetric: 29.3576 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 305/1000
2023-10-25 20:43:17.734 
Epoch 305/1000 
	 loss: 28.9362, MinusLogProbMetric: 28.9362, val_loss: 30.0261, val_MinusLogProbMetric: 30.0261

Epoch 305: val_loss did not improve from 29.18000
196/196 - 39s - loss: 28.9362 - MinusLogProbMetric: 28.9362 - val_loss: 30.0261 - val_MinusLogProbMetric: 30.0261 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 306/1000
2023-10-25 20:43:58.036 
Epoch 306/1000 
	 loss: 28.8999, MinusLogProbMetric: 28.8999, val_loss: 29.4165, val_MinusLogProbMetric: 29.4165

Epoch 306: val_loss did not improve from 29.18000
196/196 - 40s - loss: 28.8999 - MinusLogProbMetric: 28.8999 - val_loss: 29.4165 - val_MinusLogProbMetric: 29.4165 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 307/1000
2023-10-25 20:44:38.513 
Epoch 307/1000 
	 loss: 28.8457, MinusLogProbMetric: 28.8457, val_loss: 29.3227, val_MinusLogProbMetric: 29.3227

Epoch 307: val_loss did not improve from 29.18000
196/196 - 40s - loss: 28.8457 - MinusLogProbMetric: 28.8457 - val_loss: 29.3227 - val_MinusLogProbMetric: 29.3227 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 308/1000
2023-10-25 20:45:21.271 
Epoch 308/1000 
	 loss: 29.3693, MinusLogProbMetric: 29.3693, val_loss: 29.8112, val_MinusLogProbMetric: 29.8112

Epoch 308: val_loss did not improve from 29.18000
196/196 - 43s - loss: 29.3693 - MinusLogProbMetric: 29.3693 - val_loss: 29.8112 - val_MinusLogProbMetric: 29.8112 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 309/1000
2023-10-25 20:46:04.242 
Epoch 309/1000 
	 loss: 28.8752, MinusLogProbMetric: 28.8752, val_loss: 30.2321, val_MinusLogProbMetric: 30.2321

Epoch 309: val_loss did not improve from 29.18000
196/196 - 43s - loss: 28.8752 - MinusLogProbMetric: 28.8752 - val_loss: 30.2321 - val_MinusLogProbMetric: 30.2321 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 310/1000
2023-10-25 20:46:45.900 
Epoch 310/1000 
	 loss: 29.0193, MinusLogProbMetric: 29.0193, val_loss: 29.6261, val_MinusLogProbMetric: 29.6261

Epoch 310: val_loss did not improve from 29.18000
196/196 - 42s - loss: 29.0193 - MinusLogProbMetric: 29.0193 - val_loss: 29.6261 - val_MinusLogProbMetric: 29.6261 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 311/1000
2023-10-25 20:47:28.649 
Epoch 311/1000 
	 loss: 28.9027, MinusLogProbMetric: 28.9027, val_loss: 29.2368, val_MinusLogProbMetric: 29.2368

Epoch 311: val_loss did not improve from 29.18000
196/196 - 43s - loss: 28.9027 - MinusLogProbMetric: 28.9027 - val_loss: 29.2368 - val_MinusLogProbMetric: 29.2368 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 312/1000
2023-10-25 20:48:11.162 
Epoch 312/1000 
	 loss: 28.9788, MinusLogProbMetric: 28.9788, val_loss: 29.2958, val_MinusLogProbMetric: 29.2958

Epoch 312: val_loss did not improve from 29.18000
196/196 - 43s - loss: 28.9788 - MinusLogProbMetric: 28.9788 - val_loss: 29.2958 - val_MinusLogProbMetric: 29.2958 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 313/1000
2023-10-25 20:48:53.656 
Epoch 313/1000 
	 loss: 29.0844, MinusLogProbMetric: 29.0844, val_loss: 29.2482, val_MinusLogProbMetric: 29.2482

Epoch 313: val_loss did not improve from 29.18000
196/196 - 42s - loss: 29.0844 - MinusLogProbMetric: 29.0844 - val_loss: 29.2482 - val_MinusLogProbMetric: 29.2482 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 314/1000
2023-10-25 20:49:36.312 
Epoch 314/1000 
	 loss: 28.8140, MinusLogProbMetric: 28.8140, val_loss: 29.6668, val_MinusLogProbMetric: 29.6668

Epoch 314: val_loss did not improve from 29.18000
196/196 - 43s - loss: 28.8140 - MinusLogProbMetric: 28.8140 - val_loss: 29.6668 - val_MinusLogProbMetric: 29.6668 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 315/1000
2023-10-25 20:50:18.826 
Epoch 315/1000 
	 loss: 28.8409, MinusLogProbMetric: 28.8409, val_loss: 29.9999, val_MinusLogProbMetric: 29.9999

Epoch 315: val_loss did not improve from 29.18000
196/196 - 43s - loss: 28.8409 - MinusLogProbMetric: 28.8409 - val_loss: 29.9999 - val_MinusLogProbMetric: 29.9999 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 316/1000
2023-10-25 20:51:01.288 
Epoch 316/1000 
	 loss: 29.0172, MinusLogProbMetric: 29.0172, val_loss: 29.7321, val_MinusLogProbMetric: 29.7321

Epoch 316: val_loss did not improve from 29.18000
196/196 - 42s - loss: 29.0172 - MinusLogProbMetric: 29.0172 - val_loss: 29.7321 - val_MinusLogProbMetric: 29.7321 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 317/1000
2023-10-25 20:51:43.924 
Epoch 317/1000 
	 loss: 28.8164, MinusLogProbMetric: 28.8164, val_loss: 30.0507, val_MinusLogProbMetric: 30.0507

Epoch 317: val_loss did not improve from 29.18000
196/196 - 43s - loss: 28.8164 - MinusLogProbMetric: 28.8164 - val_loss: 30.0507 - val_MinusLogProbMetric: 30.0507 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 318/1000
2023-10-25 20:52:26.381 
Epoch 318/1000 
	 loss: 28.8330, MinusLogProbMetric: 28.8330, val_loss: 29.3823, val_MinusLogProbMetric: 29.3823

Epoch 318: val_loss did not improve from 29.18000
196/196 - 42s - loss: 28.8330 - MinusLogProbMetric: 28.8330 - val_loss: 29.3823 - val_MinusLogProbMetric: 29.3823 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 319/1000
2023-10-25 20:53:08.624 
Epoch 319/1000 
	 loss: 29.1154, MinusLogProbMetric: 29.1154, val_loss: 29.4017, val_MinusLogProbMetric: 29.4017

Epoch 319: val_loss did not improve from 29.18000
196/196 - 42s - loss: 29.1154 - MinusLogProbMetric: 29.1154 - val_loss: 29.4017 - val_MinusLogProbMetric: 29.4017 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 320/1000
2023-10-25 20:53:51.007 
Epoch 320/1000 
	 loss: 28.7551, MinusLogProbMetric: 28.7551, val_loss: 29.0912, val_MinusLogProbMetric: 29.0912

Epoch 320: val_loss improved from 29.18000 to 29.09121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 28.7551 - MinusLogProbMetric: 28.7551 - val_loss: 29.0912 - val_MinusLogProbMetric: 29.0912 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 321/1000
2023-10-25 20:54:33.902 
Epoch 321/1000 
	 loss: 29.0687, MinusLogProbMetric: 29.0687, val_loss: 29.5284, val_MinusLogProbMetric: 29.5284

Epoch 321: val_loss did not improve from 29.09121
196/196 - 42s - loss: 29.0687 - MinusLogProbMetric: 29.0687 - val_loss: 29.5284 - val_MinusLogProbMetric: 29.5284 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 322/1000
2023-10-25 20:55:16.311 
Epoch 322/1000 
	 loss: 28.8708, MinusLogProbMetric: 28.8708, val_loss: 30.2740, val_MinusLogProbMetric: 30.2740

Epoch 322: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.8708 - MinusLogProbMetric: 28.8708 - val_loss: 30.2740 - val_MinusLogProbMetric: 30.2740 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 323/1000
2023-10-25 20:55:58.951 
Epoch 323/1000 
	 loss: 28.7519, MinusLogProbMetric: 28.7519, val_loss: 29.5323, val_MinusLogProbMetric: 29.5323

Epoch 323: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.7519 - MinusLogProbMetric: 28.7519 - val_loss: 29.5323 - val_MinusLogProbMetric: 29.5323 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 324/1000
2023-10-25 20:56:40.525 
Epoch 324/1000 
	 loss: 28.9856, MinusLogProbMetric: 28.9856, val_loss: 29.4196, val_MinusLogProbMetric: 29.4196

Epoch 324: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.9856 - MinusLogProbMetric: 28.9856 - val_loss: 29.4196 - val_MinusLogProbMetric: 29.4196 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 325/1000
2023-10-25 20:57:23.174 
Epoch 325/1000 
	 loss: 28.8448, MinusLogProbMetric: 28.8448, val_loss: 30.8210, val_MinusLogProbMetric: 30.8210

Epoch 325: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.8448 - MinusLogProbMetric: 28.8448 - val_loss: 30.8210 - val_MinusLogProbMetric: 30.8210 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 326/1000
2023-10-25 20:58:05.661 
Epoch 326/1000 
	 loss: 28.9007, MinusLogProbMetric: 28.9007, val_loss: 29.2223, val_MinusLogProbMetric: 29.2223

Epoch 326: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.9007 - MinusLogProbMetric: 28.9007 - val_loss: 29.2223 - val_MinusLogProbMetric: 29.2223 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 327/1000
2023-10-25 20:58:47.532 
Epoch 327/1000 
	 loss: 28.7809, MinusLogProbMetric: 28.7809, val_loss: 29.1619, val_MinusLogProbMetric: 29.1619

Epoch 327: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.7809 - MinusLogProbMetric: 28.7809 - val_loss: 29.1619 - val_MinusLogProbMetric: 29.1619 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 328/1000
2023-10-25 20:59:29.437 
Epoch 328/1000 
	 loss: 28.8936, MinusLogProbMetric: 28.8936, val_loss: 29.4559, val_MinusLogProbMetric: 29.4559

Epoch 328: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.8936 - MinusLogProbMetric: 28.8936 - val_loss: 29.4559 - val_MinusLogProbMetric: 29.4559 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 329/1000
2023-10-25 21:00:11.952 
Epoch 329/1000 
	 loss: 28.8460, MinusLogProbMetric: 28.8460, val_loss: 30.3144, val_MinusLogProbMetric: 30.3144

Epoch 329: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.8460 - MinusLogProbMetric: 28.8460 - val_loss: 30.3144 - val_MinusLogProbMetric: 30.3144 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 330/1000
2023-10-25 21:00:54.114 
Epoch 330/1000 
	 loss: 28.8643, MinusLogProbMetric: 28.8643, val_loss: 29.4268, val_MinusLogProbMetric: 29.4268

Epoch 330: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.8643 - MinusLogProbMetric: 28.8643 - val_loss: 29.4268 - val_MinusLogProbMetric: 29.4268 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 331/1000
2023-10-25 21:01:36.912 
Epoch 331/1000 
	 loss: 28.9907, MinusLogProbMetric: 28.9907, val_loss: 29.3403, val_MinusLogProbMetric: 29.3403

Epoch 331: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.9907 - MinusLogProbMetric: 28.9907 - val_loss: 29.3403 - val_MinusLogProbMetric: 29.3403 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 332/1000
2023-10-25 21:02:19.467 
Epoch 332/1000 
	 loss: 28.7208, MinusLogProbMetric: 28.7208, val_loss: 29.1213, val_MinusLogProbMetric: 29.1213

Epoch 332: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.7208 - MinusLogProbMetric: 28.7208 - val_loss: 29.1213 - val_MinusLogProbMetric: 29.1213 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 333/1000
2023-10-25 21:03:00.693 
Epoch 333/1000 
	 loss: 28.9146, MinusLogProbMetric: 28.9146, val_loss: 31.9512, val_MinusLogProbMetric: 31.9512

Epoch 333: val_loss did not improve from 29.09121
196/196 - 41s - loss: 28.9146 - MinusLogProbMetric: 28.9146 - val_loss: 31.9512 - val_MinusLogProbMetric: 31.9512 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 334/1000
2023-10-25 21:03:42.432 
Epoch 334/1000 
	 loss: 28.8226, MinusLogProbMetric: 28.8226, val_loss: 29.5144, val_MinusLogProbMetric: 29.5144

Epoch 334: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.8226 - MinusLogProbMetric: 28.8226 - val_loss: 29.5144 - val_MinusLogProbMetric: 29.5144 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 335/1000
2023-10-25 21:04:24.490 
Epoch 335/1000 
	 loss: 28.8075, MinusLogProbMetric: 28.8075, val_loss: 29.7542, val_MinusLogProbMetric: 29.7542

Epoch 335: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.8075 - MinusLogProbMetric: 28.8075 - val_loss: 29.7542 - val_MinusLogProbMetric: 29.7542 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 336/1000
2023-10-25 21:05:05.539 
Epoch 336/1000 
	 loss: 28.8161, MinusLogProbMetric: 28.8161, val_loss: 29.2732, val_MinusLogProbMetric: 29.2732

Epoch 336: val_loss did not improve from 29.09121
196/196 - 41s - loss: 28.8161 - MinusLogProbMetric: 28.8161 - val_loss: 29.2732 - val_MinusLogProbMetric: 29.2732 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 337/1000
2023-10-25 21:05:48.512 
Epoch 337/1000 
	 loss: 28.6304, MinusLogProbMetric: 28.6304, val_loss: 29.8537, val_MinusLogProbMetric: 29.8537

Epoch 337: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.6304 - MinusLogProbMetric: 28.6304 - val_loss: 29.8537 - val_MinusLogProbMetric: 29.8537 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 338/1000
2023-10-25 21:06:31.434 
Epoch 338/1000 
	 loss: 29.0588, MinusLogProbMetric: 29.0588, val_loss: 29.2874, val_MinusLogProbMetric: 29.2874

Epoch 338: val_loss did not improve from 29.09121
196/196 - 43s - loss: 29.0588 - MinusLogProbMetric: 29.0588 - val_loss: 29.2874 - val_MinusLogProbMetric: 29.2874 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 339/1000
2023-10-25 21:07:14.058 
Epoch 339/1000 
	 loss: 28.7992, MinusLogProbMetric: 28.7992, val_loss: 29.2601, val_MinusLogProbMetric: 29.2601

Epoch 339: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.7992 - MinusLogProbMetric: 28.7992 - val_loss: 29.2601 - val_MinusLogProbMetric: 29.2601 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 340/1000
2023-10-25 21:07:55.811 
Epoch 340/1000 
	 loss: 28.7787, MinusLogProbMetric: 28.7787, val_loss: 29.6678, val_MinusLogProbMetric: 29.6678

Epoch 340: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.7787 - MinusLogProbMetric: 28.7787 - val_loss: 29.6678 - val_MinusLogProbMetric: 29.6678 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 341/1000
2023-10-25 21:08:38.183 
Epoch 341/1000 
	 loss: 28.7509, MinusLogProbMetric: 28.7509, val_loss: 29.3723, val_MinusLogProbMetric: 29.3723

Epoch 341: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.7509 - MinusLogProbMetric: 28.7509 - val_loss: 29.3723 - val_MinusLogProbMetric: 29.3723 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 342/1000
2023-10-25 21:09:21.114 
Epoch 342/1000 
	 loss: 28.6899, MinusLogProbMetric: 28.6899, val_loss: 29.7055, val_MinusLogProbMetric: 29.7055

Epoch 342: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.6899 - MinusLogProbMetric: 28.6899 - val_loss: 29.7055 - val_MinusLogProbMetric: 29.7055 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 343/1000
2023-10-25 21:10:03.981 
Epoch 343/1000 
	 loss: 28.7248, MinusLogProbMetric: 28.7248, val_loss: 29.4308, val_MinusLogProbMetric: 29.4308

Epoch 343: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.7248 - MinusLogProbMetric: 28.7248 - val_loss: 29.4308 - val_MinusLogProbMetric: 29.4308 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 344/1000
2023-10-25 21:10:46.535 
Epoch 344/1000 
	 loss: 28.7678, MinusLogProbMetric: 28.7678, val_loss: 29.8363, val_MinusLogProbMetric: 29.8363

Epoch 344: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.7678 - MinusLogProbMetric: 28.7678 - val_loss: 29.8363 - val_MinusLogProbMetric: 29.8363 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 345/1000
2023-10-25 21:11:29.281 
Epoch 345/1000 
	 loss: 28.9475, MinusLogProbMetric: 28.9475, val_loss: 31.1239, val_MinusLogProbMetric: 31.1239

Epoch 345: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.9475 - MinusLogProbMetric: 28.9475 - val_loss: 31.1239 - val_MinusLogProbMetric: 31.1239 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 346/1000
2023-10-25 21:12:12.215 
Epoch 346/1000 
	 loss: 28.8463, MinusLogProbMetric: 28.8463, val_loss: 29.3509, val_MinusLogProbMetric: 29.3509

Epoch 346: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.8463 - MinusLogProbMetric: 28.8463 - val_loss: 29.3509 - val_MinusLogProbMetric: 29.3509 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 347/1000
2023-10-25 21:12:55.216 
Epoch 347/1000 
	 loss: 28.6957, MinusLogProbMetric: 28.6957, val_loss: 29.5190, val_MinusLogProbMetric: 29.5190

Epoch 347: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.6957 - MinusLogProbMetric: 28.6957 - val_loss: 29.5190 - val_MinusLogProbMetric: 29.5190 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 348/1000
2023-10-25 21:13:37.836 
Epoch 348/1000 
	 loss: 28.8018, MinusLogProbMetric: 28.8018, val_loss: 29.3996, val_MinusLogProbMetric: 29.3996

Epoch 348: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.8018 - MinusLogProbMetric: 28.8018 - val_loss: 29.3996 - val_MinusLogProbMetric: 29.3996 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 349/1000
2023-10-25 21:14:20.614 
Epoch 349/1000 
	 loss: 28.5782, MinusLogProbMetric: 28.5782, val_loss: 29.3489, val_MinusLogProbMetric: 29.3489

Epoch 349: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.5782 - MinusLogProbMetric: 28.5782 - val_loss: 29.3489 - val_MinusLogProbMetric: 29.3489 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 350/1000
2023-10-25 21:15:03.188 
Epoch 350/1000 
	 loss: 28.7385, MinusLogProbMetric: 28.7385, val_loss: 29.8219, val_MinusLogProbMetric: 29.8219

Epoch 350: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.7385 - MinusLogProbMetric: 28.7385 - val_loss: 29.8219 - val_MinusLogProbMetric: 29.8219 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 351/1000
2023-10-25 21:15:45.975 
Epoch 351/1000 
	 loss: 28.7743, MinusLogProbMetric: 28.7743, val_loss: 29.3640, val_MinusLogProbMetric: 29.3640

Epoch 351: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.7743 - MinusLogProbMetric: 28.7743 - val_loss: 29.3640 - val_MinusLogProbMetric: 29.3640 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 352/1000
2023-10-25 21:16:28.406 
Epoch 352/1000 
	 loss: 28.8715, MinusLogProbMetric: 28.8715, val_loss: 29.7661, val_MinusLogProbMetric: 29.7661

Epoch 352: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.8715 - MinusLogProbMetric: 28.8715 - val_loss: 29.7661 - val_MinusLogProbMetric: 29.7661 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 353/1000
2023-10-25 21:17:11.144 
Epoch 353/1000 
	 loss: 28.8124, MinusLogProbMetric: 28.8124, val_loss: 29.1594, val_MinusLogProbMetric: 29.1594

Epoch 353: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.8124 - MinusLogProbMetric: 28.8124 - val_loss: 29.1594 - val_MinusLogProbMetric: 29.1594 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 354/1000
2023-10-25 21:17:54.138 
Epoch 354/1000 
	 loss: 28.6406, MinusLogProbMetric: 28.6406, val_loss: 29.3919, val_MinusLogProbMetric: 29.3919

Epoch 354: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.6406 - MinusLogProbMetric: 28.6406 - val_loss: 29.3919 - val_MinusLogProbMetric: 29.3919 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 355/1000
2023-10-25 21:18:36.534 
Epoch 355/1000 
	 loss: 28.7741, MinusLogProbMetric: 28.7741, val_loss: 29.8876, val_MinusLogProbMetric: 29.8876

Epoch 355: val_loss did not improve from 29.09121
196/196 - 42s - loss: 28.7741 - MinusLogProbMetric: 28.7741 - val_loss: 29.8876 - val_MinusLogProbMetric: 29.8876 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 356/1000
2023-10-25 21:19:19.266 
Epoch 356/1000 
	 loss: 28.8812, MinusLogProbMetric: 28.8812, val_loss: 29.2562, val_MinusLogProbMetric: 29.2562

Epoch 356: val_loss did not improve from 29.09121
196/196 - 43s - loss: 28.8812 - MinusLogProbMetric: 28.8812 - val_loss: 29.2562 - val_MinusLogProbMetric: 29.2562 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 357/1000
2023-10-25 21:20:02.050 
Epoch 357/1000 
	 loss: 28.5923, MinusLogProbMetric: 28.5923, val_loss: 29.0623, val_MinusLogProbMetric: 29.0623

Epoch 357: val_loss improved from 29.09121 to 29.06235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 28.5923 - MinusLogProbMetric: 28.5923 - val_loss: 29.0623 - val_MinusLogProbMetric: 29.0623 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 358/1000
2023-10-25 21:20:45.705 
Epoch 358/1000 
	 loss: 28.7441, MinusLogProbMetric: 28.7441, val_loss: 29.2379, val_MinusLogProbMetric: 29.2379

Epoch 358: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.7441 - MinusLogProbMetric: 28.7441 - val_loss: 29.2379 - val_MinusLogProbMetric: 29.2379 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 359/1000
2023-10-25 21:21:28.670 
Epoch 359/1000 
	 loss: 28.9350, MinusLogProbMetric: 28.9350, val_loss: 29.6969, val_MinusLogProbMetric: 29.6969

Epoch 359: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.9350 - MinusLogProbMetric: 28.9350 - val_loss: 29.6969 - val_MinusLogProbMetric: 29.6969 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 360/1000
2023-10-25 21:22:11.376 
Epoch 360/1000 
	 loss: 28.6093, MinusLogProbMetric: 28.6093, val_loss: 32.2484, val_MinusLogProbMetric: 32.2484

Epoch 360: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.6093 - MinusLogProbMetric: 28.6093 - val_loss: 32.2484 - val_MinusLogProbMetric: 32.2484 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 361/1000
2023-10-25 21:22:54.114 
Epoch 361/1000 
	 loss: 28.9500, MinusLogProbMetric: 28.9500, val_loss: 29.6054, val_MinusLogProbMetric: 29.6054

Epoch 361: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.9500 - MinusLogProbMetric: 28.9500 - val_loss: 29.6054 - val_MinusLogProbMetric: 29.6054 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 362/1000
2023-10-25 21:23:36.641 
Epoch 362/1000 
	 loss: 28.7406, MinusLogProbMetric: 28.7406, val_loss: 29.2126, val_MinusLogProbMetric: 29.2126

Epoch 362: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.7406 - MinusLogProbMetric: 28.7406 - val_loss: 29.2126 - val_MinusLogProbMetric: 29.2126 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 363/1000
2023-10-25 21:24:19.252 
Epoch 363/1000 
	 loss: 28.5806, MinusLogProbMetric: 28.5806, val_loss: 29.4332, val_MinusLogProbMetric: 29.4332

Epoch 363: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.5806 - MinusLogProbMetric: 28.5806 - val_loss: 29.4332 - val_MinusLogProbMetric: 29.4332 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 364/1000
2023-10-25 21:25:01.820 
Epoch 364/1000 
	 loss: 28.7876, MinusLogProbMetric: 28.7876, val_loss: 29.2225, val_MinusLogProbMetric: 29.2225

Epoch 364: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.7876 - MinusLogProbMetric: 28.7876 - val_loss: 29.2225 - val_MinusLogProbMetric: 29.2225 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 365/1000
2023-10-25 21:25:44.142 
Epoch 365/1000 
	 loss: 28.5686, MinusLogProbMetric: 28.5686, val_loss: 29.3436, val_MinusLogProbMetric: 29.3436

Epoch 365: val_loss did not improve from 29.06235
196/196 - 42s - loss: 28.5686 - MinusLogProbMetric: 28.5686 - val_loss: 29.3436 - val_MinusLogProbMetric: 29.3436 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 366/1000
2023-10-25 21:26:26.689 
Epoch 366/1000 
	 loss: 28.6609, MinusLogProbMetric: 28.6609, val_loss: 29.1516, val_MinusLogProbMetric: 29.1516

Epoch 366: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.6609 - MinusLogProbMetric: 28.6609 - val_loss: 29.1516 - val_MinusLogProbMetric: 29.1516 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 367/1000
2023-10-25 21:27:09.555 
Epoch 367/1000 
	 loss: 28.6366, MinusLogProbMetric: 28.6366, val_loss: 29.2510, val_MinusLogProbMetric: 29.2510

Epoch 367: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.6366 - MinusLogProbMetric: 28.6366 - val_loss: 29.2510 - val_MinusLogProbMetric: 29.2510 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 368/1000
2023-10-25 21:27:52.277 
Epoch 368/1000 
	 loss: 28.9947, MinusLogProbMetric: 28.9947, val_loss: 30.3669, val_MinusLogProbMetric: 30.3669

Epoch 368: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.9947 - MinusLogProbMetric: 28.9947 - val_loss: 30.3669 - val_MinusLogProbMetric: 30.3669 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 369/1000
2023-10-25 21:28:34.843 
Epoch 369/1000 
	 loss: 28.6734, MinusLogProbMetric: 28.6734, val_loss: 29.4143, val_MinusLogProbMetric: 29.4143

Epoch 369: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.6734 - MinusLogProbMetric: 28.6734 - val_loss: 29.4143 - val_MinusLogProbMetric: 29.4143 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 370/1000
2023-10-25 21:29:17.438 
Epoch 370/1000 
	 loss: 28.5659, MinusLogProbMetric: 28.5659, val_loss: 29.3000, val_MinusLogProbMetric: 29.3000

Epoch 370: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.5659 - MinusLogProbMetric: 28.5659 - val_loss: 29.3000 - val_MinusLogProbMetric: 29.3000 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 371/1000
2023-10-25 21:30:00.305 
Epoch 371/1000 
	 loss: 29.1275, MinusLogProbMetric: 29.1275, val_loss: 29.5988, val_MinusLogProbMetric: 29.5988

Epoch 371: val_loss did not improve from 29.06235
196/196 - 43s - loss: 29.1275 - MinusLogProbMetric: 29.1275 - val_loss: 29.5988 - val_MinusLogProbMetric: 29.5988 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 372/1000
2023-10-25 21:30:43.187 
Epoch 372/1000 
	 loss: 28.6092, MinusLogProbMetric: 28.6092, val_loss: 29.1109, val_MinusLogProbMetric: 29.1109

Epoch 372: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.6092 - MinusLogProbMetric: 28.6092 - val_loss: 29.1109 - val_MinusLogProbMetric: 29.1109 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 373/1000
2023-10-25 21:31:25.897 
Epoch 373/1000 
	 loss: 28.7990, MinusLogProbMetric: 28.7990, val_loss: 29.8419, val_MinusLogProbMetric: 29.8419

Epoch 373: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.7990 - MinusLogProbMetric: 28.7990 - val_loss: 29.8419 - val_MinusLogProbMetric: 29.8419 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 374/1000
2023-10-25 21:32:08.781 
Epoch 374/1000 
	 loss: 28.6553, MinusLogProbMetric: 28.6553, val_loss: 29.8292, val_MinusLogProbMetric: 29.8292

Epoch 374: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.6553 - MinusLogProbMetric: 28.6553 - val_loss: 29.8292 - val_MinusLogProbMetric: 29.8292 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 375/1000
2023-10-25 21:32:51.370 
Epoch 375/1000 
	 loss: 28.5530, MinusLogProbMetric: 28.5530, val_loss: 29.4277, val_MinusLogProbMetric: 29.4277

Epoch 375: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.5530 - MinusLogProbMetric: 28.5530 - val_loss: 29.4277 - val_MinusLogProbMetric: 29.4277 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 376/1000
2023-10-25 21:33:34.365 
Epoch 376/1000 
	 loss: 28.6745, MinusLogProbMetric: 28.6745, val_loss: 29.2473, val_MinusLogProbMetric: 29.2473

Epoch 376: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.6745 - MinusLogProbMetric: 28.6745 - val_loss: 29.2473 - val_MinusLogProbMetric: 29.2473 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 377/1000
2023-10-25 21:34:17.243 
Epoch 377/1000 
	 loss: 28.5791, MinusLogProbMetric: 28.5791, val_loss: 29.3466, val_MinusLogProbMetric: 29.3466

Epoch 377: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.5791 - MinusLogProbMetric: 28.5791 - val_loss: 29.3466 - val_MinusLogProbMetric: 29.3466 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 378/1000
2023-10-25 21:34:59.878 
Epoch 378/1000 
	 loss: 28.8794, MinusLogProbMetric: 28.8794, val_loss: 29.1996, val_MinusLogProbMetric: 29.1996

Epoch 378: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.8794 - MinusLogProbMetric: 28.8794 - val_loss: 29.1996 - val_MinusLogProbMetric: 29.1996 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 379/1000
2023-10-25 21:35:42.779 
Epoch 379/1000 
	 loss: 28.7165, MinusLogProbMetric: 28.7165, val_loss: 29.0834, val_MinusLogProbMetric: 29.0834

Epoch 379: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.7165 - MinusLogProbMetric: 28.7165 - val_loss: 29.0834 - val_MinusLogProbMetric: 29.0834 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 380/1000
2023-10-25 21:36:25.383 
Epoch 380/1000 
	 loss: 28.5851, MinusLogProbMetric: 28.5851, val_loss: 29.4138, val_MinusLogProbMetric: 29.4138

Epoch 380: val_loss did not improve from 29.06235
196/196 - 43s - loss: 28.5851 - MinusLogProbMetric: 28.5851 - val_loss: 29.4138 - val_MinusLogProbMetric: 29.4138 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 381/1000
2023-10-25 21:37:08.326 
Epoch 381/1000 
	 loss: 28.6533, MinusLogProbMetric: 28.6533, val_loss: 29.0070, val_MinusLogProbMetric: 29.0070

Epoch 381: val_loss improved from 29.06235 to 29.00700, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 28.6533 - MinusLogProbMetric: 28.6533 - val_loss: 29.0070 - val_MinusLogProbMetric: 29.0070 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 382/1000
2023-10-25 21:37:51.765 
Epoch 382/1000 
	 loss: 28.7116, MinusLogProbMetric: 28.7116, val_loss: 28.9226, val_MinusLogProbMetric: 28.9226

Epoch 382: val_loss improved from 29.00700 to 28.92257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 28.7116 - MinusLogProbMetric: 28.7116 - val_loss: 28.9226 - val_MinusLogProbMetric: 28.9226 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 383/1000
2023-10-25 21:38:35.043 
Epoch 383/1000 
	 loss: 28.9086, MinusLogProbMetric: 28.9086, val_loss: 29.7170, val_MinusLogProbMetric: 29.7170

Epoch 383: val_loss did not improve from 28.92257
196/196 - 43s - loss: 28.9086 - MinusLogProbMetric: 28.9086 - val_loss: 29.7170 - val_MinusLogProbMetric: 29.7170 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 384/1000
2023-10-25 21:39:18.040 
Epoch 384/1000 
	 loss: 28.6630, MinusLogProbMetric: 28.6630, val_loss: 30.1909, val_MinusLogProbMetric: 30.1909

Epoch 384: val_loss did not improve from 28.92257
196/196 - 43s - loss: 28.6630 - MinusLogProbMetric: 28.6630 - val_loss: 30.1909 - val_MinusLogProbMetric: 30.1909 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 385/1000
2023-10-25 21:40:00.649 
Epoch 385/1000 
	 loss: 28.5324, MinusLogProbMetric: 28.5324, val_loss: 30.0530, val_MinusLogProbMetric: 30.0530

Epoch 385: val_loss did not improve from 28.92257
196/196 - 43s - loss: 28.5324 - MinusLogProbMetric: 28.5324 - val_loss: 30.0530 - val_MinusLogProbMetric: 30.0530 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 386/1000
2023-10-25 21:40:43.545 
Epoch 386/1000 
	 loss: 28.6008, MinusLogProbMetric: 28.6008, val_loss: 28.9547, val_MinusLogProbMetric: 28.9547

Epoch 386: val_loss did not improve from 28.92257
196/196 - 43s - loss: 28.6008 - MinusLogProbMetric: 28.6008 - val_loss: 28.9547 - val_MinusLogProbMetric: 28.9547 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 387/1000
2023-10-25 21:41:25.873 
Epoch 387/1000 
	 loss: 28.4977, MinusLogProbMetric: 28.4977, val_loss: 29.1241, val_MinusLogProbMetric: 29.1241

Epoch 387: val_loss did not improve from 28.92257
196/196 - 42s - loss: 28.4977 - MinusLogProbMetric: 28.4977 - val_loss: 29.1241 - val_MinusLogProbMetric: 29.1241 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 388/1000
2023-10-25 21:42:08.478 
Epoch 388/1000 
	 loss: 28.9304, MinusLogProbMetric: 28.9304, val_loss: 30.0659, val_MinusLogProbMetric: 30.0659

Epoch 388: val_loss did not improve from 28.92257
196/196 - 43s - loss: 28.9304 - MinusLogProbMetric: 28.9304 - val_loss: 30.0659 - val_MinusLogProbMetric: 30.0659 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 389/1000
2023-10-25 21:42:51.225 
Epoch 389/1000 
	 loss: 28.6149, MinusLogProbMetric: 28.6149, val_loss: 29.6564, val_MinusLogProbMetric: 29.6564

Epoch 389: val_loss did not improve from 28.92257
196/196 - 43s - loss: 28.6149 - MinusLogProbMetric: 28.6149 - val_loss: 29.6564 - val_MinusLogProbMetric: 29.6564 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 390/1000
2023-10-25 21:43:33.772 
Epoch 390/1000 
	 loss: 28.5642, MinusLogProbMetric: 28.5642, val_loss: 28.9639, val_MinusLogProbMetric: 28.9639

Epoch 390: val_loss did not improve from 28.92257
196/196 - 43s - loss: 28.5642 - MinusLogProbMetric: 28.5642 - val_loss: 28.9639 - val_MinusLogProbMetric: 28.9639 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 391/1000
2023-10-25 21:44:16.197 
Epoch 391/1000 
	 loss: 28.7613, MinusLogProbMetric: 28.7613, val_loss: 29.4705, val_MinusLogProbMetric: 29.4705

Epoch 391: val_loss did not improve from 28.92257
196/196 - 42s - loss: 28.7613 - MinusLogProbMetric: 28.7613 - val_loss: 29.4705 - val_MinusLogProbMetric: 29.4705 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 392/1000
2023-10-25 21:44:58.102 
Epoch 392/1000 
	 loss: 28.5301, MinusLogProbMetric: 28.5301, val_loss: 28.8937, val_MinusLogProbMetric: 28.8937

Epoch 392: val_loss improved from 28.92257 to 28.89373, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 28.5301 - MinusLogProbMetric: 28.5301 - val_loss: 28.8937 - val_MinusLogProbMetric: 28.8937 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 393/1000
2023-10-25 21:45:41.391 
Epoch 393/1000 
	 loss: 28.8725, MinusLogProbMetric: 28.8725, val_loss: 29.4730, val_MinusLogProbMetric: 29.4730

Epoch 393: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.8725 - MinusLogProbMetric: 28.8725 - val_loss: 29.4730 - val_MinusLogProbMetric: 29.4730 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 394/1000
2023-10-25 21:46:23.794 
Epoch 394/1000 
	 loss: 28.6352, MinusLogProbMetric: 28.6352, val_loss: 29.2195, val_MinusLogProbMetric: 29.2195

Epoch 394: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.6352 - MinusLogProbMetric: 28.6352 - val_loss: 29.2195 - val_MinusLogProbMetric: 29.2195 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 395/1000
2023-10-25 21:47:06.188 
Epoch 395/1000 
	 loss: 28.5014, MinusLogProbMetric: 28.5014, val_loss: 29.6585, val_MinusLogProbMetric: 29.6585

Epoch 395: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.5014 - MinusLogProbMetric: 28.5014 - val_loss: 29.6585 - val_MinusLogProbMetric: 29.6585 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 396/1000
2023-10-25 21:47:48.472 
Epoch 396/1000 
	 loss: 28.6919, MinusLogProbMetric: 28.6919, val_loss: 29.1766, val_MinusLogProbMetric: 29.1766

Epoch 396: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.6919 - MinusLogProbMetric: 28.6919 - val_loss: 29.1766 - val_MinusLogProbMetric: 29.1766 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 397/1000
2023-10-25 21:48:30.545 
Epoch 397/1000 
	 loss: 28.5617, MinusLogProbMetric: 28.5617, val_loss: 29.0811, val_MinusLogProbMetric: 29.0811

Epoch 397: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.5617 - MinusLogProbMetric: 28.5617 - val_loss: 29.0811 - val_MinusLogProbMetric: 29.0811 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 398/1000
2023-10-25 21:49:12.787 
Epoch 398/1000 
	 loss: 28.8860, MinusLogProbMetric: 28.8860, val_loss: 29.2275, val_MinusLogProbMetric: 29.2275

Epoch 398: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.8860 - MinusLogProbMetric: 28.8860 - val_loss: 29.2275 - val_MinusLogProbMetric: 29.2275 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 399/1000
2023-10-25 21:49:55.234 
Epoch 399/1000 
	 loss: 28.4686, MinusLogProbMetric: 28.4686, val_loss: 29.8764, val_MinusLogProbMetric: 29.8764

Epoch 399: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4686 - MinusLogProbMetric: 28.4686 - val_loss: 29.8764 - val_MinusLogProbMetric: 29.8764 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 400/1000
2023-10-25 21:50:36.892 
Epoch 400/1000 
	 loss: 28.6636, MinusLogProbMetric: 28.6636, val_loss: 29.2301, val_MinusLogProbMetric: 29.2301

Epoch 400: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.6636 - MinusLogProbMetric: 28.6636 - val_loss: 29.2301 - val_MinusLogProbMetric: 29.2301 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 401/1000
2023-10-25 21:51:19.348 
Epoch 401/1000 
	 loss: 28.4246, MinusLogProbMetric: 28.4246, val_loss: 30.0434, val_MinusLogProbMetric: 30.0434

Epoch 401: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4246 - MinusLogProbMetric: 28.4246 - val_loss: 30.0434 - val_MinusLogProbMetric: 30.0434 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 402/1000
2023-10-25 21:52:01.673 
Epoch 402/1000 
	 loss: 28.8904, MinusLogProbMetric: 28.8904, val_loss: 29.0121, val_MinusLogProbMetric: 29.0121

Epoch 402: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.8904 - MinusLogProbMetric: 28.8904 - val_loss: 29.0121 - val_MinusLogProbMetric: 29.0121 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 403/1000
2023-10-25 21:52:44.545 
Epoch 403/1000 
	 loss: 28.6923, MinusLogProbMetric: 28.6923, val_loss: 29.5831, val_MinusLogProbMetric: 29.5831

Epoch 403: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.6923 - MinusLogProbMetric: 28.6923 - val_loss: 29.5831 - val_MinusLogProbMetric: 29.5831 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 404/1000
2023-10-25 21:53:27.334 
Epoch 404/1000 
	 loss: 28.4057, MinusLogProbMetric: 28.4057, val_loss: 28.9603, val_MinusLogProbMetric: 28.9603

Epoch 404: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.4057 - MinusLogProbMetric: 28.4057 - val_loss: 28.9603 - val_MinusLogProbMetric: 28.9603 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 405/1000
2023-10-25 21:54:09.817 
Epoch 405/1000 
	 loss: 28.6321, MinusLogProbMetric: 28.6321, val_loss: 29.0087, val_MinusLogProbMetric: 29.0087

Epoch 405: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.6321 - MinusLogProbMetric: 28.6321 - val_loss: 29.0087 - val_MinusLogProbMetric: 29.0087 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 406/1000
2023-10-25 21:54:52.538 
Epoch 406/1000 
	 loss: 28.5084, MinusLogProbMetric: 28.5084, val_loss: 29.0625, val_MinusLogProbMetric: 29.0625

Epoch 406: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.5084 - MinusLogProbMetric: 28.5084 - val_loss: 29.0625 - val_MinusLogProbMetric: 29.0625 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 407/1000
2023-10-25 21:55:34.571 
Epoch 407/1000 
	 loss: 28.4963, MinusLogProbMetric: 28.4963, val_loss: 29.1543, val_MinusLogProbMetric: 29.1543

Epoch 407: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4963 - MinusLogProbMetric: 28.4963 - val_loss: 29.1543 - val_MinusLogProbMetric: 29.1543 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 408/1000
2023-10-25 21:56:16.859 
Epoch 408/1000 
	 loss: 28.5198, MinusLogProbMetric: 28.5198, val_loss: 29.1144, val_MinusLogProbMetric: 29.1144

Epoch 408: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.5198 - MinusLogProbMetric: 28.5198 - val_loss: 29.1144 - val_MinusLogProbMetric: 29.1144 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 409/1000
2023-10-25 21:56:59.393 
Epoch 409/1000 
	 loss: 28.7929, MinusLogProbMetric: 28.7929, val_loss: 29.2086, val_MinusLogProbMetric: 29.2086

Epoch 409: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.7929 - MinusLogProbMetric: 28.7929 - val_loss: 29.2086 - val_MinusLogProbMetric: 29.2086 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 410/1000
2023-10-25 21:57:41.978 
Epoch 410/1000 
	 loss: 28.4411, MinusLogProbMetric: 28.4411, val_loss: 29.2203, val_MinusLogProbMetric: 29.2203

Epoch 410: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.4411 - MinusLogProbMetric: 28.4411 - val_loss: 29.2203 - val_MinusLogProbMetric: 29.2203 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 411/1000
2023-10-25 21:58:24.771 
Epoch 411/1000 
	 loss: 28.5262, MinusLogProbMetric: 28.5262, val_loss: 30.0004, val_MinusLogProbMetric: 30.0004

Epoch 411: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.5262 - MinusLogProbMetric: 28.5262 - val_loss: 30.0004 - val_MinusLogProbMetric: 30.0004 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 412/1000
2023-10-25 21:59:07.898 
Epoch 412/1000 
	 loss: 28.4870, MinusLogProbMetric: 28.4870, val_loss: 29.0170, val_MinusLogProbMetric: 29.0170

Epoch 412: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.4870 - MinusLogProbMetric: 28.4870 - val_loss: 29.0170 - val_MinusLogProbMetric: 29.0170 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 413/1000
2023-10-25 21:59:50.455 
Epoch 413/1000 
	 loss: 28.4154, MinusLogProbMetric: 28.4154, val_loss: 30.0188, val_MinusLogProbMetric: 30.0188

Epoch 413: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.4154 - MinusLogProbMetric: 28.4154 - val_loss: 30.0188 - val_MinusLogProbMetric: 30.0188 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 414/1000
2023-10-25 22:00:32.066 
Epoch 414/1000 
	 loss: 28.4433, MinusLogProbMetric: 28.4433, val_loss: 29.1123, val_MinusLogProbMetric: 29.1123

Epoch 414: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4433 - MinusLogProbMetric: 28.4433 - val_loss: 29.1123 - val_MinusLogProbMetric: 29.1123 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 415/1000
2023-10-25 22:01:14.253 
Epoch 415/1000 
	 loss: 28.4564, MinusLogProbMetric: 28.4564, val_loss: 29.2518, val_MinusLogProbMetric: 29.2518

Epoch 415: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4564 - MinusLogProbMetric: 28.4564 - val_loss: 29.2518 - val_MinusLogProbMetric: 29.2518 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 416/1000
2023-10-25 22:01:56.807 
Epoch 416/1000 
	 loss: 28.5507, MinusLogProbMetric: 28.5507, val_loss: 29.0452, val_MinusLogProbMetric: 29.0452

Epoch 416: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.5507 - MinusLogProbMetric: 28.5507 - val_loss: 29.0452 - val_MinusLogProbMetric: 29.0452 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 417/1000
2023-10-25 22:02:39.401 
Epoch 417/1000 
	 loss: 28.5271, MinusLogProbMetric: 28.5271, val_loss: 29.2760, val_MinusLogProbMetric: 29.2760

Epoch 417: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.5271 - MinusLogProbMetric: 28.5271 - val_loss: 29.2760 - val_MinusLogProbMetric: 29.2760 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 418/1000
2023-10-25 22:03:21.791 
Epoch 418/1000 
	 loss: 28.6528, MinusLogProbMetric: 28.6528, val_loss: 29.1212, val_MinusLogProbMetric: 29.1212

Epoch 418: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.6528 - MinusLogProbMetric: 28.6528 - val_loss: 29.1212 - val_MinusLogProbMetric: 29.1212 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 419/1000
2023-10-25 22:04:04.505 
Epoch 419/1000 
	 loss: 28.4186, MinusLogProbMetric: 28.4186, val_loss: 29.2001, val_MinusLogProbMetric: 29.2001

Epoch 419: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.4186 - MinusLogProbMetric: 28.4186 - val_loss: 29.2001 - val_MinusLogProbMetric: 29.2001 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 420/1000
2023-10-25 22:04:46.641 
Epoch 420/1000 
	 loss: 28.4055, MinusLogProbMetric: 28.4055, val_loss: 29.0960, val_MinusLogProbMetric: 29.0960

Epoch 420: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4055 - MinusLogProbMetric: 28.4055 - val_loss: 29.0960 - val_MinusLogProbMetric: 29.0960 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 421/1000
2023-10-25 22:05:29.136 
Epoch 421/1000 
	 loss: 28.8633, MinusLogProbMetric: 28.8633, val_loss: 30.1699, val_MinusLogProbMetric: 30.1699

Epoch 421: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.8633 - MinusLogProbMetric: 28.8633 - val_loss: 30.1699 - val_MinusLogProbMetric: 30.1699 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 422/1000
2023-10-25 22:06:11.297 
Epoch 422/1000 
	 loss: 28.4331, MinusLogProbMetric: 28.4331, val_loss: 29.1910, val_MinusLogProbMetric: 29.1910

Epoch 422: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4331 - MinusLogProbMetric: 28.4331 - val_loss: 29.1910 - val_MinusLogProbMetric: 29.1910 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 423/1000
2023-10-25 22:06:53.015 
Epoch 423/1000 
	 loss: 28.5093, MinusLogProbMetric: 28.5093, val_loss: 29.0292, val_MinusLogProbMetric: 29.0292

Epoch 423: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.5093 - MinusLogProbMetric: 28.5093 - val_loss: 29.0292 - val_MinusLogProbMetric: 29.0292 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 424/1000
2023-10-25 22:07:35.145 
Epoch 424/1000 
	 loss: 28.4322, MinusLogProbMetric: 28.4322, val_loss: 29.0053, val_MinusLogProbMetric: 29.0053

Epoch 424: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4322 - MinusLogProbMetric: 28.4322 - val_loss: 29.0053 - val_MinusLogProbMetric: 29.0053 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 425/1000
2023-10-25 22:08:16.963 
Epoch 425/1000 
	 loss: 28.5914, MinusLogProbMetric: 28.5914, val_loss: 29.2142, val_MinusLogProbMetric: 29.2142

Epoch 425: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.5914 - MinusLogProbMetric: 28.5914 - val_loss: 29.2142 - val_MinusLogProbMetric: 29.2142 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 426/1000
2023-10-25 22:08:59.122 
Epoch 426/1000 
	 loss: 28.4162, MinusLogProbMetric: 28.4162, val_loss: 29.4111, val_MinusLogProbMetric: 29.4111

Epoch 426: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4162 - MinusLogProbMetric: 28.4162 - val_loss: 29.4111 - val_MinusLogProbMetric: 29.4111 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 427/1000
2023-10-25 22:09:41.483 
Epoch 427/1000 
	 loss: 28.4792, MinusLogProbMetric: 28.4792, val_loss: 29.6479, val_MinusLogProbMetric: 29.6479

Epoch 427: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4792 - MinusLogProbMetric: 28.4792 - val_loss: 29.6479 - val_MinusLogProbMetric: 29.6479 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 428/1000
2023-10-25 22:10:24.038 
Epoch 428/1000 
	 loss: 28.4322, MinusLogProbMetric: 28.4322, val_loss: 29.0889, val_MinusLogProbMetric: 29.0889

Epoch 428: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.4322 - MinusLogProbMetric: 28.4322 - val_loss: 29.0889 - val_MinusLogProbMetric: 29.0889 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 429/1000
2023-10-25 22:11:06.394 
Epoch 429/1000 
	 loss: 28.9191, MinusLogProbMetric: 28.9191, val_loss: 29.2775, val_MinusLogProbMetric: 29.2775

Epoch 429: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.9191 - MinusLogProbMetric: 28.9191 - val_loss: 29.2775 - val_MinusLogProbMetric: 29.2775 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 430/1000
2023-10-25 22:11:48.434 
Epoch 430/1000 
	 loss: 28.4888, MinusLogProbMetric: 28.4888, val_loss: 29.1432, val_MinusLogProbMetric: 29.1432

Epoch 430: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4888 - MinusLogProbMetric: 28.4888 - val_loss: 29.1432 - val_MinusLogProbMetric: 29.1432 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 431/1000
2023-10-25 22:12:30.851 
Epoch 431/1000 
	 loss: 28.3766, MinusLogProbMetric: 28.3766, val_loss: 29.2196, val_MinusLogProbMetric: 29.2196

Epoch 431: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.3766 - MinusLogProbMetric: 28.3766 - val_loss: 29.2196 - val_MinusLogProbMetric: 29.2196 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 432/1000
2023-10-25 22:13:13.417 
Epoch 432/1000 
	 loss: 28.5290, MinusLogProbMetric: 28.5290, val_loss: 29.0454, val_MinusLogProbMetric: 29.0454

Epoch 432: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.5290 - MinusLogProbMetric: 28.5290 - val_loss: 29.0454 - val_MinusLogProbMetric: 29.0454 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 433/1000
2023-10-25 22:13:55.489 
Epoch 433/1000 
	 loss: 28.5232, MinusLogProbMetric: 28.5232, val_loss: 29.2739, val_MinusLogProbMetric: 29.2739

Epoch 433: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.5232 - MinusLogProbMetric: 28.5232 - val_loss: 29.2739 - val_MinusLogProbMetric: 29.2739 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 434/1000
2023-10-25 22:14:37.618 
Epoch 434/1000 
	 loss: 28.4202, MinusLogProbMetric: 28.4202, val_loss: 29.0600, val_MinusLogProbMetric: 29.0600

Epoch 434: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4202 - MinusLogProbMetric: 28.4202 - val_loss: 29.0600 - val_MinusLogProbMetric: 29.0600 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 435/1000
2023-10-25 22:15:19.748 
Epoch 435/1000 
	 loss: 28.3432, MinusLogProbMetric: 28.3432, val_loss: 29.2095, val_MinusLogProbMetric: 29.2095

Epoch 435: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.3432 - MinusLogProbMetric: 28.3432 - val_loss: 29.2095 - val_MinusLogProbMetric: 29.2095 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 436/1000
2023-10-25 22:16:01.920 
Epoch 436/1000 
	 loss: 28.7290, MinusLogProbMetric: 28.7290, val_loss: 29.0876, val_MinusLogProbMetric: 29.0876

Epoch 436: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.7290 - MinusLogProbMetric: 28.7290 - val_loss: 29.0876 - val_MinusLogProbMetric: 29.0876 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 437/1000
2023-10-25 22:16:43.989 
Epoch 437/1000 
	 loss: 28.5821, MinusLogProbMetric: 28.5821, val_loss: 31.5773, val_MinusLogProbMetric: 31.5773

Epoch 437: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.5821 - MinusLogProbMetric: 28.5821 - val_loss: 31.5773 - val_MinusLogProbMetric: 31.5773 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 438/1000
2023-10-25 22:17:26.334 
Epoch 438/1000 
	 loss: 28.3939, MinusLogProbMetric: 28.3939, val_loss: 30.0044, val_MinusLogProbMetric: 30.0044

Epoch 438: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.3939 - MinusLogProbMetric: 28.3939 - val_loss: 30.0044 - val_MinusLogProbMetric: 30.0044 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 439/1000
2023-10-25 22:18:08.645 
Epoch 439/1000 
	 loss: 28.3414, MinusLogProbMetric: 28.3414, val_loss: 29.1168, val_MinusLogProbMetric: 29.1168

Epoch 439: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.3414 - MinusLogProbMetric: 28.3414 - val_loss: 29.1168 - val_MinusLogProbMetric: 29.1168 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 440/1000
2023-10-25 22:18:50.743 
Epoch 440/1000 
	 loss: 28.4946, MinusLogProbMetric: 28.4946, val_loss: 29.2057, val_MinusLogProbMetric: 29.2057

Epoch 440: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.4946 - MinusLogProbMetric: 28.4946 - val_loss: 29.2057 - val_MinusLogProbMetric: 29.2057 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 441/1000
2023-10-25 22:19:32.808 
Epoch 441/1000 
	 loss: 28.3035, MinusLogProbMetric: 28.3035, val_loss: 29.1608, val_MinusLogProbMetric: 29.1608

Epoch 441: val_loss did not improve from 28.89373
196/196 - 42s - loss: 28.3035 - MinusLogProbMetric: 28.3035 - val_loss: 29.1608 - val_MinusLogProbMetric: 29.1608 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 442/1000
2023-10-25 22:20:15.384 
Epoch 442/1000 
	 loss: 28.4761, MinusLogProbMetric: 28.4761, val_loss: 29.2232, val_MinusLogProbMetric: 29.2232

Epoch 442: val_loss did not improve from 28.89373
196/196 - 43s - loss: 28.4761 - MinusLogProbMetric: 28.4761 - val_loss: 29.2232 - val_MinusLogProbMetric: 29.2232 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 443/1000
2023-10-25 22:20:57.704 
Epoch 443/1000 
	 loss: 27.7719, MinusLogProbMetric: 27.7719, val_loss: 28.6440, val_MinusLogProbMetric: 28.6440

Epoch 443: val_loss improved from 28.89373 to 28.64400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.7719 - MinusLogProbMetric: 27.7719 - val_loss: 28.6440 - val_MinusLogProbMetric: 28.6440 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 444/1000
2023-10-25 22:21:40.936 
Epoch 444/1000 
	 loss: 27.7286, MinusLogProbMetric: 27.7286, val_loss: 28.7098, val_MinusLogProbMetric: 28.7098

Epoch 444: val_loss did not improve from 28.64400
196/196 - 43s - loss: 27.7286 - MinusLogProbMetric: 27.7286 - val_loss: 28.7098 - val_MinusLogProbMetric: 28.7098 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 445/1000
2023-10-25 22:22:22.965 
Epoch 445/1000 
	 loss: 27.7278, MinusLogProbMetric: 27.7278, val_loss: 28.5404, val_MinusLogProbMetric: 28.5404

Epoch 445: val_loss improved from 28.64400 to 28.54039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.7278 - MinusLogProbMetric: 27.7278 - val_loss: 28.5404 - val_MinusLogProbMetric: 28.5404 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 446/1000
2023-10-25 22:23:06.036 
Epoch 446/1000 
	 loss: 27.7299, MinusLogProbMetric: 27.7299, val_loss: 28.7208, val_MinusLogProbMetric: 28.7208

Epoch 446: val_loss did not improve from 28.54039
196/196 - 42s - loss: 27.7299 - MinusLogProbMetric: 27.7299 - val_loss: 28.7208 - val_MinusLogProbMetric: 28.7208 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 447/1000
2023-10-25 22:23:48.515 
Epoch 447/1000 
	 loss: 27.7865, MinusLogProbMetric: 27.7865, val_loss: 29.1177, val_MinusLogProbMetric: 29.1177

Epoch 447: val_loss did not improve from 28.54039
196/196 - 42s - loss: 27.7865 - MinusLogProbMetric: 27.7865 - val_loss: 29.1177 - val_MinusLogProbMetric: 29.1177 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 448/1000
2023-10-25 22:24:31.180 
Epoch 448/1000 
	 loss: 27.7505, MinusLogProbMetric: 27.7505, val_loss: 28.7009, val_MinusLogProbMetric: 28.7009

Epoch 448: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7505 - MinusLogProbMetric: 27.7505 - val_loss: 28.7009 - val_MinusLogProbMetric: 28.7009 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 449/1000
2023-10-25 22:25:12.981 
Epoch 449/1000 
	 loss: 27.7164, MinusLogProbMetric: 27.7164, val_loss: 28.7443, val_MinusLogProbMetric: 28.7443

Epoch 449: val_loss did not improve from 28.54039
196/196 - 42s - loss: 27.7164 - MinusLogProbMetric: 27.7164 - val_loss: 28.7443 - val_MinusLogProbMetric: 28.7443 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 450/1000
2023-10-25 22:25:55.467 
Epoch 450/1000 
	 loss: 27.7529, MinusLogProbMetric: 27.7529, val_loss: 28.6137, val_MinusLogProbMetric: 28.6137

Epoch 450: val_loss did not improve from 28.54039
196/196 - 42s - loss: 27.7529 - MinusLogProbMetric: 27.7529 - val_loss: 28.6137 - val_MinusLogProbMetric: 28.6137 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 451/1000
2023-10-25 22:26:38.024 
Epoch 451/1000 
	 loss: 27.7801, MinusLogProbMetric: 27.7801, val_loss: 28.8540, val_MinusLogProbMetric: 28.8540

Epoch 451: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7801 - MinusLogProbMetric: 27.7801 - val_loss: 28.8540 - val_MinusLogProbMetric: 28.8540 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 452/1000
2023-10-25 22:27:20.604 
Epoch 452/1000 
	 loss: 27.7414, MinusLogProbMetric: 27.7414, val_loss: 28.6057, val_MinusLogProbMetric: 28.6057

Epoch 452: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7414 - MinusLogProbMetric: 27.7414 - val_loss: 28.6057 - val_MinusLogProbMetric: 28.6057 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 453/1000
2023-10-25 22:28:03.121 
Epoch 453/1000 
	 loss: 27.7561, MinusLogProbMetric: 27.7561, val_loss: 28.7035, val_MinusLogProbMetric: 28.7035

Epoch 453: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7561 - MinusLogProbMetric: 27.7561 - val_loss: 28.7035 - val_MinusLogProbMetric: 28.7035 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 454/1000
2023-10-25 22:28:45.460 
Epoch 454/1000 
	 loss: 27.7804, MinusLogProbMetric: 27.7804, val_loss: 28.6644, val_MinusLogProbMetric: 28.6644

Epoch 454: val_loss did not improve from 28.54039
196/196 - 42s - loss: 27.7804 - MinusLogProbMetric: 27.7804 - val_loss: 28.6644 - val_MinusLogProbMetric: 28.6644 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 455/1000
2023-10-25 22:29:25.037 
Epoch 455/1000 
	 loss: 27.7468, MinusLogProbMetric: 27.7468, val_loss: 28.6363, val_MinusLogProbMetric: 28.6363

Epoch 455: val_loss did not improve from 28.54039
196/196 - 40s - loss: 27.7468 - MinusLogProbMetric: 27.7468 - val_loss: 28.6363 - val_MinusLogProbMetric: 28.6363 - lr: 1.6667e-04 - 40s/epoch - 202ms/step
Epoch 456/1000
2023-10-25 22:30:04.372 
Epoch 456/1000 
	 loss: 27.8149, MinusLogProbMetric: 27.8149, val_loss: 28.5876, val_MinusLogProbMetric: 28.5876

Epoch 456: val_loss did not improve from 28.54039
196/196 - 39s - loss: 27.8149 - MinusLogProbMetric: 27.8149 - val_loss: 28.5876 - val_MinusLogProbMetric: 28.5876 - lr: 1.6667e-04 - 39s/epoch - 201ms/step
Epoch 457/1000
2023-10-25 22:30:46.962 
Epoch 457/1000 
	 loss: 27.7494, MinusLogProbMetric: 27.7494, val_loss: 28.6197, val_MinusLogProbMetric: 28.6197

Epoch 457: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7494 - MinusLogProbMetric: 27.7494 - val_loss: 28.6197 - val_MinusLogProbMetric: 28.6197 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 458/1000
2023-10-25 22:31:29.703 
Epoch 458/1000 
	 loss: 27.7301, MinusLogProbMetric: 27.7301, val_loss: 28.7803, val_MinusLogProbMetric: 28.7803

Epoch 458: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7301 - MinusLogProbMetric: 27.7301 - val_loss: 28.7803 - val_MinusLogProbMetric: 28.7803 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 459/1000
2023-10-25 22:32:12.601 
Epoch 459/1000 
	 loss: 27.8080, MinusLogProbMetric: 27.8080, val_loss: 28.5664, val_MinusLogProbMetric: 28.5664

Epoch 459: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.8080 - MinusLogProbMetric: 27.8080 - val_loss: 28.5664 - val_MinusLogProbMetric: 28.5664 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 460/1000
2023-10-25 22:32:55.417 
Epoch 460/1000 
	 loss: 27.7352, MinusLogProbMetric: 27.7352, val_loss: 28.8103, val_MinusLogProbMetric: 28.8103

Epoch 460: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7352 - MinusLogProbMetric: 27.7352 - val_loss: 28.8103 - val_MinusLogProbMetric: 28.8103 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 461/1000
2023-10-25 22:33:38.425 
Epoch 461/1000 
	 loss: 27.7901, MinusLogProbMetric: 27.7901, val_loss: 28.7642, val_MinusLogProbMetric: 28.7642

Epoch 461: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7901 - MinusLogProbMetric: 27.7901 - val_loss: 28.7642 - val_MinusLogProbMetric: 28.7642 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 462/1000
2023-10-25 22:34:21.142 
Epoch 462/1000 
	 loss: 27.7739, MinusLogProbMetric: 27.7739, val_loss: 28.5508, val_MinusLogProbMetric: 28.5508

Epoch 462: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7739 - MinusLogProbMetric: 27.7739 - val_loss: 28.5508 - val_MinusLogProbMetric: 28.5508 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 463/1000
2023-10-25 22:35:04.044 
Epoch 463/1000 
	 loss: 27.7651, MinusLogProbMetric: 27.7651, val_loss: 28.7117, val_MinusLogProbMetric: 28.7117

Epoch 463: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7651 - MinusLogProbMetric: 27.7651 - val_loss: 28.7117 - val_MinusLogProbMetric: 28.7117 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 464/1000
2023-10-25 22:35:47.061 
Epoch 464/1000 
	 loss: 27.7658, MinusLogProbMetric: 27.7658, val_loss: 28.5993, val_MinusLogProbMetric: 28.5993

Epoch 464: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7658 - MinusLogProbMetric: 27.7658 - val_loss: 28.5993 - val_MinusLogProbMetric: 28.5993 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 465/1000
2023-10-25 22:36:29.732 
Epoch 465/1000 
	 loss: 27.7117, MinusLogProbMetric: 27.7117, val_loss: 28.6421, val_MinusLogProbMetric: 28.6421

Epoch 465: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7117 - MinusLogProbMetric: 27.7117 - val_loss: 28.6421 - val_MinusLogProbMetric: 28.6421 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 466/1000
2023-10-25 22:37:12.734 
Epoch 466/1000 
	 loss: 27.7551, MinusLogProbMetric: 27.7551, val_loss: 28.7434, val_MinusLogProbMetric: 28.7434

Epoch 466: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7551 - MinusLogProbMetric: 27.7551 - val_loss: 28.7434 - val_MinusLogProbMetric: 28.7434 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 467/1000
2023-10-25 22:37:55.861 
Epoch 467/1000 
	 loss: 27.7443, MinusLogProbMetric: 27.7443, val_loss: 28.8232, val_MinusLogProbMetric: 28.8232

Epoch 467: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7443 - MinusLogProbMetric: 27.7443 - val_loss: 28.8232 - val_MinusLogProbMetric: 28.8232 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 468/1000
2023-10-25 22:38:38.804 
Epoch 468/1000 
	 loss: 27.7842, MinusLogProbMetric: 27.7842, val_loss: 28.6434, val_MinusLogProbMetric: 28.6434

Epoch 468: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7842 - MinusLogProbMetric: 27.7842 - val_loss: 28.6434 - val_MinusLogProbMetric: 28.6434 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 469/1000
2023-10-25 22:39:21.613 
Epoch 469/1000 
	 loss: 27.7547, MinusLogProbMetric: 27.7547, val_loss: 28.7240, val_MinusLogProbMetric: 28.7240

Epoch 469: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7547 - MinusLogProbMetric: 27.7547 - val_loss: 28.7240 - val_MinusLogProbMetric: 28.7240 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 470/1000
2023-10-25 22:40:04.258 
Epoch 470/1000 
	 loss: 27.8205, MinusLogProbMetric: 27.8205, val_loss: 28.6626, val_MinusLogProbMetric: 28.6626

Epoch 470: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.8205 - MinusLogProbMetric: 27.8205 - val_loss: 28.6626 - val_MinusLogProbMetric: 28.6626 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 471/1000
2023-10-25 22:40:46.968 
Epoch 471/1000 
	 loss: 27.7439, MinusLogProbMetric: 27.7439, val_loss: 28.6003, val_MinusLogProbMetric: 28.6003

Epoch 471: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7439 - MinusLogProbMetric: 27.7439 - val_loss: 28.6003 - val_MinusLogProbMetric: 28.6003 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 472/1000
2023-10-25 22:41:29.792 
Epoch 472/1000 
	 loss: 27.7349, MinusLogProbMetric: 27.7349, val_loss: 28.5950, val_MinusLogProbMetric: 28.5950

Epoch 472: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7349 - MinusLogProbMetric: 27.7349 - val_loss: 28.5950 - val_MinusLogProbMetric: 28.5950 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 473/1000
2023-10-25 22:42:12.387 
Epoch 473/1000 
	 loss: 27.7429, MinusLogProbMetric: 27.7429, val_loss: 28.5704, val_MinusLogProbMetric: 28.5704

Epoch 473: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7429 - MinusLogProbMetric: 27.7429 - val_loss: 28.5704 - val_MinusLogProbMetric: 28.5704 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 474/1000
2023-10-25 22:42:55.115 
Epoch 474/1000 
	 loss: 27.7077, MinusLogProbMetric: 27.7077, val_loss: 28.6622, val_MinusLogProbMetric: 28.6622

Epoch 474: val_loss did not improve from 28.54039
196/196 - 43s - loss: 27.7077 - MinusLogProbMetric: 27.7077 - val_loss: 28.6622 - val_MinusLogProbMetric: 28.6622 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 475/1000
2023-10-25 22:43:37.801 
Epoch 475/1000 
	 loss: 27.7177, MinusLogProbMetric: 27.7177, val_loss: 28.5118, val_MinusLogProbMetric: 28.5118

Epoch 475: val_loss improved from 28.54039 to 28.51180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.7177 - MinusLogProbMetric: 27.7177 - val_loss: 28.5118 - val_MinusLogProbMetric: 28.5118 - lr: 1.6667e-04 - 43s/epoch - 221ms/step
Epoch 476/1000
2023-10-25 22:44:21.110 
Epoch 476/1000 
	 loss: 27.7351, MinusLogProbMetric: 27.7351, val_loss: 28.6898, val_MinusLogProbMetric: 28.6898

Epoch 476: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7351 - MinusLogProbMetric: 27.7351 - val_loss: 28.6898 - val_MinusLogProbMetric: 28.6898 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 477/1000
2023-10-25 22:45:04.138 
Epoch 477/1000 
	 loss: 27.7263, MinusLogProbMetric: 27.7263, val_loss: 28.5785, val_MinusLogProbMetric: 28.5785

Epoch 477: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7263 - MinusLogProbMetric: 27.7263 - val_loss: 28.5785 - val_MinusLogProbMetric: 28.5785 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 478/1000
2023-10-25 22:45:46.919 
Epoch 478/1000 
	 loss: 27.7311, MinusLogProbMetric: 27.7311, val_loss: 28.8357, val_MinusLogProbMetric: 28.8357

Epoch 478: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7311 - MinusLogProbMetric: 27.7311 - val_loss: 28.8357 - val_MinusLogProbMetric: 28.8357 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 479/1000
2023-10-25 22:46:29.588 
Epoch 479/1000 
	 loss: 27.7545, MinusLogProbMetric: 27.7545, val_loss: 28.6138, val_MinusLogProbMetric: 28.6138

Epoch 479: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7545 - MinusLogProbMetric: 27.7545 - val_loss: 28.6138 - val_MinusLogProbMetric: 28.6138 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 480/1000
2023-10-25 22:47:12.032 
Epoch 480/1000 
	 loss: 27.7536, MinusLogProbMetric: 27.7536, val_loss: 28.7219, val_MinusLogProbMetric: 28.7219

Epoch 480: val_loss did not improve from 28.51180
196/196 - 42s - loss: 27.7536 - MinusLogProbMetric: 27.7536 - val_loss: 28.7219 - val_MinusLogProbMetric: 28.7219 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 481/1000
2023-10-25 22:47:54.930 
Epoch 481/1000 
	 loss: 27.6709, MinusLogProbMetric: 27.6709, val_loss: 28.5620, val_MinusLogProbMetric: 28.5620

Epoch 481: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6709 - MinusLogProbMetric: 27.6709 - val_loss: 28.5620 - val_MinusLogProbMetric: 28.5620 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 482/1000
2023-10-25 22:48:37.545 
Epoch 482/1000 
	 loss: 27.6877, MinusLogProbMetric: 27.6877, val_loss: 28.5122, val_MinusLogProbMetric: 28.5122

Epoch 482: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6877 - MinusLogProbMetric: 27.6877 - val_loss: 28.5122 - val_MinusLogProbMetric: 28.5122 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 483/1000
2023-10-25 22:49:20.435 
Epoch 483/1000 
	 loss: 27.7139, MinusLogProbMetric: 27.7139, val_loss: 28.6921, val_MinusLogProbMetric: 28.6921

Epoch 483: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7139 - MinusLogProbMetric: 27.7139 - val_loss: 28.6921 - val_MinusLogProbMetric: 28.6921 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 484/1000
2023-10-25 22:50:03.411 
Epoch 484/1000 
	 loss: 27.8272, MinusLogProbMetric: 27.8272, val_loss: 28.6430, val_MinusLogProbMetric: 28.6430

Epoch 484: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.8272 - MinusLogProbMetric: 27.8272 - val_loss: 28.6430 - val_MinusLogProbMetric: 28.6430 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 485/1000
2023-10-25 22:50:46.008 
Epoch 485/1000 
	 loss: 27.7131, MinusLogProbMetric: 27.7131, val_loss: 28.6880, val_MinusLogProbMetric: 28.6880

Epoch 485: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7131 - MinusLogProbMetric: 27.7131 - val_loss: 28.6880 - val_MinusLogProbMetric: 28.6880 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 486/1000
2023-10-25 22:51:28.029 
Epoch 486/1000 
	 loss: 27.7257, MinusLogProbMetric: 27.7257, val_loss: 28.5335, val_MinusLogProbMetric: 28.5335

Epoch 486: val_loss did not improve from 28.51180
196/196 - 42s - loss: 27.7257 - MinusLogProbMetric: 27.7257 - val_loss: 28.5335 - val_MinusLogProbMetric: 28.5335 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 487/1000
2023-10-25 22:52:10.278 
Epoch 487/1000 
	 loss: 27.7833, MinusLogProbMetric: 27.7833, val_loss: 28.7336, val_MinusLogProbMetric: 28.7336

Epoch 487: val_loss did not improve from 28.51180
196/196 - 42s - loss: 27.7833 - MinusLogProbMetric: 27.7833 - val_loss: 28.7336 - val_MinusLogProbMetric: 28.7336 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 488/1000
2023-10-25 22:52:53.392 
Epoch 488/1000 
	 loss: 27.7087, MinusLogProbMetric: 27.7087, val_loss: 28.8879, val_MinusLogProbMetric: 28.8879

Epoch 488: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7087 - MinusLogProbMetric: 27.7087 - val_loss: 28.8879 - val_MinusLogProbMetric: 28.8879 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 489/1000
2023-10-25 22:53:35.934 
Epoch 489/1000 
	 loss: 27.7253, MinusLogProbMetric: 27.7253, val_loss: 28.7922, val_MinusLogProbMetric: 28.7922

Epoch 489: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7253 - MinusLogProbMetric: 27.7253 - val_loss: 28.7922 - val_MinusLogProbMetric: 28.7922 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 490/1000
2023-10-25 22:54:18.789 
Epoch 490/1000 
	 loss: 27.7537, MinusLogProbMetric: 27.7537, val_loss: 28.6579, val_MinusLogProbMetric: 28.6579

Epoch 490: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7537 - MinusLogProbMetric: 27.7537 - val_loss: 28.6579 - val_MinusLogProbMetric: 28.6579 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 491/1000
2023-10-25 22:55:01.854 
Epoch 491/1000 
	 loss: 27.7255, MinusLogProbMetric: 27.7255, val_loss: 28.8283, val_MinusLogProbMetric: 28.8283

Epoch 491: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7255 - MinusLogProbMetric: 27.7255 - val_loss: 28.8283 - val_MinusLogProbMetric: 28.8283 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 492/1000
2023-10-25 22:55:44.632 
Epoch 492/1000 
	 loss: 27.7347, MinusLogProbMetric: 27.7347, val_loss: 28.6254, val_MinusLogProbMetric: 28.6254

Epoch 492: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7347 - MinusLogProbMetric: 27.7347 - val_loss: 28.6254 - val_MinusLogProbMetric: 28.6254 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 493/1000
2023-10-25 22:56:27.354 
Epoch 493/1000 
	 loss: 27.7380, MinusLogProbMetric: 27.7380, val_loss: 28.5750, val_MinusLogProbMetric: 28.5750

Epoch 493: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7380 - MinusLogProbMetric: 27.7380 - val_loss: 28.5750 - val_MinusLogProbMetric: 28.5750 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 494/1000
2023-10-25 22:57:10.224 
Epoch 494/1000 
	 loss: 27.7520, MinusLogProbMetric: 27.7520, val_loss: 28.9867, val_MinusLogProbMetric: 28.9867

Epoch 494: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7520 - MinusLogProbMetric: 27.7520 - val_loss: 28.9867 - val_MinusLogProbMetric: 28.9867 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 495/1000
2023-10-25 22:57:53.127 
Epoch 495/1000 
	 loss: 27.7272, MinusLogProbMetric: 27.7272, val_loss: 29.0797, val_MinusLogProbMetric: 29.0797

Epoch 495: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7272 - MinusLogProbMetric: 27.7272 - val_loss: 29.0797 - val_MinusLogProbMetric: 29.0797 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 496/1000
2023-10-25 22:58:36.346 
Epoch 496/1000 
	 loss: 27.7254, MinusLogProbMetric: 27.7254, val_loss: 28.6671, val_MinusLogProbMetric: 28.6671

Epoch 496: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7254 - MinusLogProbMetric: 27.7254 - val_loss: 28.6671 - val_MinusLogProbMetric: 28.6671 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 497/1000
2023-10-25 22:59:19.381 
Epoch 497/1000 
	 loss: 27.7484, MinusLogProbMetric: 27.7484, val_loss: 28.5465, val_MinusLogProbMetric: 28.5465

Epoch 497: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7484 - MinusLogProbMetric: 27.7484 - val_loss: 28.5465 - val_MinusLogProbMetric: 28.5465 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 498/1000
2023-10-25 23:00:02.409 
Epoch 498/1000 
	 loss: 27.7100, MinusLogProbMetric: 27.7100, val_loss: 28.8302, val_MinusLogProbMetric: 28.8302

Epoch 498: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7100 - MinusLogProbMetric: 27.7100 - val_loss: 28.8302 - val_MinusLogProbMetric: 28.8302 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 499/1000
2023-10-25 23:00:45.644 
Epoch 499/1000 
	 loss: 27.6933, MinusLogProbMetric: 27.6933, val_loss: 28.8898, val_MinusLogProbMetric: 28.8898

Epoch 499: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6933 - MinusLogProbMetric: 27.6933 - val_loss: 28.8898 - val_MinusLogProbMetric: 28.8898 - lr: 1.6667e-04 - 43s/epoch - 221ms/step
Epoch 500/1000
2023-10-25 23:01:28.454 
Epoch 500/1000 
	 loss: 27.6888, MinusLogProbMetric: 27.6888, val_loss: 28.6051, val_MinusLogProbMetric: 28.6051

Epoch 500: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6888 - MinusLogProbMetric: 27.6888 - val_loss: 28.6051 - val_MinusLogProbMetric: 28.6051 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 501/1000
2023-10-25 23:02:11.152 
Epoch 501/1000 
	 loss: 27.7481, MinusLogProbMetric: 27.7481, val_loss: 29.5168, val_MinusLogProbMetric: 29.5168

Epoch 501: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7481 - MinusLogProbMetric: 27.7481 - val_loss: 29.5168 - val_MinusLogProbMetric: 29.5168 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 502/1000
2023-10-25 23:02:53.644 
Epoch 502/1000 
	 loss: 27.7222, MinusLogProbMetric: 27.7222, val_loss: 28.5805, val_MinusLogProbMetric: 28.5805

Epoch 502: val_loss did not improve from 28.51180
196/196 - 42s - loss: 27.7222 - MinusLogProbMetric: 27.7222 - val_loss: 28.5805 - val_MinusLogProbMetric: 28.5805 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 503/1000
2023-10-25 23:03:36.709 
Epoch 503/1000 
	 loss: 27.7077, MinusLogProbMetric: 27.7077, val_loss: 28.5241, val_MinusLogProbMetric: 28.5241

Epoch 503: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7077 - MinusLogProbMetric: 27.7077 - val_loss: 28.5241 - val_MinusLogProbMetric: 28.5241 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 504/1000
2023-10-25 23:04:19.838 
Epoch 504/1000 
	 loss: 27.6956, MinusLogProbMetric: 27.6956, val_loss: 28.7623, val_MinusLogProbMetric: 28.7623

Epoch 504: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6956 - MinusLogProbMetric: 27.6956 - val_loss: 28.7623 - val_MinusLogProbMetric: 28.7623 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 505/1000
2023-10-25 23:05:02.960 
Epoch 505/1000 
	 loss: 27.6897, MinusLogProbMetric: 27.6897, val_loss: 28.7572, val_MinusLogProbMetric: 28.7572

Epoch 505: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6897 - MinusLogProbMetric: 27.6897 - val_loss: 28.7572 - val_MinusLogProbMetric: 28.7572 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 506/1000
2023-10-25 23:05:45.649 
Epoch 506/1000 
	 loss: 27.6951, MinusLogProbMetric: 27.6951, val_loss: 28.7492, val_MinusLogProbMetric: 28.7492

Epoch 506: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6951 - MinusLogProbMetric: 27.6951 - val_loss: 28.7492 - val_MinusLogProbMetric: 28.7492 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 507/1000
2023-10-25 23:06:28.411 
Epoch 507/1000 
	 loss: 27.6972, MinusLogProbMetric: 27.6972, val_loss: 28.6476, val_MinusLogProbMetric: 28.6476

Epoch 507: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6972 - MinusLogProbMetric: 27.6972 - val_loss: 28.6476 - val_MinusLogProbMetric: 28.6476 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 508/1000
2023-10-25 23:07:11.491 
Epoch 508/1000 
	 loss: 27.7628, MinusLogProbMetric: 27.7628, val_loss: 28.6058, val_MinusLogProbMetric: 28.6058

Epoch 508: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7628 - MinusLogProbMetric: 27.7628 - val_loss: 28.6058 - val_MinusLogProbMetric: 28.6058 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 509/1000
2023-10-25 23:07:54.304 
Epoch 509/1000 
	 loss: 27.7482, MinusLogProbMetric: 27.7482, val_loss: 28.6843, val_MinusLogProbMetric: 28.6843

Epoch 509: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7482 - MinusLogProbMetric: 27.7482 - val_loss: 28.6843 - val_MinusLogProbMetric: 28.6843 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 510/1000
2023-10-25 23:08:37.231 
Epoch 510/1000 
	 loss: 27.7209, MinusLogProbMetric: 27.7209, val_loss: 28.8802, val_MinusLogProbMetric: 28.8802

Epoch 510: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7209 - MinusLogProbMetric: 27.7209 - val_loss: 28.8802 - val_MinusLogProbMetric: 28.8802 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 511/1000
2023-10-25 23:09:19.880 
Epoch 511/1000 
	 loss: 27.6876, MinusLogProbMetric: 27.6876, val_loss: 28.5748, val_MinusLogProbMetric: 28.5748

Epoch 511: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6876 - MinusLogProbMetric: 27.6876 - val_loss: 28.5748 - val_MinusLogProbMetric: 28.5748 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 512/1000
2023-10-25 23:10:02.553 
Epoch 512/1000 
	 loss: 27.7909, MinusLogProbMetric: 27.7909, val_loss: 28.5642, val_MinusLogProbMetric: 28.5642

Epoch 512: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7909 - MinusLogProbMetric: 27.7909 - val_loss: 28.5642 - val_MinusLogProbMetric: 28.5642 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 513/1000
2023-10-25 23:10:45.005 
Epoch 513/1000 
	 loss: 27.6702, MinusLogProbMetric: 27.6702, val_loss: 28.7940, val_MinusLogProbMetric: 28.7940

Epoch 513: val_loss did not improve from 28.51180
196/196 - 42s - loss: 27.6702 - MinusLogProbMetric: 27.6702 - val_loss: 28.7940 - val_MinusLogProbMetric: 28.7940 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 514/1000
2023-10-25 23:11:27.591 
Epoch 514/1000 
	 loss: 27.7254, MinusLogProbMetric: 27.7254, val_loss: 28.6818, val_MinusLogProbMetric: 28.6818

Epoch 514: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7254 - MinusLogProbMetric: 27.7254 - val_loss: 28.6818 - val_MinusLogProbMetric: 28.6818 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 515/1000
2023-10-25 23:12:10.514 
Epoch 515/1000 
	 loss: 27.7134, MinusLogProbMetric: 27.7134, val_loss: 28.8206, val_MinusLogProbMetric: 28.8206

Epoch 515: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7134 - MinusLogProbMetric: 27.7134 - val_loss: 28.8206 - val_MinusLogProbMetric: 28.8206 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 516/1000
2023-10-25 23:12:53.415 
Epoch 516/1000 
	 loss: 27.7287, MinusLogProbMetric: 27.7287, val_loss: 28.7074, val_MinusLogProbMetric: 28.7074

Epoch 516: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7287 - MinusLogProbMetric: 27.7287 - val_loss: 28.7074 - val_MinusLogProbMetric: 28.7074 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 517/1000
2023-10-25 23:13:35.934 
Epoch 517/1000 
	 loss: 27.7082, MinusLogProbMetric: 27.7082, val_loss: 28.6198, val_MinusLogProbMetric: 28.6198

Epoch 517: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7082 - MinusLogProbMetric: 27.7082 - val_loss: 28.6198 - val_MinusLogProbMetric: 28.6198 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 518/1000
2023-10-25 23:14:19.095 
Epoch 518/1000 
	 loss: 27.6628, MinusLogProbMetric: 27.6628, val_loss: 28.6958, val_MinusLogProbMetric: 28.6958

Epoch 518: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6628 - MinusLogProbMetric: 27.6628 - val_loss: 28.6958 - val_MinusLogProbMetric: 28.6958 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 519/1000
2023-10-25 23:15:02.045 
Epoch 519/1000 
	 loss: 27.6912, MinusLogProbMetric: 27.6912, val_loss: 28.7955, val_MinusLogProbMetric: 28.7955

Epoch 519: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6912 - MinusLogProbMetric: 27.6912 - val_loss: 28.7955 - val_MinusLogProbMetric: 28.7955 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 520/1000
2023-10-25 23:15:44.841 
Epoch 520/1000 
	 loss: 27.6739, MinusLogProbMetric: 27.6739, val_loss: 28.9239, val_MinusLogProbMetric: 28.9239

Epoch 520: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6739 - MinusLogProbMetric: 27.6739 - val_loss: 28.9239 - val_MinusLogProbMetric: 28.9239 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 521/1000
2023-10-25 23:16:27.524 
Epoch 521/1000 
	 loss: 27.7339, MinusLogProbMetric: 27.7339, val_loss: 28.6360, val_MinusLogProbMetric: 28.6360

Epoch 521: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7339 - MinusLogProbMetric: 27.7339 - val_loss: 28.6360 - val_MinusLogProbMetric: 28.6360 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 522/1000
2023-10-25 23:17:10.002 
Epoch 522/1000 
	 loss: 27.6867, MinusLogProbMetric: 27.6867, val_loss: 28.7759, val_MinusLogProbMetric: 28.7759

Epoch 522: val_loss did not improve from 28.51180
196/196 - 42s - loss: 27.6867 - MinusLogProbMetric: 27.6867 - val_loss: 28.7759 - val_MinusLogProbMetric: 28.7759 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 523/1000
2023-10-25 23:17:52.674 
Epoch 523/1000 
	 loss: 27.6957, MinusLogProbMetric: 27.6957, val_loss: 28.6495, val_MinusLogProbMetric: 28.6495

Epoch 523: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6957 - MinusLogProbMetric: 27.6957 - val_loss: 28.6495 - val_MinusLogProbMetric: 28.6495 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 524/1000
2023-10-25 23:18:35.350 
Epoch 524/1000 
	 loss: 27.6871, MinusLogProbMetric: 27.6871, val_loss: 28.7171, val_MinusLogProbMetric: 28.7171

Epoch 524: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.6871 - MinusLogProbMetric: 27.6871 - val_loss: 28.7171 - val_MinusLogProbMetric: 28.7171 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 525/1000
2023-10-25 23:19:18.086 
Epoch 525/1000 
	 loss: 27.7228, MinusLogProbMetric: 27.7228, val_loss: 28.6762, val_MinusLogProbMetric: 28.6762

Epoch 525: val_loss did not improve from 28.51180
196/196 - 43s - loss: 27.7228 - MinusLogProbMetric: 27.7228 - val_loss: 28.6762 - val_MinusLogProbMetric: 28.6762 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 526/1000
2023-10-25 23:20:00.678 
Epoch 526/1000 
	 loss: 27.4556, MinusLogProbMetric: 27.4556, val_loss: 28.4542, val_MinusLogProbMetric: 28.4542

Epoch 526: val_loss improved from 28.51180 to 28.45422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.4556 - MinusLogProbMetric: 27.4556 - val_loss: 28.4542 - val_MinusLogProbMetric: 28.4542 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 527/1000
2023-10-25 23:20:43.938 
Epoch 527/1000 
	 loss: 27.4565, MinusLogProbMetric: 27.4565, val_loss: 28.4675, val_MinusLogProbMetric: 28.4675

Epoch 527: val_loss did not improve from 28.45422
196/196 - 43s - loss: 27.4565 - MinusLogProbMetric: 27.4565 - val_loss: 28.4675 - val_MinusLogProbMetric: 28.4675 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 528/1000
2023-10-25 23:21:26.564 
Epoch 528/1000 
	 loss: 27.4507, MinusLogProbMetric: 27.4507, val_loss: 28.5622, val_MinusLogProbMetric: 28.5622

Epoch 528: val_loss did not improve from 28.45422
196/196 - 43s - loss: 27.4507 - MinusLogProbMetric: 27.4507 - val_loss: 28.5622 - val_MinusLogProbMetric: 28.5622 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 529/1000
2023-10-25 23:22:08.860 
Epoch 529/1000 
	 loss: 27.4643, MinusLogProbMetric: 27.4643, val_loss: 28.4942, val_MinusLogProbMetric: 28.4942

Epoch 529: val_loss did not improve from 28.45422
196/196 - 42s - loss: 27.4643 - MinusLogProbMetric: 27.4643 - val_loss: 28.4942 - val_MinusLogProbMetric: 28.4942 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 530/1000
2023-10-25 23:22:51.539 
Epoch 530/1000 
	 loss: 27.4485, MinusLogProbMetric: 27.4485, val_loss: 28.5886, val_MinusLogProbMetric: 28.5886

Epoch 530: val_loss did not improve from 28.45422
196/196 - 43s - loss: 27.4485 - MinusLogProbMetric: 27.4485 - val_loss: 28.5886 - val_MinusLogProbMetric: 28.5886 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 531/1000
2023-10-25 23:23:33.875 
Epoch 531/1000 
	 loss: 27.4691, MinusLogProbMetric: 27.4691, val_loss: 28.5979, val_MinusLogProbMetric: 28.5979

Epoch 531: val_loss did not improve from 28.45422
196/196 - 42s - loss: 27.4691 - MinusLogProbMetric: 27.4691 - val_loss: 28.5979 - val_MinusLogProbMetric: 28.5979 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 532/1000
2023-10-25 23:24:16.457 
Epoch 532/1000 
	 loss: 27.4776, MinusLogProbMetric: 27.4776, val_loss: 28.5451, val_MinusLogProbMetric: 28.5451

Epoch 532: val_loss did not improve from 28.45422
196/196 - 43s - loss: 27.4776 - MinusLogProbMetric: 27.4776 - val_loss: 28.5451 - val_MinusLogProbMetric: 28.5451 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 533/1000
2023-10-25 23:24:59.099 
Epoch 533/1000 
	 loss: 27.4444, MinusLogProbMetric: 27.4444, val_loss: 28.5506, val_MinusLogProbMetric: 28.5506

Epoch 533: val_loss did not improve from 28.45422
196/196 - 43s - loss: 27.4444 - MinusLogProbMetric: 27.4444 - val_loss: 28.5506 - val_MinusLogProbMetric: 28.5506 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 534/1000
2023-10-25 23:25:41.956 
Epoch 534/1000 
	 loss: 27.4431, MinusLogProbMetric: 27.4431, val_loss: 28.4624, val_MinusLogProbMetric: 28.4624

Epoch 534: val_loss did not improve from 28.45422
196/196 - 43s - loss: 27.4431 - MinusLogProbMetric: 27.4431 - val_loss: 28.4624 - val_MinusLogProbMetric: 28.4624 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 535/1000
2023-10-25 23:26:24.577 
Epoch 535/1000 
	 loss: 27.4618, MinusLogProbMetric: 27.4618, val_loss: 28.4486, val_MinusLogProbMetric: 28.4486

Epoch 535: val_loss improved from 28.45422 to 28.44858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.4618 - MinusLogProbMetric: 27.4618 - val_loss: 28.4486 - val_MinusLogProbMetric: 28.4486 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 536/1000
2023-10-25 23:27:08.433 
Epoch 536/1000 
	 loss: 27.4384, MinusLogProbMetric: 27.4384, val_loss: 28.5128, val_MinusLogProbMetric: 28.5128

Epoch 536: val_loss did not improve from 28.44858
196/196 - 43s - loss: 27.4384 - MinusLogProbMetric: 27.4384 - val_loss: 28.5128 - val_MinusLogProbMetric: 28.5128 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 537/1000
2023-10-25 23:27:51.352 
Epoch 537/1000 
	 loss: 27.4630, MinusLogProbMetric: 27.4630, val_loss: 28.4726, val_MinusLogProbMetric: 28.4726

Epoch 537: val_loss did not improve from 28.44858
196/196 - 43s - loss: 27.4630 - MinusLogProbMetric: 27.4630 - val_loss: 28.4726 - val_MinusLogProbMetric: 28.4726 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 538/1000
2023-10-25 23:28:34.027 
Epoch 538/1000 
	 loss: 27.4519, MinusLogProbMetric: 27.4519, val_loss: 28.4305, val_MinusLogProbMetric: 28.4305

Epoch 538: val_loss improved from 28.44858 to 28.43052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.4519 - MinusLogProbMetric: 27.4519 - val_loss: 28.4305 - val_MinusLogProbMetric: 28.4305 - lr: 8.3333e-05 - 44s/epoch - 222ms/step
Epoch 539/1000
2023-10-25 23:29:17.092 
Epoch 539/1000 
	 loss: 27.4458, MinusLogProbMetric: 27.4458, val_loss: 28.4414, val_MinusLogProbMetric: 28.4414

Epoch 539: val_loss did not improve from 28.43052
196/196 - 42s - loss: 27.4458 - MinusLogProbMetric: 27.4458 - val_loss: 28.4414 - val_MinusLogProbMetric: 28.4414 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 540/1000
2023-10-25 23:29:59.967 
Epoch 540/1000 
	 loss: 27.4324, MinusLogProbMetric: 27.4324, val_loss: 28.5842, val_MinusLogProbMetric: 28.5842

Epoch 540: val_loss did not improve from 28.43052
196/196 - 43s - loss: 27.4324 - MinusLogProbMetric: 27.4324 - val_loss: 28.5842 - val_MinusLogProbMetric: 28.5842 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 541/1000
2023-10-25 23:30:42.592 
Epoch 541/1000 
	 loss: 27.4653, MinusLogProbMetric: 27.4653, val_loss: 28.4307, val_MinusLogProbMetric: 28.4307

Epoch 541: val_loss did not improve from 28.43052
196/196 - 43s - loss: 27.4653 - MinusLogProbMetric: 27.4653 - val_loss: 28.4307 - val_MinusLogProbMetric: 28.4307 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 542/1000
2023-10-25 23:31:24.999 
Epoch 542/1000 
	 loss: 27.4447, MinusLogProbMetric: 27.4447, val_loss: 28.4460, val_MinusLogProbMetric: 28.4460

Epoch 542: val_loss did not improve from 28.43052
196/196 - 42s - loss: 27.4447 - MinusLogProbMetric: 27.4447 - val_loss: 28.4460 - val_MinusLogProbMetric: 28.4460 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 543/1000
2023-10-25 23:32:07.916 
Epoch 543/1000 
	 loss: 27.4474, MinusLogProbMetric: 27.4474, val_loss: 28.4464, val_MinusLogProbMetric: 28.4464

Epoch 543: val_loss did not improve from 28.43052
196/196 - 43s - loss: 27.4474 - MinusLogProbMetric: 27.4474 - val_loss: 28.4464 - val_MinusLogProbMetric: 28.4464 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 544/1000
2023-10-25 23:32:50.900 
Epoch 544/1000 
	 loss: 27.4449, MinusLogProbMetric: 27.4449, val_loss: 28.5301, val_MinusLogProbMetric: 28.5301

Epoch 544: val_loss did not improve from 28.43052
196/196 - 43s - loss: 27.4449 - MinusLogProbMetric: 27.4449 - val_loss: 28.5301 - val_MinusLogProbMetric: 28.5301 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 545/1000
2023-10-25 23:33:33.765 
Epoch 545/1000 
	 loss: 27.4361, MinusLogProbMetric: 27.4361, val_loss: 28.4503, val_MinusLogProbMetric: 28.4503

Epoch 545: val_loss did not improve from 28.43052
196/196 - 43s - loss: 27.4361 - MinusLogProbMetric: 27.4361 - val_loss: 28.4503 - val_MinusLogProbMetric: 28.4503 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 546/1000
2023-10-25 23:34:16.439 
Epoch 546/1000 
	 loss: 27.4522, MinusLogProbMetric: 27.4522, val_loss: 28.4511, val_MinusLogProbMetric: 28.4511

Epoch 546: val_loss did not improve from 28.43052
196/196 - 43s - loss: 27.4522 - MinusLogProbMetric: 27.4522 - val_loss: 28.4511 - val_MinusLogProbMetric: 28.4511 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 547/1000
2023-10-25 23:34:59.283 
Epoch 547/1000 
	 loss: 27.4485, MinusLogProbMetric: 27.4485, val_loss: 28.4181, val_MinusLogProbMetric: 28.4181

Epoch 547: val_loss improved from 28.43052 to 28.41811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.4485 - MinusLogProbMetric: 27.4485 - val_loss: 28.4181 - val_MinusLogProbMetric: 28.4181 - lr: 8.3333e-05 - 44s/epoch - 222ms/step
Epoch 548/1000
2023-10-25 23:35:42.733 
Epoch 548/1000 
	 loss: 27.4428, MinusLogProbMetric: 27.4428, val_loss: 28.4682, val_MinusLogProbMetric: 28.4682

Epoch 548: val_loss did not improve from 28.41811
196/196 - 43s - loss: 27.4428 - MinusLogProbMetric: 27.4428 - val_loss: 28.4682 - val_MinusLogProbMetric: 28.4682 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 549/1000
2023-10-25 23:36:25.446 
Epoch 549/1000 
	 loss: 27.4407, MinusLogProbMetric: 27.4407, val_loss: 28.5796, val_MinusLogProbMetric: 28.5796

Epoch 549: val_loss did not improve from 28.41811
196/196 - 43s - loss: 27.4407 - MinusLogProbMetric: 27.4407 - val_loss: 28.5796 - val_MinusLogProbMetric: 28.5796 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 550/1000
2023-10-25 23:37:08.188 
Epoch 550/1000 
	 loss: 27.4361, MinusLogProbMetric: 27.4361, val_loss: 28.4346, val_MinusLogProbMetric: 28.4346

Epoch 550: val_loss did not improve from 28.41811
196/196 - 43s - loss: 27.4361 - MinusLogProbMetric: 27.4361 - val_loss: 28.4346 - val_MinusLogProbMetric: 28.4346 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 551/1000
2023-10-25 23:37:51.080 
Epoch 551/1000 
	 loss: 27.4401, MinusLogProbMetric: 27.4401, val_loss: 28.5468, val_MinusLogProbMetric: 28.5468

Epoch 551: val_loss did not improve from 28.41811
196/196 - 43s - loss: 27.4401 - MinusLogProbMetric: 27.4401 - val_loss: 28.5468 - val_MinusLogProbMetric: 28.5468 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 552/1000
2023-10-25 23:38:33.700 
Epoch 552/1000 
	 loss: 27.4657, MinusLogProbMetric: 27.4657, val_loss: 28.6432, val_MinusLogProbMetric: 28.6432

Epoch 552: val_loss did not improve from 28.41811
196/196 - 43s - loss: 27.4657 - MinusLogProbMetric: 27.4657 - val_loss: 28.6432 - val_MinusLogProbMetric: 28.6432 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 553/1000
2023-10-25 23:39:16.532 
Epoch 553/1000 
	 loss: 27.4568, MinusLogProbMetric: 27.4568, val_loss: 28.3958, val_MinusLogProbMetric: 28.3958

Epoch 553: val_loss improved from 28.41811 to 28.39582, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.4568 - MinusLogProbMetric: 27.4568 - val_loss: 28.3958 - val_MinusLogProbMetric: 28.3958 - lr: 8.3333e-05 - 44s/epoch - 223ms/step
Epoch 554/1000
2023-10-25 23:40:00.158 
Epoch 554/1000 
	 loss: 27.4437, MinusLogProbMetric: 27.4437, val_loss: 28.4510, val_MinusLogProbMetric: 28.4510

Epoch 554: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4437 - MinusLogProbMetric: 27.4437 - val_loss: 28.4510 - val_MinusLogProbMetric: 28.4510 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 555/1000
2023-10-25 23:40:43.059 
Epoch 555/1000 
	 loss: 27.4475, MinusLogProbMetric: 27.4475, val_loss: 28.4708, val_MinusLogProbMetric: 28.4708

Epoch 555: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4475 - MinusLogProbMetric: 27.4475 - val_loss: 28.4708 - val_MinusLogProbMetric: 28.4708 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 556/1000
2023-10-25 23:41:25.758 
Epoch 556/1000 
	 loss: 27.4444, MinusLogProbMetric: 27.4444, val_loss: 28.4475, val_MinusLogProbMetric: 28.4475

Epoch 556: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4444 - MinusLogProbMetric: 27.4444 - val_loss: 28.4475 - val_MinusLogProbMetric: 28.4475 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 557/1000
2023-10-25 23:42:08.410 
Epoch 557/1000 
	 loss: 27.4458, MinusLogProbMetric: 27.4458, val_loss: 28.4274, val_MinusLogProbMetric: 28.4274

Epoch 557: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4458 - MinusLogProbMetric: 27.4458 - val_loss: 28.4274 - val_MinusLogProbMetric: 28.4274 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 558/1000
2023-10-25 23:42:50.878 
Epoch 558/1000 
	 loss: 27.4343, MinusLogProbMetric: 27.4343, val_loss: 28.4576, val_MinusLogProbMetric: 28.4576

Epoch 558: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4343 - MinusLogProbMetric: 27.4343 - val_loss: 28.4576 - val_MinusLogProbMetric: 28.4576 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 559/1000
2023-10-25 23:43:33.316 
Epoch 559/1000 
	 loss: 27.4389, MinusLogProbMetric: 27.4389, val_loss: 28.6469, val_MinusLogProbMetric: 28.6469

Epoch 559: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4389 - MinusLogProbMetric: 27.4389 - val_loss: 28.6469 - val_MinusLogProbMetric: 28.6469 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 560/1000
2023-10-25 23:44:15.991 
Epoch 560/1000 
	 loss: 27.4524, MinusLogProbMetric: 27.4524, val_loss: 28.4510, val_MinusLogProbMetric: 28.4510

Epoch 560: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4524 - MinusLogProbMetric: 27.4524 - val_loss: 28.4510 - val_MinusLogProbMetric: 28.4510 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 561/1000
2023-10-25 23:44:58.920 
Epoch 561/1000 
	 loss: 27.4258, MinusLogProbMetric: 27.4258, val_loss: 28.4194, val_MinusLogProbMetric: 28.4194

Epoch 561: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4258 - MinusLogProbMetric: 27.4258 - val_loss: 28.4194 - val_MinusLogProbMetric: 28.4194 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 562/1000
2023-10-25 23:45:41.899 
Epoch 562/1000 
	 loss: 27.4531, MinusLogProbMetric: 27.4531, val_loss: 28.4589, val_MinusLogProbMetric: 28.4589

Epoch 562: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4531 - MinusLogProbMetric: 27.4531 - val_loss: 28.4589 - val_MinusLogProbMetric: 28.4589 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 563/1000
2023-10-25 23:46:24.230 
Epoch 563/1000 
	 loss: 27.4337, MinusLogProbMetric: 27.4337, val_loss: 28.4766, val_MinusLogProbMetric: 28.4766

Epoch 563: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4337 - MinusLogProbMetric: 27.4337 - val_loss: 28.4766 - val_MinusLogProbMetric: 28.4766 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 564/1000
2023-10-25 23:47:06.715 
Epoch 564/1000 
	 loss: 27.4337, MinusLogProbMetric: 27.4337, val_loss: 28.4929, val_MinusLogProbMetric: 28.4929

Epoch 564: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4337 - MinusLogProbMetric: 27.4337 - val_loss: 28.4929 - val_MinusLogProbMetric: 28.4929 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 565/1000
2023-10-25 23:47:48.895 
Epoch 565/1000 
	 loss: 27.4486, MinusLogProbMetric: 27.4486, val_loss: 28.4833, val_MinusLogProbMetric: 28.4833

Epoch 565: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4486 - MinusLogProbMetric: 27.4486 - val_loss: 28.4833 - val_MinusLogProbMetric: 28.4833 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 566/1000
2023-10-25 23:48:31.691 
Epoch 566/1000 
	 loss: 27.4265, MinusLogProbMetric: 27.4265, val_loss: 28.5247, val_MinusLogProbMetric: 28.5247

Epoch 566: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4265 - MinusLogProbMetric: 27.4265 - val_loss: 28.5247 - val_MinusLogProbMetric: 28.5247 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 567/1000
2023-10-25 23:49:14.236 
Epoch 567/1000 
	 loss: 27.4397, MinusLogProbMetric: 27.4397, val_loss: 28.4709, val_MinusLogProbMetric: 28.4709

Epoch 567: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4397 - MinusLogProbMetric: 27.4397 - val_loss: 28.4709 - val_MinusLogProbMetric: 28.4709 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 568/1000
2023-10-25 23:49:56.720 
Epoch 568/1000 
	 loss: 27.4433, MinusLogProbMetric: 27.4433, val_loss: 28.4843, val_MinusLogProbMetric: 28.4843

Epoch 568: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4433 - MinusLogProbMetric: 27.4433 - val_loss: 28.4843 - val_MinusLogProbMetric: 28.4843 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 569/1000
2023-10-25 23:50:39.000 
Epoch 569/1000 
	 loss: 27.4476, MinusLogProbMetric: 27.4476, val_loss: 28.4568, val_MinusLogProbMetric: 28.4568

Epoch 569: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4476 - MinusLogProbMetric: 27.4476 - val_loss: 28.4568 - val_MinusLogProbMetric: 28.4568 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 570/1000
2023-10-25 23:51:21.842 
Epoch 570/1000 
	 loss: 27.4411, MinusLogProbMetric: 27.4411, val_loss: 28.5541, val_MinusLogProbMetric: 28.5541

Epoch 570: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4411 - MinusLogProbMetric: 27.4411 - val_loss: 28.5541 - val_MinusLogProbMetric: 28.5541 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 571/1000
2023-10-25 23:52:04.630 
Epoch 571/1000 
	 loss: 27.4340, MinusLogProbMetric: 27.4340, val_loss: 28.4573, val_MinusLogProbMetric: 28.4573

Epoch 571: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4340 - MinusLogProbMetric: 27.4340 - val_loss: 28.4573 - val_MinusLogProbMetric: 28.4573 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 572/1000
2023-10-25 23:52:47.271 
Epoch 572/1000 
	 loss: 27.4331, MinusLogProbMetric: 27.4331, val_loss: 28.4649, val_MinusLogProbMetric: 28.4649

Epoch 572: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4331 - MinusLogProbMetric: 27.4331 - val_loss: 28.4649 - val_MinusLogProbMetric: 28.4649 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 573/1000
2023-10-25 23:53:29.859 
Epoch 573/1000 
	 loss: 27.4138, MinusLogProbMetric: 27.4138, val_loss: 28.4576, val_MinusLogProbMetric: 28.4576

Epoch 573: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4138 - MinusLogProbMetric: 27.4138 - val_loss: 28.4576 - val_MinusLogProbMetric: 28.4576 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 574/1000
2023-10-25 23:54:12.928 
Epoch 574/1000 
	 loss: 27.4359, MinusLogProbMetric: 27.4359, val_loss: 28.6937, val_MinusLogProbMetric: 28.6937

Epoch 574: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4359 - MinusLogProbMetric: 27.4359 - val_loss: 28.6937 - val_MinusLogProbMetric: 28.6937 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 575/1000
2023-10-25 23:54:56.534 
Epoch 575/1000 
	 loss: 27.4266, MinusLogProbMetric: 27.4266, val_loss: 28.4935, val_MinusLogProbMetric: 28.4935

Epoch 575: val_loss did not improve from 28.39582
196/196 - 44s - loss: 27.4266 - MinusLogProbMetric: 27.4266 - val_loss: 28.4935 - val_MinusLogProbMetric: 28.4935 - lr: 8.3333e-05 - 44s/epoch - 222ms/step
Epoch 576/1000
2023-10-25 23:55:39.382 
Epoch 576/1000 
	 loss: 27.4323, MinusLogProbMetric: 27.4323, val_loss: 28.5530, val_MinusLogProbMetric: 28.5530

Epoch 576: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4323 - MinusLogProbMetric: 27.4323 - val_loss: 28.5530 - val_MinusLogProbMetric: 28.5530 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 577/1000
2023-10-25 23:56:22.120 
Epoch 577/1000 
	 loss: 27.4219, MinusLogProbMetric: 27.4219, val_loss: 28.5171, val_MinusLogProbMetric: 28.5171

Epoch 577: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4219 - MinusLogProbMetric: 27.4219 - val_loss: 28.5171 - val_MinusLogProbMetric: 28.5171 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 578/1000
2023-10-25 23:57:05.076 
Epoch 578/1000 
	 loss: 27.4416, MinusLogProbMetric: 27.4416, val_loss: 28.5275, val_MinusLogProbMetric: 28.5275

Epoch 578: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4416 - MinusLogProbMetric: 27.4416 - val_loss: 28.5275 - val_MinusLogProbMetric: 28.5275 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 579/1000
2023-10-25 23:57:48.153 
Epoch 579/1000 
	 loss: 27.4310, MinusLogProbMetric: 27.4310, val_loss: 28.4360, val_MinusLogProbMetric: 28.4360

Epoch 579: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4310 - MinusLogProbMetric: 27.4310 - val_loss: 28.4360 - val_MinusLogProbMetric: 28.4360 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 580/1000
2023-10-25 23:58:30.587 
Epoch 580/1000 
	 loss: 27.4351, MinusLogProbMetric: 27.4351, val_loss: 28.5217, val_MinusLogProbMetric: 28.5217

Epoch 580: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4351 - MinusLogProbMetric: 27.4351 - val_loss: 28.5217 - val_MinusLogProbMetric: 28.5217 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 581/1000
2023-10-25 23:59:13.633 
Epoch 581/1000 
	 loss: 27.4304, MinusLogProbMetric: 27.4304, val_loss: 28.5506, val_MinusLogProbMetric: 28.5506

Epoch 581: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4304 - MinusLogProbMetric: 27.4304 - val_loss: 28.5506 - val_MinusLogProbMetric: 28.5506 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 582/1000
2023-10-25 23:59:56.699 
Epoch 582/1000 
	 loss: 27.4462, MinusLogProbMetric: 27.4462, val_loss: 28.5478, val_MinusLogProbMetric: 28.5478

Epoch 582: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4462 - MinusLogProbMetric: 27.4462 - val_loss: 28.5478 - val_MinusLogProbMetric: 28.5478 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 583/1000
2023-10-26 00:00:39.963 
Epoch 583/1000 
	 loss: 27.4375, MinusLogProbMetric: 27.4375, val_loss: 28.5576, val_MinusLogProbMetric: 28.5576

Epoch 583: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4375 - MinusLogProbMetric: 27.4375 - val_loss: 28.5576 - val_MinusLogProbMetric: 28.5576 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 584/1000
2023-10-26 00:01:19.051 
Epoch 584/1000 
	 loss: 27.4354, MinusLogProbMetric: 27.4354, val_loss: 28.4610, val_MinusLogProbMetric: 28.4610

Epoch 584: val_loss did not improve from 28.39582
196/196 - 39s - loss: 27.4354 - MinusLogProbMetric: 27.4354 - val_loss: 28.4610 - val_MinusLogProbMetric: 28.4610 - lr: 8.3333e-05 - 39s/epoch - 199ms/step
Epoch 585/1000
2023-10-26 00:02:00.077 
Epoch 585/1000 
	 loss: 27.4496, MinusLogProbMetric: 27.4496, val_loss: 28.4933, val_MinusLogProbMetric: 28.4933

Epoch 585: val_loss did not improve from 28.39582
196/196 - 41s - loss: 27.4496 - MinusLogProbMetric: 27.4496 - val_loss: 28.4933 - val_MinusLogProbMetric: 28.4933 - lr: 8.3333e-05 - 41s/epoch - 209ms/step
Epoch 586/1000
2023-10-26 00:02:42.986 
Epoch 586/1000 
	 loss: 27.4501, MinusLogProbMetric: 27.4501, val_loss: 28.5290, val_MinusLogProbMetric: 28.5290

Epoch 586: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4501 - MinusLogProbMetric: 27.4501 - val_loss: 28.5290 - val_MinusLogProbMetric: 28.5290 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 587/1000
2023-10-26 00:03:25.643 
Epoch 587/1000 
	 loss: 27.4293, MinusLogProbMetric: 27.4293, val_loss: 28.5347, val_MinusLogProbMetric: 28.5347

Epoch 587: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4293 - MinusLogProbMetric: 27.4293 - val_loss: 28.5347 - val_MinusLogProbMetric: 28.5347 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 588/1000
2023-10-26 00:04:08.206 
Epoch 588/1000 
	 loss: 27.4341, MinusLogProbMetric: 27.4341, val_loss: 28.4611, val_MinusLogProbMetric: 28.4611

Epoch 588: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4341 - MinusLogProbMetric: 27.4341 - val_loss: 28.4611 - val_MinusLogProbMetric: 28.4611 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 589/1000
2023-10-26 00:04:48.157 
Epoch 589/1000 
	 loss: 27.4282, MinusLogProbMetric: 27.4282, val_loss: 28.5141, val_MinusLogProbMetric: 28.5141

Epoch 589: val_loss did not improve from 28.39582
196/196 - 40s - loss: 27.4282 - MinusLogProbMetric: 27.4282 - val_loss: 28.5141 - val_MinusLogProbMetric: 28.5141 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 590/1000
2023-10-26 00:05:28.593 
Epoch 590/1000 
	 loss: 27.4219, MinusLogProbMetric: 27.4219, val_loss: 28.4883, val_MinusLogProbMetric: 28.4883

Epoch 590: val_loss did not improve from 28.39582
196/196 - 40s - loss: 27.4219 - MinusLogProbMetric: 27.4219 - val_loss: 28.4883 - val_MinusLogProbMetric: 28.4883 - lr: 8.3333e-05 - 40s/epoch - 206ms/step
Epoch 591/1000
2023-10-26 00:06:11.435 
Epoch 591/1000 
	 loss: 27.4267, MinusLogProbMetric: 27.4267, val_loss: 28.4272, val_MinusLogProbMetric: 28.4272

Epoch 591: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4267 - MinusLogProbMetric: 27.4267 - val_loss: 28.4272 - val_MinusLogProbMetric: 28.4272 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 592/1000
2023-10-26 00:06:53.798 
Epoch 592/1000 
	 loss: 27.4265, MinusLogProbMetric: 27.4265, val_loss: 28.4466, val_MinusLogProbMetric: 28.4466

Epoch 592: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4265 - MinusLogProbMetric: 27.4265 - val_loss: 28.4466 - val_MinusLogProbMetric: 28.4466 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 593/1000
2023-10-26 00:07:36.833 
Epoch 593/1000 
	 loss: 27.4093, MinusLogProbMetric: 27.4093, val_loss: 28.4122, val_MinusLogProbMetric: 28.4122

Epoch 593: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4093 - MinusLogProbMetric: 27.4093 - val_loss: 28.4122 - val_MinusLogProbMetric: 28.4122 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 594/1000
2023-10-26 00:08:19.632 
Epoch 594/1000 
	 loss: 27.4078, MinusLogProbMetric: 27.4078, val_loss: 28.4771, val_MinusLogProbMetric: 28.4771

Epoch 594: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4078 - MinusLogProbMetric: 27.4078 - val_loss: 28.4771 - val_MinusLogProbMetric: 28.4771 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 595/1000
2023-10-26 00:09:02.502 
Epoch 595/1000 
	 loss: 27.4144, MinusLogProbMetric: 27.4144, val_loss: 28.4506, val_MinusLogProbMetric: 28.4506

Epoch 595: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4144 - MinusLogProbMetric: 27.4144 - val_loss: 28.4506 - val_MinusLogProbMetric: 28.4506 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 596/1000
2023-10-26 00:09:45.103 
Epoch 596/1000 
	 loss: 27.4219, MinusLogProbMetric: 27.4219, val_loss: 28.4468, val_MinusLogProbMetric: 28.4468

Epoch 596: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4219 - MinusLogProbMetric: 27.4219 - val_loss: 28.4468 - val_MinusLogProbMetric: 28.4468 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 597/1000
2023-10-26 00:10:27.547 
Epoch 597/1000 
	 loss: 27.4107, MinusLogProbMetric: 27.4107, val_loss: 28.5813, val_MinusLogProbMetric: 28.5813

Epoch 597: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4107 - MinusLogProbMetric: 27.4107 - val_loss: 28.5813 - val_MinusLogProbMetric: 28.5813 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 598/1000
2023-10-26 00:11:10.012 
Epoch 598/1000 
	 loss: 27.4292, MinusLogProbMetric: 27.4292, val_loss: 28.4606, val_MinusLogProbMetric: 28.4606

Epoch 598: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.4292 - MinusLogProbMetric: 27.4292 - val_loss: 28.4606 - val_MinusLogProbMetric: 28.4606 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 599/1000
2023-10-26 00:11:52.597 
Epoch 599/1000 
	 loss: 27.4254, MinusLogProbMetric: 27.4254, val_loss: 28.4511, val_MinusLogProbMetric: 28.4511

Epoch 599: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4254 - MinusLogProbMetric: 27.4254 - val_loss: 28.4511 - val_MinusLogProbMetric: 28.4511 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 600/1000
2023-10-26 00:12:35.683 
Epoch 600/1000 
	 loss: 27.4137, MinusLogProbMetric: 27.4137, val_loss: 28.4423, val_MinusLogProbMetric: 28.4423

Epoch 600: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4137 - MinusLogProbMetric: 27.4137 - val_loss: 28.4423 - val_MinusLogProbMetric: 28.4423 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 601/1000
2023-10-26 00:13:19.023 
Epoch 601/1000 
	 loss: 27.4197, MinusLogProbMetric: 27.4197, val_loss: 28.5038, val_MinusLogProbMetric: 28.5038

Epoch 601: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4197 - MinusLogProbMetric: 27.4197 - val_loss: 28.5038 - val_MinusLogProbMetric: 28.5038 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 602/1000
2023-10-26 00:14:01.882 
Epoch 602/1000 
	 loss: 27.4139, MinusLogProbMetric: 27.4139, val_loss: 28.4997, val_MinusLogProbMetric: 28.4997

Epoch 602: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4139 - MinusLogProbMetric: 27.4139 - val_loss: 28.4997 - val_MinusLogProbMetric: 28.4997 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 603/1000
2023-10-26 00:14:44.626 
Epoch 603/1000 
	 loss: 27.4304, MinusLogProbMetric: 27.4304, val_loss: 28.6701, val_MinusLogProbMetric: 28.6701

Epoch 603: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.4304 - MinusLogProbMetric: 27.4304 - val_loss: 28.6701 - val_MinusLogProbMetric: 28.6701 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 604/1000
2023-10-26 00:15:28.096 
Epoch 604/1000 
	 loss: 27.3454, MinusLogProbMetric: 27.3454, val_loss: 28.4141, val_MinusLogProbMetric: 28.4141

Epoch 604: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.3454 - MinusLogProbMetric: 27.3454 - val_loss: 28.4141 - val_MinusLogProbMetric: 28.4141 - lr: 4.1667e-05 - 43s/epoch - 222ms/step
Epoch 605/1000
2023-10-26 00:16:10.515 
Epoch 605/1000 
	 loss: 27.3348, MinusLogProbMetric: 27.3348, val_loss: 28.5010, val_MinusLogProbMetric: 28.5010

Epoch 605: val_loss did not improve from 28.39582
196/196 - 42s - loss: 27.3348 - MinusLogProbMetric: 27.3348 - val_loss: 28.5010 - val_MinusLogProbMetric: 28.5010 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 606/1000
2023-10-26 00:16:53.354 
Epoch 606/1000 
	 loss: 27.3363, MinusLogProbMetric: 27.3363, val_loss: 28.3999, val_MinusLogProbMetric: 28.3999

Epoch 606: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.3363 - MinusLogProbMetric: 27.3363 - val_loss: 28.3999 - val_MinusLogProbMetric: 28.3999 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 607/1000
2023-10-26 00:17:36.174 
Epoch 607/1000 
	 loss: 27.3356, MinusLogProbMetric: 27.3356, val_loss: 28.3965, val_MinusLogProbMetric: 28.3965

Epoch 607: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.3356 - MinusLogProbMetric: 27.3356 - val_loss: 28.3965 - val_MinusLogProbMetric: 28.3965 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 608/1000
2023-10-26 00:18:18.879 
Epoch 608/1000 
	 loss: 27.3363, MinusLogProbMetric: 27.3363, val_loss: 28.4072, val_MinusLogProbMetric: 28.4072

Epoch 608: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.3363 - MinusLogProbMetric: 27.3363 - val_loss: 28.4072 - val_MinusLogProbMetric: 28.4072 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 609/1000
2023-10-26 00:19:01.924 
Epoch 609/1000 
	 loss: 27.3309, MinusLogProbMetric: 27.3309, val_loss: 28.4021, val_MinusLogProbMetric: 28.4021

Epoch 609: val_loss did not improve from 28.39582
196/196 - 43s - loss: 27.3309 - MinusLogProbMetric: 27.3309 - val_loss: 28.4021 - val_MinusLogProbMetric: 28.4021 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 610/1000
2023-10-26 00:19:45.006 
Epoch 610/1000 
	 loss: 27.3315, MinusLogProbMetric: 27.3315, val_loss: 28.3695, val_MinusLogProbMetric: 28.3695

Epoch 610: val_loss improved from 28.39582 to 28.36950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.3315 - MinusLogProbMetric: 27.3315 - val_loss: 28.3695 - val_MinusLogProbMetric: 28.3695 - lr: 4.1667e-05 - 44s/epoch - 223ms/step
Epoch 611/1000
2023-10-26 00:20:28.367 
Epoch 611/1000 
	 loss: 27.3268, MinusLogProbMetric: 27.3268, val_loss: 28.4175, val_MinusLogProbMetric: 28.4175

Epoch 611: val_loss did not improve from 28.36950
196/196 - 43s - loss: 27.3268 - MinusLogProbMetric: 27.3268 - val_loss: 28.4175 - val_MinusLogProbMetric: 28.4175 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 612/1000
2023-10-26 00:21:11.032 
Epoch 612/1000 
	 loss: 27.3252, MinusLogProbMetric: 27.3252, val_loss: 28.3897, val_MinusLogProbMetric: 28.3897

Epoch 612: val_loss did not improve from 28.36950
196/196 - 43s - loss: 27.3252 - MinusLogProbMetric: 27.3252 - val_loss: 28.3897 - val_MinusLogProbMetric: 28.3897 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 613/1000
2023-10-26 00:21:53.688 
Epoch 613/1000 
	 loss: 27.3360, MinusLogProbMetric: 27.3360, val_loss: 28.3849, val_MinusLogProbMetric: 28.3849

Epoch 613: val_loss did not improve from 28.36950
196/196 - 43s - loss: 27.3360 - MinusLogProbMetric: 27.3360 - val_loss: 28.3849 - val_MinusLogProbMetric: 28.3849 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 614/1000
2023-10-26 00:22:36.722 
Epoch 614/1000 
	 loss: 27.3382, MinusLogProbMetric: 27.3382, val_loss: 28.3816, val_MinusLogProbMetric: 28.3816

Epoch 614: val_loss did not improve from 28.36950
196/196 - 43s - loss: 27.3382 - MinusLogProbMetric: 27.3382 - val_loss: 28.3816 - val_MinusLogProbMetric: 28.3816 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 615/1000
2023-10-26 00:23:19.474 
Epoch 615/1000 
	 loss: 27.3263, MinusLogProbMetric: 27.3263, val_loss: 28.3990, val_MinusLogProbMetric: 28.3990

Epoch 615: val_loss did not improve from 28.36950
196/196 - 43s - loss: 27.3263 - MinusLogProbMetric: 27.3263 - val_loss: 28.3990 - val_MinusLogProbMetric: 28.3990 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 616/1000
2023-10-26 00:24:02.574 
Epoch 616/1000 
	 loss: 27.3316, MinusLogProbMetric: 27.3316, val_loss: 28.4314, val_MinusLogProbMetric: 28.4314

Epoch 616: val_loss did not improve from 28.36950
196/196 - 43s - loss: 27.3316 - MinusLogProbMetric: 27.3316 - val_loss: 28.4314 - val_MinusLogProbMetric: 28.4314 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 617/1000
2023-10-26 00:24:45.151 
Epoch 617/1000 
	 loss: 27.3276, MinusLogProbMetric: 27.3276, val_loss: 28.3687, val_MinusLogProbMetric: 28.3687

Epoch 617: val_loss improved from 28.36950 to 28.36868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.3276 - MinusLogProbMetric: 27.3276 - val_loss: 28.3687 - val_MinusLogProbMetric: 28.3687 - lr: 4.1667e-05 - 43s/epoch - 221ms/step
Epoch 618/1000
2023-10-26 00:25:28.686 
Epoch 618/1000 
	 loss: 27.3348, MinusLogProbMetric: 27.3348, val_loss: 28.4237, val_MinusLogProbMetric: 28.4237

Epoch 618: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3348 - MinusLogProbMetric: 27.3348 - val_loss: 28.4237 - val_MinusLogProbMetric: 28.4237 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 619/1000
2023-10-26 00:26:11.448 
Epoch 619/1000 
	 loss: 27.3392, MinusLogProbMetric: 27.3392, val_loss: 28.4019, val_MinusLogProbMetric: 28.4019

Epoch 619: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3392 - MinusLogProbMetric: 27.3392 - val_loss: 28.4019 - val_MinusLogProbMetric: 28.4019 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 620/1000
2023-10-26 00:26:54.714 
Epoch 620/1000 
	 loss: 27.3368, MinusLogProbMetric: 27.3368, val_loss: 28.3760, val_MinusLogProbMetric: 28.3760

Epoch 620: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3368 - MinusLogProbMetric: 27.3368 - val_loss: 28.3760 - val_MinusLogProbMetric: 28.3760 - lr: 4.1667e-05 - 43s/epoch - 221ms/step
Epoch 621/1000
2023-10-26 00:27:37.717 
Epoch 621/1000 
	 loss: 27.3335, MinusLogProbMetric: 27.3335, val_loss: 28.3713, val_MinusLogProbMetric: 28.3713

Epoch 621: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3335 - MinusLogProbMetric: 27.3335 - val_loss: 28.3713 - val_MinusLogProbMetric: 28.3713 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 622/1000
2023-10-26 00:28:20.323 
Epoch 622/1000 
	 loss: 27.3244, MinusLogProbMetric: 27.3244, val_loss: 28.3782, val_MinusLogProbMetric: 28.3782

Epoch 622: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3244 - MinusLogProbMetric: 27.3244 - val_loss: 28.3782 - val_MinusLogProbMetric: 28.3782 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 623/1000
2023-10-26 00:29:02.799 
Epoch 623/1000 
	 loss: 27.3277, MinusLogProbMetric: 27.3277, val_loss: 28.4108, val_MinusLogProbMetric: 28.4108

Epoch 623: val_loss did not improve from 28.36868
196/196 - 42s - loss: 27.3277 - MinusLogProbMetric: 27.3277 - val_loss: 28.4108 - val_MinusLogProbMetric: 28.4108 - lr: 4.1667e-05 - 42s/epoch - 217ms/step
Epoch 624/1000
2023-10-26 00:29:45.240 
Epoch 624/1000 
	 loss: 27.3309, MinusLogProbMetric: 27.3309, val_loss: 28.4220, val_MinusLogProbMetric: 28.4220

Epoch 624: val_loss did not improve from 28.36868
196/196 - 42s - loss: 27.3309 - MinusLogProbMetric: 27.3309 - val_loss: 28.4220 - val_MinusLogProbMetric: 28.4220 - lr: 4.1667e-05 - 42s/epoch - 217ms/step
Epoch 625/1000
2023-10-26 00:30:28.206 
Epoch 625/1000 
	 loss: 27.3211, MinusLogProbMetric: 27.3211, val_loss: 28.3948, val_MinusLogProbMetric: 28.3948

Epoch 625: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3211 - MinusLogProbMetric: 27.3211 - val_loss: 28.3948 - val_MinusLogProbMetric: 28.3948 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 626/1000
2023-10-26 00:31:11.278 
Epoch 626/1000 
	 loss: 27.3274, MinusLogProbMetric: 27.3274, val_loss: 28.3693, val_MinusLogProbMetric: 28.3693

Epoch 626: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3274 - MinusLogProbMetric: 27.3274 - val_loss: 28.3693 - val_MinusLogProbMetric: 28.3693 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 627/1000
2023-10-26 00:31:54.559 
Epoch 627/1000 
	 loss: 27.3235, MinusLogProbMetric: 27.3235, val_loss: 28.3967, val_MinusLogProbMetric: 28.3967

Epoch 627: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3235 - MinusLogProbMetric: 27.3235 - val_loss: 28.3967 - val_MinusLogProbMetric: 28.3967 - lr: 4.1667e-05 - 43s/epoch - 221ms/step
Epoch 628/1000
2023-10-26 00:32:37.305 
Epoch 628/1000 
	 loss: 27.3301, MinusLogProbMetric: 27.3301, val_loss: 28.3795, val_MinusLogProbMetric: 28.3795

Epoch 628: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3301 - MinusLogProbMetric: 27.3301 - val_loss: 28.3795 - val_MinusLogProbMetric: 28.3795 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 629/1000
2023-10-26 00:33:20.400 
Epoch 629/1000 
	 loss: 27.3267, MinusLogProbMetric: 27.3267, val_loss: 28.3912, val_MinusLogProbMetric: 28.3912

Epoch 629: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3267 - MinusLogProbMetric: 27.3267 - val_loss: 28.3912 - val_MinusLogProbMetric: 28.3912 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 630/1000
2023-10-26 00:34:03.422 
Epoch 630/1000 
	 loss: 27.3253, MinusLogProbMetric: 27.3253, val_loss: 28.4069, val_MinusLogProbMetric: 28.4069

Epoch 630: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3253 - MinusLogProbMetric: 27.3253 - val_loss: 28.4069 - val_MinusLogProbMetric: 28.4069 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 631/1000
2023-10-26 00:34:46.282 
Epoch 631/1000 
	 loss: 27.3345, MinusLogProbMetric: 27.3345, val_loss: 28.3943, val_MinusLogProbMetric: 28.3943

Epoch 631: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3345 - MinusLogProbMetric: 27.3345 - val_loss: 28.3943 - val_MinusLogProbMetric: 28.3943 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 632/1000
2023-10-26 00:35:29.435 
Epoch 632/1000 
	 loss: 27.3266, MinusLogProbMetric: 27.3266, val_loss: 28.4140, val_MinusLogProbMetric: 28.4140

Epoch 632: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3266 - MinusLogProbMetric: 27.3266 - val_loss: 28.4140 - val_MinusLogProbMetric: 28.4140 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 633/1000
2023-10-26 00:36:12.702 
Epoch 633/1000 
	 loss: 27.3241, MinusLogProbMetric: 27.3241, val_loss: 28.4103, val_MinusLogProbMetric: 28.4103

Epoch 633: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3241 - MinusLogProbMetric: 27.3241 - val_loss: 28.4103 - val_MinusLogProbMetric: 28.4103 - lr: 4.1667e-05 - 43s/epoch - 221ms/step
Epoch 634/1000
2023-10-26 00:36:55.056 
Epoch 634/1000 
	 loss: 27.3265, MinusLogProbMetric: 27.3265, val_loss: 28.4138, val_MinusLogProbMetric: 28.4138

Epoch 634: val_loss did not improve from 28.36868
196/196 - 42s - loss: 27.3265 - MinusLogProbMetric: 27.3265 - val_loss: 28.4138 - val_MinusLogProbMetric: 28.4138 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 635/1000
2023-10-26 00:37:38.014 
Epoch 635/1000 
	 loss: 27.3308, MinusLogProbMetric: 27.3308, val_loss: 28.4257, val_MinusLogProbMetric: 28.4257

Epoch 635: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3308 - MinusLogProbMetric: 27.3308 - val_loss: 28.4257 - val_MinusLogProbMetric: 28.4257 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 636/1000
2023-10-26 00:38:21.180 
Epoch 636/1000 
	 loss: 27.3223, MinusLogProbMetric: 27.3223, val_loss: 28.4076, val_MinusLogProbMetric: 28.4076

Epoch 636: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3223 - MinusLogProbMetric: 27.3223 - val_loss: 28.4076 - val_MinusLogProbMetric: 28.4076 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 637/1000
2023-10-26 00:39:04.512 
Epoch 637/1000 
	 loss: 27.3278, MinusLogProbMetric: 27.3278, val_loss: 28.3740, val_MinusLogProbMetric: 28.3740

Epoch 637: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3278 - MinusLogProbMetric: 27.3278 - val_loss: 28.3740 - val_MinusLogProbMetric: 28.3740 - lr: 4.1667e-05 - 43s/epoch - 221ms/step
Epoch 638/1000
2023-10-26 00:39:47.248 
Epoch 638/1000 
	 loss: 27.3258, MinusLogProbMetric: 27.3258, val_loss: 28.4165, val_MinusLogProbMetric: 28.4165

Epoch 638: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3258 - MinusLogProbMetric: 27.3258 - val_loss: 28.4165 - val_MinusLogProbMetric: 28.4165 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 639/1000
2023-10-26 00:40:29.771 
Epoch 639/1000 
	 loss: 27.3202, MinusLogProbMetric: 27.3202, val_loss: 28.3996, val_MinusLogProbMetric: 28.3996

Epoch 639: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3202 - MinusLogProbMetric: 27.3202 - val_loss: 28.3996 - val_MinusLogProbMetric: 28.3996 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 640/1000
2023-10-26 00:41:12.266 
Epoch 640/1000 
	 loss: 27.3217, MinusLogProbMetric: 27.3217, val_loss: 28.3813, val_MinusLogProbMetric: 28.3813

Epoch 640: val_loss did not improve from 28.36868
196/196 - 42s - loss: 27.3217 - MinusLogProbMetric: 27.3217 - val_loss: 28.3813 - val_MinusLogProbMetric: 28.3813 - lr: 4.1667e-05 - 42s/epoch - 217ms/step
Epoch 641/1000
2023-10-26 00:41:55.438 
Epoch 641/1000 
	 loss: 27.3221, MinusLogProbMetric: 27.3221, val_loss: 28.4045, val_MinusLogProbMetric: 28.4045

Epoch 641: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3221 - MinusLogProbMetric: 27.3221 - val_loss: 28.4045 - val_MinusLogProbMetric: 28.4045 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 642/1000
2023-10-26 00:42:38.698 
Epoch 642/1000 
	 loss: 27.3181, MinusLogProbMetric: 27.3181, val_loss: 28.4190, val_MinusLogProbMetric: 28.4190

Epoch 642: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3181 - MinusLogProbMetric: 27.3181 - val_loss: 28.4190 - val_MinusLogProbMetric: 28.4190 - lr: 4.1667e-05 - 43s/epoch - 221ms/step
Epoch 643/1000
2023-10-26 00:43:21.432 
Epoch 643/1000 
	 loss: 27.3235, MinusLogProbMetric: 27.3235, val_loss: 28.3953, val_MinusLogProbMetric: 28.3953

Epoch 643: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3235 - MinusLogProbMetric: 27.3235 - val_loss: 28.3953 - val_MinusLogProbMetric: 28.3953 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 644/1000
2023-10-26 00:44:04.058 
Epoch 644/1000 
	 loss: 27.3233, MinusLogProbMetric: 27.3233, val_loss: 28.4702, val_MinusLogProbMetric: 28.4702

Epoch 644: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3233 - MinusLogProbMetric: 27.3233 - val_loss: 28.4702 - val_MinusLogProbMetric: 28.4702 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 645/1000
2023-10-26 00:44:47.117 
Epoch 645/1000 
	 loss: 27.3235, MinusLogProbMetric: 27.3235, val_loss: 28.4129, val_MinusLogProbMetric: 28.4129

Epoch 645: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3235 - MinusLogProbMetric: 27.3235 - val_loss: 28.4129 - val_MinusLogProbMetric: 28.4129 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 646/1000
2023-10-26 00:45:29.717 
Epoch 646/1000 
	 loss: 27.3143, MinusLogProbMetric: 27.3143, val_loss: 28.4216, val_MinusLogProbMetric: 28.4216

Epoch 646: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3143 - MinusLogProbMetric: 27.3143 - val_loss: 28.4216 - val_MinusLogProbMetric: 28.4216 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 647/1000
2023-10-26 00:46:12.257 
Epoch 647/1000 
	 loss: 27.3252, MinusLogProbMetric: 27.3252, val_loss: 28.4018, val_MinusLogProbMetric: 28.4018

Epoch 647: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3252 - MinusLogProbMetric: 27.3252 - val_loss: 28.4018 - val_MinusLogProbMetric: 28.4018 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 648/1000
2023-10-26 00:46:54.824 
Epoch 648/1000 
	 loss: 27.3198, MinusLogProbMetric: 27.3198, val_loss: 28.4317, val_MinusLogProbMetric: 28.4317

Epoch 648: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3198 - MinusLogProbMetric: 27.3198 - val_loss: 28.4317 - val_MinusLogProbMetric: 28.4317 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 649/1000
2023-10-26 00:47:37.518 
Epoch 649/1000 
	 loss: 27.3180, MinusLogProbMetric: 27.3180, val_loss: 28.3908, val_MinusLogProbMetric: 28.3908

Epoch 649: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3180 - MinusLogProbMetric: 27.3180 - val_loss: 28.3908 - val_MinusLogProbMetric: 28.3908 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 650/1000
2023-10-26 00:48:19.564 
Epoch 650/1000 
	 loss: 27.3196, MinusLogProbMetric: 27.3196, val_loss: 28.3736, val_MinusLogProbMetric: 28.3736

Epoch 650: val_loss did not improve from 28.36868
196/196 - 42s - loss: 27.3196 - MinusLogProbMetric: 27.3196 - val_loss: 28.3736 - val_MinusLogProbMetric: 28.3736 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 651/1000
2023-10-26 00:49:01.870 
Epoch 651/1000 
	 loss: 27.3239, MinusLogProbMetric: 27.3239, val_loss: 28.3982, val_MinusLogProbMetric: 28.3982

Epoch 651: val_loss did not improve from 28.36868
196/196 - 42s - loss: 27.3239 - MinusLogProbMetric: 27.3239 - val_loss: 28.3982 - val_MinusLogProbMetric: 28.3982 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 652/1000
2023-10-26 00:49:44.413 
Epoch 652/1000 
	 loss: 27.3129, MinusLogProbMetric: 27.3129, val_loss: 28.4165, val_MinusLogProbMetric: 28.4165

Epoch 652: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3129 - MinusLogProbMetric: 27.3129 - val_loss: 28.4165 - val_MinusLogProbMetric: 28.4165 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 653/1000
2023-10-26 00:50:27.466 
Epoch 653/1000 
	 loss: 27.3183, MinusLogProbMetric: 27.3183, val_loss: 28.4226, val_MinusLogProbMetric: 28.4226

Epoch 653: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3183 - MinusLogProbMetric: 27.3183 - val_loss: 28.4226 - val_MinusLogProbMetric: 28.4226 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 654/1000
2023-10-26 00:51:10.606 
Epoch 654/1000 
	 loss: 27.3204, MinusLogProbMetric: 27.3204, val_loss: 28.4206, val_MinusLogProbMetric: 28.4206

Epoch 654: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3204 - MinusLogProbMetric: 27.3204 - val_loss: 28.4206 - val_MinusLogProbMetric: 28.4206 - lr: 4.1667e-05 - 43s/epoch - 220ms/step
Epoch 655/1000
2023-10-26 00:51:52.581 
Epoch 655/1000 
	 loss: 27.3154, MinusLogProbMetric: 27.3154, val_loss: 28.3820, val_MinusLogProbMetric: 28.3820

Epoch 655: val_loss did not improve from 28.36868
196/196 - 42s - loss: 27.3154 - MinusLogProbMetric: 27.3154 - val_loss: 28.3820 - val_MinusLogProbMetric: 28.3820 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 656/1000
2023-10-26 00:52:26.088 
Epoch 656/1000 
	 loss: 27.3141, MinusLogProbMetric: 27.3141, val_loss: 28.3914, val_MinusLogProbMetric: 28.3914

Epoch 656: val_loss did not improve from 28.36868
196/196 - 34s - loss: 27.3141 - MinusLogProbMetric: 27.3141 - val_loss: 28.3914 - val_MinusLogProbMetric: 28.3914 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 657/1000
2023-10-26 00:53:05.171 
Epoch 657/1000 
	 loss: 27.3214, MinusLogProbMetric: 27.3214, val_loss: 28.4049, val_MinusLogProbMetric: 28.4049

Epoch 657: val_loss did not improve from 28.36868
196/196 - 39s - loss: 27.3214 - MinusLogProbMetric: 27.3214 - val_loss: 28.4049 - val_MinusLogProbMetric: 28.4049 - lr: 4.1667e-05 - 39s/epoch - 199ms/step
Epoch 658/1000
2023-10-26 00:53:47.935 
Epoch 658/1000 
	 loss: 27.3158, MinusLogProbMetric: 27.3158, val_loss: 28.4109, val_MinusLogProbMetric: 28.4109

Epoch 658: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3158 - MinusLogProbMetric: 27.3158 - val_loss: 28.4109 - val_MinusLogProbMetric: 28.4109 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 659/1000
2023-10-26 00:54:30.746 
Epoch 659/1000 
	 loss: 27.3239, MinusLogProbMetric: 27.3239, val_loss: 28.4026, val_MinusLogProbMetric: 28.4026

Epoch 659: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3239 - MinusLogProbMetric: 27.3239 - val_loss: 28.4026 - val_MinusLogProbMetric: 28.4026 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 660/1000
2023-10-26 00:55:13.696 
Epoch 660/1000 
	 loss: 27.3202, MinusLogProbMetric: 27.3202, val_loss: 28.3972, val_MinusLogProbMetric: 28.3972

Epoch 660: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3202 - MinusLogProbMetric: 27.3202 - val_loss: 28.3972 - val_MinusLogProbMetric: 28.3972 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 661/1000
2023-10-26 00:55:56.535 
Epoch 661/1000 
	 loss: 27.3169, MinusLogProbMetric: 27.3169, val_loss: 28.3907, val_MinusLogProbMetric: 28.3907

Epoch 661: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3169 - MinusLogProbMetric: 27.3169 - val_loss: 28.3907 - val_MinusLogProbMetric: 28.3907 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 662/1000
2023-10-26 00:56:39.082 
Epoch 662/1000 
	 loss: 27.3105, MinusLogProbMetric: 27.3105, val_loss: 28.4009, val_MinusLogProbMetric: 28.4009

Epoch 662: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3105 - MinusLogProbMetric: 27.3105 - val_loss: 28.4009 - val_MinusLogProbMetric: 28.4009 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 663/1000
2023-10-26 00:57:21.812 
Epoch 663/1000 
	 loss: 27.3136, MinusLogProbMetric: 27.3136, val_loss: 28.4053, val_MinusLogProbMetric: 28.4053

Epoch 663: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3136 - MinusLogProbMetric: 27.3136 - val_loss: 28.4053 - val_MinusLogProbMetric: 28.4053 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 664/1000
2023-10-26 00:58:04.568 
Epoch 664/1000 
	 loss: 27.3146, MinusLogProbMetric: 27.3146, val_loss: 28.4289, val_MinusLogProbMetric: 28.4289

Epoch 664: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3146 - MinusLogProbMetric: 27.3146 - val_loss: 28.4289 - val_MinusLogProbMetric: 28.4289 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 665/1000
2023-10-26 00:58:47.359 
Epoch 665/1000 
	 loss: 27.3167, MinusLogProbMetric: 27.3167, val_loss: 28.4085, val_MinusLogProbMetric: 28.4085

Epoch 665: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3167 - MinusLogProbMetric: 27.3167 - val_loss: 28.4085 - val_MinusLogProbMetric: 28.4085 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 666/1000
2023-10-26 00:59:29.804 
Epoch 666/1000 
	 loss: 27.3130, MinusLogProbMetric: 27.3130, val_loss: 28.3887, val_MinusLogProbMetric: 28.3887

Epoch 666: val_loss did not improve from 28.36868
196/196 - 42s - loss: 27.3130 - MinusLogProbMetric: 27.3130 - val_loss: 28.3887 - val_MinusLogProbMetric: 28.3887 - lr: 4.1667e-05 - 42s/epoch - 217ms/step
Epoch 667/1000
2023-10-26 01:00:12.785 
Epoch 667/1000 
	 loss: 27.3111, MinusLogProbMetric: 27.3111, val_loss: 28.4285, val_MinusLogProbMetric: 28.4285

Epoch 667: val_loss did not improve from 28.36868
196/196 - 43s - loss: 27.3111 - MinusLogProbMetric: 27.3111 - val_loss: 28.4285 - val_MinusLogProbMetric: 28.4285 - lr: 4.1667e-05 - 43s/epoch - 219ms/step
Epoch 668/1000
2023-10-26 01:00:55.839 
Epoch 668/1000 
	 loss: 27.2784, MinusLogProbMetric: 27.2784, val_loss: 28.3663, val_MinusLogProbMetric: 28.3663

Epoch 668: val_loss improved from 28.36868 to 28.36626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.2784 - MinusLogProbMetric: 27.2784 - val_loss: 28.3663 - val_MinusLogProbMetric: 28.3663 - lr: 2.0833e-05 - 44s/epoch - 225ms/step
Epoch 669/1000
2023-10-26 01:01:39.441 
Epoch 669/1000 
	 loss: 27.2759, MinusLogProbMetric: 27.2759, val_loss: 28.3788, val_MinusLogProbMetric: 28.3788

Epoch 669: val_loss did not improve from 28.36626
196/196 - 43s - loss: 27.2759 - MinusLogProbMetric: 27.2759 - val_loss: 28.3788 - val_MinusLogProbMetric: 28.3788 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 670/1000
2023-10-26 01:02:22.278 
Epoch 670/1000 
	 loss: 27.2787, MinusLogProbMetric: 27.2787, val_loss: 28.3673, val_MinusLogProbMetric: 28.3673

Epoch 670: val_loss did not improve from 28.36626
196/196 - 43s - loss: 27.2787 - MinusLogProbMetric: 27.2787 - val_loss: 28.3673 - val_MinusLogProbMetric: 28.3673 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 671/1000
2023-10-26 01:03:05.136 
Epoch 671/1000 
	 loss: 27.2756, MinusLogProbMetric: 27.2756, val_loss: 28.3833, val_MinusLogProbMetric: 28.3833

Epoch 671: val_loss did not improve from 28.36626
196/196 - 43s - loss: 27.2756 - MinusLogProbMetric: 27.2756 - val_loss: 28.3833 - val_MinusLogProbMetric: 28.3833 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 672/1000
2023-10-26 01:03:48.233 
Epoch 672/1000 
	 loss: 27.2757, MinusLogProbMetric: 27.2757, val_loss: 28.3759, val_MinusLogProbMetric: 28.3759

Epoch 672: val_loss did not improve from 28.36626
196/196 - 43s - loss: 27.2757 - MinusLogProbMetric: 27.2757 - val_loss: 28.3759 - val_MinusLogProbMetric: 28.3759 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 673/1000
2023-10-26 01:04:31.099 
Epoch 673/1000 
	 loss: 27.2765, MinusLogProbMetric: 27.2765, val_loss: 28.3836, val_MinusLogProbMetric: 28.3836

Epoch 673: val_loss did not improve from 28.36626
196/196 - 43s - loss: 27.2765 - MinusLogProbMetric: 27.2765 - val_loss: 28.3836 - val_MinusLogProbMetric: 28.3836 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 674/1000
2023-10-26 01:05:13.918 
Epoch 674/1000 
	 loss: 27.2767, MinusLogProbMetric: 27.2767, val_loss: 28.3649, val_MinusLogProbMetric: 28.3649

Epoch 674: val_loss improved from 28.36626 to 28.36486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.2767 - MinusLogProbMetric: 27.2767 - val_loss: 28.3649 - val_MinusLogProbMetric: 28.3649 - lr: 2.0833e-05 - 44s/epoch - 223ms/step
Epoch 675/1000
2023-10-26 01:05:57.687 
Epoch 675/1000 
	 loss: 27.2772, MinusLogProbMetric: 27.2772, val_loss: 28.3660, val_MinusLogProbMetric: 28.3660

Epoch 675: val_loss did not improve from 28.36486
196/196 - 43s - loss: 27.2772 - MinusLogProbMetric: 27.2772 - val_loss: 28.3660 - val_MinusLogProbMetric: 28.3660 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 676/1000
2023-10-26 01:06:40.726 
Epoch 676/1000 
	 loss: 27.2769, MinusLogProbMetric: 27.2769, val_loss: 28.3726, val_MinusLogProbMetric: 28.3726

Epoch 676: val_loss did not improve from 28.36486
196/196 - 43s - loss: 27.2769 - MinusLogProbMetric: 27.2769 - val_loss: 28.3726 - val_MinusLogProbMetric: 28.3726 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 677/1000
2023-10-26 01:07:24.090 
Epoch 677/1000 
	 loss: 27.2759, MinusLogProbMetric: 27.2759, val_loss: 28.3647, val_MinusLogProbMetric: 28.3647

Epoch 677: val_loss improved from 28.36486 to 28.36465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.2759 - MinusLogProbMetric: 27.2759 - val_loss: 28.3647 - val_MinusLogProbMetric: 28.3647 - lr: 2.0833e-05 - 44s/epoch - 226ms/step
Epoch 678/1000
2023-10-26 01:08:08.111 
Epoch 678/1000 
	 loss: 27.2768, MinusLogProbMetric: 27.2768, val_loss: 28.3679, val_MinusLogProbMetric: 28.3679

Epoch 678: val_loss did not improve from 28.36465
196/196 - 43s - loss: 27.2768 - MinusLogProbMetric: 27.2768 - val_loss: 28.3679 - val_MinusLogProbMetric: 28.3679 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 679/1000
2023-10-26 01:08:50.696 
Epoch 679/1000 
	 loss: 27.2762, MinusLogProbMetric: 27.2762, val_loss: 28.3747, val_MinusLogProbMetric: 28.3747

Epoch 679: val_loss did not improve from 28.36465
196/196 - 43s - loss: 27.2762 - MinusLogProbMetric: 27.2762 - val_loss: 28.3747 - val_MinusLogProbMetric: 28.3747 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 680/1000
2023-10-26 01:09:33.255 
Epoch 680/1000 
	 loss: 27.2752, MinusLogProbMetric: 27.2752, val_loss: 28.3823, val_MinusLogProbMetric: 28.3823

Epoch 680: val_loss did not improve from 28.36465
196/196 - 43s - loss: 27.2752 - MinusLogProbMetric: 27.2752 - val_loss: 28.3823 - val_MinusLogProbMetric: 28.3823 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 681/1000
2023-10-26 01:10:15.887 
Epoch 681/1000 
	 loss: 27.2768, MinusLogProbMetric: 27.2768, val_loss: 28.3688, val_MinusLogProbMetric: 28.3688

Epoch 681: val_loss did not improve from 28.36465
196/196 - 43s - loss: 27.2768 - MinusLogProbMetric: 27.2768 - val_loss: 28.3688 - val_MinusLogProbMetric: 28.3688 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 682/1000
2023-10-26 01:10:58.207 
Epoch 682/1000 
	 loss: 27.2777, MinusLogProbMetric: 27.2777, val_loss: 28.3907, val_MinusLogProbMetric: 28.3907

Epoch 682: val_loss did not improve from 28.36465
196/196 - 42s - loss: 27.2777 - MinusLogProbMetric: 27.2777 - val_loss: 28.3907 - val_MinusLogProbMetric: 28.3907 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 683/1000
2023-10-26 01:11:41.090 
Epoch 683/1000 
	 loss: 27.2747, MinusLogProbMetric: 27.2747, val_loss: 28.3701, val_MinusLogProbMetric: 28.3701

Epoch 683: val_loss did not improve from 28.36465
196/196 - 43s - loss: 27.2747 - MinusLogProbMetric: 27.2747 - val_loss: 28.3701 - val_MinusLogProbMetric: 28.3701 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 684/1000
2023-10-26 01:12:23.860 
Epoch 684/1000 
	 loss: 27.2761, MinusLogProbMetric: 27.2761, val_loss: 28.3642, val_MinusLogProbMetric: 28.3642

Epoch 684: val_loss improved from 28.36465 to 28.36418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.2761 - MinusLogProbMetric: 27.2761 - val_loss: 28.3642 - val_MinusLogProbMetric: 28.3642 - lr: 2.0833e-05 - 44s/epoch - 222ms/step
Epoch 685/1000
2023-10-26 01:13:07.087 
Epoch 685/1000 
	 loss: 27.2756, MinusLogProbMetric: 27.2756, val_loss: 28.3735, val_MinusLogProbMetric: 28.3735

Epoch 685: val_loss did not improve from 28.36418
196/196 - 42s - loss: 27.2756 - MinusLogProbMetric: 27.2756 - val_loss: 28.3735 - val_MinusLogProbMetric: 28.3735 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 686/1000
2023-10-26 01:13:49.668 
Epoch 686/1000 
	 loss: 27.2755, MinusLogProbMetric: 27.2755, val_loss: 28.3676, val_MinusLogProbMetric: 28.3676

Epoch 686: val_loss did not improve from 28.36418
196/196 - 43s - loss: 27.2755 - MinusLogProbMetric: 27.2755 - val_loss: 28.3676 - val_MinusLogProbMetric: 28.3676 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 687/1000
2023-10-26 01:14:32.572 
Epoch 687/1000 
	 loss: 27.2763, MinusLogProbMetric: 27.2763, val_loss: 28.3876, val_MinusLogProbMetric: 28.3876

Epoch 687: val_loss did not improve from 28.36418
196/196 - 43s - loss: 27.2763 - MinusLogProbMetric: 27.2763 - val_loss: 28.3876 - val_MinusLogProbMetric: 28.3876 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 688/1000
2023-10-26 01:15:15.239 
Epoch 688/1000 
	 loss: 27.2781, MinusLogProbMetric: 27.2781, val_loss: 28.3767, val_MinusLogProbMetric: 28.3767

Epoch 688: val_loss did not improve from 28.36418
196/196 - 43s - loss: 27.2781 - MinusLogProbMetric: 27.2781 - val_loss: 28.3767 - val_MinusLogProbMetric: 28.3767 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 689/1000
2023-10-26 01:15:57.995 
Epoch 689/1000 
	 loss: 27.2768, MinusLogProbMetric: 27.2768, val_loss: 28.3640, val_MinusLogProbMetric: 28.3640

Epoch 689: val_loss improved from 28.36418 to 28.36405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.2768 - MinusLogProbMetric: 27.2768 - val_loss: 28.3640 - val_MinusLogProbMetric: 28.3640 - lr: 2.0833e-05 - 44s/epoch - 223ms/step
Epoch 690/1000
2023-10-26 01:16:41.663 
Epoch 690/1000 
	 loss: 27.2743, MinusLogProbMetric: 27.2743, val_loss: 28.3567, val_MinusLogProbMetric: 28.3567

Epoch 690: val_loss improved from 28.36405 to 28.35672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.2743 - MinusLogProbMetric: 27.2743 - val_loss: 28.3567 - val_MinusLogProbMetric: 28.3567 - lr: 2.0833e-05 - 44s/epoch - 222ms/step
Epoch 691/1000
2023-10-26 01:17:25.406 
Epoch 691/1000 
	 loss: 27.2752, MinusLogProbMetric: 27.2752, val_loss: 28.3681, val_MinusLogProbMetric: 28.3681

Epoch 691: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2752 - MinusLogProbMetric: 27.2752 - val_loss: 28.3681 - val_MinusLogProbMetric: 28.3681 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 692/1000
2023-10-26 01:18:07.714 
Epoch 692/1000 
	 loss: 27.2741, MinusLogProbMetric: 27.2741, val_loss: 28.3715, val_MinusLogProbMetric: 28.3715

Epoch 692: val_loss did not improve from 28.35672
196/196 - 42s - loss: 27.2741 - MinusLogProbMetric: 27.2741 - val_loss: 28.3715 - val_MinusLogProbMetric: 28.3715 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 693/1000
2023-10-26 01:18:50.681 
Epoch 693/1000 
	 loss: 27.2740, MinusLogProbMetric: 27.2740, val_loss: 28.3855, val_MinusLogProbMetric: 28.3855

Epoch 693: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2740 - MinusLogProbMetric: 27.2740 - val_loss: 28.3855 - val_MinusLogProbMetric: 28.3855 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 694/1000
2023-10-26 01:19:33.241 
Epoch 694/1000 
	 loss: 27.2729, MinusLogProbMetric: 27.2729, val_loss: 28.3705, val_MinusLogProbMetric: 28.3705

Epoch 694: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2729 - MinusLogProbMetric: 27.2729 - val_loss: 28.3705 - val_MinusLogProbMetric: 28.3705 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 695/1000
2023-10-26 01:20:15.990 
Epoch 695/1000 
	 loss: 27.2721, MinusLogProbMetric: 27.2721, val_loss: 28.3677, val_MinusLogProbMetric: 28.3677

Epoch 695: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2721 - MinusLogProbMetric: 27.2721 - val_loss: 28.3677 - val_MinusLogProbMetric: 28.3677 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 696/1000
2023-10-26 01:20:58.958 
Epoch 696/1000 
	 loss: 27.2761, MinusLogProbMetric: 27.2761, val_loss: 28.3631, val_MinusLogProbMetric: 28.3631

Epoch 696: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2761 - MinusLogProbMetric: 27.2761 - val_loss: 28.3631 - val_MinusLogProbMetric: 28.3631 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 697/1000
2023-10-26 01:21:41.519 
Epoch 697/1000 
	 loss: 27.2689, MinusLogProbMetric: 27.2689, val_loss: 28.3698, val_MinusLogProbMetric: 28.3698

Epoch 697: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2689 - MinusLogProbMetric: 27.2689 - val_loss: 28.3698 - val_MinusLogProbMetric: 28.3698 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 698/1000
2023-10-26 01:22:24.037 
Epoch 698/1000 
	 loss: 27.2731, MinusLogProbMetric: 27.2731, val_loss: 28.3664, val_MinusLogProbMetric: 28.3664

Epoch 698: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2731 - MinusLogProbMetric: 27.2731 - val_loss: 28.3664 - val_MinusLogProbMetric: 28.3664 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 699/1000
2023-10-26 01:23:06.756 
Epoch 699/1000 
	 loss: 27.2729, MinusLogProbMetric: 27.2729, val_loss: 28.3866, val_MinusLogProbMetric: 28.3866

Epoch 699: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2729 - MinusLogProbMetric: 27.2729 - val_loss: 28.3866 - val_MinusLogProbMetric: 28.3866 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 700/1000
2023-10-26 01:23:49.117 
Epoch 700/1000 
	 loss: 27.2733, MinusLogProbMetric: 27.2733, val_loss: 28.3667, val_MinusLogProbMetric: 28.3667

Epoch 700: val_loss did not improve from 28.35672
196/196 - 42s - loss: 27.2733 - MinusLogProbMetric: 27.2733 - val_loss: 28.3667 - val_MinusLogProbMetric: 28.3667 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 701/1000
2023-10-26 01:24:32.248 
Epoch 701/1000 
	 loss: 27.2715, MinusLogProbMetric: 27.2715, val_loss: 28.3725, val_MinusLogProbMetric: 28.3725

Epoch 701: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2715 - MinusLogProbMetric: 27.2715 - val_loss: 28.3725 - val_MinusLogProbMetric: 28.3725 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 702/1000
2023-10-26 01:25:14.701 
Epoch 702/1000 
	 loss: 27.2715, MinusLogProbMetric: 27.2715, val_loss: 28.3793, val_MinusLogProbMetric: 28.3793

Epoch 702: val_loss did not improve from 28.35672
196/196 - 42s - loss: 27.2715 - MinusLogProbMetric: 27.2715 - val_loss: 28.3793 - val_MinusLogProbMetric: 28.3793 - lr: 2.0833e-05 - 42s/epoch - 217ms/step
Epoch 703/1000
2023-10-26 01:25:57.418 
Epoch 703/1000 
	 loss: 27.2712, MinusLogProbMetric: 27.2712, val_loss: 28.3655, val_MinusLogProbMetric: 28.3655

Epoch 703: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2712 - MinusLogProbMetric: 27.2712 - val_loss: 28.3655 - val_MinusLogProbMetric: 28.3655 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 704/1000
2023-10-26 01:26:39.611 
Epoch 704/1000 
	 loss: 27.2708, MinusLogProbMetric: 27.2708, val_loss: 28.3847, val_MinusLogProbMetric: 28.3847

Epoch 704: val_loss did not improve from 28.35672
196/196 - 42s - loss: 27.2708 - MinusLogProbMetric: 27.2708 - val_loss: 28.3847 - val_MinusLogProbMetric: 28.3847 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 705/1000
2023-10-26 01:27:22.681 
Epoch 705/1000 
	 loss: 27.2750, MinusLogProbMetric: 27.2750, val_loss: 28.3697, val_MinusLogProbMetric: 28.3697

Epoch 705: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2750 - MinusLogProbMetric: 27.2750 - val_loss: 28.3697 - val_MinusLogProbMetric: 28.3697 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 706/1000
2023-10-26 01:28:05.497 
Epoch 706/1000 
	 loss: 27.2711, MinusLogProbMetric: 27.2711, val_loss: 28.3715, val_MinusLogProbMetric: 28.3715

Epoch 706: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2711 - MinusLogProbMetric: 27.2711 - val_loss: 28.3715 - val_MinusLogProbMetric: 28.3715 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 707/1000
2023-10-26 01:28:47.643 
Epoch 707/1000 
	 loss: 27.2717, MinusLogProbMetric: 27.2717, val_loss: 28.3746, val_MinusLogProbMetric: 28.3746

Epoch 707: val_loss did not improve from 28.35672
196/196 - 42s - loss: 27.2717 - MinusLogProbMetric: 27.2717 - val_loss: 28.3746 - val_MinusLogProbMetric: 28.3746 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 708/1000
2023-10-26 01:29:30.327 
Epoch 708/1000 
	 loss: 27.2711, MinusLogProbMetric: 27.2711, val_loss: 28.3680, val_MinusLogProbMetric: 28.3680

Epoch 708: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2711 - MinusLogProbMetric: 27.2711 - val_loss: 28.3680 - val_MinusLogProbMetric: 28.3680 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 709/1000
2023-10-26 01:30:13.583 
Epoch 709/1000 
	 loss: 27.2708, MinusLogProbMetric: 27.2708, val_loss: 28.3662, val_MinusLogProbMetric: 28.3662

Epoch 709: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2708 - MinusLogProbMetric: 27.2708 - val_loss: 28.3662 - val_MinusLogProbMetric: 28.3662 - lr: 2.0833e-05 - 43s/epoch - 221ms/step
Epoch 710/1000
2023-10-26 01:30:56.605 
Epoch 710/1000 
	 loss: 27.2733, MinusLogProbMetric: 27.2733, val_loss: 28.3768, val_MinusLogProbMetric: 28.3768

Epoch 710: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2733 - MinusLogProbMetric: 27.2733 - val_loss: 28.3768 - val_MinusLogProbMetric: 28.3768 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 711/1000
2023-10-26 01:31:39.594 
Epoch 711/1000 
	 loss: 27.2738, MinusLogProbMetric: 27.2738, val_loss: 28.3679, val_MinusLogProbMetric: 28.3679

Epoch 711: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2738 - MinusLogProbMetric: 27.2738 - val_loss: 28.3679 - val_MinusLogProbMetric: 28.3679 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 712/1000
2023-10-26 01:32:22.415 
Epoch 712/1000 
	 loss: 27.2714, MinusLogProbMetric: 27.2714, val_loss: 28.3799, val_MinusLogProbMetric: 28.3799

Epoch 712: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2714 - MinusLogProbMetric: 27.2714 - val_loss: 28.3799 - val_MinusLogProbMetric: 28.3799 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 713/1000
2023-10-26 01:33:05.369 
Epoch 713/1000 
	 loss: 27.2741, MinusLogProbMetric: 27.2741, val_loss: 28.3640, val_MinusLogProbMetric: 28.3640

Epoch 713: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2741 - MinusLogProbMetric: 27.2741 - val_loss: 28.3640 - val_MinusLogProbMetric: 28.3640 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 714/1000
2023-10-26 01:33:48.183 
Epoch 714/1000 
	 loss: 27.2696, MinusLogProbMetric: 27.2696, val_loss: 28.3795, val_MinusLogProbMetric: 28.3795

Epoch 714: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2696 - MinusLogProbMetric: 27.2696 - val_loss: 28.3795 - val_MinusLogProbMetric: 28.3795 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 715/1000
2023-10-26 01:34:30.603 
Epoch 715/1000 
	 loss: 27.2729, MinusLogProbMetric: 27.2729, val_loss: 28.3831, val_MinusLogProbMetric: 28.3831

Epoch 715: val_loss did not improve from 28.35672
196/196 - 42s - loss: 27.2729 - MinusLogProbMetric: 27.2729 - val_loss: 28.3831 - val_MinusLogProbMetric: 28.3831 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 716/1000
2023-10-26 01:35:13.437 
Epoch 716/1000 
	 loss: 27.2722, MinusLogProbMetric: 27.2722, val_loss: 28.3667, val_MinusLogProbMetric: 28.3667

Epoch 716: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2722 - MinusLogProbMetric: 27.2722 - val_loss: 28.3667 - val_MinusLogProbMetric: 28.3667 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 717/1000
2023-10-26 01:35:56.009 
Epoch 717/1000 
	 loss: 27.2731, MinusLogProbMetric: 27.2731, val_loss: 28.3652, val_MinusLogProbMetric: 28.3652

Epoch 717: val_loss did not improve from 28.35672
196/196 - 43s - loss: 27.2731 - MinusLogProbMetric: 27.2731 - val_loss: 28.3652 - val_MinusLogProbMetric: 28.3652 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 718/1000
2023-10-26 01:36:38.646 
Epoch 718/1000 
	 loss: 27.2743, MinusLogProbMetric: 27.2743, val_loss: 28.3557, val_MinusLogProbMetric: 28.3557

Epoch 718: val_loss improved from 28.35672 to 28.35573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.2743 - MinusLogProbMetric: 27.2743 - val_loss: 28.3557 - val_MinusLogProbMetric: 28.3557 - lr: 2.0833e-05 - 43s/epoch - 222ms/step
Epoch 719/1000
2023-10-26 01:37:22.076 
Epoch 719/1000 
	 loss: 27.2718, MinusLogProbMetric: 27.2718, val_loss: 28.3706, val_MinusLogProbMetric: 28.3706

Epoch 719: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2718 - MinusLogProbMetric: 27.2718 - val_loss: 28.3706 - val_MinusLogProbMetric: 28.3706 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 720/1000
2023-10-26 01:37:59.481 
Epoch 720/1000 
	 loss: 27.2689, MinusLogProbMetric: 27.2689, val_loss: 28.3685, val_MinusLogProbMetric: 28.3685

Epoch 720: val_loss did not improve from 28.35573
196/196 - 37s - loss: 27.2689 - MinusLogProbMetric: 27.2689 - val_loss: 28.3685 - val_MinusLogProbMetric: 28.3685 - lr: 2.0833e-05 - 37s/epoch - 191ms/step
Epoch 721/1000
2023-10-26 01:38:37.919 
Epoch 721/1000 
	 loss: 27.2731, MinusLogProbMetric: 27.2731, val_loss: 28.3621, val_MinusLogProbMetric: 28.3621

Epoch 721: val_loss did not improve from 28.35573
196/196 - 38s - loss: 27.2731 - MinusLogProbMetric: 27.2731 - val_loss: 28.3621 - val_MinusLogProbMetric: 28.3621 - lr: 2.0833e-05 - 38s/epoch - 196ms/step
Epoch 722/1000
2023-10-26 01:39:18.245 
Epoch 722/1000 
	 loss: 27.2696, MinusLogProbMetric: 27.2696, val_loss: 28.3757, val_MinusLogProbMetric: 28.3757

Epoch 722: val_loss did not improve from 28.35573
196/196 - 40s - loss: 27.2696 - MinusLogProbMetric: 27.2696 - val_loss: 28.3757 - val_MinusLogProbMetric: 28.3757 - lr: 2.0833e-05 - 40s/epoch - 206ms/step
Epoch 723/1000
2023-10-26 01:39:57.001 
Epoch 723/1000 
	 loss: 27.2720, MinusLogProbMetric: 27.2720, val_loss: 28.3831, val_MinusLogProbMetric: 28.3831

Epoch 723: val_loss did not improve from 28.35573
196/196 - 39s - loss: 27.2720 - MinusLogProbMetric: 27.2720 - val_loss: 28.3831 - val_MinusLogProbMetric: 28.3831 - lr: 2.0833e-05 - 39s/epoch - 198ms/step
Epoch 724/1000
2023-10-26 01:40:39.436 
Epoch 724/1000 
	 loss: 27.2693, MinusLogProbMetric: 27.2693, val_loss: 28.3771, val_MinusLogProbMetric: 28.3771

Epoch 724: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2693 - MinusLogProbMetric: 27.2693 - val_loss: 28.3771 - val_MinusLogProbMetric: 28.3771 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 725/1000
2023-10-26 01:41:21.956 
Epoch 725/1000 
	 loss: 27.2718, MinusLogProbMetric: 27.2718, val_loss: 28.3631, val_MinusLogProbMetric: 28.3631

Epoch 725: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2718 - MinusLogProbMetric: 27.2718 - val_loss: 28.3631 - val_MinusLogProbMetric: 28.3631 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 726/1000
2023-10-26 01:42:04.472 
Epoch 726/1000 
	 loss: 27.2697, MinusLogProbMetric: 27.2697, val_loss: 28.3874, val_MinusLogProbMetric: 28.3874

Epoch 726: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2697 - MinusLogProbMetric: 27.2697 - val_loss: 28.3874 - val_MinusLogProbMetric: 28.3874 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 727/1000
2023-10-26 01:42:46.780 
Epoch 727/1000 
	 loss: 27.2736, MinusLogProbMetric: 27.2736, val_loss: 28.3680, val_MinusLogProbMetric: 28.3680

Epoch 727: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2736 - MinusLogProbMetric: 27.2736 - val_loss: 28.3680 - val_MinusLogProbMetric: 28.3680 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 728/1000
2023-10-26 01:43:29.919 
Epoch 728/1000 
	 loss: 27.2690, MinusLogProbMetric: 27.2690, val_loss: 28.3789, val_MinusLogProbMetric: 28.3789

Epoch 728: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2690 - MinusLogProbMetric: 27.2690 - val_loss: 28.3789 - val_MinusLogProbMetric: 28.3789 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 729/1000
2023-10-26 01:44:13.079 
Epoch 729/1000 
	 loss: 27.2724, MinusLogProbMetric: 27.2724, val_loss: 28.3673, val_MinusLogProbMetric: 28.3673

Epoch 729: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2724 - MinusLogProbMetric: 27.2724 - val_loss: 28.3673 - val_MinusLogProbMetric: 28.3673 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 730/1000
2023-10-26 01:44:55.766 
Epoch 730/1000 
	 loss: 27.2726, MinusLogProbMetric: 27.2726, val_loss: 28.3834, val_MinusLogProbMetric: 28.3834

Epoch 730: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2726 - MinusLogProbMetric: 27.2726 - val_loss: 28.3834 - val_MinusLogProbMetric: 28.3834 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 731/1000
2023-10-26 01:45:38.358 
Epoch 731/1000 
	 loss: 27.2712, MinusLogProbMetric: 27.2712, val_loss: 28.3750, val_MinusLogProbMetric: 28.3750

Epoch 731: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2712 - MinusLogProbMetric: 27.2712 - val_loss: 28.3750 - val_MinusLogProbMetric: 28.3750 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 732/1000
2023-10-26 01:46:21.211 
Epoch 732/1000 
	 loss: 27.2679, MinusLogProbMetric: 27.2679, val_loss: 28.3679, val_MinusLogProbMetric: 28.3679

Epoch 732: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2679 - MinusLogProbMetric: 27.2679 - val_loss: 28.3679 - val_MinusLogProbMetric: 28.3679 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 733/1000
2023-10-26 01:47:03.736 
Epoch 733/1000 
	 loss: 27.2724, MinusLogProbMetric: 27.2724, val_loss: 28.3747, val_MinusLogProbMetric: 28.3747

Epoch 733: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2724 - MinusLogProbMetric: 27.2724 - val_loss: 28.3747 - val_MinusLogProbMetric: 28.3747 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 734/1000
2023-10-26 01:47:46.363 
Epoch 734/1000 
	 loss: 27.2688, MinusLogProbMetric: 27.2688, val_loss: 28.3664, val_MinusLogProbMetric: 28.3664

Epoch 734: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2688 - MinusLogProbMetric: 27.2688 - val_loss: 28.3664 - val_MinusLogProbMetric: 28.3664 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 735/1000
2023-10-26 01:48:28.983 
Epoch 735/1000 
	 loss: 27.2678, MinusLogProbMetric: 27.2678, val_loss: 28.3743, val_MinusLogProbMetric: 28.3743

Epoch 735: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2678 - MinusLogProbMetric: 27.2678 - val_loss: 28.3743 - val_MinusLogProbMetric: 28.3743 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 736/1000
2023-10-26 01:49:12.116 
Epoch 736/1000 
	 loss: 27.2672, MinusLogProbMetric: 27.2672, val_loss: 28.4050, val_MinusLogProbMetric: 28.4050

Epoch 736: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2672 - MinusLogProbMetric: 27.2672 - val_loss: 28.4050 - val_MinusLogProbMetric: 28.4050 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 737/1000
2023-10-26 01:49:54.724 
Epoch 737/1000 
	 loss: 27.2673, MinusLogProbMetric: 27.2673, val_loss: 28.3694, val_MinusLogProbMetric: 28.3694

Epoch 737: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2673 - MinusLogProbMetric: 27.2673 - val_loss: 28.3694 - val_MinusLogProbMetric: 28.3694 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 738/1000
2023-10-26 01:50:37.347 
Epoch 738/1000 
	 loss: 27.2711, MinusLogProbMetric: 27.2711, val_loss: 28.3797, val_MinusLogProbMetric: 28.3797

Epoch 738: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2711 - MinusLogProbMetric: 27.2711 - val_loss: 28.3797 - val_MinusLogProbMetric: 28.3797 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 739/1000
2023-10-26 01:51:19.918 
Epoch 739/1000 
	 loss: 27.2703, MinusLogProbMetric: 27.2703, val_loss: 28.3808, val_MinusLogProbMetric: 28.3808

Epoch 739: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2703 - MinusLogProbMetric: 27.2703 - val_loss: 28.3808 - val_MinusLogProbMetric: 28.3808 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 740/1000
2023-10-26 01:52:02.499 
Epoch 740/1000 
	 loss: 27.2699, MinusLogProbMetric: 27.2699, val_loss: 28.3665, val_MinusLogProbMetric: 28.3665

Epoch 740: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2699 - MinusLogProbMetric: 27.2699 - val_loss: 28.3665 - val_MinusLogProbMetric: 28.3665 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 741/1000
2023-10-26 01:52:44.929 
Epoch 741/1000 
	 loss: 27.2676, MinusLogProbMetric: 27.2676, val_loss: 28.3971, val_MinusLogProbMetric: 28.3971

Epoch 741: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2676 - MinusLogProbMetric: 27.2676 - val_loss: 28.3971 - val_MinusLogProbMetric: 28.3971 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 742/1000
2023-10-26 01:53:27.373 
Epoch 742/1000 
	 loss: 27.2663, MinusLogProbMetric: 27.2663, val_loss: 28.3646, val_MinusLogProbMetric: 28.3646

Epoch 742: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2663 - MinusLogProbMetric: 27.2663 - val_loss: 28.3646 - val_MinusLogProbMetric: 28.3646 - lr: 2.0833e-05 - 42s/epoch - 217ms/step
Epoch 743/1000
2023-10-26 01:54:10.165 
Epoch 743/1000 
	 loss: 27.2705, MinusLogProbMetric: 27.2705, val_loss: 28.3671, val_MinusLogProbMetric: 28.3671

Epoch 743: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2705 - MinusLogProbMetric: 27.2705 - val_loss: 28.3671 - val_MinusLogProbMetric: 28.3671 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 744/1000
2023-10-26 01:54:53.016 
Epoch 744/1000 
	 loss: 27.2697, MinusLogProbMetric: 27.2697, val_loss: 28.3663, val_MinusLogProbMetric: 28.3663

Epoch 744: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2697 - MinusLogProbMetric: 27.2697 - val_loss: 28.3663 - val_MinusLogProbMetric: 28.3663 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 745/1000
2023-10-26 01:55:35.597 
Epoch 745/1000 
	 loss: 27.2661, MinusLogProbMetric: 27.2661, val_loss: 28.3735, val_MinusLogProbMetric: 28.3735

Epoch 745: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2661 - MinusLogProbMetric: 27.2661 - val_loss: 28.3735 - val_MinusLogProbMetric: 28.3735 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 746/1000
2023-10-26 01:56:17.826 
Epoch 746/1000 
	 loss: 27.2683, MinusLogProbMetric: 27.2683, val_loss: 28.3708, val_MinusLogProbMetric: 28.3708

Epoch 746: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2683 - MinusLogProbMetric: 27.2683 - val_loss: 28.3708 - val_MinusLogProbMetric: 28.3708 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 747/1000
2023-10-26 01:57:00.324 
Epoch 747/1000 
	 loss: 27.2675, MinusLogProbMetric: 27.2675, val_loss: 28.3857, val_MinusLogProbMetric: 28.3857

Epoch 747: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2675 - MinusLogProbMetric: 27.2675 - val_loss: 28.3857 - val_MinusLogProbMetric: 28.3857 - lr: 2.0833e-05 - 42s/epoch - 217ms/step
Epoch 748/1000
2023-10-26 01:57:43.045 
Epoch 748/1000 
	 loss: 27.2654, MinusLogProbMetric: 27.2654, val_loss: 28.3783, val_MinusLogProbMetric: 28.3783

Epoch 748: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2654 - MinusLogProbMetric: 27.2654 - val_loss: 28.3783 - val_MinusLogProbMetric: 28.3783 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 749/1000
2023-10-26 01:58:26.044 
Epoch 749/1000 
	 loss: 27.2660, MinusLogProbMetric: 27.2660, val_loss: 28.3690, val_MinusLogProbMetric: 28.3690

Epoch 749: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2660 - MinusLogProbMetric: 27.2660 - val_loss: 28.3690 - val_MinusLogProbMetric: 28.3690 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 750/1000
2023-10-26 01:59:08.469 
Epoch 750/1000 
	 loss: 27.2669, MinusLogProbMetric: 27.2669, val_loss: 28.3734, val_MinusLogProbMetric: 28.3734

Epoch 750: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2669 - MinusLogProbMetric: 27.2669 - val_loss: 28.3734 - val_MinusLogProbMetric: 28.3734 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 751/1000
2023-10-26 01:59:50.555 
Epoch 751/1000 
	 loss: 27.2686, MinusLogProbMetric: 27.2686, val_loss: 28.3626, val_MinusLogProbMetric: 28.3626

Epoch 751: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2686 - MinusLogProbMetric: 27.2686 - val_loss: 28.3626 - val_MinusLogProbMetric: 28.3626 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 752/1000
2023-10-26 02:00:32.894 
Epoch 752/1000 
	 loss: 27.2657, MinusLogProbMetric: 27.2657, val_loss: 28.3693, val_MinusLogProbMetric: 28.3693

Epoch 752: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2657 - MinusLogProbMetric: 27.2657 - val_loss: 28.3693 - val_MinusLogProbMetric: 28.3693 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 753/1000
2023-10-26 02:01:15.067 
Epoch 753/1000 
	 loss: 27.2655, MinusLogProbMetric: 27.2655, val_loss: 28.3893, val_MinusLogProbMetric: 28.3893

Epoch 753: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2655 - MinusLogProbMetric: 27.2655 - val_loss: 28.3893 - val_MinusLogProbMetric: 28.3893 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 754/1000
2023-10-26 02:01:57.667 
Epoch 754/1000 
	 loss: 27.2682, MinusLogProbMetric: 27.2682, val_loss: 28.3852, val_MinusLogProbMetric: 28.3852

Epoch 754: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2682 - MinusLogProbMetric: 27.2682 - val_loss: 28.3852 - val_MinusLogProbMetric: 28.3852 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 755/1000
2023-10-26 02:02:39.975 
Epoch 755/1000 
	 loss: 27.2674, MinusLogProbMetric: 27.2674, val_loss: 28.3656, val_MinusLogProbMetric: 28.3656

Epoch 755: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2674 - MinusLogProbMetric: 27.2674 - val_loss: 28.3656 - val_MinusLogProbMetric: 28.3656 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 756/1000
2023-10-26 02:03:22.624 
Epoch 756/1000 
	 loss: 27.2646, MinusLogProbMetric: 27.2646, val_loss: 28.3703, val_MinusLogProbMetric: 28.3703

Epoch 756: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2646 - MinusLogProbMetric: 27.2646 - val_loss: 28.3703 - val_MinusLogProbMetric: 28.3703 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 757/1000
2023-10-26 02:04:04.623 
Epoch 757/1000 
	 loss: 27.2681, MinusLogProbMetric: 27.2681, val_loss: 28.3780, val_MinusLogProbMetric: 28.3780

Epoch 757: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2681 - MinusLogProbMetric: 27.2681 - val_loss: 28.3780 - val_MinusLogProbMetric: 28.3780 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 758/1000
2023-10-26 02:04:47.057 
Epoch 758/1000 
	 loss: 27.2707, MinusLogProbMetric: 27.2707, val_loss: 28.3921, val_MinusLogProbMetric: 28.3921

Epoch 758: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2707 - MinusLogProbMetric: 27.2707 - val_loss: 28.3921 - val_MinusLogProbMetric: 28.3921 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 759/1000
2023-10-26 02:05:29.564 
Epoch 759/1000 
	 loss: 27.2672, MinusLogProbMetric: 27.2672, val_loss: 28.3776, val_MinusLogProbMetric: 28.3776

Epoch 759: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2672 - MinusLogProbMetric: 27.2672 - val_loss: 28.3776 - val_MinusLogProbMetric: 28.3776 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 760/1000
2023-10-26 02:06:12.000 
Epoch 760/1000 
	 loss: 27.2668, MinusLogProbMetric: 27.2668, val_loss: 28.3867, val_MinusLogProbMetric: 28.3867

Epoch 760: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2668 - MinusLogProbMetric: 27.2668 - val_loss: 28.3867 - val_MinusLogProbMetric: 28.3867 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 761/1000
2023-10-26 02:06:54.335 
Epoch 761/1000 
	 loss: 27.2742, MinusLogProbMetric: 27.2742, val_loss: 28.3743, val_MinusLogProbMetric: 28.3743

Epoch 761: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2742 - MinusLogProbMetric: 27.2742 - val_loss: 28.3743 - val_MinusLogProbMetric: 28.3743 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 762/1000
2023-10-26 02:07:36.989 
Epoch 762/1000 
	 loss: 27.2644, MinusLogProbMetric: 27.2644, val_loss: 28.3740, val_MinusLogProbMetric: 28.3740

Epoch 762: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2644 - MinusLogProbMetric: 27.2644 - val_loss: 28.3740 - val_MinusLogProbMetric: 28.3740 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 763/1000
2023-10-26 02:08:19.394 
Epoch 763/1000 
	 loss: 27.2685, MinusLogProbMetric: 27.2685, val_loss: 28.3759, val_MinusLogProbMetric: 28.3759

Epoch 763: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2685 - MinusLogProbMetric: 27.2685 - val_loss: 28.3759 - val_MinusLogProbMetric: 28.3759 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 764/1000
2023-10-26 02:09:02.225 
Epoch 764/1000 
	 loss: 27.2674, MinusLogProbMetric: 27.2674, val_loss: 28.3710, val_MinusLogProbMetric: 28.3710

Epoch 764: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2674 - MinusLogProbMetric: 27.2674 - val_loss: 28.3710 - val_MinusLogProbMetric: 28.3710 - lr: 2.0833e-05 - 43s/epoch - 219ms/step
Epoch 765/1000
2023-10-26 02:09:44.747 
Epoch 765/1000 
	 loss: 27.2722, MinusLogProbMetric: 27.2722, val_loss: 28.3864, val_MinusLogProbMetric: 28.3864

Epoch 765: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2722 - MinusLogProbMetric: 27.2722 - val_loss: 28.3864 - val_MinusLogProbMetric: 28.3864 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 766/1000
2023-10-26 02:10:27.436 
Epoch 766/1000 
	 loss: 27.2663, MinusLogProbMetric: 27.2663, val_loss: 28.3709, val_MinusLogProbMetric: 28.3709

Epoch 766: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2663 - MinusLogProbMetric: 27.2663 - val_loss: 28.3709 - val_MinusLogProbMetric: 28.3709 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 767/1000
2023-10-26 02:11:10.150 
Epoch 767/1000 
	 loss: 27.2685, MinusLogProbMetric: 27.2685, val_loss: 28.3910, val_MinusLogProbMetric: 28.3910

Epoch 767: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2685 - MinusLogProbMetric: 27.2685 - val_loss: 28.3910 - val_MinusLogProbMetric: 28.3910 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 768/1000
2023-10-26 02:11:52.580 
Epoch 768/1000 
	 loss: 27.2673, MinusLogProbMetric: 27.2673, val_loss: 28.3987, val_MinusLogProbMetric: 28.3987

Epoch 768: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2673 - MinusLogProbMetric: 27.2673 - val_loss: 28.3987 - val_MinusLogProbMetric: 28.3987 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 769/1000
2023-10-26 02:12:35.516 
Epoch 769/1000 
	 loss: 27.2500, MinusLogProbMetric: 27.2500, val_loss: 28.3701, val_MinusLogProbMetric: 28.3701

Epoch 769: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2500 - MinusLogProbMetric: 27.2500 - val_loss: 28.3701 - val_MinusLogProbMetric: 28.3701 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 770/1000
2023-10-26 02:13:18.461 
Epoch 770/1000 
	 loss: 27.2485, MinusLogProbMetric: 27.2485, val_loss: 28.3619, val_MinusLogProbMetric: 28.3619

Epoch 770: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2485 - MinusLogProbMetric: 27.2485 - val_loss: 28.3619 - val_MinusLogProbMetric: 28.3619 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 771/1000
2023-10-26 02:14:01.036 
Epoch 771/1000 
	 loss: 27.2473, MinusLogProbMetric: 27.2473, val_loss: 28.3607, val_MinusLogProbMetric: 28.3607

Epoch 771: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2473 - MinusLogProbMetric: 27.2473 - val_loss: 28.3607 - val_MinusLogProbMetric: 28.3607 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 772/1000
2023-10-26 02:14:43.592 
Epoch 772/1000 
	 loss: 27.2471, MinusLogProbMetric: 27.2471, val_loss: 28.3693, val_MinusLogProbMetric: 28.3693

Epoch 772: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2471 - MinusLogProbMetric: 27.2471 - val_loss: 28.3693 - val_MinusLogProbMetric: 28.3693 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 773/1000
2023-10-26 02:15:26.838 
Epoch 773/1000 
	 loss: 27.2462, MinusLogProbMetric: 27.2462, val_loss: 28.3648, val_MinusLogProbMetric: 28.3648

Epoch 773: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2462 - MinusLogProbMetric: 27.2462 - val_loss: 28.3648 - val_MinusLogProbMetric: 28.3648 - lr: 1.0417e-05 - 43s/epoch - 221ms/step
Epoch 774/1000
2023-10-26 02:16:09.991 
Epoch 774/1000 
	 loss: 27.2468, MinusLogProbMetric: 27.2468, val_loss: 28.3631, val_MinusLogProbMetric: 28.3631

Epoch 774: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2468 - MinusLogProbMetric: 27.2468 - val_loss: 28.3631 - val_MinusLogProbMetric: 28.3631 - lr: 1.0417e-05 - 43s/epoch - 220ms/step
Epoch 775/1000
2023-10-26 02:16:53.022 
Epoch 775/1000 
	 loss: 27.2477, MinusLogProbMetric: 27.2477, val_loss: 28.3635, val_MinusLogProbMetric: 28.3635

Epoch 775: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2477 - MinusLogProbMetric: 27.2477 - val_loss: 28.3635 - val_MinusLogProbMetric: 28.3635 - lr: 1.0417e-05 - 43s/epoch - 220ms/step
Epoch 776/1000
2023-10-26 02:17:35.244 
Epoch 776/1000 
	 loss: 27.2485, MinusLogProbMetric: 27.2485, val_loss: 28.3642, val_MinusLogProbMetric: 28.3642

Epoch 776: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2485 - MinusLogProbMetric: 27.2485 - val_loss: 28.3642 - val_MinusLogProbMetric: 28.3642 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 777/1000
2023-10-26 02:18:18.009 
Epoch 777/1000 
	 loss: 27.2465, MinusLogProbMetric: 27.2465, val_loss: 28.3629, val_MinusLogProbMetric: 28.3629

Epoch 777: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2465 - MinusLogProbMetric: 27.2465 - val_loss: 28.3629 - val_MinusLogProbMetric: 28.3629 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 778/1000
2023-10-26 02:19:00.887 
Epoch 778/1000 
	 loss: 27.2472, MinusLogProbMetric: 27.2472, val_loss: 28.3612, val_MinusLogProbMetric: 28.3612

Epoch 778: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2472 - MinusLogProbMetric: 27.2472 - val_loss: 28.3612 - val_MinusLogProbMetric: 28.3612 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 779/1000
2023-10-26 02:19:43.165 
Epoch 779/1000 
	 loss: 27.2462, MinusLogProbMetric: 27.2462, val_loss: 28.3721, val_MinusLogProbMetric: 28.3721

Epoch 779: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2462 - MinusLogProbMetric: 27.2462 - val_loss: 28.3721 - val_MinusLogProbMetric: 28.3721 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 780/1000
2023-10-26 02:20:25.797 
Epoch 780/1000 
	 loss: 27.2469, MinusLogProbMetric: 27.2469, val_loss: 28.3599, val_MinusLogProbMetric: 28.3599

Epoch 780: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2469 - MinusLogProbMetric: 27.2469 - val_loss: 28.3599 - val_MinusLogProbMetric: 28.3599 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 781/1000
2023-10-26 02:21:08.450 
Epoch 781/1000 
	 loss: 27.2449, MinusLogProbMetric: 27.2449, val_loss: 28.3570, val_MinusLogProbMetric: 28.3570

Epoch 781: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2449 - MinusLogProbMetric: 27.2449 - val_loss: 28.3570 - val_MinusLogProbMetric: 28.3570 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 782/1000
2023-10-26 02:21:51.056 
Epoch 782/1000 
	 loss: 27.2473, MinusLogProbMetric: 27.2473, val_loss: 28.3618, val_MinusLogProbMetric: 28.3618

Epoch 782: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2473 - MinusLogProbMetric: 27.2473 - val_loss: 28.3618 - val_MinusLogProbMetric: 28.3618 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 783/1000
2023-10-26 02:22:33.727 
Epoch 783/1000 
	 loss: 27.2476, MinusLogProbMetric: 27.2476, val_loss: 28.3647, val_MinusLogProbMetric: 28.3647

Epoch 783: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2476 - MinusLogProbMetric: 27.2476 - val_loss: 28.3647 - val_MinusLogProbMetric: 28.3647 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 784/1000
2023-10-26 02:23:16.482 
Epoch 784/1000 
	 loss: 27.2456, MinusLogProbMetric: 27.2456, val_loss: 28.3655, val_MinusLogProbMetric: 28.3655

Epoch 784: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2456 - MinusLogProbMetric: 27.2456 - val_loss: 28.3655 - val_MinusLogProbMetric: 28.3655 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 785/1000
2023-10-26 02:23:58.741 
Epoch 785/1000 
	 loss: 27.2474, MinusLogProbMetric: 27.2474, val_loss: 28.3594, val_MinusLogProbMetric: 28.3594

Epoch 785: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2474 - MinusLogProbMetric: 27.2474 - val_loss: 28.3594 - val_MinusLogProbMetric: 28.3594 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 786/1000
2023-10-26 02:24:41.283 
Epoch 786/1000 
	 loss: 27.2471, MinusLogProbMetric: 27.2471, val_loss: 28.3598, val_MinusLogProbMetric: 28.3598

Epoch 786: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2471 - MinusLogProbMetric: 27.2471 - val_loss: 28.3598 - val_MinusLogProbMetric: 28.3598 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 787/1000
2023-10-26 02:25:23.994 
Epoch 787/1000 
	 loss: 27.2455, MinusLogProbMetric: 27.2455, val_loss: 28.3647, val_MinusLogProbMetric: 28.3647

Epoch 787: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2455 - MinusLogProbMetric: 27.2455 - val_loss: 28.3647 - val_MinusLogProbMetric: 28.3647 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 788/1000
2023-10-26 02:26:06.321 
Epoch 788/1000 
	 loss: 27.2467, MinusLogProbMetric: 27.2467, val_loss: 28.3632, val_MinusLogProbMetric: 28.3632

Epoch 788: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2467 - MinusLogProbMetric: 27.2467 - val_loss: 28.3632 - val_MinusLogProbMetric: 28.3632 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 789/1000
2023-10-26 02:26:48.425 
Epoch 789/1000 
	 loss: 27.2460, MinusLogProbMetric: 27.2460, val_loss: 28.3614, val_MinusLogProbMetric: 28.3614

Epoch 789: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2460 - MinusLogProbMetric: 27.2460 - val_loss: 28.3614 - val_MinusLogProbMetric: 28.3614 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 790/1000
2023-10-26 02:27:30.965 
Epoch 790/1000 
	 loss: 27.2477, MinusLogProbMetric: 27.2477, val_loss: 28.3715, val_MinusLogProbMetric: 28.3715

Epoch 790: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2477 - MinusLogProbMetric: 27.2477 - val_loss: 28.3715 - val_MinusLogProbMetric: 28.3715 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 791/1000
2023-10-26 02:28:13.148 
Epoch 791/1000 
	 loss: 27.2461, MinusLogProbMetric: 27.2461, val_loss: 28.3579, val_MinusLogProbMetric: 28.3579

Epoch 791: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2461 - MinusLogProbMetric: 27.2461 - val_loss: 28.3579 - val_MinusLogProbMetric: 28.3579 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 792/1000
2023-10-26 02:28:55.923 
Epoch 792/1000 
	 loss: 27.2464, MinusLogProbMetric: 27.2464, val_loss: 28.3667, val_MinusLogProbMetric: 28.3667

Epoch 792: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2464 - MinusLogProbMetric: 27.2464 - val_loss: 28.3667 - val_MinusLogProbMetric: 28.3667 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 793/1000
2023-10-26 02:29:38.762 
Epoch 793/1000 
	 loss: 27.2458, MinusLogProbMetric: 27.2458, val_loss: 28.3602, val_MinusLogProbMetric: 28.3602

Epoch 793: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2458 - MinusLogProbMetric: 27.2458 - val_loss: 28.3602 - val_MinusLogProbMetric: 28.3602 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 794/1000
2023-10-26 02:30:21.275 
Epoch 794/1000 
	 loss: 27.2467, MinusLogProbMetric: 27.2467, val_loss: 28.3602, val_MinusLogProbMetric: 28.3602

Epoch 794: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2467 - MinusLogProbMetric: 27.2467 - val_loss: 28.3602 - val_MinusLogProbMetric: 28.3602 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 795/1000
2023-10-26 02:31:03.790 
Epoch 795/1000 
	 loss: 27.2474, MinusLogProbMetric: 27.2474, val_loss: 28.3843, val_MinusLogProbMetric: 28.3843

Epoch 795: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2474 - MinusLogProbMetric: 27.2474 - val_loss: 28.3843 - val_MinusLogProbMetric: 28.3843 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 796/1000
2023-10-26 02:31:46.142 
Epoch 796/1000 
	 loss: 27.2477, MinusLogProbMetric: 27.2477, val_loss: 28.3837, val_MinusLogProbMetric: 28.3837

Epoch 796: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2477 - MinusLogProbMetric: 27.2477 - val_loss: 28.3837 - val_MinusLogProbMetric: 28.3837 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 797/1000
2023-10-26 02:32:28.515 
Epoch 797/1000 
	 loss: 27.2460, MinusLogProbMetric: 27.2460, val_loss: 28.3602, val_MinusLogProbMetric: 28.3602

Epoch 797: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2460 - MinusLogProbMetric: 27.2460 - val_loss: 28.3602 - val_MinusLogProbMetric: 28.3602 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 798/1000
2023-10-26 02:33:10.906 
Epoch 798/1000 
	 loss: 27.2458, MinusLogProbMetric: 27.2458, val_loss: 28.3667, val_MinusLogProbMetric: 28.3667

Epoch 798: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2458 - MinusLogProbMetric: 27.2458 - val_loss: 28.3667 - val_MinusLogProbMetric: 28.3667 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 799/1000
2023-10-26 02:33:53.356 
Epoch 799/1000 
	 loss: 27.2455, MinusLogProbMetric: 27.2455, val_loss: 28.3765, val_MinusLogProbMetric: 28.3765

Epoch 799: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2455 - MinusLogProbMetric: 27.2455 - val_loss: 28.3765 - val_MinusLogProbMetric: 28.3765 - lr: 1.0417e-05 - 42s/epoch - 217ms/step
Epoch 800/1000
2023-10-26 02:34:36.120 
Epoch 800/1000 
	 loss: 27.2457, MinusLogProbMetric: 27.2457, val_loss: 28.3677, val_MinusLogProbMetric: 28.3677

Epoch 800: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2457 - MinusLogProbMetric: 27.2457 - val_loss: 28.3677 - val_MinusLogProbMetric: 28.3677 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 801/1000
2023-10-26 02:35:18.761 
Epoch 801/1000 
	 loss: 27.2443, MinusLogProbMetric: 27.2443, val_loss: 28.3673, val_MinusLogProbMetric: 28.3673

Epoch 801: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2443 - MinusLogProbMetric: 27.2443 - val_loss: 28.3673 - val_MinusLogProbMetric: 28.3673 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 802/1000
2023-10-26 02:36:01.357 
Epoch 802/1000 
	 loss: 27.2462, MinusLogProbMetric: 27.2462, val_loss: 28.3639, val_MinusLogProbMetric: 28.3639

Epoch 802: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2462 - MinusLogProbMetric: 27.2462 - val_loss: 28.3639 - val_MinusLogProbMetric: 28.3639 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 803/1000
2023-10-26 02:36:43.891 
Epoch 803/1000 
	 loss: 27.2457, MinusLogProbMetric: 27.2457, val_loss: 28.3655, val_MinusLogProbMetric: 28.3655

Epoch 803: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2457 - MinusLogProbMetric: 27.2457 - val_loss: 28.3655 - val_MinusLogProbMetric: 28.3655 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 804/1000
2023-10-26 02:37:26.494 
Epoch 804/1000 
	 loss: 27.2446, MinusLogProbMetric: 27.2446, val_loss: 28.3577, val_MinusLogProbMetric: 28.3577

Epoch 804: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2446 - MinusLogProbMetric: 27.2446 - val_loss: 28.3577 - val_MinusLogProbMetric: 28.3577 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 805/1000
2023-10-26 02:38:09.082 
Epoch 805/1000 
	 loss: 27.2456, MinusLogProbMetric: 27.2456, val_loss: 28.3580, val_MinusLogProbMetric: 28.3580

Epoch 805: val_loss did not improve from 28.35573
196/196 - 43s - loss: 27.2456 - MinusLogProbMetric: 27.2456 - val_loss: 28.3580 - val_MinusLogProbMetric: 28.3580 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 806/1000
2023-10-26 02:38:51.155 
Epoch 806/1000 
	 loss: 27.2479, MinusLogProbMetric: 27.2479, val_loss: 28.3698, val_MinusLogProbMetric: 28.3698

Epoch 806: val_loss did not improve from 28.35573
196/196 - 42s - loss: 27.2479 - MinusLogProbMetric: 27.2479 - val_loss: 28.3698 - val_MinusLogProbMetric: 28.3698 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 807/1000
2023-10-26 02:39:33.746 
Epoch 807/1000 
	 loss: 27.2459, MinusLogProbMetric: 27.2459, val_loss: 28.3553, val_MinusLogProbMetric: 28.3553

Epoch 807: val_loss improved from 28.35573 to 28.35534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 44s - loss: 27.2459 - MinusLogProbMetric: 27.2459 - val_loss: 28.3553 - val_MinusLogProbMetric: 28.3553 - lr: 1.0417e-05 - 44s/epoch - 223ms/step
Epoch 808/1000
2023-10-26 02:40:17.214 
Epoch 808/1000 
	 loss: 27.2448, MinusLogProbMetric: 27.2448, val_loss: 28.3596, val_MinusLogProbMetric: 28.3596

Epoch 808: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2448 - MinusLogProbMetric: 27.2448 - val_loss: 28.3596 - val_MinusLogProbMetric: 28.3596 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 809/1000
2023-10-26 02:40:59.222 
Epoch 809/1000 
	 loss: 27.2475, MinusLogProbMetric: 27.2475, val_loss: 28.3738, val_MinusLogProbMetric: 28.3738

Epoch 809: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2475 - MinusLogProbMetric: 27.2475 - val_loss: 28.3738 - val_MinusLogProbMetric: 28.3738 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 810/1000
2023-10-26 02:41:41.797 
Epoch 810/1000 
	 loss: 27.2447, MinusLogProbMetric: 27.2447, val_loss: 28.3712, val_MinusLogProbMetric: 28.3712

Epoch 810: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2447 - MinusLogProbMetric: 27.2447 - val_loss: 28.3712 - val_MinusLogProbMetric: 28.3712 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 811/1000
2023-10-26 02:42:24.426 
Epoch 811/1000 
	 loss: 27.2439, MinusLogProbMetric: 27.2439, val_loss: 28.3586, val_MinusLogProbMetric: 28.3586

Epoch 811: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2439 - MinusLogProbMetric: 27.2439 - val_loss: 28.3586 - val_MinusLogProbMetric: 28.3586 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 812/1000
2023-10-26 02:43:06.830 
Epoch 812/1000 
	 loss: 27.2439, MinusLogProbMetric: 27.2439, val_loss: 28.3666, val_MinusLogProbMetric: 28.3666

Epoch 812: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2439 - MinusLogProbMetric: 27.2439 - val_loss: 28.3666 - val_MinusLogProbMetric: 28.3666 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 813/1000
2023-10-26 02:43:49.523 
Epoch 813/1000 
	 loss: 27.2454, MinusLogProbMetric: 27.2454, val_loss: 28.3773, val_MinusLogProbMetric: 28.3773

Epoch 813: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2454 - MinusLogProbMetric: 27.2454 - val_loss: 28.3773 - val_MinusLogProbMetric: 28.3773 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 814/1000
2023-10-26 02:44:32.298 
Epoch 814/1000 
	 loss: 27.2445, MinusLogProbMetric: 27.2445, val_loss: 28.3573, val_MinusLogProbMetric: 28.3573

Epoch 814: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2445 - MinusLogProbMetric: 27.2445 - val_loss: 28.3573 - val_MinusLogProbMetric: 28.3573 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 815/1000
2023-10-26 02:45:15.085 
Epoch 815/1000 
	 loss: 27.2440, MinusLogProbMetric: 27.2440, val_loss: 28.3657, val_MinusLogProbMetric: 28.3657

Epoch 815: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2440 - MinusLogProbMetric: 27.2440 - val_loss: 28.3657 - val_MinusLogProbMetric: 28.3657 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 816/1000
2023-10-26 02:45:57.200 
Epoch 816/1000 
	 loss: 27.2427, MinusLogProbMetric: 27.2427, val_loss: 28.3628, val_MinusLogProbMetric: 28.3628

Epoch 816: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2427 - MinusLogProbMetric: 27.2427 - val_loss: 28.3628 - val_MinusLogProbMetric: 28.3628 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 817/1000
2023-10-26 02:46:39.933 
Epoch 817/1000 
	 loss: 27.2449, MinusLogProbMetric: 27.2449, val_loss: 28.3576, val_MinusLogProbMetric: 28.3576

Epoch 817: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2449 - MinusLogProbMetric: 27.2449 - val_loss: 28.3576 - val_MinusLogProbMetric: 28.3576 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 818/1000
2023-10-26 02:47:22.409 
Epoch 818/1000 
	 loss: 27.2443, MinusLogProbMetric: 27.2443, val_loss: 28.3593, val_MinusLogProbMetric: 28.3593

Epoch 818: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2443 - MinusLogProbMetric: 27.2443 - val_loss: 28.3593 - val_MinusLogProbMetric: 28.3593 - lr: 1.0417e-05 - 42s/epoch - 217ms/step
Epoch 819/1000
2023-10-26 02:48:05.114 
Epoch 819/1000 
	 loss: 27.2445, MinusLogProbMetric: 27.2445, val_loss: 28.3615, val_MinusLogProbMetric: 28.3615

Epoch 819: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2445 - MinusLogProbMetric: 27.2445 - val_loss: 28.3615 - val_MinusLogProbMetric: 28.3615 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 820/1000
2023-10-26 02:48:47.810 
Epoch 820/1000 
	 loss: 27.2431, MinusLogProbMetric: 27.2431, val_loss: 28.3616, val_MinusLogProbMetric: 28.3616

Epoch 820: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2431 - MinusLogProbMetric: 27.2431 - val_loss: 28.3616 - val_MinusLogProbMetric: 28.3616 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 821/1000
2023-10-26 02:49:30.705 
Epoch 821/1000 
	 loss: 27.2433, MinusLogProbMetric: 27.2433, val_loss: 28.3589, val_MinusLogProbMetric: 28.3589

Epoch 821: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2433 - MinusLogProbMetric: 27.2433 - val_loss: 28.3589 - val_MinusLogProbMetric: 28.3589 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 822/1000
2023-10-26 02:50:13.001 
Epoch 822/1000 
	 loss: 27.2441, MinusLogProbMetric: 27.2441, val_loss: 28.3640, val_MinusLogProbMetric: 28.3640

Epoch 822: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2441 - MinusLogProbMetric: 27.2441 - val_loss: 28.3640 - val_MinusLogProbMetric: 28.3640 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 823/1000
2023-10-26 02:50:55.744 
Epoch 823/1000 
	 loss: 27.2426, MinusLogProbMetric: 27.2426, val_loss: 28.3657, val_MinusLogProbMetric: 28.3657

Epoch 823: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2426 - MinusLogProbMetric: 27.2426 - val_loss: 28.3657 - val_MinusLogProbMetric: 28.3657 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 824/1000
2023-10-26 02:51:38.626 
Epoch 824/1000 
	 loss: 27.2426, MinusLogProbMetric: 27.2426, val_loss: 28.3667, val_MinusLogProbMetric: 28.3667

Epoch 824: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2426 - MinusLogProbMetric: 27.2426 - val_loss: 28.3667 - val_MinusLogProbMetric: 28.3667 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 825/1000
2023-10-26 02:52:21.488 
Epoch 825/1000 
	 loss: 27.2430, MinusLogProbMetric: 27.2430, val_loss: 28.3672, val_MinusLogProbMetric: 28.3672

Epoch 825: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2430 - MinusLogProbMetric: 27.2430 - val_loss: 28.3672 - val_MinusLogProbMetric: 28.3672 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 826/1000
2023-10-26 02:53:04.625 
Epoch 826/1000 
	 loss: 27.2441, MinusLogProbMetric: 27.2441, val_loss: 28.3732, val_MinusLogProbMetric: 28.3732

Epoch 826: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2441 - MinusLogProbMetric: 27.2441 - val_loss: 28.3732 - val_MinusLogProbMetric: 28.3732 - lr: 1.0417e-05 - 43s/epoch - 220ms/step
Epoch 827/1000
2023-10-26 02:53:47.495 
Epoch 827/1000 
	 loss: 27.2433, MinusLogProbMetric: 27.2433, val_loss: 28.3626, val_MinusLogProbMetric: 28.3626

Epoch 827: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2433 - MinusLogProbMetric: 27.2433 - val_loss: 28.3626 - val_MinusLogProbMetric: 28.3626 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 828/1000
2023-10-26 02:54:29.892 
Epoch 828/1000 
	 loss: 27.2457, MinusLogProbMetric: 27.2457, val_loss: 28.3653, val_MinusLogProbMetric: 28.3653

Epoch 828: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2457 - MinusLogProbMetric: 27.2457 - val_loss: 28.3653 - val_MinusLogProbMetric: 28.3653 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 829/1000
2023-10-26 02:55:12.976 
Epoch 829/1000 
	 loss: 27.2444, MinusLogProbMetric: 27.2444, val_loss: 28.3652, val_MinusLogProbMetric: 28.3652

Epoch 829: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2444 - MinusLogProbMetric: 27.2444 - val_loss: 28.3652 - val_MinusLogProbMetric: 28.3652 - lr: 1.0417e-05 - 43s/epoch - 220ms/step
Epoch 830/1000
2023-10-26 02:55:56.034 
Epoch 830/1000 
	 loss: 27.2453, MinusLogProbMetric: 27.2453, val_loss: 28.3581, val_MinusLogProbMetric: 28.3581

Epoch 830: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2453 - MinusLogProbMetric: 27.2453 - val_loss: 28.3581 - val_MinusLogProbMetric: 28.3581 - lr: 1.0417e-05 - 43s/epoch - 220ms/step
Epoch 831/1000
2023-10-26 02:56:39.071 
Epoch 831/1000 
	 loss: 27.2435, MinusLogProbMetric: 27.2435, val_loss: 28.3574, val_MinusLogProbMetric: 28.3574

Epoch 831: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2435 - MinusLogProbMetric: 27.2435 - val_loss: 28.3574 - val_MinusLogProbMetric: 28.3574 - lr: 1.0417e-05 - 43s/epoch - 220ms/step
Epoch 832/1000
2023-10-26 02:57:21.477 
Epoch 832/1000 
	 loss: 27.2436, MinusLogProbMetric: 27.2436, val_loss: 28.3649, val_MinusLogProbMetric: 28.3649

Epoch 832: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2436 - MinusLogProbMetric: 27.2436 - val_loss: 28.3649 - val_MinusLogProbMetric: 28.3649 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 833/1000
2023-10-26 02:58:04.304 
Epoch 833/1000 
	 loss: 27.2435, MinusLogProbMetric: 27.2435, val_loss: 28.3627, val_MinusLogProbMetric: 28.3627

Epoch 833: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2435 - MinusLogProbMetric: 27.2435 - val_loss: 28.3627 - val_MinusLogProbMetric: 28.3627 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 834/1000
2023-10-26 02:58:47.320 
Epoch 834/1000 
	 loss: 27.2430, MinusLogProbMetric: 27.2430, val_loss: 28.3632, val_MinusLogProbMetric: 28.3632

Epoch 834: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2430 - MinusLogProbMetric: 27.2430 - val_loss: 28.3632 - val_MinusLogProbMetric: 28.3632 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 835/1000
2023-10-26 02:59:30.416 
Epoch 835/1000 
	 loss: 27.2428, MinusLogProbMetric: 27.2428, val_loss: 28.3644, val_MinusLogProbMetric: 28.3644

Epoch 835: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2428 - MinusLogProbMetric: 27.2428 - val_loss: 28.3644 - val_MinusLogProbMetric: 28.3644 - lr: 1.0417e-05 - 43s/epoch - 220ms/step
Epoch 836/1000
2023-10-26 03:00:12.938 
Epoch 836/1000 
	 loss: 27.2449, MinusLogProbMetric: 27.2449, val_loss: 28.3588, val_MinusLogProbMetric: 28.3588

Epoch 836: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2449 - MinusLogProbMetric: 27.2449 - val_loss: 28.3588 - val_MinusLogProbMetric: 28.3588 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 837/1000
2023-10-26 03:00:55.864 
Epoch 837/1000 
	 loss: 27.2411, MinusLogProbMetric: 27.2411, val_loss: 28.3564, val_MinusLogProbMetric: 28.3564

Epoch 837: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2411 - MinusLogProbMetric: 27.2411 - val_loss: 28.3564 - val_MinusLogProbMetric: 28.3564 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 838/1000
2023-10-26 03:01:37.946 
Epoch 838/1000 
	 loss: 27.2421, MinusLogProbMetric: 27.2421, val_loss: 28.3618, val_MinusLogProbMetric: 28.3618

Epoch 838: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2421 - MinusLogProbMetric: 27.2421 - val_loss: 28.3618 - val_MinusLogProbMetric: 28.3618 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 839/1000
2023-10-26 03:02:20.375 
Epoch 839/1000 
	 loss: 27.2442, MinusLogProbMetric: 27.2442, val_loss: 28.3648, val_MinusLogProbMetric: 28.3648

Epoch 839: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2442 - MinusLogProbMetric: 27.2442 - val_loss: 28.3648 - val_MinusLogProbMetric: 28.3648 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 840/1000
2023-10-26 03:03:02.771 
Epoch 840/1000 
	 loss: 27.2429, MinusLogProbMetric: 27.2429, val_loss: 28.3629, val_MinusLogProbMetric: 28.3629

Epoch 840: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2429 - MinusLogProbMetric: 27.2429 - val_loss: 28.3629 - val_MinusLogProbMetric: 28.3629 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 841/1000
2023-10-26 03:03:45.246 
Epoch 841/1000 
	 loss: 27.2418, MinusLogProbMetric: 27.2418, val_loss: 28.3654, val_MinusLogProbMetric: 28.3654

Epoch 841: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2418 - MinusLogProbMetric: 27.2418 - val_loss: 28.3654 - val_MinusLogProbMetric: 28.3654 - lr: 1.0417e-05 - 42s/epoch - 217ms/step
Epoch 842/1000
2023-10-26 03:04:27.670 
Epoch 842/1000 
	 loss: 27.2430, MinusLogProbMetric: 27.2430, val_loss: 28.3663, val_MinusLogProbMetric: 28.3663

Epoch 842: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2430 - MinusLogProbMetric: 27.2430 - val_loss: 28.3663 - val_MinusLogProbMetric: 28.3663 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 843/1000
2023-10-26 03:05:11.016 
Epoch 843/1000 
	 loss: 27.2418, MinusLogProbMetric: 27.2418, val_loss: 28.3709, val_MinusLogProbMetric: 28.3709

Epoch 843: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2418 - MinusLogProbMetric: 27.2418 - val_loss: 28.3709 - val_MinusLogProbMetric: 28.3709 - lr: 1.0417e-05 - 43s/epoch - 221ms/step
Epoch 844/1000
2023-10-26 03:05:53.782 
Epoch 844/1000 
	 loss: 27.2430, MinusLogProbMetric: 27.2430, val_loss: 28.3629, val_MinusLogProbMetric: 28.3629

Epoch 844: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2430 - MinusLogProbMetric: 27.2430 - val_loss: 28.3629 - val_MinusLogProbMetric: 28.3629 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 845/1000
2023-10-26 03:06:36.716 
Epoch 845/1000 
	 loss: 27.2435, MinusLogProbMetric: 27.2435, val_loss: 28.3572, val_MinusLogProbMetric: 28.3572

Epoch 845: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2435 - MinusLogProbMetric: 27.2435 - val_loss: 28.3572 - val_MinusLogProbMetric: 28.3572 - lr: 1.0417e-05 - 43s/epoch - 219ms/step
Epoch 846/1000
2023-10-26 03:07:19.123 
Epoch 846/1000 
	 loss: 27.2425, MinusLogProbMetric: 27.2425, val_loss: 28.3669, val_MinusLogProbMetric: 28.3669

Epoch 846: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2425 - MinusLogProbMetric: 27.2425 - val_loss: 28.3669 - val_MinusLogProbMetric: 28.3669 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 847/1000
2023-10-26 03:08:01.604 
Epoch 847/1000 
	 loss: 27.2416, MinusLogProbMetric: 27.2416, val_loss: 28.3684, val_MinusLogProbMetric: 28.3684

Epoch 847: val_loss did not improve from 28.35534
196/196 - 42s - loss: 27.2416 - MinusLogProbMetric: 27.2416 - val_loss: 28.3684 - val_MinusLogProbMetric: 28.3684 - lr: 1.0417e-05 - 42s/epoch - 217ms/step
Epoch 848/1000
2023-10-26 03:08:44.250 
Epoch 848/1000 
	 loss: 27.2420, MinusLogProbMetric: 27.2420, val_loss: 28.3688, val_MinusLogProbMetric: 28.3688

Epoch 848: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2420 - MinusLogProbMetric: 27.2420 - val_loss: 28.3688 - val_MinusLogProbMetric: 28.3688 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 849/1000
2023-10-26 03:09:27.700 
Epoch 849/1000 
	 loss: 27.2424, MinusLogProbMetric: 27.2424, val_loss: 28.3620, val_MinusLogProbMetric: 28.3620

Epoch 849: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2424 - MinusLogProbMetric: 27.2424 - val_loss: 28.3620 - val_MinusLogProbMetric: 28.3620 - lr: 1.0417e-05 - 43s/epoch - 222ms/step
Epoch 850/1000
2023-10-26 03:10:08.336 
Epoch 850/1000 
	 loss: 27.2426, MinusLogProbMetric: 27.2426, val_loss: 28.3661, val_MinusLogProbMetric: 28.3661

Epoch 850: val_loss did not improve from 28.35534
196/196 - 41s - loss: 27.2426 - MinusLogProbMetric: 27.2426 - val_loss: 28.3661 - val_MinusLogProbMetric: 28.3661 - lr: 1.0417e-05 - 41s/epoch - 207ms/step
Epoch 851/1000
2023-10-26 03:10:46.853 
Epoch 851/1000 
	 loss: 27.2424, MinusLogProbMetric: 27.2424, val_loss: 28.3619, val_MinusLogProbMetric: 28.3619

Epoch 851: val_loss did not improve from 28.35534
196/196 - 39s - loss: 27.2424 - MinusLogProbMetric: 27.2424 - val_loss: 28.3619 - val_MinusLogProbMetric: 28.3619 - lr: 1.0417e-05 - 39s/epoch - 197ms/step
Epoch 852/1000
2023-10-26 03:11:26.792 
Epoch 852/1000 
	 loss: 27.2420, MinusLogProbMetric: 27.2420, val_loss: 28.3591, val_MinusLogProbMetric: 28.3591

Epoch 852: val_loss did not improve from 28.35534
196/196 - 40s - loss: 27.2420 - MinusLogProbMetric: 27.2420 - val_loss: 28.3591 - val_MinusLogProbMetric: 28.3591 - lr: 1.0417e-05 - 40s/epoch - 204ms/step
Epoch 853/1000
2023-10-26 03:12:09.336 
Epoch 853/1000 
	 loss: 27.2406, MinusLogProbMetric: 27.2406, val_loss: 28.3651, val_MinusLogProbMetric: 28.3651

Epoch 853: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2406 - MinusLogProbMetric: 27.2406 - val_loss: 28.3651 - val_MinusLogProbMetric: 28.3651 - lr: 1.0417e-05 - 43s/epoch - 217ms/step
Epoch 854/1000
2023-10-26 03:12:52.635 
Epoch 854/1000 
	 loss: 27.2410, MinusLogProbMetric: 27.2410, val_loss: 28.3610, val_MinusLogProbMetric: 28.3610

Epoch 854: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2410 - MinusLogProbMetric: 27.2410 - val_loss: 28.3610 - val_MinusLogProbMetric: 28.3610 - lr: 1.0417e-05 - 43s/epoch - 221ms/step
Epoch 855/1000
2023-10-26 03:13:35.683 
Epoch 855/1000 
	 loss: 27.2424, MinusLogProbMetric: 27.2424, val_loss: 28.3691, val_MinusLogProbMetric: 28.3691

Epoch 855: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2424 - MinusLogProbMetric: 27.2424 - val_loss: 28.3691 - val_MinusLogProbMetric: 28.3691 - lr: 1.0417e-05 - 43s/epoch - 220ms/step
Epoch 856/1000
2023-10-26 03:14:18.421 
Epoch 856/1000 
	 loss: 27.2419, MinusLogProbMetric: 27.2419, val_loss: 28.3605, val_MinusLogProbMetric: 28.3605

Epoch 856: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2419 - MinusLogProbMetric: 27.2419 - val_loss: 28.3605 - val_MinusLogProbMetric: 28.3605 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 857/1000
2023-10-26 03:15:01.178 
Epoch 857/1000 
	 loss: 27.2418, MinusLogProbMetric: 27.2418, val_loss: 28.3673, val_MinusLogProbMetric: 28.3673

Epoch 857: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2418 - MinusLogProbMetric: 27.2418 - val_loss: 28.3673 - val_MinusLogProbMetric: 28.3673 - lr: 1.0417e-05 - 43s/epoch - 218ms/step
Epoch 858/1000
2023-10-26 03:15:43.955 
Epoch 858/1000 
	 loss: 27.2327, MinusLogProbMetric: 27.2327, val_loss: 28.3602, val_MinusLogProbMetric: 28.3602

Epoch 858: val_loss did not improve from 28.35534
196/196 - 43s - loss: 27.2327 - MinusLogProbMetric: 27.2327 - val_loss: 28.3602 - val_MinusLogProbMetric: 28.3602 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 859/1000
2023-10-26 03:16:26.422 
Epoch 859/1000 
	 loss: 27.2326, MinusLogProbMetric: 27.2326, val_loss: 28.3549, val_MinusLogProbMetric: 28.3549

Epoch 859: val_loss improved from 28.35534 to 28.35491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.2326 - MinusLogProbMetric: 27.2326 - val_loss: 28.3549 - val_MinusLogProbMetric: 28.3549 - lr: 5.2083e-06 - 43s/epoch - 221ms/step
Epoch 860/1000
2023-10-26 03:17:09.855 
Epoch 860/1000 
	 loss: 27.2327, MinusLogProbMetric: 27.2327, val_loss: 28.3579, val_MinusLogProbMetric: 28.3579

Epoch 860: val_loss did not improve from 28.35491
196/196 - 43s - loss: 27.2327 - MinusLogProbMetric: 27.2327 - val_loss: 28.3579 - val_MinusLogProbMetric: 28.3579 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 861/1000
2023-10-26 03:17:52.536 
Epoch 861/1000 
	 loss: 27.2328, MinusLogProbMetric: 27.2328, val_loss: 28.3601, val_MinusLogProbMetric: 28.3601

Epoch 861: val_loss did not improve from 28.35491
196/196 - 43s - loss: 27.2328 - MinusLogProbMetric: 27.2328 - val_loss: 28.3601 - val_MinusLogProbMetric: 28.3601 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 862/1000
2023-10-26 03:18:35.423 
Epoch 862/1000 
	 loss: 27.2324, MinusLogProbMetric: 27.2324, val_loss: 28.3563, val_MinusLogProbMetric: 28.3563

Epoch 862: val_loss did not improve from 28.35491
196/196 - 43s - loss: 27.2324 - MinusLogProbMetric: 27.2324 - val_loss: 28.3563 - val_MinusLogProbMetric: 28.3563 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 863/1000
2023-10-26 03:19:18.195 
Epoch 863/1000 
	 loss: 27.2325, MinusLogProbMetric: 27.2325, val_loss: 28.3601, val_MinusLogProbMetric: 28.3601

Epoch 863: val_loss did not improve from 28.35491
196/196 - 43s - loss: 27.2325 - MinusLogProbMetric: 27.2325 - val_loss: 28.3601 - val_MinusLogProbMetric: 28.3601 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 864/1000
2023-10-26 03:20:00.982 
Epoch 864/1000 
	 loss: 27.2332, MinusLogProbMetric: 27.2332, val_loss: 28.3597, val_MinusLogProbMetric: 28.3597

Epoch 864: val_loss did not improve from 28.35491
196/196 - 43s - loss: 27.2332 - MinusLogProbMetric: 27.2332 - val_loss: 28.3597 - val_MinusLogProbMetric: 28.3597 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 865/1000
2023-10-26 03:20:43.471 
Epoch 865/1000 
	 loss: 27.2319, MinusLogProbMetric: 27.2319, val_loss: 28.3564, val_MinusLogProbMetric: 28.3564

Epoch 865: val_loss did not improve from 28.35491
196/196 - 42s - loss: 27.2319 - MinusLogProbMetric: 27.2319 - val_loss: 28.3564 - val_MinusLogProbMetric: 28.3564 - lr: 5.2083e-06 - 42s/epoch - 217ms/step
Epoch 866/1000
2023-10-26 03:21:25.896 
Epoch 866/1000 
	 loss: 27.2327, MinusLogProbMetric: 27.2327, val_loss: 28.3573, val_MinusLogProbMetric: 28.3573

Epoch 866: val_loss did not improve from 28.35491
196/196 - 42s - loss: 27.2327 - MinusLogProbMetric: 27.2327 - val_loss: 28.3573 - val_MinusLogProbMetric: 28.3573 - lr: 5.2083e-06 - 42s/epoch - 216ms/step
Epoch 867/1000
2023-10-26 03:22:08.387 
Epoch 867/1000 
	 loss: 27.2318, MinusLogProbMetric: 27.2318, val_loss: 28.3553, val_MinusLogProbMetric: 28.3553

Epoch 867: val_loss did not improve from 28.35491
196/196 - 42s - loss: 27.2318 - MinusLogProbMetric: 27.2318 - val_loss: 28.3553 - val_MinusLogProbMetric: 28.3553 - lr: 5.2083e-06 - 42s/epoch - 217ms/step
Epoch 868/1000
2023-10-26 03:22:50.625 
Epoch 868/1000 
	 loss: 27.2318, MinusLogProbMetric: 27.2318, val_loss: 28.3542, val_MinusLogProbMetric: 28.3542

Epoch 868: val_loss improved from 28.35491 to 28.35420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.2318 - MinusLogProbMetric: 27.2318 - val_loss: 28.3542 - val_MinusLogProbMetric: 28.3542 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 869/1000
2023-10-26 03:23:33.987 
Epoch 869/1000 
	 loss: 27.2324, MinusLogProbMetric: 27.2324, val_loss: 28.3566, val_MinusLogProbMetric: 28.3566

Epoch 869: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2324 - MinusLogProbMetric: 27.2324 - val_loss: 28.3566 - val_MinusLogProbMetric: 28.3566 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 870/1000
2023-10-26 03:24:16.854 
Epoch 870/1000 
	 loss: 27.2323, MinusLogProbMetric: 27.2323, val_loss: 28.3633, val_MinusLogProbMetric: 28.3633

Epoch 870: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2323 - MinusLogProbMetric: 27.2323 - val_loss: 28.3633 - val_MinusLogProbMetric: 28.3633 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 871/1000
2023-10-26 03:24:59.463 
Epoch 871/1000 
	 loss: 27.2318, MinusLogProbMetric: 27.2318, val_loss: 28.3590, val_MinusLogProbMetric: 28.3590

Epoch 871: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2318 - MinusLogProbMetric: 27.2318 - val_loss: 28.3590 - val_MinusLogProbMetric: 28.3590 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 872/1000
2023-10-26 03:25:42.014 
Epoch 872/1000 
	 loss: 27.2323, MinusLogProbMetric: 27.2323, val_loss: 28.3595, val_MinusLogProbMetric: 28.3595

Epoch 872: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2323 - MinusLogProbMetric: 27.2323 - val_loss: 28.3595 - val_MinusLogProbMetric: 28.3595 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 873/1000
2023-10-26 03:26:25.271 
Epoch 873/1000 
	 loss: 27.2320, MinusLogProbMetric: 27.2320, val_loss: 28.3574, val_MinusLogProbMetric: 28.3574

Epoch 873: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2320 - MinusLogProbMetric: 27.2320 - val_loss: 28.3574 - val_MinusLogProbMetric: 28.3574 - lr: 5.2083e-06 - 43s/epoch - 221ms/step
Epoch 874/1000
2023-10-26 03:27:08.151 
Epoch 874/1000 
	 loss: 27.2321, MinusLogProbMetric: 27.2321, val_loss: 28.3636, val_MinusLogProbMetric: 28.3636

Epoch 874: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2321 - MinusLogProbMetric: 27.2321 - val_loss: 28.3636 - val_MinusLogProbMetric: 28.3636 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 875/1000
2023-10-26 03:27:51.225 
Epoch 875/1000 
	 loss: 27.2331, MinusLogProbMetric: 27.2331, val_loss: 28.3542, val_MinusLogProbMetric: 28.3542

Epoch 875: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2331 - MinusLogProbMetric: 27.2331 - val_loss: 28.3542 - val_MinusLogProbMetric: 28.3542 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 876/1000
2023-10-26 03:28:34.088 
Epoch 876/1000 
	 loss: 27.2317, MinusLogProbMetric: 27.2317, val_loss: 28.3593, val_MinusLogProbMetric: 28.3593

Epoch 876: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2317 - MinusLogProbMetric: 27.2317 - val_loss: 28.3593 - val_MinusLogProbMetric: 28.3593 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 877/1000
2023-10-26 03:29:16.955 
Epoch 877/1000 
	 loss: 27.2314, MinusLogProbMetric: 27.2314, val_loss: 28.3586, val_MinusLogProbMetric: 28.3586

Epoch 877: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2314 - MinusLogProbMetric: 27.2314 - val_loss: 28.3586 - val_MinusLogProbMetric: 28.3586 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 878/1000
2023-10-26 03:29:59.910 
Epoch 878/1000 
	 loss: 27.2321, MinusLogProbMetric: 27.2321, val_loss: 28.3548, val_MinusLogProbMetric: 28.3548

Epoch 878: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2321 - MinusLogProbMetric: 27.2321 - val_loss: 28.3548 - val_MinusLogProbMetric: 28.3548 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 879/1000
2023-10-26 03:30:42.884 
Epoch 879/1000 
	 loss: 27.2317, MinusLogProbMetric: 27.2317, val_loss: 28.3591, val_MinusLogProbMetric: 28.3591

Epoch 879: val_loss did not improve from 28.35420
196/196 - 43s - loss: 27.2317 - MinusLogProbMetric: 27.2317 - val_loss: 28.3591 - val_MinusLogProbMetric: 28.3591 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 880/1000
2023-10-26 03:31:25.256 
Epoch 880/1000 
	 loss: 27.2326, MinusLogProbMetric: 27.2326, val_loss: 28.3531, val_MinusLogProbMetric: 28.3531

Epoch 880: val_loss improved from 28.35420 to 28.35314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.2326 - MinusLogProbMetric: 27.2326 - val_loss: 28.3531 - val_MinusLogProbMetric: 28.3531 - lr: 5.2083e-06 - 43s/epoch - 221ms/step
Epoch 881/1000
2023-10-26 03:32:09.426 
Epoch 881/1000 
	 loss: 27.2316, MinusLogProbMetric: 27.2316, val_loss: 28.3545, val_MinusLogProbMetric: 28.3545

Epoch 881: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2316 - MinusLogProbMetric: 27.2316 - val_loss: 28.3545 - val_MinusLogProbMetric: 28.3545 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 882/1000
2023-10-26 03:32:52.012 
Epoch 882/1000 
	 loss: 27.2312, MinusLogProbMetric: 27.2312, val_loss: 28.3533, val_MinusLogProbMetric: 28.3533

Epoch 882: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2312 - MinusLogProbMetric: 27.2312 - val_loss: 28.3533 - val_MinusLogProbMetric: 28.3533 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 883/1000
2023-10-26 03:33:34.484 
Epoch 883/1000 
	 loss: 27.2310, MinusLogProbMetric: 27.2310, val_loss: 28.3562, val_MinusLogProbMetric: 28.3562

Epoch 883: val_loss did not improve from 28.35314
196/196 - 42s - loss: 27.2310 - MinusLogProbMetric: 27.2310 - val_loss: 28.3562 - val_MinusLogProbMetric: 28.3562 - lr: 5.2083e-06 - 42s/epoch - 217ms/step
Epoch 884/1000
2023-10-26 03:34:16.996 
Epoch 884/1000 
	 loss: 27.2325, MinusLogProbMetric: 27.2325, val_loss: 28.3601, val_MinusLogProbMetric: 28.3601

Epoch 884: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2325 - MinusLogProbMetric: 27.2325 - val_loss: 28.3601 - val_MinusLogProbMetric: 28.3601 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 885/1000
2023-10-26 03:34:59.789 
Epoch 885/1000 
	 loss: 27.2322, MinusLogProbMetric: 27.2322, val_loss: 28.3598, val_MinusLogProbMetric: 28.3598

Epoch 885: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2322 - MinusLogProbMetric: 27.2322 - val_loss: 28.3598 - val_MinusLogProbMetric: 28.3598 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 886/1000
2023-10-26 03:35:42.341 
Epoch 886/1000 
	 loss: 27.2321, MinusLogProbMetric: 27.2321, val_loss: 28.3656, val_MinusLogProbMetric: 28.3656

Epoch 886: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2321 - MinusLogProbMetric: 27.2321 - val_loss: 28.3656 - val_MinusLogProbMetric: 28.3656 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 887/1000
2023-10-26 03:36:25.067 
Epoch 887/1000 
	 loss: 27.2313, MinusLogProbMetric: 27.2313, val_loss: 28.3577, val_MinusLogProbMetric: 28.3577

Epoch 887: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2313 - MinusLogProbMetric: 27.2313 - val_loss: 28.3577 - val_MinusLogProbMetric: 28.3577 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 888/1000
2023-10-26 03:37:08.155 
Epoch 888/1000 
	 loss: 27.2324, MinusLogProbMetric: 27.2324, val_loss: 28.3607, val_MinusLogProbMetric: 28.3607

Epoch 888: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2324 - MinusLogProbMetric: 27.2324 - val_loss: 28.3607 - val_MinusLogProbMetric: 28.3607 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 889/1000
2023-10-26 03:37:50.575 
Epoch 889/1000 
	 loss: 27.2317, MinusLogProbMetric: 27.2317, val_loss: 28.3577, val_MinusLogProbMetric: 28.3577

Epoch 889: val_loss did not improve from 28.35314
196/196 - 42s - loss: 27.2317 - MinusLogProbMetric: 27.2317 - val_loss: 28.3577 - val_MinusLogProbMetric: 28.3577 - lr: 5.2083e-06 - 42s/epoch - 216ms/step
Epoch 890/1000
2023-10-26 03:38:33.736 
Epoch 890/1000 
	 loss: 27.2307, MinusLogProbMetric: 27.2307, val_loss: 28.3596, val_MinusLogProbMetric: 28.3596

Epoch 890: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2307 - MinusLogProbMetric: 27.2307 - val_loss: 28.3596 - val_MinusLogProbMetric: 28.3596 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 891/1000
2023-10-26 03:39:16.262 
Epoch 891/1000 
	 loss: 27.2317, MinusLogProbMetric: 27.2317, val_loss: 28.3565, val_MinusLogProbMetric: 28.3565

Epoch 891: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2317 - MinusLogProbMetric: 27.2317 - val_loss: 28.3565 - val_MinusLogProbMetric: 28.3565 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 892/1000
2023-10-26 03:39:58.902 
Epoch 892/1000 
	 loss: 27.2309, MinusLogProbMetric: 27.2309, val_loss: 28.3569, val_MinusLogProbMetric: 28.3569

Epoch 892: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2309 - MinusLogProbMetric: 27.2309 - val_loss: 28.3569 - val_MinusLogProbMetric: 28.3569 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 893/1000
2023-10-26 03:40:41.946 
Epoch 893/1000 
	 loss: 27.2308, MinusLogProbMetric: 27.2308, val_loss: 28.3659, val_MinusLogProbMetric: 28.3659

Epoch 893: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2308 - MinusLogProbMetric: 27.2308 - val_loss: 28.3659 - val_MinusLogProbMetric: 28.3659 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 894/1000
2023-10-26 03:41:25.088 
Epoch 894/1000 
	 loss: 27.2309, MinusLogProbMetric: 27.2309, val_loss: 28.3579, val_MinusLogProbMetric: 28.3579

Epoch 894: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2309 - MinusLogProbMetric: 27.2309 - val_loss: 28.3579 - val_MinusLogProbMetric: 28.3579 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 895/1000
2023-10-26 03:42:07.829 
Epoch 895/1000 
	 loss: 27.2307, MinusLogProbMetric: 27.2307, val_loss: 28.3570, val_MinusLogProbMetric: 28.3570

Epoch 895: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2307 - MinusLogProbMetric: 27.2307 - val_loss: 28.3570 - val_MinusLogProbMetric: 28.3570 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 896/1000
2023-10-26 03:42:50.451 
Epoch 896/1000 
	 loss: 27.2312, MinusLogProbMetric: 27.2312, val_loss: 28.3619, val_MinusLogProbMetric: 28.3619

Epoch 896: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2312 - MinusLogProbMetric: 27.2312 - val_loss: 28.3619 - val_MinusLogProbMetric: 28.3619 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 897/1000
2023-10-26 03:43:33.362 
Epoch 897/1000 
	 loss: 27.2308, MinusLogProbMetric: 27.2308, val_loss: 28.3601, val_MinusLogProbMetric: 28.3601

Epoch 897: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2308 - MinusLogProbMetric: 27.2308 - val_loss: 28.3601 - val_MinusLogProbMetric: 28.3601 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 898/1000
2023-10-26 03:44:15.908 
Epoch 898/1000 
	 loss: 27.2307, MinusLogProbMetric: 27.2307, val_loss: 28.3572, val_MinusLogProbMetric: 28.3572

Epoch 898: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2307 - MinusLogProbMetric: 27.2307 - val_loss: 28.3572 - val_MinusLogProbMetric: 28.3572 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 899/1000
2023-10-26 03:44:58.924 
Epoch 899/1000 
	 loss: 27.2309, MinusLogProbMetric: 27.2309, val_loss: 28.3583, val_MinusLogProbMetric: 28.3583

Epoch 899: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2309 - MinusLogProbMetric: 27.2309 - val_loss: 28.3583 - val_MinusLogProbMetric: 28.3583 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 900/1000
2023-10-26 03:45:41.376 
Epoch 900/1000 
	 loss: 27.2311, MinusLogProbMetric: 27.2311, val_loss: 28.3574, val_MinusLogProbMetric: 28.3574

Epoch 900: val_loss did not improve from 28.35314
196/196 - 42s - loss: 27.2311 - MinusLogProbMetric: 27.2311 - val_loss: 28.3574 - val_MinusLogProbMetric: 28.3574 - lr: 5.2083e-06 - 42s/epoch - 217ms/step
Epoch 901/1000
2023-10-26 03:46:24.263 
Epoch 901/1000 
	 loss: 27.2316, MinusLogProbMetric: 27.2316, val_loss: 28.3563, val_MinusLogProbMetric: 28.3563

Epoch 901: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2316 - MinusLogProbMetric: 27.2316 - val_loss: 28.3563 - val_MinusLogProbMetric: 28.3563 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 902/1000
2023-10-26 03:47:07.292 
Epoch 902/1000 
	 loss: 27.2315, MinusLogProbMetric: 27.2315, val_loss: 28.3577, val_MinusLogProbMetric: 28.3577

Epoch 902: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2315 - MinusLogProbMetric: 27.2315 - val_loss: 28.3577 - val_MinusLogProbMetric: 28.3577 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 903/1000
2023-10-26 03:47:50.327 
Epoch 903/1000 
	 loss: 27.2304, MinusLogProbMetric: 27.2304, val_loss: 28.3578, val_MinusLogProbMetric: 28.3578

Epoch 903: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2304 - MinusLogProbMetric: 27.2304 - val_loss: 28.3578 - val_MinusLogProbMetric: 28.3578 - lr: 5.2083e-06 - 43s/epoch - 220ms/step
Epoch 904/1000
2023-10-26 03:48:33.272 
Epoch 904/1000 
	 loss: 27.2299, MinusLogProbMetric: 27.2299, val_loss: 28.3632, val_MinusLogProbMetric: 28.3632

Epoch 904: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2299 - MinusLogProbMetric: 27.2299 - val_loss: 28.3632 - val_MinusLogProbMetric: 28.3632 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 905/1000
2023-10-26 03:49:16.159 
Epoch 905/1000 
	 loss: 27.2309, MinusLogProbMetric: 27.2309, val_loss: 28.3597, val_MinusLogProbMetric: 28.3597

Epoch 905: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2309 - MinusLogProbMetric: 27.2309 - val_loss: 28.3597 - val_MinusLogProbMetric: 28.3597 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 906/1000
2023-10-26 03:49:58.972 
Epoch 906/1000 
	 loss: 27.2308, MinusLogProbMetric: 27.2308, val_loss: 28.3613, val_MinusLogProbMetric: 28.3613

Epoch 906: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2308 - MinusLogProbMetric: 27.2308 - val_loss: 28.3613 - val_MinusLogProbMetric: 28.3613 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 907/1000
2023-10-26 03:50:41.840 
Epoch 907/1000 
	 loss: 27.2303, MinusLogProbMetric: 27.2303, val_loss: 28.3574, val_MinusLogProbMetric: 28.3574

Epoch 907: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2303 - MinusLogProbMetric: 27.2303 - val_loss: 28.3574 - val_MinusLogProbMetric: 28.3574 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 908/1000
2023-10-26 03:51:24.708 
Epoch 908/1000 
	 loss: 27.2313, MinusLogProbMetric: 27.2313, val_loss: 28.3567, val_MinusLogProbMetric: 28.3567

Epoch 908: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2313 - MinusLogProbMetric: 27.2313 - val_loss: 28.3567 - val_MinusLogProbMetric: 28.3567 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 909/1000
2023-10-26 03:52:07.583 
Epoch 909/1000 
	 loss: 27.2309, MinusLogProbMetric: 27.2309, val_loss: 28.3567, val_MinusLogProbMetric: 28.3567

Epoch 909: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2309 - MinusLogProbMetric: 27.2309 - val_loss: 28.3567 - val_MinusLogProbMetric: 28.3567 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 910/1000
2023-10-26 03:52:50.212 
Epoch 910/1000 
	 loss: 27.2310, MinusLogProbMetric: 27.2310, val_loss: 28.3588, val_MinusLogProbMetric: 28.3588

Epoch 910: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2310 - MinusLogProbMetric: 27.2310 - val_loss: 28.3588 - val_MinusLogProbMetric: 28.3588 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 911/1000
2023-10-26 03:53:32.820 
Epoch 911/1000 
	 loss: 27.2301, MinusLogProbMetric: 27.2301, val_loss: 28.3583, val_MinusLogProbMetric: 28.3583

Epoch 911: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2301 - MinusLogProbMetric: 27.2301 - val_loss: 28.3583 - val_MinusLogProbMetric: 28.3583 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 912/1000
2023-10-26 03:54:15.415 
Epoch 912/1000 
	 loss: 27.2307, MinusLogProbMetric: 27.2307, val_loss: 28.3576, val_MinusLogProbMetric: 28.3576

Epoch 912: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2307 - MinusLogProbMetric: 27.2307 - val_loss: 28.3576 - val_MinusLogProbMetric: 28.3576 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 913/1000
2023-10-26 03:54:57.954 
Epoch 913/1000 
	 loss: 27.2297, MinusLogProbMetric: 27.2297, val_loss: 28.3563, val_MinusLogProbMetric: 28.3563

Epoch 913: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2297 - MinusLogProbMetric: 27.2297 - val_loss: 28.3563 - val_MinusLogProbMetric: 28.3563 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 914/1000
2023-10-26 03:55:40.606 
Epoch 914/1000 
	 loss: 27.2304, MinusLogProbMetric: 27.2304, val_loss: 28.3533, val_MinusLogProbMetric: 28.3533

Epoch 914: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2304 - MinusLogProbMetric: 27.2304 - val_loss: 28.3533 - val_MinusLogProbMetric: 28.3533 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 915/1000
2023-10-26 03:56:23.135 
Epoch 915/1000 
	 loss: 27.2291, MinusLogProbMetric: 27.2291, val_loss: 28.3594, val_MinusLogProbMetric: 28.3594

Epoch 915: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2291 - MinusLogProbMetric: 27.2291 - val_loss: 28.3594 - val_MinusLogProbMetric: 28.3594 - lr: 5.2083e-06 - 43s/epoch - 217ms/step
Epoch 916/1000
2023-10-26 03:57:05.826 
Epoch 916/1000 
	 loss: 27.2308, MinusLogProbMetric: 27.2308, val_loss: 28.3630, val_MinusLogProbMetric: 28.3630

Epoch 916: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2308 - MinusLogProbMetric: 27.2308 - val_loss: 28.3630 - val_MinusLogProbMetric: 28.3630 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 917/1000
2023-10-26 03:57:48.657 
Epoch 917/1000 
	 loss: 27.2309, MinusLogProbMetric: 27.2309, val_loss: 28.3564, val_MinusLogProbMetric: 28.3564

Epoch 917: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2309 - MinusLogProbMetric: 27.2309 - val_loss: 28.3564 - val_MinusLogProbMetric: 28.3564 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 918/1000
2023-10-26 03:58:31.377 
Epoch 918/1000 
	 loss: 27.2299, MinusLogProbMetric: 27.2299, val_loss: 28.3555, val_MinusLogProbMetric: 28.3555

Epoch 918: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2299 - MinusLogProbMetric: 27.2299 - val_loss: 28.3555 - val_MinusLogProbMetric: 28.3555 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 919/1000
2023-10-26 03:59:14.264 
Epoch 919/1000 
	 loss: 27.2300, MinusLogProbMetric: 27.2300, val_loss: 28.3572, val_MinusLogProbMetric: 28.3572

Epoch 919: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2300 - MinusLogProbMetric: 27.2300 - val_loss: 28.3572 - val_MinusLogProbMetric: 28.3572 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 920/1000
2023-10-26 03:59:57.124 
Epoch 920/1000 
	 loss: 27.2314, MinusLogProbMetric: 27.2314, val_loss: 28.3565, val_MinusLogProbMetric: 28.3565

Epoch 920: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2314 - MinusLogProbMetric: 27.2314 - val_loss: 28.3565 - val_MinusLogProbMetric: 28.3565 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 921/1000
2023-10-26 04:00:40.078 
Epoch 921/1000 
	 loss: 27.2306, MinusLogProbMetric: 27.2306, val_loss: 28.3581, val_MinusLogProbMetric: 28.3581

Epoch 921: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2306 - MinusLogProbMetric: 27.2306 - val_loss: 28.3581 - val_MinusLogProbMetric: 28.3581 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 922/1000
2023-10-26 04:01:22.295 
Epoch 922/1000 
	 loss: 27.2304, MinusLogProbMetric: 27.2304, val_loss: 28.3538, val_MinusLogProbMetric: 28.3538

Epoch 922: val_loss did not improve from 28.35314
196/196 - 42s - loss: 27.2304 - MinusLogProbMetric: 27.2304 - val_loss: 28.3538 - val_MinusLogProbMetric: 28.3538 - lr: 5.2083e-06 - 42s/epoch - 215ms/step
Epoch 923/1000
2023-10-26 04:02:05.299 
Epoch 923/1000 
	 loss: 27.2298, MinusLogProbMetric: 27.2298, val_loss: 28.3561, val_MinusLogProbMetric: 28.3561

Epoch 923: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2298 - MinusLogProbMetric: 27.2298 - val_loss: 28.3561 - val_MinusLogProbMetric: 28.3561 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 924/1000
2023-10-26 04:02:47.637 
Epoch 924/1000 
	 loss: 27.2299, MinusLogProbMetric: 27.2299, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 924: val_loss did not improve from 28.35314
196/196 - 42s - loss: 27.2299 - MinusLogProbMetric: 27.2299 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 5.2083e-06 - 42s/epoch - 216ms/step
Epoch 925/1000
2023-10-26 04:03:29.963 
Epoch 925/1000 
	 loss: 27.2294, MinusLogProbMetric: 27.2294, val_loss: 28.3538, val_MinusLogProbMetric: 28.3538

Epoch 925: val_loss did not improve from 28.35314
196/196 - 42s - loss: 27.2294 - MinusLogProbMetric: 27.2294 - val_loss: 28.3538 - val_MinusLogProbMetric: 28.3538 - lr: 5.2083e-06 - 42s/epoch - 216ms/step
Epoch 926/1000
2023-10-26 04:04:12.450 
Epoch 926/1000 
	 loss: 27.2296, MinusLogProbMetric: 27.2296, val_loss: 28.3560, val_MinusLogProbMetric: 28.3560

Epoch 926: val_loss did not improve from 28.35314
196/196 - 42s - loss: 27.2296 - MinusLogProbMetric: 27.2296 - val_loss: 28.3560 - val_MinusLogProbMetric: 28.3560 - lr: 5.2083e-06 - 42s/epoch - 217ms/step
Epoch 927/1000
2023-10-26 04:04:55.453 
Epoch 927/1000 
	 loss: 27.2298, MinusLogProbMetric: 27.2298, val_loss: 28.3627, val_MinusLogProbMetric: 28.3627

Epoch 927: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2298 - MinusLogProbMetric: 27.2298 - val_loss: 28.3627 - val_MinusLogProbMetric: 28.3627 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 928/1000
2023-10-26 04:05:38.441 
Epoch 928/1000 
	 loss: 27.2305, MinusLogProbMetric: 27.2305, val_loss: 28.3573, val_MinusLogProbMetric: 28.3573

Epoch 928: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2305 - MinusLogProbMetric: 27.2305 - val_loss: 28.3573 - val_MinusLogProbMetric: 28.3573 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 929/1000
2023-10-26 04:06:21.319 
Epoch 929/1000 
	 loss: 27.2292, MinusLogProbMetric: 27.2292, val_loss: 28.3538, val_MinusLogProbMetric: 28.3538

Epoch 929: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2292 - MinusLogProbMetric: 27.2292 - val_loss: 28.3538 - val_MinusLogProbMetric: 28.3538 - lr: 5.2083e-06 - 43s/epoch - 219ms/step
Epoch 930/1000
2023-10-26 04:07:04.146 
Epoch 930/1000 
	 loss: 27.2302, MinusLogProbMetric: 27.2302, val_loss: 28.3582, val_MinusLogProbMetric: 28.3582

Epoch 930: val_loss did not improve from 28.35314
196/196 - 43s - loss: 27.2302 - MinusLogProbMetric: 27.2302 - val_loss: 28.3582 - val_MinusLogProbMetric: 28.3582 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 931/1000
2023-10-26 04:07:46.769 
Epoch 931/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3513, val_MinusLogProbMetric: 28.3513

Epoch 931: val_loss improved from 28.35314 to 28.35134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 43s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3513 - val_MinusLogProbMetric: 28.3513 - lr: 2.6042e-06 - 43s/epoch - 222ms/step
Epoch 932/1000
2023-10-26 04:08:30.542 
Epoch 932/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3543, val_MinusLogProbMetric: 28.3543

Epoch 932: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3543 - val_MinusLogProbMetric: 28.3543 - lr: 2.6042e-06 - 43s/epoch - 219ms/step
Epoch 933/1000
2023-10-26 04:09:13.127 
Epoch 933/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 28.3563, val_MinusLogProbMetric: 28.3563

Epoch 933: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 28.3563 - val_MinusLogProbMetric: 28.3563 - lr: 2.6042e-06 - 43s/epoch - 217ms/step
Epoch 934/1000
2023-10-26 04:09:55.800 
Epoch 934/1000 
	 loss: 27.2252, MinusLogProbMetric: 27.2252, val_loss: 28.3548, val_MinusLogProbMetric: 28.3548

Epoch 934: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2252 - MinusLogProbMetric: 27.2252 - val_loss: 28.3548 - val_MinusLogProbMetric: 28.3548 - lr: 2.6042e-06 - 43s/epoch - 218ms/step
Epoch 935/1000
2023-10-26 04:10:38.379 
Epoch 935/1000 
	 loss: 27.2257, MinusLogProbMetric: 27.2257, val_loss: 28.3550, val_MinusLogProbMetric: 28.3550

Epoch 935: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2257 - MinusLogProbMetric: 27.2257 - val_loss: 28.3550 - val_MinusLogProbMetric: 28.3550 - lr: 2.6042e-06 - 43s/epoch - 217ms/step
Epoch 936/1000
2023-10-26 04:11:20.112 
Epoch 936/1000 
	 loss: 27.2249, MinusLogProbMetric: 27.2249, val_loss: 28.3527, val_MinusLogProbMetric: 28.3527

Epoch 936: val_loss did not improve from 28.35134
196/196 - 42s - loss: 27.2249 - MinusLogProbMetric: 27.2249 - val_loss: 28.3527 - val_MinusLogProbMetric: 28.3527 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 937/1000
2023-10-26 04:12:01.971 
Epoch 937/1000 
	 loss: 27.2256, MinusLogProbMetric: 27.2256, val_loss: 28.3536, val_MinusLogProbMetric: 28.3536

Epoch 937: val_loss did not improve from 28.35134
196/196 - 42s - loss: 27.2256 - MinusLogProbMetric: 27.2256 - val_loss: 28.3536 - val_MinusLogProbMetric: 28.3536 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 938/1000
2023-10-26 04:12:44.019 
Epoch 938/1000 
	 loss: 27.2253, MinusLogProbMetric: 27.2253, val_loss: 28.3558, val_MinusLogProbMetric: 28.3558

Epoch 938: val_loss did not improve from 28.35134
196/196 - 42s - loss: 27.2253 - MinusLogProbMetric: 27.2253 - val_loss: 28.3558 - val_MinusLogProbMetric: 28.3558 - lr: 2.6042e-06 - 42s/epoch - 215ms/step
Epoch 939/1000
2023-10-26 04:13:24.224 
Epoch 939/1000 
	 loss: 27.2252, MinusLogProbMetric: 27.2252, val_loss: 28.3538, val_MinusLogProbMetric: 28.3538

Epoch 939: val_loss did not improve from 28.35134
196/196 - 40s - loss: 27.2252 - MinusLogProbMetric: 27.2252 - val_loss: 28.3538 - val_MinusLogProbMetric: 28.3538 - lr: 2.6042e-06 - 40s/epoch - 205ms/step
Epoch 940/1000
2023-10-26 04:14:02.393 
Epoch 940/1000 
	 loss: 27.2253, MinusLogProbMetric: 27.2253, val_loss: 28.3548, val_MinusLogProbMetric: 28.3548

Epoch 940: val_loss did not improve from 28.35134
196/196 - 38s - loss: 27.2253 - MinusLogProbMetric: 27.2253 - val_loss: 28.3548 - val_MinusLogProbMetric: 28.3548 - lr: 2.6042e-06 - 38s/epoch - 195ms/step
Epoch 941/1000
2023-10-26 04:14:45.331 
Epoch 941/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 28.3542, val_MinusLogProbMetric: 28.3542

Epoch 941: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 28.3542 - val_MinusLogProbMetric: 28.3542 - lr: 2.6042e-06 - 43s/epoch - 219ms/step
Epoch 942/1000
2023-10-26 04:15:27.963 
Epoch 942/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 28.3539, val_MinusLogProbMetric: 28.3539

Epoch 942: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 28.3539 - val_MinusLogProbMetric: 28.3539 - lr: 2.6042e-06 - 43s/epoch - 218ms/step
Epoch 943/1000
2023-10-26 04:16:10.814 
Epoch 943/1000 
	 loss: 27.2248, MinusLogProbMetric: 27.2248, val_loss: 28.3555, val_MinusLogProbMetric: 28.3555

Epoch 943: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2248 - MinusLogProbMetric: 27.2248 - val_loss: 28.3555 - val_MinusLogProbMetric: 28.3555 - lr: 2.6042e-06 - 43s/epoch - 219ms/step
Epoch 944/1000
2023-10-26 04:16:53.041 
Epoch 944/1000 
	 loss: 27.2248, MinusLogProbMetric: 27.2248, val_loss: 28.3521, val_MinusLogProbMetric: 28.3521

Epoch 944: val_loss did not improve from 28.35134
196/196 - 42s - loss: 27.2248 - MinusLogProbMetric: 27.2248 - val_loss: 28.3521 - val_MinusLogProbMetric: 28.3521 - lr: 2.6042e-06 - 42s/epoch - 215ms/step
Epoch 945/1000
2023-10-26 04:17:35.377 
Epoch 945/1000 
	 loss: 27.2252, MinusLogProbMetric: 27.2252, val_loss: 28.3535, val_MinusLogProbMetric: 28.3535

Epoch 945: val_loss did not improve from 28.35134
196/196 - 42s - loss: 27.2252 - MinusLogProbMetric: 27.2252 - val_loss: 28.3535 - val_MinusLogProbMetric: 28.3535 - lr: 2.6042e-06 - 42s/epoch - 216ms/step
Epoch 946/1000
2023-10-26 04:18:18.207 
Epoch 946/1000 
	 loss: 27.2245, MinusLogProbMetric: 27.2245, val_loss: 28.3530, val_MinusLogProbMetric: 28.3530

Epoch 946: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2245 - MinusLogProbMetric: 27.2245 - val_loss: 28.3530 - val_MinusLogProbMetric: 28.3530 - lr: 2.6042e-06 - 43s/epoch - 218ms/step
Epoch 947/1000
2023-10-26 04:19:01.094 
Epoch 947/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3548, val_MinusLogProbMetric: 28.3548

Epoch 947: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3548 - val_MinusLogProbMetric: 28.3548 - lr: 2.6042e-06 - 43s/epoch - 219ms/step
Epoch 948/1000
2023-10-26 04:19:43.856 
Epoch 948/1000 
	 loss: 27.2254, MinusLogProbMetric: 27.2254, val_loss: 28.3528, val_MinusLogProbMetric: 28.3528

Epoch 948: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2254 - MinusLogProbMetric: 27.2254 - val_loss: 28.3528 - val_MinusLogProbMetric: 28.3528 - lr: 2.6042e-06 - 43s/epoch - 218ms/step
Epoch 949/1000
2023-10-26 04:20:26.811 
Epoch 949/1000 
	 loss: 27.2253, MinusLogProbMetric: 27.2253, val_loss: 28.3515, val_MinusLogProbMetric: 28.3515

Epoch 949: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2253 - MinusLogProbMetric: 27.2253 - val_loss: 28.3515 - val_MinusLogProbMetric: 28.3515 - lr: 2.6042e-06 - 43s/epoch - 219ms/step
Epoch 950/1000
2023-10-26 04:21:09.710 
Epoch 950/1000 
	 loss: 27.2248, MinusLogProbMetric: 27.2248, val_loss: 28.3571, val_MinusLogProbMetric: 28.3571

Epoch 950: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2248 - MinusLogProbMetric: 27.2248 - val_loss: 28.3571 - val_MinusLogProbMetric: 28.3571 - lr: 2.6042e-06 - 43s/epoch - 219ms/step
Epoch 951/1000
2023-10-26 04:21:52.597 
Epoch 951/1000 
	 loss: 27.2249, MinusLogProbMetric: 27.2249, val_loss: 28.3554, val_MinusLogProbMetric: 28.3554

Epoch 951: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2249 - MinusLogProbMetric: 27.2249 - val_loss: 28.3554 - val_MinusLogProbMetric: 28.3554 - lr: 2.6042e-06 - 43s/epoch - 219ms/step
Epoch 952/1000
2023-10-26 04:22:35.598 
Epoch 952/1000 
	 loss: 27.2253, MinusLogProbMetric: 27.2253, val_loss: 28.3545, val_MinusLogProbMetric: 28.3545

Epoch 952: val_loss did not improve from 28.35134
196/196 - 43s - loss: 27.2253 - MinusLogProbMetric: 27.2253 - val_loss: 28.3545 - val_MinusLogProbMetric: 28.3545 - lr: 2.6042e-06 - 43s/epoch - 219ms/step
Epoch 953/1000
2023-10-26 04:23:12.688 
Epoch 953/1000 
	 loss: 27.2249, MinusLogProbMetric: 27.2249, val_loss: 28.3548, val_MinusLogProbMetric: 28.3548

Epoch 953: val_loss did not improve from 28.35134
196/196 - 37s - loss: 27.2249 - MinusLogProbMetric: 27.2249 - val_loss: 28.3548 - val_MinusLogProbMetric: 28.3548 - lr: 2.6042e-06 - 37s/epoch - 189ms/step
Epoch 954/1000
2023-10-26 04:23:46.829 
Epoch 954/1000 
	 loss: 27.2260, MinusLogProbMetric: 27.2260, val_loss: 28.3532, val_MinusLogProbMetric: 28.3532

Epoch 954: val_loss did not improve from 28.35134
196/196 - 34s - loss: 27.2260 - MinusLogProbMetric: 27.2260 - val_loss: 28.3532 - val_MinusLogProbMetric: 28.3532 - lr: 2.6042e-06 - 34s/epoch - 174ms/step
Epoch 955/1000
2023-10-26 04:24:23.445 
Epoch 955/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 955: val_loss did not improve from 28.35134
196/196 - 37s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 2.6042e-06 - 37s/epoch - 187ms/step
Epoch 956/1000
2023-10-26 04:25:05.334 
Epoch 956/1000 
	 loss: 27.2248, MinusLogProbMetric: 27.2248, val_loss: 28.3555, val_MinusLogProbMetric: 28.3555

Epoch 956: val_loss did not improve from 28.35134
196/196 - 42s - loss: 27.2248 - MinusLogProbMetric: 27.2248 - val_loss: 28.3555 - val_MinusLogProbMetric: 28.3555 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 957/1000
2023-10-26 04:25:41.950 
Epoch 957/1000 
	 loss: 27.2243, MinusLogProbMetric: 27.2243, val_loss: 28.3565, val_MinusLogProbMetric: 28.3565

Epoch 957: val_loss did not improve from 28.35134
196/196 - 37s - loss: 27.2243 - MinusLogProbMetric: 27.2243 - val_loss: 28.3565 - val_MinusLogProbMetric: 28.3565 - lr: 2.6042e-06 - 37s/epoch - 187ms/step
Epoch 958/1000
2023-10-26 04:26:15.859 
Epoch 958/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3510, val_MinusLogProbMetric: 28.3510

Epoch 958: val_loss improved from 28.35134 to 28.35102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_364/weights/best_weights.h5
196/196 - 35s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3510 - val_MinusLogProbMetric: 28.3510 - lr: 2.6042e-06 - 35s/epoch - 177ms/step
Epoch 959/1000
2023-10-26 04:26:51.292 
Epoch 959/1000 
	 loss: 27.2254, MinusLogProbMetric: 27.2254, val_loss: 28.3580, val_MinusLogProbMetric: 28.3580

Epoch 959: val_loss did not improve from 28.35102
196/196 - 35s - loss: 27.2254 - MinusLogProbMetric: 27.2254 - val_loss: 28.3580 - val_MinusLogProbMetric: 28.3580 - lr: 2.6042e-06 - 35s/epoch - 177ms/step
Epoch 960/1000
2023-10-26 04:27:32.588 
Epoch 960/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3539, val_MinusLogProbMetric: 28.3539

Epoch 960: val_loss did not improve from 28.35102
196/196 - 41s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3539 - val_MinusLogProbMetric: 28.3539 - lr: 2.6042e-06 - 41s/epoch - 211ms/step
Epoch 961/1000
2023-10-26 04:28:07.582 
Epoch 961/1000 
	 loss: 27.2242, MinusLogProbMetric: 27.2242, val_loss: 28.3540, val_MinusLogProbMetric: 28.3540

Epoch 961: val_loss did not improve from 28.35102
196/196 - 35s - loss: 27.2242 - MinusLogProbMetric: 27.2242 - val_loss: 28.3540 - val_MinusLogProbMetric: 28.3540 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 962/1000
2023-10-26 04:28:41.675 
Epoch 962/1000 
	 loss: 27.2257, MinusLogProbMetric: 27.2257, val_loss: 28.3540, val_MinusLogProbMetric: 28.3540

Epoch 962: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2257 - MinusLogProbMetric: 27.2257 - val_loss: 28.3540 - val_MinusLogProbMetric: 28.3540 - lr: 2.6042e-06 - 34s/epoch - 174ms/step
Epoch 963/1000
2023-10-26 04:29:17.306 
Epoch 963/1000 
	 loss: 27.2249, MinusLogProbMetric: 27.2249, val_loss: 28.3561, val_MinusLogProbMetric: 28.3561

Epoch 963: val_loss did not improve from 28.35102
196/196 - 36s - loss: 27.2249 - MinusLogProbMetric: 27.2249 - val_loss: 28.3561 - val_MinusLogProbMetric: 28.3561 - lr: 2.6042e-06 - 36s/epoch - 182ms/step
Epoch 964/1000
2023-10-26 04:29:53.193 
Epoch 964/1000 
	 loss: 27.2245, MinusLogProbMetric: 27.2245, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 964: val_loss did not improve from 28.35102
196/196 - 36s - loss: 27.2245 - MinusLogProbMetric: 27.2245 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 2.6042e-06 - 36s/epoch - 183ms/step
Epoch 965/1000
2023-10-26 04:30:26.791 
Epoch 965/1000 
	 loss: 27.2253, MinusLogProbMetric: 27.2253, val_loss: 28.3533, val_MinusLogProbMetric: 28.3533

Epoch 965: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2253 - MinusLogProbMetric: 27.2253 - val_loss: 28.3533 - val_MinusLogProbMetric: 28.3533 - lr: 2.6042e-06 - 34s/epoch - 171ms/step
Epoch 966/1000
2023-10-26 04:31:01.453 
Epoch 966/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 28.3543, val_MinusLogProbMetric: 28.3543

Epoch 966: val_loss did not improve from 28.35102
196/196 - 35s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 28.3543 - val_MinusLogProbMetric: 28.3543 - lr: 2.6042e-06 - 35s/epoch - 177ms/step
Epoch 967/1000
2023-10-26 04:31:38.755 
Epoch 967/1000 
	 loss: 27.2245, MinusLogProbMetric: 27.2245, val_loss: 28.3552, val_MinusLogProbMetric: 28.3552

Epoch 967: val_loss did not improve from 28.35102
196/196 - 37s - loss: 27.2245 - MinusLogProbMetric: 27.2245 - val_loss: 28.3552 - val_MinusLogProbMetric: 28.3552 - lr: 2.6042e-06 - 37s/epoch - 190ms/step
Epoch 968/1000
2023-10-26 04:32:15.323 
Epoch 968/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3564, val_MinusLogProbMetric: 28.3564

Epoch 968: val_loss did not improve from 28.35102
196/196 - 37s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3564 - val_MinusLogProbMetric: 28.3564 - lr: 2.6042e-06 - 37s/epoch - 187ms/step
Epoch 969/1000
2023-10-26 04:32:50.664 
Epoch 969/1000 
	 loss: 27.2244, MinusLogProbMetric: 27.2244, val_loss: 28.3563, val_MinusLogProbMetric: 28.3563

Epoch 969: val_loss did not improve from 28.35102
196/196 - 35s - loss: 27.2244 - MinusLogProbMetric: 27.2244 - val_loss: 28.3563 - val_MinusLogProbMetric: 28.3563 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Epoch 970/1000
2023-10-26 04:33:24.258 
Epoch 970/1000 
	 loss: 27.2247, MinusLogProbMetric: 27.2247, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 970: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2247 - MinusLogProbMetric: 27.2247 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 2.6042e-06 - 34s/epoch - 171ms/step
Epoch 971/1000
2023-10-26 04:34:02.377 
Epoch 971/1000 
	 loss: 27.2244, MinusLogProbMetric: 27.2244, val_loss: 28.3561, val_MinusLogProbMetric: 28.3561

Epoch 971: val_loss did not improve from 28.35102
196/196 - 38s - loss: 27.2244 - MinusLogProbMetric: 27.2244 - val_loss: 28.3561 - val_MinusLogProbMetric: 28.3561 - lr: 2.6042e-06 - 38s/epoch - 194ms/step
Epoch 972/1000
2023-10-26 04:34:43.374 
Epoch 972/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3560, val_MinusLogProbMetric: 28.3560

Epoch 972: val_loss did not improve from 28.35102
196/196 - 41s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3560 - val_MinusLogProbMetric: 28.3560 - lr: 2.6042e-06 - 41s/epoch - 209ms/step
Epoch 973/1000
2023-10-26 04:35:18.116 
Epoch 973/1000 
	 loss: 27.2249, MinusLogProbMetric: 27.2249, val_loss: 28.3552, val_MinusLogProbMetric: 28.3552

Epoch 973: val_loss did not improve from 28.35102
196/196 - 35s - loss: 27.2249 - MinusLogProbMetric: 27.2249 - val_loss: 28.3552 - val_MinusLogProbMetric: 28.3552 - lr: 2.6042e-06 - 35s/epoch - 177ms/step
Epoch 974/1000
2023-10-26 04:35:52.047 
Epoch 974/1000 
	 loss: 27.2248, MinusLogProbMetric: 27.2248, val_loss: 28.3564, val_MinusLogProbMetric: 28.3564

Epoch 974: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2248 - MinusLogProbMetric: 27.2248 - val_loss: 28.3564 - val_MinusLogProbMetric: 28.3564 - lr: 2.6042e-06 - 34s/epoch - 173ms/step
Epoch 975/1000
2023-10-26 04:36:32.359 
Epoch 975/1000 
	 loss: 27.2242, MinusLogProbMetric: 27.2242, val_loss: 28.3540, val_MinusLogProbMetric: 28.3540

Epoch 975: val_loss did not improve from 28.35102
196/196 - 40s - loss: 27.2242 - MinusLogProbMetric: 27.2242 - val_loss: 28.3540 - val_MinusLogProbMetric: 28.3540 - lr: 2.6042e-06 - 40s/epoch - 206ms/step
Epoch 976/1000
2023-10-26 04:37:11.670 
Epoch 976/1000 
	 loss: 27.2247, MinusLogProbMetric: 27.2247, val_loss: 28.3512, val_MinusLogProbMetric: 28.3512

Epoch 976: val_loss did not improve from 28.35102
196/196 - 39s - loss: 27.2247 - MinusLogProbMetric: 27.2247 - val_loss: 28.3512 - val_MinusLogProbMetric: 28.3512 - lr: 2.6042e-06 - 39s/epoch - 201ms/step
Epoch 977/1000
2023-10-26 04:37:45.433 
Epoch 977/1000 
	 loss: 27.2247, MinusLogProbMetric: 27.2247, val_loss: 28.3535, val_MinusLogProbMetric: 28.3535

Epoch 977: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2247 - MinusLogProbMetric: 27.2247 - val_loss: 28.3535 - val_MinusLogProbMetric: 28.3535 - lr: 2.6042e-06 - 34s/epoch - 172ms/step
Epoch 978/1000
2023-10-26 04:38:19.416 
Epoch 978/1000 
	 loss: 27.2241, MinusLogProbMetric: 27.2241, val_loss: 28.3564, val_MinusLogProbMetric: 28.3564

Epoch 978: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2241 - MinusLogProbMetric: 27.2241 - val_loss: 28.3564 - val_MinusLogProbMetric: 28.3564 - lr: 2.6042e-06 - 34s/epoch - 173ms/step
Epoch 979/1000
2023-10-26 04:38:59.724 
Epoch 979/1000 
	 loss: 27.2249, MinusLogProbMetric: 27.2249, val_loss: 28.3556, val_MinusLogProbMetric: 28.3556

Epoch 979: val_loss did not improve from 28.35102
196/196 - 40s - loss: 27.2249 - MinusLogProbMetric: 27.2249 - val_loss: 28.3556 - val_MinusLogProbMetric: 28.3556 - lr: 2.6042e-06 - 40s/epoch - 206ms/step
Epoch 980/1000
2023-10-26 04:39:38.343 
Epoch 980/1000 
	 loss: 27.2241, MinusLogProbMetric: 27.2241, val_loss: 28.3560, val_MinusLogProbMetric: 28.3560

Epoch 980: val_loss did not improve from 28.35102
196/196 - 39s - loss: 27.2241 - MinusLogProbMetric: 27.2241 - val_loss: 28.3560 - val_MinusLogProbMetric: 28.3560 - lr: 2.6042e-06 - 39s/epoch - 197ms/step
Epoch 981/1000
2023-10-26 04:40:12.595 
Epoch 981/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 981: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 2.6042e-06 - 34s/epoch - 175ms/step
Epoch 982/1000
2023-10-26 04:40:46.241 
Epoch 982/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3546, val_MinusLogProbMetric: 28.3546

Epoch 982: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3546 - val_MinusLogProbMetric: 28.3546 - lr: 2.6042e-06 - 34s/epoch - 172ms/step
Epoch 983/1000
2023-10-26 04:41:26.270 
Epoch 983/1000 
	 loss: 27.2245, MinusLogProbMetric: 27.2245, val_loss: 28.3604, val_MinusLogProbMetric: 28.3604

Epoch 983: val_loss did not improve from 28.35102
196/196 - 40s - loss: 27.2245 - MinusLogProbMetric: 27.2245 - val_loss: 28.3604 - val_MinusLogProbMetric: 28.3604 - lr: 2.6042e-06 - 40s/epoch - 204ms/step
Epoch 984/1000
2023-10-26 04:42:06.615 
Epoch 984/1000 
	 loss: 27.2246, MinusLogProbMetric: 27.2246, val_loss: 28.3557, val_MinusLogProbMetric: 28.3557

Epoch 984: val_loss did not improve from 28.35102
196/196 - 40s - loss: 27.2246 - MinusLogProbMetric: 27.2246 - val_loss: 28.3557 - val_MinusLogProbMetric: 28.3557 - lr: 2.6042e-06 - 40s/epoch - 206ms/step
Epoch 985/1000
2023-10-26 04:42:41.008 
Epoch 985/1000 
	 loss: 27.2245, MinusLogProbMetric: 27.2245, val_loss: 28.3512, val_MinusLogProbMetric: 28.3512

Epoch 985: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2245 - MinusLogProbMetric: 27.2245 - val_loss: 28.3512 - val_MinusLogProbMetric: 28.3512 - lr: 2.6042e-06 - 34s/epoch - 175ms/step
Epoch 986/1000
2023-10-26 04:43:15.146 
Epoch 986/1000 
	 loss: 27.2244, MinusLogProbMetric: 27.2244, val_loss: 28.3540, val_MinusLogProbMetric: 28.3540

Epoch 986: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2244 - MinusLogProbMetric: 27.2244 - val_loss: 28.3540 - val_MinusLogProbMetric: 28.3540 - lr: 2.6042e-06 - 34s/epoch - 174ms/step
Epoch 987/1000
2023-10-26 04:43:54.854 
Epoch 987/1000 
	 loss: 27.2243, MinusLogProbMetric: 27.2243, val_loss: 28.3540, val_MinusLogProbMetric: 28.3540

Epoch 987: val_loss did not improve from 28.35102
196/196 - 40s - loss: 27.2243 - MinusLogProbMetric: 27.2243 - val_loss: 28.3540 - val_MinusLogProbMetric: 28.3540 - lr: 2.6042e-06 - 40s/epoch - 203ms/step
Epoch 988/1000
2023-10-26 04:44:36.563 
Epoch 988/1000 
	 loss: 27.2243, MinusLogProbMetric: 27.2243, val_loss: 28.3554, val_MinusLogProbMetric: 28.3554

Epoch 988: val_loss did not improve from 28.35102
196/196 - 42s - loss: 27.2243 - MinusLogProbMetric: 27.2243 - val_loss: 28.3554 - val_MinusLogProbMetric: 28.3554 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 989/1000
2023-10-26 04:45:10.664 
Epoch 989/1000 
	 loss: 27.2242, MinusLogProbMetric: 27.2242, val_loss: 28.3535, val_MinusLogProbMetric: 28.3535

Epoch 989: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2242 - MinusLogProbMetric: 27.2242 - val_loss: 28.3535 - val_MinusLogProbMetric: 28.3535 - lr: 2.6042e-06 - 34s/epoch - 174ms/step
Epoch 990/1000
2023-10-26 04:45:44.416 
Epoch 990/1000 
	 loss: 27.2244, MinusLogProbMetric: 27.2244, val_loss: 28.3569, val_MinusLogProbMetric: 28.3569

Epoch 990: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2244 - MinusLogProbMetric: 27.2244 - val_loss: 28.3569 - val_MinusLogProbMetric: 28.3569 - lr: 2.6042e-06 - 34s/epoch - 172ms/step
Epoch 991/1000
2023-10-26 04:46:21.516 
Epoch 991/1000 
	 loss: 27.2247, MinusLogProbMetric: 27.2247, val_loss: 28.3558, val_MinusLogProbMetric: 28.3558

Epoch 991: val_loss did not improve from 28.35102
196/196 - 37s - loss: 27.2247 - MinusLogProbMetric: 27.2247 - val_loss: 28.3558 - val_MinusLogProbMetric: 28.3558 - lr: 2.6042e-06 - 37s/epoch - 189ms/step
Epoch 992/1000
2023-10-26 04:46:58.707 
Epoch 992/1000 
	 loss: 27.2246, MinusLogProbMetric: 27.2246, val_loss: 28.3541, val_MinusLogProbMetric: 28.3541

Epoch 992: val_loss did not improve from 28.35102
196/196 - 37s - loss: 27.2246 - MinusLogProbMetric: 27.2246 - val_loss: 28.3541 - val_MinusLogProbMetric: 28.3541 - lr: 2.6042e-06 - 37s/epoch - 190ms/step
Epoch 993/1000
2023-10-26 04:47:32.853 
Epoch 993/1000 
	 loss: 27.2236, MinusLogProbMetric: 27.2236, val_loss: 28.3533, val_MinusLogProbMetric: 28.3533

Epoch 993: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2236 - MinusLogProbMetric: 27.2236 - val_loss: 28.3533 - val_MinusLogProbMetric: 28.3533 - lr: 2.6042e-06 - 34s/epoch - 174ms/step
Epoch 994/1000
2023-10-26 04:48:06.374 
Epoch 994/1000 
	 loss: 27.2240, MinusLogProbMetric: 27.2240, val_loss: 28.3540, val_MinusLogProbMetric: 28.3540

Epoch 994: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2240 - MinusLogProbMetric: 27.2240 - val_loss: 28.3540 - val_MinusLogProbMetric: 28.3540 - lr: 2.6042e-06 - 34s/epoch - 171ms/step
Epoch 995/1000
2023-10-26 04:48:45.241 
Epoch 995/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 28.3547, val_MinusLogProbMetric: 28.3547

Epoch 995: val_loss did not improve from 28.35102
196/196 - 39s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 28.3547 - val_MinusLogProbMetric: 28.3547 - lr: 2.6042e-06 - 39s/epoch - 198ms/step
Epoch 996/1000
2023-10-26 04:49:22.433 
Epoch 996/1000 
	 loss: 27.2240, MinusLogProbMetric: 27.2240, val_loss: 28.3574, val_MinusLogProbMetric: 28.3574

Epoch 996: val_loss did not improve from 28.35102
196/196 - 37s - loss: 27.2240 - MinusLogProbMetric: 27.2240 - val_loss: 28.3574 - val_MinusLogProbMetric: 28.3574 - lr: 2.6042e-06 - 37s/epoch - 190ms/step
Epoch 997/1000
2023-10-26 04:49:57.339 
Epoch 997/1000 
	 loss: 27.2246, MinusLogProbMetric: 27.2246, val_loss: 28.3514, val_MinusLogProbMetric: 28.3514

Epoch 997: val_loss did not improve from 28.35102
196/196 - 35s - loss: 27.2246 - MinusLogProbMetric: 27.2246 - val_loss: 28.3514 - val_MinusLogProbMetric: 28.3514 - lr: 2.6042e-06 - 35s/epoch - 178ms/step
Epoch 998/1000
2023-10-26 04:50:31.296 
Epoch 998/1000 
	 loss: 27.2242, MinusLogProbMetric: 27.2242, val_loss: 28.3538, val_MinusLogProbMetric: 28.3538

Epoch 998: val_loss did not improve from 28.35102
196/196 - 34s - loss: 27.2242 - MinusLogProbMetric: 27.2242 - val_loss: 28.3538 - val_MinusLogProbMetric: 28.3538 - lr: 2.6042e-06 - 34s/epoch - 173ms/step
Epoch 999/1000
2023-10-26 04:51:12.037 
Epoch 999/1000 
	 loss: 27.2237, MinusLogProbMetric: 27.2237, val_loss: 28.3548, val_MinusLogProbMetric: 28.3548

Epoch 999: val_loss did not improve from 28.35102
196/196 - 41s - loss: 27.2237 - MinusLogProbMetric: 27.2237 - val_loss: 28.3548 - val_MinusLogProbMetric: 28.3548 - lr: 2.6042e-06 - 41s/epoch - 208ms/step
Epoch 1000/1000
2023-10-26 04:51:53.046 
Epoch 1000/1000 
	 loss: 27.2240, MinusLogProbMetric: 27.2240, val_loss: 28.3604, val_MinusLogProbMetric: 28.3604

Epoch 1000: val_loss did not improve from 28.35102
196/196 - 41s - loss: 27.2240 - MinusLogProbMetric: 27.2240 - val_loss: 28.3604 - val_MinusLogProbMetric: 28.3604 - lr: 2.6042e-06 - 41s/epoch - 209ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 541.
Model trained in 42101.83 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.63 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.11 s.
===========
Run 364/720 done in 42224.22 s.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

===========
Generating train data for run 375.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_143"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_144 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7ff05d580b20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff00937c490>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff00937c490>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff05d514760>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff009089e40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff00908a3b0>, <keras.callbacks.ModelCheckpoint object at 0x7ff00908a470>, <keras.callbacks.EarlyStopping object at 0x7ff00908a6e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff00908a710>, <keras.callbacks.TerminateOnNaN object at 0x7ff00908a350>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 04:52:03.632482
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:55:00.644 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 177s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 177s/epoch - 902ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 375.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_154"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_155 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7ff740463f70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff7486d7a00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff7486d7a00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff740439e70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff74049b760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff74049bcd0>, <keras.callbacks.ModelCheckpoint object at 0x7ff74049bd90>, <keras.callbacks.EarlyStopping object at 0x7ff74049bc70>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff74049bca0>, <keras.callbacks.TerminateOnNaN object at 0x7ff74049bfa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 04:55:11.765574
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:58:19.876 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 188s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 188s/epoch - 958ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0001111111111111111.
===========
Generating train data for run 375.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_165"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_166 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7ff0a8ceab30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff066c65a80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff066c65a80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff05c388550>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef343c6b00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef343c7460>, <keras.callbacks.ModelCheckpoint object at 0x7fef343c64d0>, <keras.callbacks.EarlyStopping object at 0x7fef343c7e50>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef343c7700>, <keras.callbacks.TerminateOnNaN object at 0x7fef343c7640>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 04:58:33.078430
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:01:33.272 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 180s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 180s/epoch - 918ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 3.703703703703703e-05.
===========
Generating train data for run 375.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_176"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_177 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7ff05e3071c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0a99d49a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0a99d49a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff035cfd2a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fefe755ce20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fefe755e560>, <keras.callbacks.ModelCheckpoint object at 0x7fefe755e350>, <keras.callbacks.EarlyStopping object at 0x7fefe755e920>, <keras.callbacks.ReduceLROnPlateau object at 0x7fefe755e980>, <keras.callbacks.TerminateOnNaN object at 0x7fefe755e410>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 05:01:43.424825
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 196004 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff035325240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:04:40.265 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 177s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 177s/epoch - 902ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.2345679012345677e-05.
===========
Generating train data for run 375.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_187"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_188 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7fef8659e0b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef4f10bc10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef4f10bc10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0c2786680>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0c27dbb80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef4f4305e0>, <keras.callbacks.ModelCheckpoint object at 0x7fef4f4306a0>, <keras.callbacks.EarlyStopping object at 0x7fef4f430910>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef4f430940>, <keras.callbacks.TerminateOnNaN object at 0x7fef4f430580>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 05:04:52.780070
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 196005 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff0c05ecb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:08:01.997 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 189s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 189s/epoch - 964ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.115226337448558e-06.
===========
Generating train data for run 375.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_198"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_199 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7fefe7333d00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fefa43bce20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fefa43bce20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff008455990>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff761043760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff761041ab0>, <keras.callbacks.ModelCheckpoint object at 0x7ff7610414e0>, <keras.callbacks.EarlyStopping object at 0x7ff761041870>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff761041840>, <keras.callbacks.TerminateOnNaN object at 0x7ff761041990>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 05:08:15.903570
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:11:10.598 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 174s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 174s/epoch - 890ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.3717421124828526e-06.
===========
Generating train data for run 375.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_209"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_210 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7ff7376c3d00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff737326a40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff737326a40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef1d177d30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff71edb1360>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff71edb18d0>, <keras.callbacks.ModelCheckpoint object at 0x7ff71edb1990>, <keras.callbacks.EarlyStopping object at 0x7ff71edb1c00>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff71edb1c30>, <keras.callbacks.TerminateOnNaN object at 0x7ff71edb1870>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 05:11:22.273908
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:14:33.651 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 191s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 191s/epoch - 977ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.572473708276175e-07.
===========
Generating train data for run 375.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_220"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_221 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7fef7d3fab30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef8443a470>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef8443a470>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef1c8344f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff03414a530>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff03414aaa0>, <keras.callbacks.ModelCheckpoint object at 0x7ff03414ab60>, <keras.callbacks.EarlyStopping object at 0x7ff03414add0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff03414ae00>, <keras.callbacks.TerminateOnNaN object at 0x7ff03414aa40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 05:14:47.234387
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:17:56.270 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 189s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 189s/epoch - 963ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.524157902758725e-07.
===========
Generating train data for run 375.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_231"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_232 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7fef347c6e60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff034a87df0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff034a87df0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef4fbc5780>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef7e8f1ea0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef7e8f2410>, <keras.callbacks.ModelCheckpoint object at 0x7fef7e8f24d0>, <keras.callbacks.EarlyStopping object at 0x7fef7e8f2740>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef7e8f2770>, <keras.callbacks.TerminateOnNaN object at 0x7fef7e8f23b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 05:18:05.733029
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:21:08.267 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 182s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 182s/epoch - 931ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 5.0805263425290834e-08.
===========
Generating train data for run 375.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_242"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_243 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7ff0673f18a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef56c4d0f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef56c4d0f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0673c2e30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0b9e31300>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0b9e31870>, <keras.callbacks.ModelCheckpoint object at 0x7ff0b9e31930>, <keras.callbacks.EarlyStopping object at 0x7ff0b9e31ba0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0b9e31bd0>, <keras.callbacks.TerminateOnNaN object at 0x7ff0b9e31810>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 05:21:21.465304
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:24:27.637 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 186s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 186s/epoch - 948ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.6935087808430278e-08.
===========
Generating train data for run 375.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_375/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_375
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_253"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_254 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7fef500f1e10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0350122c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0350122c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef007cb100>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef34495f90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_375/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef34496500>, <keras.callbacks.ModelCheckpoint object at 0x7fef344965c0>, <keras.callbacks.EarlyStopping object at 0x7fef34496830>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef34496860>, <keras.callbacks.TerminateOnNaN object at 0x7fef344964a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_375/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 375/720 with hyperparameters:
timestamp = 2023-10-26 05:24:40.267408
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:27:41.713 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7731.3101, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 181s - loss: nan - MinusLogProbMetric: 7731.3101 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 181s/epoch - 925ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 5.645029269476759e-09.
===========
Run 375/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

===========
Generating train data for run 377.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_259"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_260 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7fef4ecd9750>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff05f4a72e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff05f4a72e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef56c55030>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef56fc73a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef56fc7910>, <keras.callbacks.ModelCheckpoint object at 0x7fef56fc79d0>, <keras.callbacks.EarlyStopping object at 0x7fef56fc7c40>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef56fc7c70>, <keras.callbacks.TerminateOnNaN object at 0x7fef56fc78b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:27:46.890142
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:28:54.834 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 68s/epoch - 346ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 377.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_265"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_266 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7ff71ea6f670>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff71e527790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff71e527790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef87e2a740>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff71e4be1d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff71e4be740>, <keras.callbacks.ModelCheckpoint object at 0x7ff71e4be800>, <keras.callbacks.EarlyStopping object at 0x7ff71e4bea70>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff71e4beaa0>, <keras.callbacks.TerminateOnNaN object at 0x7ff71e4be6e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:28:59.669955
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:30:02.840 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 63s/epoch - 322ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 377.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_271"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_272 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7ff7152b1e70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff714f90b50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff714f90b50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feed5133280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff704c5de40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff704c5e3b0>, <keras.callbacks.ModelCheckpoint object at 0x7ff704c5e470>, <keras.callbacks.EarlyStopping object at 0x7ff704c5e6e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff704c5e710>, <keras.callbacks.TerminateOnNaN object at 0x7ff704c5e350>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:30:07.043526
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:31:22.254 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 75s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 75s/epoch - 383ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 377.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_277"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_278 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7ff0b8f46710>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff79d7a0640>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff79d7a0640>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff79dad35e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0a98f64a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0a98f7910>, <keras.callbacks.ModelCheckpoint object at 0x7ff0a98f7f70>, <keras.callbacks.EarlyStopping object at 0x7ff0a98f5900>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0a98f65c0>, <keras.callbacks.TerminateOnNaN object at 0x7ff0a98f62f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:31:27.281495
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:32:30.700 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 63s/epoch - 323ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 377.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_283"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_284 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7fef7c97f760>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0bb772e60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0bb772e60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef346d3c70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feee55b42e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feee55b4850>, <keras.callbacks.ModelCheckpoint object at 0x7feee55b4910>, <keras.callbacks.EarlyStopping object at 0x7feee55b4b80>, <keras.callbacks.ReduceLROnPlateau object at 0x7feee55b4bb0>, <keras.callbacks.TerminateOnNaN object at 0x7feee55b47f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:32:35.986967
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:33:36.326 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 60s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 60s/epoch - 307ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 377.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_289"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_290 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7feeed0b4ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0652c66e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0652c66e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feeed476c50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0ab049cf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0ab02c640>, <keras.callbacks.ModelCheckpoint object at 0x7ff0ab02c700>, <keras.callbacks.EarlyStopping object at 0x7ff0ab02c970>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0ab02c9a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff0ab02c5e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:33:41.010802
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:34:47.875 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 67s/epoch - 341ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 377.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_295"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_296 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7ff034122710>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff2e82b4190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff2e82b4190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff03416e140>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff71eb34e80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff71eb35cc0>, <keras.callbacks.ModelCheckpoint object at 0x7fef7eb83820>, <keras.callbacks.EarlyStopping object at 0x7fef7eb82530>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef7eb83e50>, <keras.callbacks.TerminateOnNaN object at 0x7fef7eb82a40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:34:52.201111
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:35:55.862 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 64s/epoch - 324ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 377.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_301"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_302 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7ff009cca980>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef4d271150>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef4d271150>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0c0bbf880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff05eb5c520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff05eb5ca90>, <keras.callbacks.ModelCheckpoint object at 0x7ff05eb5cb50>, <keras.callbacks.EarlyStopping object at 0x7ff05eb5cdc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff05eb5cdf0>, <keras.callbacks.TerminateOnNaN object at 0x7ff05eb5ca30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:36:01.123591
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:37:05.158 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 64s/epoch - 326ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 377.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_307"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_308 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7ff0649334c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feee5af5d20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feee5af5d20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feee5a30d60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feee5a332b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feee5a33df0>, <keras.callbacks.ModelCheckpoint object at 0x7feee5a31240>, <keras.callbacks.EarlyStopping object at 0x7feee5a33d30>, <keras.callbacks.ReduceLROnPlateau object at 0x7feee5a33b20>, <keras.callbacks.TerminateOnNaN object at 0x7feee5a33f70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:37:19.074732
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:38:20.262 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 61s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 61s/epoch - 311ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 377.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_313"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_314 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7ff0a889dc00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0a8164580>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0a8164580>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff16056f370>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff16057d9c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff16057d120>, <keras.callbacks.ModelCheckpoint object at 0x7ff4001f8100>, <keras.callbacks.EarlyStopping object at 0x7ff16057e200>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4001fa110>, <keras.callbacks.TerminateOnNaN object at 0x7ff16057feb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:38:24.445812
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:39:24.054 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 59s/epoch - 304ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 377.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_377/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_377
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_319"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_320 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7ff6fbf73eb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff6fc2f1ea0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff6fc2f1ea0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feeec23d360>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff6fbfcf6a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_377/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff6fbfcfc10>, <keras.callbacks.ModelCheckpoint object at 0x7ff6fbfcfcd0>, <keras.callbacks.EarlyStopping object at 0x7ff6fbfcff40>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff6fbfcff70>, <keras.callbacks.TerminateOnNaN object at 0x7ff6fbfcfbb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_377/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 377/720 with hyperparameters:
timestamp = 2023-10-26 05:39:28.280983
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:40:43.446 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5687.0610, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 75s - loss: nan - MinusLogProbMetric: 5687.0610 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 75s/epoch - 383ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 377/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 378.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_378/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_378/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_378/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_378
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_325"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_326 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7ff0c034a4a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0bb085300>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0bb085300>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff3486016f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff3487414b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff348743700>, <keras.callbacks.ModelCheckpoint object at 0x7ff348743d90>, <keras.callbacks.EarlyStopping object at 0x7ff348740b50>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff3487400d0>, <keras.callbacks.TerminateOnNaN object at 0x7ff348742380>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_378/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 378/720 with hyperparameters:
timestamp = 2023-10-26 05:40:48.470562
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 46: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:41:59.178 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2096.8315, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 2096.8315 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 71s/epoch - 360ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 378.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_378/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_378/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_378/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_378
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_331"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_332 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7feee4f028f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef4c2707c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef4c2707c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feee5859810>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee38226f20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee38227490>, <keras.callbacks.ModelCheckpoint object at 0x7fee38227550>, <keras.callbacks.EarlyStopping object at 0x7fee382277c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee382277f0>, <keras.callbacks.TerminateOnNaN object at 0x7fee38227430>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_378/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 378/720 with hyperparameters:
timestamp = 2023-10-26 05:42:02.424988
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-26 05:43:38.954 
Epoch 1/1000 
	 loss: 599.5871, MinusLogProbMetric: 599.5871, val_loss: 145.4347, val_MinusLogProbMetric: 145.4347

Epoch 1: val_loss improved from inf to 145.43472, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 97s - loss: 599.5871 - MinusLogProbMetric: 599.5871 - val_loss: 145.4347 - val_MinusLogProbMetric: 145.4347 - lr: 3.3333e-04 - 97s/epoch - 494ms/step
Epoch 2/1000
2023-10-26 05:44:15.652 
Epoch 2/1000 
	 loss: 123.5062, MinusLogProbMetric: 123.5062, val_loss: 101.4782, val_MinusLogProbMetric: 101.4782

Epoch 2: val_loss improved from 145.43472 to 101.47817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 123.5062 - MinusLogProbMetric: 123.5062 - val_loss: 101.4782 - val_MinusLogProbMetric: 101.4782 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 3/1000
2023-10-26 05:44:50.701 
Epoch 3/1000 
	 loss: 91.2335, MinusLogProbMetric: 91.2335, val_loss: 85.4221, val_MinusLogProbMetric: 85.4221

Epoch 3: val_loss improved from 101.47817 to 85.42209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 35s - loss: 91.2335 - MinusLogProbMetric: 91.2335 - val_loss: 85.4221 - val_MinusLogProbMetric: 85.4221 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 4/1000
2023-10-26 05:45:27.741 
Epoch 4/1000 
	 loss: 74.8238, MinusLogProbMetric: 74.8238, val_loss: 69.2423, val_MinusLogProbMetric: 69.2423

Epoch 4: val_loss improved from 85.42209 to 69.24229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 74.8238 - MinusLogProbMetric: 74.8238 - val_loss: 69.2423 - val_MinusLogProbMetric: 69.2423 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 5/1000
2023-10-26 05:46:05.076 
Epoch 5/1000 
	 loss: 65.9988, MinusLogProbMetric: 65.9988, val_loss: 62.8408, val_MinusLogProbMetric: 62.8408

Epoch 5: val_loss improved from 69.24229 to 62.84079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 65.9988 - MinusLogProbMetric: 65.9988 - val_loss: 62.8408 - val_MinusLogProbMetric: 62.8408 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 6/1000
2023-10-26 05:46:42.066 
Epoch 6/1000 
	 loss: 58.0188, MinusLogProbMetric: 58.0188, val_loss: 56.0043, val_MinusLogProbMetric: 56.0043

Epoch 6: val_loss improved from 62.84079 to 56.00428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 58.0188 - MinusLogProbMetric: 58.0188 - val_loss: 56.0043 - val_MinusLogProbMetric: 56.0043 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 7/1000
2023-10-26 05:47:19.213 
Epoch 7/1000 
	 loss: 53.6394, MinusLogProbMetric: 53.6394, val_loss: 52.2365, val_MinusLogProbMetric: 52.2365

Epoch 7: val_loss improved from 56.00428 to 52.23645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 53.6394 - MinusLogProbMetric: 53.6394 - val_loss: 52.2365 - val_MinusLogProbMetric: 52.2365 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 8/1000
2023-10-26 05:47:55.924 
Epoch 8/1000 
	 loss: 50.4066, MinusLogProbMetric: 50.4066, val_loss: 50.7691, val_MinusLogProbMetric: 50.7691

Epoch 8: val_loss improved from 52.23645 to 50.76915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 50.4066 - MinusLogProbMetric: 50.4066 - val_loss: 50.7691 - val_MinusLogProbMetric: 50.7691 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 9/1000
2023-10-26 05:48:33.296 
Epoch 9/1000 
	 loss: 47.8581, MinusLogProbMetric: 47.8581, val_loss: 48.4946, val_MinusLogProbMetric: 48.4946

Epoch 9: val_loss improved from 50.76915 to 48.49462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 47.8581 - MinusLogProbMetric: 47.8581 - val_loss: 48.4946 - val_MinusLogProbMetric: 48.4946 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 10/1000
2023-10-26 05:49:10.514 
Epoch 10/1000 
	 loss: 45.9827, MinusLogProbMetric: 45.9827, val_loss: 44.8207, val_MinusLogProbMetric: 44.8207

Epoch 10: val_loss improved from 48.49462 to 44.82068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 45.9827 - MinusLogProbMetric: 45.9827 - val_loss: 44.8207 - val_MinusLogProbMetric: 44.8207 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 11/1000
2023-10-26 05:49:47.768 
Epoch 11/1000 
	 loss: 44.3099, MinusLogProbMetric: 44.3099, val_loss: 42.8644, val_MinusLogProbMetric: 42.8644

Epoch 11: val_loss improved from 44.82068 to 42.86436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 44.3099 - MinusLogProbMetric: 44.3099 - val_loss: 42.8644 - val_MinusLogProbMetric: 42.8644 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 12/1000
2023-10-26 05:50:24.659 
Epoch 12/1000 
	 loss: 43.0508, MinusLogProbMetric: 43.0508, val_loss: 42.7153, val_MinusLogProbMetric: 42.7153

Epoch 12: val_loss improved from 42.86436 to 42.71534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 43.0508 - MinusLogProbMetric: 43.0508 - val_loss: 42.7153 - val_MinusLogProbMetric: 42.7153 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 13/1000
2023-10-26 05:51:02.139 
Epoch 13/1000 
	 loss: 41.7160, MinusLogProbMetric: 41.7160, val_loss: 42.6688, val_MinusLogProbMetric: 42.6688

Epoch 13: val_loss improved from 42.71534 to 42.66883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 38s - loss: 41.7160 - MinusLogProbMetric: 41.7160 - val_loss: 42.6688 - val_MinusLogProbMetric: 42.6688 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 14/1000
2023-10-26 05:51:40.006 
Epoch 14/1000 
	 loss: 41.3230, MinusLogProbMetric: 41.3230, val_loss: 41.9813, val_MinusLogProbMetric: 41.9813

Epoch 14: val_loss improved from 42.66883 to 41.98132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 38s - loss: 41.3230 - MinusLogProbMetric: 41.3230 - val_loss: 41.9813 - val_MinusLogProbMetric: 41.9813 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 15/1000
2023-10-26 05:52:16.979 
Epoch 15/1000 
	 loss: 40.5349, MinusLogProbMetric: 40.5349, val_loss: 39.5719, val_MinusLogProbMetric: 39.5719

Epoch 15: val_loss improved from 41.98132 to 39.57185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 40.5349 - MinusLogProbMetric: 40.5349 - val_loss: 39.5719 - val_MinusLogProbMetric: 39.5719 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 16/1000
2023-10-26 05:52:53.968 
Epoch 16/1000 
	 loss: 39.4387, MinusLogProbMetric: 39.4387, val_loss: 40.0168, val_MinusLogProbMetric: 40.0168

Epoch 16: val_loss did not improve from 39.57185
196/196 - 36s - loss: 39.4387 - MinusLogProbMetric: 39.4387 - val_loss: 40.0168 - val_MinusLogProbMetric: 40.0168 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 17/1000
2023-10-26 05:53:30.610 
Epoch 17/1000 
	 loss: 39.2851, MinusLogProbMetric: 39.2851, val_loss: 40.2772, val_MinusLogProbMetric: 40.2772

Epoch 17: val_loss did not improve from 39.57185
196/196 - 37s - loss: 39.2851 - MinusLogProbMetric: 39.2851 - val_loss: 40.2772 - val_MinusLogProbMetric: 40.2772 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 18/1000
2023-10-26 05:54:06.295 
Epoch 18/1000 
	 loss: 38.8273, MinusLogProbMetric: 38.8273, val_loss: 39.8927, val_MinusLogProbMetric: 39.8927

Epoch 18: val_loss did not improve from 39.57185
196/196 - 36s - loss: 38.8273 - MinusLogProbMetric: 38.8273 - val_loss: 39.8927 - val_MinusLogProbMetric: 39.8927 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 19/1000
2023-10-26 05:54:42.850 
Epoch 19/1000 
	 loss: 38.7525, MinusLogProbMetric: 38.7525, val_loss: 37.3198, val_MinusLogProbMetric: 37.3198

Epoch 19: val_loss improved from 39.57185 to 37.31982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 38.7525 - MinusLogProbMetric: 38.7525 - val_loss: 37.3198 - val_MinusLogProbMetric: 37.3198 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 20/1000
2023-10-26 05:55:19.892 
Epoch 20/1000 
	 loss: 37.6887, MinusLogProbMetric: 37.6887, val_loss: 39.2787, val_MinusLogProbMetric: 39.2787

Epoch 20: val_loss did not improve from 37.31982
196/196 - 37s - loss: 37.6887 - MinusLogProbMetric: 37.6887 - val_loss: 39.2787 - val_MinusLogProbMetric: 39.2787 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 21/1000
2023-10-26 05:55:55.921 
Epoch 21/1000 
	 loss: 37.4040, MinusLogProbMetric: 37.4040, val_loss: 38.2947, val_MinusLogProbMetric: 38.2947

Epoch 21: val_loss did not improve from 37.31982
196/196 - 36s - loss: 37.4040 - MinusLogProbMetric: 37.4040 - val_loss: 38.2947 - val_MinusLogProbMetric: 38.2947 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 22/1000
2023-10-26 05:56:32.591 
Epoch 22/1000 
	 loss: 37.5069, MinusLogProbMetric: 37.5069, val_loss: 36.7403, val_MinusLogProbMetric: 36.7403

Epoch 22: val_loss improved from 37.31982 to 36.74031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 37.5069 - MinusLogProbMetric: 37.5069 - val_loss: 36.7403 - val_MinusLogProbMetric: 36.7403 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 23/1000
2023-10-26 05:57:09.944 
Epoch 23/1000 
	 loss: 37.0794, MinusLogProbMetric: 37.0794, val_loss: 40.0482, val_MinusLogProbMetric: 40.0482

Epoch 23: val_loss did not improve from 36.74031
196/196 - 37s - loss: 37.0794 - MinusLogProbMetric: 37.0794 - val_loss: 40.0482 - val_MinusLogProbMetric: 40.0482 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 24/1000
2023-10-26 05:57:46.477 
Epoch 24/1000 
	 loss: 36.7339, MinusLogProbMetric: 36.7339, val_loss: 36.8894, val_MinusLogProbMetric: 36.8894

Epoch 24: val_loss did not improve from 36.74031
196/196 - 37s - loss: 36.7339 - MinusLogProbMetric: 36.7339 - val_loss: 36.8894 - val_MinusLogProbMetric: 36.8894 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 25/1000
2023-10-26 05:58:22.856 
Epoch 25/1000 
	 loss: 36.3706, MinusLogProbMetric: 36.3706, val_loss: 37.2372, val_MinusLogProbMetric: 37.2372

Epoch 25: val_loss did not improve from 36.74031
196/196 - 36s - loss: 36.3706 - MinusLogProbMetric: 36.3706 - val_loss: 37.2372 - val_MinusLogProbMetric: 37.2372 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 26/1000
2023-10-26 05:58:59.077 
Epoch 26/1000 
	 loss: 36.0287, MinusLogProbMetric: 36.0287, val_loss: 36.8755, val_MinusLogProbMetric: 36.8755

Epoch 26: val_loss did not improve from 36.74031
196/196 - 36s - loss: 36.0287 - MinusLogProbMetric: 36.0287 - val_loss: 36.8755 - val_MinusLogProbMetric: 36.8755 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 27/1000
2023-10-26 05:59:35.552 
Epoch 27/1000 
	 loss: 35.9961, MinusLogProbMetric: 35.9961, val_loss: 35.2827, val_MinusLogProbMetric: 35.2827

Epoch 27: val_loss improved from 36.74031 to 35.28273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 35.9961 - MinusLogProbMetric: 35.9961 - val_loss: 35.2827 - val_MinusLogProbMetric: 35.2827 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 28/1000
2023-10-26 06:00:13.046 
Epoch 28/1000 
	 loss: 35.8726, MinusLogProbMetric: 35.8726, val_loss: 35.5684, val_MinusLogProbMetric: 35.5684

Epoch 28: val_loss did not improve from 35.28273
196/196 - 37s - loss: 35.8726 - MinusLogProbMetric: 35.8726 - val_loss: 35.5684 - val_MinusLogProbMetric: 35.5684 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 29/1000
2023-10-26 06:00:49.718 
Epoch 29/1000 
	 loss: 35.9034, MinusLogProbMetric: 35.9034, val_loss: 35.3325, val_MinusLogProbMetric: 35.3325

Epoch 29: val_loss did not improve from 35.28273
196/196 - 37s - loss: 35.9034 - MinusLogProbMetric: 35.9034 - val_loss: 35.3325 - val_MinusLogProbMetric: 35.3325 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 30/1000
2023-10-26 06:01:26.444 
Epoch 30/1000 
	 loss: 35.6981, MinusLogProbMetric: 35.6981, val_loss: 34.5795, val_MinusLogProbMetric: 34.5795

Epoch 30: val_loss improved from 35.28273 to 34.57951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 35.6981 - MinusLogProbMetric: 35.6981 - val_loss: 34.5795 - val_MinusLogProbMetric: 34.5795 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 31/1000
2023-10-26 06:02:03.106 
Epoch 31/1000 
	 loss: 35.0160, MinusLogProbMetric: 35.0160, val_loss: 35.4247, val_MinusLogProbMetric: 35.4247

Epoch 31: val_loss did not improve from 34.57951
196/196 - 36s - loss: 35.0160 - MinusLogProbMetric: 35.0160 - val_loss: 35.4247 - val_MinusLogProbMetric: 35.4247 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 32/1000
2023-10-26 06:02:40.091 
Epoch 32/1000 
	 loss: 35.7022, MinusLogProbMetric: 35.7022, val_loss: 35.0213, val_MinusLogProbMetric: 35.0213

Epoch 32: val_loss did not improve from 34.57951
196/196 - 37s - loss: 35.7022 - MinusLogProbMetric: 35.7022 - val_loss: 35.0213 - val_MinusLogProbMetric: 35.0213 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 33/1000
2023-10-26 06:03:16.522 
Epoch 33/1000 
	 loss: 34.7292, MinusLogProbMetric: 34.7292, val_loss: 36.0826, val_MinusLogProbMetric: 36.0826

Epoch 33: val_loss did not improve from 34.57951
196/196 - 36s - loss: 34.7292 - MinusLogProbMetric: 34.7292 - val_loss: 36.0826 - val_MinusLogProbMetric: 36.0826 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 34/1000
2023-10-26 06:03:53.062 
Epoch 34/1000 
	 loss: 34.6417, MinusLogProbMetric: 34.6417, val_loss: 34.2009, val_MinusLogProbMetric: 34.2009

Epoch 34: val_loss improved from 34.57951 to 34.20092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 34.6417 - MinusLogProbMetric: 34.6417 - val_loss: 34.2009 - val_MinusLogProbMetric: 34.2009 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 35/1000
2023-10-26 06:04:29.663 
Epoch 35/1000 
	 loss: 34.5334, MinusLogProbMetric: 34.5334, val_loss: 35.1919, val_MinusLogProbMetric: 35.1919

Epoch 35: val_loss did not improve from 34.20092
196/196 - 36s - loss: 34.5334 - MinusLogProbMetric: 34.5334 - val_loss: 35.1919 - val_MinusLogProbMetric: 35.1919 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 36/1000
2023-10-26 06:05:06.235 
Epoch 36/1000 
	 loss: 34.3265, MinusLogProbMetric: 34.3265, val_loss: 34.8980, val_MinusLogProbMetric: 34.8980

Epoch 36: val_loss did not improve from 34.20092
196/196 - 37s - loss: 34.3265 - MinusLogProbMetric: 34.3265 - val_loss: 34.8980 - val_MinusLogProbMetric: 34.8980 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 37/1000
2023-10-26 06:05:42.886 
Epoch 37/1000 
	 loss: 34.3208, MinusLogProbMetric: 34.3208, val_loss: 34.2561, val_MinusLogProbMetric: 34.2561

Epoch 37: val_loss did not improve from 34.20092
196/196 - 37s - loss: 34.3208 - MinusLogProbMetric: 34.3208 - val_loss: 34.2561 - val_MinusLogProbMetric: 34.2561 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 38/1000
2023-10-26 06:06:19.218 
Epoch 38/1000 
	 loss: 34.4634, MinusLogProbMetric: 34.4634, val_loss: 36.3880, val_MinusLogProbMetric: 36.3880

Epoch 38: val_loss did not improve from 34.20092
196/196 - 36s - loss: 34.4634 - MinusLogProbMetric: 34.4634 - val_loss: 36.3880 - val_MinusLogProbMetric: 36.3880 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 39/1000
2023-10-26 06:06:54.741 
Epoch 39/1000 
	 loss: 33.9122, MinusLogProbMetric: 33.9122, val_loss: 34.4945, val_MinusLogProbMetric: 34.4945

Epoch 39: val_loss did not improve from 34.20092
196/196 - 36s - loss: 33.9122 - MinusLogProbMetric: 33.9122 - val_loss: 34.4945 - val_MinusLogProbMetric: 34.4945 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 40/1000
2023-10-26 06:07:31.476 
Epoch 40/1000 
	 loss: 34.0356, MinusLogProbMetric: 34.0356, val_loss: 35.0781, val_MinusLogProbMetric: 35.0781

Epoch 40: val_loss did not improve from 34.20092
196/196 - 37s - loss: 34.0356 - MinusLogProbMetric: 34.0356 - val_loss: 35.0781 - val_MinusLogProbMetric: 35.0781 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 41/1000
2023-10-26 06:08:08.303 
Epoch 41/1000 
	 loss: 33.9085, MinusLogProbMetric: 33.9085, val_loss: 35.2109, val_MinusLogProbMetric: 35.2109

Epoch 41: val_loss did not improve from 34.20092
196/196 - 37s - loss: 33.9085 - MinusLogProbMetric: 33.9085 - val_loss: 35.2109 - val_MinusLogProbMetric: 35.2109 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 42/1000
2023-10-26 06:08:44.760 
Epoch 42/1000 
	 loss: 34.0015, MinusLogProbMetric: 34.0015, val_loss: 35.4128, val_MinusLogProbMetric: 35.4128

Epoch 42: val_loss did not improve from 34.20092
196/196 - 36s - loss: 34.0015 - MinusLogProbMetric: 34.0015 - val_loss: 35.4128 - val_MinusLogProbMetric: 35.4128 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 43/1000
2023-10-26 06:09:21.256 
Epoch 43/1000 
	 loss: 33.6701, MinusLogProbMetric: 33.6701, val_loss: 35.2601, val_MinusLogProbMetric: 35.2601

Epoch 43: val_loss did not improve from 34.20092
196/196 - 36s - loss: 33.6701 - MinusLogProbMetric: 33.6701 - val_loss: 35.2601 - val_MinusLogProbMetric: 35.2601 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 44/1000
2023-10-26 06:09:57.541 
Epoch 44/1000 
	 loss: 33.6253, MinusLogProbMetric: 33.6253, val_loss: 33.5868, val_MinusLogProbMetric: 33.5868

Epoch 44: val_loss improved from 34.20092 to 33.58680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 33.6253 - MinusLogProbMetric: 33.6253 - val_loss: 33.5868 - val_MinusLogProbMetric: 33.5868 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 45/1000
2023-10-26 06:10:34.965 
Epoch 45/1000 
	 loss: 33.3227, MinusLogProbMetric: 33.3227, val_loss: 33.4096, val_MinusLogProbMetric: 33.4096

Epoch 45: val_loss improved from 33.58680 to 33.40956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 33.3227 - MinusLogProbMetric: 33.3227 - val_loss: 33.4096 - val_MinusLogProbMetric: 33.4096 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 46/1000
2023-10-26 06:11:12.239 
Epoch 46/1000 
	 loss: 33.5363, MinusLogProbMetric: 33.5363, val_loss: 34.2136, val_MinusLogProbMetric: 34.2136

Epoch 46: val_loss did not improve from 33.40956
196/196 - 37s - loss: 33.5363 - MinusLogProbMetric: 33.5363 - val_loss: 34.2136 - val_MinusLogProbMetric: 34.2136 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 47/1000
2023-10-26 06:11:49.283 
Epoch 47/1000 
	 loss: 33.2950, MinusLogProbMetric: 33.2950, val_loss: 33.7501, val_MinusLogProbMetric: 33.7501

Epoch 47: val_loss did not improve from 33.40956
196/196 - 37s - loss: 33.2950 - MinusLogProbMetric: 33.2950 - val_loss: 33.7501 - val_MinusLogProbMetric: 33.7501 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 48/1000
2023-10-26 06:12:25.834 
Epoch 48/1000 
	 loss: 33.0802, MinusLogProbMetric: 33.0802, val_loss: 33.1336, val_MinusLogProbMetric: 33.1336

Epoch 48: val_loss improved from 33.40956 to 33.13356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 33.0802 - MinusLogProbMetric: 33.0802 - val_loss: 33.1336 - val_MinusLogProbMetric: 33.1336 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 49/1000
2023-10-26 06:13:03.092 
Epoch 49/1000 
	 loss: 33.0741, MinusLogProbMetric: 33.0741, val_loss: 33.8571, val_MinusLogProbMetric: 33.8571

Epoch 49: val_loss did not improve from 33.13356
196/196 - 37s - loss: 33.0741 - MinusLogProbMetric: 33.0741 - val_loss: 33.8571 - val_MinusLogProbMetric: 33.8571 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 50/1000
2023-10-26 06:13:38.822 
Epoch 50/1000 
	 loss: 32.9925, MinusLogProbMetric: 32.9925, val_loss: 33.4577, val_MinusLogProbMetric: 33.4577

Epoch 50: val_loss did not improve from 33.13356
196/196 - 36s - loss: 32.9925 - MinusLogProbMetric: 32.9925 - val_loss: 33.4577 - val_MinusLogProbMetric: 33.4577 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 51/1000
2023-10-26 06:14:15.291 
Epoch 51/1000 
	 loss: 32.9194, MinusLogProbMetric: 32.9194, val_loss: 32.9140, val_MinusLogProbMetric: 32.9140

Epoch 51: val_loss improved from 33.13356 to 32.91399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 32.9194 - MinusLogProbMetric: 32.9194 - val_loss: 32.9140 - val_MinusLogProbMetric: 32.9140 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 52/1000
2023-10-26 06:14:52.488 
Epoch 52/1000 
	 loss: 32.8560, MinusLogProbMetric: 32.8560, val_loss: 32.8257, val_MinusLogProbMetric: 32.8257

Epoch 52: val_loss improved from 32.91399 to 32.82567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 32.8560 - MinusLogProbMetric: 32.8560 - val_loss: 32.8257 - val_MinusLogProbMetric: 32.8257 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 53/1000
2023-10-26 06:15:29.136 
Epoch 53/1000 
	 loss: 32.8706, MinusLogProbMetric: 32.8706, val_loss: 33.8419, val_MinusLogProbMetric: 33.8419

Epoch 53: val_loss did not improve from 32.82567
196/196 - 36s - loss: 32.8706 - MinusLogProbMetric: 32.8706 - val_loss: 33.8419 - val_MinusLogProbMetric: 33.8419 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 54/1000
2023-10-26 06:16:05.270 
Epoch 54/1000 
	 loss: 32.9152, MinusLogProbMetric: 32.9152, val_loss: 32.9307, val_MinusLogProbMetric: 32.9307

Epoch 54: val_loss did not improve from 32.82567
196/196 - 36s - loss: 32.9152 - MinusLogProbMetric: 32.9152 - val_loss: 32.9307 - val_MinusLogProbMetric: 32.9307 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 55/1000
2023-10-26 06:16:41.693 
Epoch 55/1000 
	 loss: 32.9153, MinusLogProbMetric: 32.9153, val_loss: 33.4069, val_MinusLogProbMetric: 33.4069

Epoch 55: val_loss did not improve from 32.82567
196/196 - 36s - loss: 32.9153 - MinusLogProbMetric: 32.9153 - val_loss: 33.4069 - val_MinusLogProbMetric: 33.4069 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 56/1000
2023-10-26 06:17:17.784 
Epoch 56/1000 
	 loss: 32.7708, MinusLogProbMetric: 32.7708, val_loss: 33.9402, val_MinusLogProbMetric: 33.9402

Epoch 56: val_loss did not improve from 32.82567
196/196 - 36s - loss: 32.7708 - MinusLogProbMetric: 32.7708 - val_loss: 33.9402 - val_MinusLogProbMetric: 33.9402 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 57/1000
2023-10-26 06:17:54.118 
Epoch 57/1000 
	 loss: 32.5769, MinusLogProbMetric: 32.5769, val_loss: 34.2146, val_MinusLogProbMetric: 34.2146

Epoch 57: val_loss did not improve from 32.82567
196/196 - 36s - loss: 32.5769 - MinusLogProbMetric: 32.5769 - val_loss: 34.2146 - val_MinusLogProbMetric: 34.2146 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 58/1000
2023-10-26 06:18:29.939 
Epoch 58/1000 
	 loss: 32.5099, MinusLogProbMetric: 32.5099, val_loss: 33.2149, val_MinusLogProbMetric: 33.2149

Epoch 58: val_loss did not improve from 32.82567
196/196 - 36s - loss: 32.5099 - MinusLogProbMetric: 32.5099 - val_loss: 33.2149 - val_MinusLogProbMetric: 33.2149 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 59/1000
2023-10-26 06:19:05.715 
Epoch 59/1000 
	 loss: 32.3211, MinusLogProbMetric: 32.3211, val_loss: 33.8006, val_MinusLogProbMetric: 33.8006

Epoch 59: val_loss did not improve from 32.82567
196/196 - 36s - loss: 32.3211 - MinusLogProbMetric: 32.3211 - val_loss: 33.8006 - val_MinusLogProbMetric: 33.8006 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 60/1000
2023-10-26 06:19:41.875 
Epoch 60/1000 
	 loss: 32.4871, MinusLogProbMetric: 32.4871, val_loss: 32.4836, val_MinusLogProbMetric: 32.4836

Epoch 60: val_loss improved from 32.82567 to 32.48363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 32.4871 - MinusLogProbMetric: 32.4871 - val_loss: 32.4836 - val_MinusLogProbMetric: 32.4836 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 61/1000
2023-10-26 06:20:18.621 
Epoch 61/1000 
	 loss: 32.1960, MinusLogProbMetric: 32.1960, val_loss: 32.9324, val_MinusLogProbMetric: 32.9324

Epoch 61: val_loss did not improve from 32.48363
196/196 - 36s - loss: 32.1960 - MinusLogProbMetric: 32.1960 - val_loss: 32.9324 - val_MinusLogProbMetric: 32.9324 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 62/1000
2023-10-26 06:20:55.029 
Epoch 62/1000 
	 loss: 32.3729, MinusLogProbMetric: 32.3729, val_loss: 33.9091, val_MinusLogProbMetric: 33.9091

Epoch 62: val_loss did not improve from 32.48363
196/196 - 36s - loss: 32.3729 - MinusLogProbMetric: 32.3729 - val_loss: 33.9091 - val_MinusLogProbMetric: 33.9091 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 63/1000
2023-10-26 06:21:31.130 
Epoch 63/1000 
	 loss: 32.1955, MinusLogProbMetric: 32.1955, val_loss: 32.3302, val_MinusLogProbMetric: 32.3302

Epoch 63: val_loss improved from 32.48363 to 32.33019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 32.1955 - MinusLogProbMetric: 32.1955 - val_loss: 32.3302 - val_MinusLogProbMetric: 32.3302 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 64/1000
2023-10-26 06:22:07.593 
Epoch 64/1000 
	 loss: 32.0768, MinusLogProbMetric: 32.0768, val_loss: 31.9534, val_MinusLogProbMetric: 31.9534

Epoch 64: val_loss improved from 32.33019 to 31.95338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 32.0768 - MinusLogProbMetric: 32.0768 - val_loss: 31.9534 - val_MinusLogProbMetric: 31.9534 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 65/1000
2023-10-26 06:22:45.061 
Epoch 65/1000 
	 loss: 32.2721, MinusLogProbMetric: 32.2721, val_loss: 32.3312, val_MinusLogProbMetric: 32.3312

Epoch 65: val_loss did not improve from 31.95338
196/196 - 37s - loss: 32.2721 - MinusLogProbMetric: 32.2721 - val_loss: 32.3312 - val_MinusLogProbMetric: 32.3312 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 66/1000
2023-10-26 06:23:21.013 
Epoch 66/1000 
	 loss: 32.0703, MinusLogProbMetric: 32.0703, val_loss: 31.9221, val_MinusLogProbMetric: 31.9221

Epoch 66: val_loss improved from 31.95338 to 31.92211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 32.0703 - MinusLogProbMetric: 32.0703 - val_loss: 31.9221 - val_MinusLogProbMetric: 31.9221 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 67/1000
2023-10-26 06:23:57.738 
Epoch 67/1000 
	 loss: 32.0716, MinusLogProbMetric: 32.0716, val_loss: 32.2640, val_MinusLogProbMetric: 32.2640

Epoch 67: val_loss did not improve from 31.92211
196/196 - 36s - loss: 32.0716 - MinusLogProbMetric: 32.0716 - val_loss: 32.2640 - val_MinusLogProbMetric: 32.2640 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 68/1000
2023-10-26 06:24:33.842 
Epoch 68/1000 
	 loss: 31.9501, MinusLogProbMetric: 31.9501, val_loss: 31.9620, val_MinusLogProbMetric: 31.9620

Epoch 68: val_loss did not improve from 31.92211
196/196 - 36s - loss: 31.9501 - MinusLogProbMetric: 31.9501 - val_loss: 31.9620 - val_MinusLogProbMetric: 31.9620 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 69/1000
2023-10-26 06:25:10.003 
Epoch 69/1000 
	 loss: 31.8069, MinusLogProbMetric: 31.8069, val_loss: 32.3273, val_MinusLogProbMetric: 32.3273

Epoch 69: val_loss did not improve from 31.92211
196/196 - 36s - loss: 31.8069 - MinusLogProbMetric: 31.8069 - val_loss: 32.3273 - val_MinusLogProbMetric: 32.3273 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 70/1000
2023-10-26 06:25:46.420 
Epoch 70/1000 
	 loss: 31.8533, MinusLogProbMetric: 31.8533, val_loss: 32.5864, val_MinusLogProbMetric: 32.5864

Epoch 70: val_loss did not improve from 31.92211
196/196 - 36s - loss: 31.8533 - MinusLogProbMetric: 31.8533 - val_loss: 32.5864 - val_MinusLogProbMetric: 32.5864 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 71/1000
2023-10-26 06:26:22.898 
Epoch 71/1000 
	 loss: 31.7607, MinusLogProbMetric: 31.7607, val_loss: 31.5061, val_MinusLogProbMetric: 31.5061

Epoch 71: val_loss improved from 31.92211 to 31.50614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 31.7607 - MinusLogProbMetric: 31.7607 - val_loss: 31.5061 - val_MinusLogProbMetric: 31.5061 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 72/1000
2023-10-26 06:26:59.506 
Epoch 72/1000 
	 loss: 31.9054, MinusLogProbMetric: 31.9054, val_loss: 32.2644, val_MinusLogProbMetric: 32.2644

Epoch 72: val_loss did not improve from 31.50614
196/196 - 36s - loss: 31.9054 - MinusLogProbMetric: 31.9054 - val_loss: 32.2644 - val_MinusLogProbMetric: 32.2644 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 73/1000
2023-10-26 06:27:35.738 
Epoch 73/1000 
	 loss: 31.6887, MinusLogProbMetric: 31.6887, val_loss: 32.8710, val_MinusLogProbMetric: 32.8710

Epoch 73: val_loss did not improve from 31.50614
196/196 - 36s - loss: 31.6887 - MinusLogProbMetric: 31.6887 - val_loss: 32.8710 - val_MinusLogProbMetric: 32.8710 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 74/1000
2023-10-26 06:28:11.432 
Epoch 74/1000 
	 loss: 31.6572, MinusLogProbMetric: 31.6572, val_loss: 32.8580, val_MinusLogProbMetric: 32.8580

Epoch 74: val_loss did not improve from 31.50614
196/196 - 36s - loss: 31.6572 - MinusLogProbMetric: 31.6572 - val_loss: 32.8580 - val_MinusLogProbMetric: 32.8580 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 75/1000
2023-10-26 06:28:47.608 
Epoch 75/1000 
	 loss: 31.6445, MinusLogProbMetric: 31.6445, val_loss: 32.3872, val_MinusLogProbMetric: 32.3872

Epoch 75: val_loss did not improve from 31.50614
196/196 - 36s - loss: 31.6445 - MinusLogProbMetric: 31.6445 - val_loss: 32.3872 - val_MinusLogProbMetric: 32.3872 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 76/1000
2023-10-26 06:29:23.943 
Epoch 76/1000 
	 loss: 31.4685, MinusLogProbMetric: 31.4685, val_loss: 32.5904, val_MinusLogProbMetric: 32.5904

Epoch 76: val_loss did not improve from 31.50614
196/196 - 36s - loss: 31.4685 - MinusLogProbMetric: 31.4685 - val_loss: 32.5904 - val_MinusLogProbMetric: 32.5904 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 77/1000
2023-10-26 06:29:59.833 
Epoch 77/1000 
	 loss: 31.5090, MinusLogProbMetric: 31.5090, val_loss: 31.4573, val_MinusLogProbMetric: 31.4573

Epoch 77: val_loss improved from 31.50614 to 31.45729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 31.5090 - MinusLogProbMetric: 31.5090 - val_loss: 31.4573 - val_MinusLogProbMetric: 31.4573 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 78/1000
2023-10-26 06:30:35.922 
Epoch 78/1000 
	 loss: 31.5437, MinusLogProbMetric: 31.5437, val_loss: 31.6407, val_MinusLogProbMetric: 31.6407

Epoch 78: val_loss did not improve from 31.45729
196/196 - 36s - loss: 31.5437 - MinusLogProbMetric: 31.5437 - val_loss: 31.6407 - val_MinusLogProbMetric: 31.6407 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 79/1000
2023-10-26 06:31:12.075 
Epoch 79/1000 
	 loss: 31.5806, MinusLogProbMetric: 31.5806, val_loss: 31.9190, val_MinusLogProbMetric: 31.9190

Epoch 79: val_loss did not improve from 31.45729
196/196 - 36s - loss: 31.5806 - MinusLogProbMetric: 31.5806 - val_loss: 31.9190 - val_MinusLogProbMetric: 31.9190 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 80/1000
2023-10-26 06:31:48.378 
Epoch 80/1000 
	 loss: 31.5314, MinusLogProbMetric: 31.5314, val_loss: 32.8453, val_MinusLogProbMetric: 32.8453

Epoch 80: val_loss did not improve from 31.45729
196/196 - 36s - loss: 31.5314 - MinusLogProbMetric: 31.5314 - val_loss: 32.8453 - val_MinusLogProbMetric: 32.8453 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 81/1000
2023-10-26 06:32:24.558 
Epoch 81/1000 
	 loss: 31.6195, MinusLogProbMetric: 31.6195, val_loss: 31.6905, val_MinusLogProbMetric: 31.6905

Epoch 81: val_loss did not improve from 31.45729
196/196 - 36s - loss: 31.6195 - MinusLogProbMetric: 31.6195 - val_loss: 31.6905 - val_MinusLogProbMetric: 31.6905 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 82/1000
2023-10-26 06:33:00.671 
Epoch 82/1000 
	 loss: 31.3847, MinusLogProbMetric: 31.3847, val_loss: 31.6567, val_MinusLogProbMetric: 31.6567

Epoch 82: val_loss did not improve from 31.45729
196/196 - 36s - loss: 31.3847 - MinusLogProbMetric: 31.3847 - val_loss: 31.6567 - val_MinusLogProbMetric: 31.6567 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 83/1000
2023-10-26 06:33:36.507 
Epoch 83/1000 
	 loss: 31.5358, MinusLogProbMetric: 31.5358, val_loss: 31.4907, val_MinusLogProbMetric: 31.4907

Epoch 83: val_loss did not improve from 31.45729
196/196 - 36s - loss: 31.5358 - MinusLogProbMetric: 31.5358 - val_loss: 31.4907 - val_MinusLogProbMetric: 31.4907 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 84/1000
2023-10-26 06:34:12.521 
Epoch 84/1000 
	 loss: 31.2876, MinusLogProbMetric: 31.2876, val_loss: 31.3745, val_MinusLogProbMetric: 31.3745

Epoch 84: val_loss improved from 31.45729 to 31.37445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 31.2876 - MinusLogProbMetric: 31.2876 - val_loss: 31.3745 - val_MinusLogProbMetric: 31.3745 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 85/1000
2023-10-26 06:34:49.462 
Epoch 85/1000 
	 loss: 31.3790, MinusLogProbMetric: 31.3790, val_loss: 31.1662, val_MinusLogProbMetric: 31.1662

Epoch 85: val_loss improved from 31.37445 to 31.16616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 31.3790 - MinusLogProbMetric: 31.3790 - val_loss: 31.1662 - val_MinusLogProbMetric: 31.1662 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 86/1000
2023-10-26 06:35:26.110 
Epoch 86/1000 
	 loss: 31.2530, MinusLogProbMetric: 31.2530, val_loss: 31.9480, val_MinusLogProbMetric: 31.9480

Epoch 86: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.2530 - MinusLogProbMetric: 31.2530 - val_loss: 31.9480 - val_MinusLogProbMetric: 31.9480 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 87/1000
2023-10-26 06:36:02.079 
Epoch 87/1000 
	 loss: 31.2147, MinusLogProbMetric: 31.2147, val_loss: 33.0219, val_MinusLogProbMetric: 33.0219

Epoch 87: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.2147 - MinusLogProbMetric: 31.2147 - val_loss: 33.0219 - val_MinusLogProbMetric: 33.0219 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 88/1000
2023-10-26 06:36:38.765 
Epoch 88/1000 
	 loss: 31.1568, MinusLogProbMetric: 31.1568, val_loss: 31.7118, val_MinusLogProbMetric: 31.7118

Epoch 88: val_loss did not improve from 31.16616
196/196 - 37s - loss: 31.1568 - MinusLogProbMetric: 31.1568 - val_loss: 31.7118 - val_MinusLogProbMetric: 31.7118 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 89/1000
2023-10-26 06:37:14.865 
Epoch 89/1000 
	 loss: 31.1887, MinusLogProbMetric: 31.1887, val_loss: 31.8678, val_MinusLogProbMetric: 31.8678

Epoch 89: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.1887 - MinusLogProbMetric: 31.1887 - val_loss: 31.8678 - val_MinusLogProbMetric: 31.8678 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 90/1000
2023-10-26 06:37:51.091 
Epoch 90/1000 
	 loss: 31.2526, MinusLogProbMetric: 31.2526, val_loss: 31.6751, val_MinusLogProbMetric: 31.6751

Epoch 90: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.2526 - MinusLogProbMetric: 31.2526 - val_loss: 31.6751 - val_MinusLogProbMetric: 31.6751 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 91/1000
2023-10-26 06:38:27.406 
Epoch 91/1000 
	 loss: 31.0328, MinusLogProbMetric: 31.0328, val_loss: 31.9881, val_MinusLogProbMetric: 31.9881

Epoch 91: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.0328 - MinusLogProbMetric: 31.0328 - val_loss: 31.9881 - val_MinusLogProbMetric: 31.9881 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 92/1000
2023-10-26 06:39:03.516 
Epoch 92/1000 
	 loss: 31.1609, MinusLogProbMetric: 31.1609, val_loss: 31.3579, val_MinusLogProbMetric: 31.3579

Epoch 92: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.1609 - MinusLogProbMetric: 31.1609 - val_loss: 31.3579 - val_MinusLogProbMetric: 31.3579 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 93/1000
2023-10-26 06:39:39.456 
Epoch 93/1000 
	 loss: 31.0410, MinusLogProbMetric: 31.0410, val_loss: 31.2090, val_MinusLogProbMetric: 31.2090

Epoch 93: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.0410 - MinusLogProbMetric: 31.0410 - val_loss: 31.2090 - val_MinusLogProbMetric: 31.2090 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 94/1000
2023-10-26 06:40:15.343 
Epoch 94/1000 
	 loss: 31.1062, MinusLogProbMetric: 31.1062, val_loss: 31.3289, val_MinusLogProbMetric: 31.3289

Epoch 94: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.1062 - MinusLogProbMetric: 31.1062 - val_loss: 31.3289 - val_MinusLogProbMetric: 31.3289 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 95/1000
2023-10-26 06:40:51.343 
Epoch 95/1000 
	 loss: 31.0844, MinusLogProbMetric: 31.0844, val_loss: 31.7937, val_MinusLogProbMetric: 31.7937

Epoch 95: val_loss did not improve from 31.16616
196/196 - 36s - loss: 31.0844 - MinusLogProbMetric: 31.0844 - val_loss: 31.7937 - val_MinusLogProbMetric: 31.7937 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 96/1000
2023-10-26 06:41:27.181 
Epoch 96/1000 
	 loss: 30.9961, MinusLogProbMetric: 30.9961, val_loss: 32.4565, val_MinusLogProbMetric: 32.4565

Epoch 96: val_loss did not improve from 31.16616
196/196 - 36s - loss: 30.9961 - MinusLogProbMetric: 30.9961 - val_loss: 32.4565 - val_MinusLogProbMetric: 32.4565 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 97/1000
2023-10-26 06:42:03.341 
Epoch 97/1000 
	 loss: 30.9824, MinusLogProbMetric: 30.9824, val_loss: 30.9534, val_MinusLogProbMetric: 30.9534

Epoch 97: val_loss improved from 31.16616 to 30.95340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 30.9824 - MinusLogProbMetric: 30.9824 - val_loss: 30.9534 - val_MinusLogProbMetric: 30.9534 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 98/1000
2023-10-26 06:42:40.009 
Epoch 98/1000 
	 loss: 30.9741, MinusLogProbMetric: 30.9741, val_loss: 30.4935, val_MinusLogProbMetric: 30.4935

Epoch 98: val_loss improved from 30.95340 to 30.49347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 37s - loss: 30.9741 - MinusLogProbMetric: 30.9741 - val_loss: 30.4935 - val_MinusLogProbMetric: 30.4935 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 99/1000
2023-10-26 06:43:16.487 
Epoch 99/1000 
	 loss: 30.8875, MinusLogProbMetric: 30.8875, val_loss: 30.8238, val_MinusLogProbMetric: 30.8238

Epoch 99: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.8875 - MinusLogProbMetric: 30.8875 - val_loss: 30.8238 - val_MinusLogProbMetric: 30.8238 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 100/1000
2023-10-26 06:43:52.499 
Epoch 100/1000 
	 loss: 30.9215, MinusLogProbMetric: 30.9215, val_loss: 31.3259, val_MinusLogProbMetric: 31.3259

Epoch 100: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.9215 - MinusLogProbMetric: 30.9215 - val_loss: 31.3259 - val_MinusLogProbMetric: 31.3259 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 101/1000
2023-10-26 06:44:28.560 
Epoch 101/1000 
	 loss: 30.9227, MinusLogProbMetric: 30.9227, val_loss: 30.9855, val_MinusLogProbMetric: 30.9855

Epoch 101: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.9227 - MinusLogProbMetric: 30.9227 - val_loss: 30.9855 - val_MinusLogProbMetric: 30.9855 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 102/1000
2023-10-26 06:45:04.385 
Epoch 102/1000 
	 loss: 30.8995, MinusLogProbMetric: 30.8995, val_loss: 31.0015, val_MinusLogProbMetric: 31.0015

Epoch 102: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.8995 - MinusLogProbMetric: 30.8995 - val_loss: 31.0015 - val_MinusLogProbMetric: 31.0015 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 103/1000
2023-10-26 06:45:40.296 
Epoch 103/1000 
	 loss: 30.7976, MinusLogProbMetric: 30.7976, val_loss: 33.3387, val_MinusLogProbMetric: 33.3387

Epoch 103: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.7976 - MinusLogProbMetric: 30.7976 - val_loss: 33.3387 - val_MinusLogProbMetric: 33.3387 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 104/1000
2023-10-26 06:46:16.013 
Epoch 104/1000 
	 loss: 30.9294, MinusLogProbMetric: 30.9294, val_loss: 31.9290, val_MinusLogProbMetric: 31.9290

Epoch 104: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.9294 - MinusLogProbMetric: 30.9294 - val_loss: 31.9290 - val_MinusLogProbMetric: 31.9290 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 105/1000
2023-10-26 06:46:52.375 
Epoch 105/1000 
	 loss: 30.9551, MinusLogProbMetric: 30.9551, val_loss: 31.1839, val_MinusLogProbMetric: 31.1839

Epoch 105: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.9551 - MinusLogProbMetric: 30.9551 - val_loss: 31.1839 - val_MinusLogProbMetric: 31.1839 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 106/1000
2023-10-26 06:47:28.715 
Epoch 106/1000 
	 loss: 30.8130, MinusLogProbMetric: 30.8130, val_loss: 31.5868, val_MinusLogProbMetric: 31.5868

Epoch 106: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.8130 - MinusLogProbMetric: 30.8130 - val_loss: 31.5868 - val_MinusLogProbMetric: 31.5868 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 107/1000
2023-10-26 06:48:04.811 
Epoch 107/1000 
	 loss: 30.7299, MinusLogProbMetric: 30.7299, val_loss: 30.9910, val_MinusLogProbMetric: 30.9910

Epoch 107: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.7299 - MinusLogProbMetric: 30.7299 - val_loss: 30.9910 - val_MinusLogProbMetric: 30.9910 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 108/1000
2023-10-26 06:48:40.958 
Epoch 108/1000 
	 loss: 30.7446, MinusLogProbMetric: 30.7446, val_loss: 31.2628, val_MinusLogProbMetric: 31.2628

Epoch 108: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.7446 - MinusLogProbMetric: 30.7446 - val_loss: 31.2628 - val_MinusLogProbMetric: 31.2628 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 109/1000
2023-10-26 06:49:16.785 
Epoch 109/1000 
	 loss: 30.9422, MinusLogProbMetric: 30.9422, val_loss: 31.2449, val_MinusLogProbMetric: 31.2449

Epoch 109: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.9422 - MinusLogProbMetric: 30.9422 - val_loss: 31.2449 - val_MinusLogProbMetric: 31.2449 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 110/1000
2023-10-26 06:49:52.885 
Epoch 110/1000 
	 loss: 30.6542, MinusLogProbMetric: 30.6542, val_loss: 31.4630, val_MinusLogProbMetric: 31.4630

Epoch 110: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.6542 - MinusLogProbMetric: 30.6542 - val_loss: 31.4630 - val_MinusLogProbMetric: 31.4630 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 111/1000
2023-10-26 06:50:28.767 
Epoch 111/1000 
	 loss: 30.5540, MinusLogProbMetric: 30.5540, val_loss: 30.7782, val_MinusLogProbMetric: 30.7782

Epoch 111: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.5540 - MinusLogProbMetric: 30.5540 - val_loss: 30.7782 - val_MinusLogProbMetric: 30.7782 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 112/1000
2023-10-26 06:51:04.638 
Epoch 112/1000 
	 loss: 30.7742, MinusLogProbMetric: 30.7742, val_loss: 31.1586, val_MinusLogProbMetric: 31.1586

Epoch 112: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.7742 - MinusLogProbMetric: 30.7742 - val_loss: 31.1586 - val_MinusLogProbMetric: 31.1586 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 113/1000
2023-10-26 06:51:40.131 
Epoch 113/1000 
	 loss: 30.6264, MinusLogProbMetric: 30.6264, val_loss: 32.9380, val_MinusLogProbMetric: 32.9380

Epoch 113: val_loss did not improve from 30.49347
196/196 - 35s - loss: 30.6264 - MinusLogProbMetric: 30.6264 - val_loss: 32.9380 - val_MinusLogProbMetric: 32.9380 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 114/1000
2023-10-26 06:52:14.489 
Epoch 114/1000 
	 loss: 30.7178, MinusLogProbMetric: 30.7178, val_loss: 31.2177, val_MinusLogProbMetric: 31.2177

Epoch 114: val_loss did not improve from 30.49347
196/196 - 34s - loss: 30.7178 - MinusLogProbMetric: 30.7178 - val_loss: 31.2177 - val_MinusLogProbMetric: 31.2177 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 115/1000
2023-10-26 06:52:49.665 
Epoch 115/1000 
	 loss: 30.5185, MinusLogProbMetric: 30.5185, val_loss: 30.6510, val_MinusLogProbMetric: 30.6510

Epoch 115: val_loss did not improve from 30.49347
196/196 - 35s - loss: 30.5185 - MinusLogProbMetric: 30.5185 - val_loss: 30.6510 - val_MinusLogProbMetric: 30.6510 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 116/1000
2023-10-26 06:53:23.930 
Epoch 116/1000 
	 loss: 30.5099, MinusLogProbMetric: 30.5099, val_loss: 31.3568, val_MinusLogProbMetric: 31.3568

Epoch 116: val_loss did not improve from 30.49347
196/196 - 34s - loss: 30.5099 - MinusLogProbMetric: 30.5099 - val_loss: 31.3568 - val_MinusLogProbMetric: 31.3568 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 117/1000
2023-10-26 06:53:57.645 
Epoch 117/1000 
	 loss: 30.5724, MinusLogProbMetric: 30.5724, val_loss: 30.9827, val_MinusLogProbMetric: 30.9827

Epoch 117: val_loss did not improve from 30.49347
196/196 - 34s - loss: 30.5724 - MinusLogProbMetric: 30.5724 - val_loss: 30.9827 - val_MinusLogProbMetric: 30.9827 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 118/1000
2023-10-26 06:54:32.677 
Epoch 118/1000 
	 loss: 30.5863, MinusLogProbMetric: 30.5863, val_loss: 30.8100, val_MinusLogProbMetric: 30.8100

Epoch 118: val_loss did not improve from 30.49347
196/196 - 35s - loss: 30.5863 - MinusLogProbMetric: 30.5863 - val_loss: 30.8100 - val_MinusLogProbMetric: 30.8100 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 119/1000
2023-10-26 06:55:07.915 
Epoch 119/1000 
	 loss: 30.5402, MinusLogProbMetric: 30.5402, val_loss: 31.0934, val_MinusLogProbMetric: 31.0934

Epoch 119: val_loss did not improve from 30.49347
196/196 - 35s - loss: 30.5402 - MinusLogProbMetric: 30.5402 - val_loss: 31.0934 - val_MinusLogProbMetric: 31.0934 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 120/1000
2023-10-26 06:55:42.175 
Epoch 120/1000 
	 loss: 30.4567, MinusLogProbMetric: 30.4567, val_loss: 31.4716, val_MinusLogProbMetric: 31.4716

Epoch 120: val_loss did not improve from 30.49347
196/196 - 34s - loss: 30.4567 - MinusLogProbMetric: 30.4567 - val_loss: 31.4716 - val_MinusLogProbMetric: 31.4716 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 121/1000
2023-10-26 06:56:17.215 
Epoch 121/1000 
	 loss: 30.4272, MinusLogProbMetric: 30.4272, val_loss: 30.6679, val_MinusLogProbMetric: 30.6679

Epoch 121: val_loss did not improve from 30.49347
196/196 - 35s - loss: 30.4272 - MinusLogProbMetric: 30.4272 - val_loss: 30.6679 - val_MinusLogProbMetric: 30.6679 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 122/1000
2023-10-26 06:56:50.580 
Epoch 122/1000 
	 loss: 30.6015, MinusLogProbMetric: 30.6015, val_loss: 30.7060, val_MinusLogProbMetric: 30.7060

Epoch 122: val_loss did not improve from 30.49347
196/196 - 33s - loss: 30.6015 - MinusLogProbMetric: 30.6015 - val_loss: 30.7060 - val_MinusLogProbMetric: 30.7060 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 123/1000
2023-10-26 06:57:23.828 
Epoch 123/1000 
	 loss: 30.3471, MinusLogProbMetric: 30.3471, val_loss: 30.8597, val_MinusLogProbMetric: 30.8597

Epoch 123: val_loss did not improve from 30.49347
196/196 - 33s - loss: 30.3471 - MinusLogProbMetric: 30.3471 - val_loss: 30.8597 - val_MinusLogProbMetric: 30.8597 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 124/1000
2023-10-26 06:57:57.037 
Epoch 124/1000 
	 loss: 30.3449, MinusLogProbMetric: 30.3449, val_loss: 32.0168, val_MinusLogProbMetric: 32.0168

Epoch 124: val_loss did not improve from 30.49347
196/196 - 33s - loss: 30.3449 - MinusLogProbMetric: 30.3449 - val_loss: 32.0168 - val_MinusLogProbMetric: 32.0168 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 125/1000
2023-10-26 06:58:32.671 
Epoch 125/1000 
	 loss: 30.4777, MinusLogProbMetric: 30.4777, val_loss: 31.2699, val_MinusLogProbMetric: 31.2699

Epoch 125: val_loss did not improve from 30.49347
196/196 - 36s - loss: 30.4777 - MinusLogProbMetric: 30.4777 - val_loss: 31.2699 - val_MinusLogProbMetric: 31.2699 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 126/1000
2023-10-26 06:59:08.119 
Epoch 126/1000 
	 loss: 30.4359, MinusLogProbMetric: 30.4359, val_loss: 31.5918, val_MinusLogProbMetric: 31.5918

Epoch 126: val_loss did not improve from 30.49347
196/196 - 35s - loss: 30.4359 - MinusLogProbMetric: 30.4359 - val_loss: 31.5918 - val_MinusLogProbMetric: 31.5918 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 127/1000
2023-10-26 06:59:43.073 
Epoch 127/1000 
	 loss: 30.4277, MinusLogProbMetric: 30.4277, val_loss: 30.4440, val_MinusLogProbMetric: 30.4440

Epoch 127: val_loss improved from 30.49347 to 30.44401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 30.4277 - MinusLogProbMetric: 30.4277 - val_loss: 30.4440 - val_MinusLogProbMetric: 30.4440 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 128/1000
2023-10-26 07:00:18.076 
Epoch 128/1000 
	 loss: 30.5202, MinusLogProbMetric: 30.5202, val_loss: 30.7264, val_MinusLogProbMetric: 30.7264

Epoch 128: val_loss did not improve from 30.44401
196/196 - 34s - loss: 30.5202 - MinusLogProbMetric: 30.5202 - val_loss: 30.7264 - val_MinusLogProbMetric: 30.7264 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 129/1000
2023-10-26 07:00:53.508 
Epoch 129/1000 
	 loss: 30.3792, MinusLogProbMetric: 30.3792, val_loss: 31.3825, val_MinusLogProbMetric: 31.3825

Epoch 129: val_loss did not improve from 30.44401
196/196 - 35s - loss: 30.3792 - MinusLogProbMetric: 30.3792 - val_loss: 31.3825 - val_MinusLogProbMetric: 31.3825 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 130/1000
2023-10-26 07:01:28.707 
Epoch 130/1000 
	 loss: 30.3310, MinusLogProbMetric: 30.3310, val_loss: 30.2768, val_MinusLogProbMetric: 30.2768

Epoch 130: val_loss improved from 30.44401 to 30.27675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 30.3310 - MinusLogProbMetric: 30.3310 - val_loss: 30.2768 - val_MinusLogProbMetric: 30.2768 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 131/1000
2023-10-26 07:02:04.709 
Epoch 131/1000 
	 loss: 30.2715, MinusLogProbMetric: 30.2715, val_loss: 30.9574, val_MinusLogProbMetric: 30.9574

Epoch 131: val_loss did not improve from 30.27675
196/196 - 35s - loss: 30.2715 - MinusLogProbMetric: 30.2715 - val_loss: 30.9574 - val_MinusLogProbMetric: 30.9574 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 132/1000
2023-10-26 07:02:40.180 
Epoch 132/1000 
	 loss: 30.4107, MinusLogProbMetric: 30.4107, val_loss: 30.2983, val_MinusLogProbMetric: 30.2983

Epoch 132: val_loss did not improve from 30.27675
196/196 - 35s - loss: 30.4107 - MinusLogProbMetric: 30.4107 - val_loss: 30.2983 - val_MinusLogProbMetric: 30.2983 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 133/1000
2023-10-26 07:03:15.555 
Epoch 133/1000 
	 loss: 30.3866, MinusLogProbMetric: 30.3866, val_loss: 31.5736, val_MinusLogProbMetric: 31.5736

Epoch 133: val_loss did not improve from 30.27675
196/196 - 35s - loss: 30.3866 - MinusLogProbMetric: 30.3866 - val_loss: 31.5736 - val_MinusLogProbMetric: 31.5736 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 134/1000
2023-10-26 07:03:50.819 
Epoch 134/1000 
	 loss: 30.2653, MinusLogProbMetric: 30.2653, val_loss: 30.5924, val_MinusLogProbMetric: 30.5924

Epoch 134: val_loss did not improve from 30.27675
196/196 - 35s - loss: 30.2653 - MinusLogProbMetric: 30.2653 - val_loss: 30.5924 - val_MinusLogProbMetric: 30.5924 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 135/1000
2023-10-26 07:04:26.350 
Epoch 135/1000 
	 loss: 30.2629, MinusLogProbMetric: 30.2629, val_loss: 30.2582, val_MinusLogProbMetric: 30.2582

Epoch 135: val_loss improved from 30.27675 to 30.25816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 30.2629 - MinusLogProbMetric: 30.2629 - val_loss: 30.2582 - val_MinusLogProbMetric: 30.2582 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 136/1000
2023-10-26 07:05:02.496 
Epoch 136/1000 
	 loss: 30.2252, MinusLogProbMetric: 30.2252, val_loss: 30.5018, val_MinusLogProbMetric: 30.5018

Epoch 136: val_loss did not improve from 30.25816
196/196 - 36s - loss: 30.2252 - MinusLogProbMetric: 30.2252 - val_loss: 30.5018 - val_MinusLogProbMetric: 30.5018 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 137/1000
2023-10-26 07:05:37.997 
Epoch 137/1000 
	 loss: 30.0925, MinusLogProbMetric: 30.0925, val_loss: 30.6781, val_MinusLogProbMetric: 30.6781

Epoch 137: val_loss did not improve from 30.25816
196/196 - 35s - loss: 30.0925 - MinusLogProbMetric: 30.0925 - val_loss: 30.6781 - val_MinusLogProbMetric: 30.6781 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 138/1000
2023-10-26 07:06:13.525 
Epoch 138/1000 
	 loss: 30.2527, MinusLogProbMetric: 30.2527, val_loss: 30.9042, val_MinusLogProbMetric: 30.9042

Epoch 138: val_loss did not improve from 30.25816
196/196 - 36s - loss: 30.2527 - MinusLogProbMetric: 30.2527 - val_loss: 30.9042 - val_MinusLogProbMetric: 30.9042 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 139/1000
2023-10-26 07:06:49.039 
Epoch 139/1000 
	 loss: 30.1973, MinusLogProbMetric: 30.1973, val_loss: 31.7154, val_MinusLogProbMetric: 31.7154

Epoch 139: val_loss did not improve from 30.25816
196/196 - 36s - loss: 30.1973 - MinusLogProbMetric: 30.1973 - val_loss: 31.7154 - val_MinusLogProbMetric: 31.7154 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 140/1000
2023-10-26 07:07:24.300 
Epoch 140/1000 
	 loss: 30.2323, MinusLogProbMetric: 30.2323, val_loss: 30.1296, val_MinusLogProbMetric: 30.1296

Epoch 140: val_loss improved from 30.25816 to 30.12961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 30.2323 - MinusLogProbMetric: 30.2323 - val_loss: 30.1296 - val_MinusLogProbMetric: 30.1296 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 141/1000
2023-10-26 07:08:00.347 
Epoch 141/1000 
	 loss: 30.0269, MinusLogProbMetric: 30.0269, val_loss: 30.5267, val_MinusLogProbMetric: 30.5267

Epoch 141: val_loss did not improve from 30.12961
196/196 - 35s - loss: 30.0269 - MinusLogProbMetric: 30.0269 - val_loss: 30.5267 - val_MinusLogProbMetric: 30.5267 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 142/1000
2023-10-26 07:08:35.894 
Epoch 142/1000 
	 loss: 30.1495, MinusLogProbMetric: 30.1495, val_loss: 31.1899, val_MinusLogProbMetric: 31.1899

Epoch 142: val_loss did not improve from 30.12961
196/196 - 36s - loss: 30.1495 - MinusLogProbMetric: 30.1495 - val_loss: 31.1899 - val_MinusLogProbMetric: 31.1899 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 143/1000
2023-10-26 07:09:11.431 
Epoch 143/1000 
	 loss: 30.1488, MinusLogProbMetric: 30.1488, val_loss: 29.8609, val_MinusLogProbMetric: 29.8609

Epoch 143: val_loss improved from 30.12961 to 29.86086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 30.1488 - MinusLogProbMetric: 30.1488 - val_loss: 29.8609 - val_MinusLogProbMetric: 29.8609 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 144/1000
2023-10-26 07:09:47.261 
Epoch 144/1000 
	 loss: 30.2292, MinusLogProbMetric: 30.2292, val_loss: 30.4965, val_MinusLogProbMetric: 30.4965

Epoch 144: val_loss did not improve from 29.86086
196/196 - 35s - loss: 30.2292 - MinusLogProbMetric: 30.2292 - val_loss: 30.4965 - val_MinusLogProbMetric: 30.4965 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 145/1000
2023-10-26 07:10:22.799 
Epoch 145/1000 
	 loss: 30.1377, MinusLogProbMetric: 30.1377, val_loss: 30.0102, val_MinusLogProbMetric: 30.0102

Epoch 145: val_loss did not improve from 29.86086
196/196 - 36s - loss: 30.1377 - MinusLogProbMetric: 30.1377 - val_loss: 30.0102 - val_MinusLogProbMetric: 30.0102 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 146/1000
2023-10-26 07:10:58.215 
Epoch 146/1000 
	 loss: 30.0737, MinusLogProbMetric: 30.0737, val_loss: 30.1293, val_MinusLogProbMetric: 30.1293

Epoch 146: val_loss did not improve from 29.86086
196/196 - 35s - loss: 30.0737 - MinusLogProbMetric: 30.0737 - val_loss: 30.1293 - val_MinusLogProbMetric: 30.1293 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 147/1000
2023-10-26 07:11:33.345 
Epoch 147/1000 
	 loss: 30.0713, MinusLogProbMetric: 30.0713, val_loss: 32.6922, val_MinusLogProbMetric: 32.6922

Epoch 147: val_loss did not improve from 29.86086
196/196 - 35s - loss: 30.0713 - MinusLogProbMetric: 30.0713 - val_loss: 32.6922 - val_MinusLogProbMetric: 32.6922 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 148/1000
2023-10-26 07:12:08.362 
Epoch 148/1000 
	 loss: 30.0663, MinusLogProbMetric: 30.0663, val_loss: 31.4209, val_MinusLogProbMetric: 31.4209

Epoch 148: val_loss did not improve from 29.86086
196/196 - 35s - loss: 30.0663 - MinusLogProbMetric: 30.0663 - val_loss: 31.4209 - val_MinusLogProbMetric: 31.4209 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 149/1000
2023-10-26 07:12:43.740 
Epoch 149/1000 
	 loss: 30.0849, MinusLogProbMetric: 30.0849, val_loss: 30.0050, val_MinusLogProbMetric: 30.0050

Epoch 149: val_loss did not improve from 29.86086
196/196 - 35s - loss: 30.0849 - MinusLogProbMetric: 30.0849 - val_loss: 30.0050 - val_MinusLogProbMetric: 30.0050 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 150/1000
2023-10-26 07:13:19.368 
Epoch 150/1000 
	 loss: 30.0634, MinusLogProbMetric: 30.0634, val_loss: 30.0866, val_MinusLogProbMetric: 30.0866

Epoch 150: val_loss did not improve from 29.86086
196/196 - 36s - loss: 30.0634 - MinusLogProbMetric: 30.0634 - val_loss: 30.0866 - val_MinusLogProbMetric: 30.0866 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 151/1000
2023-10-26 07:13:54.486 
Epoch 151/1000 
	 loss: 29.9712, MinusLogProbMetric: 29.9712, val_loss: 30.3682, val_MinusLogProbMetric: 30.3682

Epoch 151: val_loss did not improve from 29.86086
196/196 - 35s - loss: 29.9712 - MinusLogProbMetric: 29.9712 - val_loss: 30.3682 - val_MinusLogProbMetric: 30.3682 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 152/1000
2023-10-26 07:14:30.132 
Epoch 152/1000 
	 loss: 30.0480, MinusLogProbMetric: 30.0480, val_loss: 29.9972, val_MinusLogProbMetric: 29.9972

Epoch 152: val_loss did not improve from 29.86086
196/196 - 36s - loss: 30.0480 - MinusLogProbMetric: 30.0480 - val_loss: 29.9972 - val_MinusLogProbMetric: 29.9972 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 153/1000
2023-10-26 07:15:05.404 
Epoch 153/1000 
	 loss: 30.0945, MinusLogProbMetric: 30.0945, val_loss: 31.6401, val_MinusLogProbMetric: 31.6401

Epoch 153: val_loss did not improve from 29.86086
196/196 - 35s - loss: 30.0945 - MinusLogProbMetric: 30.0945 - val_loss: 31.6401 - val_MinusLogProbMetric: 31.6401 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 154/1000
2023-10-26 07:15:40.576 
Epoch 154/1000 
	 loss: 30.0121, MinusLogProbMetric: 30.0121, val_loss: 32.2337, val_MinusLogProbMetric: 32.2337

Epoch 154: val_loss did not improve from 29.86086
196/196 - 35s - loss: 30.0121 - MinusLogProbMetric: 30.0121 - val_loss: 32.2337 - val_MinusLogProbMetric: 32.2337 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 155/1000
2023-10-26 07:16:15.930 
Epoch 155/1000 
	 loss: 30.0569, MinusLogProbMetric: 30.0569, val_loss: 29.9830, val_MinusLogProbMetric: 29.9830

Epoch 155: val_loss did not improve from 29.86086
196/196 - 35s - loss: 30.0569 - MinusLogProbMetric: 30.0569 - val_loss: 29.9830 - val_MinusLogProbMetric: 29.9830 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 156/1000
2023-10-26 07:16:51.442 
Epoch 156/1000 
	 loss: 29.9380, MinusLogProbMetric: 29.9380, val_loss: 34.2141, val_MinusLogProbMetric: 34.2141

Epoch 156: val_loss did not improve from 29.86086
196/196 - 36s - loss: 29.9380 - MinusLogProbMetric: 29.9380 - val_loss: 34.2141 - val_MinusLogProbMetric: 34.2141 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 157/1000
2023-10-26 07:17:26.968 
Epoch 157/1000 
	 loss: 30.0809, MinusLogProbMetric: 30.0809, val_loss: 29.8734, val_MinusLogProbMetric: 29.8734

Epoch 157: val_loss did not improve from 29.86086
196/196 - 36s - loss: 30.0809 - MinusLogProbMetric: 30.0809 - val_loss: 29.8734 - val_MinusLogProbMetric: 29.8734 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 158/1000
2023-10-26 07:18:02.758 
Epoch 158/1000 
	 loss: 29.9310, MinusLogProbMetric: 29.9310, val_loss: 30.1165, val_MinusLogProbMetric: 30.1165

Epoch 158: val_loss did not improve from 29.86086
196/196 - 36s - loss: 29.9310 - MinusLogProbMetric: 29.9310 - val_loss: 30.1165 - val_MinusLogProbMetric: 30.1165 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 159/1000
2023-10-26 07:18:38.202 
Epoch 159/1000 
	 loss: 29.8960, MinusLogProbMetric: 29.8960, val_loss: 30.0040, val_MinusLogProbMetric: 30.0040

Epoch 159: val_loss did not improve from 29.86086
196/196 - 35s - loss: 29.8960 - MinusLogProbMetric: 29.8960 - val_loss: 30.0040 - val_MinusLogProbMetric: 30.0040 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 160/1000
2023-10-26 07:19:13.379 
Epoch 160/1000 
	 loss: 29.8745, MinusLogProbMetric: 29.8745, val_loss: 30.6447, val_MinusLogProbMetric: 30.6447

Epoch 160: val_loss did not improve from 29.86086
196/196 - 35s - loss: 29.8745 - MinusLogProbMetric: 29.8745 - val_loss: 30.6447 - val_MinusLogProbMetric: 30.6447 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 161/1000
2023-10-26 07:19:48.458 
Epoch 161/1000 
	 loss: 29.9703, MinusLogProbMetric: 29.9703, val_loss: 29.8257, val_MinusLogProbMetric: 29.8257

Epoch 161: val_loss improved from 29.86086 to 29.82565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 29.9703 - MinusLogProbMetric: 29.9703 - val_loss: 29.8257 - val_MinusLogProbMetric: 29.8257 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 162/1000
2023-10-26 07:20:24.789 
Epoch 162/1000 
	 loss: 29.8709, MinusLogProbMetric: 29.8709, val_loss: 29.7579, val_MinusLogProbMetric: 29.7579

Epoch 162: val_loss improved from 29.82565 to 29.75786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 29.8709 - MinusLogProbMetric: 29.8709 - val_loss: 29.7579 - val_MinusLogProbMetric: 29.7579 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 163/1000
2023-10-26 07:21:00.744 
Epoch 163/1000 
	 loss: 29.7927, MinusLogProbMetric: 29.7927, val_loss: 30.0587, val_MinusLogProbMetric: 30.0587

Epoch 163: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.7927 - MinusLogProbMetric: 29.7927 - val_loss: 30.0587 - val_MinusLogProbMetric: 30.0587 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 164/1000
2023-10-26 07:21:36.143 
Epoch 164/1000 
	 loss: 29.9397, MinusLogProbMetric: 29.9397, val_loss: 30.6824, val_MinusLogProbMetric: 30.6824

Epoch 164: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.9397 - MinusLogProbMetric: 29.9397 - val_loss: 30.6824 - val_MinusLogProbMetric: 30.6824 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 165/1000
2023-10-26 07:22:11.466 
Epoch 165/1000 
	 loss: 29.9506, MinusLogProbMetric: 29.9506, val_loss: 30.8763, val_MinusLogProbMetric: 30.8763

Epoch 165: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.9506 - MinusLogProbMetric: 29.9506 - val_loss: 30.8763 - val_MinusLogProbMetric: 30.8763 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 166/1000
2023-10-26 07:22:47.037 
Epoch 166/1000 
	 loss: 29.8160, MinusLogProbMetric: 29.8160, val_loss: 30.2558, val_MinusLogProbMetric: 30.2558

Epoch 166: val_loss did not improve from 29.75786
196/196 - 36s - loss: 29.8160 - MinusLogProbMetric: 29.8160 - val_loss: 30.2558 - val_MinusLogProbMetric: 30.2558 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 167/1000
2023-10-26 07:23:22.494 
Epoch 167/1000 
	 loss: 29.9143, MinusLogProbMetric: 29.9143, val_loss: 30.7563, val_MinusLogProbMetric: 30.7563

Epoch 167: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.9143 - MinusLogProbMetric: 29.9143 - val_loss: 30.7563 - val_MinusLogProbMetric: 30.7563 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 168/1000
2023-10-26 07:23:57.825 
Epoch 168/1000 
	 loss: 29.8730, MinusLogProbMetric: 29.8730, val_loss: 30.3110, val_MinusLogProbMetric: 30.3110

Epoch 168: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.8730 - MinusLogProbMetric: 29.8730 - val_loss: 30.3110 - val_MinusLogProbMetric: 30.3110 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 169/1000
2023-10-26 07:24:33.382 
Epoch 169/1000 
	 loss: 29.6752, MinusLogProbMetric: 29.6752, val_loss: 30.3671, val_MinusLogProbMetric: 30.3671

Epoch 169: val_loss did not improve from 29.75786
196/196 - 36s - loss: 29.6752 - MinusLogProbMetric: 29.6752 - val_loss: 30.3671 - val_MinusLogProbMetric: 30.3671 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 170/1000
2023-10-26 07:25:08.898 
Epoch 170/1000 
	 loss: 29.9623, MinusLogProbMetric: 29.9623, val_loss: 30.5206, val_MinusLogProbMetric: 30.5206

Epoch 170: val_loss did not improve from 29.75786
196/196 - 36s - loss: 29.9623 - MinusLogProbMetric: 29.9623 - val_loss: 30.5206 - val_MinusLogProbMetric: 30.5206 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 171/1000
2023-10-26 07:25:44.338 
Epoch 171/1000 
	 loss: 29.7792, MinusLogProbMetric: 29.7792, val_loss: 30.1288, val_MinusLogProbMetric: 30.1288

Epoch 171: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.7792 - MinusLogProbMetric: 29.7792 - val_loss: 30.1288 - val_MinusLogProbMetric: 30.1288 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 172/1000
2023-10-26 07:26:19.983 
Epoch 172/1000 
	 loss: 29.8736, MinusLogProbMetric: 29.8736, val_loss: 30.0704, val_MinusLogProbMetric: 30.0704

Epoch 172: val_loss did not improve from 29.75786
196/196 - 36s - loss: 29.8736 - MinusLogProbMetric: 29.8736 - val_loss: 30.0704 - val_MinusLogProbMetric: 30.0704 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 173/1000
2023-10-26 07:26:55.700 
Epoch 173/1000 
	 loss: 29.7429, MinusLogProbMetric: 29.7429, val_loss: 31.3721, val_MinusLogProbMetric: 31.3721

Epoch 173: val_loss did not improve from 29.75786
196/196 - 36s - loss: 29.7429 - MinusLogProbMetric: 29.7429 - val_loss: 31.3721 - val_MinusLogProbMetric: 31.3721 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 174/1000
2023-10-26 07:27:31.547 
Epoch 174/1000 
	 loss: 29.8206, MinusLogProbMetric: 29.8206, val_loss: 31.8768, val_MinusLogProbMetric: 31.8768

Epoch 174: val_loss did not improve from 29.75786
196/196 - 36s - loss: 29.8206 - MinusLogProbMetric: 29.8206 - val_loss: 31.8768 - val_MinusLogProbMetric: 31.8768 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 175/1000
2023-10-26 07:28:06.537 
Epoch 175/1000 
	 loss: 29.8144, MinusLogProbMetric: 29.8144, val_loss: 30.4847, val_MinusLogProbMetric: 30.4847

Epoch 175: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.8144 - MinusLogProbMetric: 29.8144 - val_loss: 30.4847 - val_MinusLogProbMetric: 30.4847 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 176/1000
2023-10-26 07:28:42.361 
Epoch 176/1000 
	 loss: 29.8537, MinusLogProbMetric: 29.8537, val_loss: 30.5631, val_MinusLogProbMetric: 30.5631

Epoch 176: val_loss did not improve from 29.75786
196/196 - 36s - loss: 29.8537 - MinusLogProbMetric: 29.8537 - val_loss: 30.5631 - val_MinusLogProbMetric: 30.5631 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 177/1000
2023-10-26 07:29:17.541 
Epoch 177/1000 
	 loss: 29.6752, MinusLogProbMetric: 29.6752, val_loss: 31.2639, val_MinusLogProbMetric: 31.2639

Epoch 177: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.6752 - MinusLogProbMetric: 29.6752 - val_loss: 31.2639 - val_MinusLogProbMetric: 31.2639 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 178/1000
2023-10-26 07:29:52.841 
Epoch 178/1000 
	 loss: 29.7735, MinusLogProbMetric: 29.7735, val_loss: 30.6002, val_MinusLogProbMetric: 30.6002

Epoch 178: val_loss did not improve from 29.75786
196/196 - 35s - loss: 29.7735 - MinusLogProbMetric: 29.7735 - val_loss: 30.6002 - val_MinusLogProbMetric: 30.6002 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 179/1000
2023-10-26 07:30:27.760 
Epoch 179/1000 
	 loss: 29.7013, MinusLogProbMetric: 29.7013, val_loss: 29.7342, val_MinusLogProbMetric: 29.7342

Epoch 179: val_loss improved from 29.75786 to 29.73417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 35s - loss: 29.7013 - MinusLogProbMetric: 29.7013 - val_loss: 29.7342 - val_MinusLogProbMetric: 29.7342 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 180/1000
2023-10-26 07:31:01.280 
Epoch 180/1000 
	 loss: 29.8553, MinusLogProbMetric: 29.8553, val_loss: 30.4132, val_MinusLogProbMetric: 30.4132

Epoch 180: val_loss did not improve from 29.73417
196/196 - 33s - loss: 29.8553 - MinusLogProbMetric: 29.8553 - val_loss: 30.4132 - val_MinusLogProbMetric: 30.4132 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 181/1000
2023-10-26 07:31:34.514 
Epoch 181/1000 
	 loss: 29.6127, MinusLogProbMetric: 29.6127, val_loss: 29.8207, val_MinusLogProbMetric: 29.8207

Epoch 181: val_loss did not improve from 29.73417
196/196 - 33s - loss: 29.6127 - MinusLogProbMetric: 29.6127 - val_loss: 29.8207 - val_MinusLogProbMetric: 29.8207 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 182/1000
2023-10-26 07:32:08.405 
Epoch 182/1000 
	 loss: 29.8749, MinusLogProbMetric: 29.8749, val_loss: 29.8295, val_MinusLogProbMetric: 29.8295

Epoch 182: val_loss did not improve from 29.73417
196/196 - 34s - loss: 29.8749 - MinusLogProbMetric: 29.8749 - val_loss: 29.8295 - val_MinusLogProbMetric: 29.8295 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 183/1000
2023-10-26 07:32:43.450 
Epoch 183/1000 
	 loss: 29.6277, MinusLogProbMetric: 29.6277, val_loss: 30.5857, val_MinusLogProbMetric: 30.5857

Epoch 183: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.6277 - MinusLogProbMetric: 29.6277 - val_loss: 30.5857 - val_MinusLogProbMetric: 30.5857 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 184/1000
2023-10-26 07:33:18.702 
Epoch 184/1000 
	 loss: 29.6826, MinusLogProbMetric: 29.6826, val_loss: 29.9363, val_MinusLogProbMetric: 29.9363

Epoch 184: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.6826 - MinusLogProbMetric: 29.6826 - val_loss: 29.9363 - val_MinusLogProbMetric: 29.9363 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 185/1000
2023-10-26 07:33:53.370 
Epoch 185/1000 
	 loss: 29.5901, MinusLogProbMetric: 29.5901, val_loss: 30.3255, val_MinusLogProbMetric: 30.3255

Epoch 185: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.5901 - MinusLogProbMetric: 29.5901 - val_loss: 30.3255 - val_MinusLogProbMetric: 30.3255 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 186/1000
2023-10-26 07:34:28.676 
Epoch 186/1000 
	 loss: 29.6166, MinusLogProbMetric: 29.6166, val_loss: 30.6464, val_MinusLogProbMetric: 30.6464

Epoch 186: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.6166 - MinusLogProbMetric: 29.6166 - val_loss: 30.6464 - val_MinusLogProbMetric: 30.6464 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 187/1000
2023-10-26 07:35:03.936 
Epoch 187/1000 
	 loss: 29.6016, MinusLogProbMetric: 29.6016, val_loss: 30.9999, val_MinusLogProbMetric: 30.9999

Epoch 187: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.6016 - MinusLogProbMetric: 29.6016 - val_loss: 30.9999 - val_MinusLogProbMetric: 30.9999 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 188/1000
2023-10-26 07:35:39.026 
Epoch 188/1000 
	 loss: 29.6979, MinusLogProbMetric: 29.6979, val_loss: 30.2112, val_MinusLogProbMetric: 30.2112

Epoch 188: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.6979 - MinusLogProbMetric: 29.6979 - val_loss: 30.2112 - val_MinusLogProbMetric: 30.2112 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 189/1000
2023-10-26 07:36:14.713 
Epoch 189/1000 
	 loss: 29.5239, MinusLogProbMetric: 29.5239, val_loss: 31.2356, val_MinusLogProbMetric: 31.2356

Epoch 189: val_loss did not improve from 29.73417
196/196 - 36s - loss: 29.5239 - MinusLogProbMetric: 29.5239 - val_loss: 31.2356 - val_MinusLogProbMetric: 31.2356 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 190/1000
2023-10-26 07:36:49.530 
Epoch 190/1000 
	 loss: 29.6937, MinusLogProbMetric: 29.6937, val_loss: 29.7837, val_MinusLogProbMetric: 29.7837

Epoch 190: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.6937 - MinusLogProbMetric: 29.6937 - val_loss: 29.7837 - val_MinusLogProbMetric: 29.7837 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 191/1000
2023-10-26 07:37:25.178 
Epoch 191/1000 
	 loss: 29.6984, MinusLogProbMetric: 29.6984, val_loss: 30.4336, val_MinusLogProbMetric: 30.4336

Epoch 191: val_loss did not improve from 29.73417
196/196 - 36s - loss: 29.6984 - MinusLogProbMetric: 29.6984 - val_loss: 30.4336 - val_MinusLogProbMetric: 30.4336 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 192/1000
2023-10-26 07:38:00.557 
Epoch 192/1000 
	 loss: 29.5759, MinusLogProbMetric: 29.5759, val_loss: 29.8410, val_MinusLogProbMetric: 29.8410

Epoch 192: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.5759 - MinusLogProbMetric: 29.5759 - val_loss: 29.8410 - val_MinusLogProbMetric: 29.8410 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 193/1000
2023-10-26 07:38:35.729 
Epoch 193/1000 
	 loss: 29.6318, MinusLogProbMetric: 29.6318, val_loss: 30.0705, val_MinusLogProbMetric: 30.0705

Epoch 193: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.6318 - MinusLogProbMetric: 29.6318 - val_loss: 30.0705 - val_MinusLogProbMetric: 30.0705 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 194/1000
2023-10-26 07:39:10.901 
Epoch 194/1000 
	 loss: 29.5155, MinusLogProbMetric: 29.5155, val_loss: 29.8975, val_MinusLogProbMetric: 29.8975

Epoch 194: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.5155 - MinusLogProbMetric: 29.5155 - val_loss: 29.8975 - val_MinusLogProbMetric: 29.8975 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 195/1000
2023-10-26 07:39:46.221 
Epoch 195/1000 
	 loss: 29.5273, MinusLogProbMetric: 29.5273, val_loss: 29.7347, val_MinusLogProbMetric: 29.7347

Epoch 195: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.5273 - MinusLogProbMetric: 29.5273 - val_loss: 29.7347 - val_MinusLogProbMetric: 29.7347 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 196/1000
2023-10-26 07:40:21.084 
Epoch 196/1000 
	 loss: 29.5402, MinusLogProbMetric: 29.5402, val_loss: 30.7960, val_MinusLogProbMetric: 30.7960

Epoch 196: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.5402 - MinusLogProbMetric: 29.5402 - val_loss: 30.7960 - val_MinusLogProbMetric: 30.7960 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 197/1000
2023-10-26 07:40:56.260 
Epoch 197/1000 
	 loss: 29.5513, MinusLogProbMetric: 29.5513, val_loss: 30.8657, val_MinusLogProbMetric: 30.8657

Epoch 197: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.5513 - MinusLogProbMetric: 29.5513 - val_loss: 30.8657 - val_MinusLogProbMetric: 30.8657 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 198/1000
2023-10-26 07:41:31.595 
Epoch 198/1000 
	 loss: 29.5221, MinusLogProbMetric: 29.5221, val_loss: 30.0582, val_MinusLogProbMetric: 30.0582

Epoch 198: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.5221 - MinusLogProbMetric: 29.5221 - val_loss: 30.0582 - val_MinusLogProbMetric: 30.0582 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 199/1000
2023-10-26 07:42:07.002 
Epoch 199/1000 
	 loss: 29.5600, MinusLogProbMetric: 29.5600, val_loss: 30.0797, val_MinusLogProbMetric: 30.0797

Epoch 199: val_loss did not improve from 29.73417
196/196 - 35s - loss: 29.5600 - MinusLogProbMetric: 29.5600 - val_loss: 30.0797 - val_MinusLogProbMetric: 30.0797 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 200/1000
2023-10-26 07:42:42.729 
Epoch 200/1000 
	 loss: 29.4953, MinusLogProbMetric: 29.4953, val_loss: 29.5347, val_MinusLogProbMetric: 29.5347

Epoch 200: val_loss improved from 29.73417 to 29.53465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 29.4953 - MinusLogProbMetric: 29.4953 - val_loss: 29.5347 - val_MinusLogProbMetric: 29.5347 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 201/1000
2023-10-26 07:43:18.532 
Epoch 201/1000 
	 loss: 29.4765, MinusLogProbMetric: 29.4765, val_loss: 30.1904, val_MinusLogProbMetric: 30.1904

Epoch 201: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4765 - MinusLogProbMetric: 29.4765 - val_loss: 30.1904 - val_MinusLogProbMetric: 30.1904 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 202/1000
2023-10-26 07:43:53.683 
Epoch 202/1000 
	 loss: 29.5540, MinusLogProbMetric: 29.5540, val_loss: 31.2942, val_MinusLogProbMetric: 31.2942

Epoch 202: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.5540 - MinusLogProbMetric: 29.5540 - val_loss: 31.2942 - val_MinusLogProbMetric: 31.2942 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 203/1000
2023-10-26 07:44:29.121 
Epoch 203/1000 
	 loss: 29.4995, MinusLogProbMetric: 29.4995, val_loss: 29.9986, val_MinusLogProbMetric: 29.9986

Epoch 203: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4995 - MinusLogProbMetric: 29.4995 - val_loss: 29.9986 - val_MinusLogProbMetric: 29.9986 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 204/1000
2023-10-26 07:45:04.001 
Epoch 204/1000 
	 loss: 29.5454, MinusLogProbMetric: 29.5454, val_loss: 29.5678, val_MinusLogProbMetric: 29.5678

Epoch 204: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.5454 - MinusLogProbMetric: 29.5454 - val_loss: 29.5678 - val_MinusLogProbMetric: 29.5678 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 205/1000
2023-10-26 07:45:39.475 
Epoch 205/1000 
	 loss: 29.4142, MinusLogProbMetric: 29.4142, val_loss: 30.0639, val_MinusLogProbMetric: 30.0639

Epoch 205: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4142 - MinusLogProbMetric: 29.4142 - val_loss: 30.0639 - val_MinusLogProbMetric: 30.0639 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 206/1000
2023-10-26 07:46:14.859 
Epoch 206/1000 
	 loss: 29.4547, MinusLogProbMetric: 29.4547, val_loss: 29.5571, val_MinusLogProbMetric: 29.5571

Epoch 206: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4547 - MinusLogProbMetric: 29.4547 - val_loss: 29.5571 - val_MinusLogProbMetric: 29.5571 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 207/1000
2023-10-26 07:46:50.110 
Epoch 207/1000 
	 loss: 29.4446, MinusLogProbMetric: 29.4446, val_loss: 29.7050, val_MinusLogProbMetric: 29.7050

Epoch 207: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4446 - MinusLogProbMetric: 29.4446 - val_loss: 29.7050 - val_MinusLogProbMetric: 29.7050 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 208/1000
2023-10-26 07:47:24.931 
Epoch 208/1000 
	 loss: 29.4274, MinusLogProbMetric: 29.4274, val_loss: 29.9710, val_MinusLogProbMetric: 29.9710

Epoch 208: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4274 - MinusLogProbMetric: 29.4274 - val_loss: 29.9710 - val_MinusLogProbMetric: 29.9710 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 209/1000
2023-10-26 07:48:00.305 
Epoch 209/1000 
	 loss: 29.4598, MinusLogProbMetric: 29.4598, val_loss: 29.5853, val_MinusLogProbMetric: 29.5853

Epoch 209: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4598 - MinusLogProbMetric: 29.4598 - val_loss: 29.5853 - val_MinusLogProbMetric: 29.5853 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 210/1000
2023-10-26 07:48:35.857 
Epoch 210/1000 
	 loss: 29.5703, MinusLogProbMetric: 29.5703, val_loss: 30.4039, val_MinusLogProbMetric: 30.4039

Epoch 210: val_loss did not improve from 29.53465
196/196 - 36s - loss: 29.5703 - MinusLogProbMetric: 29.5703 - val_loss: 30.4039 - val_MinusLogProbMetric: 30.4039 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 211/1000
2023-10-26 07:49:11.160 
Epoch 211/1000 
	 loss: 29.5052, MinusLogProbMetric: 29.5052, val_loss: 30.7934, val_MinusLogProbMetric: 30.7934

Epoch 211: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.5052 - MinusLogProbMetric: 29.5052 - val_loss: 30.7934 - val_MinusLogProbMetric: 30.7934 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 212/1000
2023-10-26 07:49:46.651 
Epoch 212/1000 
	 loss: 29.4117, MinusLogProbMetric: 29.4117, val_loss: 30.3406, val_MinusLogProbMetric: 30.3406

Epoch 212: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4117 - MinusLogProbMetric: 29.4117 - val_loss: 30.3406 - val_MinusLogProbMetric: 30.3406 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 213/1000
2023-10-26 07:50:21.659 
Epoch 213/1000 
	 loss: 29.4542, MinusLogProbMetric: 29.4542, val_loss: 29.6511, val_MinusLogProbMetric: 29.6511

Epoch 213: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.4542 - MinusLogProbMetric: 29.4542 - val_loss: 29.6511 - val_MinusLogProbMetric: 29.6511 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 214/1000
2023-10-26 07:50:55.602 
Epoch 214/1000 
	 loss: 29.3584, MinusLogProbMetric: 29.3584, val_loss: 31.3591, val_MinusLogProbMetric: 31.3591

Epoch 214: val_loss did not improve from 29.53465
196/196 - 34s - loss: 29.3584 - MinusLogProbMetric: 29.3584 - val_loss: 31.3591 - val_MinusLogProbMetric: 31.3591 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 215/1000
2023-10-26 07:51:30.744 
Epoch 215/1000 
	 loss: 29.3102, MinusLogProbMetric: 29.3102, val_loss: 31.2299, val_MinusLogProbMetric: 31.2299

Epoch 215: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.3102 - MinusLogProbMetric: 29.3102 - val_loss: 31.2299 - val_MinusLogProbMetric: 31.2299 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 216/1000
2023-10-26 07:52:04.340 
Epoch 216/1000 
	 loss: 29.3754, MinusLogProbMetric: 29.3754, val_loss: 29.7888, val_MinusLogProbMetric: 29.7888

Epoch 216: val_loss did not improve from 29.53465
196/196 - 34s - loss: 29.3754 - MinusLogProbMetric: 29.3754 - val_loss: 29.7888 - val_MinusLogProbMetric: 29.7888 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 217/1000
2023-10-26 07:52:38.784 
Epoch 217/1000 
	 loss: 29.4616, MinusLogProbMetric: 29.4616, val_loss: 29.6223, val_MinusLogProbMetric: 29.6223

Epoch 217: val_loss did not improve from 29.53465
196/196 - 34s - loss: 29.4616 - MinusLogProbMetric: 29.4616 - val_loss: 29.6223 - val_MinusLogProbMetric: 29.6223 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 218/1000
2023-10-26 07:53:14.304 
Epoch 218/1000 
	 loss: 29.3615, MinusLogProbMetric: 29.3615, val_loss: 29.6007, val_MinusLogProbMetric: 29.6007

Epoch 218: val_loss did not improve from 29.53465
196/196 - 36s - loss: 29.3615 - MinusLogProbMetric: 29.3615 - val_loss: 29.6007 - val_MinusLogProbMetric: 29.6007 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 219/1000
2023-10-26 07:53:49.581 
Epoch 219/1000 
	 loss: 29.3275, MinusLogProbMetric: 29.3275, val_loss: 29.7805, val_MinusLogProbMetric: 29.7805

Epoch 219: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.3275 - MinusLogProbMetric: 29.3275 - val_loss: 29.7805 - val_MinusLogProbMetric: 29.7805 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 220/1000
2023-10-26 07:54:25.170 
Epoch 220/1000 
	 loss: 29.4278, MinusLogProbMetric: 29.4278, val_loss: 30.4409, val_MinusLogProbMetric: 30.4409

Epoch 220: val_loss did not improve from 29.53465
196/196 - 36s - loss: 29.4278 - MinusLogProbMetric: 29.4278 - val_loss: 30.4409 - val_MinusLogProbMetric: 30.4409 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 221/1000
2023-10-26 07:55:00.151 
Epoch 221/1000 
	 loss: 29.2711, MinusLogProbMetric: 29.2711, val_loss: 31.1865, val_MinusLogProbMetric: 31.1865

Epoch 221: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.2711 - MinusLogProbMetric: 29.2711 - val_loss: 31.1865 - val_MinusLogProbMetric: 31.1865 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 222/1000
2023-10-26 07:55:35.710 
Epoch 222/1000 
	 loss: 29.5068, MinusLogProbMetric: 29.5068, val_loss: 30.2396, val_MinusLogProbMetric: 30.2396

Epoch 222: val_loss did not improve from 29.53465
196/196 - 36s - loss: 29.5068 - MinusLogProbMetric: 29.5068 - val_loss: 30.2396 - val_MinusLogProbMetric: 30.2396 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 223/1000
2023-10-26 07:56:11.304 
Epoch 223/1000 
	 loss: 29.3096, MinusLogProbMetric: 29.3096, val_loss: 29.5461, val_MinusLogProbMetric: 29.5461

Epoch 223: val_loss did not improve from 29.53465
196/196 - 36s - loss: 29.3096 - MinusLogProbMetric: 29.3096 - val_loss: 29.5461 - val_MinusLogProbMetric: 29.5461 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 224/1000
2023-10-26 07:56:47.211 
Epoch 224/1000 
	 loss: 29.3812, MinusLogProbMetric: 29.3812, val_loss: 29.8710, val_MinusLogProbMetric: 29.8710

Epoch 224: val_loss did not improve from 29.53465
196/196 - 36s - loss: 29.3812 - MinusLogProbMetric: 29.3812 - val_loss: 29.8710 - val_MinusLogProbMetric: 29.8710 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 225/1000
2023-10-26 07:57:23.001 
Epoch 225/1000 
	 loss: 29.2740, MinusLogProbMetric: 29.2740, val_loss: 29.8949, val_MinusLogProbMetric: 29.8949

Epoch 225: val_loss did not improve from 29.53465
196/196 - 36s - loss: 29.2740 - MinusLogProbMetric: 29.2740 - val_loss: 29.8949 - val_MinusLogProbMetric: 29.8949 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 226/1000
2023-10-26 07:57:58.463 
Epoch 226/1000 
	 loss: 29.3357, MinusLogProbMetric: 29.3357, val_loss: 29.7021, val_MinusLogProbMetric: 29.7021

Epoch 226: val_loss did not improve from 29.53465
196/196 - 35s - loss: 29.3357 - MinusLogProbMetric: 29.3357 - val_loss: 29.7021 - val_MinusLogProbMetric: 29.7021 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 227/1000
2023-10-26 07:58:28.609 
Epoch 227/1000 
	 loss: 29.3637, MinusLogProbMetric: 29.3637, val_loss: 30.1503, val_MinusLogProbMetric: 30.1503

Epoch 227: val_loss did not improve from 29.53465
196/196 - 30s - loss: 29.3637 - MinusLogProbMetric: 29.3637 - val_loss: 30.1503 - val_MinusLogProbMetric: 30.1503 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 228/1000
2023-10-26 07:58:58.712 
Epoch 228/1000 
	 loss: 29.3095, MinusLogProbMetric: 29.3095, val_loss: 29.4620, val_MinusLogProbMetric: 29.4620

Epoch 228: val_loss improved from 29.53465 to 29.46200, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 31s - loss: 29.3095 - MinusLogProbMetric: 29.3095 - val_loss: 29.4620 - val_MinusLogProbMetric: 29.4620 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 229/1000
2023-10-26 07:59:29.347 
Epoch 229/1000 
	 loss: 29.3569, MinusLogProbMetric: 29.3569, val_loss: 29.9960, val_MinusLogProbMetric: 29.9960

Epoch 229: val_loss did not improve from 29.46200
196/196 - 30s - loss: 29.3569 - MinusLogProbMetric: 29.3569 - val_loss: 29.9960 - val_MinusLogProbMetric: 29.9960 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 230/1000
2023-10-26 08:00:00.309 
Epoch 230/1000 
	 loss: 29.3721, MinusLogProbMetric: 29.3721, val_loss: 29.5130, val_MinusLogProbMetric: 29.5130

Epoch 230: val_loss did not improve from 29.46200
196/196 - 31s - loss: 29.3721 - MinusLogProbMetric: 29.3721 - val_loss: 29.5130 - val_MinusLogProbMetric: 29.5130 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 231/1000
2023-10-26 08:00:33.271 
Epoch 231/1000 
	 loss: 29.3060, MinusLogProbMetric: 29.3060, val_loss: 29.2720, val_MinusLogProbMetric: 29.2720

Epoch 231: val_loss improved from 29.46200 to 29.27195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 33s - loss: 29.3060 - MinusLogProbMetric: 29.3060 - val_loss: 29.2720 - val_MinusLogProbMetric: 29.2720 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 232/1000
2023-10-26 08:01:09.088 
Epoch 232/1000 
	 loss: 29.2721, MinusLogProbMetric: 29.2721, val_loss: 29.8173, val_MinusLogProbMetric: 29.8173

Epoch 232: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.2721 - MinusLogProbMetric: 29.2721 - val_loss: 29.8173 - val_MinusLogProbMetric: 29.8173 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 233/1000
2023-10-26 08:01:44.409 
Epoch 233/1000 
	 loss: 29.3662, MinusLogProbMetric: 29.3662, val_loss: 31.1532, val_MinusLogProbMetric: 31.1532

Epoch 233: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.3662 - MinusLogProbMetric: 29.3662 - val_loss: 31.1532 - val_MinusLogProbMetric: 31.1532 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 234/1000
2023-10-26 08:02:18.874 
Epoch 234/1000 
	 loss: 29.3284, MinusLogProbMetric: 29.3284, val_loss: 29.8835, val_MinusLogProbMetric: 29.8835

Epoch 234: val_loss did not improve from 29.27195
196/196 - 34s - loss: 29.3284 - MinusLogProbMetric: 29.3284 - val_loss: 29.8835 - val_MinusLogProbMetric: 29.8835 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 235/1000
2023-10-26 08:02:53.990 
Epoch 235/1000 
	 loss: 29.3167, MinusLogProbMetric: 29.3167, val_loss: 29.7095, val_MinusLogProbMetric: 29.7095

Epoch 235: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.3167 - MinusLogProbMetric: 29.3167 - val_loss: 29.7095 - val_MinusLogProbMetric: 29.7095 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 236/1000
2023-10-26 08:03:29.346 
Epoch 236/1000 
	 loss: 29.1840, MinusLogProbMetric: 29.1840, val_loss: 30.3075, val_MinusLogProbMetric: 30.3075

Epoch 236: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.1840 - MinusLogProbMetric: 29.1840 - val_loss: 30.3075 - val_MinusLogProbMetric: 30.3075 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 237/1000
2023-10-26 08:04:04.751 
Epoch 237/1000 
	 loss: 29.3336, MinusLogProbMetric: 29.3336, val_loss: 29.5313, val_MinusLogProbMetric: 29.5313

Epoch 237: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.3336 - MinusLogProbMetric: 29.3336 - val_loss: 29.5313 - val_MinusLogProbMetric: 29.5313 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 238/1000
2023-10-26 08:04:40.482 
Epoch 238/1000 
	 loss: 29.1890, MinusLogProbMetric: 29.1890, val_loss: 29.8613, val_MinusLogProbMetric: 29.8613

Epoch 238: val_loss did not improve from 29.27195
196/196 - 36s - loss: 29.1890 - MinusLogProbMetric: 29.1890 - val_loss: 29.8613 - val_MinusLogProbMetric: 29.8613 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 239/1000
2023-10-26 08:05:16.023 
Epoch 239/1000 
	 loss: 29.2677, MinusLogProbMetric: 29.2677, val_loss: 29.6156, val_MinusLogProbMetric: 29.6156

Epoch 239: val_loss did not improve from 29.27195
196/196 - 36s - loss: 29.2677 - MinusLogProbMetric: 29.2677 - val_loss: 29.6156 - val_MinusLogProbMetric: 29.6156 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 240/1000
2023-10-26 08:05:51.387 
Epoch 240/1000 
	 loss: 29.2857, MinusLogProbMetric: 29.2857, val_loss: 29.8550, val_MinusLogProbMetric: 29.8550

Epoch 240: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.2857 - MinusLogProbMetric: 29.2857 - val_loss: 29.8550 - val_MinusLogProbMetric: 29.8550 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 241/1000
2023-10-26 08:06:26.752 
Epoch 241/1000 
	 loss: 29.1815, MinusLogProbMetric: 29.1815, val_loss: 29.3973, val_MinusLogProbMetric: 29.3973

Epoch 241: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.1815 - MinusLogProbMetric: 29.1815 - val_loss: 29.3973 - val_MinusLogProbMetric: 29.3973 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 242/1000
2023-10-26 08:07:01.994 
Epoch 242/1000 
	 loss: 29.2481, MinusLogProbMetric: 29.2481, val_loss: 30.5744, val_MinusLogProbMetric: 30.5744

Epoch 242: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.2481 - MinusLogProbMetric: 29.2481 - val_loss: 30.5744 - val_MinusLogProbMetric: 30.5744 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 243/1000
2023-10-26 08:07:37.311 
Epoch 243/1000 
	 loss: 29.2164, MinusLogProbMetric: 29.2164, val_loss: 29.6797, val_MinusLogProbMetric: 29.6797

Epoch 243: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.2164 - MinusLogProbMetric: 29.2164 - val_loss: 29.6797 - val_MinusLogProbMetric: 29.6797 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 244/1000
2023-10-26 08:08:12.481 
Epoch 244/1000 
	 loss: 29.1734, MinusLogProbMetric: 29.1734, val_loss: 30.3371, val_MinusLogProbMetric: 30.3371

Epoch 244: val_loss did not improve from 29.27195
196/196 - 35s - loss: 29.1734 - MinusLogProbMetric: 29.1734 - val_loss: 30.3371 - val_MinusLogProbMetric: 30.3371 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 245/1000
2023-10-26 08:08:47.716 
Epoch 245/1000 
	 loss: 29.3000, MinusLogProbMetric: 29.3000, val_loss: 29.2011, val_MinusLogProbMetric: 29.2011

Epoch 245: val_loss improved from 29.27195 to 29.20114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 29.3000 - MinusLogProbMetric: 29.3000 - val_loss: 29.2011 - val_MinusLogProbMetric: 29.2011 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 246/1000
2023-10-26 08:09:23.574 
Epoch 246/1000 
	 loss: 29.1865, MinusLogProbMetric: 29.1865, val_loss: 29.3511, val_MinusLogProbMetric: 29.3511

Epoch 246: val_loss did not improve from 29.20114
196/196 - 35s - loss: 29.1865 - MinusLogProbMetric: 29.1865 - val_loss: 29.3511 - val_MinusLogProbMetric: 29.3511 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 247/1000
2023-10-26 08:09:58.895 
Epoch 247/1000 
	 loss: 29.1388, MinusLogProbMetric: 29.1388, val_loss: 29.9557, val_MinusLogProbMetric: 29.9557

Epoch 247: val_loss did not improve from 29.20114
196/196 - 35s - loss: 29.1388 - MinusLogProbMetric: 29.1388 - val_loss: 29.9557 - val_MinusLogProbMetric: 29.9557 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 248/1000
2023-10-26 08:10:29.226 
Epoch 248/1000 
	 loss: 29.1991, MinusLogProbMetric: 29.1991, val_loss: 29.5862, val_MinusLogProbMetric: 29.5862

Epoch 248: val_loss did not improve from 29.20114
196/196 - 30s - loss: 29.1991 - MinusLogProbMetric: 29.1991 - val_loss: 29.5862 - val_MinusLogProbMetric: 29.5862 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 249/1000
2023-10-26 08:10:59.568 
Epoch 249/1000 
	 loss: 29.1752, MinusLogProbMetric: 29.1752, val_loss: 29.6384, val_MinusLogProbMetric: 29.6384

Epoch 249: val_loss did not improve from 29.20114
196/196 - 30s - loss: 29.1752 - MinusLogProbMetric: 29.1752 - val_loss: 29.6384 - val_MinusLogProbMetric: 29.6384 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 250/1000
2023-10-26 08:11:33.431 
Epoch 250/1000 
	 loss: 29.1070, MinusLogProbMetric: 29.1070, val_loss: 29.6003, val_MinusLogProbMetric: 29.6003

Epoch 250: val_loss did not improve from 29.20114
196/196 - 34s - loss: 29.1070 - MinusLogProbMetric: 29.1070 - val_loss: 29.6003 - val_MinusLogProbMetric: 29.6003 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 251/1000
2023-10-26 08:12:04.913 
Epoch 251/1000 
	 loss: 29.1277, MinusLogProbMetric: 29.1277, val_loss: 29.6226, val_MinusLogProbMetric: 29.6226

Epoch 251: val_loss did not improve from 29.20114
196/196 - 31s - loss: 29.1277 - MinusLogProbMetric: 29.1277 - val_loss: 29.6226 - val_MinusLogProbMetric: 29.6226 - lr: 3.3333e-04 - 31s/epoch - 161ms/step
Epoch 252/1000
2023-10-26 08:12:35.450 
Epoch 252/1000 
	 loss: 29.1637, MinusLogProbMetric: 29.1637, val_loss: 29.5445, val_MinusLogProbMetric: 29.5445

Epoch 252: val_loss did not improve from 29.20114
196/196 - 31s - loss: 29.1637 - MinusLogProbMetric: 29.1637 - val_loss: 29.5445 - val_MinusLogProbMetric: 29.5445 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 253/1000
2023-10-26 08:13:09.848 
Epoch 253/1000 
	 loss: 29.1413, MinusLogProbMetric: 29.1413, val_loss: 30.3383, val_MinusLogProbMetric: 30.3383

Epoch 253: val_loss did not improve from 29.20114
196/196 - 34s - loss: 29.1413 - MinusLogProbMetric: 29.1413 - val_loss: 30.3383 - val_MinusLogProbMetric: 30.3383 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 254/1000
2023-10-26 08:13:40.341 
Epoch 254/1000 
	 loss: 29.1106, MinusLogProbMetric: 29.1106, val_loss: 29.5971, val_MinusLogProbMetric: 29.5971

Epoch 254: val_loss did not improve from 29.20114
196/196 - 30s - loss: 29.1106 - MinusLogProbMetric: 29.1106 - val_loss: 29.5971 - val_MinusLogProbMetric: 29.5971 - lr: 3.3333e-04 - 30s/epoch - 156ms/step
Epoch 255/1000
2023-10-26 08:14:13.664 
Epoch 255/1000 
	 loss: 29.3001, MinusLogProbMetric: 29.3001, val_loss: 29.6116, val_MinusLogProbMetric: 29.6116

Epoch 255: val_loss did not improve from 29.20114
196/196 - 33s - loss: 29.3001 - MinusLogProbMetric: 29.3001 - val_loss: 29.6116 - val_MinusLogProbMetric: 29.6116 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 256/1000
2023-10-26 08:14:47.132 
Epoch 256/1000 
	 loss: 29.1362, MinusLogProbMetric: 29.1362, val_loss: 30.0688, val_MinusLogProbMetric: 30.0688

Epoch 256: val_loss did not improve from 29.20114
196/196 - 33s - loss: 29.1362 - MinusLogProbMetric: 29.1362 - val_loss: 30.0688 - val_MinusLogProbMetric: 30.0688 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 257/1000
2023-10-26 08:15:17.720 
Epoch 257/1000 
	 loss: 29.0794, MinusLogProbMetric: 29.0794, val_loss: 29.3270, val_MinusLogProbMetric: 29.3270

Epoch 257: val_loss did not improve from 29.20114
196/196 - 31s - loss: 29.0794 - MinusLogProbMetric: 29.0794 - val_loss: 29.3270 - val_MinusLogProbMetric: 29.3270 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 258/1000
2023-10-26 08:15:50.726 
Epoch 258/1000 
	 loss: 29.1732, MinusLogProbMetric: 29.1732, val_loss: 29.7085, val_MinusLogProbMetric: 29.7085

Epoch 258: val_loss did not improve from 29.20114
196/196 - 33s - loss: 29.1732 - MinusLogProbMetric: 29.1732 - val_loss: 29.7085 - val_MinusLogProbMetric: 29.7085 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 259/1000
2023-10-26 08:16:24.538 
Epoch 259/1000 
	 loss: 29.1496, MinusLogProbMetric: 29.1496, val_loss: 29.9403, val_MinusLogProbMetric: 29.9403

Epoch 259: val_loss did not improve from 29.20114
196/196 - 34s - loss: 29.1496 - MinusLogProbMetric: 29.1496 - val_loss: 29.9403 - val_MinusLogProbMetric: 29.9403 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 260/1000
2023-10-26 08:16:54.820 
Epoch 260/1000 
	 loss: 29.1967, MinusLogProbMetric: 29.1967, val_loss: 29.3107, val_MinusLogProbMetric: 29.3107

Epoch 260: val_loss did not improve from 29.20114
196/196 - 30s - loss: 29.1967 - MinusLogProbMetric: 29.1967 - val_loss: 29.3107 - val_MinusLogProbMetric: 29.3107 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 261/1000
2023-10-26 08:17:26.334 
Epoch 261/1000 
	 loss: 29.1965, MinusLogProbMetric: 29.1965, val_loss: 29.3880, val_MinusLogProbMetric: 29.3880

Epoch 261: val_loss did not improve from 29.20114
196/196 - 32s - loss: 29.1965 - MinusLogProbMetric: 29.1965 - val_loss: 29.3880 - val_MinusLogProbMetric: 29.3880 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 262/1000
2023-10-26 08:18:01.791 
Epoch 262/1000 
	 loss: 29.0658, MinusLogProbMetric: 29.0658, val_loss: 29.8896, val_MinusLogProbMetric: 29.8896

Epoch 262: val_loss did not improve from 29.20114
196/196 - 35s - loss: 29.0658 - MinusLogProbMetric: 29.0658 - val_loss: 29.8896 - val_MinusLogProbMetric: 29.8896 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 263/1000
2023-10-26 08:18:33.421 
Epoch 263/1000 
	 loss: 29.1331, MinusLogProbMetric: 29.1331, val_loss: 29.5098, val_MinusLogProbMetric: 29.5098

Epoch 263: val_loss did not improve from 29.20114
196/196 - 32s - loss: 29.1331 - MinusLogProbMetric: 29.1331 - val_loss: 29.5098 - val_MinusLogProbMetric: 29.5098 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 264/1000
2023-10-26 08:19:06.237 
Epoch 264/1000 
	 loss: 29.1501, MinusLogProbMetric: 29.1501, val_loss: 29.5963, val_MinusLogProbMetric: 29.5963

Epoch 264: val_loss did not improve from 29.20114
196/196 - 33s - loss: 29.1501 - MinusLogProbMetric: 29.1501 - val_loss: 29.5963 - val_MinusLogProbMetric: 29.5963 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 265/1000
2023-10-26 08:19:40.298 
Epoch 265/1000 
	 loss: 29.0451, MinusLogProbMetric: 29.0451, val_loss: 29.8505, val_MinusLogProbMetric: 29.8505

Epoch 265: val_loss did not improve from 29.20114
196/196 - 34s - loss: 29.0451 - MinusLogProbMetric: 29.0451 - val_loss: 29.8505 - val_MinusLogProbMetric: 29.8505 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 266/1000
2023-10-26 08:20:10.349 
Epoch 266/1000 
	 loss: 29.0919, MinusLogProbMetric: 29.0919, val_loss: 29.5108, val_MinusLogProbMetric: 29.5108

Epoch 266: val_loss did not improve from 29.20114
196/196 - 30s - loss: 29.0919 - MinusLogProbMetric: 29.0919 - val_loss: 29.5108 - val_MinusLogProbMetric: 29.5108 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 267/1000
2023-10-26 08:20:44.340 
Epoch 267/1000 
	 loss: 29.1456, MinusLogProbMetric: 29.1456, val_loss: 30.3010, val_MinusLogProbMetric: 30.3010

Epoch 267: val_loss did not improve from 29.20114
196/196 - 34s - loss: 29.1456 - MinusLogProbMetric: 29.1456 - val_loss: 30.3010 - val_MinusLogProbMetric: 30.3010 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 268/1000
2023-10-26 08:21:17.583 
Epoch 268/1000 
	 loss: 29.1616, MinusLogProbMetric: 29.1616, val_loss: 30.0131, val_MinusLogProbMetric: 30.0131

Epoch 268: val_loss did not improve from 29.20114
196/196 - 33s - loss: 29.1616 - MinusLogProbMetric: 29.1616 - val_loss: 30.0131 - val_MinusLogProbMetric: 30.0131 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 269/1000
2023-10-26 08:21:47.481 
Epoch 269/1000 
	 loss: 28.9989, MinusLogProbMetric: 28.9989, val_loss: 29.2678, val_MinusLogProbMetric: 29.2678

Epoch 269: val_loss did not improve from 29.20114
196/196 - 30s - loss: 28.9989 - MinusLogProbMetric: 28.9989 - val_loss: 29.2678 - val_MinusLogProbMetric: 29.2678 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 270/1000
2023-10-26 08:22:18.352 
Epoch 270/1000 
	 loss: 29.0774, MinusLogProbMetric: 29.0774, val_loss: 30.4869, val_MinusLogProbMetric: 30.4869

Epoch 270: val_loss did not improve from 29.20114
196/196 - 31s - loss: 29.0774 - MinusLogProbMetric: 29.0774 - val_loss: 30.4869 - val_MinusLogProbMetric: 30.4869 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 271/1000
2023-10-26 08:22:53.042 
Epoch 271/1000 
	 loss: 29.0303, MinusLogProbMetric: 29.0303, val_loss: 30.5391, val_MinusLogProbMetric: 30.5391

Epoch 271: val_loss did not improve from 29.20114
196/196 - 35s - loss: 29.0303 - MinusLogProbMetric: 29.0303 - val_loss: 30.5391 - val_MinusLogProbMetric: 30.5391 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 272/1000
2023-10-26 08:23:24.617 
Epoch 272/1000 
	 loss: 29.1406, MinusLogProbMetric: 29.1406, val_loss: 29.5117, val_MinusLogProbMetric: 29.5117

Epoch 272: val_loss did not improve from 29.20114
196/196 - 32s - loss: 29.1406 - MinusLogProbMetric: 29.1406 - val_loss: 29.5117 - val_MinusLogProbMetric: 29.5117 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 273/1000
2023-10-26 08:23:56.124 
Epoch 273/1000 
	 loss: 29.1118, MinusLogProbMetric: 29.1118, val_loss: 29.6837, val_MinusLogProbMetric: 29.6837

Epoch 273: val_loss did not improve from 29.20114
196/196 - 32s - loss: 29.1118 - MinusLogProbMetric: 29.1118 - val_loss: 29.6837 - val_MinusLogProbMetric: 29.6837 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 274/1000
2023-10-26 08:24:29.400 
Epoch 274/1000 
	 loss: 29.0813, MinusLogProbMetric: 29.0813, val_loss: 29.7578, val_MinusLogProbMetric: 29.7578

Epoch 274: val_loss did not improve from 29.20114
196/196 - 33s - loss: 29.0813 - MinusLogProbMetric: 29.0813 - val_loss: 29.7578 - val_MinusLogProbMetric: 29.7578 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 275/1000
2023-10-26 08:24:59.484 
Epoch 275/1000 
	 loss: 29.1678, MinusLogProbMetric: 29.1678, val_loss: 29.2962, val_MinusLogProbMetric: 29.2962

Epoch 275: val_loss did not improve from 29.20114
196/196 - 30s - loss: 29.1678 - MinusLogProbMetric: 29.1678 - val_loss: 29.2962 - val_MinusLogProbMetric: 29.2962 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 276/1000
2023-10-26 08:25:33.185 
Epoch 276/1000 
	 loss: 29.0133, MinusLogProbMetric: 29.0133, val_loss: 29.4669, val_MinusLogProbMetric: 29.4669

Epoch 276: val_loss did not improve from 29.20114
196/196 - 34s - loss: 29.0133 - MinusLogProbMetric: 29.0133 - val_loss: 29.4669 - val_MinusLogProbMetric: 29.4669 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 277/1000
2023-10-26 08:26:06.235 
Epoch 277/1000 
	 loss: 29.0662, MinusLogProbMetric: 29.0662, val_loss: 29.4549, val_MinusLogProbMetric: 29.4549

Epoch 277: val_loss did not improve from 29.20114
196/196 - 33s - loss: 29.0662 - MinusLogProbMetric: 29.0662 - val_loss: 29.4549 - val_MinusLogProbMetric: 29.4549 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 278/1000
2023-10-26 08:26:37.265 
Epoch 278/1000 
	 loss: 29.0119, MinusLogProbMetric: 29.0119, val_loss: 29.7937, val_MinusLogProbMetric: 29.7937

Epoch 278: val_loss did not improve from 29.20114
196/196 - 31s - loss: 29.0119 - MinusLogProbMetric: 29.0119 - val_loss: 29.7937 - val_MinusLogProbMetric: 29.7937 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 279/1000
2023-10-26 08:27:11.797 
Epoch 279/1000 
	 loss: 29.0325, MinusLogProbMetric: 29.0325, val_loss: 29.3284, val_MinusLogProbMetric: 29.3284

Epoch 279: val_loss did not improve from 29.20114
196/196 - 35s - loss: 29.0325 - MinusLogProbMetric: 29.0325 - val_loss: 29.3284 - val_MinusLogProbMetric: 29.3284 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 280/1000
2023-10-26 08:27:44.760 
Epoch 280/1000 
	 loss: 29.3421, MinusLogProbMetric: 29.3421, val_loss: 29.8263, val_MinusLogProbMetric: 29.8263

Epoch 280: val_loss did not improve from 29.20114
196/196 - 33s - loss: 29.3421 - MinusLogProbMetric: 29.3421 - val_loss: 29.8263 - val_MinusLogProbMetric: 29.8263 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 281/1000
2023-10-26 08:28:14.696 
Epoch 281/1000 
	 loss: 29.0150, MinusLogProbMetric: 29.0150, val_loss: 29.4430, val_MinusLogProbMetric: 29.4430

Epoch 281: val_loss did not improve from 29.20114
196/196 - 30s - loss: 29.0150 - MinusLogProbMetric: 29.0150 - val_loss: 29.4430 - val_MinusLogProbMetric: 29.4430 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 282/1000
2023-10-26 08:28:49.181 
Epoch 282/1000 
	 loss: 29.0324, MinusLogProbMetric: 29.0324, val_loss: 29.6675, val_MinusLogProbMetric: 29.6675

Epoch 282: val_loss did not improve from 29.20114
196/196 - 34s - loss: 29.0324 - MinusLogProbMetric: 29.0324 - val_loss: 29.6675 - val_MinusLogProbMetric: 29.6675 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 283/1000
2023-10-26 08:29:24.157 
Epoch 283/1000 
	 loss: 28.9978, MinusLogProbMetric: 28.9978, val_loss: 29.3960, val_MinusLogProbMetric: 29.3960

Epoch 283: val_loss did not improve from 29.20114
196/196 - 35s - loss: 28.9978 - MinusLogProbMetric: 28.9978 - val_loss: 29.3960 - val_MinusLogProbMetric: 29.3960 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 284/1000
2023-10-26 08:29:58.042 
Epoch 284/1000 
	 loss: 28.9383, MinusLogProbMetric: 28.9383, val_loss: 29.6860, val_MinusLogProbMetric: 29.6860

Epoch 284: val_loss did not improve from 29.20114
196/196 - 34s - loss: 28.9383 - MinusLogProbMetric: 28.9383 - val_loss: 29.6860 - val_MinusLogProbMetric: 29.6860 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 285/1000
2023-10-26 08:30:33.549 
Epoch 285/1000 
	 loss: 28.9985, MinusLogProbMetric: 28.9985, val_loss: 29.6052, val_MinusLogProbMetric: 29.6052

Epoch 285: val_loss did not improve from 29.20114
196/196 - 36s - loss: 28.9985 - MinusLogProbMetric: 28.9985 - val_loss: 29.6052 - val_MinusLogProbMetric: 29.6052 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 286/1000
2023-10-26 08:31:08.731 
Epoch 286/1000 
	 loss: 29.0404, MinusLogProbMetric: 29.0404, val_loss: 29.3755, val_MinusLogProbMetric: 29.3755

Epoch 286: val_loss did not improve from 29.20114
196/196 - 35s - loss: 29.0404 - MinusLogProbMetric: 29.0404 - val_loss: 29.3755 - val_MinusLogProbMetric: 29.3755 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 287/1000
2023-10-26 08:31:43.807 
Epoch 287/1000 
	 loss: 28.9297, MinusLogProbMetric: 28.9297, val_loss: 29.9753, val_MinusLogProbMetric: 29.9753

Epoch 287: val_loss did not improve from 29.20114
196/196 - 35s - loss: 28.9297 - MinusLogProbMetric: 28.9297 - val_loss: 29.9753 - val_MinusLogProbMetric: 29.9753 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 288/1000
2023-10-26 08:32:18.988 
Epoch 288/1000 
	 loss: 28.8889, MinusLogProbMetric: 28.8889, val_loss: 29.4821, val_MinusLogProbMetric: 29.4821

Epoch 288: val_loss did not improve from 29.20114
196/196 - 35s - loss: 28.8889 - MinusLogProbMetric: 28.8889 - val_loss: 29.4821 - val_MinusLogProbMetric: 29.4821 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 289/1000
2023-10-26 08:32:53.258 
Epoch 289/1000 
	 loss: 28.9707, MinusLogProbMetric: 28.9707, val_loss: 29.3027, val_MinusLogProbMetric: 29.3027

Epoch 289: val_loss did not improve from 29.20114
196/196 - 34s - loss: 28.9707 - MinusLogProbMetric: 28.9707 - val_loss: 29.3027 - val_MinusLogProbMetric: 29.3027 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 290/1000
2023-10-26 08:33:28.064 
Epoch 290/1000 
	 loss: 28.9900, MinusLogProbMetric: 28.9900, val_loss: 30.3524, val_MinusLogProbMetric: 30.3524

Epoch 290: val_loss did not improve from 29.20114
196/196 - 35s - loss: 28.9900 - MinusLogProbMetric: 28.9900 - val_loss: 30.3524 - val_MinusLogProbMetric: 30.3524 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 291/1000
2023-10-26 08:34:02.307 
Epoch 291/1000 
	 loss: 28.9334, MinusLogProbMetric: 28.9334, val_loss: 29.2515, val_MinusLogProbMetric: 29.2515

Epoch 291: val_loss did not improve from 29.20114
196/196 - 34s - loss: 28.9334 - MinusLogProbMetric: 28.9334 - val_loss: 29.2515 - val_MinusLogProbMetric: 29.2515 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 292/1000
2023-10-26 08:34:37.340 
Epoch 292/1000 
	 loss: 28.9058, MinusLogProbMetric: 28.9058, val_loss: 29.2670, val_MinusLogProbMetric: 29.2670

Epoch 292: val_loss did not improve from 29.20114
196/196 - 35s - loss: 28.9058 - MinusLogProbMetric: 28.9058 - val_loss: 29.2670 - val_MinusLogProbMetric: 29.2670 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 293/1000
2023-10-26 08:35:12.651 
Epoch 293/1000 
	 loss: 28.8569, MinusLogProbMetric: 28.8569, val_loss: 29.2778, val_MinusLogProbMetric: 29.2778

Epoch 293: val_loss did not improve from 29.20114
196/196 - 35s - loss: 28.8569 - MinusLogProbMetric: 28.8569 - val_loss: 29.2778 - val_MinusLogProbMetric: 29.2778 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 294/1000
2023-10-26 08:35:47.895 
Epoch 294/1000 
	 loss: 28.9849, MinusLogProbMetric: 28.9849, val_loss: 29.1757, val_MinusLogProbMetric: 29.1757

Epoch 294: val_loss improved from 29.20114 to 29.17573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 28.9849 - MinusLogProbMetric: 28.9849 - val_loss: 29.1757 - val_MinusLogProbMetric: 29.1757 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 295/1000
2023-10-26 08:36:23.879 
Epoch 295/1000 
	 loss: 29.0073, MinusLogProbMetric: 29.0073, val_loss: 29.0821, val_MinusLogProbMetric: 29.0821

Epoch 295: val_loss improved from 29.17573 to 29.08209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 29.0073 - MinusLogProbMetric: 29.0073 - val_loss: 29.0821 - val_MinusLogProbMetric: 29.0821 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 296/1000
2023-10-26 08:36:59.932 
Epoch 296/1000 
	 loss: 28.8583, MinusLogProbMetric: 28.8583, val_loss: 29.5080, val_MinusLogProbMetric: 29.5080

Epoch 296: val_loss did not improve from 29.08209
196/196 - 36s - loss: 28.8583 - MinusLogProbMetric: 28.8583 - val_loss: 29.5080 - val_MinusLogProbMetric: 29.5080 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 297/1000
2023-10-26 08:37:34.810 
Epoch 297/1000 
	 loss: 28.9443, MinusLogProbMetric: 28.9443, val_loss: 29.8203, val_MinusLogProbMetric: 29.8203

Epoch 297: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9443 - MinusLogProbMetric: 28.9443 - val_loss: 29.8203 - val_MinusLogProbMetric: 29.8203 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 298/1000
2023-10-26 08:38:10.089 
Epoch 298/1000 
	 loss: 29.0129, MinusLogProbMetric: 29.0129, val_loss: 29.7205, val_MinusLogProbMetric: 29.7205

Epoch 298: val_loss did not improve from 29.08209
196/196 - 35s - loss: 29.0129 - MinusLogProbMetric: 29.0129 - val_loss: 29.7205 - val_MinusLogProbMetric: 29.7205 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 299/1000
2023-10-26 08:38:45.234 
Epoch 299/1000 
	 loss: 28.9632, MinusLogProbMetric: 28.9632, val_loss: 29.2563, val_MinusLogProbMetric: 29.2563

Epoch 299: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9632 - MinusLogProbMetric: 28.9632 - val_loss: 29.2563 - val_MinusLogProbMetric: 29.2563 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 300/1000
2023-10-26 08:39:20.347 
Epoch 300/1000 
	 loss: 28.9177, MinusLogProbMetric: 28.9177, val_loss: 30.4546, val_MinusLogProbMetric: 30.4546

Epoch 300: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9177 - MinusLogProbMetric: 28.9177 - val_loss: 30.4546 - val_MinusLogProbMetric: 30.4546 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 301/1000
2023-10-26 08:39:55.705 
Epoch 301/1000 
	 loss: 28.9663, MinusLogProbMetric: 28.9663, val_loss: 29.6935, val_MinusLogProbMetric: 29.6935

Epoch 301: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9663 - MinusLogProbMetric: 28.9663 - val_loss: 29.6935 - val_MinusLogProbMetric: 29.6935 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 302/1000
2023-10-26 08:40:31.331 
Epoch 302/1000 
	 loss: 28.9660, MinusLogProbMetric: 28.9660, val_loss: 29.3396, val_MinusLogProbMetric: 29.3396

Epoch 302: val_loss did not improve from 29.08209
196/196 - 36s - loss: 28.9660 - MinusLogProbMetric: 28.9660 - val_loss: 29.3396 - val_MinusLogProbMetric: 29.3396 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 303/1000
2023-10-26 08:41:06.119 
Epoch 303/1000 
	 loss: 28.9676, MinusLogProbMetric: 28.9676, val_loss: 29.4607, val_MinusLogProbMetric: 29.4607

Epoch 303: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9676 - MinusLogProbMetric: 28.9676 - val_loss: 29.4607 - val_MinusLogProbMetric: 29.4607 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 304/1000
2023-10-26 08:41:41.031 
Epoch 304/1000 
	 loss: 28.9078, MinusLogProbMetric: 28.9078, val_loss: 29.3887, val_MinusLogProbMetric: 29.3887

Epoch 304: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9078 - MinusLogProbMetric: 28.9078 - val_loss: 29.3887 - val_MinusLogProbMetric: 29.3887 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 305/1000
2023-10-26 08:42:15.981 
Epoch 305/1000 
	 loss: 28.9055, MinusLogProbMetric: 28.9055, val_loss: 29.6214, val_MinusLogProbMetric: 29.6214

Epoch 305: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9055 - MinusLogProbMetric: 28.9055 - val_loss: 29.6214 - val_MinusLogProbMetric: 29.6214 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 306/1000
2023-10-26 08:42:51.511 
Epoch 306/1000 
	 loss: 28.9593, MinusLogProbMetric: 28.9593, val_loss: 29.5299, val_MinusLogProbMetric: 29.5299

Epoch 306: val_loss did not improve from 29.08209
196/196 - 36s - loss: 28.9593 - MinusLogProbMetric: 28.9593 - val_loss: 29.5299 - val_MinusLogProbMetric: 29.5299 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 307/1000
2023-10-26 08:43:26.652 
Epoch 307/1000 
	 loss: 28.8723, MinusLogProbMetric: 28.8723, val_loss: 29.2812, val_MinusLogProbMetric: 29.2812

Epoch 307: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.8723 - MinusLogProbMetric: 28.8723 - val_loss: 29.2812 - val_MinusLogProbMetric: 29.2812 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 308/1000
2023-10-26 08:44:01.900 
Epoch 308/1000 
	 loss: 28.8807, MinusLogProbMetric: 28.8807, val_loss: 29.1773, val_MinusLogProbMetric: 29.1773

Epoch 308: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.8807 - MinusLogProbMetric: 28.8807 - val_loss: 29.1773 - val_MinusLogProbMetric: 29.1773 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 309/1000
2023-10-26 08:44:36.934 
Epoch 309/1000 
	 loss: 28.8051, MinusLogProbMetric: 28.8051, val_loss: 30.0292, val_MinusLogProbMetric: 30.0292

Epoch 309: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.8051 - MinusLogProbMetric: 28.8051 - val_loss: 30.0292 - val_MinusLogProbMetric: 30.0292 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 310/1000
2023-10-26 08:45:10.634 
Epoch 310/1000 
	 loss: 28.8573, MinusLogProbMetric: 28.8573, val_loss: 30.2533, val_MinusLogProbMetric: 30.2533

Epoch 310: val_loss did not improve from 29.08209
196/196 - 34s - loss: 28.8573 - MinusLogProbMetric: 28.8573 - val_loss: 30.2533 - val_MinusLogProbMetric: 30.2533 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 311/1000
2023-10-26 08:45:45.863 
Epoch 311/1000 
	 loss: 28.9506, MinusLogProbMetric: 28.9506, val_loss: 29.2677, val_MinusLogProbMetric: 29.2677

Epoch 311: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9506 - MinusLogProbMetric: 28.9506 - val_loss: 29.2677 - val_MinusLogProbMetric: 29.2677 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 312/1000
2023-10-26 08:46:20.430 
Epoch 312/1000 
	 loss: 28.9247, MinusLogProbMetric: 28.9247, val_loss: 29.1432, val_MinusLogProbMetric: 29.1432

Epoch 312: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9247 - MinusLogProbMetric: 28.9247 - val_loss: 29.1432 - val_MinusLogProbMetric: 29.1432 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 313/1000
2023-10-26 08:46:55.426 
Epoch 313/1000 
	 loss: 28.9774, MinusLogProbMetric: 28.9774, val_loss: 29.8872, val_MinusLogProbMetric: 29.8872

Epoch 313: val_loss did not improve from 29.08209
196/196 - 35s - loss: 28.9774 - MinusLogProbMetric: 28.9774 - val_loss: 29.8872 - val_MinusLogProbMetric: 29.8872 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 314/1000
2023-10-26 08:47:30.770 
Epoch 314/1000 
	 loss: 28.9002, MinusLogProbMetric: 28.9002, val_loss: 29.0685, val_MinusLogProbMetric: 29.0685

Epoch 314: val_loss improved from 29.08209 to 29.06847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 28.9002 - MinusLogProbMetric: 28.9002 - val_loss: 29.0685 - val_MinusLogProbMetric: 29.0685 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 315/1000
2023-10-26 08:48:06.837 
Epoch 315/1000 
	 loss: 28.7836, MinusLogProbMetric: 28.7836, val_loss: 29.3884, val_MinusLogProbMetric: 29.3884

Epoch 315: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.7836 - MinusLogProbMetric: 28.7836 - val_loss: 29.3884 - val_MinusLogProbMetric: 29.3884 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 316/1000
2023-10-26 08:48:41.995 
Epoch 316/1000 
	 loss: 28.9140, MinusLogProbMetric: 28.9140, val_loss: 29.5482, val_MinusLogProbMetric: 29.5482

Epoch 316: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.9140 - MinusLogProbMetric: 28.9140 - val_loss: 29.5482 - val_MinusLogProbMetric: 29.5482 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 317/1000
2023-10-26 08:49:16.699 
Epoch 317/1000 
	 loss: 28.8404, MinusLogProbMetric: 28.8404, val_loss: 29.7354, val_MinusLogProbMetric: 29.7354

Epoch 317: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.8404 - MinusLogProbMetric: 28.8404 - val_loss: 29.7354 - val_MinusLogProbMetric: 29.7354 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 318/1000
2023-10-26 08:49:50.321 
Epoch 318/1000 
	 loss: 28.7567, MinusLogProbMetric: 28.7567, val_loss: 29.5471, val_MinusLogProbMetric: 29.5471

Epoch 318: val_loss did not improve from 29.06847
196/196 - 34s - loss: 28.7567 - MinusLogProbMetric: 28.7567 - val_loss: 29.5471 - val_MinusLogProbMetric: 29.5471 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 319/1000
2023-10-26 08:50:25.709 
Epoch 319/1000 
	 loss: 28.9258, MinusLogProbMetric: 28.9258, val_loss: 29.3857, val_MinusLogProbMetric: 29.3857

Epoch 319: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.9258 - MinusLogProbMetric: 28.9258 - val_loss: 29.3857 - val_MinusLogProbMetric: 29.3857 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 320/1000
2023-10-26 08:51:00.796 
Epoch 320/1000 
	 loss: 28.8131, MinusLogProbMetric: 28.8131, val_loss: 29.1540, val_MinusLogProbMetric: 29.1540

Epoch 320: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.8131 - MinusLogProbMetric: 28.8131 - val_loss: 29.1540 - val_MinusLogProbMetric: 29.1540 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 321/1000
2023-10-26 08:51:36.300 
Epoch 321/1000 
	 loss: 28.7636, MinusLogProbMetric: 28.7636, val_loss: 29.2884, val_MinusLogProbMetric: 29.2884

Epoch 321: val_loss did not improve from 29.06847
196/196 - 36s - loss: 28.7636 - MinusLogProbMetric: 28.7636 - val_loss: 29.2884 - val_MinusLogProbMetric: 29.2884 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 322/1000
2023-10-26 08:52:11.614 
Epoch 322/1000 
	 loss: 28.8202, MinusLogProbMetric: 28.8202, val_loss: 29.3258, val_MinusLogProbMetric: 29.3258

Epoch 322: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.8202 - MinusLogProbMetric: 28.8202 - val_loss: 29.3258 - val_MinusLogProbMetric: 29.3258 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 323/1000
2023-10-26 08:52:46.771 
Epoch 323/1000 
	 loss: 28.8944, MinusLogProbMetric: 28.8944, val_loss: 29.4441, val_MinusLogProbMetric: 29.4441

Epoch 323: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.8944 - MinusLogProbMetric: 28.8944 - val_loss: 29.4441 - val_MinusLogProbMetric: 29.4441 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 324/1000
2023-10-26 08:53:22.375 
Epoch 324/1000 
	 loss: 28.7404, MinusLogProbMetric: 28.7404, val_loss: 29.3365, val_MinusLogProbMetric: 29.3365

Epoch 324: val_loss did not improve from 29.06847
196/196 - 36s - loss: 28.7404 - MinusLogProbMetric: 28.7404 - val_loss: 29.3365 - val_MinusLogProbMetric: 29.3365 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 325/1000
2023-10-26 08:53:57.882 
Epoch 325/1000 
	 loss: 28.8869, MinusLogProbMetric: 28.8869, val_loss: 30.7820, val_MinusLogProbMetric: 30.7820

Epoch 325: val_loss did not improve from 29.06847
196/196 - 36s - loss: 28.8869 - MinusLogProbMetric: 28.8869 - val_loss: 30.7820 - val_MinusLogProbMetric: 30.7820 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 326/1000
2023-10-26 08:54:33.421 
Epoch 326/1000 
	 loss: 28.8655, MinusLogProbMetric: 28.8655, val_loss: 29.4064, val_MinusLogProbMetric: 29.4064

Epoch 326: val_loss did not improve from 29.06847
196/196 - 36s - loss: 28.8655 - MinusLogProbMetric: 28.8655 - val_loss: 29.4064 - val_MinusLogProbMetric: 29.4064 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 327/1000
2023-10-26 08:55:08.794 
Epoch 327/1000 
	 loss: 28.8343, MinusLogProbMetric: 28.8343, val_loss: 29.3729, val_MinusLogProbMetric: 29.3729

Epoch 327: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.8343 - MinusLogProbMetric: 28.8343 - val_loss: 29.3729 - val_MinusLogProbMetric: 29.3729 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 328/1000
2023-10-26 08:55:44.081 
Epoch 328/1000 
	 loss: 28.8677, MinusLogProbMetric: 28.8677, val_loss: 29.1716, val_MinusLogProbMetric: 29.1716

Epoch 328: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.8677 - MinusLogProbMetric: 28.8677 - val_loss: 29.1716 - val_MinusLogProbMetric: 29.1716 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 329/1000
2023-10-26 08:56:19.311 
Epoch 329/1000 
	 loss: 28.8102, MinusLogProbMetric: 28.8102, val_loss: 29.5804, val_MinusLogProbMetric: 29.5804

Epoch 329: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.8102 - MinusLogProbMetric: 28.8102 - val_loss: 29.5804 - val_MinusLogProbMetric: 29.5804 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 330/1000
2023-10-26 08:56:53.816 
Epoch 330/1000 
	 loss: 28.7870, MinusLogProbMetric: 28.7870, val_loss: 29.0923, val_MinusLogProbMetric: 29.0923

Epoch 330: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.7870 - MinusLogProbMetric: 28.7870 - val_loss: 29.0923 - val_MinusLogProbMetric: 29.0923 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 331/1000
2023-10-26 08:57:29.145 
Epoch 331/1000 
	 loss: 28.7768, MinusLogProbMetric: 28.7768, val_loss: 29.5991, val_MinusLogProbMetric: 29.5991

Epoch 331: val_loss did not improve from 29.06847
196/196 - 35s - loss: 28.7768 - MinusLogProbMetric: 28.7768 - val_loss: 29.5991 - val_MinusLogProbMetric: 29.5991 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 332/1000
2023-10-26 08:58:04.416 
Epoch 332/1000 
	 loss: 28.7354, MinusLogProbMetric: 28.7354, val_loss: 29.0556, val_MinusLogProbMetric: 29.0556

Epoch 332: val_loss improved from 29.06847 to 29.05556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 28.7354 - MinusLogProbMetric: 28.7354 - val_loss: 29.0556 - val_MinusLogProbMetric: 29.0556 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 333/1000
2023-10-26 08:58:40.125 
Epoch 333/1000 
	 loss: 28.8302, MinusLogProbMetric: 28.8302, val_loss: 29.7206, val_MinusLogProbMetric: 29.7206

Epoch 333: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.8302 - MinusLogProbMetric: 28.8302 - val_loss: 29.7206 - val_MinusLogProbMetric: 29.7206 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 334/1000
2023-10-26 08:59:15.257 
Epoch 334/1000 
	 loss: 28.8229, MinusLogProbMetric: 28.8229, val_loss: 29.4053, val_MinusLogProbMetric: 29.4053

Epoch 334: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.8229 - MinusLogProbMetric: 28.8229 - val_loss: 29.4053 - val_MinusLogProbMetric: 29.4053 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 335/1000
2023-10-26 08:59:50.848 
Epoch 335/1000 
	 loss: 28.7968, MinusLogProbMetric: 28.7968, val_loss: 29.3943, val_MinusLogProbMetric: 29.3943

Epoch 335: val_loss did not improve from 29.05556
196/196 - 36s - loss: 28.7968 - MinusLogProbMetric: 28.7968 - val_loss: 29.3943 - val_MinusLogProbMetric: 29.3943 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 336/1000
2023-10-26 09:00:25.687 
Epoch 336/1000 
	 loss: 28.7499, MinusLogProbMetric: 28.7499, val_loss: 29.1422, val_MinusLogProbMetric: 29.1422

Epoch 336: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.7499 - MinusLogProbMetric: 28.7499 - val_loss: 29.1422 - val_MinusLogProbMetric: 29.1422 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 337/1000
2023-10-26 09:01:01.040 
Epoch 337/1000 
	 loss: 28.7986, MinusLogProbMetric: 28.7986, val_loss: 29.1634, val_MinusLogProbMetric: 29.1634

Epoch 337: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.7986 - MinusLogProbMetric: 28.7986 - val_loss: 29.1634 - val_MinusLogProbMetric: 29.1634 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 338/1000
2023-10-26 09:01:36.064 
Epoch 338/1000 
	 loss: 28.8248, MinusLogProbMetric: 28.8248, val_loss: 29.3855, val_MinusLogProbMetric: 29.3855

Epoch 338: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.8248 - MinusLogProbMetric: 28.8248 - val_loss: 29.3855 - val_MinusLogProbMetric: 29.3855 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 339/1000
2023-10-26 09:02:11.740 
Epoch 339/1000 
	 loss: 28.6907, MinusLogProbMetric: 28.6907, val_loss: 29.6484, val_MinusLogProbMetric: 29.6484

Epoch 339: val_loss did not improve from 29.05556
196/196 - 36s - loss: 28.6907 - MinusLogProbMetric: 28.6907 - val_loss: 29.6484 - val_MinusLogProbMetric: 29.6484 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 340/1000
2023-10-26 09:02:46.759 
Epoch 340/1000 
	 loss: 28.8009, MinusLogProbMetric: 28.8009, val_loss: 29.0584, val_MinusLogProbMetric: 29.0584

Epoch 340: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.8009 - MinusLogProbMetric: 28.8009 - val_loss: 29.0584 - val_MinusLogProbMetric: 29.0584 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 341/1000
2023-10-26 09:03:20.787 
Epoch 341/1000 
	 loss: 28.8388, MinusLogProbMetric: 28.8388, val_loss: 29.4277, val_MinusLogProbMetric: 29.4277

Epoch 341: val_loss did not improve from 29.05556
196/196 - 34s - loss: 28.8388 - MinusLogProbMetric: 28.8388 - val_loss: 29.4277 - val_MinusLogProbMetric: 29.4277 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 342/1000
2023-10-26 09:03:56.033 
Epoch 342/1000 
	 loss: 28.6820, MinusLogProbMetric: 28.6820, val_loss: 29.3799, val_MinusLogProbMetric: 29.3799

Epoch 342: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.6820 - MinusLogProbMetric: 28.6820 - val_loss: 29.3799 - val_MinusLogProbMetric: 29.3799 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 343/1000
2023-10-26 09:04:31.461 
Epoch 343/1000 
	 loss: 28.6927, MinusLogProbMetric: 28.6927, val_loss: 29.6230, val_MinusLogProbMetric: 29.6230

Epoch 343: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.6927 - MinusLogProbMetric: 28.6927 - val_loss: 29.6230 - val_MinusLogProbMetric: 29.6230 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 344/1000
2023-10-26 09:05:06.810 
Epoch 344/1000 
	 loss: 28.6634, MinusLogProbMetric: 28.6634, val_loss: 29.1093, val_MinusLogProbMetric: 29.1093

Epoch 344: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.6634 - MinusLogProbMetric: 28.6634 - val_loss: 29.1093 - val_MinusLogProbMetric: 29.1093 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 345/1000
2023-10-26 09:05:41.503 
Epoch 345/1000 
	 loss: 28.7525, MinusLogProbMetric: 28.7525, val_loss: 29.2456, val_MinusLogProbMetric: 29.2456

Epoch 345: val_loss did not improve from 29.05556
196/196 - 35s - loss: 28.7525 - MinusLogProbMetric: 28.7525 - val_loss: 29.2456 - val_MinusLogProbMetric: 29.2456 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 346/1000
2023-10-26 09:06:16.695 
Epoch 346/1000 
	 loss: 28.7198, MinusLogProbMetric: 28.7198, val_loss: 28.8912, val_MinusLogProbMetric: 28.8912

Epoch 346: val_loss improved from 29.05556 to 28.89122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 28.7198 - MinusLogProbMetric: 28.7198 - val_loss: 28.8912 - val_MinusLogProbMetric: 28.8912 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 347/1000
2023-10-26 09:06:52.629 
Epoch 347/1000 
	 loss: 28.7164, MinusLogProbMetric: 28.7164, val_loss: 28.8377, val_MinusLogProbMetric: 28.8377

Epoch 347: val_loss improved from 28.89122 to 28.83774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 28.7164 - MinusLogProbMetric: 28.7164 - val_loss: 28.8377 - val_MinusLogProbMetric: 28.8377 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 348/1000
2023-10-26 09:07:28.439 
Epoch 348/1000 
	 loss: 28.7780, MinusLogProbMetric: 28.7780, val_loss: 29.5217, val_MinusLogProbMetric: 29.5217

Epoch 348: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.7780 - MinusLogProbMetric: 28.7780 - val_loss: 29.5217 - val_MinusLogProbMetric: 29.5217 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 349/1000
2023-10-26 09:08:03.356 
Epoch 349/1000 
	 loss: 28.6988, MinusLogProbMetric: 28.6988, val_loss: 29.4507, val_MinusLogProbMetric: 29.4507

Epoch 349: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6988 - MinusLogProbMetric: 28.6988 - val_loss: 29.4507 - val_MinusLogProbMetric: 29.4507 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 350/1000
2023-10-26 09:08:38.728 
Epoch 350/1000 
	 loss: 28.7252, MinusLogProbMetric: 28.7252, val_loss: 29.3652, val_MinusLogProbMetric: 29.3652

Epoch 350: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.7252 - MinusLogProbMetric: 28.7252 - val_loss: 29.3652 - val_MinusLogProbMetric: 29.3652 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 351/1000
2023-10-26 09:09:13.337 
Epoch 351/1000 
	 loss: 28.7087, MinusLogProbMetric: 28.7087, val_loss: 29.6966, val_MinusLogProbMetric: 29.6966

Epoch 351: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.7087 - MinusLogProbMetric: 28.7087 - val_loss: 29.6966 - val_MinusLogProbMetric: 29.6966 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 352/1000
2023-10-26 09:09:48.538 
Epoch 352/1000 
	 loss: 28.6815, MinusLogProbMetric: 28.6815, val_loss: 29.2577, val_MinusLogProbMetric: 29.2577

Epoch 352: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6815 - MinusLogProbMetric: 28.6815 - val_loss: 29.2577 - val_MinusLogProbMetric: 29.2577 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 353/1000
2023-10-26 09:10:24.093 
Epoch 353/1000 
	 loss: 28.6951, MinusLogProbMetric: 28.6951, val_loss: 29.2734, val_MinusLogProbMetric: 29.2734

Epoch 353: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.6951 - MinusLogProbMetric: 28.6951 - val_loss: 29.2734 - val_MinusLogProbMetric: 29.2734 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 354/1000
2023-10-26 09:10:59.986 
Epoch 354/1000 
	 loss: 28.6992, MinusLogProbMetric: 28.6992, val_loss: 29.0784, val_MinusLogProbMetric: 29.0784

Epoch 354: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.6992 - MinusLogProbMetric: 28.6992 - val_loss: 29.0784 - val_MinusLogProbMetric: 29.0784 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 355/1000
2023-10-26 09:11:35.297 
Epoch 355/1000 
	 loss: 28.6963, MinusLogProbMetric: 28.6963, val_loss: 29.1097, val_MinusLogProbMetric: 29.1097

Epoch 355: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6963 - MinusLogProbMetric: 28.6963 - val_loss: 29.1097 - val_MinusLogProbMetric: 29.1097 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 356/1000
2023-10-26 09:12:11.016 
Epoch 356/1000 
	 loss: 28.6419, MinusLogProbMetric: 28.6419, val_loss: 28.9707, val_MinusLogProbMetric: 28.9707

Epoch 356: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.6419 - MinusLogProbMetric: 28.6419 - val_loss: 28.9707 - val_MinusLogProbMetric: 28.9707 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 357/1000
2023-10-26 09:12:46.268 
Epoch 357/1000 
	 loss: 28.6611, MinusLogProbMetric: 28.6611, val_loss: 29.3341, val_MinusLogProbMetric: 29.3341

Epoch 357: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6611 - MinusLogProbMetric: 28.6611 - val_loss: 29.3341 - val_MinusLogProbMetric: 29.3341 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 358/1000
2023-10-26 09:13:21.423 
Epoch 358/1000 
	 loss: 28.6732, MinusLogProbMetric: 28.6732, val_loss: 29.8526, val_MinusLogProbMetric: 29.8526

Epoch 358: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6732 - MinusLogProbMetric: 28.6732 - val_loss: 29.8526 - val_MinusLogProbMetric: 29.8526 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 359/1000
2023-10-26 09:13:55.534 
Epoch 359/1000 
	 loss: 28.6741, MinusLogProbMetric: 28.6741, val_loss: 29.0069, val_MinusLogProbMetric: 29.0069

Epoch 359: val_loss did not improve from 28.83774
196/196 - 34s - loss: 28.6741 - MinusLogProbMetric: 28.6741 - val_loss: 29.0069 - val_MinusLogProbMetric: 29.0069 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 360/1000
2023-10-26 09:14:30.717 
Epoch 360/1000 
	 loss: 28.6742, MinusLogProbMetric: 28.6742, val_loss: 28.9935, val_MinusLogProbMetric: 28.9935

Epoch 360: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6742 - MinusLogProbMetric: 28.6742 - val_loss: 28.9935 - val_MinusLogProbMetric: 28.9935 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 361/1000
2023-10-26 09:15:06.301 
Epoch 361/1000 
	 loss: 28.6426, MinusLogProbMetric: 28.6426, val_loss: 28.9314, val_MinusLogProbMetric: 28.9314

Epoch 361: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.6426 - MinusLogProbMetric: 28.6426 - val_loss: 28.9314 - val_MinusLogProbMetric: 28.9314 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 362/1000
2023-10-26 09:15:41.497 
Epoch 362/1000 
	 loss: 28.6799, MinusLogProbMetric: 28.6799, val_loss: 28.9903, val_MinusLogProbMetric: 28.9903

Epoch 362: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6799 - MinusLogProbMetric: 28.6799 - val_loss: 28.9903 - val_MinusLogProbMetric: 28.9903 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 363/1000
2023-10-26 09:16:16.948 
Epoch 363/1000 
	 loss: 28.7092, MinusLogProbMetric: 28.7092, val_loss: 29.6846, val_MinusLogProbMetric: 29.6846

Epoch 363: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.7092 - MinusLogProbMetric: 28.7092 - val_loss: 29.6846 - val_MinusLogProbMetric: 29.6846 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 364/1000
2023-10-26 09:16:52.613 
Epoch 364/1000 
	 loss: 28.6424, MinusLogProbMetric: 28.6424, val_loss: 28.9653, val_MinusLogProbMetric: 28.9653

Epoch 364: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.6424 - MinusLogProbMetric: 28.6424 - val_loss: 28.9653 - val_MinusLogProbMetric: 28.9653 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 365/1000
2023-10-26 09:17:27.843 
Epoch 365/1000 
	 loss: 28.7078, MinusLogProbMetric: 28.7078, val_loss: 29.5041, val_MinusLogProbMetric: 29.5041

Epoch 365: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.7078 - MinusLogProbMetric: 28.7078 - val_loss: 29.5041 - val_MinusLogProbMetric: 29.5041 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 366/1000
2023-10-26 09:18:03.144 
Epoch 366/1000 
	 loss: 28.6468, MinusLogProbMetric: 28.6468, val_loss: 29.1632, val_MinusLogProbMetric: 29.1632

Epoch 366: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6468 - MinusLogProbMetric: 28.6468 - val_loss: 29.1632 - val_MinusLogProbMetric: 29.1632 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 367/1000
2023-10-26 09:18:38.492 
Epoch 367/1000 
	 loss: 28.5989, MinusLogProbMetric: 28.5989, val_loss: 29.7231, val_MinusLogProbMetric: 29.7231

Epoch 367: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5989 - MinusLogProbMetric: 28.5989 - val_loss: 29.7231 - val_MinusLogProbMetric: 29.7231 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 368/1000
2023-10-26 09:19:12.792 
Epoch 368/1000 
	 loss: 28.6443, MinusLogProbMetric: 28.6443, val_loss: 29.2995, val_MinusLogProbMetric: 29.2995

Epoch 368: val_loss did not improve from 28.83774
196/196 - 34s - loss: 28.6443 - MinusLogProbMetric: 28.6443 - val_loss: 29.2995 - val_MinusLogProbMetric: 29.2995 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 369/1000
2023-10-26 09:19:48.189 
Epoch 369/1000 
	 loss: 28.6923, MinusLogProbMetric: 28.6923, val_loss: 29.5033, val_MinusLogProbMetric: 29.5033

Epoch 369: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6923 - MinusLogProbMetric: 28.6923 - val_loss: 29.5033 - val_MinusLogProbMetric: 29.5033 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 370/1000
2023-10-26 09:20:23.815 
Epoch 370/1000 
	 loss: 28.5500, MinusLogProbMetric: 28.5500, val_loss: 29.4130, val_MinusLogProbMetric: 29.4130

Epoch 370: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.5500 - MinusLogProbMetric: 28.5500 - val_loss: 29.4130 - val_MinusLogProbMetric: 29.4130 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 371/1000
2023-10-26 09:20:59.285 
Epoch 371/1000 
	 loss: 28.7317, MinusLogProbMetric: 28.7317, val_loss: 29.4610, val_MinusLogProbMetric: 29.4610

Epoch 371: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.7317 - MinusLogProbMetric: 28.7317 - val_loss: 29.4610 - val_MinusLogProbMetric: 29.4610 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 372/1000
2023-10-26 09:21:34.303 
Epoch 372/1000 
	 loss: 28.6292, MinusLogProbMetric: 28.6292, val_loss: 29.0964, val_MinusLogProbMetric: 29.0964

Epoch 372: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6292 - MinusLogProbMetric: 28.6292 - val_loss: 29.0964 - val_MinusLogProbMetric: 29.0964 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 373/1000
2023-10-26 09:22:09.427 
Epoch 373/1000 
	 loss: 28.5663, MinusLogProbMetric: 28.5663, val_loss: 29.3031, val_MinusLogProbMetric: 29.3031

Epoch 373: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5663 - MinusLogProbMetric: 28.5663 - val_loss: 29.3031 - val_MinusLogProbMetric: 29.3031 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 374/1000
2023-10-26 09:22:44.748 
Epoch 374/1000 
	 loss: 28.6910, MinusLogProbMetric: 28.6910, val_loss: 29.3260, val_MinusLogProbMetric: 29.3260

Epoch 374: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6910 - MinusLogProbMetric: 28.6910 - val_loss: 29.3260 - val_MinusLogProbMetric: 29.3260 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 375/1000
2023-10-26 09:23:20.376 
Epoch 375/1000 
	 loss: 28.6371, MinusLogProbMetric: 28.6371, val_loss: 29.3671, val_MinusLogProbMetric: 29.3671

Epoch 375: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.6371 - MinusLogProbMetric: 28.6371 - val_loss: 29.3671 - val_MinusLogProbMetric: 29.3671 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 376/1000
2023-10-26 09:23:55.974 
Epoch 376/1000 
	 loss: 28.6464, MinusLogProbMetric: 28.6464, val_loss: 29.1897, val_MinusLogProbMetric: 29.1897

Epoch 376: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.6464 - MinusLogProbMetric: 28.6464 - val_loss: 29.1897 - val_MinusLogProbMetric: 29.1897 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 377/1000
2023-10-26 09:24:31.101 
Epoch 377/1000 
	 loss: 28.5500, MinusLogProbMetric: 28.5500, val_loss: 29.0775, val_MinusLogProbMetric: 29.0775

Epoch 377: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5500 - MinusLogProbMetric: 28.5500 - val_loss: 29.0775 - val_MinusLogProbMetric: 29.0775 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 378/1000
2023-10-26 09:25:05.854 
Epoch 378/1000 
	 loss: 28.6850, MinusLogProbMetric: 28.6850, val_loss: 29.2811, val_MinusLogProbMetric: 29.2811

Epoch 378: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6850 - MinusLogProbMetric: 28.6850 - val_loss: 29.2811 - val_MinusLogProbMetric: 29.2811 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 379/1000
2023-10-26 09:25:41.201 
Epoch 379/1000 
	 loss: 28.5270, MinusLogProbMetric: 28.5270, val_loss: 29.0677, val_MinusLogProbMetric: 29.0677

Epoch 379: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5270 - MinusLogProbMetric: 28.5270 - val_loss: 29.0677 - val_MinusLogProbMetric: 29.0677 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 380/1000
2023-10-26 09:26:16.436 
Epoch 380/1000 
	 loss: 28.6253, MinusLogProbMetric: 28.6253, val_loss: 29.0973, val_MinusLogProbMetric: 29.0973

Epoch 380: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6253 - MinusLogProbMetric: 28.6253 - val_loss: 29.0973 - val_MinusLogProbMetric: 29.0973 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 381/1000
2023-10-26 09:26:51.755 
Epoch 381/1000 
	 loss: 28.6600, MinusLogProbMetric: 28.6600, val_loss: 29.5607, val_MinusLogProbMetric: 29.5607

Epoch 381: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6600 - MinusLogProbMetric: 28.6600 - val_loss: 29.5607 - val_MinusLogProbMetric: 29.5607 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 382/1000
2023-10-26 09:27:27.385 
Epoch 382/1000 
	 loss: 28.5470, MinusLogProbMetric: 28.5470, val_loss: 29.7575, val_MinusLogProbMetric: 29.7575

Epoch 382: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.5470 - MinusLogProbMetric: 28.5470 - val_loss: 29.7575 - val_MinusLogProbMetric: 29.7575 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 383/1000
2023-10-26 09:28:03.192 
Epoch 383/1000 
	 loss: 28.5967, MinusLogProbMetric: 28.5967, val_loss: 29.5700, val_MinusLogProbMetric: 29.5700

Epoch 383: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.5967 - MinusLogProbMetric: 28.5967 - val_loss: 29.5700 - val_MinusLogProbMetric: 29.5700 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 384/1000
2023-10-26 09:28:38.906 
Epoch 384/1000 
	 loss: 28.7394, MinusLogProbMetric: 28.7394, val_loss: 29.0112, val_MinusLogProbMetric: 29.0112

Epoch 384: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.7394 - MinusLogProbMetric: 28.7394 - val_loss: 29.0112 - val_MinusLogProbMetric: 29.0112 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 385/1000
2023-10-26 09:29:14.283 
Epoch 385/1000 
	 loss: 28.5171, MinusLogProbMetric: 28.5171, val_loss: 29.0659, val_MinusLogProbMetric: 29.0659

Epoch 385: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5171 - MinusLogProbMetric: 28.5171 - val_loss: 29.0659 - val_MinusLogProbMetric: 29.0659 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 386/1000
2023-10-26 09:29:49.042 
Epoch 386/1000 
	 loss: 28.5678, MinusLogProbMetric: 28.5678, val_loss: 29.2735, val_MinusLogProbMetric: 29.2735

Epoch 386: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5678 - MinusLogProbMetric: 28.5678 - val_loss: 29.2735 - val_MinusLogProbMetric: 29.2735 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 387/1000
2023-10-26 09:30:24.894 
Epoch 387/1000 
	 loss: 28.5717, MinusLogProbMetric: 28.5717, val_loss: 29.0734, val_MinusLogProbMetric: 29.0734

Epoch 387: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.5717 - MinusLogProbMetric: 28.5717 - val_loss: 29.0734 - val_MinusLogProbMetric: 29.0734 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 388/1000
2023-10-26 09:31:00.718 
Epoch 388/1000 
	 loss: 28.5188, MinusLogProbMetric: 28.5188, val_loss: 29.3829, val_MinusLogProbMetric: 29.3829

Epoch 388: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.5188 - MinusLogProbMetric: 28.5188 - val_loss: 29.3829 - val_MinusLogProbMetric: 29.3829 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 389/1000
2023-10-26 09:31:36.079 
Epoch 389/1000 
	 loss: 28.5792, MinusLogProbMetric: 28.5792, val_loss: 29.2214, val_MinusLogProbMetric: 29.2214

Epoch 389: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5792 - MinusLogProbMetric: 28.5792 - val_loss: 29.2214 - val_MinusLogProbMetric: 29.2214 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 390/1000
2023-10-26 09:32:11.479 
Epoch 390/1000 
	 loss: 28.6599, MinusLogProbMetric: 28.6599, val_loss: 30.2567, val_MinusLogProbMetric: 30.2567

Epoch 390: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6599 - MinusLogProbMetric: 28.6599 - val_loss: 30.2567 - val_MinusLogProbMetric: 30.2567 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 391/1000
2023-10-26 09:32:46.712 
Epoch 391/1000 
	 loss: 28.5287, MinusLogProbMetric: 28.5287, val_loss: 29.0722, val_MinusLogProbMetric: 29.0722

Epoch 391: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5287 - MinusLogProbMetric: 28.5287 - val_loss: 29.0722 - val_MinusLogProbMetric: 29.0722 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 392/1000
2023-10-26 09:33:22.014 
Epoch 392/1000 
	 loss: 28.5158, MinusLogProbMetric: 28.5158, val_loss: 28.9586, val_MinusLogProbMetric: 28.9586

Epoch 392: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5158 - MinusLogProbMetric: 28.5158 - val_loss: 28.9586 - val_MinusLogProbMetric: 28.9586 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 393/1000
2023-10-26 09:33:57.622 
Epoch 393/1000 
	 loss: 28.5774, MinusLogProbMetric: 28.5774, val_loss: 29.1068, val_MinusLogProbMetric: 29.1068

Epoch 393: val_loss did not improve from 28.83774
196/196 - 36s - loss: 28.5774 - MinusLogProbMetric: 28.5774 - val_loss: 29.1068 - val_MinusLogProbMetric: 29.1068 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 394/1000
2023-10-26 09:34:32.794 
Epoch 394/1000 
	 loss: 28.5086, MinusLogProbMetric: 28.5086, val_loss: 29.5088, val_MinusLogProbMetric: 29.5088

Epoch 394: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5086 - MinusLogProbMetric: 28.5086 - val_loss: 29.5088 - val_MinusLogProbMetric: 29.5088 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 395/1000
2023-10-26 09:35:08.124 
Epoch 395/1000 
	 loss: 28.6080, MinusLogProbMetric: 28.6080, val_loss: 29.6886, val_MinusLogProbMetric: 29.6886

Epoch 395: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.6080 - MinusLogProbMetric: 28.6080 - val_loss: 29.6886 - val_MinusLogProbMetric: 29.6886 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 396/1000
2023-10-26 09:35:43.328 
Epoch 396/1000 
	 loss: 28.5554, MinusLogProbMetric: 28.5554, val_loss: 28.8732, val_MinusLogProbMetric: 28.8732

Epoch 396: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5554 - MinusLogProbMetric: 28.5554 - val_loss: 28.8732 - val_MinusLogProbMetric: 28.8732 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 397/1000
2023-10-26 09:36:18.534 
Epoch 397/1000 
	 loss: 28.5375, MinusLogProbMetric: 28.5375, val_loss: 29.4147, val_MinusLogProbMetric: 29.4147

Epoch 397: val_loss did not improve from 28.83774
196/196 - 35s - loss: 28.5375 - MinusLogProbMetric: 28.5375 - val_loss: 29.4147 - val_MinusLogProbMetric: 29.4147 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 398/1000
2023-10-26 09:36:54.087 
Epoch 398/1000 
	 loss: 27.9547, MinusLogProbMetric: 27.9547, val_loss: 28.7585, val_MinusLogProbMetric: 28.7585

Epoch 398: val_loss improved from 28.83774 to 28.75854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.9547 - MinusLogProbMetric: 27.9547 - val_loss: 28.7585 - val_MinusLogProbMetric: 28.7585 - lr: 1.6667e-04 - 36s/epoch - 185ms/step
Epoch 399/1000
2023-10-26 09:37:29.765 
Epoch 399/1000 
	 loss: 27.9452, MinusLogProbMetric: 27.9452, val_loss: 28.6099, val_MinusLogProbMetric: 28.6099

Epoch 399: val_loss improved from 28.75854 to 28.60991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.9452 - MinusLogProbMetric: 27.9452 - val_loss: 28.6099 - val_MinusLogProbMetric: 28.6099 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 400/1000
2023-10-26 09:38:05.222 
Epoch 400/1000 
	 loss: 27.9377, MinusLogProbMetric: 27.9377, val_loss: 28.5390, val_MinusLogProbMetric: 28.5390

Epoch 400: val_loss improved from 28.60991 to 28.53901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 35s - loss: 27.9377 - MinusLogProbMetric: 27.9377 - val_loss: 28.5390 - val_MinusLogProbMetric: 28.5390 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 401/1000
2023-10-26 09:38:40.456 
Epoch 401/1000 
	 loss: 27.9370, MinusLogProbMetric: 27.9370, val_loss: 28.6858, val_MinusLogProbMetric: 28.6858

Epoch 401: val_loss did not improve from 28.53901
196/196 - 35s - loss: 27.9370 - MinusLogProbMetric: 27.9370 - val_loss: 28.6858 - val_MinusLogProbMetric: 28.6858 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 402/1000
2023-10-26 09:39:15.552 
Epoch 402/1000 
	 loss: 27.9525, MinusLogProbMetric: 27.9525, val_loss: 29.3028, val_MinusLogProbMetric: 29.3028

Epoch 402: val_loss did not improve from 28.53901
196/196 - 35s - loss: 27.9525 - MinusLogProbMetric: 27.9525 - val_loss: 29.3028 - val_MinusLogProbMetric: 29.3028 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 403/1000
2023-10-26 09:39:50.457 
Epoch 403/1000 
	 loss: 27.9599, MinusLogProbMetric: 27.9599, val_loss: 28.6018, val_MinusLogProbMetric: 28.6018

Epoch 403: val_loss did not improve from 28.53901
196/196 - 35s - loss: 27.9599 - MinusLogProbMetric: 27.9599 - val_loss: 28.6018 - val_MinusLogProbMetric: 28.6018 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 404/1000
2023-10-26 09:40:25.711 
Epoch 404/1000 
	 loss: 27.9329, MinusLogProbMetric: 27.9329, val_loss: 28.6496, val_MinusLogProbMetric: 28.6496

Epoch 404: val_loss did not improve from 28.53901
196/196 - 35s - loss: 27.9329 - MinusLogProbMetric: 27.9329 - val_loss: 28.6496 - val_MinusLogProbMetric: 28.6496 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 405/1000
2023-10-26 09:41:00.821 
Epoch 405/1000 
	 loss: 27.9112, MinusLogProbMetric: 27.9112, val_loss: 28.7651, val_MinusLogProbMetric: 28.7651

Epoch 405: val_loss did not improve from 28.53901
196/196 - 35s - loss: 27.9112 - MinusLogProbMetric: 27.9112 - val_loss: 28.7651 - val_MinusLogProbMetric: 28.7651 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 406/1000
2023-10-26 09:41:36.476 
Epoch 406/1000 
	 loss: 27.9766, MinusLogProbMetric: 27.9766, val_loss: 28.6538, val_MinusLogProbMetric: 28.6538

Epoch 406: val_loss did not improve from 28.53901
196/196 - 36s - loss: 27.9766 - MinusLogProbMetric: 27.9766 - val_loss: 28.6538 - val_MinusLogProbMetric: 28.6538 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 407/1000
2023-10-26 09:42:11.852 
Epoch 407/1000 
	 loss: 27.9580, MinusLogProbMetric: 27.9580, val_loss: 28.5796, val_MinusLogProbMetric: 28.5796

Epoch 407: val_loss did not improve from 28.53901
196/196 - 35s - loss: 27.9580 - MinusLogProbMetric: 27.9580 - val_loss: 28.5796 - val_MinusLogProbMetric: 28.5796 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 408/1000
2023-10-26 09:42:46.959 
Epoch 408/1000 
	 loss: 27.9647, MinusLogProbMetric: 27.9647, val_loss: 28.4776, val_MinusLogProbMetric: 28.4776

Epoch 408: val_loss improved from 28.53901 to 28.47763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.9647 - MinusLogProbMetric: 27.9647 - val_loss: 28.4776 - val_MinusLogProbMetric: 28.4776 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 409/1000
2023-10-26 09:43:22.898 
Epoch 409/1000 
	 loss: 27.9433, MinusLogProbMetric: 27.9433, val_loss: 28.6078, val_MinusLogProbMetric: 28.6078

Epoch 409: val_loss did not improve from 28.47763
196/196 - 35s - loss: 27.9433 - MinusLogProbMetric: 27.9433 - val_loss: 28.6078 - val_MinusLogProbMetric: 28.6078 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 410/1000
2023-10-26 09:43:58.166 
Epoch 410/1000 
	 loss: 27.9074, MinusLogProbMetric: 27.9074, val_loss: 28.5972, val_MinusLogProbMetric: 28.5972

Epoch 410: val_loss did not improve from 28.47763
196/196 - 35s - loss: 27.9074 - MinusLogProbMetric: 27.9074 - val_loss: 28.5972 - val_MinusLogProbMetric: 28.5972 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 411/1000
2023-10-26 09:44:33.269 
Epoch 411/1000 
	 loss: 27.9900, MinusLogProbMetric: 27.9900, val_loss: 28.5458, val_MinusLogProbMetric: 28.5458

Epoch 411: val_loss did not improve from 28.47763
196/196 - 35s - loss: 27.9900 - MinusLogProbMetric: 27.9900 - val_loss: 28.5458 - val_MinusLogProbMetric: 28.5458 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 412/1000
2023-10-26 09:45:07.860 
Epoch 412/1000 
	 loss: 27.9528, MinusLogProbMetric: 27.9528, val_loss: 28.6104, val_MinusLogProbMetric: 28.6104

Epoch 412: val_loss did not improve from 28.47763
196/196 - 35s - loss: 27.9528 - MinusLogProbMetric: 27.9528 - val_loss: 28.6104 - val_MinusLogProbMetric: 28.6104 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 413/1000
2023-10-26 09:45:43.476 
Epoch 413/1000 
	 loss: 27.9297, MinusLogProbMetric: 27.9297, val_loss: 28.5308, val_MinusLogProbMetric: 28.5308

Epoch 413: val_loss did not improve from 28.47763
196/196 - 36s - loss: 27.9297 - MinusLogProbMetric: 27.9297 - val_loss: 28.5308 - val_MinusLogProbMetric: 28.5308 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 414/1000
2023-10-26 09:46:18.682 
Epoch 414/1000 
	 loss: 27.9599, MinusLogProbMetric: 27.9599, val_loss: 28.7531, val_MinusLogProbMetric: 28.7531

Epoch 414: val_loss did not improve from 28.47763
196/196 - 35s - loss: 27.9599 - MinusLogProbMetric: 27.9599 - val_loss: 28.7531 - val_MinusLogProbMetric: 28.7531 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 415/1000
2023-10-26 09:46:53.845 
Epoch 415/1000 
	 loss: 27.9288, MinusLogProbMetric: 27.9288, val_loss: 28.5544, val_MinusLogProbMetric: 28.5544

Epoch 415: val_loss did not improve from 28.47763
196/196 - 35s - loss: 27.9288 - MinusLogProbMetric: 27.9288 - val_loss: 28.5544 - val_MinusLogProbMetric: 28.5544 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 416/1000
2023-10-26 09:47:29.128 
Epoch 416/1000 
	 loss: 28.0241, MinusLogProbMetric: 28.0241, val_loss: 28.4587, val_MinusLogProbMetric: 28.4587

Epoch 416: val_loss improved from 28.47763 to 28.45865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 28.0241 - MinusLogProbMetric: 28.0241 - val_loss: 28.4587 - val_MinusLogProbMetric: 28.4587 - lr: 1.6667e-04 - 36s/epoch - 183ms/step
Epoch 417/1000
2023-10-26 09:48:03.788 
Epoch 417/1000 
	 loss: 27.8882, MinusLogProbMetric: 27.8882, val_loss: 28.5381, val_MinusLogProbMetric: 28.5381

Epoch 417: val_loss did not improve from 28.45865
196/196 - 34s - loss: 27.8882 - MinusLogProbMetric: 27.8882 - val_loss: 28.5381 - val_MinusLogProbMetric: 28.5381 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 418/1000
2023-10-26 09:48:38.354 
Epoch 418/1000 
	 loss: 27.9518, MinusLogProbMetric: 27.9518, val_loss: 28.5803, val_MinusLogProbMetric: 28.5803

Epoch 418: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9518 - MinusLogProbMetric: 27.9518 - val_loss: 28.5803 - val_MinusLogProbMetric: 28.5803 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 419/1000
2023-10-26 09:49:13.443 
Epoch 419/1000 
	 loss: 27.9317, MinusLogProbMetric: 27.9317, val_loss: 28.5705, val_MinusLogProbMetric: 28.5705

Epoch 419: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9317 - MinusLogProbMetric: 27.9317 - val_loss: 28.5705 - val_MinusLogProbMetric: 28.5705 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 420/1000
2023-10-26 09:49:48.916 
Epoch 420/1000 
	 loss: 27.9262, MinusLogProbMetric: 27.9262, val_loss: 28.5712, val_MinusLogProbMetric: 28.5712

Epoch 420: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9262 - MinusLogProbMetric: 27.9262 - val_loss: 28.5712 - val_MinusLogProbMetric: 28.5712 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 421/1000
2023-10-26 09:50:24.452 
Epoch 421/1000 
	 loss: 28.0075, MinusLogProbMetric: 28.0075, val_loss: 29.1795, val_MinusLogProbMetric: 29.1795

Epoch 421: val_loss did not improve from 28.45865
196/196 - 36s - loss: 28.0075 - MinusLogProbMetric: 28.0075 - val_loss: 29.1795 - val_MinusLogProbMetric: 29.1795 - lr: 1.6667e-04 - 36s/epoch - 181ms/step
Epoch 422/1000
2023-10-26 09:50:59.885 
Epoch 422/1000 
	 loss: 27.9236, MinusLogProbMetric: 27.9236, val_loss: 28.6848, val_MinusLogProbMetric: 28.6848

Epoch 422: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9236 - MinusLogProbMetric: 27.9236 - val_loss: 28.6848 - val_MinusLogProbMetric: 28.6848 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 423/1000
2023-10-26 09:51:35.065 
Epoch 423/1000 
	 loss: 27.8922, MinusLogProbMetric: 27.8922, val_loss: 28.5674, val_MinusLogProbMetric: 28.5674

Epoch 423: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.8922 - MinusLogProbMetric: 27.8922 - val_loss: 28.5674 - val_MinusLogProbMetric: 28.5674 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 424/1000
2023-10-26 09:52:10.619 
Epoch 424/1000 
	 loss: 27.9653, MinusLogProbMetric: 27.9653, val_loss: 28.5909, val_MinusLogProbMetric: 28.5909

Epoch 424: val_loss did not improve from 28.45865
196/196 - 36s - loss: 27.9653 - MinusLogProbMetric: 27.9653 - val_loss: 28.5909 - val_MinusLogProbMetric: 28.5909 - lr: 1.6667e-04 - 36s/epoch - 181ms/step
Epoch 425/1000
2023-10-26 09:52:45.728 
Epoch 425/1000 
	 loss: 27.9480, MinusLogProbMetric: 27.9480, val_loss: 28.6616, val_MinusLogProbMetric: 28.6616

Epoch 425: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9480 - MinusLogProbMetric: 27.9480 - val_loss: 28.6616 - val_MinusLogProbMetric: 28.6616 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 426/1000
2023-10-26 09:53:19.946 
Epoch 426/1000 
	 loss: 27.9908, MinusLogProbMetric: 27.9908, val_loss: 28.5800, val_MinusLogProbMetric: 28.5800

Epoch 426: val_loss did not improve from 28.45865
196/196 - 34s - loss: 27.9908 - MinusLogProbMetric: 27.9908 - val_loss: 28.5800 - val_MinusLogProbMetric: 28.5800 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 427/1000
2023-10-26 09:53:55.381 
Epoch 427/1000 
	 loss: 27.9044, MinusLogProbMetric: 27.9044, val_loss: 28.5397, val_MinusLogProbMetric: 28.5397

Epoch 427: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9044 - MinusLogProbMetric: 27.9044 - val_loss: 28.5397 - val_MinusLogProbMetric: 28.5397 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 428/1000
2023-10-26 09:54:30.211 
Epoch 428/1000 
	 loss: 27.9513, MinusLogProbMetric: 27.9513, val_loss: 28.4977, val_MinusLogProbMetric: 28.4977

Epoch 428: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9513 - MinusLogProbMetric: 27.9513 - val_loss: 28.4977 - val_MinusLogProbMetric: 28.4977 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 429/1000
2023-10-26 09:55:05.277 
Epoch 429/1000 
	 loss: 27.9583, MinusLogProbMetric: 27.9583, val_loss: 28.6632, val_MinusLogProbMetric: 28.6632

Epoch 429: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9583 - MinusLogProbMetric: 27.9583 - val_loss: 28.6632 - val_MinusLogProbMetric: 28.6632 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 430/1000
2023-10-26 09:55:40.633 
Epoch 430/1000 
	 loss: 27.9729, MinusLogProbMetric: 27.9729, val_loss: 28.5206, val_MinusLogProbMetric: 28.5206

Epoch 430: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9729 - MinusLogProbMetric: 27.9729 - val_loss: 28.5206 - val_MinusLogProbMetric: 28.5206 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 431/1000
2023-10-26 09:56:15.942 
Epoch 431/1000 
	 loss: 27.8976, MinusLogProbMetric: 27.8976, val_loss: 28.7393, val_MinusLogProbMetric: 28.7393

Epoch 431: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.8976 - MinusLogProbMetric: 27.8976 - val_loss: 28.7393 - val_MinusLogProbMetric: 28.7393 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 432/1000
2023-10-26 09:56:51.894 
Epoch 432/1000 
	 loss: 27.9529, MinusLogProbMetric: 27.9529, val_loss: 28.7823, val_MinusLogProbMetric: 28.7823

Epoch 432: val_loss did not improve from 28.45865
196/196 - 36s - loss: 27.9529 - MinusLogProbMetric: 27.9529 - val_loss: 28.7823 - val_MinusLogProbMetric: 28.7823 - lr: 1.6667e-04 - 36s/epoch - 183ms/step
Epoch 433/1000
2023-10-26 09:57:27.211 
Epoch 433/1000 
	 loss: 27.9263, MinusLogProbMetric: 27.9263, val_loss: 28.6913, val_MinusLogProbMetric: 28.6913

Epoch 433: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9263 - MinusLogProbMetric: 27.9263 - val_loss: 28.6913 - val_MinusLogProbMetric: 28.6913 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 434/1000
2023-10-26 09:58:02.349 
Epoch 434/1000 
	 loss: 27.9169, MinusLogProbMetric: 27.9169, val_loss: 28.6204, val_MinusLogProbMetric: 28.6204

Epoch 434: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9169 - MinusLogProbMetric: 27.9169 - val_loss: 28.6204 - val_MinusLogProbMetric: 28.6204 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 435/1000
2023-10-26 09:58:37.198 
Epoch 435/1000 
	 loss: 27.9485, MinusLogProbMetric: 27.9485, val_loss: 28.6569, val_MinusLogProbMetric: 28.6569

Epoch 435: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9485 - MinusLogProbMetric: 27.9485 - val_loss: 28.6569 - val_MinusLogProbMetric: 28.6569 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 436/1000
2023-10-26 09:59:12.507 
Epoch 436/1000 
	 loss: 27.9311, MinusLogProbMetric: 27.9311, val_loss: 28.5994, val_MinusLogProbMetric: 28.5994

Epoch 436: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9311 - MinusLogProbMetric: 27.9311 - val_loss: 28.5994 - val_MinusLogProbMetric: 28.5994 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 437/1000
2023-10-26 09:59:45.858 
Epoch 437/1000 
	 loss: 27.9204, MinusLogProbMetric: 27.9204, val_loss: 28.5517, val_MinusLogProbMetric: 28.5517

Epoch 437: val_loss did not improve from 28.45865
196/196 - 33s - loss: 27.9204 - MinusLogProbMetric: 27.9204 - val_loss: 28.5517 - val_MinusLogProbMetric: 28.5517 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 438/1000
2023-10-26 10:00:20.920 
Epoch 438/1000 
	 loss: 27.9381, MinusLogProbMetric: 27.9381, val_loss: 28.6056, val_MinusLogProbMetric: 28.6056

Epoch 438: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9381 - MinusLogProbMetric: 27.9381 - val_loss: 28.6056 - val_MinusLogProbMetric: 28.6056 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 439/1000
2023-10-26 10:00:52.624 
Epoch 439/1000 
	 loss: 27.9051, MinusLogProbMetric: 27.9051, val_loss: 28.6378, val_MinusLogProbMetric: 28.6378

Epoch 439: val_loss did not improve from 28.45865
196/196 - 32s - loss: 27.9051 - MinusLogProbMetric: 27.9051 - val_loss: 28.6378 - val_MinusLogProbMetric: 28.6378 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 440/1000
2023-10-26 10:01:23.253 
Epoch 440/1000 
	 loss: 27.9348, MinusLogProbMetric: 27.9348, val_loss: 28.6324, val_MinusLogProbMetric: 28.6324

Epoch 440: val_loss did not improve from 28.45865
196/196 - 31s - loss: 27.9348 - MinusLogProbMetric: 27.9348 - val_loss: 28.6324 - val_MinusLogProbMetric: 28.6324 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 441/1000
2023-10-26 10:01:55.036 
Epoch 441/1000 
	 loss: 27.9270, MinusLogProbMetric: 27.9270, val_loss: 28.4673, val_MinusLogProbMetric: 28.4673

Epoch 441: val_loss did not improve from 28.45865
196/196 - 32s - loss: 27.9270 - MinusLogProbMetric: 27.9270 - val_loss: 28.4673 - val_MinusLogProbMetric: 28.4673 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 442/1000
2023-10-26 10:02:28.265 
Epoch 442/1000 
	 loss: 27.9347, MinusLogProbMetric: 27.9347, val_loss: 28.4998, val_MinusLogProbMetric: 28.4998

Epoch 442: val_loss did not improve from 28.45865
196/196 - 33s - loss: 27.9347 - MinusLogProbMetric: 27.9347 - val_loss: 28.4998 - val_MinusLogProbMetric: 28.4998 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 443/1000
2023-10-26 10:03:03.857 
Epoch 443/1000 
	 loss: 27.9420, MinusLogProbMetric: 27.9420, val_loss: 28.7647, val_MinusLogProbMetric: 28.7647

Epoch 443: val_loss did not improve from 28.45865
196/196 - 36s - loss: 27.9420 - MinusLogProbMetric: 27.9420 - val_loss: 28.7647 - val_MinusLogProbMetric: 28.7647 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 444/1000
2023-10-26 10:03:39.188 
Epoch 444/1000 
	 loss: 27.9164, MinusLogProbMetric: 27.9164, val_loss: 29.3794, val_MinusLogProbMetric: 29.3794

Epoch 444: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9164 - MinusLogProbMetric: 27.9164 - val_loss: 29.3794 - val_MinusLogProbMetric: 29.3794 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 445/1000
2023-10-26 10:04:13.629 
Epoch 445/1000 
	 loss: 27.9487, MinusLogProbMetric: 27.9487, val_loss: 28.4797, val_MinusLogProbMetric: 28.4797

Epoch 445: val_loss did not improve from 28.45865
196/196 - 34s - loss: 27.9487 - MinusLogProbMetric: 27.9487 - val_loss: 28.4797 - val_MinusLogProbMetric: 28.4797 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 446/1000
2023-10-26 10:04:49.284 
Epoch 446/1000 
	 loss: 27.9119, MinusLogProbMetric: 27.9119, val_loss: 28.6681, val_MinusLogProbMetric: 28.6681

Epoch 446: val_loss did not improve from 28.45865
196/196 - 36s - loss: 27.9119 - MinusLogProbMetric: 27.9119 - val_loss: 28.6681 - val_MinusLogProbMetric: 28.6681 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 447/1000
2023-10-26 10:05:24.113 
Epoch 447/1000 
	 loss: 27.9181, MinusLogProbMetric: 27.9181, val_loss: 28.5474, val_MinusLogProbMetric: 28.5474

Epoch 447: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9181 - MinusLogProbMetric: 27.9181 - val_loss: 28.5474 - val_MinusLogProbMetric: 28.5474 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 448/1000
2023-10-26 10:05:59.486 
Epoch 448/1000 
	 loss: 27.8969, MinusLogProbMetric: 27.8969, val_loss: 28.6305, val_MinusLogProbMetric: 28.6305

Epoch 448: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.8969 - MinusLogProbMetric: 27.8969 - val_loss: 28.6305 - val_MinusLogProbMetric: 28.6305 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 449/1000
2023-10-26 10:06:34.086 
Epoch 449/1000 
	 loss: 27.9752, MinusLogProbMetric: 27.9752, val_loss: 28.8866, val_MinusLogProbMetric: 28.8866

Epoch 449: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9752 - MinusLogProbMetric: 27.9752 - val_loss: 28.8866 - val_MinusLogProbMetric: 28.8866 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 450/1000
2023-10-26 10:07:09.231 
Epoch 450/1000 
	 loss: 27.8909, MinusLogProbMetric: 27.8909, val_loss: 28.5855, val_MinusLogProbMetric: 28.5855

Epoch 450: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.8909 - MinusLogProbMetric: 27.8909 - val_loss: 28.5855 - val_MinusLogProbMetric: 28.5855 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 451/1000
2023-10-26 10:07:44.872 
Epoch 451/1000 
	 loss: 27.8788, MinusLogProbMetric: 27.8788, val_loss: 28.6608, val_MinusLogProbMetric: 28.6608

Epoch 451: val_loss did not improve from 28.45865
196/196 - 36s - loss: 27.8788 - MinusLogProbMetric: 27.8788 - val_loss: 28.6608 - val_MinusLogProbMetric: 28.6608 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 452/1000
2023-10-26 10:08:20.217 
Epoch 452/1000 
	 loss: 27.9148, MinusLogProbMetric: 27.9148, val_loss: 28.6339, val_MinusLogProbMetric: 28.6339

Epoch 452: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9148 - MinusLogProbMetric: 27.9148 - val_loss: 28.6339 - val_MinusLogProbMetric: 28.6339 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 453/1000
2023-10-26 10:08:55.396 
Epoch 453/1000 
	 loss: 27.8655, MinusLogProbMetric: 27.8655, val_loss: 28.5604, val_MinusLogProbMetric: 28.5604

Epoch 453: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.8655 - MinusLogProbMetric: 27.8655 - val_loss: 28.5604 - val_MinusLogProbMetric: 28.5604 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 454/1000
2023-10-26 10:09:30.054 
Epoch 454/1000 
	 loss: 27.8854, MinusLogProbMetric: 27.8854, val_loss: 28.6509, val_MinusLogProbMetric: 28.6509

Epoch 454: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.8854 - MinusLogProbMetric: 27.8854 - val_loss: 28.6509 - val_MinusLogProbMetric: 28.6509 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 455/1000
2023-10-26 10:10:05.019 
Epoch 455/1000 
	 loss: 27.9511, MinusLogProbMetric: 27.9511, val_loss: 28.9134, val_MinusLogProbMetric: 28.9134

Epoch 455: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9511 - MinusLogProbMetric: 27.9511 - val_loss: 28.9134 - val_MinusLogProbMetric: 28.9134 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 456/1000
2023-10-26 10:10:39.066 
Epoch 456/1000 
	 loss: 27.9087, MinusLogProbMetric: 27.9087, val_loss: 28.6590, val_MinusLogProbMetric: 28.6590

Epoch 456: val_loss did not improve from 28.45865
196/196 - 34s - loss: 27.9087 - MinusLogProbMetric: 27.9087 - val_loss: 28.6590 - val_MinusLogProbMetric: 28.6590 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 457/1000
2023-10-26 10:11:14.253 
Epoch 457/1000 
	 loss: 28.0006, MinusLogProbMetric: 28.0006, val_loss: 28.6035, val_MinusLogProbMetric: 28.6035

Epoch 457: val_loss did not improve from 28.45865
196/196 - 35s - loss: 28.0006 - MinusLogProbMetric: 28.0006 - val_loss: 28.6035 - val_MinusLogProbMetric: 28.6035 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 458/1000
2023-10-26 10:11:48.987 
Epoch 458/1000 
	 loss: 27.9121, MinusLogProbMetric: 27.9121, val_loss: 28.8573, val_MinusLogProbMetric: 28.8573

Epoch 458: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9121 - MinusLogProbMetric: 27.9121 - val_loss: 28.8573 - val_MinusLogProbMetric: 28.8573 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 459/1000
2023-10-26 10:12:24.283 
Epoch 459/1000 
	 loss: 27.9207, MinusLogProbMetric: 27.9207, val_loss: 28.6133, val_MinusLogProbMetric: 28.6133

Epoch 459: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9207 - MinusLogProbMetric: 27.9207 - val_loss: 28.6133 - val_MinusLogProbMetric: 28.6133 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 460/1000
2023-10-26 10:12:59.973 
Epoch 460/1000 
	 loss: 27.9419, MinusLogProbMetric: 27.9419, val_loss: 28.9068, val_MinusLogProbMetric: 28.9068

Epoch 460: val_loss did not improve from 28.45865
196/196 - 36s - loss: 27.9419 - MinusLogProbMetric: 27.9419 - val_loss: 28.9068 - val_MinusLogProbMetric: 28.9068 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 461/1000
2023-10-26 10:13:35.266 
Epoch 461/1000 
	 loss: 27.9090, MinusLogProbMetric: 27.9090, val_loss: 29.0245, val_MinusLogProbMetric: 29.0245

Epoch 461: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9090 - MinusLogProbMetric: 27.9090 - val_loss: 29.0245 - val_MinusLogProbMetric: 29.0245 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 462/1000
2023-10-26 10:14:09.634 
Epoch 462/1000 
	 loss: 27.9488, MinusLogProbMetric: 27.9488, val_loss: 29.0043, val_MinusLogProbMetric: 29.0043

Epoch 462: val_loss did not improve from 28.45865
196/196 - 34s - loss: 27.9488 - MinusLogProbMetric: 27.9488 - val_loss: 29.0043 - val_MinusLogProbMetric: 29.0043 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 463/1000
2023-10-26 10:14:44.977 
Epoch 463/1000 
	 loss: 27.9278, MinusLogProbMetric: 27.9278, val_loss: 28.5266, val_MinusLogProbMetric: 28.5266

Epoch 463: val_loss did not improve from 28.45865
196/196 - 35s - loss: 27.9278 - MinusLogProbMetric: 27.9278 - val_loss: 28.5266 - val_MinusLogProbMetric: 28.5266 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 464/1000
2023-10-26 10:15:20.350 
Epoch 464/1000 
	 loss: 27.8739, MinusLogProbMetric: 27.8739, val_loss: 28.4527, val_MinusLogProbMetric: 28.4527

Epoch 464: val_loss improved from 28.45865 to 28.45273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.8739 - MinusLogProbMetric: 27.8739 - val_loss: 28.4527 - val_MinusLogProbMetric: 28.4527 - lr: 1.6667e-04 - 36s/epoch - 183ms/step
Epoch 465/1000
2023-10-26 10:15:55.722 
Epoch 465/1000 
	 loss: 27.8986, MinusLogProbMetric: 27.8986, val_loss: 28.6899, val_MinusLogProbMetric: 28.6899

Epoch 465: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.8986 - MinusLogProbMetric: 27.8986 - val_loss: 28.6899 - val_MinusLogProbMetric: 28.6899 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 466/1000
2023-10-26 10:16:29.376 
Epoch 466/1000 
	 loss: 27.9612, MinusLogProbMetric: 27.9612, val_loss: 28.5840, val_MinusLogProbMetric: 28.5840

Epoch 466: val_loss did not improve from 28.45273
196/196 - 34s - loss: 27.9612 - MinusLogProbMetric: 27.9612 - val_loss: 28.5840 - val_MinusLogProbMetric: 28.5840 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 467/1000
2023-10-26 10:17:04.519 
Epoch 467/1000 
	 loss: 27.8714, MinusLogProbMetric: 27.8714, val_loss: 28.7335, val_MinusLogProbMetric: 28.7335

Epoch 467: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.8714 - MinusLogProbMetric: 27.8714 - val_loss: 28.7335 - val_MinusLogProbMetric: 28.7335 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 468/1000
2023-10-26 10:17:39.963 
Epoch 468/1000 
	 loss: 27.9127, MinusLogProbMetric: 27.9127, val_loss: 28.6074, val_MinusLogProbMetric: 28.6074

Epoch 468: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.9127 - MinusLogProbMetric: 27.9127 - val_loss: 28.6074 - val_MinusLogProbMetric: 28.6074 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 469/1000
2023-10-26 10:18:15.015 
Epoch 469/1000 
	 loss: 27.9020, MinusLogProbMetric: 27.9020, val_loss: 28.7787, val_MinusLogProbMetric: 28.7787

Epoch 469: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.9020 - MinusLogProbMetric: 27.9020 - val_loss: 28.7787 - val_MinusLogProbMetric: 28.7787 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 470/1000
2023-10-26 10:18:51.311 
Epoch 470/1000 
	 loss: 27.9121, MinusLogProbMetric: 27.9121, val_loss: 28.9649, val_MinusLogProbMetric: 28.9649

Epoch 470: val_loss did not improve from 28.45273
196/196 - 36s - loss: 27.9121 - MinusLogProbMetric: 27.9121 - val_loss: 28.9649 - val_MinusLogProbMetric: 28.9649 - lr: 1.6667e-04 - 36s/epoch - 185ms/step
Epoch 471/1000
2023-10-26 10:19:30.462 
Epoch 471/1000 
	 loss: 27.8736, MinusLogProbMetric: 27.8736, val_loss: 28.6482, val_MinusLogProbMetric: 28.6482

Epoch 471: val_loss did not improve from 28.45273
196/196 - 39s - loss: 27.8736 - MinusLogProbMetric: 27.8736 - val_loss: 28.6482 - val_MinusLogProbMetric: 28.6482 - lr: 1.6667e-04 - 39s/epoch - 200ms/step
Epoch 472/1000
2023-10-26 10:20:05.870 
Epoch 472/1000 
	 loss: 27.8830, MinusLogProbMetric: 27.8830, val_loss: 28.6837, val_MinusLogProbMetric: 28.6837

Epoch 472: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.8830 - MinusLogProbMetric: 27.8830 - val_loss: 28.6837 - val_MinusLogProbMetric: 28.6837 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 473/1000
2023-10-26 10:20:41.178 
Epoch 473/1000 
	 loss: 27.9739, MinusLogProbMetric: 27.9739, val_loss: 28.7362, val_MinusLogProbMetric: 28.7362

Epoch 473: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.9739 - MinusLogProbMetric: 27.9739 - val_loss: 28.7362 - val_MinusLogProbMetric: 28.7362 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 474/1000
2023-10-26 10:21:16.032 
Epoch 474/1000 
	 loss: 27.8793, MinusLogProbMetric: 27.8793, val_loss: 28.6390, val_MinusLogProbMetric: 28.6390

Epoch 474: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.8793 - MinusLogProbMetric: 27.8793 - val_loss: 28.6390 - val_MinusLogProbMetric: 28.6390 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 475/1000
2023-10-26 10:21:51.401 
Epoch 475/1000 
	 loss: 27.8625, MinusLogProbMetric: 27.8625, val_loss: 28.5116, val_MinusLogProbMetric: 28.5116

Epoch 475: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.8625 - MinusLogProbMetric: 27.8625 - val_loss: 28.5116 - val_MinusLogProbMetric: 28.5116 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 476/1000
2023-10-26 10:22:26.960 
Epoch 476/1000 
	 loss: 27.9337, MinusLogProbMetric: 27.9337, val_loss: 28.6661, val_MinusLogProbMetric: 28.6661

Epoch 476: val_loss did not improve from 28.45273
196/196 - 36s - loss: 27.9337 - MinusLogProbMetric: 27.9337 - val_loss: 28.6661 - val_MinusLogProbMetric: 28.6661 - lr: 1.6667e-04 - 36s/epoch - 181ms/step
Epoch 477/1000
2023-10-26 10:23:02.031 
Epoch 477/1000 
	 loss: 27.9636, MinusLogProbMetric: 27.9636, val_loss: 28.6443, val_MinusLogProbMetric: 28.6443

Epoch 477: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.9636 - MinusLogProbMetric: 27.9636 - val_loss: 28.6443 - val_MinusLogProbMetric: 28.6443 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 478/1000
2023-10-26 10:23:37.055 
Epoch 478/1000 
	 loss: 27.8693, MinusLogProbMetric: 27.8693, val_loss: 28.5384, val_MinusLogProbMetric: 28.5384

Epoch 478: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.8693 - MinusLogProbMetric: 27.8693 - val_loss: 28.5384 - val_MinusLogProbMetric: 28.5384 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 479/1000
2023-10-26 10:24:12.104 
Epoch 479/1000 
	 loss: 27.8975, MinusLogProbMetric: 27.8975, val_loss: 28.7283, val_MinusLogProbMetric: 28.7283

Epoch 479: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.8975 - MinusLogProbMetric: 27.8975 - val_loss: 28.7283 - val_MinusLogProbMetric: 28.7283 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 480/1000
2023-10-26 10:24:46.767 
Epoch 480/1000 
	 loss: 27.8663, MinusLogProbMetric: 27.8663, val_loss: 28.7285, val_MinusLogProbMetric: 28.7285

Epoch 480: val_loss did not improve from 28.45273
196/196 - 35s - loss: 27.8663 - MinusLogProbMetric: 27.8663 - val_loss: 28.7285 - val_MinusLogProbMetric: 28.7285 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 481/1000
2023-10-26 10:25:22.245 
Epoch 481/1000 
	 loss: 27.9002, MinusLogProbMetric: 27.9002, val_loss: 28.4421, val_MinusLogProbMetric: 28.4421

Epoch 481: val_loss improved from 28.45273 to 28.44206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.9002 - MinusLogProbMetric: 27.9002 - val_loss: 28.4421 - val_MinusLogProbMetric: 28.4421 - lr: 1.6667e-04 - 36s/epoch - 184ms/step
Epoch 482/1000
2023-10-26 10:25:58.418 
Epoch 482/1000 
	 loss: 27.8778, MinusLogProbMetric: 27.8778, val_loss: 28.6217, val_MinusLogProbMetric: 28.6217

Epoch 482: val_loss did not improve from 28.44206
196/196 - 36s - loss: 27.8778 - MinusLogProbMetric: 27.8778 - val_loss: 28.6217 - val_MinusLogProbMetric: 28.6217 - lr: 1.6667e-04 - 36s/epoch - 181ms/step
Epoch 483/1000
2023-10-26 10:26:32.654 
Epoch 483/1000 
	 loss: 27.9320, MinusLogProbMetric: 27.9320, val_loss: 28.6010, val_MinusLogProbMetric: 28.6010

Epoch 483: val_loss did not improve from 28.44206
196/196 - 34s - loss: 27.9320 - MinusLogProbMetric: 27.9320 - val_loss: 28.6010 - val_MinusLogProbMetric: 28.6010 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 484/1000
2023-10-26 10:27:07.907 
Epoch 484/1000 
	 loss: 27.8625, MinusLogProbMetric: 27.8625, val_loss: 28.5418, val_MinusLogProbMetric: 28.5418

Epoch 484: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8625 - MinusLogProbMetric: 27.8625 - val_loss: 28.5418 - val_MinusLogProbMetric: 28.5418 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 485/1000
2023-10-26 10:27:43.206 
Epoch 485/1000 
	 loss: 27.9146, MinusLogProbMetric: 27.9146, val_loss: 28.7517, val_MinusLogProbMetric: 28.7517

Epoch 485: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9146 - MinusLogProbMetric: 27.9146 - val_loss: 28.7517 - val_MinusLogProbMetric: 28.7517 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 486/1000
2023-10-26 10:28:18.338 
Epoch 486/1000 
	 loss: 27.8576, MinusLogProbMetric: 27.8576, val_loss: 28.7415, val_MinusLogProbMetric: 28.7415

Epoch 486: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8576 - MinusLogProbMetric: 27.8576 - val_loss: 28.7415 - val_MinusLogProbMetric: 28.7415 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 487/1000
2023-10-26 10:28:53.230 
Epoch 487/1000 
	 loss: 27.9038, MinusLogProbMetric: 27.9038, val_loss: 28.7826, val_MinusLogProbMetric: 28.7826

Epoch 487: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9038 - MinusLogProbMetric: 27.9038 - val_loss: 28.7826 - val_MinusLogProbMetric: 28.7826 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 488/1000
2023-10-26 10:29:28.013 
Epoch 488/1000 
	 loss: 27.8868, MinusLogProbMetric: 27.8868, val_loss: 28.4504, val_MinusLogProbMetric: 28.4504

Epoch 488: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8868 - MinusLogProbMetric: 27.8868 - val_loss: 28.4504 - val_MinusLogProbMetric: 28.4504 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 489/1000
2023-10-26 10:30:03.370 
Epoch 489/1000 
	 loss: 27.8752, MinusLogProbMetric: 27.8752, val_loss: 28.8518, val_MinusLogProbMetric: 28.8518

Epoch 489: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8752 - MinusLogProbMetric: 27.8752 - val_loss: 28.8518 - val_MinusLogProbMetric: 28.8518 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 490/1000
2023-10-26 10:30:38.571 
Epoch 490/1000 
	 loss: 27.8860, MinusLogProbMetric: 27.8860, val_loss: 28.7652, val_MinusLogProbMetric: 28.7652

Epoch 490: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8860 - MinusLogProbMetric: 27.8860 - val_loss: 28.7652 - val_MinusLogProbMetric: 28.7652 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 491/1000
2023-10-26 10:31:13.696 
Epoch 491/1000 
	 loss: 27.9065, MinusLogProbMetric: 27.9065, val_loss: 29.4036, val_MinusLogProbMetric: 29.4036

Epoch 491: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9065 - MinusLogProbMetric: 27.9065 - val_loss: 29.4036 - val_MinusLogProbMetric: 29.4036 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 492/1000
2023-10-26 10:31:49.308 
Epoch 492/1000 
	 loss: 27.8580, MinusLogProbMetric: 27.8580, val_loss: 28.6154, val_MinusLogProbMetric: 28.6154

Epoch 492: val_loss did not improve from 28.44206
196/196 - 36s - loss: 27.8580 - MinusLogProbMetric: 27.8580 - val_loss: 28.6154 - val_MinusLogProbMetric: 28.6154 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 493/1000
2023-10-26 10:32:24.609 
Epoch 493/1000 
	 loss: 27.8289, MinusLogProbMetric: 27.8289, val_loss: 28.5376, val_MinusLogProbMetric: 28.5376

Epoch 493: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8289 - MinusLogProbMetric: 27.8289 - val_loss: 28.5376 - val_MinusLogProbMetric: 28.5376 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 494/1000
2023-10-26 10:33:00.201 
Epoch 494/1000 
	 loss: 27.8589, MinusLogProbMetric: 27.8589, val_loss: 28.5848, val_MinusLogProbMetric: 28.5848

Epoch 494: val_loss did not improve from 28.44206
196/196 - 36s - loss: 27.8589 - MinusLogProbMetric: 27.8589 - val_loss: 28.5848 - val_MinusLogProbMetric: 28.5848 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 495/1000
2023-10-26 10:33:35.560 
Epoch 495/1000 
	 loss: 27.8727, MinusLogProbMetric: 27.8727, val_loss: 28.7671, val_MinusLogProbMetric: 28.7671

Epoch 495: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8727 - MinusLogProbMetric: 27.8727 - val_loss: 28.7671 - val_MinusLogProbMetric: 28.7671 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 496/1000
2023-10-26 10:34:10.786 
Epoch 496/1000 
	 loss: 27.8712, MinusLogProbMetric: 27.8712, val_loss: 28.7181, val_MinusLogProbMetric: 28.7181

Epoch 496: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8712 - MinusLogProbMetric: 27.8712 - val_loss: 28.7181 - val_MinusLogProbMetric: 28.7181 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 497/1000
2023-10-26 10:34:46.484 
Epoch 497/1000 
	 loss: 27.8258, MinusLogProbMetric: 27.8258, val_loss: 28.9716, val_MinusLogProbMetric: 28.9716

Epoch 497: val_loss did not improve from 28.44206
196/196 - 36s - loss: 27.8258 - MinusLogProbMetric: 27.8258 - val_loss: 28.9716 - val_MinusLogProbMetric: 28.9716 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 498/1000
2023-10-26 10:35:21.846 
Epoch 498/1000 
	 loss: 27.9255, MinusLogProbMetric: 27.9255, val_loss: 28.5756, val_MinusLogProbMetric: 28.5756

Epoch 498: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9255 - MinusLogProbMetric: 27.9255 - val_loss: 28.5756 - val_MinusLogProbMetric: 28.5756 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 499/1000
2023-10-26 10:35:57.279 
Epoch 499/1000 
	 loss: 27.8502, MinusLogProbMetric: 27.8502, val_loss: 28.8268, val_MinusLogProbMetric: 28.8268

Epoch 499: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8502 - MinusLogProbMetric: 27.8502 - val_loss: 28.8268 - val_MinusLogProbMetric: 28.8268 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 500/1000
2023-10-26 10:36:32.700 
Epoch 500/1000 
	 loss: 27.9032, MinusLogProbMetric: 27.9032, val_loss: 28.7736, val_MinusLogProbMetric: 28.7736

Epoch 500: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9032 - MinusLogProbMetric: 27.9032 - val_loss: 28.7736 - val_MinusLogProbMetric: 28.7736 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 501/1000
2023-10-26 10:37:08.084 
Epoch 501/1000 
	 loss: 27.8850, MinusLogProbMetric: 27.8850, val_loss: 29.1527, val_MinusLogProbMetric: 29.1527

Epoch 501: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8850 - MinusLogProbMetric: 27.8850 - val_loss: 29.1527 - val_MinusLogProbMetric: 29.1527 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 502/1000
2023-10-26 10:37:42.878 
Epoch 502/1000 
	 loss: 27.8999, MinusLogProbMetric: 27.8999, val_loss: 28.7012, val_MinusLogProbMetric: 28.7012

Epoch 502: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8999 - MinusLogProbMetric: 27.8999 - val_loss: 28.7012 - val_MinusLogProbMetric: 28.7012 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 503/1000
2023-10-26 10:38:18.302 
Epoch 503/1000 
	 loss: 27.8956, MinusLogProbMetric: 27.8956, val_loss: 28.4468, val_MinusLogProbMetric: 28.4468

Epoch 503: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8956 - MinusLogProbMetric: 27.8956 - val_loss: 28.4468 - val_MinusLogProbMetric: 28.4468 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 504/1000
2023-10-26 10:38:53.278 
Epoch 504/1000 
	 loss: 27.8251, MinusLogProbMetric: 27.8251, val_loss: 28.4422, val_MinusLogProbMetric: 28.4422

Epoch 504: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8251 - MinusLogProbMetric: 27.8251 - val_loss: 28.4422 - val_MinusLogProbMetric: 28.4422 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 505/1000
2023-10-26 10:39:28.063 
Epoch 505/1000 
	 loss: 27.8918, MinusLogProbMetric: 27.8918, val_loss: 28.8585, val_MinusLogProbMetric: 28.8585

Epoch 505: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8918 - MinusLogProbMetric: 27.8918 - val_loss: 28.8585 - val_MinusLogProbMetric: 28.8585 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 506/1000
2023-10-26 10:40:03.221 
Epoch 506/1000 
	 loss: 27.8365, MinusLogProbMetric: 27.8365, val_loss: 28.7248, val_MinusLogProbMetric: 28.7248

Epoch 506: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8365 - MinusLogProbMetric: 27.8365 - val_loss: 28.7248 - val_MinusLogProbMetric: 28.7248 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 507/1000
2023-10-26 10:40:39.032 
Epoch 507/1000 
	 loss: 27.8891, MinusLogProbMetric: 27.8891, val_loss: 28.5076, val_MinusLogProbMetric: 28.5076

Epoch 507: val_loss did not improve from 28.44206
196/196 - 36s - loss: 27.8891 - MinusLogProbMetric: 27.8891 - val_loss: 28.5076 - val_MinusLogProbMetric: 28.5076 - lr: 1.6667e-04 - 36s/epoch - 183ms/step
Epoch 508/1000
2023-10-26 10:41:14.420 
Epoch 508/1000 
	 loss: 27.8925, MinusLogProbMetric: 27.8925, val_loss: 28.5411, val_MinusLogProbMetric: 28.5411

Epoch 508: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8925 - MinusLogProbMetric: 27.8925 - val_loss: 28.5411 - val_MinusLogProbMetric: 28.5411 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 509/1000
2023-10-26 10:41:49.600 
Epoch 509/1000 
	 loss: 27.9021, MinusLogProbMetric: 27.9021, val_loss: 28.9179, val_MinusLogProbMetric: 28.9179

Epoch 509: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9021 - MinusLogProbMetric: 27.9021 - val_loss: 28.9179 - val_MinusLogProbMetric: 28.9179 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 510/1000
2023-10-26 10:42:24.965 
Epoch 510/1000 
	 loss: 27.9137, MinusLogProbMetric: 27.9137, val_loss: 28.8327, val_MinusLogProbMetric: 28.8327

Epoch 510: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9137 - MinusLogProbMetric: 27.9137 - val_loss: 28.8327 - val_MinusLogProbMetric: 28.8327 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 511/1000
2023-10-26 10:43:00.167 
Epoch 511/1000 
	 loss: 27.9162, MinusLogProbMetric: 27.9162, val_loss: 28.5360, val_MinusLogProbMetric: 28.5360

Epoch 511: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9162 - MinusLogProbMetric: 27.9162 - val_loss: 28.5360 - val_MinusLogProbMetric: 28.5360 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 512/1000
2023-10-26 10:43:35.193 
Epoch 512/1000 
	 loss: 27.8585, MinusLogProbMetric: 27.8585, val_loss: 28.5372, val_MinusLogProbMetric: 28.5372

Epoch 512: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8585 - MinusLogProbMetric: 27.8585 - val_loss: 28.5372 - val_MinusLogProbMetric: 28.5372 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 513/1000
2023-10-26 10:44:09.717 
Epoch 513/1000 
	 loss: 27.8535, MinusLogProbMetric: 27.8535, val_loss: 28.6072, val_MinusLogProbMetric: 28.6072

Epoch 513: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8535 - MinusLogProbMetric: 27.8535 - val_loss: 28.6072 - val_MinusLogProbMetric: 28.6072 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 514/1000
2023-10-26 10:44:44.747 
Epoch 514/1000 
	 loss: 27.8726, MinusLogProbMetric: 27.8726, val_loss: 29.2165, val_MinusLogProbMetric: 29.2165

Epoch 514: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8726 - MinusLogProbMetric: 27.8726 - val_loss: 29.2165 - val_MinusLogProbMetric: 29.2165 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 515/1000
2023-10-26 10:45:20.163 
Epoch 515/1000 
	 loss: 27.9249, MinusLogProbMetric: 27.9249, val_loss: 28.6625, val_MinusLogProbMetric: 28.6625

Epoch 515: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9249 - MinusLogProbMetric: 27.9249 - val_loss: 28.6625 - val_MinusLogProbMetric: 28.6625 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 516/1000
2023-10-26 10:45:55.044 
Epoch 516/1000 
	 loss: 27.8475, MinusLogProbMetric: 27.8475, val_loss: 28.7640, val_MinusLogProbMetric: 28.7640

Epoch 516: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8475 - MinusLogProbMetric: 27.8475 - val_loss: 28.7640 - val_MinusLogProbMetric: 28.7640 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 517/1000
2023-10-26 10:46:30.366 
Epoch 517/1000 
	 loss: 27.8546, MinusLogProbMetric: 27.8546, val_loss: 28.4945, val_MinusLogProbMetric: 28.4945

Epoch 517: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8546 - MinusLogProbMetric: 27.8546 - val_loss: 28.4945 - val_MinusLogProbMetric: 28.4945 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 518/1000
2023-10-26 10:47:05.739 
Epoch 518/1000 
	 loss: 27.8094, MinusLogProbMetric: 27.8094, val_loss: 28.5913, val_MinusLogProbMetric: 28.5913

Epoch 518: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8094 - MinusLogProbMetric: 27.8094 - val_loss: 28.5913 - val_MinusLogProbMetric: 28.5913 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 519/1000
2023-10-26 10:47:40.561 
Epoch 519/1000 
	 loss: 27.8538, MinusLogProbMetric: 27.8538, val_loss: 28.4914, val_MinusLogProbMetric: 28.4914

Epoch 519: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8538 - MinusLogProbMetric: 27.8538 - val_loss: 28.4914 - val_MinusLogProbMetric: 28.4914 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 520/1000
2023-10-26 10:48:15.867 
Epoch 520/1000 
	 loss: 27.8662, MinusLogProbMetric: 27.8662, val_loss: 28.6411, val_MinusLogProbMetric: 28.6411

Epoch 520: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8662 - MinusLogProbMetric: 27.8662 - val_loss: 28.6411 - val_MinusLogProbMetric: 28.6411 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 521/1000
2023-10-26 10:48:51.086 
Epoch 521/1000 
	 loss: 27.9005, MinusLogProbMetric: 27.9005, val_loss: 28.5888, val_MinusLogProbMetric: 28.5888

Epoch 521: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.9005 - MinusLogProbMetric: 27.9005 - val_loss: 28.5888 - val_MinusLogProbMetric: 28.5888 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 522/1000
2023-10-26 10:49:26.474 
Epoch 522/1000 
	 loss: 27.8532, MinusLogProbMetric: 27.8532, val_loss: 29.1646, val_MinusLogProbMetric: 29.1646

Epoch 522: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8532 - MinusLogProbMetric: 27.8532 - val_loss: 29.1646 - val_MinusLogProbMetric: 29.1646 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 523/1000
2023-10-26 10:50:01.809 
Epoch 523/1000 
	 loss: 27.8802, MinusLogProbMetric: 27.8802, val_loss: 28.5975, val_MinusLogProbMetric: 28.5975

Epoch 523: val_loss did not improve from 28.44206
196/196 - 35s - loss: 27.8802 - MinusLogProbMetric: 27.8802 - val_loss: 28.5975 - val_MinusLogProbMetric: 28.5975 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 524/1000
2023-10-26 10:50:37.324 
Epoch 524/1000 
	 loss: 27.8186, MinusLogProbMetric: 27.8186, val_loss: 28.4109, val_MinusLogProbMetric: 28.4109

Epoch 524: val_loss improved from 28.44206 to 28.41088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.8186 - MinusLogProbMetric: 27.8186 - val_loss: 28.4109 - val_MinusLogProbMetric: 28.4109 - lr: 1.6667e-04 - 36s/epoch - 184ms/step
Epoch 525/1000
2023-10-26 10:51:13.305 
Epoch 525/1000 
	 loss: 27.8661, MinusLogProbMetric: 27.8661, val_loss: 28.7910, val_MinusLogProbMetric: 28.7910

Epoch 525: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8661 - MinusLogProbMetric: 27.8661 - val_loss: 28.7910 - val_MinusLogProbMetric: 28.7910 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 526/1000
2023-10-26 10:51:47.993 
Epoch 526/1000 
	 loss: 27.8375, MinusLogProbMetric: 27.8375, val_loss: 29.0727, val_MinusLogProbMetric: 29.0727

Epoch 526: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8375 - MinusLogProbMetric: 27.8375 - val_loss: 29.0727 - val_MinusLogProbMetric: 29.0727 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 527/1000
2023-10-26 10:52:23.375 
Epoch 527/1000 
	 loss: 27.8697, MinusLogProbMetric: 27.8697, val_loss: 28.5038, val_MinusLogProbMetric: 28.5038

Epoch 527: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8697 - MinusLogProbMetric: 27.8697 - val_loss: 28.5038 - val_MinusLogProbMetric: 28.5038 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 528/1000
2023-10-26 10:52:58.699 
Epoch 528/1000 
	 loss: 27.8476, MinusLogProbMetric: 27.8476, val_loss: 29.0218, val_MinusLogProbMetric: 29.0218

Epoch 528: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8476 - MinusLogProbMetric: 27.8476 - val_loss: 29.0218 - val_MinusLogProbMetric: 29.0218 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 529/1000
2023-10-26 10:53:33.860 
Epoch 529/1000 
	 loss: 27.9334, MinusLogProbMetric: 27.9334, val_loss: 29.0946, val_MinusLogProbMetric: 29.0946

Epoch 529: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.9334 - MinusLogProbMetric: 27.9334 - val_loss: 29.0946 - val_MinusLogProbMetric: 29.0946 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 530/1000
2023-10-26 10:54:09.095 
Epoch 530/1000 
	 loss: 27.8403, MinusLogProbMetric: 27.8403, val_loss: 28.4578, val_MinusLogProbMetric: 28.4578

Epoch 530: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8403 - MinusLogProbMetric: 27.8403 - val_loss: 28.4578 - val_MinusLogProbMetric: 28.4578 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 531/1000
2023-10-26 10:54:44.349 
Epoch 531/1000 
	 loss: 27.8774, MinusLogProbMetric: 27.8774, val_loss: 28.6109, val_MinusLogProbMetric: 28.6109

Epoch 531: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8774 - MinusLogProbMetric: 27.8774 - val_loss: 28.6109 - val_MinusLogProbMetric: 28.6109 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 532/1000
2023-10-26 10:55:19.705 
Epoch 532/1000 
	 loss: 27.8098, MinusLogProbMetric: 27.8098, val_loss: 28.4973, val_MinusLogProbMetric: 28.4973

Epoch 532: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8098 - MinusLogProbMetric: 27.8098 - val_loss: 28.4973 - val_MinusLogProbMetric: 28.4973 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 533/1000
2023-10-26 10:55:55.592 
Epoch 533/1000 
	 loss: 27.8830, MinusLogProbMetric: 27.8830, val_loss: 28.5263, val_MinusLogProbMetric: 28.5263

Epoch 533: val_loss did not improve from 28.41088
196/196 - 36s - loss: 27.8830 - MinusLogProbMetric: 27.8830 - val_loss: 28.5263 - val_MinusLogProbMetric: 28.5263 - lr: 1.6667e-04 - 36s/epoch - 183ms/step
Epoch 534/1000
2023-10-26 10:56:30.810 
Epoch 534/1000 
	 loss: 27.8558, MinusLogProbMetric: 27.8558, val_loss: 28.5429, val_MinusLogProbMetric: 28.5429

Epoch 534: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8558 - MinusLogProbMetric: 27.8558 - val_loss: 28.5429 - val_MinusLogProbMetric: 28.5429 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 535/1000
2023-10-26 10:57:06.152 
Epoch 535/1000 
	 loss: 27.8411, MinusLogProbMetric: 27.8411, val_loss: 28.4375, val_MinusLogProbMetric: 28.4375

Epoch 535: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8411 - MinusLogProbMetric: 27.8411 - val_loss: 28.4375 - val_MinusLogProbMetric: 28.4375 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 536/1000
2023-10-26 10:57:41.603 
Epoch 536/1000 
	 loss: 27.8215, MinusLogProbMetric: 27.8215, val_loss: 28.6062, val_MinusLogProbMetric: 28.6062

Epoch 536: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8215 - MinusLogProbMetric: 27.8215 - val_loss: 28.6062 - val_MinusLogProbMetric: 28.6062 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 537/1000
2023-10-26 10:58:16.806 
Epoch 537/1000 
	 loss: 27.8512, MinusLogProbMetric: 27.8512, val_loss: 28.4865, val_MinusLogProbMetric: 28.4865

Epoch 537: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8512 - MinusLogProbMetric: 27.8512 - val_loss: 28.4865 - val_MinusLogProbMetric: 28.4865 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 538/1000
2023-10-26 10:58:50.611 
Epoch 538/1000 
	 loss: 27.8871, MinusLogProbMetric: 27.8871, val_loss: 28.5248, val_MinusLogProbMetric: 28.5248

Epoch 538: val_loss did not improve from 28.41088
196/196 - 34s - loss: 27.8871 - MinusLogProbMetric: 27.8871 - val_loss: 28.5248 - val_MinusLogProbMetric: 28.5248 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 539/1000
2023-10-26 10:59:25.678 
Epoch 539/1000 
	 loss: 27.8286, MinusLogProbMetric: 27.8286, val_loss: 28.5696, val_MinusLogProbMetric: 28.5696

Epoch 539: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8286 - MinusLogProbMetric: 27.8286 - val_loss: 28.5696 - val_MinusLogProbMetric: 28.5696 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 540/1000
2023-10-26 11:00:01.002 
Epoch 540/1000 
	 loss: 27.8307, MinusLogProbMetric: 27.8307, val_loss: 28.5952, val_MinusLogProbMetric: 28.5952

Epoch 540: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8307 - MinusLogProbMetric: 27.8307 - val_loss: 28.5952 - val_MinusLogProbMetric: 28.5952 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 541/1000
2023-10-26 11:00:36.013 
Epoch 541/1000 
	 loss: 27.8326, MinusLogProbMetric: 27.8326, val_loss: 28.9432, val_MinusLogProbMetric: 28.9432

Epoch 541: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8326 - MinusLogProbMetric: 27.8326 - val_loss: 28.9432 - val_MinusLogProbMetric: 28.9432 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 542/1000
2023-10-26 11:01:11.710 
Epoch 542/1000 
	 loss: 27.8581, MinusLogProbMetric: 27.8581, val_loss: 29.0725, val_MinusLogProbMetric: 29.0725

Epoch 542: val_loss did not improve from 28.41088
196/196 - 36s - loss: 27.8581 - MinusLogProbMetric: 27.8581 - val_loss: 29.0725 - val_MinusLogProbMetric: 29.0725 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 543/1000
2023-10-26 11:01:46.892 
Epoch 543/1000 
	 loss: 27.8031, MinusLogProbMetric: 27.8031, val_loss: 28.5618, val_MinusLogProbMetric: 28.5618

Epoch 543: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8031 - MinusLogProbMetric: 27.8031 - val_loss: 28.5618 - val_MinusLogProbMetric: 28.5618 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 544/1000
2023-10-26 11:02:22.394 
Epoch 544/1000 
	 loss: 27.8163, MinusLogProbMetric: 27.8163, val_loss: 28.7247, val_MinusLogProbMetric: 28.7247

Epoch 544: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8163 - MinusLogProbMetric: 27.8163 - val_loss: 28.7247 - val_MinusLogProbMetric: 28.7247 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 545/1000
2023-10-26 11:02:58.009 
Epoch 545/1000 
	 loss: 27.8340, MinusLogProbMetric: 27.8340, val_loss: 28.6124, val_MinusLogProbMetric: 28.6124

Epoch 545: val_loss did not improve from 28.41088
196/196 - 36s - loss: 27.8340 - MinusLogProbMetric: 27.8340 - val_loss: 28.6124 - val_MinusLogProbMetric: 28.6124 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 546/1000
2023-10-26 11:03:33.283 
Epoch 546/1000 
	 loss: 27.8815, MinusLogProbMetric: 27.8815, val_loss: 28.5106, val_MinusLogProbMetric: 28.5106

Epoch 546: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8815 - MinusLogProbMetric: 27.8815 - val_loss: 28.5106 - val_MinusLogProbMetric: 28.5106 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 547/1000
2023-10-26 11:04:08.744 
Epoch 547/1000 
	 loss: 27.7915, MinusLogProbMetric: 27.7915, val_loss: 28.6773, val_MinusLogProbMetric: 28.6773

Epoch 547: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.7915 - MinusLogProbMetric: 27.7915 - val_loss: 28.6773 - val_MinusLogProbMetric: 28.6773 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 548/1000
2023-10-26 11:04:43.799 
Epoch 548/1000 
	 loss: 27.8567, MinusLogProbMetric: 27.8567, val_loss: 28.8973, val_MinusLogProbMetric: 28.8973

Epoch 548: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8567 - MinusLogProbMetric: 27.8567 - val_loss: 28.8973 - val_MinusLogProbMetric: 28.8973 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 549/1000
2023-10-26 11:05:18.807 
Epoch 549/1000 
	 loss: 27.8557, MinusLogProbMetric: 27.8557, val_loss: 28.6329, val_MinusLogProbMetric: 28.6329

Epoch 549: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8557 - MinusLogProbMetric: 27.8557 - val_loss: 28.6329 - val_MinusLogProbMetric: 28.6329 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 550/1000
2023-10-26 11:05:54.047 
Epoch 550/1000 
	 loss: 27.7724, MinusLogProbMetric: 27.7724, val_loss: 28.5460, val_MinusLogProbMetric: 28.5460

Epoch 550: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.7724 - MinusLogProbMetric: 27.7724 - val_loss: 28.5460 - val_MinusLogProbMetric: 28.5460 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 551/1000
2023-10-26 11:06:29.352 
Epoch 551/1000 
	 loss: 27.8359, MinusLogProbMetric: 27.8359, val_loss: 28.7026, val_MinusLogProbMetric: 28.7026

Epoch 551: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8359 - MinusLogProbMetric: 27.8359 - val_loss: 28.7026 - val_MinusLogProbMetric: 28.7026 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 552/1000
2023-10-26 11:07:04.487 
Epoch 552/1000 
	 loss: 27.8129, MinusLogProbMetric: 27.8129, val_loss: 28.9132, val_MinusLogProbMetric: 28.9132

Epoch 552: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8129 - MinusLogProbMetric: 27.8129 - val_loss: 28.9132 - val_MinusLogProbMetric: 28.9132 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 553/1000
2023-10-26 11:07:39.814 
Epoch 553/1000 
	 loss: 27.8467, MinusLogProbMetric: 27.8467, val_loss: 28.6988, val_MinusLogProbMetric: 28.6988

Epoch 553: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8467 - MinusLogProbMetric: 27.8467 - val_loss: 28.6988 - val_MinusLogProbMetric: 28.6988 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 554/1000
2023-10-26 11:08:15.266 
Epoch 554/1000 
	 loss: 27.8196, MinusLogProbMetric: 27.8196, val_loss: 28.9004, val_MinusLogProbMetric: 28.9004

Epoch 554: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8196 - MinusLogProbMetric: 27.8196 - val_loss: 28.9004 - val_MinusLogProbMetric: 28.9004 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 555/1000
2023-10-26 11:08:50.568 
Epoch 555/1000 
	 loss: 27.8507, MinusLogProbMetric: 27.8507, val_loss: 28.5011, val_MinusLogProbMetric: 28.5011

Epoch 555: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8507 - MinusLogProbMetric: 27.8507 - val_loss: 28.5011 - val_MinusLogProbMetric: 28.5011 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 556/1000
2023-10-26 11:09:25.668 
Epoch 556/1000 
	 loss: 27.8606, MinusLogProbMetric: 27.8606, val_loss: 28.6125, val_MinusLogProbMetric: 28.6125

Epoch 556: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8606 - MinusLogProbMetric: 27.8606 - val_loss: 28.6125 - val_MinusLogProbMetric: 28.6125 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 557/1000
2023-10-26 11:10:00.912 
Epoch 557/1000 
	 loss: 27.8379, MinusLogProbMetric: 27.8379, val_loss: 28.5468, val_MinusLogProbMetric: 28.5468

Epoch 557: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8379 - MinusLogProbMetric: 27.8379 - val_loss: 28.5468 - val_MinusLogProbMetric: 28.5468 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 558/1000
2023-10-26 11:10:36.018 
Epoch 558/1000 
	 loss: 27.8236, MinusLogProbMetric: 27.8236, val_loss: 28.5898, val_MinusLogProbMetric: 28.5898

Epoch 558: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8236 - MinusLogProbMetric: 27.8236 - val_loss: 28.5898 - val_MinusLogProbMetric: 28.5898 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 559/1000
2023-10-26 11:11:10.944 
Epoch 559/1000 
	 loss: 27.7961, MinusLogProbMetric: 27.7961, val_loss: 28.6900, val_MinusLogProbMetric: 28.6900

Epoch 559: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.7961 - MinusLogProbMetric: 27.7961 - val_loss: 28.6900 - val_MinusLogProbMetric: 28.6900 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 560/1000
2023-10-26 11:11:46.323 
Epoch 560/1000 
	 loss: 27.8211, MinusLogProbMetric: 27.8211, val_loss: 28.6594, val_MinusLogProbMetric: 28.6594

Epoch 560: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8211 - MinusLogProbMetric: 27.8211 - val_loss: 28.6594 - val_MinusLogProbMetric: 28.6594 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 561/1000
2023-10-26 11:12:21.606 
Epoch 561/1000 
	 loss: 27.8064, MinusLogProbMetric: 27.8064, val_loss: 28.6838, val_MinusLogProbMetric: 28.6838

Epoch 561: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8064 - MinusLogProbMetric: 27.8064 - val_loss: 28.6838 - val_MinusLogProbMetric: 28.6838 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 562/1000
2023-10-26 11:12:56.743 
Epoch 562/1000 
	 loss: 27.8785, MinusLogProbMetric: 27.8785, val_loss: 28.5336, val_MinusLogProbMetric: 28.5336

Epoch 562: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8785 - MinusLogProbMetric: 27.8785 - val_loss: 28.5336 - val_MinusLogProbMetric: 28.5336 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 563/1000
2023-10-26 11:13:32.032 
Epoch 563/1000 
	 loss: 27.8449, MinusLogProbMetric: 27.8449, val_loss: 29.0058, val_MinusLogProbMetric: 29.0058

Epoch 563: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8449 - MinusLogProbMetric: 27.8449 - val_loss: 29.0058 - val_MinusLogProbMetric: 29.0058 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 564/1000
2023-10-26 11:14:06.621 
Epoch 564/1000 
	 loss: 27.8198, MinusLogProbMetric: 27.8198, val_loss: 28.5360, val_MinusLogProbMetric: 28.5360

Epoch 564: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8198 - MinusLogProbMetric: 27.8198 - val_loss: 28.5360 - val_MinusLogProbMetric: 28.5360 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 565/1000
2023-10-26 11:14:41.583 
Epoch 565/1000 
	 loss: 27.8477, MinusLogProbMetric: 27.8477, val_loss: 28.8164, val_MinusLogProbMetric: 28.8164

Epoch 565: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8477 - MinusLogProbMetric: 27.8477 - val_loss: 28.8164 - val_MinusLogProbMetric: 28.8164 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 566/1000
2023-10-26 11:15:16.779 
Epoch 566/1000 
	 loss: 27.7874, MinusLogProbMetric: 27.7874, val_loss: 28.6464, val_MinusLogProbMetric: 28.6464

Epoch 566: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.7874 - MinusLogProbMetric: 27.7874 - val_loss: 28.6464 - val_MinusLogProbMetric: 28.6464 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 567/1000
2023-10-26 11:15:52.241 
Epoch 567/1000 
	 loss: 27.8355, MinusLogProbMetric: 27.8355, val_loss: 28.5726, val_MinusLogProbMetric: 28.5726

Epoch 567: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8355 - MinusLogProbMetric: 27.8355 - val_loss: 28.5726 - val_MinusLogProbMetric: 28.5726 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 568/1000
2023-10-26 11:16:27.487 
Epoch 568/1000 
	 loss: 27.8439, MinusLogProbMetric: 27.8439, val_loss: 28.5616, val_MinusLogProbMetric: 28.5616

Epoch 568: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8439 - MinusLogProbMetric: 27.8439 - val_loss: 28.5616 - val_MinusLogProbMetric: 28.5616 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 569/1000
2023-10-26 11:17:03.193 
Epoch 569/1000 
	 loss: 27.8649, MinusLogProbMetric: 27.8649, val_loss: 28.7735, val_MinusLogProbMetric: 28.7735

Epoch 569: val_loss did not improve from 28.41088
196/196 - 36s - loss: 27.8649 - MinusLogProbMetric: 27.8649 - val_loss: 28.7735 - val_MinusLogProbMetric: 28.7735 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 570/1000
2023-10-26 11:17:38.360 
Epoch 570/1000 
	 loss: 27.7956, MinusLogProbMetric: 27.7956, val_loss: 28.5931, val_MinusLogProbMetric: 28.5931

Epoch 570: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.7956 - MinusLogProbMetric: 27.7956 - val_loss: 28.5931 - val_MinusLogProbMetric: 28.5931 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 571/1000
2023-10-26 11:18:13.544 
Epoch 571/1000 
	 loss: 27.8343, MinusLogProbMetric: 27.8343, val_loss: 28.5085, val_MinusLogProbMetric: 28.5085

Epoch 571: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8343 - MinusLogProbMetric: 27.8343 - val_loss: 28.5085 - val_MinusLogProbMetric: 28.5085 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 572/1000
2023-10-26 11:18:48.642 
Epoch 572/1000 
	 loss: 27.8674, MinusLogProbMetric: 27.8674, val_loss: 28.6025, val_MinusLogProbMetric: 28.6025

Epoch 572: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8674 - MinusLogProbMetric: 27.8674 - val_loss: 28.6025 - val_MinusLogProbMetric: 28.6025 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 573/1000
2023-10-26 11:19:24.086 
Epoch 573/1000 
	 loss: 27.8094, MinusLogProbMetric: 27.8094, val_loss: 28.5068, val_MinusLogProbMetric: 28.5068

Epoch 573: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.8094 - MinusLogProbMetric: 27.8094 - val_loss: 28.5068 - val_MinusLogProbMetric: 28.5068 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 574/1000
2023-10-26 11:19:59.688 
Epoch 574/1000 
	 loss: 27.8329, MinusLogProbMetric: 27.8329, val_loss: 28.5040, val_MinusLogProbMetric: 28.5040

Epoch 574: val_loss did not improve from 28.41088
196/196 - 36s - loss: 27.8329 - MinusLogProbMetric: 27.8329 - val_loss: 28.5040 - val_MinusLogProbMetric: 28.5040 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 575/1000
2023-10-26 11:20:35.039 
Epoch 575/1000 
	 loss: 27.5767, MinusLogProbMetric: 27.5767, val_loss: 28.4312, val_MinusLogProbMetric: 28.4312

Epoch 575: val_loss did not improve from 28.41088
196/196 - 35s - loss: 27.5767 - MinusLogProbMetric: 27.5767 - val_loss: 28.4312 - val_MinusLogProbMetric: 28.4312 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 576/1000
2023-10-26 11:21:10.687 
Epoch 576/1000 
	 loss: 27.5697, MinusLogProbMetric: 27.5697, val_loss: 28.4340, val_MinusLogProbMetric: 28.4340

Epoch 576: val_loss did not improve from 28.41088
196/196 - 36s - loss: 27.5697 - MinusLogProbMetric: 27.5697 - val_loss: 28.4340 - val_MinusLogProbMetric: 28.4340 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 577/1000
2023-10-26 11:21:45.592 
Epoch 577/1000 
	 loss: 27.5647, MinusLogProbMetric: 27.5647, val_loss: 28.3703, val_MinusLogProbMetric: 28.3703

Epoch 577: val_loss improved from 28.41088 to 28.37031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 35s - loss: 27.5647 - MinusLogProbMetric: 27.5647 - val_loss: 28.3703 - val_MinusLogProbMetric: 28.3703 - lr: 8.3333e-05 - 35s/epoch - 181ms/step
Epoch 578/1000
2023-10-26 11:22:21.411 
Epoch 578/1000 
	 loss: 27.5845, MinusLogProbMetric: 27.5845, val_loss: 28.3974, val_MinusLogProbMetric: 28.3974

Epoch 578: val_loss did not improve from 28.37031
196/196 - 35s - loss: 27.5845 - MinusLogProbMetric: 27.5845 - val_loss: 28.3974 - val_MinusLogProbMetric: 28.3974 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 579/1000
2023-10-26 11:22:56.676 
Epoch 579/1000 
	 loss: 27.5703, MinusLogProbMetric: 27.5703, val_loss: 28.3975, val_MinusLogProbMetric: 28.3975

Epoch 579: val_loss did not improve from 28.37031
196/196 - 35s - loss: 27.5703 - MinusLogProbMetric: 27.5703 - val_loss: 28.3975 - val_MinusLogProbMetric: 28.3975 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 580/1000
2023-10-26 11:23:31.909 
Epoch 580/1000 
	 loss: 27.5812, MinusLogProbMetric: 27.5812, val_loss: 28.3679, val_MinusLogProbMetric: 28.3679

Epoch 580: val_loss improved from 28.37031 to 28.36793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.5812 - MinusLogProbMetric: 27.5812 - val_loss: 28.3679 - val_MinusLogProbMetric: 28.3679 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 581/1000
2023-10-26 11:24:08.055 
Epoch 581/1000 
	 loss: 27.5576, MinusLogProbMetric: 27.5576, val_loss: 28.3611, val_MinusLogProbMetric: 28.3611

Epoch 581: val_loss improved from 28.36793 to 28.36110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.5576 - MinusLogProbMetric: 27.5576 - val_loss: 28.3611 - val_MinusLogProbMetric: 28.3611 - lr: 8.3333e-05 - 36s/epoch - 184ms/step
Epoch 582/1000
2023-10-26 11:24:43.776 
Epoch 582/1000 
	 loss: 27.5556, MinusLogProbMetric: 27.5556, val_loss: 28.3472, val_MinusLogProbMetric: 28.3472

Epoch 582: val_loss improved from 28.36110 to 28.34723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.5556 - MinusLogProbMetric: 27.5556 - val_loss: 28.3472 - val_MinusLogProbMetric: 28.3472 - lr: 8.3333e-05 - 36s/epoch - 183ms/step
Epoch 583/1000
2023-10-26 11:25:19.794 
Epoch 583/1000 
	 loss: 27.5578, MinusLogProbMetric: 27.5578, val_loss: 28.3826, val_MinusLogProbMetric: 28.3826

Epoch 583: val_loss did not improve from 28.34723
196/196 - 35s - loss: 27.5578 - MinusLogProbMetric: 27.5578 - val_loss: 28.3826 - val_MinusLogProbMetric: 28.3826 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 584/1000
2023-10-26 11:25:54.586 
Epoch 584/1000 
	 loss: 27.5604, MinusLogProbMetric: 27.5604, val_loss: 28.4044, val_MinusLogProbMetric: 28.4044

Epoch 584: val_loss did not improve from 28.34723
196/196 - 35s - loss: 27.5604 - MinusLogProbMetric: 27.5604 - val_loss: 28.4044 - val_MinusLogProbMetric: 28.4044 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 585/1000
2023-10-26 11:26:29.601 
Epoch 585/1000 
	 loss: 27.5516, MinusLogProbMetric: 27.5516, val_loss: 28.3813, val_MinusLogProbMetric: 28.3813

Epoch 585: val_loss did not improve from 28.34723
196/196 - 35s - loss: 27.5516 - MinusLogProbMetric: 27.5516 - val_loss: 28.3813 - val_MinusLogProbMetric: 28.3813 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 586/1000
2023-10-26 11:27:04.759 
Epoch 586/1000 
	 loss: 27.5720, MinusLogProbMetric: 27.5720, val_loss: 28.3747, val_MinusLogProbMetric: 28.3747

Epoch 586: val_loss did not improve from 28.34723
196/196 - 35s - loss: 27.5720 - MinusLogProbMetric: 27.5720 - val_loss: 28.3747 - val_MinusLogProbMetric: 28.3747 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 587/1000
2023-10-26 11:27:39.855 
Epoch 587/1000 
	 loss: 27.5615, MinusLogProbMetric: 27.5615, val_loss: 28.3392, val_MinusLogProbMetric: 28.3392

Epoch 587: val_loss improved from 28.34723 to 28.33923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.5615 - MinusLogProbMetric: 27.5615 - val_loss: 28.3392 - val_MinusLogProbMetric: 28.3392 - lr: 8.3333e-05 - 36s/epoch - 183ms/step
Epoch 588/1000
2023-10-26 11:28:16.110 
Epoch 588/1000 
	 loss: 27.5440, MinusLogProbMetric: 27.5440, val_loss: 28.3908, val_MinusLogProbMetric: 28.3908

Epoch 588: val_loss did not improve from 28.33923
196/196 - 36s - loss: 27.5440 - MinusLogProbMetric: 27.5440 - val_loss: 28.3908 - val_MinusLogProbMetric: 28.3908 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 589/1000
2023-10-26 11:28:51.174 
Epoch 589/1000 
	 loss: 27.5538, MinusLogProbMetric: 27.5538, val_loss: 28.3132, val_MinusLogProbMetric: 28.3132

Epoch 589: val_loss improved from 28.33923 to 28.31319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.5538 - MinusLogProbMetric: 27.5538 - val_loss: 28.3132 - val_MinusLogProbMetric: 28.3132 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 590/1000
2023-10-26 11:29:27.046 
Epoch 590/1000 
	 loss: 27.5613, MinusLogProbMetric: 27.5613, val_loss: 28.3406, val_MinusLogProbMetric: 28.3406

Epoch 590: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5613 - MinusLogProbMetric: 27.5613 - val_loss: 28.3406 - val_MinusLogProbMetric: 28.3406 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 591/1000
2023-10-26 11:30:02.373 
Epoch 591/1000 
	 loss: 27.5581, MinusLogProbMetric: 27.5581, val_loss: 28.4314, val_MinusLogProbMetric: 28.4314

Epoch 591: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5581 - MinusLogProbMetric: 27.5581 - val_loss: 28.4314 - val_MinusLogProbMetric: 28.4314 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 592/1000
2023-10-26 11:30:37.714 
Epoch 592/1000 
	 loss: 27.5556, MinusLogProbMetric: 27.5556, val_loss: 28.4212, val_MinusLogProbMetric: 28.4212

Epoch 592: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5556 - MinusLogProbMetric: 27.5556 - val_loss: 28.4212 - val_MinusLogProbMetric: 28.4212 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 593/1000
2023-10-26 11:31:13.158 
Epoch 593/1000 
	 loss: 27.5398, MinusLogProbMetric: 27.5398, val_loss: 28.4735, val_MinusLogProbMetric: 28.4735

Epoch 593: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5398 - MinusLogProbMetric: 27.5398 - val_loss: 28.4735 - val_MinusLogProbMetric: 28.4735 - lr: 8.3333e-05 - 35s/epoch - 181ms/step
Epoch 594/1000
2023-10-26 11:31:48.564 
Epoch 594/1000 
	 loss: 27.5611, MinusLogProbMetric: 27.5611, val_loss: 28.3191, val_MinusLogProbMetric: 28.3191

Epoch 594: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5611 - MinusLogProbMetric: 27.5611 - val_loss: 28.3191 - val_MinusLogProbMetric: 28.3191 - lr: 8.3333e-05 - 35s/epoch - 181ms/step
Epoch 595/1000
2023-10-26 11:32:23.755 
Epoch 595/1000 
	 loss: 27.5442, MinusLogProbMetric: 27.5442, val_loss: 28.3197, val_MinusLogProbMetric: 28.3197

Epoch 595: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5442 - MinusLogProbMetric: 27.5442 - val_loss: 28.3197 - val_MinusLogProbMetric: 28.3197 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 596/1000
2023-10-26 11:32:59.226 
Epoch 596/1000 
	 loss: 27.5652, MinusLogProbMetric: 27.5652, val_loss: 28.3543, val_MinusLogProbMetric: 28.3543

Epoch 596: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5652 - MinusLogProbMetric: 27.5652 - val_loss: 28.3543 - val_MinusLogProbMetric: 28.3543 - lr: 8.3333e-05 - 35s/epoch - 181ms/step
Epoch 597/1000
2023-10-26 11:33:34.798 
Epoch 597/1000 
	 loss: 27.5584, MinusLogProbMetric: 27.5584, val_loss: 28.3447, val_MinusLogProbMetric: 28.3447

Epoch 597: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5584 - MinusLogProbMetric: 27.5584 - val_loss: 28.3447 - val_MinusLogProbMetric: 28.3447 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 598/1000
2023-10-26 11:34:09.987 
Epoch 598/1000 
	 loss: 27.5580, MinusLogProbMetric: 27.5580, val_loss: 28.4004, val_MinusLogProbMetric: 28.4004

Epoch 598: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5580 - MinusLogProbMetric: 27.5580 - val_loss: 28.4004 - val_MinusLogProbMetric: 28.4004 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 599/1000
2023-10-26 11:34:45.935 
Epoch 599/1000 
	 loss: 27.5525, MinusLogProbMetric: 27.5525, val_loss: 28.3490, val_MinusLogProbMetric: 28.3490

Epoch 599: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5525 - MinusLogProbMetric: 27.5525 - val_loss: 28.3490 - val_MinusLogProbMetric: 28.3490 - lr: 8.3333e-05 - 36s/epoch - 183ms/step
Epoch 600/1000
2023-10-26 11:35:20.454 
Epoch 600/1000 
	 loss: 27.5474, MinusLogProbMetric: 27.5474, val_loss: 28.4593, val_MinusLogProbMetric: 28.4593

Epoch 600: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5474 - MinusLogProbMetric: 27.5474 - val_loss: 28.4593 - val_MinusLogProbMetric: 28.4593 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 601/1000
2023-10-26 11:35:56.116 
Epoch 601/1000 
	 loss: 27.5726, MinusLogProbMetric: 27.5726, val_loss: 28.4162, val_MinusLogProbMetric: 28.4162

Epoch 601: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5726 - MinusLogProbMetric: 27.5726 - val_loss: 28.4162 - val_MinusLogProbMetric: 28.4162 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 602/1000
2023-10-26 11:36:31.764 
Epoch 602/1000 
	 loss: 27.5541, MinusLogProbMetric: 27.5541, val_loss: 28.3999, val_MinusLogProbMetric: 28.3999

Epoch 602: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5541 - MinusLogProbMetric: 27.5541 - val_loss: 28.3999 - val_MinusLogProbMetric: 28.3999 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 603/1000
2023-10-26 11:37:06.973 
Epoch 603/1000 
	 loss: 27.5450, MinusLogProbMetric: 27.5450, val_loss: 28.4400, val_MinusLogProbMetric: 28.4400

Epoch 603: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5450 - MinusLogProbMetric: 27.5450 - val_loss: 28.4400 - val_MinusLogProbMetric: 28.4400 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 604/1000
2023-10-26 11:37:42.273 
Epoch 604/1000 
	 loss: 27.5602, MinusLogProbMetric: 27.5602, val_loss: 28.4434, val_MinusLogProbMetric: 28.4434

Epoch 604: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5602 - MinusLogProbMetric: 27.5602 - val_loss: 28.4434 - val_MinusLogProbMetric: 28.4434 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 605/1000
2023-10-26 11:38:18.307 
Epoch 605/1000 
	 loss: 27.5555, MinusLogProbMetric: 27.5555, val_loss: 28.3686, val_MinusLogProbMetric: 28.3686

Epoch 605: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5555 - MinusLogProbMetric: 27.5555 - val_loss: 28.3686 - val_MinusLogProbMetric: 28.3686 - lr: 8.3333e-05 - 36s/epoch - 184ms/step
Epoch 606/1000
2023-10-26 11:38:53.586 
Epoch 606/1000 
	 loss: 27.5456, MinusLogProbMetric: 27.5456, val_loss: 28.3472, val_MinusLogProbMetric: 28.3472

Epoch 606: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5456 - MinusLogProbMetric: 27.5456 - val_loss: 28.3472 - val_MinusLogProbMetric: 28.3472 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 607/1000
2023-10-26 11:39:28.639 
Epoch 607/1000 
	 loss: 27.5529, MinusLogProbMetric: 27.5529, val_loss: 28.6088, val_MinusLogProbMetric: 28.6088

Epoch 607: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5529 - MinusLogProbMetric: 27.5529 - val_loss: 28.6088 - val_MinusLogProbMetric: 28.6088 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 608/1000
2023-10-26 11:40:04.312 
Epoch 608/1000 
	 loss: 27.5874, MinusLogProbMetric: 27.5874, val_loss: 28.4735, val_MinusLogProbMetric: 28.4735

Epoch 608: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5874 - MinusLogProbMetric: 27.5874 - val_loss: 28.4735 - val_MinusLogProbMetric: 28.4735 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 609/1000
2023-10-26 11:40:39.988 
Epoch 609/1000 
	 loss: 27.5425, MinusLogProbMetric: 27.5425, val_loss: 28.3474, val_MinusLogProbMetric: 28.3474

Epoch 609: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5425 - MinusLogProbMetric: 27.5425 - val_loss: 28.3474 - val_MinusLogProbMetric: 28.3474 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 610/1000
2023-10-26 11:41:14.121 
Epoch 610/1000 
	 loss: 27.5508, MinusLogProbMetric: 27.5508, val_loss: 28.3573, val_MinusLogProbMetric: 28.3573

Epoch 610: val_loss did not improve from 28.31319
196/196 - 34s - loss: 27.5508 - MinusLogProbMetric: 27.5508 - val_loss: 28.3573 - val_MinusLogProbMetric: 28.3573 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 611/1000
2023-10-26 11:41:49.353 
Epoch 611/1000 
	 loss: 27.5432, MinusLogProbMetric: 27.5432, val_loss: 28.3389, val_MinusLogProbMetric: 28.3389

Epoch 611: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5432 - MinusLogProbMetric: 27.5432 - val_loss: 28.3389 - val_MinusLogProbMetric: 28.3389 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 612/1000
2023-10-26 11:42:24.927 
Epoch 612/1000 
	 loss: 27.5644, MinusLogProbMetric: 27.5644, val_loss: 28.3419, val_MinusLogProbMetric: 28.3419

Epoch 612: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5644 - MinusLogProbMetric: 27.5644 - val_loss: 28.3419 - val_MinusLogProbMetric: 28.3419 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 613/1000
2023-10-26 11:43:00.114 
Epoch 613/1000 
	 loss: 27.5393, MinusLogProbMetric: 27.5393, val_loss: 28.3835, val_MinusLogProbMetric: 28.3835

Epoch 613: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5393 - MinusLogProbMetric: 27.5393 - val_loss: 28.3835 - val_MinusLogProbMetric: 28.3835 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 614/1000
2023-10-26 11:43:35.348 
Epoch 614/1000 
	 loss: 27.5548, MinusLogProbMetric: 27.5548, val_loss: 28.3706, val_MinusLogProbMetric: 28.3706

Epoch 614: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5548 - MinusLogProbMetric: 27.5548 - val_loss: 28.3706 - val_MinusLogProbMetric: 28.3706 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 615/1000
2023-10-26 11:44:10.894 
Epoch 615/1000 
	 loss: 27.5375, MinusLogProbMetric: 27.5375, val_loss: 28.4293, val_MinusLogProbMetric: 28.4293

Epoch 615: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5375 - MinusLogProbMetric: 27.5375 - val_loss: 28.4293 - val_MinusLogProbMetric: 28.4293 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 616/1000
2023-10-26 11:44:46.224 
Epoch 616/1000 
	 loss: 27.5624, MinusLogProbMetric: 27.5624, val_loss: 28.3685, val_MinusLogProbMetric: 28.3685

Epoch 616: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5624 - MinusLogProbMetric: 27.5624 - val_loss: 28.3685 - val_MinusLogProbMetric: 28.3685 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 617/1000
2023-10-26 11:45:21.581 
Epoch 617/1000 
	 loss: 27.5493, MinusLogProbMetric: 27.5493, val_loss: 28.6448, val_MinusLogProbMetric: 28.6448

Epoch 617: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5493 - MinusLogProbMetric: 27.5493 - val_loss: 28.6448 - val_MinusLogProbMetric: 28.6448 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 618/1000
2023-10-26 11:45:56.942 
Epoch 618/1000 
	 loss: 27.5411, MinusLogProbMetric: 27.5411, val_loss: 28.3273, val_MinusLogProbMetric: 28.3273

Epoch 618: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5411 - MinusLogProbMetric: 27.5411 - val_loss: 28.3273 - val_MinusLogProbMetric: 28.3273 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 619/1000
2023-10-26 11:46:31.745 
Epoch 619/1000 
	 loss: 27.5556, MinusLogProbMetric: 27.5556, val_loss: 28.5022, val_MinusLogProbMetric: 28.5022

Epoch 619: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5556 - MinusLogProbMetric: 27.5556 - val_loss: 28.5022 - val_MinusLogProbMetric: 28.5022 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 620/1000
2023-10-26 11:47:07.093 
Epoch 620/1000 
	 loss: 27.5566, MinusLogProbMetric: 27.5566, val_loss: 28.3988, val_MinusLogProbMetric: 28.3988

Epoch 620: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5566 - MinusLogProbMetric: 27.5566 - val_loss: 28.3988 - val_MinusLogProbMetric: 28.3988 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 621/1000
2023-10-26 11:47:42.277 
Epoch 621/1000 
	 loss: 27.5435, MinusLogProbMetric: 27.5435, val_loss: 28.4063, val_MinusLogProbMetric: 28.4063

Epoch 621: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5435 - MinusLogProbMetric: 27.5435 - val_loss: 28.4063 - val_MinusLogProbMetric: 28.4063 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 622/1000
2023-10-26 11:48:17.741 
Epoch 622/1000 
	 loss: 27.5507, MinusLogProbMetric: 27.5507, val_loss: 28.3897, val_MinusLogProbMetric: 28.3897

Epoch 622: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5507 - MinusLogProbMetric: 27.5507 - val_loss: 28.3897 - val_MinusLogProbMetric: 28.3897 - lr: 8.3333e-05 - 35s/epoch - 181ms/step
Epoch 623/1000
2023-10-26 11:48:53.284 
Epoch 623/1000 
	 loss: 27.5403, MinusLogProbMetric: 27.5403, val_loss: 28.3697, val_MinusLogProbMetric: 28.3697

Epoch 623: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5403 - MinusLogProbMetric: 27.5403 - val_loss: 28.3697 - val_MinusLogProbMetric: 28.3697 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 624/1000
2023-10-26 11:49:28.634 
Epoch 624/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 28.4229, val_MinusLogProbMetric: 28.4229

Epoch 624: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 28.4229 - val_MinusLogProbMetric: 28.4229 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 625/1000
2023-10-26 11:50:03.604 
Epoch 625/1000 
	 loss: 27.5383, MinusLogProbMetric: 27.5383, val_loss: 28.4926, val_MinusLogProbMetric: 28.4926

Epoch 625: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5383 - MinusLogProbMetric: 27.5383 - val_loss: 28.4926 - val_MinusLogProbMetric: 28.4926 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 626/1000
2023-10-26 11:50:38.949 
Epoch 626/1000 
	 loss: 27.5499, MinusLogProbMetric: 27.5499, val_loss: 28.3252, val_MinusLogProbMetric: 28.3252

Epoch 626: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5499 - MinusLogProbMetric: 27.5499 - val_loss: 28.3252 - val_MinusLogProbMetric: 28.3252 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 627/1000
2023-10-26 11:51:14.302 
Epoch 627/1000 
	 loss: 27.5576, MinusLogProbMetric: 27.5576, val_loss: 28.5252, val_MinusLogProbMetric: 28.5252

Epoch 627: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5576 - MinusLogProbMetric: 27.5576 - val_loss: 28.5252 - val_MinusLogProbMetric: 28.5252 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 628/1000
2023-10-26 11:51:49.979 
Epoch 628/1000 
	 loss: 27.5386, MinusLogProbMetric: 27.5386, val_loss: 28.3197, val_MinusLogProbMetric: 28.3197

Epoch 628: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5386 - MinusLogProbMetric: 27.5386 - val_loss: 28.3197 - val_MinusLogProbMetric: 28.3197 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 629/1000
2023-10-26 11:52:25.143 
Epoch 629/1000 
	 loss: 27.5368, MinusLogProbMetric: 27.5368, val_loss: 28.3937, val_MinusLogProbMetric: 28.3937

Epoch 629: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5368 - MinusLogProbMetric: 27.5368 - val_loss: 28.3937 - val_MinusLogProbMetric: 28.3937 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 630/1000
2023-10-26 11:52:59.927 
Epoch 630/1000 
	 loss: 27.5557, MinusLogProbMetric: 27.5557, val_loss: 28.4108, val_MinusLogProbMetric: 28.4108

Epoch 630: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5557 - MinusLogProbMetric: 27.5557 - val_loss: 28.4108 - val_MinusLogProbMetric: 28.4108 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 631/1000
2023-10-26 11:53:35.090 
Epoch 631/1000 
	 loss: 27.5432, MinusLogProbMetric: 27.5432, val_loss: 28.4012, val_MinusLogProbMetric: 28.4012

Epoch 631: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5432 - MinusLogProbMetric: 27.5432 - val_loss: 28.4012 - val_MinusLogProbMetric: 28.4012 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 632/1000
2023-10-26 11:54:10.543 
Epoch 632/1000 
	 loss: 27.5591, MinusLogProbMetric: 27.5591, val_loss: 28.4247, val_MinusLogProbMetric: 28.4247

Epoch 632: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5591 - MinusLogProbMetric: 27.5591 - val_loss: 28.4247 - val_MinusLogProbMetric: 28.4247 - lr: 8.3333e-05 - 35s/epoch - 181ms/step
Epoch 633/1000
2023-10-26 11:54:46.107 
Epoch 633/1000 
	 loss: 27.5398, MinusLogProbMetric: 27.5398, val_loss: 28.5127, val_MinusLogProbMetric: 28.5127

Epoch 633: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5398 - MinusLogProbMetric: 27.5398 - val_loss: 28.5127 - val_MinusLogProbMetric: 28.5127 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 634/1000
2023-10-26 11:55:21.935 
Epoch 634/1000 
	 loss: 27.5525, MinusLogProbMetric: 27.5525, val_loss: 28.3470, val_MinusLogProbMetric: 28.3470

Epoch 634: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5525 - MinusLogProbMetric: 27.5525 - val_loss: 28.3470 - val_MinusLogProbMetric: 28.3470 - lr: 8.3333e-05 - 36s/epoch - 183ms/step
Epoch 635/1000
2023-10-26 11:55:57.039 
Epoch 635/1000 
	 loss: 27.5338, MinusLogProbMetric: 27.5338, val_loss: 28.3133, val_MinusLogProbMetric: 28.3133

Epoch 635: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5338 - MinusLogProbMetric: 27.5338 - val_loss: 28.3133 - val_MinusLogProbMetric: 28.3133 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 636/1000
2023-10-26 11:56:32.402 
Epoch 636/1000 
	 loss: 27.5591, MinusLogProbMetric: 27.5591, val_loss: 28.4853, val_MinusLogProbMetric: 28.4853

Epoch 636: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5591 - MinusLogProbMetric: 27.5591 - val_loss: 28.4853 - val_MinusLogProbMetric: 28.4853 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 637/1000
2023-10-26 11:57:07.568 
Epoch 637/1000 
	 loss: 27.5352, MinusLogProbMetric: 27.5352, val_loss: 28.5244, val_MinusLogProbMetric: 28.5244

Epoch 637: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5352 - MinusLogProbMetric: 27.5352 - val_loss: 28.5244 - val_MinusLogProbMetric: 28.5244 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 638/1000
2023-10-26 11:57:43.242 
Epoch 638/1000 
	 loss: 27.5593, MinusLogProbMetric: 27.5593, val_loss: 28.3635, val_MinusLogProbMetric: 28.3635

Epoch 638: val_loss did not improve from 28.31319
196/196 - 36s - loss: 27.5593 - MinusLogProbMetric: 27.5593 - val_loss: 28.3635 - val_MinusLogProbMetric: 28.3635 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 639/1000
2023-10-26 11:58:18.243 
Epoch 639/1000 
	 loss: 27.5433, MinusLogProbMetric: 27.5433, val_loss: 28.3898, val_MinusLogProbMetric: 28.3898

Epoch 639: val_loss did not improve from 28.31319
196/196 - 35s - loss: 27.5433 - MinusLogProbMetric: 27.5433 - val_loss: 28.3898 - val_MinusLogProbMetric: 28.3898 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 640/1000
2023-10-26 11:58:53.486 
Epoch 640/1000 
	 loss: 27.4520, MinusLogProbMetric: 27.4520, val_loss: 28.2881, val_MinusLogProbMetric: 28.2881

Epoch 640: val_loss improved from 28.31319 to 28.28808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.4520 - MinusLogProbMetric: 27.4520 - val_loss: 28.2881 - val_MinusLogProbMetric: 28.2881 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 641/1000
2023-10-26 11:59:29.047 
Epoch 641/1000 
	 loss: 27.4443, MinusLogProbMetric: 27.4443, val_loss: 28.3163, val_MinusLogProbMetric: 28.3163

Epoch 641: val_loss did not improve from 28.28808
196/196 - 35s - loss: 27.4443 - MinusLogProbMetric: 27.4443 - val_loss: 28.3163 - val_MinusLogProbMetric: 28.3163 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 642/1000
2023-10-26 12:00:04.308 
Epoch 642/1000 
	 loss: 27.4433, MinusLogProbMetric: 27.4433, val_loss: 28.3070, val_MinusLogProbMetric: 28.3070

Epoch 642: val_loss did not improve from 28.28808
196/196 - 35s - loss: 27.4433 - MinusLogProbMetric: 27.4433 - val_loss: 28.3070 - val_MinusLogProbMetric: 28.3070 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 643/1000
2023-10-26 12:00:39.774 
Epoch 643/1000 
	 loss: 27.4496, MinusLogProbMetric: 27.4496, val_loss: 28.2665, val_MinusLogProbMetric: 28.2665

Epoch 643: val_loss improved from 28.28808 to 28.26649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 36s - loss: 27.4496 - MinusLogProbMetric: 27.4496 - val_loss: 28.2665 - val_MinusLogProbMetric: 28.2665 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 644/1000
2023-10-26 12:01:16.000 
Epoch 644/1000 
	 loss: 27.4383, MinusLogProbMetric: 27.4383, val_loss: 28.2915, val_MinusLogProbMetric: 28.2915

Epoch 644: val_loss did not improve from 28.26649
196/196 - 36s - loss: 27.4383 - MinusLogProbMetric: 27.4383 - val_loss: 28.2915 - val_MinusLogProbMetric: 28.2915 - lr: 4.1667e-05 - 36s/epoch - 181ms/step
Epoch 645/1000
2023-10-26 12:01:50.558 
Epoch 645/1000 
	 loss: 27.4514, MinusLogProbMetric: 27.4514, val_loss: 28.3373, val_MinusLogProbMetric: 28.3373

Epoch 645: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4514 - MinusLogProbMetric: 27.4514 - val_loss: 28.3373 - val_MinusLogProbMetric: 28.3373 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 646/1000
2023-10-26 12:02:25.247 
Epoch 646/1000 
	 loss: 27.4500, MinusLogProbMetric: 27.4500, val_loss: 28.3129, val_MinusLogProbMetric: 28.3129

Epoch 646: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4500 - MinusLogProbMetric: 27.4500 - val_loss: 28.3129 - val_MinusLogProbMetric: 28.3129 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 647/1000
2023-10-26 12:03:00.450 
Epoch 647/1000 
	 loss: 27.4465, MinusLogProbMetric: 27.4465, val_loss: 28.2853, val_MinusLogProbMetric: 28.2853

Epoch 647: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4465 - MinusLogProbMetric: 27.4465 - val_loss: 28.2853 - val_MinusLogProbMetric: 28.2853 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 648/1000
2023-10-26 12:03:35.901 
Epoch 648/1000 
	 loss: 27.4434, MinusLogProbMetric: 27.4434, val_loss: 28.3349, val_MinusLogProbMetric: 28.3349

Epoch 648: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4434 - MinusLogProbMetric: 27.4434 - val_loss: 28.3349 - val_MinusLogProbMetric: 28.3349 - lr: 4.1667e-05 - 35s/epoch - 181ms/step
Epoch 649/1000
2023-10-26 12:04:11.398 
Epoch 649/1000 
	 loss: 27.4491, MinusLogProbMetric: 27.4491, val_loss: 28.3012, val_MinusLogProbMetric: 28.3012

Epoch 649: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4491 - MinusLogProbMetric: 27.4491 - val_loss: 28.3012 - val_MinusLogProbMetric: 28.3012 - lr: 4.1667e-05 - 35s/epoch - 181ms/step
Epoch 650/1000
2023-10-26 12:04:46.679 
Epoch 650/1000 
	 loss: 27.4488, MinusLogProbMetric: 27.4488, val_loss: 28.3295, val_MinusLogProbMetric: 28.3295

Epoch 650: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4488 - MinusLogProbMetric: 27.4488 - val_loss: 28.3295 - val_MinusLogProbMetric: 28.3295 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 651/1000
2023-10-26 12:05:21.878 
Epoch 651/1000 
	 loss: 27.4465, MinusLogProbMetric: 27.4465, val_loss: 28.2875, val_MinusLogProbMetric: 28.2875

Epoch 651: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4465 - MinusLogProbMetric: 27.4465 - val_loss: 28.2875 - val_MinusLogProbMetric: 28.2875 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 652/1000
2023-10-26 12:05:57.227 
Epoch 652/1000 
	 loss: 27.4447, MinusLogProbMetric: 27.4447, val_loss: 28.2700, val_MinusLogProbMetric: 28.2700

Epoch 652: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4447 - MinusLogProbMetric: 27.4447 - val_loss: 28.2700 - val_MinusLogProbMetric: 28.2700 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 653/1000
2023-10-26 12:06:32.462 
Epoch 653/1000 
	 loss: 27.4395, MinusLogProbMetric: 27.4395, val_loss: 28.3270, val_MinusLogProbMetric: 28.3270

Epoch 653: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4395 - MinusLogProbMetric: 27.4395 - val_loss: 28.3270 - val_MinusLogProbMetric: 28.3270 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 654/1000
2023-10-26 12:07:07.748 
Epoch 654/1000 
	 loss: 27.4409, MinusLogProbMetric: 27.4409, val_loss: 28.3510, val_MinusLogProbMetric: 28.3510

Epoch 654: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4409 - MinusLogProbMetric: 27.4409 - val_loss: 28.3510 - val_MinusLogProbMetric: 28.3510 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 655/1000
2023-10-26 12:07:42.867 
Epoch 655/1000 
	 loss: 27.4422, MinusLogProbMetric: 27.4422, val_loss: 28.2888, val_MinusLogProbMetric: 28.2888

Epoch 655: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4422 - MinusLogProbMetric: 27.4422 - val_loss: 28.2888 - val_MinusLogProbMetric: 28.2888 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 656/1000
2023-10-26 12:08:17.931 
Epoch 656/1000 
	 loss: 27.4484, MinusLogProbMetric: 27.4484, val_loss: 28.2801, val_MinusLogProbMetric: 28.2801

Epoch 656: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4484 - MinusLogProbMetric: 27.4484 - val_loss: 28.2801 - val_MinusLogProbMetric: 28.2801 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 657/1000
2023-10-26 12:08:52.684 
Epoch 657/1000 
	 loss: 27.4512, MinusLogProbMetric: 27.4512, val_loss: 28.3717, val_MinusLogProbMetric: 28.3717

Epoch 657: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4512 - MinusLogProbMetric: 27.4512 - val_loss: 28.3717 - val_MinusLogProbMetric: 28.3717 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 658/1000
2023-10-26 12:09:28.103 
Epoch 658/1000 
	 loss: 27.4463, MinusLogProbMetric: 27.4463, val_loss: 28.3139, val_MinusLogProbMetric: 28.3139

Epoch 658: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4463 - MinusLogProbMetric: 27.4463 - val_loss: 28.3139 - val_MinusLogProbMetric: 28.3139 - lr: 4.1667e-05 - 35s/epoch - 181ms/step
Epoch 659/1000
2023-10-26 12:10:03.454 
Epoch 659/1000 
	 loss: 27.4374, MinusLogProbMetric: 27.4374, val_loss: 28.3024, val_MinusLogProbMetric: 28.3024

Epoch 659: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4374 - MinusLogProbMetric: 27.4374 - val_loss: 28.3024 - val_MinusLogProbMetric: 28.3024 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 660/1000
2023-10-26 12:10:38.715 
Epoch 660/1000 
	 loss: 27.4354, MinusLogProbMetric: 27.4354, val_loss: 28.3116, val_MinusLogProbMetric: 28.3116

Epoch 660: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4354 - MinusLogProbMetric: 27.4354 - val_loss: 28.3116 - val_MinusLogProbMetric: 28.3116 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 661/1000
2023-10-26 12:11:13.974 
Epoch 661/1000 
	 loss: 27.4413, MinusLogProbMetric: 27.4413, val_loss: 28.2909, val_MinusLogProbMetric: 28.2909

Epoch 661: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4413 - MinusLogProbMetric: 27.4413 - val_loss: 28.2909 - val_MinusLogProbMetric: 28.2909 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 662/1000
2023-10-26 12:11:49.023 
Epoch 662/1000 
	 loss: 27.4356, MinusLogProbMetric: 27.4356, val_loss: 28.3160, val_MinusLogProbMetric: 28.3160

Epoch 662: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4356 - MinusLogProbMetric: 27.4356 - val_loss: 28.3160 - val_MinusLogProbMetric: 28.3160 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 663/1000
2023-10-26 12:12:24.096 
Epoch 663/1000 
	 loss: 27.4370, MinusLogProbMetric: 27.4370, val_loss: 28.2862, val_MinusLogProbMetric: 28.2862

Epoch 663: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4370 - MinusLogProbMetric: 27.4370 - val_loss: 28.2862 - val_MinusLogProbMetric: 28.2862 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 664/1000
2023-10-26 12:12:58.246 
Epoch 664/1000 
	 loss: 27.4340, MinusLogProbMetric: 27.4340, val_loss: 28.3176, val_MinusLogProbMetric: 28.3176

Epoch 664: val_loss did not improve from 28.26649
196/196 - 34s - loss: 27.4340 - MinusLogProbMetric: 27.4340 - val_loss: 28.3176 - val_MinusLogProbMetric: 28.3176 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 665/1000
2023-10-26 12:13:33.494 
Epoch 665/1000 
	 loss: 27.4414, MinusLogProbMetric: 27.4414, val_loss: 28.2999, val_MinusLogProbMetric: 28.2999

Epoch 665: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4414 - MinusLogProbMetric: 27.4414 - val_loss: 28.2999 - val_MinusLogProbMetric: 28.2999 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 666/1000
2023-10-26 12:14:08.711 
Epoch 666/1000 
	 loss: 27.4443, MinusLogProbMetric: 27.4443, val_loss: 28.2989, val_MinusLogProbMetric: 28.2989

Epoch 666: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4443 - MinusLogProbMetric: 27.4443 - val_loss: 28.2989 - val_MinusLogProbMetric: 28.2989 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 667/1000
2023-10-26 12:14:43.988 
Epoch 667/1000 
	 loss: 27.4377, MinusLogProbMetric: 27.4377, val_loss: 28.3229, val_MinusLogProbMetric: 28.3229

Epoch 667: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4377 - MinusLogProbMetric: 27.4377 - val_loss: 28.3229 - val_MinusLogProbMetric: 28.3229 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 668/1000
2023-10-26 12:15:18.863 
Epoch 668/1000 
	 loss: 27.4414, MinusLogProbMetric: 27.4414, val_loss: 28.3031, val_MinusLogProbMetric: 28.3031

Epoch 668: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4414 - MinusLogProbMetric: 27.4414 - val_loss: 28.3031 - val_MinusLogProbMetric: 28.3031 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 669/1000
2023-10-26 12:15:54.446 
Epoch 669/1000 
	 loss: 27.4434, MinusLogProbMetric: 27.4434, val_loss: 28.3553, val_MinusLogProbMetric: 28.3553

Epoch 669: val_loss did not improve from 28.26649
196/196 - 36s - loss: 27.4434 - MinusLogProbMetric: 27.4434 - val_loss: 28.3553 - val_MinusLogProbMetric: 28.3553 - lr: 4.1667e-05 - 36s/epoch - 182ms/step
Epoch 670/1000
2023-10-26 12:16:29.687 
Epoch 670/1000 
	 loss: 27.4412, MinusLogProbMetric: 27.4412, val_loss: 28.3282, val_MinusLogProbMetric: 28.3282

Epoch 670: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4412 - MinusLogProbMetric: 27.4412 - val_loss: 28.3282 - val_MinusLogProbMetric: 28.3282 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 671/1000
2023-10-26 12:17:04.399 
Epoch 671/1000 
	 loss: 27.4394, MinusLogProbMetric: 27.4394, val_loss: 28.3153, val_MinusLogProbMetric: 28.3153

Epoch 671: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4394 - MinusLogProbMetric: 27.4394 - val_loss: 28.3153 - val_MinusLogProbMetric: 28.3153 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 672/1000
2023-10-26 12:17:39.320 
Epoch 672/1000 
	 loss: 27.4344, MinusLogProbMetric: 27.4344, val_loss: 28.2921, val_MinusLogProbMetric: 28.2921

Epoch 672: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4344 - MinusLogProbMetric: 27.4344 - val_loss: 28.2921 - val_MinusLogProbMetric: 28.2921 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 673/1000
2023-10-26 12:18:13.487 
Epoch 673/1000 
	 loss: 27.4434, MinusLogProbMetric: 27.4434, val_loss: 28.3278, val_MinusLogProbMetric: 28.3278

Epoch 673: val_loss did not improve from 28.26649
196/196 - 34s - loss: 27.4434 - MinusLogProbMetric: 27.4434 - val_loss: 28.3278 - val_MinusLogProbMetric: 28.3278 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 674/1000
2023-10-26 12:18:48.557 
Epoch 674/1000 
	 loss: 27.4381, MinusLogProbMetric: 27.4381, val_loss: 28.3102, val_MinusLogProbMetric: 28.3102

Epoch 674: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4381 - MinusLogProbMetric: 27.4381 - val_loss: 28.3102 - val_MinusLogProbMetric: 28.3102 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 675/1000
2023-10-26 12:19:23.921 
Epoch 675/1000 
	 loss: 27.4429, MinusLogProbMetric: 27.4429, val_loss: 28.3001, val_MinusLogProbMetric: 28.3001

Epoch 675: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4429 - MinusLogProbMetric: 27.4429 - val_loss: 28.3001 - val_MinusLogProbMetric: 28.3001 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 676/1000
2023-10-26 12:19:59.411 
Epoch 676/1000 
	 loss: 27.4357, MinusLogProbMetric: 27.4357, val_loss: 28.2913, val_MinusLogProbMetric: 28.2913

Epoch 676: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4357 - MinusLogProbMetric: 27.4357 - val_loss: 28.2913 - val_MinusLogProbMetric: 28.2913 - lr: 4.1667e-05 - 35s/epoch - 181ms/step
Epoch 677/1000
2023-10-26 12:20:34.655 
Epoch 677/1000 
	 loss: 27.4384, MinusLogProbMetric: 27.4384, val_loss: 28.3028, val_MinusLogProbMetric: 28.3028

Epoch 677: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4384 - MinusLogProbMetric: 27.4384 - val_loss: 28.3028 - val_MinusLogProbMetric: 28.3028 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 678/1000
2023-10-26 12:21:09.856 
Epoch 678/1000 
	 loss: 27.4344, MinusLogProbMetric: 27.4344, val_loss: 28.3912, val_MinusLogProbMetric: 28.3912

Epoch 678: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4344 - MinusLogProbMetric: 27.4344 - val_loss: 28.3912 - val_MinusLogProbMetric: 28.3912 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 679/1000
2023-10-26 12:21:44.704 
Epoch 679/1000 
	 loss: 27.4409, MinusLogProbMetric: 27.4409, val_loss: 28.4756, val_MinusLogProbMetric: 28.4756

Epoch 679: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4409 - MinusLogProbMetric: 27.4409 - val_loss: 28.4756 - val_MinusLogProbMetric: 28.4756 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 680/1000
2023-10-26 12:22:20.023 
Epoch 680/1000 
	 loss: 27.4386, MinusLogProbMetric: 27.4386, val_loss: 28.3410, val_MinusLogProbMetric: 28.3410

Epoch 680: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4386 - MinusLogProbMetric: 27.4386 - val_loss: 28.3410 - val_MinusLogProbMetric: 28.3410 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 681/1000
2023-10-26 12:22:55.329 
Epoch 681/1000 
	 loss: 27.4372, MinusLogProbMetric: 27.4372, val_loss: 28.2978, val_MinusLogProbMetric: 28.2978

Epoch 681: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4372 - MinusLogProbMetric: 27.4372 - val_loss: 28.2978 - val_MinusLogProbMetric: 28.2978 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 682/1000
2023-10-26 12:23:31.123 
Epoch 682/1000 
	 loss: 27.4409, MinusLogProbMetric: 27.4409, val_loss: 28.3148, val_MinusLogProbMetric: 28.3148

Epoch 682: val_loss did not improve from 28.26649
196/196 - 36s - loss: 27.4409 - MinusLogProbMetric: 27.4409 - val_loss: 28.3148 - val_MinusLogProbMetric: 28.3148 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 683/1000
2023-10-26 12:24:06.279 
Epoch 683/1000 
	 loss: 27.4310, MinusLogProbMetric: 27.4310, val_loss: 28.3050, val_MinusLogProbMetric: 28.3050

Epoch 683: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4310 - MinusLogProbMetric: 27.4310 - val_loss: 28.3050 - val_MinusLogProbMetric: 28.3050 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 684/1000
2023-10-26 12:24:41.328 
Epoch 684/1000 
	 loss: 27.4358, MinusLogProbMetric: 27.4358, val_loss: 28.3657, val_MinusLogProbMetric: 28.3657

Epoch 684: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4358 - MinusLogProbMetric: 27.4358 - val_loss: 28.3657 - val_MinusLogProbMetric: 28.3657 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 685/1000
2023-10-26 12:25:16.320 
Epoch 685/1000 
	 loss: 27.4429, MinusLogProbMetric: 27.4429, val_loss: 28.3313, val_MinusLogProbMetric: 28.3313

Epoch 685: val_loss did not improve from 28.26649
196/196 - 35s - loss: 27.4429 - MinusLogProbMetric: 27.4429 - val_loss: 28.3313 - val_MinusLogProbMetric: 28.3313 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 686/1000
2023-10-26 12:25:49.738 
Epoch 686/1000 
	 loss: 27.4322, MinusLogProbMetric: 27.4322, val_loss: 28.3060, val_MinusLogProbMetric: 28.3060

Epoch 686: val_loss did not improve from 28.26649
196/196 - 33s - loss: 27.4322 - MinusLogProbMetric: 27.4322 - val_loss: 28.3060 - val_MinusLogProbMetric: 28.3060 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 687/1000
2023-10-26 12:26:17.721 
Epoch 687/1000 
	 loss: 27.4407, MinusLogProbMetric: 27.4407, val_loss: 28.3851, val_MinusLogProbMetric: 28.3851

Epoch 687: val_loss did not improve from 28.26649
196/196 - 28s - loss: 27.4407 - MinusLogProbMetric: 27.4407 - val_loss: 28.3851 - val_MinusLogProbMetric: 28.3851 - lr: 4.1667e-05 - 28s/epoch - 143ms/step
Epoch 688/1000
2023-10-26 12:26:45.407 
Epoch 688/1000 
	 loss: 27.4354, MinusLogProbMetric: 27.4354, val_loss: 28.2865, val_MinusLogProbMetric: 28.2865

Epoch 688: val_loss did not improve from 28.26649
196/196 - 28s - loss: 27.4354 - MinusLogProbMetric: 27.4354 - val_loss: 28.2865 - val_MinusLogProbMetric: 28.2865 - lr: 4.1667e-05 - 28s/epoch - 141ms/step
Epoch 689/1000
2023-10-26 12:27:13.584 
Epoch 689/1000 
	 loss: 27.4392, MinusLogProbMetric: 27.4392, val_loss: 28.3126, val_MinusLogProbMetric: 28.3126

Epoch 689: val_loss did not improve from 28.26649
196/196 - 28s - loss: 27.4392 - MinusLogProbMetric: 27.4392 - val_loss: 28.3126 - val_MinusLogProbMetric: 28.3126 - lr: 4.1667e-05 - 28s/epoch - 144ms/step
Epoch 690/1000
2023-10-26 12:27:47.942 
Epoch 690/1000 
	 loss: 27.4437, MinusLogProbMetric: 27.4437, val_loss: 28.3342, val_MinusLogProbMetric: 28.3342

Epoch 690: val_loss did not improve from 28.26649
196/196 - 34s - loss: 27.4437 - MinusLogProbMetric: 27.4437 - val_loss: 28.3342 - val_MinusLogProbMetric: 28.3342 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 691/1000
2023-10-26 12:28:17.693 
Epoch 691/1000 
	 loss: 27.4347, MinusLogProbMetric: 27.4347, val_loss: 28.2905, val_MinusLogProbMetric: 28.2905

Epoch 691: val_loss did not improve from 28.26649
196/196 - 30s - loss: 27.4347 - MinusLogProbMetric: 27.4347 - val_loss: 28.2905 - val_MinusLogProbMetric: 28.2905 - lr: 4.1667e-05 - 30s/epoch - 152ms/step
Epoch 692/1000
2023-10-26 12:28:45.023 
Epoch 692/1000 
	 loss: 27.4379, MinusLogProbMetric: 27.4379, val_loss: 28.3010, val_MinusLogProbMetric: 28.3010

Epoch 692: val_loss did not improve from 28.26649
196/196 - 27s - loss: 27.4379 - MinusLogProbMetric: 27.4379 - val_loss: 28.3010 - val_MinusLogProbMetric: 28.3010 - lr: 4.1667e-05 - 27s/epoch - 139ms/step
Epoch 693/1000
2023-10-26 12:29:12.798 
Epoch 693/1000 
	 loss: 27.4392, MinusLogProbMetric: 27.4392, val_loss: 28.3067, val_MinusLogProbMetric: 28.3067

Epoch 693: val_loss did not improve from 28.26649
196/196 - 28s - loss: 27.4392 - MinusLogProbMetric: 27.4392 - val_loss: 28.3067 - val_MinusLogProbMetric: 28.3067 - lr: 4.1667e-05 - 28s/epoch - 142ms/step
Epoch 694/1000
2023-10-26 12:29:42.809 
Epoch 694/1000 
	 loss: 27.4005, MinusLogProbMetric: 27.4005, val_loss: 28.2721, val_MinusLogProbMetric: 28.2721

Epoch 694: val_loss did not improve from 28.26649
196/196 - 30s - loss: 27.4005 - MinusLogProbMetric: 27.4005 - val_loss: 28.2721 - val_MinusLogProbMetric: 28.2721 - lr: 2.0833e-05 - 30s/epoch - 153ms/step
Epoch 695/1000
2023-10-26 12:30:14.886 
Epoch 695/1000 
	 loss: 27.3972, MinusLogProbMetric: 27.3972, val_loss: 28.3262, val_MinusLogProbMetric: 28.3262

Epoch 695: val_loss did not improve from 28.26649
196/196 - 32s - loss: 27.3972 - MinusLogProbMetric: 27.3972 - val_loss: 28.3262 - val_MinusLogProbMetric: 28.3262 - lr: 2.0833e-05 - 32s/epoch - 164ms/step
Epoch 696/1000
2023-10-26 12:30:44.167 
Epoch 696/1000 
	 loss: 27.3995, MinusLogProbMetric: 27.3995, val_loss: 28.2892, val_MinusLogProbMetric: 28.2892

Epoch 696: val_loss did not improve from 28.26649
196/196 - 29s - loss: 27.3995 - MinusLogProbMetric: 27.3995 - val_loss: 28.2892 - val_MinusLogProbMetric: 28.2892 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 697/1000
2023-10-26 12:31:11.051 
Epoch 697/1000 
	 loss: 27.4006, MinusLogProbMetric: 27.4006, val_loss: 28.2823, val_MinusLogProbMetric: 28.2823

Epoch 697: val_loss did not improve from 28.26649
196/196 - 27s - loss: 27.4006 - MinusLogProbMetric: 27.4006 - val_loss: 28.2823 - val_MinusLogProbMetric: 28.2823 - lr: 2.0833e-05 - 27s/epoch - 137ms/step
Epoch 698/1000
2023-10-26 12:31:38.114 
Epoch 698/1000 
	 loss: 27.3932, MinusLogProbMetric: 27.3932, val_loss: 28.2918, val_MinusLogProbMetric: 28.2918

Epoch 698: val_loss did not improve from 28.26649
196/196 - 27s - loss: 27.3932 - MinusLogProbMetric: 27.3932 - val_loss: 28.2918 - val_MinusLogProbMetric: 28.2918 - lr: 2.0833e-05 - 27s/epoch - 138ms/step
Epoch 699/1000
2023-10-26 12:32:10.704 
Epoch 699/1000 
	 loss: 27.3969, MinusLogProbMetric: 27.3969, val_loss: 28.2859, val_MinusLogProbMetric: 28.2859

Epoch 699: val_loss did not improve from 28.26649
196/196 - 33s - loss: 27.3969 - MinusLogProbMetric: 27.3969 - val_loss: 28.2859 - val_MinusLogProbMetric: 28.2859 - lr: 2.0833e-05 - 33s/epoch - 166ms/step
Epoch 700/1000
2023-10-26 12:32:41.244 
Epoch 700/1000 
	 loss: 27.3991, MinusLogProbMetric: 27.3991, val_loss: 28.3018, val_MinusLogProbMetric: 28.3018

Epoch 700: val_loss did not improve from 28.26649
196/196 - 31s - loss: 27.3991 - MinusLogProbMetric: 27.3991 - val_loss: 28.3018 - val_MinusLogProbMetric: 28.3018 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 701/1000
2023-10-26 12:33:08.462 
Epoch 701/1000 
	 loss: 27.3962, MinusLogProbMetric: 27.3962, val_loss: 28.2651, val_MinusLogProbMetric: 28.2651

Epoch 701: val_loss improved from 28.26649 to 28.26513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 28s - loss: 27.3962 - MinusLogProbMetric: 27.3962 - val_loss: 28.2651 - val_MinusLogProbMetric: 28.2651 - lr: 2.0833e-05 - 28s/epoch - 141ms/step
Epoch 702/1000
2023-10-26 12:33:36.161 
Epoch 702/1000 
	 loss: 27.3961, MinusLogProbMetric: 27.3961, val_loss: 28.2831, val_MinusLogProbMetric: 28.2831

Epoch 702: val_loss did not improve from 28.26513
196/196 - 27s - loss: 27.3961 - MinusLogProbMetric: 27.3961 - val_loss: 28.2831 - val_MinusLogProbMetric: 28.2831 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 703/1000
2023-10-26 12:34:03.881 
Epoch 703/1000 
	 loss: 27.3945, MinusLogProbMetric: 27.3945, val_loss: 28.2772, val_MinusLogProbMetric: 28.2772

Epoch 703: val_loss did not improve from 28.26513
196/196 - 28s - loss: 27.3945 - MinusLogProbMetric: 27.3945 - val_loss: 28.2772 - val_MinusLogProbMetric: 28.2772 - lr: 2.0833e-05 - 28s/epoch - 141ms/step
Epoch 704/1000
2023-10-26 12:34:37.578 
Epoch 704/1000 
	 loss: 27.3966, MinusLogProbMetric: 27.3966, val_loss: 28.2828, val_MinusLogProbMetric: 28.2828

Epoch 704: val_loss did not improve from 28.26513
196/196 - 34s - loss: 27.3966 - MinusLogProbMetric: 27.3966 - val_loss: 28.2828 - val_MinusLogProbMetric: 28.2828 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 705/1000
2023-10-26 12:35:08.489 
Epoch 705/1000 
	 loss: 27.3968, MinusLogProbMetric: 27.3968, val_loss: 28.2823, val_MinusLogProbMetric: 28.2823

Epoch 705: val_loss did not improve from 28.26513
196/196 - 31s - loss: 27.3968 - MinusLogProbMetric: 27.3968 - val_loss: 28.2823 - val_MinusLogProbMetric: 28.2823 - lr: 2.0833e-05 - 31s/epoch - 158ms/step
Epoch 706/1000
2023-10-26 12:35:35.913 
Epoch 706/1000 
	 loss: 27.3959, MinusLogProbMetric: 27.3959, val_loss: 28.2937, val_MinusLogProbMetric: 28.2937

Epoch 706: val_loss did not improve from 28.26513
196/196 - 27s - loss: 27.3959 - MinusLogProbMetric: 27.3959 - val_loss: 28.2937 - val_MinusLogProbMetric: 28.2937 - lr: 2.0833e-05 - 27s/epoch - 140ms/step
Epoch 707/1000
2023-10-26 12:36:02.859 
Epoch 707/1000 
	 loss: 27.3939, MinusLogProbMetric: 27.3939, val_loss: 28.2734, val_MinusLogProbMetric: 28.2734

Epoch 707: val_loss did not improve from 28.26513
196/196 - 27s - loss: 27.3939 - MinusLogProbMetric: 27.3939 - val_loss: 28.2734 - val_MinusLogProbMetric: 28.2734 - lr: 2.0833e-05 - 27s/epoch - 137ms/step
Epoch 708/1000
2023-10-26 12:36:32.303 
Epoch 708/1000 
	 loss: 27.3940, MinusLogProbMetric: 27.3940, val_loss: 28.2889, val_MinusLogProbMetric: 28.2889

Epoch 708: val_loss did not improve from 28.26513
196/196 - 29s - loss: 27.3940 - MinusLogProbMetric: 27.3940 - val_loss: 28.2889 - val_MinusLogProbMetric: 28.2889 - lr: 2.0833e-05 - 29s/epoch - 150ms/step
Epoch 709/1000
2023-10-26 12:37:05.335 
Epoch 709/1000 
	 loss: 27.3956, MinusLogProbMetric: 27.3956, val_loss: 28.2825, val_MinusLogProbMetric: 28.2825

Epoch 709: val_loss did not improve from 28.26513
196/196 - 33s - loss: 27.3956 - MinusLogProbMetric: 27.3956 - val_loss: 28.2825 - val_MinusLogProbMetric: 28.2825 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 710/1000
2023-10-26 12:37:33.377 
Epoch 710/1000 
	 loss: 27.3943, MinusLogProbMetric: 27.3943, val_loss: 28.3052, val_MinusLogProbMetric: 28.3052

Epoch 710: val_loss did not improve from 28.26513
196/196 - 28s - loss: 27.3943 - MinusLogProbMetric: 27.3943 - val_loss: 28.3052 - val_MinusLogProbMetric: 28.3052 - lr: 2.0833e-05 - 28s/epoch - 143ms/step
Epoch 711/1000
2023-10-26 12:38:00.932 
Epoch 711/1000 
	 loss: 27.3963, MinusLogProbMetric: 27.3963, val_loss: 28.2696, val_MinusLogProbMetric: 28.2696

Epoch 711: val_loss did not improve from 28.26513
196/196 - 28s - loss: 27.3963 - MinusLogProbMetric: 27.3963 - val_loss: 28.2696 - val_MinusLogProbMetric: 28.2696 - lr: 2.0833e-05 - 28s/epoch - 141ms/step
Epoch 712/1000
2023-10-26 12:38:28.109 
Epoch 712/1000 
	 loss: 27.3916, MinusLogProbMetric: 27.3916, val_loss: 28.2767, val_MinusLogProbMetric: 28.2767

Epoch 712: val_loss did not improve from 28.26513
196/196 - 27s - loss: 27.3916 - MinusLogProbMetric: 27.3916 - val_loss: 28.2767 - val_MinusLogProbMetric: 28.2767 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 713/1000
2023-10-26 12:38:58.694 
Epoch 713/1000 
	 loss: 27.3937, MinusLogProbMetric: 27.3937, val_loss: 28.3220, val_MinusLogProbMetric: 28.3220

Epoch 713: val_loss did not improve from 28.26513
196/196 - 31s - loss: 27.3937 - MinusLogProbMetric: 27.3937 - val_loss: 28.3220 - val_MinusLogProbMetric: 28.3220 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 714/1000
2023-10-26 12:39:29.285 
Epoch 714/1000 
	 loss: 27.3946, MinusLogProbMetric: 27.3946, val_loss: 28.2724, val_MinusLogProbMetric: 28.2724

Epoch 714: val_loss did not improve from 28.26513
196/196 - 31s - loss: 27.3946 - MinusLogProbMetric: 27.3946 - val_loss: 28.2724 - val_MinusLogProbMetric: 28.2724 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 715/1000
2023-10-26 12:39:56.456 
Epoch 715/1000 
	 loss: 27.4027, MinusLogProbMetric: 27.4027, val_loss: 28.2888, val_MinusLogProbMetric: 28.2888

Epoch 715: val_loss did not improve from 28.26513
196/196 - 27s - loss: 27.4027 - MinusLogProbMetric: 27.4027 - val_loss: 28.2888 - val_MinusLogProbMetric: 28.2888 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 716/1000
2023-10-26 12:40:23.792 
Epoch 716/1000 
	 loss: 27.3939, MinusLogProbMetric: 27.3939, val_loss: 28.2958, val_MinusLogProbMetric: 28.2958

Epoch 716: val_loss did not improve from 28.26513
196/196 - 27s - loss: 27.3939 - MinusLogProbMetric: 27.3939 - val_loss: 28.2958 - val_MinusLogProbMetric: 28.2958 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 717/1000
2023-10-26 12:40:51.277 
Epoch 717/1000 
	 loss: 27.3957, MinusLogProbMetric: 27.3957, val_loss: 28.2897, val_MinusLogProbMetric: 28.2897

Epoch 717: val_loss did not improve from 28.26513
196/196 - 27s - loss: 27.3957 - MinusLogProbMetric: 27.3957 - val_loss: 28.2897 - val_MinusLogProbMetric: 28.2897 - lr: 2.0833e-05 - 27s/epoch - 140ms/step
Epoch 718/1000
2023-10-26 12:41:24.055 
Epoch 718/1000 
	 loss: 27.3927, MinusLogProbMetric: 27.3927, val_loss: 28.2857, val_MinusLogProbMetric: 28.2857

Epoch 718: val_loss did not improve from 28.26513
196/196 - 33s - loss: 27.3927 - MinusLogProbMetric: 27.3927 - val_loss: 28.2857 - val_MinusLogProbMetric: 28.2857 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 719/1000
2023-10-26 12:41:53.579 
Epoch 719/1000 
	 loss: 27.3953, MinusLogProbMetric: 27.3953, val_loss: 28.2970, val_MinusLogProbMetric: 28.2970

Epoch 719: val_loss did not improve from 28.26513
196/196 - 30s - loss: 27.3953 - MinusLogProbMetric: 27.3953 - val_loss: 28.2970 - val_MinusLogProbMetric: 28.2970 - lr: 2.0833e-05 - 30s/epoch - 151ms/step
Epoch 720/1000
2023-10-26 12:42:20.863 
Epoch 720/1000 
	 loss: 27.3938, MinusLogProbMetric: 27.3938, val_loss: 28.2808, val_MinusLogProbMetric: 28.2808

Epoch 720: val_loss did not improve from 28.26513
196/196 - 27s - loss: 27.3938 - MinusLogProbMetric: 27.3938 - val_loss: 28.2808 - val_MinusLogProbMetric: 28.2808 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 721/1000
2023-10-26 12:42:47.742 
Epoch 721/1000 
	 loss: 27.3936, MinusLogProbMetric: 27.3936, val_loss: 28.2629, val_MinusLogProbMetric: 28.2629

Epoch 721: val_loss improved from 28.26513 to 28.26288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 27s - loss: 27.3936 - MinusLogProbMetric: 27.3936 - val_loss: 28.2629 - val_MinusLogProbMetric: 28.2629 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 722/1000
2023-10-26 12:43:15.974 
Epoch 722/1000 
	 loss: 27.3957, MinusLogProbMetric: 27.3957, val_loss: 28.2781, val_MinusLogProbMetric: 28.2781

Epoch 722: val_loss did not improve from 28.26288
196/196 - 28s - loss: 27.3957 - MinusLogProbMetric: 27.3957 - val_loss: 28.2781 - val_MinusLogProbMetric: 28.2781 - lr: 2.0833e-05 - 28s/epoch - 142ms/step
Epoch 723/1000
2023-10-26 12:43:48.581 
Epoch 723/1000 
	 loss: 27.3951, MinusLogProbMetric: 27.3951, val_loss: 28.2916, val_MinusLogProbMetric: 28.2916

Epoch 723: val_loss did not improve from 28.26288
196/196 - 33s - loss: 27.3951 - MinusLogProbMetric: 27.3951 - val_loss: 28.2916 - val_MinusLogProbMetric: 28.2916 - lr: 2.0833e-05 - 33s/epoch - 166ms/step
Epoch 724/1000
2023-10-26 12:44:18.816 
Epoch 724/1000 
	 loss: 27.3932, MinusLogProbMetric: 27.3932, val_loss: 28.2696, val_MinusLogProbMetric: 28.2696

Epoch 724: val_loss did not improve from 28.26288
196/196 - 30s - loss: 27.3932 - MinusLogProbMetric: 27.3932 - val_loss: 28.2696 - val_MinusLogProbMetric: 28.2696 - lr: 2.0833e-05 - 30s/epoch - 154ms/step
Epoch 725/1000
2023-10-26 12:44:46.111 
Epoch 725/1000 
	 loss: 27.3967, MinusLogProbMetric: 27.3967, val_loss: 28.2693, val_MinusLogProbMetric: 28.2693

Epoch 725: val_loss did not improve from 28.26288
196/196 - 27s - loss: 27.3967 - MinusLogProbMetric: 27.3967 - val_loss: 28.2693 - val_MinusLogProbMetric: 28.2693 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 726/1000
2023-10-26 12:45:12.747 
Epoch 726/1000 
	 loss: 27.3926, MinusLogProbMetric: 27.3926, val_loss: 28.2614, val_MinusLogProbMetric: 28.2614

Epoch 726: val_loss improved from 28.26288 to 28.26139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 27s - loss: 27.3926 - MinusLogProbMetric: 27.3926 - val_loss: 28.2614 - val_MinusLogProbMetric: 28.2614 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 727/1000
2023-10-26 12:45:40.840 
Epoch 727/1000 
	 loss: 27.3918, MinusLogProbMetric: 27.3918, val_loss: 28.2768, val_MinusLogProbMetric: 28.2768

Epoch 727: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3918 - MinusLogProbMetric: 27.3918 - val_loss: 28.2768 - val_MinusLogProbMetric: 28.2768 - lr: 2.0833e-05 - 28s/epoch - 141ms/step
Epoch 728/1000
2023-10-26 12:46:08.650 
Epoch 728/1000 
	 loss: 27.3901, MinusLogProbMetric: 27.3901, val_loss: 28.2784, val_MinusLogProbMetric: 28.2784

Epoch 728: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3901 - MinusLogProbMetric: 27.3901 - val_loss: 28.2784 - val_MinusLogProbMetric: 28.2784 - lr: 2.0833e-05 - 28s/epoch - 142ms/step
Epoch 729/1000
2023-10-26 12:46:36.164 
Epoch 729/1000 
	 loss: 27.3928, MinusLogProbMetric: 27.3928, val_loss: 28.2825, val_MinusLogProbMetric: 28.2825

Epoch 729: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3928 - MinusLogProbMetric: 27.3928 - val_loss: 28.2825 - val_MinusLogProbMetric: 28.2825 - lr: 2.0833e-05 - 28s/epoch - 140ms/step
Epoch 730/1000
2023-10-26 12:47:03.043 
Epoch 730/1000 
	 loss: 27.3936, MinusLogProbMetric: 27.3936, val_loss: 28.2790, val_MinusLogProbMetric: 28.2790

Epoch 730: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3936 - MinusLogProbMetric: 27.3936 - val_loss: 28.2790 - val_MinusLogProbMetric: 28.2790 - lr: 2.0833e-05 - 27s/epoch - 137ms/step
Epoch 731/1000
2023-10-26 12:47:30.043 
Epoch 731/1000 
	 loss: 27.3938, MinusLogProbMetric: 27.3938, val_loss: 28.2765, val_MinusLogProbMetric: 28.2765

Epoch 731: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3938 - MinusLogProbMetric: 27.3938 - val_loss: 28.2765 - val_MinusLogProbMetric: 28.2765 - lr: 2.0833e-05 - 27s/epoch - 138ms/step
Epoch 732/1000
2023-10-26 12:47:57.950 
Epoch 732/1000 
	 loss: 27.3945, MinusLogProbMetric: 27.3945, val_loss: 28.2836, val_MinusLogProbMetric: 28.2836

Epoch 732: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3945 - MinusLogProbMetric: 27.3945 - val_loss: 28.2836 - val_MinusLogProbMetric: 28.2836 - lr: 2.0833e-05 - 28s/epoch - 142ms/step
Epoch 733/1000
2023-10-26 12:48:25.844 
Epoch 733/1000 
	 loss: 27.3965, MinusLogProbMetric: 27.3965, val_loss: 28.2728, val_MinusLogProbMetric: 28.2728

Epoch 733: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3965 - MinusLogProbMetric: 27.3965 - val_loss: 28.2728 - val_MinusLogProbMetric: 28.2728 - lr: 2.0833e-05 - 28s/epoch - 142ms/step
Epoch 734/1000
2023-10-26 12:48:53.254 
Epoch 734/1000 
	 loss: 27.3938, MinusLogProbMetric: 27.3938, val_loss: 28.2784, val_MinusLogProbMetric: 28.2784

Epoch 734: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3938 - MinusLogProbMetric: 27.3938 - val_loss: 28.2784 - val_MinusLogProbMetric: 28.2784 - lr: 2.0833e-05 - 27s/epoch - 140ms/step
Epoch 735/1000
2023-10-26 12:49:20.145 
Epoch 735/1000 
	 loss: 27.3944, MinusLogProbMetric: 27.3944, val_loss: 28.2702, val_MinusLogProbMetric: 28.2702

Epoch 735: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3944 - MinusLogProbMetric: 27.3944 - val_loss: 28.2702 - val_MinusLogProbMetric: 28.2702 - lr: 2.0833e-05 - 27s/epoch - 137ms/step
Epoch 736/1000
2023-10-26 12:49:47.020 
Epoch 736/1000 
	 loss: 27.3932, MinusLogProbMetric: 27.3932, val_loss: 28.2813, val_MinusLogProbMetric: 28.2813

Epoch 736: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3932 - MinusLogProbMetric: 27.3932 - val_loss: 28.2813 - val_MinusLogProbMetric: 28.2813 - lr: 2.0833e-05 - 27s/epoch - 137ms/step
Epoch 737/1000
2023-10-26 12:50:14.485 
Epoch 737/1000 
	 loss: 27.3904, MinusLogProbMetric: 27.3904, val_loss: 28.2903, val_MinusLogProbMetric: 28.2903

Epoch 737: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3904 - MinusLogProbMetric: 27.3904 - val_loss: 28.2903 - val_MinusLogProbMetric: 28.2903 - lr: 2.0833e-05 - 27s/epoch - 140ms/step
Epoch 738/1000
2023-10-26 12:50:42.614 
Epoch 738/1000 
	 loss: 27.3917, MinusLogProbMetric: 27.3917, val_loss: 28.2719, val_MinusLogProbMetric: 28.2719

Epoch 738: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3917 - MinusLogProbMetric: 27.3917 - val_loss: 28.2719 - val_MinusLogProbMetric: 28.2719 - lr: 2.0833e-05 - 28s/epoch - 144ms/step
Epoch 739/1000
2023-10-26 12:51:09.691 
Epoch 739/1000 
	 loss: 27.3918, MinusLogProbMetric: 27.3918, val_loss: 28.2767, val_MinusLogProbMetric: 28.2767

Epoch 739: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3918 - MinusLogProbMetric: 27.3918 - val_loss: 28.2767 - val_MinusLogProbMetric: 28.2767 - lr: 2.0833e-05 - 27s/epoch - 138ms/step
Epoch 740/1000
2023-10-26 12:51:36.363 
Epoch 740/1000 
	 loss: 27.3947, MinusLogProbMetric: 27.3947, val_loss: 28.2896, val_MinusLogProbMetric: 28.2896

Epoch 740: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3947 - MinusLogProbMetric: 27.3947 - val_loss: 28.2896 - val_MinusLogProbMetric: 28.2896 - lr: 2.0833e-05 - 27s/epoch - 136ms/step
Epoch 741/1000
2023-10-26 12:52:03.497 
Epoch 741/1000 
	 loss: 27.3893, MinusLogProbMetric: 27.3893, val_loss: 28.2926, val_MinusLogProbMetric: 28.2926

Epoch 741: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3893 - MinusLogProbMetric: 27.3893 - val_loss: 28.2926 - val_MinusLogProbMetric: 28.2926 - lr: 2.0833e-05 - 27s/epoch - 138ms/step
Epoch 742/1000
2023-10-26 12:52:30.709 
Epoch 742/1000 
	 loss: 27.3938, MinusLogProbMetric: 27.3938, val_loss: 28.2773, val_MinusLogProbMetric: 28.2773

Epoch 742: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3938 - MinusLogProbMetric: 27.3938 - val_loss: 28.2773 - val_MinusLogProbMetric: 28.2773 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 743/1000
2023-10-26 12:53:02.608 
Epoch 743/1000 
	 loss: 27.3927, MinusLogProbMetric: 27.3927, val_loss: 28.2811, val_MinusLogProbMetric: 28.2811

Epoch 743: val_loss did not improve from 28.26139
196/196 - 32s - loss: 27.3927 - MinusLogProbMetric: 27.3927 - val_loss: 28.2811 - val_MinusLogProbMetric: 28.2811 - lr: 2.0833e-05 - 32s/epoch - 163ms/step
Epoch 744/1000
2023-10-26 12:53:35.669 
Epoch 744/1000 
	 loss: 27.3882, MinusLogProbMetric: 27.3882, val_loss: 28.2878, val_MinusLogProbMetric: 28.2878

Epoch 744: val_loss did not improve from 28.26139
196/196 - 33s - loss: 27.3882 - MinusLogProbMetric: 27.3882 - val_loss: 28.2878 - val_MinusLogProbMetric: 28.2878 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 745/1000
2023-10-26 12:54:03.856 
Epoch 745/1000 
	 loss: 27.3903, MinusLogProbMetric: 27.3903, val_loss: 28.2836, val_MinusLogProbMetric: 28.2836

Epoch 745: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3903 - MinusLogProbMetric: 27.3903 - val_loss: 28.2836 - val_MinusLogProbMetric: 28.2836 - lr: 2.0833e-05 - 28s/epoch - 144ms/step
Epoch 746/1000
2023-10-26 12:54:31.341 
Epoch 746/1000 
	 loss: 27.3906, MinusLogProbMetric: 27.3906, val_loss: 28.2943, val_MinusLogProbMetric: 28.2943

Epoch 746: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3906 - MinusLogProbMetric: 27.3906 - val_loss: 28.2943 - val_MinusLogProbMetric: 28.2943 - lr: 2.0833e-05 - 27s/epoch - 140ms/step
Epoch 747/1000
2023-10-26 12:54:58.114 
Epoch 747/1000 
	 loss: 27.3959, MinusLogProbMetric: 27.3959, val_loss: 28.2948, val_MinusLogProbMetric: 28.2948

Epoch 747: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3959 - MinusLogProbMetric: 27.3959 - val_loss: 28.2948 - val_MinusLogProbMetric: 28.2948 - lr: 2.0833e-05 - 27s/epoch - 137ms/step
Epoch 748/1000
2023-10-26 12:55:26.198 
Epoch 748/1000 
	 loss: 27.3904, MinusLogProbMetric: 27.3904, val_loss: 28.2725, val_MinusLogProbMetric: 28.2725

Epoch 748: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3904 - MinusLogProbMetric: 27.3904 - val_loss: 28.2725 - val_MinusLogProbMetric: 28.2725 - lr: 2.0833e-05 - 28s/epoch - 143ms/step
Epoch 749/1000
2023-10-26 12:55:55.215 
Epoch 749/1000 
	 loss: 27.3888, MinusLogProbMetric: 27.3888, val_loss: 28.3049, val_MinusLogProbMetric: 28.3049

Epoch 749: val_loss did not improve from 28.26139
196/196 - 29s - loss: 27.3888 - MinusLogProbMetric: 27.3888 - val_loss: 28.3049 - val_MinusLogProbMetric: 28.3049 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 750/1000
2023-10-26 12:56:27.608 
Epoch 750/1000 
	 loss: 27.3903, MinusLogProbMetric: 27.3903, val_loss: 28.2907, val_MinusLogProbMetric: 28.2907

Epoch 750: val_loss did not improve from 28.26139
196/196 - 32s - loss: 27.3903 - MinusLogProbMetric: 27.3903 - val_loss: 28.2907 - val_MinusLogProbMetric: 28.2907 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 751/1000
2023-10-26 12:56:56.029 
Epoch 751/1000 
	 loss: 27.3908, MinusLogProbMetric: 27.3908, val_loss: 28.2780, val_MinusLogProbMetric: 28.2780

Epoch 751: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3908 - MinusLogProbMetric: 27.3908 - val_loss: 28.2780 - val_MinusLogProbMetric: 28.2780 - lr: 2.0833e-05 - 28s/epoch - 145ms/step
Epoch 752/1000
2023-10-26 12:57:23.061 
Epoch 752/1000 
	 loss: 27.3900, MinusLogProbMetric: 27.3900, val_loss: 28.2801, val_MinusLogProbMetric: 28.2801

Epoch 752: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3900 - MinusLogProbMetric: 27.3900 - val_loss: 28.2801 - val_MinusLogProbMetric: 28.2801 - lr: 2.0833e-05 - 27s/epoch - 138ms/step
Epoch 753/1000
2023-10-26 12:57:50.260 
Epoch 753/1000 
	 loss: 27.3910, MinusLogProbMetric: 27.3910, val_loss: 28.2743, val_MinusLogProbMetric: 28.2743

Epoch 753: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3910 - MinusLogProbMetric: 27.3910 - val_loss: 28.2743 - val_MinusLogProbMetric: 28.2743 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 754/1000
2023-10-26 12:58:17.154 
Epoch 754/1000 
	 loss: 27.3927, MinusLogProbMetric: 27.3927, val_loss: 28.2762, val_MinusLogProbMetric: 28.2762

Epoch 754: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3927 - MinusLogProbMetric: 27.3927 - val_loss: 28.2762 - val_MinusLogProbMetric: 28.2762 - lr: 2.0833e-05 - 27s/epoch - 137ms/step
Epoch 755/1000
2023-10-26 12:58:45.089 
Epoch 755/1000 
	 loss: 27.3920, MinusLogProbMetric: 27.3920, val_loss: 28.2987, val_MinusLogProbMetric: 28.2987

Epoch 755: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3920 - MinusLogProbMetric: 27.3920 - val_loss: 28.2987 - val_MinusLogProbMetric: 28.2987 - lr: 2.0833e-05 - 28s/epoch - 143ms/step
Epoch 756/1000
2023-10-26 12:59:12.983 
Epoch 756/1000 
	 loss: 27.3889, MinusLogProbMetric: 27.3889, val_loss: 28.2639, val_MinusLogProbMetric: 28.2639

Epoch 756: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3889 - MinusLogProbMetric: 27.3889 - val_loss: 28.2639 - val_MinusLogProbMetric: 28.2639 - lr: 2.0833e-05 - 28s/epoch - 142ms/step
Epoch 757/1000
2023-10-26 12:59:40.320 
Epoch 757/1000 
	 loss: 27.3879, MinusLogProbMetric: 27.3879, val_loss: 28.2931, val_MinusLogProbMetric: 28.2931

Epoch 757: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3879 - MinusLogProbMetric: 27.3879 - val_loss: 28.2931 - val_MinusLogProbMetric: 28.2931 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 758/1000
2023-10-26 13:00:07.508 
Epoch 758/1000 
	 loss: 27.3886, MinusLogProbMetric: 27.3886, val_loss: 28.2974, val_MinusLogProbMetric: 28.2974

Epoch 758: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3886 - MinusLogProbMetric: 27.3886 - val_loss: 28.2974 - val_MinusLogProbMetric: 28.2974 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 759/1000
2023-10-26 13:00:34.604 
Epoch 759/1000 
	 loss: 27.3896, MinusLogProbMetric: 27.3896, val_loss: 28.2834, val_MinusLogProbMetric: 28.2834

Epoch 759: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3896 - MinusLogProbMetric: 27.3896 - val_loss: 28.2834 - val_MinusLogProbMetric: 28.2834 - lr: 2.0833e-05 - 27s/epoch - 138ms/step
Epoch 760/1000
2023-10-26 13:01:02.092 
Epoch 760/1000 
	 loss: 27.3865, MinusLogProbMetric: 27.3865, val_loss: 28.2838, val_MinusLogProbMetric: 28.2838

Epoch 760: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3865 - MinusLogProbMetric: 27.3865 - val_loss: 28.2838 - val_MinusLogProbMetric: 28.2838 - lr: 2.0833e-05 - 27s/epoch - 140ms/step
Epoch 761/1000
2023-10-26 13:01:35.565 
Epoch 761/1000 
	 loss: 27.3914, MinusLogProbMetric: 27.3914, val_loss: 28.2679, val_MinusLogProbMetric: 28.2679

Epoch 761: val_loss did not improve from 28.26139
196/196 - 33s - loss: 27.3914 - MinusLogProbMetric: 27.3914 - val_loss: 28.2679 - val_MinusLogProbMetric: 28.2679 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 762/1000
2023-10-26 13:02:06.176 
Epoch 762/1000 
	 loss: 27.3876, MinusLogProbMetric: 27.3876, val_loss: 28.2871, val_MinusLogProbMetric: 28.2871

Epoch 762: val_loss did not improve from 28.26139
196/196 - 31s - loss: 27.3876 - MinusLogProbMetric: 27.3876 - val_loss: 28.2871 - val_MinusLogProbMetric: 28.2871 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 763/1000
2023-10-26 13:02:35.602 
Epoch 763/1000 
	 loss: 27.3878, MinusLogProbMetric: 27.3878, val_loss: 28.2859, val_MinusLogProbMetric: 28.2859

Epoch 763: val_loss did not improve from 28.26139
196/196 - 29s - loss: 27.3878 - MinusLogProbMetric: 27.3878 - val_loss: 28.2859 - val_MinusLogProbMetric: 28.2859 - lr: 2.0833e-05 - 29s/epoch - 150ms/step
Epoch 764/1000
2023-10-26 13:03:02.754 
Epoch 764/1000 
	 loss: 27.3874, MinusLogProbMetric: 27.3874, val_loss: 28.2739, val_MinusLogProbMetric: 28.2739

Epoch 764: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3874 - MinusLogProbMetric: 27.3874 - val_loss: 28.2739 - val_MinusLogProbMetric: 28.2739 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 765/1000
2023-10-26 13:03:30.346 
Epoch 765/1000 
	 loss: 27.3936, MinusLogProbMetric: 27.3936, val_loss: 28.2805, val_MinusLogProbMetric: 28.2805

Epoch 765: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3936 - MinusLogProbMetric: 27.3936 - val_loss: 28.2805 - val_MinusLogProbMetric: 28.2805 - lr: 2.0833e-05 - 28s/epoch - 141ms/step
Epoch 766/1000
2023-10-26 13:03:57.497 
Epoch 766/1000 
	 loss: 27.3894, MinusLogProbMetric: 27.3894, val_loss: 28.2881, val_MinusLogProbMetric: 28.2881

Epoch 766: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3894 - MinusLogProbMetric: 27.3894 - val_loss: 28.2881 - val_MinusLogProbMetric: 28.2881 - lr: 2.0833e-05 - 27s/epoch - 139ms/step
Epoch 767/1000
2023-10-26 13:04:30.544 
Epoch 767/1000 
	 loss: 27.3913, MinusLogProbMetric: 27.3913, val_loss: 28.2711, val_MinusLogProbMetric: 28.2711

Epoch 767: val_loss did not improve from 28.26139
196/196 - 33s - loss: 27.3913 - MinusLogProbMetric: 27.3913 - val_loss: 28.2711 - val_MinusLogProbMetric: 28.2711 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 768/1000
2023-10-26 13:05:03.885 
Epoch 768/1000 
	 loss: 27.3896, MinusLogProbMetric: 27.3896, val_loss: 28.2709, val_MinusLogProbMetric: 28.2709

Epoch 768: val_loss did not improve from 28.26139
196/196 - 33s - loss: 27.3896 - MinusLogProbMetric: 27.3896 - val_loss: 28.2709 - val_MinusLogProbMetric: 28.2709 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 769/1000
2023-10-26 13:05:34.268 
Epoch 769/1000 
	 loss: 27.3864, MinusLogProbMetric: 27.3864, val_loss: 28.2745, val_MinusLogProbMetric: 28.2745

Epoch 769: val_loss did not improve from 28.26139
196/196 - 30s - loss: 27.3864 - MinusLogProbMetric: 27.3864 - val_loss: 28.2745 - val_MinusLogProbMetric: 28.2745 - lr: 2.0833e-05 - 30s/epoch - 155ms/step
Epoch 770/1000
2023-10-26 13:06:01.632 
Epoch 770/1000 
	 loss: 27.3902, MinusLogProbMetric: 27.3902, val_loss: 28.2637, val_MinusLogProbMetric: 28.2637

Epoch 770: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3902 - MinusLogProbMetric: 27.3902 - val_loss: 28.2637 - val_MinusLogProbMetric: 28.2637 - lr: 2.0833e-05 - 27s/epoch - 140ms/step
Epoch 771/1000
2023-10-26 13:06:28.708 
Epoch 771/1000 
	 loss: 27.3892, MinusLogProbMetric: 27.3892, val_loss: 28.2972, val_MinusLogProbMetric: 28.2972

Epoch 771: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3892 - MinusLogProbMetric: 27.3892 - val_loss: 28.2972 - val_MinusLogProbMetric: 28.2972 - lr: 2.0833e-05 - 27s/epoch - 138ms/step
Epoch 772/1000
2023-10-26 13:06:56.307 
Epoch 772/1000 
	 loss: 27.3884, MinusLogProbMetric: 27.3884, val_loss: 28.2707, val_MinusLogProbMetric: 28.2707

Epoch 772: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3884 - MinusLogProbMetric: 27.3884 - val_loss: 28.2707 - val_MinusLogProbMetric: 28.2707 - lr: 2.0833e-05 - 28s/epoch - 141ms/step
Epoch 773/1000
2023-10-26 13:07:28.424 
Epoch 773/1000 
	 loss: 27.3876, MinusLogProbMetric: 27.3876, val_loss: 28.2995, val_MinusLogProbMetric: 28.2995

Epoch 773: val_loss did not improve from 28.26139
196/196 - 32s - loss: 27.3876 - MinusLogProbMetric: 27.3876 - val_loss: 28.2995 - val_MinusLogProbMetric: 28.2995 - lr: 2.0833e-05 - 32s/epoch - 164ms/step
Epoch 774/1000
2023-10-26 13:08:01.419 
Epoch 774/1000 
	 loss: 27.3920, MinusLogProbMetric: 27.3920, val_loss: 28.2995, val_MinusLogProbMetric: 28.2995

Epoch 774: val_loss did not improve from 28.26139
196/196 - 33s - loss: 27.3920 - MinusLogProbMetric: 27.3920 - val_loss: 28.2995 - val_MinusLogProbMetric: 28.2995 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 775/1000
2023-10-26 13:08:30.508 
Epoch 775/1000 
	 loss: 27.3879, MinusLogProbMetric: 27.3879, val_loss: 28.2866, val_MinusLogProbMetric: 28.2866

Epoch 775: val_loss did not improve from 28.26139
196/196 - 29s - loss: 27.3879 - MinusLogProbMetric: 27.3879 - val_loss: 28.2866 - val_MinusLogProbMetric: 28.2866 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 776/1000
2023-10-26 13:08:57.916 
Epoch 776/1000 
	 loss: 27.3890, MinusLogProbMetric: 27.3890, val_loss: 28.2687, val_MinusLogProbMetric: 28.2687

Epoch 776: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3890 - MinusLogProbMetric: 27.3890 - val_loss: 28.2687 - val_MinusLogProbMetric: 28.2687 - lr: 2.0833e-05 - 27s/epoch - 140ms/step
Epoch 777/1000
2023-10-26 13:09:25.772 
Epoch 777/1000 
	 loss: 27.3699, MinusLogProbMetric: 27.3699, val_loss: 28.2684, val_MinusLogProbMetric: 28.2684

Epoch 777: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3699 - MinusLogProbMetric: 27.3699 - val_loss: 28.2684 - val_MinusLogProbMetric: 28.2684 - lr: 1.0417e-05 - 28s/epoch - 142ms/step
Epoch 778/1000
2023-10-26 13:09:52.991 
Epoch 778/1000 
	 loss: 27.3729, MinusLogProbMetric: 27.3729, val_loss: 28.2681, val_MinusLogProbMetric: 28.2681

Epoch 778: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3729 - MinusLogProbMetric: 27.3729 - val_loss: 28.2681 - val_MinusLogProbMetric: 28.2681 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 779/1000
2023-10-26 13:10:22.508 
Epoch 779/1000 
	 loss: 27.3700, MinusLogProbMetric: 27.3700, val_loss: 28.2744, val_MinusLogProbMetric: 28.2744

Epoch 779: val_loss did not improve from 28.26139
196/196 - 30s - loss: 27.3700 - MinusLogProbMetric: 27.3700 - val_loss: 28.2744 - val_MinusLogProbMetric: 28.2744 - lr: 1.0417e-05 - 30s/epoch - 151ms/step
Epoch 780/1000
2023-10-26 13:10:51.593 
Epoch 780/1000 
	 loss: 27.3690, MinusLogProbMetric: 27.3690, val_loss: 28.2709, val_MinusLogProbMetric: 28.2709

Epoch 780: val_loss did not improve from 28.26139
196/196 - 29s - loss: 27.3690 - MinusLogProbMetric: 27.3690 - val_loss: 28.2709 - val_MinusLogProbMetric: 28.2709 - lr: 1.0417e-05 - 29s/epoch - 148ms/step
Epoch 781/1000
2023-10-26 13:11:20.875 
Epoch 781/1000 
	 loss: 27.3694, MinusLogProbMetric: 27.3694, val_loss: 28.2627, val_MinusLogProbMetric: 28.2627

Epoch 781: val_loss did not improve from 28.26139
196/196 - 29s - loss: 27.3694 - MinusLogProbMetric: 27.3694 - val_loss: 28.2627 - val_MinusLogProbMetric: 28.2627 - lr: 1.0417e-05 - 29s/epoch - 149ms/step
Epoch 782/1000
2023-10-26 13:11:47.924 
Epoch 782/1000 
	 loss: 27.3708, MinusLogProbMetric: 27.3708, val_loss: 28.2667, val_MinusLogProbMetric: 28.2667

Epoch 782: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3708 - MinusLogProbMetric: 27.3708 - val_loss: 28.2667 - val_MinusLogProbMetric: 28.2667 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 783/1000
2023-10-26 13:12:15.446 
Epoch 783/1000 
	 loss: 27.3687, MinusLogProbMetric: 27.3687, val_loss: 28.2654, val_MinusLogProbMetric: 28.2654

Epoch 783: val_loss did not improve from 28.26139
196/196 - 28s - loss: 27.3687 - MinusLogProbMetric: 27.3687 - val_loss: 28.2654 - val_MinusLogProbMetric: 28.2654 - lr: 1.0417e-05 - 28s/epoch - 140ms/step
Epoch 784/1000
2023-10-26 13:12:42.700 
Epoch 784/1000 
	 loss: 27.3695, MinusLogProbMetric: 27.3695, val_loss: 28.2824, val_MinusLogProbMetric: 28.2824

Epoch 784: val_loss did not improve from 28.26139
196/196 - 27s - loss: 27.3695 - MinusLogProbMetric: 27.3695 - val_loss: 28.2824 - val_MinusLogProbMetric: 28.2824 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 785/1000
2023-10-26 13:13:12.325 
Epoch 785/1000 
	 loss: 27.3697, MinusLogProbMetric: 27.3697, val_loss: 28.2644, val_MinusLogProbMetric: 28.2644

Epoch 785: val_loss did not improve from 28.26139
196/196 - 30s - loss: 27.3697 - MinusLogProbMetric: 27.3697 - val_loss: 28.2644 - val_MinusLogProbMetric: 28.2644 - lr: 1.0417e-05 - 30s/epoch - 151ms/step
Epoch 786/1000
2023-10-26 13:13:45.940 
Epoch 786/1000 
	 loss: 27.3700, MinusLogProbMetric: 27.3700, val_loss: 28.2619, val_MinusLogProbMetric: 28.2619

Epoch 786: val_loss did not improve from 28.26139
196/196 - 34s - loss: 27.3700 - MinusLogProbMetric: 27.3700 - val_loss: 28.2619 - val_MinusLogProbMetric: 28.2619 - lr: 1.0417e-05 - 34s/epoch - 171ms/step
Epoch 787/1000
2023-10-26 13:14:15.642 
Epoch 787/1000 
	 loss: 27.3688, MinusLogProbMetric: 27.3688, val_loss: 28.2765, val_MinusLogProbMetric: 28.2765

Epoch 787: val_loss did not improve from 28.26139
196/196 - 30s - loss: 27.3688 - MinusLogProbMetric: 27.3688 - val_loss: 28.2765 - val_MinusLogProbMetric: 28.2765 - lr: 1.0417e-05 - 30s/epoch - 152ms/step
Epoch 788/1000
2023-10-26 13:14:42.798 
Epoch 788/1000 
	 loss: 27.3695, MinusLogProbMetric: 27.3695, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 788: val_loss improved from 28.26139 to 28.26031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 28s - loss: 27.3695 - MinusLogProbMetric: 27.3695 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 1.0417e-05 - 28s/epoch - 141ms/step
Epoch 789/1000
2023-10-26 13:15:10.254 
Epoch 789/1000 
	 loss: 27.3685, MinusLogProbMetric: 27.3685, val_loss: 28.2702, val_MinusLogProbMetric: 28.2702

Epoch 789: val_loss did not improve from 28.26031
196/196 - 27s - loss: 27.3685 - MinusLogProbMetric: 27.3685 - val_loss: 28.2702 - val_MinusLogProbMetric: 28.2702 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 790/1000
2023-10-26 13:15:37.570 
Epoch 790/1000 
	 loss: 27.3702, MinusLogProbMetric: 27.3702, val_loss: 28.2783, val_MinusLogProbMetric: 28.2783

Epoch 790: val_loss did not improve from 28.26031
196/196 - 27s - loss: 27.3702 - MinusLogProbMetric: 27.3702 - val_loss: 28.2783 - val_MinusLogProbMetric: 28.2783 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 791/1000
2023-10-26 13:16:09.272 
Epoch 791/1000 
	 loss: 27.3686, MinusLogProbMetric: 27.3686, val_loss: 28.2717, val_MinusLogProbMetric: 28.2717

Epoch 791: val_loss did not improve from 28.26031
196/196 - 32s - loss: 27.3686 - MinusLogProbMetric: 27.3686 - val_loss: 28.2717 - val_MinusLogProbMetric: 28.2717 - lr: 1.0417e-05 - 32s/epoch - 162ms/step
Epoch 792/1000
2023-10-26 13:16:42.087 
Epoch 792/1000 
	 loss: 27.3700, MinusLogProbMetric: 27.3700, val_loss: 28.2776, val_MinusLogProbMetric: 28.2776

Epoch 792: val_loss did not improve from 28.26031
196/196 - 33s - loss: 27.3700 - MinusLogProbMetric: 27.3700 - val_loss: 28.2776 - val_MinusLogProbMetric: 28.2776 - lr: 1.0417e-05 - 33s/epoch - 167ms/step
Epoch 793/1000
2023-10-26 13:17:11.925 
Epoch 793/1000 
	 loss: 27.3686, MinusLogProbMetric: 27.3686, val_loss: 28.2772, val_MinusLogProbMetric: 28.2772

Epoch 793: val_loss did not improve from 28.26031
196/196 - 30s - loss: 27.3686 - MinusLogProbMetric: 27.3686 - val_loss: 28.2772 - val_MinusLogProbMetric: 28.2772 - lr: 1.0417e-05 - 30s/epoch - 152ms/step
Epoch 794/1000
2023-10-26 13:17:38.824 
Epoch 794/1000 
	 loss: 27.3697, MinusLogProbMetric: 27.3697, val_loss: 28.2750, val_MinusLogProbMetric: 28.2750

Epoch 794: val_loss did not improve from 28.26031
196/196 - 27s - loss: 27.3697 - MinusLogProbMetric: 27.3697 - val_loss: 28.2750 - val_MinusLogProbMetric: 28.2750 - lr: 1.0417e-05 - 27s/epoch - 137ms/step
Epoch 795/1000
2023-10-26 13:18:05.939 
Epoch 795/1000 
	 loss: 27.3693, MinusLogProbMetric: 27.3693, val_loss: 28.2806, val_MinusLogProbMetric: 28.2806

Epoch 795: val_loss did not improve from 28.26031
196/196 - 27s - loss: 27.3693 - MinusLogProbMetric: 27.3693 - val_loss: 28.2806 - val_MinusLogProbMetric: 28.2806 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 796/1000
2023-10-26 13:18:32.994 
Epoch 796/1000 
	 loss: 27.3696, MinusLogProbMetric: 27.3696, val_loss: 28.2666, val_MinusLogProbMetric: 28.2666

Epoch 796: val_loss did not improve from 28.26031
196/196 - 27s - loss: 27.3696 - MinusLogProbMetric: 27.3696 - val_loss: 28.2666 - val_MinusLogProbMetric: 28.2666 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 797/1000
2023-10-26 13:19:03.808 
Epoch 797/1000 
	 loss: 27.3688, MinusLogProbMetric: 27.3688, val_loss: 28.2716, val_MinusLogProbMetric: 28.2716

Epoch 797: val_loss did not improve from 28.26031
196/196 - 31s - loss: 27.3688 - MinusLogProbMetric: 27.3688 - val_loss: 28.2716 - val_MinusLogProbMetric: 28.2716 - lr: 1.0417e-05 - 31s/epoch - 157ms/step
Epoch 798/1000
2023-10-26 13:19:35.958 
Epoch 798/1000 
	 loss: 27.3686, MinusLogProbMetric: 27.3686, val_loss: 28.2711, val_MinusLogProbMetric: 28.2711

Epoch 798: val_loss did not improve from 28.26031
196/196 - 32s - loss: 27.3686 - MinusLogProbMetric: 27.3686 - val_loss: 28.2711 - val_MinusLogProbMetric: 28.2711 - lr: 1.0417e-05 - 32s/epoch - 164ms/step
Epoch 799/1000
2023-10-26 13:20:06.070 
Epoch 799/1000 
	 loss: 27.3680, MinusLogProbMetric: 27.3680, val_loss: 28.2623, val_MinusLogProbMetric: 28.2623

Epoch 799: val_loss did not improve from 28.26031
196/196 - 30s - loss: 27.3680 - MinusLogProbMetric: 27.3680 - val_loss: 28.2623 - val_MinusLogProbMetric: 28.2623 - lr: 1.0417e-05 - 30s/epoch - 154ms/step
Epoch 800/1000
2023-10-26 13:20:34.078 
Epoch 800/1000 
	 loss: 27.3689, MinusLogProbMetric: 27.3689, val_loss: 28.2731, val_MinusLogProbMetric: 28.2731

Epoch 800: val_loss did not improve from 28.26031
196/196 - 28s - loss: 27.3689 - MinusLogProbMetric: 27.3689 - val_loss: 28.2731 - val_MinusLogProbMetric: 28.2731 - lr: 1.0417e-05 - 28s/epoch - 143ms/step
Epoch 801/1000
2023-10-26 13:21:01.152 
Epoch 801/1000 
	 loss: 27.3674, MinusLogProbMetric: 27.3674, val_loss: 28.2744, val_MinusLogProbMetric: 28.2744

Epoch 801: val_loss did not improve from 28.26031
196/196 - 27s - loss: 27.3674 - MinusLogProbMetric: 27.3674 - val_loss: 28.2744 - val_MinusLogProbMetric: 28.2744 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 802/1000
2023-10-26 13:21:29.676 
Epoch 802/1000 
	 loss: 27.3689, MinusLogProbMetric: 27.3689, val_loss: 28.2709, val_MinusLogProbMetric: 28.2709

Epoch 802: val_loss did not improve from 28.26031
196/196 - 29s - loss: 27.3689 - MinusLogProbMetric: 27.3689 - val_loss: 28.2709 - val_MinusLogProbMetric: 28.2709 - lr: 1.0417e-05 - 29s/epoch - 146ms/step
Epoch 803/1000
2023-10-26 13:21:58.654 
Epoch 803/1000 
	 loss: 27.3693, MinusLogProbMetric: 27.3693, val_loss: 28.2684, val_MinusLogProbMetric: 28.2684

Epoch 803: val_loss did not improve from 28.26031
196/196 - 29s - loss: 27.3693 - MinusLogProbMetric: 27.3693 - val_loss: 28.2684 - val_MinusLogProbMetric: 28.2684 - lr: 1.0417e-05 - 29s/epoch - 148ms/step
Epoch 804/1000
2023-10-26 13:22:31.230 
Epoch 804/1000 
	 loss: 27.3683, MinusLogProbMetric: 27.3683, val_loss: 28.2698, val_MinusLogProbMetric: 28.2698

Epoch 804: val_loss did not improve from 28.26031
196/196 - 33s - loss: 27.3683 - MinusLogProbMetric: 27.3683 - val_loss: 28.2698 - val_MinusLogProbMetric: 28.2698 - lr: 1.0417e-05 - 33s/epoch - 166ms/step
Epoch 805/1000
2023-10-26 13:23:02.485 
Epoch 805/1000 
	 loss: 27.3686, MinusLogProbMetric: 27.3686, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 805: val_loss improved from 28.26031 to 28.26026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 32s - loss: 27.3686 - MinusLogProbMetric: 27.3686 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 1.0417e-05 - 32s/epoch - 161ms/step
Epoch 806/1000
2023-10-26 13:23:29.676 
Epoch 806/1000 
	 loss: 27.3691, MinusLogProbMetric: 27.3691, val_loss: 28.2654, val_MinusLogProbMetric: 28.2654

Epoch 806: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3691 - MinusLogProbMetric: 27.3691 - val_loss: 28.2654 - val_MinusLogProbMetric: 28.2654 - lr: 1.0417e-05 - 27s/epoch - 137ms/step
Epoch 807/1000
2023-10-26 13:23:56.919 
Epoch 807/1000 
	 loss: 27.3683, MinusLogProbMetric: 27.3683, val_loss: 28.2628, val_MinusLogProbMetric: 28.2628

Epoch 807: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3683 - MinusLogProbMetric: 27.3683 - val_loss: 28.2628 - val_MinusLogProbMetric: 28.2628 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 808/1000
2023-10-26 13:24:24.122 
Epoch 808/1000 
	 loss: 27.3690, MinusLogProbMetric: 27.3690, val_loss: 28.2912, val_MinusLogProbMetric: 28.2912

Epoch 808: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3690 - MinusLogProbMetric: 27.3690 - val_loss: 28.2912 - val_MinusLogProbMetric: 28.2912 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 809/1000
2023-10-26 13:24:52.147 
Epoch 809/1000 
	 loss: 27.3689, MinusLogProbMetric: 27.3689, val_loss: 28.2642, val_MinusLogProbMetric: 28.2642

Epoch 809: val_loss did not improve from 28.26026
196/196 - 28s - loss: 27.3689 - MinusLogProbMetric: 27.3689 - val_loss: 28.2642 - val_MinusLogProbMetric: 28.2642 - lr: 1.0417e-05 - 28s/epoch - 143ms/step
Epoch 810/1000
2023-10-26 13:25:23.231 
Epoch 810/1000 
	 loss: 27.3694, MinusLogProbMetric: 27.3694, val_loss: 28.2705, val_MinusLogProbMetric: 28.2705

Epoch 810: val_loss did not improve from 28.26026
196/196 - 31s - loss: 27.3694 - MinusLogProbMetric: 27.3694 - val_loss: 28.2705 - val_MinusLogProbMetric: 28.2705 - lr: 1.0417e-05 - 31s/epoch - 159ms/step
Epoch 811/1000
2023-10-26 13:25:56.722 
Epoch 811/1000 
	 loss: 27.3673, MinusLogProbMetric: 27.3673, val_loss: 28.2759, val_MinusLogProbMetric: 28.2759

Epoch 811: val_loss did not improve from 28.26026
196/196 - 33s - loss: 27.3673 - MinusLogProbMetric: 27.3673 - val_loss: 28.2759 - val_MinusLogProbMetric: 28.2759 - lr: 1.0417e-05 - 33s/epoch - 171ms/step
Epoch 812/1000
2023-10-26 13:26:23.955 
Epoch 812/1000 
	 loss: 27.3668, MinusLogProbMetric: 27.3668, val_loss: 28.2663, val_MinusLogProbMetric: 28.2663

Epoch 812: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3668 - MinusLogProbMetric: 27.3668 - val_loss: 28.2663 - val_MinusLogProbMetric: 28.2663 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 813/1000
2023-10-26 13:26:51.010 
Epoch 813/1000 
	 loss: 27.3692, MinusLogProbMetric: 27.3692, val_loss: 28.2705, val_MinusLogProbMetric: 28.2705

Epoch 813: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3692 - MinusLogProbMetric: 27.3692 - val_loss: 28.2705 - val_MinusLogProbMetric: 28.2705 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 814/1000
2023-10-26 13:27:17.843 
Epoch 814/1000 
	 loss: 27.3696, MinusLogProbMetric: 27.3696, val_loss: 28.2605, val_MinusLogProbMetric: 28.2605

Epoch 814: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3696 - MinusLogProbMetric: 27.3696 - val_loss: 28.2605 - val_MinusLogProbMetric: 28.2605 - lr: 1.0417e-05 - 27s/epoch - 137ms/step
Epoch 815/1000
2023-10-26 13:27:45.019 
Epoch 815/1000 
	 loss: 27.3697, MinusLogProbMetric: 27.3697, val_loss: 28.2733, val_MinusLogProbMetric: 28.2733

Epoch 815: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3697 - MinusLogProbMetric: 27.3697 - val_loss: 28.2733 - val_MinusLogProbMetric: 28.2733 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 816/1000
2023-10-26 13:28:15.545 
Epoch 816/1000 
	 loss: 27.3674, MinusLogProbMetric: 27.3674, val_loss: 28.2720, val_MinusLogProbMetric: 28.2720

Epoch 816: val_loss did not improve from 28.26026
196/196 - 31s - loss: 27.3674 - MinusLogProbMetric: 27.3674 - val_loss: 28.2720 - val_MinusLogProbMetric: 28.2720 - lr: 1.0417e-05 - 31s/epoch - 156ms/step
Epoch 817/1000
2023-10-26 13:28:49.563 
Epoch 817/1000 
	 loss: 27.3683, MinusLogProbMetric: 27.3683, val_loss: 28.2694, val_MinusLogProbMetric: 28.2694

Epoch 817: val_loss did not improve from 28.26026
196/196 - 34s - loss: 27.3683 - MinusLogProbMetric: 27.3683 - val_loss: 28.2694 - val_MinusLogProbMetric: 28.2694 - lr: 1.0417e-05 - 34s/epoch - 174ms/step
Epoch 818/1000
2023-10-26 13:29:19.120 
Epoch 818/1000 
	 loss: 27.3691, MinusLogProbMetric: 27.3691, val_loss: 28.2773, val_MinusLogProbMetric: 28.2773

Epoch 818: val_loss did not improve from 28.26026
196/196 - 30s - loss: 27.3691 - MinusLogProbMetric: 27.3691 - val_loss: 28.2773 - val_MinusLogProbMetric: 28.2773 - lr: 1.0417e-05 - 30s/epoch - 151ms/step
Epoch 819/1000
2023-10-26 13:29:46.648 
Epoch 819/1000 
	 loss: 27.3670, MinusLogProbMetric: 27.3670, val_loss: 28.2672, val_MinusLogProbMetric: 28.2672

Epoch 819: val_loss did not improve from 28.26026
196/196 - 28s - loss: 27.3670 - MinusLogProbMetric: 27.3670 - val_loss: 28.2672 - val_MinusLogProbMetric: 28.2672 - lr: 1.0417e-05 - 28s/epoch - 140ms/step
Epoch 820/1000
2023-10-26 13:30:13.202 
Epoch 820/1000 
	 loss: 27.3666, MinusLogProbMetric: 27.3666, val_loss: 28.2697, val_MinusLogProbMetric: 28.2697

Epoch 820: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3666 - MinusLogProbMetric: 27.3666 - val_loss: 28.2697 - val_MinusLogProbMetric: 28.2697 - lr: 1.0417e-05 - 27s/epoch - 135ms/step
Epoch 821/1000
2023-10-26 13:30:40.610 
Epoch 821/1000 
	 loss: 27.3664, MinusLogProbMetric: 27.3664, val_loss: 28.2709, val_MinusLogProbMetric: 28.2709

Epoch 821: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3664 - MinusLogProbMetric: 27.3664 - val_loss: 28.2709 - val_MinusLogProbMetric: 28.2709 - lr: 1.0417e-05 - 27s/epoch - 140ms/step
Epoch 822/1000
2023-10-26 13:31:08.169 
Epoch 822/1000 
	 loss: 27.3662, MinusLogProbMetric: 27.3662, val_loss: 28.2740, val_MinusLogProbMetric: 28.2740

Epoch 822: val_loss did not improve from 28.26026
196/196 - 28s - loss: 27.3662 - MinusLogProbMetric: 27.3662 - val_loss: 28.2740 - val_MinusLogProbMetric: 28.2740 - lr: 1.0417e-05 - 28s/epoch - 141ms/step
Epoch 823/1000
2023-10-26 13:31:39.354 
Epoch 823/1000 
	 loss: 27.3691, MinusLogProbMetric: 27.3691, val_loss: 28.2680, val_MinusLogProbMetric: 28.2680

Epoch 823: val_loss did not improve from 28.26026
196/196 - 31s - loss: 27.3691 - MinusLogProbMetric: 27.3691 - val_loss: 28.2680 - val_MinusLogProbMetric: 28.2680 - lr: 1.0417e-05 - 31s/epoch - 159ms/step
Epoch 824/1000
2023-10-26 13:32:11.311 
Epoch 824/1000 
	 loss: 27.3668, MinusLogProbMetric: 27.3668, val_loss: 28.2751, val_MinusLogProbMetric: 28.2751

Epoch 824: val_loss did not improve from 28.26026
196/196 - 32s - loss: 27.3668 - MinusLogProbMetric: 27.3668 - val_loss: 28.2751 - val_MinusLogProbMetric: 28.2751 - lr: 1.0417e-05 - 32s/epoch - 163ms/step
Epoch 825/1000
2023-10-26 13:32:38.296 
Epoch 825/1000 
	 loss: 27.3673, MinusLogProbMetric: 27.3673, val_loss: 28.2714, val_MinusLogProbMetric: 28.2714

Epoch 825: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3673 - MinusLogProbMetric: 27.3673 - val_loss: 28.2714 - val_MinusLogProbMetric: 28.2714 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 826/1000
2023-10-26 13:33:05.833 
Epoch 826/1000 
	 loss: 27.3682, MinusLogProbMetric: 27.3682, val_loss: 28.2698, val_MinusLogProbMetric: 28.2698

Epoch 826: val_loss did not improve from 28.26026
196/196 - 28s - loss: 27.3682 - MinusLogProbMetric: 27.3682 - val_loss: 28.2698 - val_MinusLogProbMetric: 28.2698 - lr: 1.0417e-05 - 28s/epoch - 140ms/step
Epoch 827/1000
2023-10-26 13:33:33.030 
Epoch 827/1000 
	 loss: 27.3662, MinusLogProbMetric: 27.3662, val_loss: 28.2689, val_MinusLogProbMetric: 28.2689

Epoch 827: val_loss did not improve from 28.26026
196/196 - 27s - loss: 27.3662 - MinusLogProbMetric: 27.3662 - val_loss: 28.2689 - val_MinusLogProbMetric: 28.2689 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 828/1000
2023-10-26 13:34:00.625 
Epoch 828/1000 
	 loss: 27.3680, MinusLogProbMetric: 27.3680, val_loss: 28.2816, val_MinusLogProbMetric: 28.2816

Epoch 828: val_loss did not improve from 28.26026
196/196 - 28s - loss: 27.3680 - MinusLogProbMetric: 27.3680 - val_loss: 28.2816 - val_MinusLogProbMetric: 28.2816 - lr: 1.0417e-05 - 28s/epoch - 141ms/step
Epoch 829/1000
2023-10-26 13:34:29.273 
Epoch 829/1000 
	 loss: 27.3669, MinusLogProbMetric: 27.3669, val_loss: 28.2617, val_MinusLogProbMetric: 28.2617

Epoch 829: val_loss did not improve from 28.26026
196/196 - 29s - loss: 27.3669 - MinusLogProbMetric: 27.3669 - val_loss: 28.2617 - val_MinusLogProbMetric: 28.2617 - lr: 1.0417e-05 - 29s/epoch - 146ms/step
Epoch 830/1000
2023-10-26 13:34:59.930 
Epoch 830/1000 
	 loss: 27.3664, MinusLogProbMetric: 27.3664, val_loss: 28.2799, val_MinusLogProbMetric: 28.2799

Epoch 830: val_loss did not improve from 28.26026
196/196 - 31s - loss: 27.3664 - MinusLogProbMetric: 27.3664 - val_loss: 28.2799 - val_MinusLogProbMetric: 28.2799 - lr: 1.0417e-05 - 31s/epoch - 156ms/step
Epoch 831/1000
2023-10-26 13:35:31.063 
Epoch 831/1000 
	 loss: 27.3670, MinusLogProbMetric: 27.3670, val_loss: 28.2591, val_MinusLogProbMetric: 28.2591

Epoch 831: val_loss improved from 28.26026 to 28.25911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 32s - loss: 27.3670 - MinusLogProbMetric: 27.3670 - val_loss: 28.2591 - val_MinusLogProbMetric: 28.2591 - lr: 1.0417e-05 - 32s/epoch - 161ms/step
Epoch 832/1000
2023-10-26 13:35:58.504 
Epoch 832/1000 
	 loss: 27.3675, MinusLogProbMetric: 27.3675, val_loss: 28.2621, val_MinusLogProbMetric: 28.2621

Epoch 832: val_loss did not improve from 28.25911
196/196 - 27s - loss: 27.3675 - MinusLogProbMetric: 27.3675 - val_loss: 28.2621 - val_MinusLogProbMetric: 28.2621 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 833/1000
2023-10-26 13:36:25.632 
Epoch 833/1000 
	 loss: 27.3676, MinusLogProbMetric: 27.3676, val_loss: 28.2725, val_MinusLogProbMetric: 28.2725

Epoch 833: val_loss did not improve from 28.25911
196/196 - 27s - loss: 27.3676 - MinusLogProbMetric: 27.3676 - val_loss: 28.2725 - val_MinusLogProbMetric: 28.2725 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 834/1000
2023-10-26 13:36:52.172 
Epoch 834/1000 
	 loss: 27.3676, MinusLogProbMetric: 27.3676, val_loss: 28.2644, val_MinusLogProbMetric: 28.2644

Epoch 834: val_loss did not improve from 28.25911
196/196 - 27s - loss: 27.3676 - MinusLogProbMetric: 27.3676 - val_loss: 28.2644 - val_MinusLogProbMetric: 28.2644 - lr: 1.0417e-05 - 27s/epoch - 135ms/step
Epoch 835/1000
2023-10-26 13:37:21.846 
Epoch 835/1000 
	 loss: 27.3662, MinusLogProbMetric: 27.3662, val_loss: 28.2846, val_MinusLogProbMetric: 28.2846

Epoch 835: val_loss did not improve from 28.25911
196/196 - 30s - loss: 27.3662 - MinusLogProbMetric: 27.3662 - val_loss: 28.2846 - val_MinusLogProbMetric: 28.2846 - lr: 1.0417e-05 - 30s/epoch - 151ms/step
Epoch 836/1000
2023-10-26 13:37:55.809 
Epoch 836/1000 
	 loss: 27.3673, MinusLogProbMetric: 27.3673, val_loss: 28.2708, val_MinusLogProbMetric: 28.2708

Epoch 836: val_loss did not improve from 28.25911
196/196 - 34s - loss: 27.3673 - MinusLogProbMetric: 27.3673 - val_loss: 28.2708 - val_MinusLogProbMetric: 28.2708 - lr: 1.0417e-05 - 34s/epoch - 173ms/step
Epoch 837/1000
2023-10-26 13:38:28.270 
Epoch 837/1000 
	 loss: 27.3670, MinusLogProbMetric: 27.3670, val_loss: 28.2647, val_MinusLogProbMetric: 28.2647

Epoch 837: val_loss did not improve from 28.25911
196/196 - 32s - loss: 27.3670 - MinusLogProbMetric: 27.3670 - val_loss: 28.2647 - val_MinusLogProbMetric: 28.2647 - lr: 1.0417e-05 - 32s/epoch - 166ms/step
Epoch 838/1000
2023-10-26 13:38:57.638 
Epoch 838/1000 
	 loss: 27.3662, MinusLogProbMetric: 27.3662, val_loss: 28.2764, val_MinusLogProbMetric: 28.2764

Epoch 838: val_loss did not improve from 28.25911
196/196 - 29s - loss: 27.3662 - MinusLogProbMetric: 27.3662 - val_loss: 28.2764 - val_MinusLogProbMetric: 28.2764 - lr: 1.0417e-05 - 29s/epoch - 150ms/step
Epoch 839/1000
2023-10-26 13:39:25.200 
Epoch 839/1000 
	 loss: 27.3669, MinusLogProbMetric: 27.3669, val_loss: 28.2693, val_MinusLogProbMetric: 28.2693

Epoch 839: val_loss did not improve from 28.25911
196/196 - 28s - loss: 27.3669 - MinusLogProbMetric: 27.3669 - val_loss: 28.2693 - val_MinusLogProbMetric: 28.2693 - lr: 1.0417e-05 - 28s/epoch - 141ms/step
Epoch 840/1000
2023-10-26 13:39:52.457 
Epoch 840/1000 
	 loss: 27.3655, MinusLogProbMetric: 27.3655, val_loss: 28.2731, val_MinusLogProbMetric: 28.2731

Epoch 840: val_loss did not improve from 28.25911
196/196 - 27s - loss: 27.3655 - MinusLogProbMetric: 27.3655 - val_loss: 28.2731 - val_MinusLogProbMetric: 28.2731 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 841/1000
2023-10-26 13:40:21.107 
Epoch 841/1000 
	 loss: 27.3676, MinusLogProbMetric: 27.3676, val_loss: 28.2633, val_MinusLogProbMetric: 28.2633

Epoch 841: val_loss did not improve from 28.25911
196/196 - 29s - loss: 27.3676 - MinusLogProbMetric: 27.3676 - val_loss: 28.2633 - val_MinusLogProbMetric: 28.2633 - lr: 1.0417e-05 - 29s/epoch - 146ms/step
Epoch 842/1000
2023-10-26 13:40:53.986 
Epoch 842/1000 
	 loss: 27.3658, MinusLogProbMetric: 27.3658, val_loss: 28.2711, val_MinusLogProbMetric: 28.2711

Epoch 842: val_loss did not improve from 28.25911
196/196 - 33s - loss: 27.3658 - MinusLogProbMetric: 27.3658 - val_loss: 28.2711 - val_MinusLogProbMetric: 28.2711 - lr: 1.0417e-05 - 33s/epoch - 168ms/step
Epoch 843/1000
2023-10-26 13:41:26.939 
Epoch 843/1000 
	 loss: 27.3645, MinusLogProbMetric: 27.3645, val_loss: 28.2655, val_MinusLogProbMetric: 28.2655

Epoch 843: val_loss did not improve from 28.25911
196/196 - 33s - loss: 27.3645 - MinusLogProbMetric: 27.3645 - val_loss: 28.2655 - val_MinusLogProbMetric: 28.2655 - lr: 1.0417e-05 - 33s/epoch - 168ms/step
Epoch 844/1000
2023-10-26 13:41:57.203 
Epoch 844/1000 
	 loss: 27.3680, MinusLogProbMetric: 27.3680, val_loss: 28.2667, val_MinusLogProbMetric: 28.2667

Epoch 844: val_loss did not improve from 28.25911
196/196 - 30s - loss: 27.3680 - MinusLogProbMetric: 27.3680 - val_loss: 28.2667 - val_MinusLogProbMetric: 28.2667 - lr: 1.0417e-05 - 30s/epoch - 154ms/step
Epoch 845/1000
2023-10-26 13:42:26.706 
Epoch 845/1000 
	 loss: 27.3655, MinusLogProbMetric: 27.3655, val_loss: 28.2670, val_MinusLogProbMetric: 28.2670

Epoch 845: val_loss did not improve from 28.25911
196/196 - 29s - loss: 27.3655 - MinusLogProbMetric: 27.3655 - val_loss: 28.2670 - val_MinusLogProbMetric: 28.2670 - lr: 1.0417e-05 - 29s/epoch - 151ms/step
Epoch 846/1000
2023-10-26 13:42:55.043 
Epoch 846/1000 
	 loss: 27.3672, MinusLogProbMetric: 27.3672, val_loss: 28.2673, val_MinusLogProbMetric: 28.2673

Epoch 846: val_loss did not improve from 28.25911
196/196 - 28s - loss: 27.3672 - MinusLogProbMetric: 27.3672 - val_loss: 28.2673 - val_MinusLogProbMetric: 28.2673 - lr: 1.0417e-05 - 28s/epoch - 145ms/step
Epoch 847/1000
2023-10-26 13:43:25.445 
Epoch 847/1000 
	 loss: 27.3661, MinusLogProbMetric: 27.3661, val_loss: 28.2825, val_MinusLogProbMetric: 28.2825

Epoch 847: val_loss did not improve from 28.25911
196/196 - 30s - loss: 27.3661 - MinusLogProbMetric: 27.3661 - val_loss: 28.2825 - val_MinusLogProbMetric: 28.2825 - lr: 1.0417e-05 - 30s/epoch - 155ms/step
Epoch 848/1000
2023-10-26 13:43:53.723 
Epoch 848/1000 
	 loss: 27.3681, MinusLogProbMetric: 27.3681, val_loss: 28.2824, val_MinusLogProbMetric: 28.2824

Epoch 848: val_loss did not improve from 28.25911
196/196 - 28s - loss: 27.3681 - MinusLogProbMetric: 27.3681 - val_loss: 28.2824 - val_MinusLogProbMetric: 28.2824 - lr: 1.0417e-05 - 28s/epoch - 144ms/step
Epoch 849/1000
2023-10-26 13:44:26.408 
Epoch 849/1000 
	 loss: 27.3664, MinusLogProbMetric: 27.3664, val_loss: 28.2668, val_MinusLogProbMetric: 28.2668

Epoch 849: val_loss did not improve from 28.25911
196/196 - 33s - loss: 27.3664 - MinusLogProbMetric: 27.3664 - val_loss: 28.2668 - val_MinusLogProbMetric: 28.2668 - lr: 1.0417e-05 - 33s/epoch - 167ms/step
Epoch 850/1000
2023-10-26 13:44:58.747 
Epoch 850/1000 
	 loss: 27.3664, MinusLogProbMetric: 27.3664, val_loss: 28.2780, val_MinusLogProbMetric: 28.2780

Epoch 850: val_loss did not improve from 28.25911
196/196 - 32s - loss: 27.3664 - MinusLogProbMetric: 27.3664 - val_loss: 28.2780 - val_MinusLogProbMetric: 28.2780 - lr: 1.0417e-05 - 32s/epoch - 165ms/step
Epoch 851/1000
2023-10-26 13:45:27.600 
Epoch 851/1000 
	 loss: 27.3650, MinusLogProbMetric: 27.3650, val_loss: 28.2653, val_MinusLogProbMetric: 28.2653

Epoch 851: val_loss did not improve from 28.25911
196/196 - 29s - loss: 27.3650 - MinusLogProbMetric: 27.3650 - val_loss: 28.2653 - val_MinusLogProbMetric: 28.2653 - lr: 1.0417e-05 - 29s/epoch - 147ms/step
Epoch 852/1000
2023-10-26 13:45:55.087 
Epoch 852/1000 
	 loss: 27.3675, MinusLogProbMetric: 27.3675, val_loss: 28.2685, val_MinusLogProbMetric: 28.2685

Epoch 852: val_loss did not improve from 28.25911
196/196 - 27s - loss: 27.3675 - MinusLogProbMetric: 27.3675 - val_loss: 28.2685 - val_MinusLogProbMetric: 28.2685 - lr: 1.0417e-05 - 27s/epoch - 140ms/step
Epoch 853/1000
2023-10-26 13:46:22.424 
Epoch 853/1000 
	 loss: 27.3665, MinusLogProbMetric: 27.3665, val_loss: 28.2728, val_MinusLogProbMetric: 28.2728

Epoch 853: val_loss did not improve from 28.25911
196/196 - 27s - loss: 27.3665 - MinusLogProbMetric: 27.3665 - val_loss: 28.2728 - val_MinusLogProbMetric: 28.2728 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 854/1000
2023-10-26 13:46:50.070 
Epoch 854/1000 
	 loss: 27.3678, MinusLogProbMetric: 27.3678, val_loss: 28.2673, val_MinusLogProbMetric: 28.2673

Epoch 854: val_loss did not improve from 28.25911
196/196 - 28s - loss: 27.3678 - MinusLogProbMetric: 27.3678 - val_loss: 28.2673 - val_MinusLogProbMetric: 28.2673 - lr: 1.0417e-05 - 28s/epoch - 141ms/step
Epoch 855/1000
2023-10-26 13:47:23.087 
Epoch 855/1000 
	 loss: 27.3659, MinusLogProbMetric: 27.3659, val_loss: 28.2845, val_MinusLogProbMetric: 28.2845

Epoch 855: val_loss did not improve from 28.25911
196/196 - 33s - loss: 27.3659 - MinusLogProbMetric: 27.3659 - val_loss: 28.2845 - val_MinusLogProbMetric: 28.2845 - lr: 1.0417e-05 - 33s/epoch - 168ms/step
Epoch 856/1000
2023-10-26 13:47:57.591 
Epoch 856/1000 
	 loss: 27.3656, MinusLogProbMetric: 27.3656, val_loss: 28.2631, val_MinusLogProbMetric: 28.2631

Epoch 856: val_loss did not improve from 28.25911
196/196 - 34s - loss: 27.3656 - MinusLogProbMetric: 27.3656 - val_loss: 28.2631 - val_MinusLogProbMetric: 28.2631 - lr: 1.0417e-05 - 34s/epoch - 176ms/step
Epoch 857/1000
2023-10-26 13:48:29.991 
Epoch 857/1000 
	 loss: 27.3642, MinusLogProbMetric: 27.3642, val_loss: 28.2563, val_MinusLogProbMetric: 28.2563

Epoch 857: val_loss improved from 28.25911 to 28.25634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 33s - loss: 27.3642 - MinusLogProbMetric: 27.3642 - val_loss: 28.2563 - val_MinusLogProbMetric: 28.2563 - lr: 1.0417e-05 - 33s/epoch - 168ms/step
Epoch 858/1000
2023-10-26 13:48:58.872 
Epoch 858/1000 
	 loss: 27.3651, MinusLogProbMetric: 27.3651, val_loss: 28.2839, val_MinusLogProbMetric: 28.2839

Epoch 858: val_loss did not improve from 28.25634
196/196 - 28s - loss: 27.3651 - MinusLogProbMetric: 27.3651 - val_loss: 28.2839 - val_MinusLogProbMetric: 28.2839 - lr: 1.0417e-05 - 28s/epoch - 145ms/step
Epoch 859/1000
2023-10-26 13:49:26.319 
Epoch 859/1000 
	 loss: 27.3657, MinusLogProbMetric: 27.3657, val_loss: 28.2747, val_MinusLogProbMetric: 28.2747

Epoch 859: val_loss did not improve from 28.25634
196/196 - 27s - loss: 27.3657 - MinusLogProbMetric: 27.3657 - val_loss: 28.2747 - val_MinusLogProbMetric: 28.2747 - lr: 1.0417e-05 - 27s/epoch - 140ms/step
Epoch 860/1000
2023-10-26 13:49:54.033 
Epoch 860/1000 
	 loss: 27.3652, MinusLogProbMetric: 27.3652, val_loss: 28.2671, val_MinusLogProbMetric: 28.2671

Epoch 860: val_loss did not improve from 28.25634
196/196 - 28s - loss: 27.3652 - MinusLogProbMetric: 27.3652 - val_loss: 28.2671 - val_MinusLogProbMetric: 28.2671 - lr: 1.0417e-05 - 28s/epoch - 141ms/step
Epoch 861/1000
2023-10-26 13:50:23.372 
Epoch 861/1000 
	 loss: 27.3638, MinusLogProbMetric: 27.3638, val_loss: 28.2730, val_MinusLogProbMetric: 28.2730

Epoch 861: val_loss did not improve from 28.25634
196/196 - 29s - loss: 27.3638 - MinusLogProbMetric: 27.3638 - val_loss: 28.2730 - val_MinusLogProbMetric: 28.2730 - lr: 1.0417e-05 - 29s/epoch - 150ms/step
Epoch 862/1000
2023-10-26 13:50:54.441 
Epoch 862/1000 
	 loss: 27.3645, MinusLogProbMetric: 27.3645, val_loss: 28.2715, val_MinusLogProbMetric: 28.2715

Epoch 862: val_loss did not improve from 28.25634
196/196 - 31s - loss: 27.3645 - MinusLogProbMetric: 27.3645 - val_loss: 28.2715 - val_MinusLogProbMetric: 28.2715 - lr: 1.0417e-05 - 31s/epoch - 159ms/step
Epoch 863/1000
2023-10-26 13:51:28.522 
Epoch 863/1000 
	 loss: 27.3662, MinusLogProbMetric: 27.3662, val_loss: 28.2591, val_MinusLogProbMetric: 28.2591

Epoch 863: val_loss did not improve from 28.25634
196/196 - 34s - loss: 27.3662 - MinusLogProbMetric: 27.3662 - val_loss: 28.2591 - val_MinusLogProbMetric: 28.2591 - lr: 1.0417e-05 - 34s/epoch - 174ms/step
Epoch 864/1000
2023-10-26 13:52:00.070 
Epoch 864/1000 
	 loss: 27.3655, MinusLogProbMetric: 27.3655, val_loss: 28.2880, val_MinusLogProbMetric: 28.2880

Epoch 864: val_loss did not improve from 28.25634
196/196 - 32s - loss: 27.3655 - MinusLogProbMetric: 27.3655 - val_loss: 28.2880 - val_MinusLogProbMetric: 28.2880 - lr: 1.0417e-05 - 32s/epoch - 161ms/step
Epoch 865/1000
2023-10-26 13:52:28.248 
Epoch 865/1000 
	 loss: 27.3643, MinusLogProbMetric: 27.3643, val_loss: 28.2664, val_MinusLogProbMetric: 28.2664

Epoch 865: val_loss did not improve from 28.25634
196/196 - 28s - loss: 27.3643 - MinusLogProbMetric: 27.3643 - val_loss: 28.2664 - val_MinusLogProbMetric: 28.2664 - lr: 1.0417e-05 - 28s/epoch - 144ms/step
Epoch 866/1000
2023-10-26 13:52:55.301 
Epoch 866/1000 
	 loss: 27.3648, MinusLogProbMetric: 27.3648, val_loss: 28.2792, val_MinusLogProbMetric: 28.2792

Epoch 866: val_loss did not improve from 28.25634
196/196 - 27s - loss: 27.3648 - MinusLogProbMetric: 27.3648 - val_loss: 28.2792 - val_MinusLogProbMetric: 28.2792 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 867/1000
2023-10-26 13:53:21.864 
Epoch 867/1000 
	 loss: 27.3644, MinusLogProbMetric: 27.3644, val_loss: 28.2684, val_MinusLogProbMetric: 28.2684

Epoch 867: val_loss did not improve from 28.25634
196/196 - 27s - loss: 27.3644 - MinusLogProbMetric: 27.3644 - val_loss: 28.2684 - val_MinusLogProbMetric: 28.2684 - lr: 1.0417e-05 - 27s/epoch - 136ms/step
Epoch 868/1000
2023-10-26 13:53:49.094 
Epoch 868/1000 
	 loss: 27.3663, MinusLogProbMetric: 27.3663, val_loss: 28.2749, val_MinusLogProbMetric: 28.2749

Epoch 868: val_loss did not improve from 28.25634
196/196 - 27s - loss: 27.3663 - MinusLogProbMetric: 27.3663 - val_loss: 28.2749 - val_MinusLogProbMetric: 28.2749 - lr: 1.0417e-05 - 27s/epoch - 139ms/step
Epoch 869/1000
2023-10-26 13:54:22.513 
Epoch 869/1000 
	 loss: 27.3652, MinusLogProbMetric: 27.3652, val_loss: 28.2752, val_MinusLogProbMetric: 28.2752

Epoch 869: val_loss did not improve from 28.25634
196/196 - 33s - loss: 27.3652 - MinusLogProbMetric: 27.3652 - val_loss: 28.2752 - val_MinusLogProbMetric: 28.2752 - lr: 1.0417e-05 - 33s/epoch - 170ms/step
Epoch 870/1000
2023-10-26 13:54:56.215 
Epoch 870/1000 
	 loss: 27.3649, MinusLogProbMetric: 27.3649, val_loss: 28.2706, val_MinusLogProbMetric: 28.2706

Epoch 870: val_loss did not improve from 28.25634
196/196 - 34s - loss: 27.3649 - MinusLogProbMetric: 27.3649 - val_loss: 28.2706 - val_MinusLogProbMetric: 28.2706 - lr: 1.0417e-05 - 34s/epoch - 172ms/step
Epoch 871/1000
2023-10-26 13:55:25.426 
Epoch 871/1000 
	 loss: 27.3665, MinusLogProbMetric: 27.3665, val_loss: 28.2728, val_MinusLogProbMetric: 28.2728

Epoch 871: val_loss did not improve from 28.25634
196/196 - 29s - loss: 27.3665 - MinusLogProbMetric: 27.3665 - val_loss: 28.2728 - val_MinusLogProbMetric: 28.2728 - lr: 1.0417e-05 - 29s/epoch - 149ms/step
Epoch 872/1000
2023-10-26 13:55:52.456 
Epoch 872/1000 
	 loss: 27.3651, MinusLogProbMetric: 27.3651, val_loss: 28.2651, val_MinusLogProbMetric: 28.2651

Epoch 872: val_loss did not improve from 28.25634
196/196 - 27s - loss: 27.3651 - MinusLogProbMetric: 27.3651 - val_loss: 28.2651 - val_MinusLogProbMetric: 28.2651 - lr: 1.0417e-05 - 27s/epoch - 138ms/step
Epoch 873/1000
2023-10-26 13:56:21.853 
Epoch 873/1000 
	 loss: 27.3675, MinusLogProbMetric: 27.3675, val_loss: 28.2886, val_MinusLogProbMetric: 28.2886

Epoch 873: val_loss did not improve from 28.25634
196/196 - 29s - loss: 27.3675 - MinusLogProbMetric: 27.3675 - val_loss: 28.2886 - val_MinusLogProbMetric: 28.2886 - lr: 1.0417e-05 - 29s/epoch - 150ms/step
Epoch 874/1000
2023-10-26 13:56:50.697 
Epoch 874/1000 
	 loss: 27.3655, MinusLogProbMetric: 27.3655, val_loss: 28.2717, val_MinusLogProbMetric: 28.2717

Epoch 874: val_loss did not improve from 28.25634
196/196 - 29s - loss: 27.3655 - MinusLogProbMetric: 27.3655 - val_loss: 28.2717 - val_MinusLogProbMetric: 28.2717 - lr: 1.0417e-05 - 29s/epoch - 147ms/step
Epoch 875/1000
2023-10-26 13:57:20.913 
Epoch 875/1000 
	 loss: 27.3638, MinusLogProbMetric: 27.3638, val_loss: 28.2720, val_MinusLogProbMetric: 28.2720

Epoch 875: val_loss did not improve from 28.25634
196/196 - 30s - loss: 27.3638 - MinusLogProbMetric: 27.3638 - val_loss: 28.2720 - val_MinusLogProbMetric: 28.2720 - lr: 1.0417e-05 - 30s/epoch - 154ms/step
Epoch 876/1000
2023-10-26 13:57:55.266 
Epoch 876/1000 
	 loss: 27.3670, MinusLogProbMetric: 27.3670, val_loss: 28.2683, val_MinusLogProbMetric: 28.2683

Epoch 876: val_loss did not improve from 28.25634
196/196 - 34s - loss: 27.3670 - MinusLogProbMetric: 27.3670 - val_loss: 28.2683 - val_MinusLogProbMetric: 28.2683 - lr: 1.0417e-05 - 34s/epoch - 175ms/step
Epoch 877/1000
2023-10-26 13:58:26.723 
Epoch 877/1000 
	 loss: 27.3646, MinusLogProbMetric: 27.3646, val_loss: 28.2788, val_MinusLogProbMetric: 28.2788

Epoch 877: val_loss did not improve from 28.25634
196/196 - 31s - loss: 27.3646 - MinusLogProbMetric: 27.3646 - val_loss: 28.2788 - val_MinusLogProbMetric: 28.2788 - lr: 1.0417e-05 - 31s/epoch - 160ms/step
Epoch 878/1000
2023-10-26 13:58:56.531 
Epoch 878/1000 
	 loss: 27.3660, MinusLogProbMetric: 27.3660, val_loss: 28.2922, val_MinusLogProbMetric: 28.2922

Epoch 878: val_loss did not improve from 28.25634
196/196 - 30s - loss: 27.3660 - MinusLogProbMetric: 27.3660 - val_loss: 28.2922 - val_MinusLogProbMetric: 28.2922 - lr: 1.0417e-05 - 30s/epoch - 152ms/step
Epoch 879/1000
2023-10-26 13:59:26.227 
Epoch 879/1000 
	 loss: 27.3661, MinusLogProbMetric: 27.3661, val_loss: 28.2803, val_MinusLogProbMetric: 28.2803

Epoch 879: val_loss did not improve from 28.25634
196/196 - 30s - loss: 27.3661 - MinusLogProbMetric: 27.3661 - val_loss: 28.2803 - val_MinusLogProbMetric: 28.2803 - lr: 1.0417e-05 - 30s/epoch - 151ms/step
Epoch 880/1000
2023-10-26 13:59:58.028 
Epoch 880/1000 
	 loss: 27.3664, MinusLogProbMetric: 27.3664, val_loss: 28.2751, val_MinusLogProbMetric: 28.2751

Epoch 880: val_loss did not improve from 28.25634
196/196 - 32s - loss: 27.3664 - MinusLogProbMetric: 27.3664 - val_loss: 28.2751 - val_MinusLogProbMetric: 28.2751 - lr: 1.0417e-05 - 32s/epoch - 162ms/step
Epoch 881/1000
2023-10-26 14:00:25.594 
Epoch 881/1000 
	 loss: 27.3644, MinusLogProbMetric: 27.3644, val_loss: 28.2659, val_MinusLogProbMetric: 28.2659

Epoch 881: val_loss did not improve from 28.25634
196/196 - 28s - loss: 27.3644 - MinusLogProbMetric: 27.3644 - val_loss: 28.2659 - val_MinusLogProbMetric: 28.2659 - lr: 1.0417e-05 - 28s/epoch - 141ms/step
Epoch 882/1000
2023-10-26 14:00:55.445 
Epoch 882/1000 
	 loss: 27.3642, MinusLogProbMetric: 27.3642, val_loss: 28.2775, val_MinusLogProbMetric: 28.2775

Epoch 882: val_loss did not improve from 28.25634
196/196 - 30s - loss: 27.3642 - MinusLogProbMetric: 27.3642 - val_loss: 28.2775 - val_MinusLogProbMetric: 28.2775 - lr: 1.0417e-05 - 30s/epoch - 152ms/step
Epoch 883/1000
2023-10-26 14:01:26.808 
Epoch 883/1000 
	 loss: 27.3643, MinusLogProbMetric: 27.3643, val_loss: 28.2640, val_MinusLogProbMetric: 28.2640

Epoch 883: val_loss did not improve from 28.25634
196/196 - 31s - loss: 27.3643 - MinusLogProbMetric: 27.3643 - val_loss: 28.2640 - val_MinusLogProbMetric: 28.2640 - lr: 1.0417e-05 - 31s/epoch - 160ms/step
Epoch 884/1000
2023-10-26 14:01:56.048 
Epoch 884/1000 
	 loss: 27.3654, MinusLogProbMetric: 27.3654, val_loss: 28.2716, val_MinusLogProbMetric: 28.2716

Epoch 884: val_loss did not improve from 28.25634
196/196 - 29s - loss: 27.3654 - MinusLogProbMetric: 27.3654 - val_loss: 28.2716 - val_MinusLogProbMetric: 28.2716 - lr: 1.0417e-05 - 29s/epoch - 149ms/step
Epoch 885/1000
2023-10-26 14:02:30.791 
Epoch 885/1000 
	 loss: 27.3655, MinusLogProbMetric: 27.3655, val_loss: 28.2732, val_MinusLogProbMetric: 28.2732

Epoch 885: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3655 - MinusLogProbMetric: 27.3655 - val_loss: 28.2732 - val_MinusLogProbMetric: 28.2732 - lr: 1.0417e-05 - 35s/epoch - 177ms/step
Epoch 886/1000
2023-10-26 14:03:06.180 
Epoch 886/1000 
	 loss: 27.3645, MinusLogProbMetric: 27.3645, val_loss: 28.2684, val_MinusLogProbMetric: 28.2684

Epoch 886: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3645 - MinusLogProbMetric: 27.3645 - val_loss: 28.2684 - val_MinusLogProbMetric: 28.2684 - lr: 1.0417e-05 - 35s/epoch - 181ms/step
Epoch 887/1000
2023-10-26 14:03:41.261 
Epoch 887/1000 
	 loss: 27.3640, MinusLogProbMetric: 27.3640, val_loss: 28.2802, val_MinusLogProbMetric: 28.2802

Epoch 887: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3640 - MinusLogProbMetric: 27.3640 - val_loss: 28.2802 - val_MinusLogProbMetric: 28.2802 - lr: 1.0417e-05 - 35s/epoch - 179ms/step
Epoch 888/1000
2023-10-26 14:04:16.627 
Epoch 888/1000 
	 loss: 27.3641, MinusLogProbMetric: 27.3641, val_loss: 28.2678, val_MinusLogProbMetric: 28.2678

Epoch 888: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3641 - MinusLogProbMetric: 27.3641 - val_loss: 28.2678 - val_MinusLogProbMetric: 28.2678 - lr: 1.0417e-05 - 35s/epoch - 180ms/step
Epoch 889/1000
2023-10-26 14:04:51.749 
Epoch 889/1000 
	 loss: 27.3649, MinusLogProbMetric: 27.3649, val_loss: 28.2775, val_MinusLogProbMetric: 28.2775

Epoch 889: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3649 - MinusLogProbMetric: 27.3649 - val_loss: 28.2775 - val_MinusLogProbMetric: 28.2775 - lr: 1.0417e-05 - 35s/epoch - 179ms/step
Epoch 890/1000
2023-10-26 14:05:27.199 
Epoch 890/1000 
	 loss: 27.3638, MinusLogProbMetric: 27.3638, val_loss: 28.2698, val_MinusLogProbMetric: 28.2698

Epoch 890: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3638 - MinusLogProbMetric: 27.3638 - val_loss: 28.2698 - val_MinusLogProbMetric: 28.2698 - lr: 1.0417e-05 - 35s/epoch - 181ms/step
Epoch 891/1000
2023-10-26 14:06:02.412 
Epoch 891/1000 
	 loss: 27.3645, MinusLogProbMetric: 27.3645, val_loss: 28.2864, val_MinusLogProbMetric: 28.2864

Epoch 891: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3645 - MinusLogProbMetric: 27.3645 - val_loss: 28.2864 - val_MinusLogProbMetric: 28.2864 - lr: 1.0417e-05 - 35s/epoch - 180ms/step
Epoch 892/1000
2023-10-26 14:06:37.338 
Epoch 892/1000 
	 loss: 27.3639, MinusLogProbMetric: 27.3639, val_loss: 28.2702, val_MinusLogProbMetric: 28.2702

Epoch 892: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3639 - MinusLogProbMetric: 27.3639 - val_loss: 28.2702 - val_MinusLogProbMetric: 28.2702 - lr: 1.0417e-05 - 35s/epoch - 178ms/step
Epoch 893/1000
2023-10-26 14:07:12.726 
Epoch 893/1000 
	 loss: 27.3647, MinusLogProbMetric: 27.3647, val_loss: 28.2670, val_MinusLogProbMetric: 28.2670

Epoch 893: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3647 - MinusLogProbMetric: 27.3647 - val_loss: 28.2670 - val_MinusLogProbMetric: 28.2670 - lr: 1.0417e-05 - 35s/epoch - 181ms/step
Epoch 894/1000
2023-10-26 14:07:48.170 
Epoch 894/1000 
	 loss: 27.3646, MinusLogProbMetric: 27.3646, val_loss: 28.2669, val_MinusLogProbMetric: 28.2669

Epoch 894: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3646 - MinusLogProbMetric: 27.3646 - val_loss: 28.2669 - val_MinusLogProbMetric: 28.2669 - lr: 1.0417e-05 - 35s/epoch - 181ms/step
Epoch 895/1000
2023-10-26 14:08:23.253 
Epoch 895/1000 
	 loss: 27.3641, MinusLogProbMetric: 27.3641, val_loss: 28.2660, val_MinusLogProbMetric: 28.2660

Epoch 895: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3641 - MinusLogProbMetric: 27.3641 - val_loss: 28.2660 - val_MinusLogProbMetric: 28.2660 - lr: 1.0417e-05 - 35s/epoch - 179ms/step
Epoch 896/1000
2023-10-26 14:08:58.440 
Epoch 896/1000 
	 loss: 27.3645, MinusLogProbMetric: 27.3645, val_loss: 28.2735, val_MinusLogProbMetric: 28.2735

Epoch 896: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3645 - MinusLogProbMetric: 27.3645 - val_loss: 28.2735 - val_MinusLogProbMetric: 28.2735 - lr: 1.0417e-05 - 35s/epoch - 180ms/step
Epoch 897/1000
2023-10-26 14:09:33.593 
Epoch 897/1000 
	 loss: 27.3634, MinusLogProbMetric: 27.3634, val_loss: 28.2617, val_MinusLogProbMetric: 28.2617

Epoch 897: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3634 - MinusLogProbMetric: 27.3634 - val_loss: 28.2617 - val_MinusLogProbMetric: 28.2617 - lr: 1.0417e-05 - 35s/epoch - 179ms/step
Epoch 898/1000
2023-10-26 14:10:04.852 
Epoch 898/1000 
	 loss: 27.3634, MinusLogProbMetric: 27.3634, val_loss: 28.2723, val_MinusLogProbMetric: 28.2723

Epoch 898: val_loss did not improve from 28.25634
196/196 - 31s - loss: 27.3634 - MinusLogProbMetric: 27.3634 - val_loss: 28.2723 - val_MinusLogProbMetric: 28.2723 - lr: 1.0417e-05 - 31s/epoch - 159ms/step
Epoch 899/1000
2023-10-26 14:10:34.506 
Epoch 899/1000 
	 loss: 27.3637, MinusLogProbMetric: 27.3637, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 899: val_loss did not improve from 28.25634
196/196 - 30s - loss: 27.3637 - MinusLogProbMetric: 27.3637 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 1.0417e-05 - 30s/epoch - 151ms/step
Epoch 900/1000
2023-10-26 14:11:04.454 
Epoch 900/1000 
	 loss: 27.3660, MinusLogProbMetric: 27.3660, val_loss: 28.2717, val_MinusLogProbMetric: 28.2717

Epoch 900: val_loss did not improve from 28.25634
196/196 - 30s - loss: 27.3660 - MinusLogProbMetric: 27.3660 - val_loss: 28.2717 - val_MinusLogProbMetric: 28.2717 - lr: 1.0417e-05 - 30s/epoch - 153ms/step
Epoch 901/1000
2023-10-26 14:11:38.679 
Epoch 901/1000 
	 loss: 27.3657, MinusLogProbMetric: 27.3657, val_loss: 28.2610, val_MinusLogProbMetric: 28.2610

Epoch 901: val_loss did not improve from 28.25634
196/196 - 34s - loss: 27.3657 - MinusLogProbMetric: 27.3657 - val_loss: 28.2610 - val_MinusLogProbMetric: 28.2610 - lr: 1.0417e-05 - 34s/epoch - 175ms/step
Epoch 902/1000
2023-10-26 14:12:13.949 
Epoch 902/1000 
	 loss: 27.3654, MinusLogProbMetric: 27.3654, val_loss: 28.2697, val_MinusLogProbMetric: 28.2697

Epoch 902: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3654 - MinusLogProbMetric: 27.3654 - val_loss: 28.2697 - val_MinusLogProbMetric: 28.2697 - lr: 1.0417e-05 - 35s/epoch - 180ms/step
Epoch 903/1000
2023-10-26 14:12:48.292 
Epoch 903/1000 
	 loss: 27.3627, MinusLogProbMetric: 27.3627, val_loss: 28.2736, val_MinusLogProbMetric: 28.2736

Epoch 903: val_loss did not improve from 28.25634
196/196 - 34s - loss: 27.3627 - MinusLogProbMetric: 27.3627 - val_loss: 28.2736 - val_MinusLogProbMetric: 28.2736 - lr: 1.0417e-05 - 34s/epoch - 175ms/step
Epoch 904/1000
2023-10-26 14:13:23.215 
Epoch 904/1000 
	 loss: 27.3639, MinusLogProbMetric: 27.3639, val_loss: 28.2665, val_MinusLogProbMetric: 28.2665

Epoch 904: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3639 - MinusLogProbMetric: 27.3639 - val_loss: 28.2665 - val_MinusLogProbMetric: 28.2665 - lr: 1.0417e-05 - 35s/epoch - 178ms/step
Epoch 905/1000
2023-10-26 14:13:58.732 
Epoch 905/1000 
	 loss: 27.3638, MinusLogProbMetric: 27.3638, val_loss: 28.2650, val_MinusLogProbMetric: 28.2650

Epoch 905: val_loss did not improve from 28.25634
196/196 - 36s - loss: 27.3638 - MinusLogProbMetric: 27.3638 - val_loss: 28.2650 - val_MinusLogProbMetric: 28.2650 - lr: 1.0417e-05 - 36s/epoch - 181ms/step
Epoch 906/1000
2023-10-26 14:14:33.709 
Epoch 906/1000 
	 loss: 27.3658, MinusLogProbMetric: 27.3658, val_loss: 28.2869, val_MinusLogProbMetric: 28.2869

Epoch 906: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3658 - MinusLogProbMetric: 27.3658 - val_loss: 28.2869 - val_MinusLogProbMetric: 28.2869 - lr: 1.0417e-05 - 35s/epoch - 178ms/step
Epoch 907/1000
2023-10-26 14:15:08.431 
Epoch 907/1000 
	 loss: 27.3633, MinusLogProbMetric: 27.3633, val_loss: 28.2657, val_MinusLogProbMetric: 28.2657

Epoch 907: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3633 - MinusLogProbMetric: 27.3633 - val_loss: 28.2657 - val_MinusLogProbMetric: 28.2657 - lr: 1.0417e-05 - 35s/epoch - 177ms/step
Epoch 908/1000
2023-10-26 14:15:43.274 
Epoch 908/1000 
	 loss: 27.3546, MinusLogProbMetric: 27.3546, val_loss: 28.2689, val_MinusLogProbMetric: 28.2689

Epoch 908: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3546 - MinusLogProbMetric: 27.3546 - val_loss: 28.2689 - val_MinusLogProbMetric: 28.2689 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 909/1000
2023-10-26 14:16:17.833 
Epoch 909/1000 
	 loss: 27.3548, MinusLogProbMetric: 27.3548, val_loss: 28.2779, val_MinusLogProbMetric: 28.2779

Epoch 909: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3548 - MinusLogProbMetric: 27.3548 - val_loss: 28.2779 - val_MinusLogProbMetric: 28.2779 - lr: 5.2083e-06 - 35s/epoch - 176ms/step
Epoch 910/1000
2023-10-26 14:16:52.912 
Epoch 910/1000 
	 loss: 27.3549, MinusLogProbMetric: 27.3549, val_loss: 28.2649, val_MinusLogProbMetric: 28.2649

Epoch 910: val_loss did not improve from 28.25634
196/196 - 35s - loss: 27.3549 - MinusLogProbMetric: 27.3549 - val_loss: 28.2649 - val_MinusLogProbMetric: 28.2649 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 911/1000
2023-10-26 14:17:28.864 
Epoch 911/1000 
	 loss: 27.3539, MinusLogProbMetric: 27.3539, val_loss: 28.2720, val_MinusLogProbMetric: 28.2720

Epoch 911: val_loss did not improve from 28.25634
196/196 - 36s - loss: 27.3539 - MinusLogProbMetric: 27.3539 - val_loss: 28.2720 - val_MinusLogProbMetric: 28.2720 - lr: 5.2083e-06 - 36s/epoch - 184ms/step
Epoch 912/1000
2023-10-26 14:18:02.636 
Epoch 912/1000 
	 loss: 27.3543, MinusLogProbMetric: 27.3543, val_loss: 28.2554, val_MinusLogProbMetric: 28.2554

Epoch 912: val_loss improved from 28.25634 to 28.25536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_378/weights/best_weights.h5
196/196 - 34s - loss: 27.3543 - MinusLogProbMetric: 27.3543 - val_loss: 28.2554 - val_MinusLogProbMetric: 28.2554 - lr: 5.2083e-06 - 34s/epoch - 175ms/step
Epoch 913/1000
2023-10-26 14:18:33.264 
Epoch 913/1000 
	 loss: 27.3543, MinusLogProbMetric: 27.3543, val_loss: 28.2708, val_MinusLogProbMetric: 28.2708

Epoch 913: val_loss did not improve from 28.25536
196/196 - 30s - loss: 27.3543 - MinusLogProbMetric: 27.3543 - val_loss: 28.2708 - val_MinusLogProbMetric: 28.2708 - lr: 5.2083e-06 - 30s/epoch - 153ms/step
Epoch 914/1000
2023-10-26 14:19:02.821 
Epoch 914/1000 
	 loss: 27.3546, MinusLogProbMetric: 27.3546, val_loss: 28.2671, val_MinusLogProbMetric: 28.2671

Epoch 914: val_loss did not improve from 28.25536
196/196 - 30s - loss: 27.3546 - MinusLogProbMetric: 27.3546 - val_loss: 28.2671 - val_MinusLogProbMetric: 28.2671 - lr: 5.2083e-06 - 30s/epoch - 151ms/step
Epoch 915/1000
2023-10-26 14:19:35.869 
Epoch 915/1000 
	 loss: 27.3546, MinusLogProbMetric: 27.3546, val_loss: 28.2635, val_MinusLogProbMetric: 28.2635

Epoch 915: val_loss did not improve from 28.25536
196/196 - 33s - loss: 27.3546 - MinusLogProbMetric: 27.3546 - val_loss: 28.2635 - val_MinusLogProbMetric: 28.2635 - lr: 5.2083e-06 - 33s/epoch - 169ms/step
Epoch 916/1000
2023-10-26 14:20:10.382 
Epoch 916/1000 
	 loss: 27.3553, MinusLogProbMetric: 27.3553, val_loss: 28.2740, val_MinusLogProbMetric: 28.2740

Epoch 916: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3553 - MinusLogProbMetric: 27.3553 - val_loss: 28.2740 - val_MinusLogProbMetric: 28.2740 - lr: 5.2083e-06 - 35s/epoch - 176ms/step
Epoch 917/1000
2023-10-26 14:20:45.612 
Epoch 917/1000 
	 loss: 27.3542, MinusLogProbMetric: 27.3542, val_loss: 28.2577, val_MinusLogProbMetric: 28.2577

Epoch 917: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3542 - MinusLogProbMetric: 27.3542 - val_loss: 28.2577 - val_MinusLogProbMetric: 28.2577 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 918/1000
2023-10-26 14:21:20.295 
Epoch 918/1000 
	 loss: 27.3559, MinusLogProbMetric: 27.3559, val_loss: 28.2623, val_MinusLogProbMetric: 28.2623

Epoch 918: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3559 - MinusLogProbMetric: 27.3559 - val_loss: 28.2623 - val_MinusLogProbMetric: 28.2623 - lr: 5.2083e-06 - 35s/epoch - 177ms/step
Epoch 919/1000
2023-10-26 14:21:55.423 
Epoch 919/1000 
	 loss: 27.3543, MinusLogProbMetric: 27.3543, val_loss: 28.2648, val_MinusLogProbMetric: 28.2648

Epoch 919: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3543 - MinusLogProbMetric: 27.3543 - val_loss: 28.2648 - val_MinusLogProbMetric: 28.2648 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 920/1000
2023-10-26 14:22:30.473 
Epoch 920/1000 
	 loss: 27.3532, MinusLogProbMetric: 27.3532, val_loss: 28.2577, val_MinusLogProbMetric: 28.2577

Epoch 920: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3532 - MinusLogProbMetric: 27.3532 - val_loss: 28.2577 - val_MinusLogProbMetric: 28.2577 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 921/1000
2023-10-26 14:23:05.832 
Epoch 921/1000 
	 loss: 27.3542, MinusLogProbMetric: 27.3542, val_loss: 28.2633, val_MinusLogProbMetric: 28.2633

Epoch 921: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3542 - MinusLogProbMetric: 27.3542 - val_loss: 28.2633 - val_MinusLogProbMetric: 28.2633 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 922/1000
2023-10-26 14:23:41.124 
Epoch 922/1000 
	 loss: 27.3540, MinusLogProbMetric: 27.3540, val_loss: 28.2651, val_MinusLogProbMetric: 28.2651

Epoch 922: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3540 - MinusLogProbMetric: 27.3540 - val_loss: 28.2651 - val_MinusLogProbMetric: 28.2651 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 923/1000
2023-10-26 14:24:15.907 
Epoch 923/1000 
	 loss: 27.3541, MinusLogProbMetric: 27.3541, val_loss: 28.2666, val_MinusLogProbMetric: 28.2666

Epoch 923: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3541 - MinusLogProbMetric: 27.3541 - val_loss: 28.2666 - val_MinusLogProbMetric: 28.2666 - lr: 5.2083e-06 - 35s/epoch - 177ms/step
Epoch 924/1000
2023-10-26 14:24:51.412 
Epoch 924/1000 
	 loss: 27.3535, MinusLogProbMetric: 27.3535, val_loss: 28.2692, val_MinusLogProbMetric: 28.2692

Epoch 924: val_loss did not improve from 28.25536
196/196 - 36s - loss: 27.3535 - MinusLogProbMetric: 27.3535 - val_loss: 28.2692 - val_MinusLogProbMetric: 28.2692 - lr: 5.2083e-06 - 36s/epoch - 181ms/step
Epoch 925/1000
2023-10-26 14:25:26.213 
Epoch 925/1000 
	 loss: 27.3539, MinusLogProbMetric: 27.3539, val_loss: 28.2680, val_MinusLogProbMetric: 28.2680

Epoch 925: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3539 - MinusLogProbMetric: 27.3539 - val_loss: 28.2680 - val_MinusLogProbMetric: 28.2680 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 926/1000
2023-10-26 14:26:01.336 
Epoch 926/1000 
	 loss: 27.3550, MinusLogProbMetric: 27.3550, val_loss: 28.2616, val_MinusLogProbMetric: 28.2616

Epoch 926: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3550 - MinusLogProbMetric: 27.3550 - val_loss: 28.2616 - val_MinusLogProbMetric: 28.2616 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 927/1000
2023-10-26 14:26:36.144 
Epoch 927/1000 
	 loss: 27.3537, MinusLogProbMetric: 27.3537, val_loss: 28.2687, val_MinusLogProbMetric: 28.2687

Epoch 927: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3537 - MinusLogProbMetric: 27.3537 - val_loss: 28.2687 - val_MinusLogProbMetric: 28.2687 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 928/1000
2023-10-26 14:27:11.383 
Epoch 928/1000 
	 loss: 27.3541, MinusLogProbMetric: 27.3541, val_loss: 28.2715, val_MinusLogProbMetric: 28.2715

Epoch 928: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3541 - MinusLogProbMetric: 27.3541 - val_loss: 28.2715 - val_MinusLogProbMetric: 28.2715 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 929/1000
2023-10-26 14:27:46.453 
Epoch 929/1000 
	 loss: 27.3555, MinusLogProbMetric: 27.3555, val_loss: 28.2615, val_MinusLogProbMetric: 28.2615

Epoch 929: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3555 - MinusLogProbMetric: 27.3555 - val_loss: 28.2615 - val_MinusLogProbMetric: 28.2615 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 930/1000
2023-10-26 14:28:21.208 
Epoch 930/1000 
	 loss: 27.3536, MinusLogProbMetric: 27.3536, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 930: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3536 - MinusLogProbMetric: 27.3536 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 5.2083e-06 - 35s/epoch - 177ms/step
Epoch 931/1000
2023-10-26 14:28:56.699 
Epoch 931/1000 
	 loss: 27.3542, MinusLogProbMetric: 27.3542, val_loss: 28.2688, val_MinusLogProbMetric: 28.2688

Epoch 931: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3542 - MinusLogProbMetric: 27.3542 - val_loss: 28.2688 - val_MinusLogProbMetric: 28.2688 - lr: 5.2083e-06 - 35s/epoch - 181ms/step
Epoch 932/1000
2023-10-26 14:29:32.006 
Epoch 932/1000 
	 loss: 27.3535, MinusLogProbMetric: 27.3535, val_loss: 28.2690, val_MinusLogProbMetric: 28.2690

Epoch 932: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3535 - MinusLogProbMetric: 27.3535 - val_loss: 28.2690 - val_MinusLogProbMetric: 28.2690 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 933/1000
2023-10-26 14:30:07.642 
Epoch 933/1000 
	 loss: 27.3526, MinusLogProbMetric: 27.3526, val_loss: 28.2658, val_MinusLogProbMetric: 28.2658

Epoch 933: val_loss did not improve from 28.25536
196/196 - 36s - loss: 27.3526 - MinusLogProbMetric: 27.3526 - val_loss: 28.2658 - val_MinusLogProbMetric: 28.2658 - lr: 5.2083e-06 - 36s/epoch - 182ms/step
Epoch 934/1000
2023-10-26 14:30:42.900 
Epoch 934/1000 
	 loss: 27.3542, MinusLogProbMetric: 27.3542, val_loss: 28.2593, val_MinusLogProbMetric: 28.2593

Epoch 934: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3542 - MinusLogProbMetric: 27.3542 - val_loss: 28.2593 - val_MinusLogProbMetric: 28.2593 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 935/1000
2023-10-26 14:31:18.277 
Epoch 935/1000 
	 loss: 27.3544, MinusLogProbMetric: 27.3544, val_loss: 28.2666, val_MinusLogProbMetric: 28.2666

Epoch 935: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3544 - MinusLogProbMetric: 27.3544 - val_loss: 28.2666 - val_MinusLogProbMetric: 28.2666 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 936/1000
2023-10-26 14:31:53.557 
Epoch 936/1000 
	 loss: 27.3540, MinusLogProbMetric: 27.3540, val_loss: 28.2721, val_MinusLogProbMetric: 28.2721

Epoch 936: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3540 - MinusLogProbMetric: 27.3540 - val_loss: 28.2721 - val_MinusLogProbMetric: 28.2721 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 937/1000
2023-10-26 14:32:28.168 
Epoch 937/1000 
	 loss: 27.3557, MinusLogProbMetric: 27.3557, val_loss: 28.2639, val_MinusLogProbMetric: 28.2639

Epoch 937: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3557 - MinusLogProbMetric: 27.3557 - val_loss: 28.2639 - val_MinusLogProbMetric: 28.2639 - lr: 5.2083e-06 - 35s/epoch - 177ms/step
Epoch 938/1000
2023-10-26 14:33:03.483 
Epoch 938/1000 
	 loss: 27.3532, MinusLogProbMetric: 27.3532, val_loss: 28.2580, val_MinusLogProbMetric: 28.2580

Epoch 938: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3532 - MinusLogProbMetric: 27.3532 - val_loss: 28.2580 - val_MinusLogProbMetric: 28.2580 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 939/1000
2023-10-26 14:33:38.683 
Epoch 939/1000 
	 loss: 27.3545, MinusLogProbMetric: 27.3545, val_loss: 28.2747, val_MinusLogProbMetric: 28.2747

Epoch 939: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3545 - MinusLogProbMetric: 27.3545 - val_loss: 28.2747 - val_MinusLogProbMetric: 28.2747 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 940/1000
2023-10-26 14:34:13.786 
Epoch 940/1000 
	 loss: 27.3545, MinusLogProbMetric: 27.3545, val_loss: 28.2715, val_MinusLogProbMetric: 28.2715

Epoch 940: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3545 - MinusLogProbMetric: 27.3545 - val_loss: 28.2715 - val_MinusLogProbMetric: 28.2715 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 941/1000
2023-10-26 14:34:49.047 
Epoch 941/1000 
	 loss: 27.3538, MinusLogProbMetric: 27.3538, val_loss: 28.2579, val_MinusLogProbMetric: 28.2579

Epoch 941: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3538 - MinusLogProbMetric: 27.3538 - val_loss: 28.2579 - val_MinusLogProbMetric: 28.2579 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 942/1000
2023-10-26 14:35:23.315 
Epoch 942/1000 
	 loss: 27.3543, MinusLogProbMetric: 27.3543, val_loss: 28.2674, val_MinusLogProbMetric: 28.2674

Epoch 942: val_loss did not improve from 28.25536
196/196 - 34s - loss: 27.3543 - MinusLogProbMetric: 27.3543 - val_loss: 28.2674 - val_MinusLogProbMetric: 28.2674 - lr: 5.2083e-06 - 34s/epoch - 175ms/step
Epoch 943/1000
2023-10-26 14:35:58.112 
Epoch 943/1000 
	 loss: 27.3536, MinusLogProbMetric: 27.3536, val_loss: 28.2671, val_MinusLogProbMetric: 28.2671

Epoch 943: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3536 - MinusLogProbMetric: 27.3536 - val_loss: 28.2671 - val_MinusLogProbMetric: 28.2671 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 944/1000
2023-10-26 14:36:33.161 
Epoch 944/1000 
	 loss: 27.3530, MinusLogProbMetric: 27.3530, val_loss: 28.2709, val_MinusLogProbMetric: 28.2709

Epoch 944: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3530 - MinusLogProbMetric: 27.3530 - val_loss: 28.2709 - val_MinusLogProbMetric: 28.2709 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 945/1000
2023-10-26 14:37:07.979 
Epoch 945/1000 
	 loss: 27.3531, MinusLogProbMetric: 27.3531, val_loss: 28.2669, val_MinusLogProbMetric: 28.2669

Epoch 945: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3531 - MinusLogProbMetric: 27.3531 - val_loss: 28.2669 - val_MinusLogProbMetric: 28.2669 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 946/1000
2023-10-26 14:37:42.945 
Epoch 946/1000 
	 loss: 27.3533, MinusLogProbMetric: 27.3533, val_loss: 28.2618, val_MinusLogProbMetric: 28.2618

Epoch 946: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3533 - MinusLogProbMetric: 27.3533 - val_loss: 28.2618 - val_MinusLogProbMetric: 28.2618 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 947/1000
2023-10-26 14:38:17.743 
Epoch 947/1000 
	 loss: 27.3530, MinusLogProbMetric: 27.3530, val_loss: 28.2643, val_MinusLogProbMetric: 28.2643

Epoch 947: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3530 - MinusLogProbMetric: 27.3530 - val_loss: 28.2643 - val_MinusLogProbMetric: 28.2643 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 948/1000
2023-10-26 14:38:53.013 
Epoch 948/1000 
	 loss: 27.3533, MinusLogProbMetric: 27.3533, val_loss: 28.2743, val_MinusLogProbMetric: 28.2743

Epoch 948: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3533 - MinusLogProbMetric: 27.3533 - val_loss: 28.2743 - val_MinusLogProbMetric: 28.2743 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 949/1000
2023-10-26 14:39:28.232 
Epoch 949/1000 
	 loss: 27.3517, MinusLogProbMetric: 27.3517, val_loss: 28.2684, val_MinusLogProbMetric: 28.2684

Epoch 949: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3517 - MinusLogProbMetric: 27.3517 - val_loss: 28.2684 - val_MinusLogProbMetric: 28.2684 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 950/1000
2023-10-26 14:40:02.117 
Epoch 950/1000 
	 loss: 27.3535, MinusLogProbMetric: 27.3535, val_loss: 28.2627, val_MinusLogProbMetric: 28.2627

Epoch 950: val_loss did not improve from 28.25536
196/196 - 34s - loss: 27.3535 - MinusLogProbMetric: 27.3535 - val_loss: 28.2627 - val_MinusLogProbMetric: 28.2627 - lr: 5.2083e-06 - 34s/epoch - 173ms/step
Epoch 951/1000
2023-10-26 14:40:36.387 
Epoch 951/1000 
	 loss: 27.3538, MinusLogProbMetric: 27.3538, val_loss: 28.2746, val_MinusLogProbMetric: 28.2746

Epoch 951: val_loss did not improve from 28.25536
196/196 - 34s - loss: 27.3538 - MinusLogProbMetric: 27.3538 - val_loss: 28.2746 - val_MinusLogProbMetric: 28.2746 - lr: 5.2083e-06 - 34s/epoch - 175ms/step
Epoch 952/1000
2023-10-26 14:41:10.515 
Epoch 952/1000 
	 loss: 27.3529, MinusLogProbMetric: 27.3529, val_loss: 28.2662, val_MinusLogProbMetric: 28.2662

Epoch 952: val_loss did not improve from 28.25536
196/196 - 34s - loss: 27.3529 - MinusLogProbMetric: 27.3529 - val_loss: 28.2662 - val_MinusLogProbMetric: 28.2662 - lr: 5.2083e-06 - 34s/epoch - 174ms/step
Epoch 953/1000
2023-10-26 14:41:45.431 
Epoch 953/1000 
	 loss: 27.3530, MinusLogProbMetric: 27.3530, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 953: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3530 - MinusLogProbMetric: 27.3530 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 954/1000
2023-10-26 14:42:20.390 
Epoch 954/1000 
	 loss: 27.3531, MinusLogProbMetric: 27.3531, val_loss: 28.2610, val_MinusLogProbMetric: 28.2610

Epoch 954: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3531 - MinusLogProbMetric: 27.3531 - val_loss: 28.2610 - val_MinusLogProbMetric: 28.2610 - lr: 5.2083e-06 - 35s/epoch - 178ms/step
Epoch 955/1000
2023-10-26 14:42:55.734 
Epoch 955/1000 
	 loss: 27.3537, MinusLogProbMetric: 27.3537, val_loss: 28.2572, val_MinusLogProbMetric: 28.2572

Epoch 955: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3537 - MinusLogProbMetric: 27.3537 - val_loss: 28.2572 - val_MinusLogProbMetric: 28.2572 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 956/1000
2023-10-26 14:43:31.077 
Epoch 956/1000 
	 loss: 27.3534, MinusLogProbMetric: 27.3534, val_loss: 28.2645, val_MinusLogProbMetric: 28.2645

Epoch 956: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3534 - MinusLogProbMetric: 27.3534 - val_loss: 28.2645 - val_MinusLogProbMetric: 28.2645 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 957/1000
2023-10-26 14:44:04.239 
Epoch 957/1000 
	 loss: 27.3537, MinusLogProbMetric: 27.3537, val_loss: 28.2679, val_MinusLogProbMetric: 28.2679

Epoch 957: val_loss did not improve from 28.25536
196/196 - 33s - loss: 27.3537 - MinusLogProbMetric: 27.3537 - val_loss: 28.2679 - val_MinusLogProbMetric: 28.2679 - lr: 5.2083e-06 - 33s/epoch - 169ms/step
Epoch 958/1000
2023-10-26 14:44:39.323 
Epoch 958/1000 
	 loss: 27.3535, MinusLogProbMetric: 27.3535, val_loss: 28.2711, val_MinusLogProbMetric: 28.2711

Epoch 958: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3535 - MinusLogProbMetric: 27.3535 - val_loss: 28.2711 - val_MinusLogProbMetric: 28.2711 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 959/1000
2023-10-26 14:45:14.412 
Epoch 959/1000 
	 loss: 27.3542, MinusLogProbMetric: 27.3542, val_loss: 28.2595, val_MinusLogProbMetric: 28.2595

Epoch 959: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3542 - MinusLogProbMetric: 27.3542 - val_loss: 28.2595 - val_MinusLogProbMetric: 28.2595 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 960/1000
2023-10-26 14:45:49.907 
Epoch 960/1000 
	 loss: 27.3539, MinusLogProbMetric: 27.3539, val_loss: 28.2690, val_MinusLogProbMetric: 28.2690

Epoch 960: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3539 - MinusLogProbMetric: 27.3539 - val_loss: 28.2690 - val_MinusLogProbMetric: 28.2690 - lr: 5.2083e-06 - 35s/epoch - 181ms/step
Epoch 961/1000
2023-10-26 14:46:25.160 
Epoch 961/1000 
	 loss: 27.3536, MinusLogProbMetric: 27.3536, val_loss: 28.2684, val_MinusLogProbMetric: 28.2684

Epoch 961: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3536 - MinusLogProbMetric: 27.3536 - val_loss: 28.2684 - val_MinusLogProbMetric: 28.2684 - lr: 5.2083e-06 - 35s/epoch - 180ms/step
Epoch 962/1000
2023-10-26 14:47:00.157 
Epoch 962/1000 
	 loss: 27.3534, MinusLogProbMetric: 27.3534, val_loss: 28.2625, val_MinusLogProbMetric: 28.2625

Epoch 962: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3534 - MinusLogProbMetric: 27.3534 - val_loss: 28.2625 - val_MinusLogProbMetric: 28.2625 - lr: 5.2083e-06 - 35s/epoch - 179ms/step
Epoch 963/1000
2023-10-26 14:47:35.551 
Epoch 963/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.2615, val_MinusLogProbMetric: 28.2615

Epoch 963: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.2615 - val_MinusLogProbMetric: 28.2615 - lr: 2.6042e-06 - 35s/epoch - 181ms/step
Epoch 964/1000
2023-10-26 14:48:11.047 
Epoch 964/1000 
	 loss: 27.3492, MinusLogProbMetric: 27.3492, val_loss: 28.2639, val_MinusLogProbMetric: 28.2639

Epoch 964: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3492 - MinusLogProbMetric: 27.3492 - val_loss: 28.2639 - val_MinusLogProbMetric: 28.2639 - lr: 2.6042e-06 - 35s/epoch - 181ms/step
Epoch 965/1000
2023-10-26 14:48:46.167 
Epoch 965/1000 
	 loss: 27.3481, MinusLogProbMetric: 27.3481, val_loss: 28.2563, val_MinusLogProbMetric: 28.2563

Epoch 965: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3481 - MinusLogProbMetric: 27.3481 - val_loss: 28.2563 - val_MinusLogProbMetric: 28.2563 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 966/1000
2023-10-26 14:49:21.278 
Epoch 966/1000 
	 loss: 27.3482, MinusLogProbMetric: 27.3482, val_loss: 28.2675, val_MinusLogProbMetric: 28.2675

Epoch 966: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3482 - MinusLogProbMetric: 27.3482 - val_loss: 28.2675 - val_MinusLogProbMetric: 28.2675 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 967/1000
2023-10-26 14:49:56.534 
Epoch 967/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.2704, val_MinusLogProbMetric: 28.2704

Epoch 967: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.2704 - val_MinusLogProbMetric: 28.2704 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Epoch 968/1000
2023-10-26 14:50:32.065 
Epoch 968/1000 
	 loss: 27.3491, MinusLogProbMetric: 27.3491, val_loss: 28.2711, val_MinusLogProbMetric: 28.2711

Epoch 968: val_loss did not improve from 28.25536
196/196 - 36s - loss: 27.3491 - MinusLogProbMetric: 27.3491 - val_loss: 28.2711 - val_MinusLogProbMetric: 28.2711 - lr: 2.6042e-06 - 36s/epoch - 181ms/step
Epoch 969/1000
2023-10-26 14:51:07.359 
Epoch 969/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.2642, val_MinusLogProbMetric: 28.2642

Epoch 969: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.2642 - val_MinusLogProbMetric: 28.2642 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Epoch 970/1000
2023-10-26 14:51:42.488 
Epoch 970/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.2598, val_MinusLogProbMetric: 28.2598

Epoch 970: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.2598 - val_MinusLogProbMetric: 28.2598 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 971/1000
2023-10-26 14:52:17.675 
Epoch 971/1000 
	 loss: 27.3484, MinusLogProbMetric: 27.3484, val_loss: 28.2649, val_MinusLogProbMetric: 28.2649

Epoch 971: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3484 - MinusLogProbMetric: 27.3484 - val_loss: 28.2649 - val_MinusLogProbMetric: 28.2649 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Epoch 972/1000
2023-10-26 14:52:52.995 
Epoch 972/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.2641, val_MinusLogProbMetric: 28.2641

Epoch 972: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.2641 - val_MinusLogProbMetric: 28.2641 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Epoch 973/1000
2023-10-26 14:53:28.627 
Epoch 973/1000 
	 loss: 27.3487, MinusLogProbMetric: 27.3487, val_loss: 28.2657, val_MinusLogProbMetric: 28.2657

Epoch 973: val_loss did not improve from 28.25536
196/196 - 36s - loss: 27.3487 - MinusLogProbMetric: 27.3487 - val_loss: 28.2657 - val_MinusLogProbMetric: 28.2657 - lr: 2.6042e-06 - 36s/epoch - 182ms/step
Epoch 974/1000
2023-10-26 14:54:03.913 
Epoch 974/1000 
	 loss: 27.3490, MinusLogProbMetric: 27.3490, val_loss: 28.2637, val_MinusLogProbMetric: 28.2637

Epoch 974: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3490 - MinusLogProbMetric: 27.3490 - val_loss: 28.2637 - val_MinusLogProbMetric: 28.2637 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Epoch 975/1000
2023-10-26 14:54:39.021 
Epoch 975/1000 
	 loss: 27.3485, MinusLogProbMetric: 27.3485, val_loss: 28.2652, val_MinusLogProbMetric: 28.2652

Epoch 975: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3485 - MinusLogProbMetric: 27.3485 - val_loss: 28.2652 - val_MinusLogProbMetric: 28.2652 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 976/1000
2023-10-26 14:55:13.018 
Epoch 976/1000 
	 loss: 27.3491, MinusLogProbMetric: 27.3491, val_loss: 28.2712, val_MinusLogProbMetric: 28.2712

Epoch 976: val_loss did not improve from 28.25536
196/196 - 34s - loss: 27.3491 - MinusLogProbMetric: 27.3491 - val_loss: 28.2712 - val_MinusLogProbMetric: 28.2712 - lr: 2.6042e-06 - 34s/epoch - 173ms/step
Epoch 977/1000
2023-10-26 14:55:47.981 
Epoch 977/1000 
	 loss: 27.3485, MinusLogProbMetric: 27.3485, val_loss: 28.2614, val_MinusLogProbMetric: 28.2614

Epoch 977: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3485 - MinusLogProbMetric: 27.3485 - val_loss: 28.2614 - val_MinusLogProbMetric: 28.2614 - lr: 2.6042e-06 - 35s/epoch - 178ms/step
Epoch 978/1000
2023-10-26 14:56:23.227 
Epoch 978/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.2617, val_MinusLogProbMetric: 28.2617

Epoch 978: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.2617 - val_MinusLogProbMetric: 28.2617 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Epoch 979/1000
2023-10-26 14:56:58.243 
Epoch 979/1000 
	 loss: 27.3484, MinusLogProbMetric: 27.3484, val_loss: 28.2693, val_MinusLogProbMetric: 28.2693

Epoch 979: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3484 - MinusLogProbMetric: 27.3484 - val_loss: 28.2693 - val_MinusLogProbMetric: 28.2693 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 980/1000
2023-10-26 14:57:33.021 
Epoch 980/1000 
	 loss: 27.3478, MinusLogProbMetric: 27.3478, val_loss: 28.2626, val_MinusLogProbMetric: 28.2626

Epoch 980: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3478 - MinusLogProbMetric: 27.3478 - val_loss: 28.2626 - val_MinusLogProbMetric: 28.2626 - lr: 2.6042e-06 - 35s/epoch - 177ms/step
Epoch 981/1000
2023-10-26 14:58:08.157 
Epoch 981/1000 
	 loss: 27.3483, MinusLogProbMetric: 27.3483, val_loss: 28.2652, val_MinusLogProbMetric: 28.2652

Epoch 981: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3483 - MinusLogProbMetric: 27.3483 - val_loss: 28.2652 - val_MinusLogProbMetric: 28.2652 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 982/1000
2023-10-26 14:58:43.193 
Epoch 982/1000 
	 loss: 27.3482, MinusLogProbMetric: 27.3482, val_loss: 28.2567, val_MinusLogProbMetric: 28.2567

Epoch 982: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3482 - MinusLogProbMetric: 27.3482 - val_loss: 28.2567 - val_MinusLogProbMetric: 28.2567 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 983/1000
2023-10-26 14:59:18.188 
Epoch 983/1000 
	 loss: 27.3485, MinusLogProbMetric: 27.3485, val_loss: 28.2596, val_MinusLogProbMetric: 28.2596

Epoch 983: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3485 - MinusLogProbMetric: 27.3485 - val_loss: 28.2596 - val_MinusLogProbMetric: 28.2596 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 984/1000
2023-10-26 14:59:53.090 
Epoch 984/1000 
	 loss: 27.3483, MinusLogProbMetric: 27.3483, val_loss: 28.2636, val_MinusLogProbMetric: 28.2636

Epoch 984: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3483 - MinusLogProbMetric: 27.3483 - val_loss: 28.2636 - val_MinusLogProbMetric: 28.2636 - lr: 2.6042e-06 - 35s/epoch - 178ms/step
Epoch 985/1000
2023-10-26 15:00:28.525 
Epoch 985/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.2613, val_MinusLogProbMetric: 28.2613

Epoch 985: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.2613 - val_MinusLogProbMetric: 28.2613 - lr: 2.6042e-06 - 35s/epoch - 181ms/step
Epoch 986/1000
2023-10-26 15:01:03.500 
Epoch 986/1000 
	 loss: 27.3479, MinusLogProbMetric: 27.3479, val_loss: 28.2643, val_MinusLogProbMetric: 28.2643

Epoch 986: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3479 - MinusLogProbMetric: 27.3479 - val_loss: 28.2643 - val_MinusLogProbMetric: 28.2643 - lr: 2.6042e-06 - 35s/epoch - 178ms/step
Epoch 987/1000
2023-10-26 15:01:37.327 
Epoch 987/1000 
	 loss: 27.3485, MinusLogProbMetric: 27.3485, val_loss: 28.2656, val_MinusLogProbMetric: 28.2656

Epoch 987: val_loss did not improve from 28.25536
196/196 - 34s - loss: 27.3485 - MinusLogProbMetric: 27.3485 - val_loss: 28.2656 - val_MinusLogProbMetric: 28.2656 - lr: 2.6042e-06 - 34s/epoch - 173ms/step
Epoch 988/1000
2023-10-26 15:02:07.229 
Epoch 988/1000 
	 loss: 27.3481, MinusLogProbMetric: 27.3481, val_loss: 28.2565, val_MinusLogProbMetric: 28.2565

Epoch 988: val_loss did not improve from 28.25536
196/196 - 30s - loss: 27.3481 - MinusLogProbMetric: 27.3481 - val_loss: 28.2565 - val_MinusLogProbMetric: 28.2565 - lr: 2.6042e-06 - 30s/epoch - 153ms/step
Epoch 989/1000
2023-10-26 15:02:36.916 
Epoch 989/1000 
	 loss: 27.3477, MinusLogProbMetric: 27.3477, val_loss: 28.2665, val_MinusLogProbMetric: 28.2665

Epoch 989: val_loss did not improve from 28.25536
196/196 - 30s - loss: 27.3477 - MinusLogProbMetric: 27.3477 - val_loss: 28.2665 - val_MinusLogProbMetric: 28.2665 - lr: 2.6042e-06 - 30s/epoch - 151ms/step
Epoch 990/1000
2023-10-26 15:03:08.153 
Epoch 990/1000 
	 loss: 27.3483, MinusLogProbMetric: 27.3483, val_loss: 28.2693, val_MinusLogProbMetric: 28.2693

Epoch 990: val_loss did not improve from 28.25536
196/196 - 31s - loss: 27.3483 - MinusLogProbMetric: 27.3483 - val_loss: 28.2693 - val_MinusLogProbMetric: 28.2693 - lr: 2.6042e-06 - 31s/epoch - 159ms/step
Epoch 991/1000
2023-10-26 15:03:42.610 
Epoch 991/1000 
	 loss: 27.3482, MinusLogProbMetric: 27.3482, val_loss: 28.2621, val_MinusLogProbMetric: 28.2621

Epoch 991: val_loss did not improve from 28.25536
196/196 - 34s - loss: 27.3482 - MinusLogProbMetric: 27.3482 - val_loss: 28.2621 - val_MinusLogProbMetric: 28.2621 - lr: 2.6042e-06 - 34s/epoch - 176ms/step
Epoch 992/1000
2023-10-26 15:04:18.044 
Epoch 992/1000 
	 loss: 27.3483, MinusLogProbMetric: 27.3483, val_loss: 28.2704, val_MinusLogProbMetric: 28.2704

Epoch 992: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3483 - MinusLogProbMetric: 27.3483 - val_loss: 28.2704 - val_MinusLogProbMetric: 28.2704 - lr: 2.6042e-06 - 35s/epoch - 181ms/step
Epoch 993/1000
2023-10-26 15:04:52.835 
Epoch 993/1000 
	 loss: 27.3481, MinusLogProbMetric: 27.3481, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 993: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3481 - MinusLogProbMetric: 27.3481 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 2.6042e-06 - 35s/epoch - 177ms/step
Epoch 994/1000
2023-10-26 15:05:27.783 
Epoch 994/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.2595, val_MinusLogProbMetric: 28.2595

Epoch 994: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.2595 - val_MinusLogProbMetric: 28.2595 - lr: 2.6042e-06 - 35s/epoch - 178ms/step
Epoch 995/1000
2023-10-26 15:06:02.721 
Epoch 995/1000 
	 loss: 27.3473, MinusLogProbMetric: 27.3473, val_loss: 28.2567, val_MinusLogProbMetric: 28.2567

Epoch 995: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3473 - MinusLogProbMetric: 27.3473 - val_loss: 28.2567 - val_MinusLogProbMetric: 28.2567 - lr: 2.6042e-06 - 35s/epoch - 178ms/step
Epoch 996/1000
2023-10-26 15:06:38.177 
Epoch 996/1000 
	 loss: 27.3480, MinusLogProbMetric: 27.3480, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 996: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3480 - MinusLogProbMetric: 27.3480 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 2.6042e-06 - 35s/epoch - 181ms/step
Epoch 997/1000
2023-10-26 15:07:13.170 
Epoch 997/1000 
	 loss: 27.3481, MinusLogProbMetric: 27.3481, val_loss: 28.2593, val_MinusLogProbMetric: 28.2593

Epoch 997: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3481 - MinusLogProbMetric: 27.3481 - val_loss: 28.2593 - val_MinusLogProbMetric: 28.2593 - lr: 2.6042e-06 - 35s/epoch - 179ms/step
Epoch 998/1000
2023-10-26 15:07:48.482 
Epoch 998/1000 
	 loss: 27.3479, MinusLogProbMetric: 27.3479, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 998: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3479 - MinusLogProbMetric: 27.3479 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Epoch 999/1000
2023-10-26 15:08:24.175 
Epoch 999/1000 
	 loss: 27.3474, MinusLogProbMetric: 27.3474, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 999: val_loss did not improve from 28.25536
196/196 - 36s - loss: 27.3474 - MinusLogProbMetric: 27.3474 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 2.6042e-06 - 36s/epoch - 182ms/step
Epoch 1000/1000
2023-10-26 15:08:59.523 
Epoch 1000/1000 
	 loss: 27.3482, MinusLogProbMetric: 27.3482, val_loss: 28.2635, val_MinusLogProbMetric: 28.2635

Epoch 1000: val_loss did not improve from 28.25536
196/196 - 35s - loss: 27.3482 - MinusLogProbMetric: 27.3482 - val_loss: 28.2635 - val_MinusLogProbMetric: 28.2635 - lr: 2.6042e-06 - 35s/epoch - 180ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 869.
Model trained in 34017.17 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.80 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.12 s.
===========
Run 378/720 done in 34096.98 s.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

===========
Generating train data for run 386.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_386/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_386/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_386/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_386
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_337"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_338 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7feee5e87b50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feef43da8f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feef43da8f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff71e021360>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0ab60acb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0ab609480>, <keras.callbacks.ModelCheckpoint object at 0x7ff0ab60b1c0>, <keras.callbacks.EarlyStopping object at 0x7ff0ab60b2e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0ab60b3a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff0ab609060>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_386/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 386/720 with hyperparameters:
timestamp = 2023-10-26 15:09:05.537164
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
2023-10-26 15:10:40.843 
Epoch 1/1000 
	 loss: 266.7783, MinusLogProbMetric: 266.7783, val_loss: 69.1294, val_MinusLogProbMetric: 69.1294

Epoch 1: val_loss improved from inf to 69.12936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 96s - loss: 266.7783 - MinusLogProbMetric: 266.7783 - val_loss: 69.1294 - val_MinusLogProbMetric: 69.1294 - lr: 0.0010 - 96s/epoch - 489ms/step
Epoch 2/1000
2023-10-26 15:11:17.058 
Epoch 2/1000 
	 loss: 58.9733, MinusLogProbMetric: 58.9733, val_loss: 51.2761, val_MinusLogProbMetric: 51.2761

Epoch 2: val_loss improved from 69.12936 to 51.27611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 36s - loss: 58.9733 - MinusLogProbMetric: 58.9733 - val_loss: 51.2761 - val_MinusLogProbMetric: 51.2761 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 3/1000
2023-10-26 15:11:53.978 
Epoch 3/1000 
	 loss: 48.8651, MinusLogProbMetric: 48.8651, val_loss: 45.0256, val_MinusLogProbMetric: 45.0256

Epoch 3: val_loss improved from 51.27611 to 45.02564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 48.8651 - MinusLogProbMetric: 48.8651 - val_loss: 45.0256 - val_MinusLogProbMetric: 45.0256 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 4/1000
2023-10-26 15:12:31.318 
Epoch 4/1000 
	 loss: 43.9279, MinusLogProbMetric: 43.9279, val_loss: 42.0589, val_MinusLogProbMetric: 42.0589

Epoch 4: val_loss improved from 45.02564 to 42.05893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 43.9279 - MinusLogProbMetric: 43.9279 - val_loss: 42.0589 - val_MinusLogProbMetric: 42.0589 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 5/1000
2023-10-26 15:13:08.723 
Epoch 5/1000 
	 loss: 41.5153, MinusLogProbMetric: 41.5153, val_loss: 40.8799, val_MinusLogProbMetric: 40.8799

Epoch 5: val_loss improved from 42.05893 to 40.87987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 38s - loss: 41.5153 - MinusLogProbMetric: 41.5153 - val_loss: 40.8799 - val_MinusLogProbMetric: 40.8799 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 6/1000
2023-10-26 15:13:46.185 
Epoch 6/1000 
	 loss: 39.8566, MinusLogProbMetric: 39.8566, val_loss: 37.9481, val_MinusLogProbMetric: 37.9481

Epoch 6: val_loss improved from 40.87987 to 37.94814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 39.8566 - MinusLogProbMetric: 39.8566 - val_loss: 37.9481 - val_MinusLogProbMetric: 37.9481 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 7/1000
2023-10-26 15:14:22.574 
Epoch 7/1000 
	 loss: 38.2626, MinusLogProbMetric: 38.2626, val_loss: 38.0148, val_MinusLogProbMetric: 38.0148

Epoch 7: val_loss did not improve from 37.94814
196/196 - 36s - loss: 38.2626 - MinusLogProbMetric: 38.2626 - val_loss: 38.0148 - val_MinusLogProbMetric: 38.0148 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 8/1000
2023-10-26 15:14:59.340 
Epoch 8/1000 
	 loss: 37.3043, MinusLogProbMetric: 37.3043, val_loss: 37.1145, val_MinusLogProbMetric: 37.1145

Epoch 8: val_loss improved from 37.94814 to 37.11446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 37.3043 - MinusLogProbMetric: 37.3043 - val_loss: 37.1145 - val_MinusLogProbMetric: 37.1145 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 9/1000
2023-10-26 15:15:36.270 
Epoch 9/1000 
	 loss: 36.5228, MinusLogProbMetric: 36.5228, val_loss: 37.0859, val_MinusLogProbMetric: 37.0859

Epoch 9: val_loss improved from 37.11446 to 37.08594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 36.5228 - MinusLogProbMetric: 36.5228 - val_loss: 37.0859 - val_MinusLogProbMetric: 37.0859 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 10/1000
2023-10-26 15:16:13.525 
Epoch 10/1000 
	 loss: 36.2670, MinusLogProbMetric: 36.2670, val_loss: 36.4173, val_MinusLogProbMetric: 36.4173

Epoch 10: val_loss improved from 37.08594 to 36.41733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 36.2670 - MinusLogProbMetric: 36.2670 - val_loss: 36.4173 - val_MinusLogProbMetric: 36.4173 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 11/1000
2023-10-26 15:16:50.634 
Epoch 11/1000 
	 loss: 35.3404, MinusLogProbMetric: 35.3404, val_loss: 35.0944, val_MinusLogProbMetric: 35.0944

Epoch 11: val_loss improved from 36.41733 to 35.09439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 35.3404 - MinusLogProbMetric: 35.3404 - val_loss: 35.0944 - val_MinusLogProbMetric: 35.0944 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 12/1000
2023-10-26 15:17:27.611 
Epoch 12/1000 
	 loss: 34.8446, MinusLogProbMetric: 34.8446, val_loss: 35.1121, val_MinusLogProbMetric: 35.1121

Epoch 12: val_loss did not improve from 35.09439
196/196 - 36s - loss: 34.8446 - MinusLogProbMetric: 34.8446 - val_loss: 35.1121 - val_MinusLogProbMetric: 35.1121 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 13/1000
2023-10-26 15:18:04.611 
Epoch 13/1000 
	 loss: 34.5230, MinusLogProbMetric: 34.5230, val_loss: 36.3422, val_MinusLogProbMetric: 36.3422

Epoch 13: val_loss did not improve from 35.09439
196/196 - 37s - loss: 34.5230 - MinusLogProbMetric: 34.5230 - val_loss: 36.3422 - val_MinusLogProbMetric: 36.3422 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 14/1000
2023-10-26 15:18:40.655 
Epoch 14/1000 
	 loss: 34.1370, MinusLogProbMetric: 34.1370, val_loss: 34.7162, val_MinusLogProbMetric: 34.7162

Epoch 14: val_loss improved from 35.09439 to 34.71619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 34.1370 - MinusLogProbMetric: 34.1370 - val_loss: 34.7162 - val_MinusLogProbMetric: 34.7162 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 15/1000
2023-10-26 15:19:18.011 
Epoch 15/1000 
	 loss: 33.9531, MinusLogProbMetric: 33.9531, val_loss: 34.5527, val_MinusLogProbMetric: 34.5527

Epoch 15: val_loss improved from 34.71619 to 34.55273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 33.9531 - MinusLogProbMetric: 33.9531 - val_loss: 34.5527 - val_MinusLogProbMetric: 34.5527 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 16/1000
2023-10-26 15:19:55.339 
Epoch 16/1000 
	 loss: 33.4183, MinusLogProbMetric: 33.4183, val_loss: 33.7223, val_MinusLogProbMetric: 33.7223

Epoch 16: val_loss improved from 34.55273 to 33.72235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 33.4183 - MinusLogProbMetric: 33.4183 - val_loss: 33.7223 - val_MinusLogProbMetric: 33.7223 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 17/1000
2023-10-26 15:20:32.759 
Epoch 17/1000 
	 loss: 33.2760, MinusLogProbMetric: 33.2760, val_loss: 33.1237, val_MinusLogProbMetric: 33.1237

Epoch 17: val_loss improved from 33.72235 to 33.12373, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 38s - loss: 33.2760 - MinusLogProbMetric: 33.2760 - val_loss: 33.1237 - val_MinusLogProbMetric: 33.1237 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 18/1000
2023-10-26 15:21:09.921 
Epoch 18/1000 
	 loss: 33.2800, MinusLogProbMetric: 33.2800, val_loss: 34.2294, val_MinusLogProbMetric: 34.2294

Epoch 18: val_loss did not improve from 33.12373
196/196 - 37s - loss: 33.2800 - MinusLogProbMetric: 33.2800 - val_loss: 34.2294 - val_MinusLogProbMetric: 34.2294 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 19/1000
2023-10-26 15:21:45.879 
Epoch 19/1000 
	 loss: 32.8551, MinusLogProbMetric: 32.8551, val_loss: 34.5714, val_MinusLogProbMetric: 34.5714

Epoch 19: val_loss did not improve from 33.12373
196/196 - 36s - loss: 32.8551 - MinusLogProbMetric: 32.8551 - val_loss: 34.5714 - val_MinusLogProbMetric: 34.5714 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 20/1000
2023-10-26 15:22:22.600 
Epoch 20/1000 
	 loss: 32.6697, MinusLogProbMetric: 32.6697, val_loss: 32.8508, val_MinusLogProbMetric: 32.8508

Epoch 20: val_loss improved from 33.12373 to 32.85080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 32.6697 - MinusLogProbMetric: 32.6697 - val_loss: 32.8508 - val_MinusLogProbMetric: 32.8508 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 21/1000
2023-10-26 15:22:59.432 
Epoch 21/1000 
	 loss: 32.6408, MinusLogProbMetric: 32.6408, val_loss: 32.7978, val_MinusLogProbMetric: 32.7978

Epoch 21: val_loss improved from 32.85080 to 32.79777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 32.6408 - MinusLogProbMetric: 32.6408 - val_loss: 32.7978 - val_MinusLogProbMetric: 32.7978 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 22/1000
2023-10-26 15:23:36.939 
Epoch 22/1000 
	 loss: 32.2225, MinusLogProbMetric: 32.2225, val_loss: 32.3487, val_MinusLogProbMetric: 32.3487

Epoch 22: val_loss improved from 32.79777 to 32.34874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 32.2225 - MinusLogProbMetric: 32.2225 - val_loss: 32.3487 - val_MinusLogProbMetric: 32.3487 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 23/1000
2023-10-26 15:24:14.050 
Epoch 23/1000 
	 loss: 32.1511, MinusLogProbMetric: 32.1511, val_loss: 32.3590, val_MinusLogProbMetric: 32.3590

Epoch 23: val_loss did not improve from 32.34874
196/196 - 37s - loss: 32.1511 - MinusLogProbMetric: 32.1511 - val_loss: 32.3590 - val_MinusLogProbMetric: 32.3590 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 24/1000
2023-10-26 15:24:49.920 
Epoch 24/1000 
	 loss: 32.1039, MinusLogProbMetric: 32.1039, val_loss: 33.6530, val_MinusLogProbMetric: 33.6530

Epoch 24: val_loss did not improve from 32.34874
196/196 - 36s - loss: 32.1039 - MinusLogProbMetric: 32.1039 - val_loss: 33.6530 - val_MinusLogProbMetric: 33.6530 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 25/1000
2023-10-26 15:25:26.675 
Epoch 25/1000 
	 loss: 31.9604, MinusLogProbMetric: 31.9604, val_loss: 32.2901, val_MinusLogProbMetric: 32.2901

Epoch 25: val_loss improved from 32.34874 to 32.29007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 31.9604 - MinusLogProbMetric: 31.9604 - val_loss: 32.2901 - val_MinusLogProbMetric: 32.2901 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 26/1000
2023-10-26 15:26:03.717 
Epoch 26/1000 
	 loss: 31.9286, MinusLogProbMetric: 31.9286, val_loss: 32.2814, val_MinusLogProbMetric: 32.2814

Epoch 26: val_loss improved from 32.29007 to 32.28143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 31.9286 - MinusLogProbMetric: 31.9286 - val_loss: 32.2814 - val_MinusLogProbMetric: 32.2814 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 27/1000
2023-10-26 15:26:34.667 
Epoch 27/1000 
	 loss: 31.6146, MinusLogProbMetric: 31.6146, val_loss: 33.7199, val_MinusLogProbMetric: 33.7199

Epoch 27: val_loss did not improve from 32.28143
196/196 - 30s - loss: 31.6146 - MinusLogProbMetric: 31.6146 - val_loss: 33.7199 - val_MinusLogProbMetric: 33.7199 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 28/1000
2023-10-26 15:27:03.615 
Epoch 28/1000 
	 loss: 31.5405, MinusLogProbMetric: 31.5405, val_loss: 31.9114, val_MinusLogProbMetric: 31.9114

Epoch 28: val_loss improved from 32.28143 to 31.91142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 29s - loss: 31.5405 - MinusLogProbMetric: 31.5405 - val_loss: 31.9114 - val_MinusLogProbMetric: 31.9114 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 29/1000
2023-10-26 15:27:33.149 
Epoch 29/1000 
	 loss: 31.4888, MinusLogProbMetric: 31.4888, val_loss: 31.6527, val_MinusLogProbMetric: 31.6527

Epoch 29: val_loss improved from 31.91142 to 31.65273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 29s - loss: 31.4888 - MinusLogProbMetric: 31.4888 - val_loss: 31.6527 - val_MinusLogProbMetric: 31.6527 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 30/1000
2023-10-26 15:28:07.033 
Epoch 30/1000 
	 loss: 31.3514, MinusLogProbMetric: 31.3514, val_loss: 32.9861, val_MinusLogProbMetric: 32.9861

Epoch 30: val_loss did not improve from 31.65273
196/196 - 33s - loss: 31.3514 - MinusLogProbMetric: 31.3514 - val_loss: 32.9861 - val_MinusLogProbMetric: 32.9861 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 31/1000
2023-10-26 15:28:42.539 
Epoch 31/1000 
	 loss: 31.3572, MinusLogProbMetric: 31.3572, val_loss: 30.9057, val_MinusLogProbMetric: 30.9057

Epoch 31: val_loss improved from 31.65273 to 30.90570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 36s - loss: 31.3572 - MinusLogProbMetric: 31.3572 - val_loss: 30.9057 - val_MinusLogProbMetric: 30.9057 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 32/1000
2023-10-26 15:29:18.074 
Epoch 32/1000 
	 loss: 31.3388, MinusLogProbMetric: 31.3388, val_loss: 31.5376, val_MinusLogProbMetric: 31.5376

Epoch 32: val_loss did not improve from 30.90570
196/196 - 35s - loss: 31.3388 - MinusLogProbMetric: 31.3388 - val_loss: 31.5376 - val_MinusLogProbMetric: 31.5376 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 33/1000
2023-10-26 15:29:52.474 
Epoch 33/1000 
	 loss: 31.0848, MinusLogProbMetric: 31.0848, val_loss: 31.3775, val_MinusLogProbMetric: 31.3775

Epoch 33: val_loss did not improve from 30.90570
196/196 - 34s - loss: 31.0848 - MinusLogProbMetric: 31.0848 - val_loss: 31.3775 - val_MinusLogProbMetric: 31.3775 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 34/1000
2023-10-26 15:30:21.169 
Epoch 34/1000 
	 loss: 31.2040, MinusLogProbMetric: 31.2040, val_loss: 31.7784, val_MinusLogProbMetric: 31.7784

Epoch 34: val_loss did not improve from 30.90570
196/196 - 29s - loss: 31.2040 - MinusLogProbMetric: 31.2040 - val_loss: 31.7784 - val_MinusLogProbMetric: 31.7784 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 35/1000
2023-10-26 15:30:49.983 
Epoch 35/1000 
	 loss: 31.0591, MinusLogProbMetric: 31.0591, val_loss: 31.4620, val_MinusLogProbMetric: 31.4620

Epoch 35: val_loss did not improve from 30.90570
196/196 - 29s - loss: 31.0591 - MinusLogProbMetric: 31.0591 - val_loss: 31.4620 - val_MinusLogProbMetric: 31.4620 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 36/1000
2023-10-26 15:31:21.524 
Epoch 36/1000 
	 loss: 30.9903, MinusLogProbMetric: 30.9903, val_loss: 30.6840, val_MinusLogProbMetric: 30.6840

Epoch 36: val_loss improved from 30.90570 to 30.68402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 32s - loss: 30.9903 - MinusLogProbMetric: 30.9903 - val_loss: 30.6840 - val_MinusLogProbMetric: 30.6840 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 37/1000
2023-10-26 15:31:54.739 
Epoch 37/1000 
	 loss: 30.7561, MinusLogProbMetric: 30.7561, val_loss: 30.9829, val_MinusLogProbMetric: 30.9829

Epoch 37: val_loss did not improve from 30.68402
196/196 - 33s - loss: 30.7561 - MinusLogProbMetric: 30.7561 - val_loss: 30.9829 - val_MinusLogProbMetric: 30.9829 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 38/1000
2023-10-26 15:32:26.705 
Epoch 38/1000 
	 loss: 30.8557, MinusLogProbMetric: 30.8557, val_loss: 30.8158, val_MinusLogProbMetric: 30.8158

Epoch 38: val_loss did not improve from 30.68402
196/196 - 32s - loss: 30.8557 - MinusLogProbMetric: 30.8557 - val_loss: 30.8158 - val_MinusLogProbMetric: 30.8158 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 39/1000
2023-10-26 15:32:55.037 
Epoch 39/1000 
	 loss: 30.7629, MinusLogProbMetric: 30.7629, val_loss: 30.4302, val_MinusLogProbMetric: 30.4302

Epoch 39: val_loss improved from 30.68402 to 30.43019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 29s - loss: 30.7629 - MinusLogProbMetric: 30.7629 - val_loss: 30.4302 - val_MinusLogProbMetric: 30.4302 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 40/1000
2023-10-26 15:33:24.800 
Epoch 40/1000 
	 loss: 30.7713, MinusLogProbMetric: 30.7713, val_loss: 30.4426, val_MinusLogProbMetric: 30.4426

Epoch 40: val_loss did not improve from 30.43019
196/196 - 29s - loss: 30.7713 - MinusLogProbMetric: 30.7713 - val_loss: 30.4426 - val_MinusLogProbMetric: 30.4426 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 41/1000
2023-10-26 15:33:58.625 
Epoch 41/1000 
	 loss: 30.4544, MinusLogProbMetric: 30.4544, val_loss: 30.3400, val_MinusLogProbMetric: 30.3400

Epoch 41: val_loss improved from 30.43019 to 30.33998, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 34s - loss: 30.4544 - MinusLogProbMetric: 30.4544 - val_loss: 30.3400 - val_MinusLogProbMetric: 30.3400 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 42/1000
2023-10-26 15:34:33.884 
Epoch 42/1000 
	 loss: 30.6800, MinusLogProbMetric: 30.6800, val_loss: 30.7449, val_MinusLogProbMetric: 30.7449

Epoch 42: val_loss did not improve from 30.33998
196/196 - 35s - loss: 30.6800 - MinusLogProbMetric: 30.6800 - val_loss: 30.7449 - val_MinusLogProbMetric: 30.7449 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 43/1000
2023-10-26 15:35:04.045 
Epoch 43/1000 
	 loss: 30.6578, MinusLogProbMetric: 30.6578, val_loss: 30.5778, val_MinusLogProbMetric: 30.5778

Epoch 43: val_loss did not improve from 30.33998
196/196 - 30s - loss: 30.6578 - MinusLogProbMetric: 30.6578 - val_loss: 30.5778 - val_MinusLogProbMetric: 30.5778 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 44/1000
2023-10-26 15:35:33.092 
Epoch 44/1000 
	 loss: 30.4453, MinusLogProbMetric: 30.4453, val_loss: 31.5261, val_MinusLogProbMetric: 31.5261

Epoch 44: val_loss did not improve from 30.33998
196/196 - 29s - loss: 30.4453 - MinusLogProbMetric: 30.4453 - val_loss: 31.5261 - val_MinusLogProbMetric: 31.5261 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 45/1000
2023-10-26 15:36:03.628 
Epoch 45/1000 
	 loss: 30.3172, MinusLogProbMetric: 30.3172, val_loss: 31.4527, val_MinusLogProbMetric: 31.4527

Epoch 45: val_loss did not improve from 30.33998
196/196 - 31s - loss: 30.3172 - MinusLogProbMetric: 30.3172 - val_loss: 31.4527 - val_MinusLogProbMetric: 31.4527 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 46/1000
2023-10-26 15:36:38.137 
Epoch 46/1000 
	 loss: 30.2663, MinusLogProbMetric: 30.2663, val_loss: 30.3745, val_MinusLogProbMetric: 30.3745

Epoch 46: val_loss did not improve from 30.33998
196/196 - 35s - loss: 30.2663 - MinusLogProbMetric: 30.2663 - val_loss: 30.3745 - val_MinusLogProbMetric: 30.3745 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 47/1000
2023-10-26 15:37:11.047 
Epoch 47/1000 
	 loss: 30.4504, MinusLogProbMetric: 30.4504, val_loss: 30.4051, val_MinusLogProbMetric: 30.4051

Epoch 47: val_loss did not improve from 30.33998
196/196 - 33s - loss: 30.4504 - MinusLogProbMetric: 30.4504 - val_loss: 30.4051 - val_MinusLogProbMetric: 30.4051 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 48/1000
2023-10-26 15:37:40.003 
Epoch 48/1000 
	 loss: 30.3350, MinusLogProbMetric: 30.3350, val_loss: 30.4427, val_MinusLogProbMetric: 30.4427

Epoch 48: val_loss did not improve from 30.33998
196/196 - 29s - loss: 30.3350 - MinusLogProbMetric: 30.3350 - val_loss: 30.4427 - val_MinusLogProbMetric: 30.4427 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 49/1000
2023-10-26 15:38:13.952 
Epoch 49/1000 
	 loss: 30.0996, MinusLogProbMetric: 30.0996, val_loss: 29.6967, val_MinusLogProbMetric: 29.6967

Epoch 49: val_loss improved from 30.33998 to 29.69666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 30.0996 - MinusLogProbMetric: 30.0996 - val_loss: 29.6967 - val_MinusLogProbMetric: 29.6967 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 50/1000
2023-10-26 15:38:47.926 
Epoch 50/1000 
	 loss: 30.1748, MinusLogProbMetric: 30.1748, val_loss: 31.5111, val_MinusLogProbMetric: 31.5111

Epoch 50: val_loss did not improve from 29.69666
196/196 - 33s - loss: 30.1748 - MinusLogProbMetric: 30.1748 - val_loss: 31.5111 - val_MinusLogProbMetric: 31.5111 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 51/1000
2023-10-26 15:39:18.595 
Epoch 51/1000 
	 loss: 30.0619, MinusLogProbMetric: 30.0619, val_loss: 29.8148, val_MinusLogProbMetric: 29.8148

Epoch 51: val_loss did not improve from 29.69666
196/196 - 31s - loss: 30.0619 - MinusLogProbMetric: 30.0619 - val_loss: 29.8148 - val_MinusLogProbMetric: 29.8148 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 52/1000
2023-10-26 15:39:52.605 
Epoch 52/1000 
	 loss: 30.2288, MinusLogProbMetric: 30.2288, val_loss: 30.5922, val_MinusLogProbMetric: 30.5922

Epoch 52: val_loss did not improve from 29.69666
196/196 - 34s - loss: 30.2288 - MinusLogProbMetric: 30.2288 - val_loss: 30.5922 - val_MinusLogProbMetric: 30.5922 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 53/1000
2023-10-26 15:40:21.619 
Epoch 53/1000 
	 loss: 30.1210, MinusLogProbMetric: 30.1210, val_loss: 30.9525, val_MinusLogProbMetric: 30.9525

Epoch 53: val_loss did not improve from 29.69666
196/196 - 29s - loss: 30.1210 - MinusLogProbMetric: 30.1210 - val_loss: 30.9525 - val_MinusLogProbMetric: 30.9525 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 54/1000
2023-10-26 15:40:52.040 
Epoch 54/1000 
	 loss: 29.9967, MinusLogProbMetric: 29.9967, val_loss: 29.7371, val_MinusLogProbMetric: 29.7371

Epoch 54: val_loss did not improve from 29.69666
196/196 - 30s - loss: 29.9967 - MinusLogProbMetric: 29.9967 - val_loss: 29.7371 - val_MinusLogProbMetric: 29.7371 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 55/1000
2023-10-26 15:41:26.492 
Epoch 55/1000 
	 loss: 30.1099, MinusLogProbMetric: 30.1099, val_loss: 30.4923, val_MinusLogProbMetric: 30.4923

Epoch 55: val_loss did not improve from 29.69666
196/196 - 34s - loss: 30.1099 - MinusLogProbMetric: 30.1099 - val_loss: 30.4923 - val_MinusLogProbMetric: 30.4923 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 56/1000
2023-10-26 15:41:54.939 
Epoch 56/1000 
	 loss: 29.8671, MinusLogProbMetric: 29.8671, val_loss: 30.3523, val_MinusLogProbMetric: 30.3523

Epoch 56: val_loss did not improve from 29.69666
196/196 - 28s - loss: 29.8671 - MinusLogProbMetric: 29.8671 - val_loss: 30.3523 - val_MinusLogProbMetric: 30.3523 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 57/1000
2023-10-26 15:42:28.748 
Epoch 57/1000 
	 loss: 29.7877, MinusLogProbMetric: 29.7877, val_loss: 29.7385, val_MinusLogProbMetric: 29.7385

Epoch 57: val_loss did not improve from 29.69666
196/196 - 34s - loss: 29.7877 - MinusLogProbMetric: 29.7877 - val_loss: 29.7385 - val_MinusLogProbMetric: 29.7385 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 58/1000
2023-10-26 15:43:02.093 
Epoch 58/1000 
	 loss: 29.9486, MinusLogProbMetric: 29.9486, val_loss: 29.8226, val_MinusLogProbMetric: 29.8226

Epoch 58: val_loss did not improve from 29.69666
196/196 - 33s - loss: 29.9486 - MinusLogProbMetric: 29.9486 - val_loss: 29.8226 - val_MinusLogProbMetric: 29.8226 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 59/1000
2023-10-26 15:43:31.001 
Epoch 59/1000 
	 loss: 29.9742, MinusLogProbMetric: 29.9742, val_loss: 30.7407, val_MinusLogProbMetric: 30.7407

Epoch 59: val_loss did not improve from 29.69666
196/196 - 29s - loss: 29.9742 - MinusLogProbMetric: 29.9742 - val_loss: 30.7407 - val_MinusLogProbMetric: 30.7407 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 60/1000
2023-10-26 15:44:06.715 
Epoch 60/1000 
	 loss: 29.7677, MinusLogProbMetric: 29.7677, val_loss: 29.4338, val_MinusLogProbMetric: 29.4338

Epoch 60: val_loss improved from 29.69666 to 29.43380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 36s - loss: 29.7677 - MinusLogProbMetric: 29.7677 - val_loss: 29.4338 - val_MinusLogProbMetric: 29.4338 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 61/1000
2023-10-26 15:44:39.339 
Epoch 61/1000 
	 loss: 29.8991, MinusLogProbMetric: 29.8991, val_loss: 30.0541, val_MinusLogProbMetric: 30.0541

Epoch 61: val_loss did not improve from 29.43380
196/196 - 32s - loss: 29.8991 - MinusLogProbMetric: 29.8991 - val_loss: 30.0541 - val_MinusLogProbMetric: 30.0541 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 62/1000
2023-10-26 15:45:07.962 
Epoch 62/1000 
	 loss: 29.7526, MinusLogProbMetric: 29.7526, val_loss: 31.0643, val_MinusLogProbMetric: 31.0643

Epoch 62: val_loss did not improve from 29.43380
196/196 - 29s - loss: 29.7526 - MinusLogProbMetric: 29.7526 - val_loss: 31.0643 - val_MinusLogProbMetric: 31.0643 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 63/1000
2023-10-26 15:45:43.481 
Epoch 63/1000 
	 loss: 30.0112, MinusLogProbMetric: 30.0112, val_loss: 29.7761, val_MinusLogProbMetric: 29.7761

Epoch 63: val_loss did not improve from 29.43380
196/196 - 36s - loss: 30.0112 - MinusLogProbMetric: 30.0112 - val_loss: 29.7761 - val_MinusLogProbMetric: 29.7761 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 64/1000
2023-10-26 15:46:12.998 
Epoch 64/1000 
	 loss: 29.7236, MinusLogProbMetric: 29.7236, val_loss: 30.2711, val_MinusLogProbMetric: 30.2711

Epoch 64: val_loss did not improve from 29.43380
196/196 - 30s - loss: 29.7236 - MinusLogProbMetric: 29.7236 - val_loss: 30.2711 - val_MinusLogProbMetric: 30.2711 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 65/1000
2023-10-26 15:46:44.594 
Epoch 65/1000 
	 loss: 29.6307, MinusLogProbMetric: 29.6307, val_loss: 29.4942, val_MinusLogProbMetric: 29.4942

Epoch 65: val_loss did not improve from 29.43380
196/196 - 32s - loss: 29.6307 - MinusLogProbMetric: 29.6307 - val_loss: 29.4942 - val_MinusLogProbMetric: 29.4942 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 66/1000
2023-10-26 15:47:17.870 
Epoch 66/1000 
	 loss: 29.8720, MinusLogProbMetric: 29.8720, val_loss: 29.8244, val_MinusLogProbMetric: 29.8244

Epoch 66: val_loss did not improve from 29.43380
196/196 - 33s - loss: 29.8720 - MinusLogProbMetric: 29.8720 - val_loss: 29.8244 - val_MinusLogProbMetric: 29.8244 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 67/1000
2023-10-26 15:47:46.859 
Epoch 67/1000 
	 loss: 29.7355, MinusLogProbMetric: 29.7355, val_loss: 29.7537, val_MinusLogProbMetric: 29.7537

Epoch 67: val_loss did not improve from 29.43380
196/196 - 29s - loss: 29.7355 - MinusLogProbMetric: 29.7355 - val_loss: 29.7537 - val_MinusLogProbMetric: 29.7537 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 68/1000
2023-10-26 15:48:19.658 
Epoch 68/1000 
	 loss: 29.4449, MinusLogProbMetric: 29.4449, val_loss: 29.4441, val_MinusLogProbMetric: 29.4441

Epoch 68: val_loss did not improve from 29.43380
196/196 - 33s - loss: 29.4449 - MinusLogProbMetric: 29.4449 - val_loss: 29.4441 - val_MinusLogProbMetric: 29.4441 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 69/1000
2023-10-26 15:48:55.955 
Epoch 69/1000 
	 loss: 29.5773, MinusLogProbMetric: 29.5773, val_loss: 30.0285, val_MinusLogProbMetric: 30.0285

Epoch 69: val_loss did not improve from 29.43380
196/196 - 36s - loss: 29.5773 - MinusLogProbMetric: 29.5773 - val_loss: 30.0285 - val_MinusLogProbMetric: 30.0285 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 70/1000
2023-10-26 15:49:25.123 
Epoch 70/1000 
	 loss: 29.6363, MinusLogProbMetric: 29.6363, val_loss: 30.5957, val_MinusLogProbMetric: 30.5957

Epoch 70: val_loss did not improve from 29.43380
196/196 - 29s - loss: 29.6363 - MinusLogProbMetric: 29.6363 - val_loss: 30.5957 - val_MinusLogProbMetric: 30.5957 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 71/1000
2023-10-26 15:49:58.136 
Epoch 71/1000 
	 loss: 29.4490, MinusLogProbMetric: 29.4490, val_loss: 29.5937, val_MinusLogProbMetric: 29.5937

Epoch 71: val_loss did not improve from 29.43380
196/196 - 33s - loss: 29.4490 - MinusLogProbMetric: 29.4490 - val_loss: 29.5937 - val_MinusLogProbMetric: 29.5937 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 72/1000
2023-10-26 15:50:31.753 
Epoch 72/1000 
	 loss: 29.5467, MinusLogProbMetric: 29.5467, val_loss: 30.8323, val_MinusLogProbMetric: 30.8323

Epoch 72: val_loss did not improve from 29.43380
196/196 - 34s - loss: 29.5467 - MinusLogProbMetric: 29.5467 - val_loss: 30.8323 - val_MinusLogProbMetric: 30.8323 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 73/1000
2023-10-26 15:50:59.931 
Epoch 73/1000 
	 loss: 29.5171, MinusLogProbMetric: 29.5171, val_loss: 30.4016, val_MinusLogProbMetric: 30.4016

Epoch 73: val_loss did not improve from 29.43380
196/196 - 28s - loss: 29.5171 - MinusLogProbMetric: 29.5171 - val_loss: 30.4016 - val_MinusLogProbMetric: 30.4016 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 74/1000
2023-10-26 15:51:32.373 
Epoch 74/1000 
	 loss: 29.5454, MinusLogProbMetric: 29.5454, val_loss: 30.2472, val_MinusLogProbMetric: 30.2472

Epoch 74: val_loss did not improve from 29.43380
196/196 - 32s - loss: 29.5454 - MinusLogProbMetric: 29.5454 - val_loss: 30.2472 - val_MinusLogProbMetric: 30.2472 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 75/1000
2023-10-26 15:52:07.351 
Epoch 75/1000 
	 loss: 29.5599, MinusLogProbMetric: 29.5599, val_loss: 29.8818, val_MinusLogProbMetric: 29.8818

Epoch 75: val_loss did not improve from 29.43380
196/196 - 35s - loss: 29.5599 - MinusLogProbMetric: 29.5599 - val_loss: 29.8818 - val_MinusLogProbMetric: 29.8818 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 76/1000
2023-10-26 15:52:35.611 
Epoch 76/1000 
	 loss: 29.4487, MinusLogProbMetric: 29.4487, val_loss: 29.6872, val_MinusLogProbMetric: 29.6872

Epoch 76: val_loss did not improve from 29.43380
196/196 - 28s - loss: 29.4487 - MinusLogProbMetric: 29.4487 - val_loss: 29.6872 - val_MinusLogProbMetric: 29.6872 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 77/1000
2023-10-26 15:53:09.008 
Epoch 77/1000 
	 loss: 29.4276, MinusLogProbMetric: 29.4276, val_loss: 30.5209, val_MinusLogProbMetric: 30.5209

Epoch 77: val_loss did not improve from 29.43380
196/196 - 33s - loss: 29.4276 - MinusLogProbMetric: 29.4276 - val_loss: 30.5209 - val_MinusLogProbMetric: 30.5209 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 78/1000
2023-10-26 15:53:41.861 
Epoch 78/1000 
	 loss: 29.4988, MinusLogProbMetric: 29.4988, val_loss: 31.3747, val_MinusLogProbMetric: 31.3747

Epoch 78: val_loss did not improve from 29.43380
196/196 - 33s - loss: 29.4988 - MinusLogProbMetric: 29.4988 - val_loss: 31.3747 - val_MinusLogProbMetric: 31.3747 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 79/1000
2023-10-26 15:54:10.587 
Epoch 79/1000 
	 loss: 29.6728, MinusLogProbMetric: 29.6728, val_loss: 29.7858, val_MinusLogProbMetric: 29.7858

Epoch 79: val_loss did not improve from 29.43380
196/196 - 29s - loss: 29.6728 - MinusLogProbMetric: 29.6728 - val_loss: 29.7858 - val_MinusLogProbMetric: 29.7858 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 80/1000
2023-10-26 15:54:41.161 
Epoch 80/1000 
	 loss: 29.4018, MinusLogProbMetric: 29.4018, val_loss: 29.7901, val_MinusLogProbMetric: 29.7901

Epoch 80: val_loss did not improve from 29.43380
196/196 - 31s - loss: 29.4018 - MinusLogProbMetric: 29.4018 - val_loss: 29.7901 - val_MinusLogProbMetric: 29.7901 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 81/1000
2023-10-26 15:55:15.269 
Epoch 81/1000 
	 loss: 29.4403, MinusLogProbMetric: 29.4403, val_loss: 29.6456, val_MinusLogProbMetric: 29.6456

Epoch 81: val_loss did not improve from 29.43380
196/196 - 34s - loss: 29.4403 - MinusLogProbMetric: 29.4403 - val_loss: 29.6456 - val_MinusLogProbMetric: 29.6456 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 82/1000
2023-10-26 15:55:48.523 
Epoch 82/1000 
	 loss: 29.2219, MinusLogProbMetric: 29.2219, val_loss: 29.5520, val_MinusLogProbMetric: 29.5520

Epoch 82: val_loss did not improve from 29.43380
196/196 - 33s - loss: 29.2219 - MinusLogProbMetric: 29.2219 - val_loss: 29.5520 - val_MinusLogProbMetric: 29.5520 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 83/1000
2023-10-26 15:56:22.384 
Epoch 83/1000 
	 loss: 29.3644, MinusLogProbMetric: 29.3644, val_loss: 30.0536, val_MinusLogProbMetric: 30.0536

Epoch 83: val_loss did not improve from 29.43380
196/196 - 34s - loss: 29.3644 - MinusLogProbMetric: 29.3644 - val_loss: 30.0536 - val_MinusLogProbMetric: 30.0536 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 84/1000
2023-10-26 15:56:56.046 
Epoch 84/1000 
	 loss: 29.4603, MinusLogProbMetric: 29.4603, val_loss: 30.8447, val_MinusLogProbMetric: 30.8447

Epoch 84: val_loss did not improve from 29.43380
196/196 - 34s - loss: 29.4603 - MinusLogProbMetric: 29.4603 - val_loss: 30.8447 - val_MinusLogProbMetric: 30.8447 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 85/1000
2023-10-26 15:57:29.671 
Epoch 85/1000 
	 loss: 29.3747, MinusLogProbMetric: 29.3747, val_loss: 30.1093, val_MinusLogProbMetric: 30.1093

Epoch 85: val_loss did not improve from 29.43380
196/196 - 34s - loss: 29.3747 - MinusLogProbMetric: 29.3747 - val_loss: 30.1093 - val_MinusLogProbMetric: 30.1093 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 86/1000
2023-10-26 15:58:01.691 
Epoch 86/1000 
	 loss: 29.3246, MinusLogProbMetric: 29.3246, val_loss: 30.1000, val_MinusLogProbMetric: 30.1000

Epoch 86: val_loss did not improve from 29.43380
196/196 - 32s - loss: 29.3246 - MinusLogProbMetric: 29.3246 - val_loss: 30.1000 - val_MinusLogProbMetric: 30.1000 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 87/1000
2023-10-26 15:58:30.735 
Epoch 87/1000 
	 loss: 29.4050, MinusLogProbMetric: 29.4050, val_loss: 29.5822, val_MinusLogProbMetric: 29.5822

Epoch 87: val_loss did not improve from 29.43380
196/196 - 29s - loss: 29.4050 - MinusLogProbMetric: 29.4050 - val_loss: 29.5822 - val_MinusLogProbMetric: 29.5822 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 88/1000
2023-10-26 15:58:59.899 
Epoch 88/1000 
	 loss: 29.2991, MinusLogProbMetric: 29.2991, val_loss: 30.9004, val_MinusLogProbMetric: 30.9004

Epoch 88: val_loss did not improve from 29.43380
196/196 - 29s - loss: 29.2991 - MinusLogProbMetric: 29.2991 - val_loss: 30.9004 - val_MinusLogProbMetric: 30.9004 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 89/1000
2023-10-26 15:59:30.658 
Epoch 89/1000 
	 loss: 29.1811, MinusLogProbMetric: 29.1811, val_loss: 30.5051, val_MinusLogProbMetric: 30.5051

Epoch 89: val_loss did not improve from 29.43380
196/196 - 31s - loss: 29.1811 - MinusLogProbMetric: 29.1811 - val_loss: 30.5051 - val_MinusLogProbMetric: 30.5051 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 90/1000
2023-10-26 16:00:01.131 
Epoch 90/1000 
	 loss: 29.1059, MinusLogProbMetric: 29.1059, val_loss: 29.7864, val_MinusLogProbMetric: 29.7864

Epoch 90: val_loss did not improve from 29.43380
196/196 - 30s - loss: 29.1059 - MinusLogProbMetric: 29.1059 - val_loss: 29.7864 - val_MinusLogProbMetric: 29.7864 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 91/1000
2023-10-26 16:00:32.830 
Epoch 91/1000 
	 loss: 29.2612, MinusLogProbMetric: 29.2612, val_loss: 29.6076, val_MinusLogProbMetric: 29.6076

Epoch 91: val_loss did not improve from 29.43380
196/196 - 32s - loss: 29.2612 - MinusLogProbMetric: 29.2612 - val_loss: 29.6076 - val_MinusLogProbMetric: 29.6076 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 92/1000
2023-10-26 16:01:07.934 
Epoch 92/1000 
	 loss: 29.1872, MinusLogProbMetric: 29.1872, val_loss: 30.0816, val_MinusLogProbMetric: 30.0816

Epoch 92: val_loss did not improve from 29.43380
196/196 - 35s - loss: 29.1872 - MinusLogProbMetric: 29.1872 - val_loss: 30.0816 - val_MinusLogProbMetric: 30.0816 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 93/1000
2023-10-26 16:01:42.473 
Epoch 93/1000 
	 loss: 29.0547, MinusLogProbMetric: 29.0547, val_loss: 29.3681, val_MinusLogProbMetric: 29.3681

Epoch 93: val_loss improved from 29.43380 to 29.36813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 29.0547 - MinusLogProbMetric: 29.0547 - val_loss: 29.3681 - val_MinusLogProbMetric: 29.3681 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 94/1000
2023-10-26 16:02:12.322 
Epoch 94/1000 
	 loss: 29.1183, MinusLogProbMetric: 29.1183, val_loss: 29.2282, val_MinusLogProbMetric: 29.2282

Epoch 94: val_loss improved from 29.36813 to 29.22817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 30s - loss: 29.1183 - MinusLogProbMetric: 29.1183 - val_loss: 29.2282 - val_MinusLogProbMetric: 29.2282 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 95/1000
2023-10-26 16:02:42.221 
Epoch 95/1000 
	 loss: 29.1280, MinusLogProbMetric: 29.1280, val_loss: 29.4975, val_MinusLogProbMetric: 29.4975

Epoch 95: val_loss did not improve from 29.22817
196/196 - 29s - loss: 29.1280 - MinusLogProbMetric: 29.1280 - val_loss: 29.4975 - val_MinusLogProbMetric: 29.4975 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 96/1000
2023-10-26 16:03:12.401 
Epoch 96/1000 
	 loss: 29.0719, MinusLogProbMetric: 29.0719, val_loss: 29.9401, val_MinusLogProbMetric: 29.9401

Epoch 96: val_loss did not improve from 29.22817
196/196 - 30s - loss: 29.0719 - MinusLogProbMetric: 29.0719 - val_loss: 29.9401 - val_MinusLogProbMetric: 29.9401 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 97/1000
2023-10-26 16:03:46.029 
Epoch 97/1000 
	 loss: 29.1176, MinusLogProbMetric: 29.1176, val_loss: 29.4386, val_MinusLogProbMetric: 29.4386

Epoch 97: val_loss did not improve from 29.22817
196/196 - 34s - loss: 29.1176 - MinusLogProbMetric: 29.1176 - val_loss: 29.4386 - val_MinusLogProbMetric: 29.4386 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 98/1000
2023-10-26 16:04:19.304 
Epoch 98/1000 
	 loss: 29.3000, MinusLogProbMetric: 29.3000, val_loss: 29.0803, val_MinusLogProbMetric: 29.0803

Epoch 98: val_loss improved from 29.22817 to 29.08027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 34s - loss: 29.3000 - MinusLogProbMetric: 29.3000 - val_loss: 29.0803 - val_MinusLogProbMetric: 29.0803 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 99/1000
2023-10-26 16:04:49.151 
Epoch 99/1000 
	 loss: 29.0208, MinusLogProbMetric: 29.0208, val_loss: 29.2031, val_MinusLogProbMetric: 29.2031

Epoch 99: val_loss did not improve from 29.08027
196/196 - 29s - loss: 29.0208 - MinusLogProbMetric: 29.0208 - val_loss: 29.2031 - val_MinusLogProbMetric: 29.2031 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 100/1000
2023-10-26 16:05:18.937 
Epoch 100/1000 
	 loss: 29.1989, MinusLogProbMetric: 29.1989, val_loss: 29.1514, val_MinusLogProbMetric: 29.1514

Epoch 100: val_loss did not improve from 29.08027
196/196 - 30s - loss: 29.1989 - MinusLogProbMetric: 29.1989 - val_loss: 29.1514 - val_MinusLogProbMetric: 29.1514 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 101/1000
2023-10-26 16:05:49.692 
Epoch 101/1000 
	 loss: 29.1711, MinusLogProbMetric: 29.1711, val_loss: 29.9138, val_MinusLogProbMetric: 29.9138

Epoch 101: val_loss did not improve from 29.08027
196/196 - 31s - loss: 29.1711 - MinusLogProbMetric: 29.1711 - val_loss: 29.9138 - val_MinusLogProbMetric: 29.9138 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 102/1000
2023-10-26 16:06:24.653 
Epoch 102/1000 
	 loss: 29.0154, MinusLogProbMetric: 29.0154, val_loss: 29.1007, val_MinusLogProbMetric: 29.1007

Epoch 102: val_loss did not improve from 29.08027
196/196 - 35s - loss: 29.0154 - MinusLogProbMetric: 29.0154 - val_loss: 29.1007 - val_MinusLogProbMetric: 29.1007 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 103/1000
2023-10-26 16:07:01.567 
Epoch 103/1000 
	 loss: 28.9887, MinusLogProbMetric: 28.9887, val_loss: 29.2753, val_MinusLogProbMetric: 29.2753

Epoch 103: val_loss did not improve from 29.08027
196/196 - 37s - loss: 28.9887 - MinusLogProbMetric: 28.9887 - val_loss: 29.2753 - val_MinusLogProbMetric: 29.2753 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 104/1000
2023-10-26 16:07:35.354 
Epoch 104/1000 
	 loss: 28.9522, MinusLogProbMetric: 28.9522, val_loss: 29.3405, val_MinusLogProbMetric: 29.3405

Epoch 104: val_loss did not improve from 29.08027
196/196 - 34s - loss: 28.9522 - MinusLogProbMetric: 28.9522 - val_loss: 29.3405 - val_MinusLogProbMetric: 29.3405 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 105/1000
2023-10-26 16:08:07.893 
Epoch 105/1000 
	 loss: 29.0082, MinusLogProbMetric: 29.0082, val_loss: 29.1374, val_MinusLogProbMetric: 29.1374

Epoch 105: val_loss did not improve from 29.08027
196/196 - 33s - loss: 29.0082 - MinusLogProbMetric: 29.0082 - val_loss: 29.1374 - val_MinusLogProbMetric: 29.1374 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 106/1000
2023-10-26 16:08:44.602 
Epoch 106/1000 
	 loss: 28.9964, MinusLogProbMetric: 28.9964, val_loss: 30.3400, val_MinusLogProbMetric: 30.3400

Epoch 106: val_loss did not improve from 29.08027
196/196 - 37s - loss: 28.9964 - MinusLogProbMetric: 28.9964 - val_loss: 30.3400 - val_MinusLogProbMetric: 30.3400 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 107/1000
2023-10-26 16:09:21.381 
Epoch 107/1000 
	 loss: 29.1157, MinusLogProbMetric: 29.1157, val_loss: 29.9750, val_MinusLogProbMetric: 29.9750

Epoch 107: val_loss did not improve from 29.08027
196/196 - 37s - loss: 29.1157 - MinusLogProbMetric: 29.1157 - val_loss: 29.9750 - val_MinusLogProbMetric: 29.9750 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 108/1000
2023-10-26 16:09:58.379 
Epoch 108/1000 
	 loss: 28.8850, MinusLogProbMetric: 28.8850, val_loss: 29.2266, val_MinusLogProbMetric: 29.2266

Epoch 108: val_loss did not improve from 29.08027
196/196 - 37s - loss: 28.8850 - MinusLogProbMetric: 28.8850 - val_loss: 29.2266 - val_MinusLogProbMetric: 29.2266 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 109/1000
2023-10-26 16:10:32.994 
Epoch 109/1000 
	 loss: 29.0005, MinusLogProbMetric: 29.0005, val_loss: 30.3782, val_MinusLogProbMetric: 30.3782

Epoch 109: val_loss did not improve from 29.08027
196/196 - 35s - loss: 29.0005 - MinusLogProbMetric: 29.0005 - val_loss: 30.3782 - val_MinusLogProbMetric: 30.3782 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 110/1000
2023-10-26 16:11:07.550 
Epoch 110/1000 
	 loss: 28.8044, MinusLogProbMetric: 28.8044, val_loss: 29.6070, val_MinusLogProbMetric: 29.6070

Epoch 110: val_loss did not improve from 29.08027
196/196 - 35s - loss: 28.8044 - MinusLogProbMetric: 28.8044 - val_loss: 29.6070 - val_MinusLogProbMetric: 29.6070 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 111/1000
2023-10-26 16:11:44.377 
Epoch 111/1000 
	 loss: 28.9061, MinusLogProbMetric: 28.9061, val_loss: 29.4065, val_MinusLogProbMetric: 29.4065

Epoch 111: val_loss did not improve from 29.08027
196/196 - 37s - loss: 28.9061 - MinusLogProbMetric: 28.9061 - val_loss: 29.4065 - val_MinusLogProbMetric: 29.4065 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 112/1000
2023-10-26 16:12:21.172 
Epoch 112/1000 
	 loss: 28.9198, MinusLogProbMetric: 28.9198, val_loss: 30.3399, val_MinusLogProbMetric: 30.3399

Epoch 112: val_loss did not improve from 29.08027
196/196 - 37s - loss: 28.9198 - MinusLogProbMetric: 28.9198 - val_loss: 30.3399 - val_MinusLogProbMetric: 30.3399 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 113/1000
2023-10-26 16:12:57.139 
Epoch 113/1000 
	 loss: 28.9129, MinusLogProbMetric: 28.9129, val_loss: 29.2908, val_MinusLogProbMetric: 29.2908

Epoch 113: val_loss did not improve from 29.08027
196/196 - 36s - loss: 28.9129 - MinusLogProbMetric: 28.9129 - val_loss: 29.2908 - val_MinusLogProbMetric: 29.2908 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 114/1000
2023-10-26 16:13:33.421 
Epoch 114/1000 
	 loss: 28.7320, MinusLogProbMetric: 28.7320, val_loss: 29.2478, val_MinusLogProbMetric: 29.2478

Epoch 114: val_loss did not improve from 29.08027
196/196 - 36s - loss: 28.7320 - MinusLogProbMetric: 28.7320 - val_loss: 29.2478 - val_MinusLogProbMetric: 29.2478 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 115/1000
2023-10-26 16:14:08.252 
Epoch 115/1000 
	 loss: 28.8362, MinusLogProbMetric: 28.8362, val_loss: 29.1556, val_MinusLogProbMetric: 29.1556

Epoch 115: val_loss did not improve from 29.08027
196/196 - 35s - loss: 28.8362 - MinusLogProbMetric: 28.8362 - val_loss: 29.1556 - val_MinusLogProbMetric: 29.1556 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 116/1000
2023-10-26 16:14:42.667 
Epoch 116/1000 
	 loss: 28.7621, MinusLogProbMetric: 28.7621, val_loss: 29.3614, val_MinusLogProbMetric: 29.3614

Epoch 116: val_loss did not improve from 29.08027
196/196 - 34s - loss: 28.7621 - MinusLogProbMetric: 28.7621 - val_loss: 29.3614 - val_MinusLogProbMetric: 29.3614 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 117/1000
2023-10-26 16:15:19.490 
Epoch 117/1000 
	 loss: 28.8894, MinusLogProbMetric: 28.8894, val_loss: 29.1098, val_MinusLogProbMetric: 29.1098

Epoch 117: val_loss did not improve from 29.08027
196/196 - 37s - loss: 28.8894 - MinusLogProbMetric: 28.8894 - val_loss: 29.1098 - val_MinusLogProbMetric: 29.1098 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 118/1000
2023-10-26 16:15:56.276 
Epoch 118/1000 
	 loss: 28.8884, MinusLogProbMetric: 28.8884, val_loss: 29.1192, val_MinusLogProbMetric: 29.1192

Epoch 118: val_loss did not improve from 29.08027
196/196 - 37s - loss: 28.8884 - MinusLogProbMetric: 28.8884 - val_loss: 29.1192 - val_MinusLogProbMetric: 29.1192 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 119/1000
2023-10-26 16:16:32.841 
Epoch 119/1000 
	 loss: 28.7487, MinusLogProbMetric: 28.7487, val_loss: 28.9530, val_MinusLogProbMetric: 28.9530

Epoch 119: val_loss improved from 29.08027 to 28.95305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 28.7487 - MinusLogProbMetric: 28.7487 - val_loss: 28.9530 - val_MinusLogProbMetric: 28.9530 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 120/1000
2023-10-26 16:17:07.682 
Epoch 120/1000 
	 loss: 28.7077, MinusLogProbMetric: 28.7077, val_loss: 29.0914, val_MinusLogProbMetric: 29.0914

Epoch 120: val_loss did not improve from 28.95305
196/196 - 34s - loss: 28.7077 - MinusLogProbMetric: 28.7077 - val_loss: 29.0914 - val_MinusLogProbMetric: 29.0914 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 121/1000
2023-10-26 16:17:40.564 
Epoch 121/1000 
	 loss: 28.9181, MinusLogProbMetric: 28.9181, val_loss: 29.3294, val_MinusLogProbMetric: 29.3294

Epoch 121: val_loss did not improve from 28.95305
196/196 - 33s - loss: 28.9181 - MinusLogProbMetric: 28.9181 - val_loss: 29.3294 - val_MinusLogProbMetric: 29.3294 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 122/1000
2023-10-26 16:18:17.131 
Epoch 122/1000 
	 loss: 28.6468, MinusLogProbMetric: 28.6468, val_loss: 29.2352, val_MinusLogProbMetric: 29.2352

Epoch 122: val_loss did not improve from 28.95305
196/196 - 37s - loss: 28.6468 - MinusLogProbMetric: 28.6468 - val_loss: 29.2352 - val_MinusLogProbMetric: 29.2352 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 123/1000
2023-10-26 16:18:53.419 
Epoch 123/1000 
	 loss: 28.9085, MinusLogProbMetric: 28.9085, val_loss: 28.9122, val_MinusLogProbMetric: 28.9122

Epoch 123: val_loss improved from 28.95305 to 28.91224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 28.9085 - MinusLogProbMetric: 28.9085 - val_loss: 28.9122 - val_MinusLogProbMetric: 28.9122 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 124/1000
2023-10-26 16:19:29.529 
Epoch 124/1000 
	 loss: 28.8743, MinusLogProbMetric: 28.8743, val_loss: 29.3543, val_MinusLogProbMetric: 29.3543

Epoch 124: val_loss did not improve from 28.91224
196/196 - 35s - loss: 28.8743 - MinusLogProbMetric: 28.8743 - val_loss: 29.3543 - val_MinusLogProbMetric: 29.3543 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 125/1000
2023-10-26 16:20:06.214 
Epoch 125/1000 
	 loss: 28.6386, MinusLogProbMetric: 28.6386, val_loss: 28.9883, val_MinusLogProbMetric: 28.9883

Epoch 125: val_loss did not improve from 28.91224
196/196 - 37s - loss: 28.6386 - MinusLogProbMetric: 28.6386 - val_loss: 28.9883 - val_MinusLogProbMetric: 28.9883 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 126/1000
2023-10-26 16:20:40.282 
Epoch 126/1000 
	 loss: 28.7778, MinusLogProbMetric: 28.7778, val_loss: 29.2695, val_MinusLogProbMetric: 29.2695

Epoch 126: val_loss did not improve from 28.91224
196/196 - 34s - loss: 28.7778 - MinusLogProbMetric: 28.7778 - val_loss: 29.2695 - val_MinusLogProbMetric: 29.2695 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 127/1000
2023-10-26 16:21:17.115 
Epoch 127/1000 
	 loss: 28.7672, MinusLogProbMetric: 28.7672, val_loss: 29.2490, val_MinusLogProbMetric: 29.2490

Epoch 127: val_loss did not improve from 28.91224
196/196 - 37s - loss: 28.7672 - MinusLogProbMetric: 28.7672 - val_loss: 29.2490 - val_MinusLogProbMetric: 29.2490 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 128/1000
2023-10-26 16:21:53.928 
Epoch 128/1000 
	 loss: 28.6321, MinusLogProbMetric: 28.6321, val_loss: 29.1966, val_MinusLogProbMetric: 29.1966

Epoch 128: val_loss did not improve from 28.91224
196/196 - 37s - loss: 28.6321 - MinusLogProbMetric: 28.6321 - val_loss: 29.1966 - val_MinusLogProbMetric: 29.1966 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 129/1000
2023-10-26 16:22:28.786 
Epoch 129/1000 
	 loss: 28.6344, MinusLogProbMetric: 28.6344, val_loss: 29.0208, val_MinusLogProbMetric: 29.0208

Epoch 129: val_loss did not improve from 28.91224
196/196 - 35s - loss: 28.6344 - MinusLogProbMetric: 28.6344 - val_loss: 29.0208 - val_MinusLogProbMetric: 29.0208 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 130/1000
2023-10-26 16:23:04.087 
Epoch 130/1000 
	 loss: 28.7900, MinusLogProbMetric: 28.7900, val_loss: 29.4289, val_MinusLogProbMetric: 29.4289

Epoch 130: val_loss did not improve from 28.91224
196/196 - 35s - loss: 28.7900 - MinusLogProbMetric: 28.7900 - val_loss: 29.4289 - val_MinusLogProbMetric: 29.4289 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 131/1000
2023-10-26 16:23:36.348 
Epoch 131/1000 
	 loss: 28.6760, MinusLogProbMetric: 28.6760, val_loss: 28.8426, val_MinusLogProbMetric: 28.8426

Epoch 131: val_loss improved from 28.91224 to 28.84260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 33s - loss: 28.6760 - MinusLogProbMetric: 28.6760 - val_loss: 28.8426 - val_MinusLogProbMetric: 28.8426 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 132/1000
2023-10-26 16:24:13.379 
Epoch 132/1000 
	 loss: 28.6753, MinusLogProbMetric: 28.6753, val_loss: 28.9342, val_MinusLogProbMetric: 28.9342

Epoch 132: val_loss did not improve from 28.84260
196/196 - 37s - loss: 28.6753 - MinusLogProbMetric: 28.6753 - val_loss: 28.9342 - val_MinusLogProbMetric: 28.9342 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 133/1000
2023-10-26 16:24:44.250 
Epoch 133/1000 
	 loss: 28.7158, MinusLogProbMetric: 28.7158, val_loss: 29.5507, val_MinusLogProbMetric: 29.5507

Epoch 133: val_loss did not improve from 28.84260
196/196 - 31s - loss: 28.7158 - MinusLogProbMetric: 28.7158 - val_loss: 29.5507 - val_MinusLogProbMetric: 29.5507 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 134/1000
2023-10-26 16:25:13.073 
Epoch 134/1000 
	 loss: 28.7619, MinusLogProbMetric: 28.7619, val_loss: 29.9177, val_MinusLogProbMetric: 29.9177

Epoch 134: val_loss did not improve from 28.84260
196/196 - 29s - loss: 28.7619 - MinusLogProbMetric: 28.7619 - val_loss: 29.9177 - val_MinusLogProbMetric: 29.9177 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 135/1000
2023-10-26 16:25:42.108 
Epoch 135/1000 
	 loss: 28.7440, MinusLogProbMetric: 28.7440, val_loss: 29.9125, val_MinusLogProbMetric: 29.9125

Epoch 135: val_loss did not improve from 28.84260
196/196 - 29s - loss: 28.7440 - MinusLogProbMetric: 28.7440 - val_loss: 29.9125 - val_MinusLogProbMetric: 29.9125 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 136/1000
2023-10-26 16:26:18.500 
Epoch 136/1000 
	 loss: 28.6450, MinusLogProbMetric: 28.6450, val_loss: 29.2361, val_MinusLogProbMetric: 29.2361

Epoch 136: val_loss did not improve from 28.84260
196/196 - 36s - loss: 28.6450 - MinusLogProbMetric: 28.6450 - val_loss: 29.2361 - val_MinusLogProbMetric: 29.2361 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 137/1000
2023-10-26 16:26:55.177 
Epoch 137/1000 
	 loss: 28.7427, MinusLogProbMetric: 28.7427, val_loss: 29.1018, val_MinusLogProbMetric: 29.1018

Epoch 137: val_loss did not improve from 28.84260
196/196 - 37s - loss: 28.7427 - MinusLogProbMetric: 28.7427 - val_loss: 29.1018 - val_MinusLogProbMetric: 29.1018 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 138/1000
2023-10-26 16:27:28.939 
Epoch 138/1000 
	 loss: 28.4935, MinusLogProbMetric: 28.4935, val_loss: 28.8900, val_MinusLogProbMetric: 28.8900

Epoch 138: val_loss did not improve from 28.84260
196/196 - 34s - loss: 28.4935 - MinusLogProbMetric: 28.4935 - val_loss: 28.8900 - val_MinusLogProbMetric: 28.8900 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 139/1000
2023-10-26 16:28:05.903 
Epoch 139/1000 
	 loss: 28.7208, MinusLogProbMetric: 28.7208, val_loss: 28.9301, val_MinusLogProbMetric: 28.9301

Epoch 139: val_loss did not improve from 28.84260
196/196 - 37s - loss: 28.7208 - MinusLogProbMetric: 28.7208 - val_loss: 28.9301 - val_MinusLogProbMetric: 28.9301 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 140/1000
2023-10-26 16:28:41.722 
Epoch 140/1000 
	 loss: 28.7323, MinusLogProbMetric: 28.7323, val_loss: 29.3601, val_MinusLogProbMetric: 29.3601

Epoch 140: val_loss did not improve from 28.84260
196/196 - 36s - loss: 28.7323 - MinusLogProbMetric: 28.7323 - val_loss: 29.3601 - val_MinusLogProbMetric: 29.3601 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 141/1000
2023-10-26 16:29:18.172 
Epoch 141/1000 
	 loss: 28.7073, MinusLogProbMetric: 28.7073, val_loss: 28.7074, val_MinusLogProbMetric: 28.7074

Epoch 141: val_loss improved from 28.84260 to 28.70742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 37s - loss: 28.7073 - MinusLogProbMetric: 28.7073 - val_loss: 28.7074 - val_MinusLogProbMetric: 28.7074 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 142/1000
2023-10-26 16:29:49.689 
Epoch 142/1000 
	 loss: 28.5359, MinusLogProbMetric: 28.5359, val_loss: 28.8703, val_MinusLogProbMetric: 28.8703

Epoch 142: val_loss did not improve from 28.70742
196/196 - 31s - loss: 28.5359 - MinusLogProbMetric: 28.5359 - val_loss: 28.8703 - val_MinusLogProbMetric: 28.8703 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 143/1000
2023-10-26 16:30:26.358 
Epoch 143/1000 
	 loss: 28.5422, MinusLogProbMetric: 28.5422, val_loss: 29.2267, val_MinusLogProbMetric: 29.2267

Epoch 143: val_loss did not improve from 28.70742
196/196 - 37s - loss: 28.5422 - MinusLogProbMetric: 28.5422 - val_loss: 29.2267 - val_MinusLogProbMetric: 29.2267 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 144/1000
2023-10-26 16:31:03.061 
Epoch 144/1000 
	 loss: 28.6464, MinusLogProbMetric: 28.6464, val_loss: 29.4135, val_MinusLogProbMetric: 29.4135

Epoch 144: val_loss did not improve from 28.70742
196/196 - 37s - loss: 28.6464 - MinusLogProbMetric: 28.6464 - val_loss: 29.4135 - val_MinusLogProbMetric: 29.4135 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 145/1000
2023-10-26 16:31:37.843 
Epoch 145/1000 
	 loss: 28.6305, MinusLogProbMetric: 28.6305, val_loss: 29.4325, val_MinusLogProbMetric: 29.4325

Epoch 145: val_loss did not improve from 28.70742
196/196 - 35s - loss: 28.6305 - MinusLogProbMetric: 28.6305 - val_loss: 29.4325 - val_MinusLogProbMetric: 29.4325 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 146/1000
2023-10-26 16:32:14.224 
Epoch 146/1000 
	 loss: 28.4695, MinusLogProbMetric: 28.4695, val_loss: 29.8783, val_MinusLogProbMetric: 29.8783

Epoch 146: val_loss did not improve from 28.70742
196/196 - 36s - loss: 28.4695 - MinusLogProbMetric: 28.4695 - val_loss: 29.8783 - val_MinusLogProbMetric: 29.8783 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 147/1000
2023-10-26 16:32:51.277 
Epoch 147/1000 
	 loss: 28.6443, MinusLogProbMetric: 28.6443, val_loss: 29.3836, val_MinusLogProbMetric: 29.3836

Epoch 147: val_loss did not improve from 28.70742
196/196 - 37s - loss: 28.6443 - MinusLogProbMetric: 28.6443 - val_loss: 29.3836 - val_MinusLogProbMetric: 29.3836 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 148/1000
2023-10-26 16:33:24.789 
Epoch 148/1000 
	 loss: 28.5964, MinusLogProbMetric: 28.5964, val_loss: 29.2100, val_MinusLogProbMetric: 29.2100

Epoch 148: val_loss did not improve from 28.70742
196/196 - 34s - loss: 28.5964 - MinusLogProbMetric: 28.5964 - val_loss: 29.2100 - val_MinusLogProbMetric: 29.2100 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 149/1000
2023-10-26 16:33:56.080 
Epoch 149/1000 
	 loss: 28.4937, MinusLogProbMetric: 28.4937, val_loss: 28.8498, val_MinusLogProbMetric: 28.8498

Epoch 149: val_loss did not improve from 28.70742
196/196 - 31s - loss: 28.4937 - MinusLogProbMetric: 28.4937 - val_loss: 28.8498 - val_MinusLogProbMetric: 28.8498 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 150/1000
2023-10-26 16:34:45.750 
Epoch 150/1000 
	 loss: 28.5576, MinusLogProbMetric: 28.5576, val_loss: 29.0954, val_MinusLogProbMetric: 29.0954

Epoch 150: val_loss did not improve from 28.70742
196/196 - 50s - loss: 28.5576 - MinusLogProbMetric: 28.5576 - val_loss: 29.0954 - val_MinusLogProbMetric: 29.0954 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 151/1000
2023-10-26 16:35:22.221 
Epoch 151/1000 
	 loss: 28.5038, MinusLogProbMetric: 28.5038, val_loss: 28.9901, val_MinusLogProbMetric: 28.9901

Epoch 151: val_loss did not improve from 28.70742
196/196 - 36s - loss: 28.5038 - MinusLogProbMetric: 28.5038 - val_loss: 28.9901 - val_MinusLogProbMetric: 28.9901 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 152/1000
2023-10-26 16:35:57.860 
Epoch 152/1000 
	 loss: 28.4029, MinusLogProbMetric: 28.4029, val_loss: 30.1108, val_MinusLogProbMetric: 30.1108

Epoch 152: val_loss did not improve from 28.70742
196/196 - 36s - loss: 28.4029 - MinusLogProbMetric: 28.4029 - val_loss: 30.1108 - val_MinusLogProbMetric: 30.1108 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 153/1000
2023-10-26 16:36:33.404 
Epoch 153/1000 
	 loss: 28.4927, MinusLogProbMetric: 28.4927, val_loss: 29.2477, val_MinusLogProbMetric: 29.2477

Epoch 153: val_loss did not improve from 28.70742
196/196 - 36s - loss: 28.4927 - MinusLogProbMetric: 28.4927 - val_loss: 29.2477 - val_MinusLogProbMetric: 29.2477 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 154/1000
2023-10-26 16:37:07.974 
Epoch 154/1000 
	 loss: 28.5306, MinusLogProbMetric: 28.5306, val_loss: 29.2262, val_MinusLogProbMetric: 29.2262

Epoch 154: val_loss did not improve from 28.70742
196/196 - 35s - loss: 28.5306 - MinusLogProbMetric: 28.5306 - val_loss: 29.2262 - val_MinusLogProbMetric: 29.2262 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 155/1000
2023-10-26 16:37:40.535 
Epoch 155/1000 
	 loss: 28.3636, MinusLogProbMetric: 28.3636, val_loss: 29.1178, val_MinusLogProbMetric: 29.1178

Epoch 155: val_loss did not improve from 28.70742
196/196 - 33s - loss: 28.3636 - MinusLogProbMetric: 28.3636 - val_loss: 29.1178 - val_MinusLogProbMetric: 29.1178 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 156/1000
2023-10-26 16:38:13.145 
Epoch 156/1000 
	 loss: 28.4755, MinusLogProbMetric: 28.4755, val_loss: 29.0210, val_MinusLogProbMetric: 29.0210

Epoch 156: val_loss did not improve from 28.70742
196/196 - 33s - loss: 28.4755 - MinusLogProbMetric: 28.4755 - val_loss: 29.0210 - val_MinusLogProbMetric: 29.0210 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 157/1000
2023-10-26 16:38:45.181 
Epoch 157/1000 
	 loss: 28.5599, MinusLogProbMetric: 28.5599, val_loss: 28.9582, val_MinusLogProbMetric: 28.9582

Epoch 157: val_loss did not improve from 28.70742
196/196 - 32s - loss: 28.5599 - MinusLogProbMetric: 28.5599 - val_loss: 28.9582 - val_MinusLogProbMetric: 28.9582 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 158/1000
2023-10-26 16:39:18.987 
Epoch 158/1000 
	 loss: 28.3597, MinusLogProbMetric: 28.3597, val_loss: 29.0509, val_MinusLogProbMetric: 29.0509

Epoch 158: val_loss did not improve from 28.70742
196/196 - 34s - loss: 28.3597 - MinusLogProbMetric: 28.3597 - val_loss: 29.0509 - val_MinusLogProbMetric: 29.0509 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 159/1000
2023-10-26 16:39:53.085 
Epoch 159/1000 
	 loss: 28.3978, MinusLogProbMetric: 28.3978, val_loss: 29.3332, val_MinusLogProbMetric: 29.3332

Epoch 159: val_loss did not improve from 28.70742
196/196 - 34s - loss: 28.3978 - MinusLogProbMetric: 28.3978 - val_loss: 29.3332 - val_MinusLogProbMetric: 29.3332 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 160/1000
2023-10-26 16:40:27.279 
Epoch 160/1000 
	 loss: 28.4751, MinusLogProbMetric: 28.4751, val_loss: 28.9503, val_MinusLogProbMetric: 28.9503

Epoch 160: val_loss did not improve from 28.70742
196/196 - 34s - loss: 28.4751 - MinusLogProbMetric: 28.4751 - val_loss: 28.9503 - val_MinusLogProbMetric: 28.9503 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 161/1000
2023-10-26 16:41:01.529 
Epoch 161/1000 
	 loss: 28.3988, MinusLogProbMetric: 28.3988, val_loss: 28.8725, val_MinusLogProbMetric: 28.8725

Epoch 161: val_loss did not improve from 28.70742
196/196 - 34s - loss: 28.3988 - MinusLogProbMetric: 28.3988 - val_loss: 28.8725 - val_MinusLogProbMetric: 28.8725 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 162/1000
2023-10-26 16:41:34.881 
Epoch 162/1000 
	 loss: 28.4129, MinusLogProbMetric: 28.4129, val_loss: 29.0798, val_MinusLogProbMetric: 29.0798

Epoch 162: val_loss did not improve from 28.70742
196/196 - 33s - loss: 28.4129 - MinusLogProbMetric: 28.4129 - val_loss: 29.0798 - val_MinusLogProbMetric: 29.0798 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 163/1000
2023-10-26 16:42:08.177 
Epoch 163/1000 
	 loss: 28.5296, MinusLogProbMetric: 28.5296, val_loss: 29.0936, val_MinusLogProbMetric: 29.0936

Epoch 163: val_loss did not improve from 28.70742
196/196 - 33s - loss: 28.5296 - MinusLogProbMetric: 28.5296 - val_loss: 29.0936 - val_MinusLogProbMetric: 29.0936 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 164/1000
2023-10-26 16:42:43.533 
Epoch 164/1000 
	 loss: 28.4225, MinusLogProbMetric: 28.4225, val_loss: 28.8400, val_MinusLogProbMetric: 28.8400

Epoch 164: val_loss did not improve from 28.70742
196/196 - 35s - loss: 28.4225 - MinusLogProbMetric: 28.4225 - val_loss: 28.8400 - val_MinusLogProbMetric: 28.8400 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 165/1000
2023-10-26 16:43:17.651 
Epoch 165/1000 
	 loss: 28.3005, MinusLogProbMetric: 28.3005, val_loss: 29.0781, val_MinusLogProbMetric: 29.0781

Epoch 165: val_loss did not improve from 28.70742
196/196 - 34s - loss: 28.3005 - MinusLogProbMetric: 28.3005 - val_loss: 29.0781 - val_MinusLogProbMetric: 29.0781 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 166/1000
2023-10-26 16:43:51.421 
Epoch 166/1000 
	 loss: 28.5233, MinusLogProbMetric: 28.5233, val_loss: 30.0807, val_MinusLogProbMetric: 30.0807

Epoch 166: val_loss did not improve from 28.70742
196/196 - 34s - loss: 28.5233 - MinusLogProbMetric: 28.5233 - val_loss: 30.0807 - val_MinusLogProbMetric: 30.0807 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 167/1000
2023-10-26 16:44:24.875 
Epoch 167/1000 
	 loss: 28.4767, MinusLogProbMetric: 28.4767, val_loss: 28.8687, val_MinusLogProbMetric: 28.8687

Epoch 167: val_loss did not improve from 28.70742
196/196 - 33s - loss: 28.4767 - MinusLogProbMetric: 28.4767 - val_loss: 28.8687 - val_MinusLogProbMetric: 28.8687 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 168/1000
2023-10-26 16:44:59.573 
Epoch 168/1000 
	 loss: 28.3600, MinusLogProbMetric: 28.3600, val_loss: 29.0203, val_MinusLogProbMetric: 29.0203

Epoch 168: val_loss did not improve from 28.70742
196/196 - 35s - loss: 28.3600 - MinusLogProbMetric: 28.3600 - val_loss: 29.0203 - val_MinusLogProbMetric: 29.0203 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 169/1000
2023-10-26 16:45:33.599 
Epoch 169/1000 
	 loss: 28.4607, MinusLogProbMetric: 28.4607, val_loss: 28.9667, val_MinusLogProbMetric: 28.9667

Epoch 169: val_loss did not improve from 28.70742
196/196 - 34s - loss: 28.4607 - MinusLogProbMetric: 28.4607 - val_loss: 28.9667 - val_MinusLogProbMetric: 28.9667 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 170/1000
2023-10-26 16:46:05.997 
Epoch 170/1000 
	 loss: 28.3031, MinusLogProbMetric: 28.3031, val_loss: 28.8829, val_MinusLogProbMetric: 28.8829

Epoch 170: val_loss did not improve from 28.70742
196/196 - 32s - loss: 28.3031 - MinusLogProbMetric: 28.3031 - val_loss: 28.8829 - val_MinusLogProbMetric: 28.8829 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 171/1000
2023-10-26 16:46:39.394 
Epoch 171/1000 
	 loss: 28.2194, MinusLogProbMetric: 28.2194, val_loss: 29.6372, val_MinusLogProbMetric: 29.6372

Epoch 171: val_loss did not improve from 28.70742
196/196 - 33s - loss: 28.2194 - MinusLogProbMetric: 28.2194 - val_loss: 29.6372 - val_MinusLogProbMetric: 29.6372 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 172/1000
2023-10-26 16:47:11.506 
Epoch 172/1000 
	 loss: 28.4381, MinusLogProbMetric: 28.4381, val_loss: 28.8092, val_MinusLogProbMetric: 28.8092

Epoch 172: val_loss did not improve from 28.70742
196/196 - 32s - loss: 28.4381 - MinusLogProbMetric: 28.4381 - val_loss: 28.8092 - val_MinusLogProbMetric: 28.8092 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 173/1000
2023-10-26 16:47:46.227 
Epoch 173/1000 
	 loss: 28.4717, MinusLogProbMetric: 28.4717, val_loss: 28.9905, val_MinusLogProbMetric: 28.9905

Epoch 173: val_loss did not improve from 28.70742
196/196 - 35s - loss: 28.4717 - MinusLogProbMetric: 28.4717 - val_loss: 28.9905 - val_MinusLogProbMetric: 28.9905 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 174/1000
2023-10-26 16:48:20.855 
Epoch 174/1000 
	 loss: 28.2965, MinusLogProbMetric: 28.2965, val_loss: 28.9253, val_MinusLogProbMetric: 28.9253

Epoch 174: val_loss did not improve from 28.70742
196/196 - 35s - loss: 28.2965 - MinusLogProbMetric: 28.2965 - val_loss: 28.9253 - val_MinusLogProbMetric: 28.9253 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 175/1000
2023-10-26 16:48:55.124 
Epoch 175/1000 
	 loss: 28.3913, MinusLogProbMetric: 28.3913, val_loss: 28.6849, val_MinusLogProbMetric: 28.6849

Epoch 175: val_loss improved from 28.70742 to 28.68488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 28.3913 - MinusLogProbMetric: 28.3913 - val_loss: 28.6849 - val_MinusLogProbMetric: 28.6849 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 176/1000
2023-10-26 16:49:28.691 
Epoch 176/1000 
	 loss: 28.3244, MinusLogProbMetric: 28.3244, val_loss: 28.9608, val_MinusLogProbMetric: 28.9608

Epoch 176: val_loss did not improve from 28.68488
196/196 - 33s - loss: 28.3244 - MinusLogProbMetric: 28.3244 - val_loss: 28.9608 - val_MinusLogProbMetric: 28.9608 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 177/1000
2023-10-26 16:49:59.075 
Epoch 177/1000 
	 loss: 28.4528, MinusLogProbMetric: 28.4528, val_loss: 29.8206, val_MinusLogProbMetric: 29.8206

Epoch 177: val_loss did not improve from 28.68488
196/196 - 30s - loss: 28.4528 - MinusLogProbMetric: 28.4528 - val_loss: 29.8206 - val_MinusLogProbMetric: 29.8206 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 178/1000
2023-10-26 16:50:31.194 
Epoch 178/1000 
	 loss: 28.3059, MinusLogProbMetric: 28.3059, val_loss: 28.8161, val_MinusLogProbMetric: 28.8161

Epoch 178: val_loss did not improve from 28.68488
196/196 - 32s - loss: 28.3059 - MinusLogProbMetric: 28.3059 - val_loss: 28.8161 - val_MinusLogProbMetric: 28.8161 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 179/1000
2023-10-26 16:51:04.992 
Epoch 179/1000 
	 loss: 28.2895, MinusLogProbMetric: 28.2895, val_loss: 28.6334, val_MinusLogProbMetric: 28.6334

Epoch 179: val_loss improved from 28.68488 to 28.63338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 28.2895 - MinusLogProbMetric: 28.2895 - val_loss: 28.6334 - val_MinusLogProbMetric: 28.6334 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 180/1000
2023-10-26 16:51:41.179 
Epoch 180/1000 
	 loss: 28.2489, MinusLogProbMetric: 28.2489, val_loss: 28.7513, val_MinusLogProbMetric: 28.7513

Epoch 180: val_loss did not improve from 28.63338
196/196 - 36s - loss: 28.2489 - MinusLogProbMetric: 28.2489 - val_loss: 28.7513 - val_MinusLogProbMetric: 28.7513 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 181/1000
2023-10-26 16:52:15.715 
Epoch 181/1000 
	 loss: 28.3713, MinusLogProbMetric: 28.3713, val_loss: 29.1329, val_MinusLogProbMetric: 29.1329

Epoch 181: val_loss did not improve from 28.63338
196/196 - 34s - loss: 28.3713 - MinusLogProbMetric: 28.3713 - val_loss: 29.1329 - val_MinusLogProbMetric: 29.1329 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 182/1000
2023-10-26 16:52:47.440 
Epoch 182/1000 
	 loss: 28.2670, MinusLogProbMetric: 28.2670, val_loss: 28.8190, val_MinusLogProbMetric: 28.8190

Epoch 182: val_loss did not improve from 28.63338
196/196 - 32s - loss: 28.2670 - MinusLogProbMetric: 28.2670 - val_loss: 28.8190 - val_MinusLogProbMetric: 28.8190 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 183/1000
2023-10-26 16:53:16.547 
Epoch 183/1000 
	 loss: 28.2825, MinusLogProbMetric: 28.2825, val_loss: 30.3265, val_MinusLogProbMetric: 30.3265

Epoch 183: val_loss did not improve from 28.63338
196/196 - 29s - loss: 28.2825 - MinusLogProbMetric: 28.2825 - val_loss: 30.3265 - val_MinusLogProbMetric: 30.3265 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 184/1000
2023-10-26 16:53:45.388 
Epoch 184/1000 
	 loss: 28.3053, MinusLogProbMetric: 28.3053, val_loss: 28.5109, val_MinusLogProbMetric: 28.5109

Epoch 184: val_loss improved from 28.63338 to 28.51091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 29s - loss: 28.3053 - MinusLogProbMetric: 28.3053 - val_loss: 28.5109 - val_MinusLogProbMetric: 28.5109 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 185/1000
2023-10-26 16:54:13.938 
Epoch 185/1000 
	 loss: 28.2583, MinusLogProbMetric: 28.2583, val_loss: 28.6943, val_MinusLogProbMetric: 28.6943

Epoch 185: val_loss did not improve from 28.51091
196/196 - 28s - loss: 28.2583 - MinusLogProbMetric: 28.2583 - val_loss: 28.6943 - val_MinusLogProbMetric: 28.6943 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 186/1000
2023-10-26 16:54:47.991 
Epoch 186/1000 
	 loss: 28.2770, MinusLogProbMetric: 28.2770, val_loss: 29.6845, val_MinusLogProbMetric: 29.6845

Epoch 186: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.2770 - MinusLogProbMetric: 28.2770 - val_loss: 29.6845 - val_MinusLogProbMetric: 29.6845 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 187/1000
2023-10-26 16:55:21.816 
Epoch 187/1000 
	 loss: 28.2361, MinusLogProbMetric: 28.2361, val_loss: 29.0662, val_MinusLogProbMetric: 29.0662

Epoch 187: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.2361 - MinusLogProbMetric: 28.2361 - val_loss: 29.0662 - val_MinusLogProbMetric: 29.0662 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 188/1000
2023-10-26 16:55:58.071 
Epoch 188/1000 
	 loss: 28.3784, MinusLogProbMetric: 28.3784, val_loss: 28.8026, val_MinusLogProbMetric: 28.8026

Epoch 188: val_loss did not improve from 28.51091
196/196 - 36s - loss: 28.3784 - MinusLogProbMetric: 28.3784 - val_loss: 28.8026 - val_MinusLogProbMetric: 28.8026 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 189/1000
2023-10-26 16:56:34.413 
Epoch 189/1000 
	 loss: 28.2619, MinusLogProbMetric: 28.2619, val_loss: 28.7888, val_MinusLogProbMetric: 28.7888

Epoch 189: val_loss did not improve from 28.51091
196/196 - 36s - loss: 28.2619 - MinusLogProbMetric: 28.2619 - val_loss: 28.7888 - val_MinusLogProbMetric: 28.7888 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 190/1000
2023-10-26 16:57:08.508 
Epoch 190/1000 
	 loss: 28.2962, MinusLogProbMetric: 28.2962, val_loss: 29.2485, val_MinusLogProbMetric: 29.2485

Epoch 190: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.2962 - MinusLogProbMetric: 28.2962 - val_loss: 29.2485 - val_MinusLogProbMetric: 29.2485 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 191/1000
2023-10-26 16:57:44.484 
Epoch 191/1000 
	 loss: 28.2520, MinusLogProbMetric: 28.2520, val_loss: 28.9395, val_MinusLogProbMetric: 28.9395

Epoch 191: val_loss did not improve from 28.51091
196/196 - 36s - loss: 28.2520 - MinusLogProbMetric: 28.2520 - val_loss: 28.9395 - val_MinusLogProbMetric: 28.9395 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 192/1000
2023-10-26 16:58:19.040 
Epoch 192/1000 
	 loss: 28.1359, MinusLogProbMetric: 28.1359, val_loss: 28.6200, val_MinusLogProbMetric: 28.6200

Epoch 192: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.1359 - MinusLogProbMetric: 28.1359 - val_loss: 28.6200 - val_MinusLogProbMetric: 28.6200 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 193/1000
2023-10-26 16:58:54.684 
Epoch 193/1000 
	 loss: 28.2989, MinusLogProbMetric: 28.2989, val_loss: 28.7974, val_MinusLogProbMetric: 28.7974

Epoch 193: val_loss did not improve from 28.51091
196/196 - 36s - loss: 28.2989 - MinusLogProbMetric: 28.2989 - val_loss: 28.7974 - val_MinusLogProbMetric: 28.7974 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 194/1000
2023-10-26 16:59:30.162 
Epoch 194/1000 
	 loss: 28.3081, MinusLogProbMetric: 28.3081, val_loss: 28.7038, val_MinusLogProbMetric: 28.7038

Epoch 194: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.3081 - MinusLogProbMetric: 28.3081 - val_loss: 28.7038 - val_MinusLogProbMetric: 28.7038 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 195/1000
2023-10-26 17:00:04.951 
Epoch 195/1000 
	 loss: 28.2666, MinusLogProbMetric: 28.2666, val_loss: 28.7848, val_MinusLogProbMetric: 28.7848

Epoch 195: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.2666 - MinusLogProbMetric: 28.2666 - val_loss: 28.7848 - val_MinusLogProbMetric: 28.7848 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 196/1000
2023-10-26 17:00:38.918 
Epoch 196/1000 
	 loss: 28.2117, MinusLogProbMetric: 28.2117, val_loss: 28.6834, val_MinusLogProbMetric: 28.6834

Epoch 196: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.2117 - MinusLogProbMetric: 28.2117 - val_loss: 28.6834 - val_MinusLogProbMetric: 28.6834 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 197/1000
2023-10-26 17:01:14.025 
Epoch 197/1000 
	 loss: 28.2131, MinusLogProbMetric: 28.2131, val_loss: 29.0058, val_MinusLogProbMetric: 29.0058

Epoch 197: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.2131 - MinusLogProbMetric: 28.2131 - val_loss: 29.0058 - val_MinusLogProbMetric: 29.0058 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 198/1000
2023-10-26 17:01:48.620 
Epoch 198/1000 
	 loss: 28.2631, MinusLogProbMetric: 28.2631, val_loss: 29.1679, val_MinusLogProbMetric: 29.1679

Epoch 198: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.2631 - MinusLogProbMetric: 28.2631 - val_loss: 29.1679 - val_MinusLogProbMetric: 29.1679 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 199/1000
2023-10-26 17:02:20.101 
Epoch 199/1000 
	 loss: 28.0911, MinusLogProbMetric: 28.0911, val_loss: 28.8933, val_MinusLogProbMetric: 28.8933

Epoch 199: val_loss did not improve from 28.51091
196/196 - 31s - loss: 28.0911 - MinusLogProbMetric: 28.0911 - val_loss: 28.8933 - val_MinusLogProbMetric: 28.8933 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 200/1000
2023-10-26 17:02:54.964 
Epoch 200/1000 
	 loss: 28.2912, MinusLogProbMetric: 28.2912, val_loss: 28.7928, val_MinusLogProbMetric: 28.7928

Epoch 200: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.2912 - MinusLogProbMetric: 28.2912 - val_loss: 28.7928 - val_MinusLogProbMetric: 28.7928 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 201/1000
2023-10-26 17:03:29.089 
Epoch 201/1000 
	 loss: 28.1639, MinusLogProbMetric: 28.1639, val_loss: 29.5404, val_MinusLogProbMetric: 29.5404

Epoch 201: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.1639 - MinusLogProbMetric: 28.1639 - val_loss: 29.5404 - val_MinusLogProbMetric: 29.5404 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 202/1000
2023-10-26 17:04:03.684 
Epoch 202/1000 
	 loss: 28.1316, MinusLogProbMetric: 28.1316, val_loss: 28.6003, val_MinusLogProbMetric: 28.6003

Epoch 202: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.1316 - MinusLogProbMetric: 28.1316 - val_loss: 28.6003 - val_MinusLogProbMetric: 28.6003 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 203/1000
2023-10-26 17:04:33.546 
Epoch 203/1000 
	 loss: 28.2144, MinusLogProbMetric: 28.2144, val_loss: 28.9425, val_MinusLogProbMetric: 28.9425

Epoch 203: val_loss did not improve from 28.51091
196/196 - 30s - loss: 28.2144 - MinusLogProbMetric: 28.2144 - val_loss: 28.9425 - val_MinusLogProbMetric: 28.9425 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 204/1000
2023-10-26 17:05:06.155 
Epoch 204/1000 
	 loss: 28.0441, MinusLogProbMetric: 28.0441, val_loss: 28.8526, val_MinusLogProbMetric: 28.8526

Epoch 204: val_loss did not improve from 28.51091
196/196 - 33s - loss: 28.0441 - MinusLogProbMetric: 28.0441 - val_loss: 28.8526 - val_MinusLogProbMetric: 28.8526 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 205/1000
2023-10-26 17:05:40.099 
Epoch 205/1000 
	 loss: 28.2040, MinusLogProbMetric: 28.2040, val_loss: 28.7523, val_MinusLogProbMetric: 28.7523

Epoch 205: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.2040 - MinusLogProbMetric: 28.2040 - val_loss: 28.7523 - val_MinusLogProbMetric: 28.7523 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 206/1000
2023-10-26 17:06:14.771 
Epoch 206/1000 
	 loss: 28.1499, MinusLogProbMetric: 28.1499, val_loss: 29.2294, val_MinusLogProbMetric: 29.2294

Epoch 206: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.1499 - MinusLogProbMetric: 28.1499 - val_loss: 29.2294 - val_MinusLogProbMetric: 29.2294 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 207/1000
2023-10-26 17:06:47.390 
Epoch 207/1000 
	 loss: 28.0678, MinusLogProbMetric: 28.0678, val_loss: 29.4498, val_MinusLogProbMetric: 29.4498

Epoch 207: val_loss did not improve from 28.51091
196/196 - 33s - loss: 28.0678 - MinusLogProbMetric: 28.0678 - val_loss: 29.4498 - val_MinusLogProbMetric: 29.4498 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 208/1000
2023-10-26 17:07:22.256 
Epoch 208/1000 
	 loss: 28.1358, MinusLogProbMetric: 28.1358, val_loss: 28.6942, val_MinusLogProbMetric: 28.6942

Epoch 208: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.1358 - MinusLogProbMetric: 28.1358 - val_loss: 28.6942 - val_MinusLogProbMetric: 28.6942 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 209/1000
2023-10-26 17:07:55.971 
Epoch 209/1000 
	 loss: 28.0579, MinusLogProbMetric: 28.0579, val_loss: 29.0196, val_MinusLogProbMetric: 29.0196

Epoch 209: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.0579 - MinusLogProbMetric: 28.0579 - val_loss: 29.0196 - val_MinusLogProbMetric: 29.0196 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 210/1000
2023-10-26 17:08:29.496 
Epoch 210/1000 
	 loss: 28.1278, MinusLogProbMetric: 28.1278, val_loss: 28.6781, val_MinusLogProbMetric: 28.6781

Epoch 210: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.1278 - MinusLogProbMetric: 28.1278 - val_loss: 28.6781 - val_MinusLogProbMetric: 28.6781 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 211/1000
2023-10-26 17:09:03.125 
Epoch 211/1000 
	 loss: 28.1104, MinusLogProbMetric: 28.1104, val_loss: 28.7878, val_MinusLogProbMetric: 28.7878

Epoch 211: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.1104 - MinusLogProbMetric: 28.1104 - val_loss: 28.7878 - val_MinusLogProbMetric: 28.7878 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 212/1000
2023-10-26 17:09:37.058 
Epoch 212/1000 
	 loss: 28.0418, MinusLogProbMetric: 28.0418, val_loss: 28.7117, val_MinusLogProbMetric: 28.7117

Epoch 212: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.0418 - MinusLogProbMetric: 28.0418 - val_loss: 28.7117 - val_MinusLogProbMetric: 28.7117 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 213/1000
2023-10-26 17:10:09.504 
Epoch 213/1000 
	 loss: 28.0696, MinusLogProbMetric: 28.0696, val_loss: 29.2300, val_MinusLogProbMetric: 29.2300

Epoch 213: val_loss did not improve from 28.51091
196/196 - 32s - loss: 28.0696 - MinusLogProbMetric: 28.0696 - val_loss: 29.2300 - val_MinusLogProbMetric: 29.2300 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 214/1000
2023-10-26 17:10:44.370 
Epoch 214/1000 
	 loss: 28.1920, MinusLogProbMetric: 28.1920, val_loss: 28.8300, val_MinusLogProbMetric: 28.8300

Epoch 214: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.1920 - MinusLogProbMetric: 28.1920 - val_loss: 28.8300 - val_MinusLogProbMetric: 28.8300 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 215/1000
2023-10-26 17:11:19.193 
Epoch 215/1000 
	 loss: 28.0321, MinusLogProbMetric: 28.0321, val_loss: 28.6749, val_MinusLogProbMetric: 28.6749

Epoch 215: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.0321 - MinusLogProbMetric: 28.0321 - val_loss: 28.6749 - val_MinusLogProbMetric: 28.6749 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 216/1000
2023-10-26 17:11:53.744 
Epoch 216/1000 
	 loss: 28.0005, MinusLogProbMetric: 28.0005, val_loss: 28.6570, val_MinusLogProbMetric: 28.6570

Epoch 216: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.0005 - MinusLogProbMetric: 28.0005 - val_loss: 28.6570 - val_MinusLogProbMetric: 28.6570 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 217/1000
2023-10-26 17:12:25.852 
Epoch 217/1000 
	 loss: 28.1250, MinusLogProbMetric: 28.1250, val_loss: 28.9099, val_MinusLogProbMetric: 28.9099

Epoch 217: val_loss did not improve from 28.51091
196/196 - 32s - loss: 28.1250 - MinusLogProbMetric: 28.1250 - val_loss: 28.9099 - val_MinusLogProbMetric: 28.9099 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 218/1000
2023-10-26 17:12:59.436 
Epoch 218/1000 
	 loss: 28.1215, MinusLogProbMetric: 28.1215, val_loss: 28.8569, val_MinusLogProbMetric: 28.8569

Epoch 218: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.1215 - MinusLogProbMetric: 28.1215 - val_loss: 28.8569 - val_MinusLogProbMetric: 28.8569 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 219/1000
2023-10-26 17:13:33.131 
Epoch 219/1000 
	 loss: 28.0473, MinusLogProbMetric: 28.0473, val_loss: 29.7495, val_MinusLogProbMetric: 29.7495

Epoch 219: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.0473 - MinusLogProbMetric: 28.0473 - val_loss: 29.7495 - val_MinusLogProbMetric: 29.7495 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 220/1000
2023-10-26 17:14:07.738 
Epoch 220/1000 
	 loss: 28.0312, MinusLogProbMetric: 28.0312, val_loss: 28.8280, val_MinusLogProbMetric: 28.8280

Epoch 220: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.0312 - MinusLogProbMetric: 28.0312 - val_loss: 28.8280 - val_MinusLogProbMetric: 28.8280 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 221/1000
2023-10-26 17:14:42.208 
Epoch 221/1000 
	 loss: 28.0168, MinusLogProbMetric: 28.0168, val_loss: 30.9417, val_MinusLogProbMetric: 30.9417

Epoch 221: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.0168 - MinusLogProbMetric: 28.0168 - val_loss: 30.9417 - val_MinusLogProbMetric: 30.9417 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 222/1000
2023-10-26 17:15:14.570 
Epoch 222/1000 
	 loss: 28.0738, MinusLogProbMetric: 28.0738, val_loss: 29.3469, val_MinusLogProbMetric: 29.3469

Epoch 222: val_loss did not improve from 28.51091
196/196 - 32s - loss: 28.0738 - MinusLogProbMetric: 28.0738 - val_loss: 29.3469 - val_MinusLogProbMetric: 29.3469 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 223/1000
2023-10-26 17:15:48.701 
Epoch 223/1000 
	 loss: 28.0832, MinusLogProbMetric: 28.0832, val_loss: 29.1534, val_MinusLogProbMetric: 29.1534

Epoch 223: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.0832 - MinusLogProbMetric: 28.0832 - val_loss: 29.1534 - val_MinusLogProbMetric: 29.1534 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 224/1000
2023-10-26 17:16:23.123 
Epoch 224/1000 
	 loss: 28.0566, MinusLogProbMetric: 28.0566, val_loss: 28.7324, val_MinusLogProbMetric: 28.7324

Epoch 224: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.0566 - MinusLogProbMetric: 28.0566 - val_loss: 28.7324 - val_MinusLogProbMetric: 28.7324 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 225/1000
2023-10-26 17:16:57.635 
Epoch 225/1000 
	 loss: 28.1203, MinusLogProbMetric: 28.1203, val_loss: 30.1940, val_MinusLogProbMetric: 30.1940

Epoch 225: val_loss did not improve from 28.51091
196/196 - 35s - loss: 28.1203 - MinusLogProbMetric: 28.1203 - val_loss: 30.1940 - val_MinusLogProbMetric: 30.1940 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 226/1000
2023-10-26 17:17:31.878 
Epoch 226/1000 
	 loss: 28.1577, MinusLogProbMetric: 28.1577, val_loss: 28.7624, val_MinusLogProbMetric: 28.7624

Epoch 226: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.1577 - MinusLogProbMetric: 28.1577 - val_loss: 28.7624 - val_MinusLogProbMetric: 28.7624 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 227/1000
2023-10-26 17:18:05.695 
Epoch 227/1000 
	 loss: 28.1588, MinusLogProbMetric: 28.1588, val_loss: 28.8126, val_MinusLogProbMetric: 28.8126

Epoch 227: val_loss did not improve from 28.51091
196/196 - 34s - loss: 28.1588 - MinusLogProbMetric: 28.1588 - val_loss: 28.8126 - val_MinusLogProbMetric: 28.8126 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 228/1000
2023-10-26 17:18:36.149 
Epoch 228/1000 
	 loss: 28.0745, MinusLogProbMetric: 28.0745, val_loss: 29.7821, val_MinusLogProbMetric: 29.7821

Epoch 228: val_loss did not improve from 28.51091
196/196 - 30s - loss: 28.0745 - MinusLogProbMetric: 28.0745 - val_loss: 29.7821 - val_MinusLogProbMetric: 29.7821 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 229/1000
2023-10-26 17:19:08.029 
Epoch 229/1000 
	 loss: 28.1543, MinusLogProbMetric: 28.1543, val_loss: 29.0543, val_MinusLogProbMetric: 29.0543

Epoch 229: val_loss did not improve from 28.51091
196/196 - 32s - loss: 28.1543 - MinusLogProbMetric: 28.1543 - val_loss: 29.0543 - val_MinusLogProbMetric: 29.0543 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 230/1000
2023-10-26 17:19:39.245 
Epoch 230/1000 
	 loss: 28.0836, MinusLogProbMetric: 28.0836, val_loss: 28.5891, val_MinusLogProbMetric: 28.5891

Epoch 230: val_loss did not improve from 28.51091
196/196 - 31s - loss: 28.0836 - MinusLogProbMetric: 28.0836 - val_loss: 28.5891 - val_MinusLogProbMetric: 28.5891 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 231/1000
2023-10-26 17:20:14.575 
Epoch 231/1000 
	 loss: 27.9776, MinusLogProbMetric: 27.9776, val_loss: 28.6524, val_MinusLogProbMetric: 28.6524

Epoch 231: val_loss did not improve from 28.51091
196/196 - 35s - loss: 27.9776 - MinusLogProbMetric: 27.9776 - val_loss: 28.6524 - val_MinusLogProbMetric: 28.6524 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 232/1000
2023-10-26 17:20:47.972 
Epoch 232/1000 
	 loss: 27.9364, MinusLogProbMetric: 27.9364, val_loss: 28.9503, val_MinusLogProbMetric: 28.9503

Epoch 232: val_loss did not improve from 28.51091
196/196 - 33s - loss: 27.9364 - MinusLogProbMetric: 27.9364 - val_loss: 28.9503 - val_MinusLogProbMetric: 28.9503 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 233/1000
2023-10-26 17:21:21.409 
Epoch 233/1000 
	 loss: 28.0200, MinusLogProbMetric: 28.0200, val_loss: 28.8502, val_MinusLogProbMetric: 28.8502

Epoch 233: val_loss did not improve from 28.51091
196/196 - 33s - loss: 28.0200 - MinusLogProbMetric: 28.0200 - val_loss: 28.8502 - val_MinusLogProbMetric: 28.8502 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 234/1000
2023-10-26 17:21:54.063 
Epoch 234/1000 
	 loss: 28.0562, MinusLogProbMetric: 28.0562, val_loss: 29.2442, val_MinusLogProbMetric: 29.2442

Epoch 234: val_loss did not improve from 28.51091
196/196 - 33s - loss: 28.0562 - MinusLogProbMetric: 28.0562 - val_loss: 29.2442 - val_MinusLogProbMetric: 29.2442 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 235/1000
2023-10-26 17:22:27.896 
Epoch 235/1000 
	 loss: 27.4838, MinusLogProbMetric: 27.4838, val_loss: 28.4320, val_MinusLogProbMetric: 28.4320

Epoch 235: val_loss improved from 28.51091 to 28.43204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 34s - loss: 27.4838 - MinusLogProbMetric: 27.4838 - val_loss: 28.4320 - val_MinusLogProbMetric: 28.4320 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 236/1000
2023-10-26 17:23:03.177 
Epoch 236/1000 
	 loss: 27.4813, MinusLogProbMetric: 27.4813, val_loss: 28.3174, val_MinusLogProbMetric: 28.3174

Epoch 236: val_loss improved from 28.43204 to 28.31743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 27.4813 - MinusLogProbMetric: 27.4813 - val_loss: 28.3174 - val_MinusLogProbMetric: 28.3174 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 237/1000
2023-10-26 17:23:38.298 
Epoch 237/1000 
	 loss: 27.4577, MinusLogProbMetric: 27.4577, val_loss: 28.3517, val_MinusLogProbMetric: 28.3517

Epoch 237: val_loss did not improve from 28.31743
196/196 - 34s - loss: 27.4577 - MinusLogProbMetric: 27.4577 - val_loss: 28.3517 - val_MinusLogProbMetric: 28.3517 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 238/1000
2023-10-26 17:24:09.996 
Epoch 238/1000 
	 loss: 27.4834, MinusLogProbMetric: 27.4834, val_loss: 28.2861, val_MinusLogProbMetric: 28.2861

Epoch 238: val_loss improved from 28.31743 to 28.28609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 32s - loss: 27.4834 - MinusLogProbMetric: 27.4834 - val_loss: 28.2861 - val_MinusLogProbMetric: 28.2861 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 239/1000
2023-10-26 17:24:43.501 
Epoch 239/1000 
	 loss: 27.4373, MinusLogProbMetric: 27.4373, val_loss: 28.4269, val_MinusLogProbMetric: 28.4269

Epoch 239: val_loss did not improve from 28.28609
196/196 - 33s - loss: 27.4373 - MinusLogProbMetric: 27.4373 - val_loss: 28.4269 - val_MinusLogProbMetric: 28.4269 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 240/1000
2023-10-26 17:25:18.320 
Epoch 240/1000 
	 loss: 27.4694, MinusLogProbMetric: 27.4694, val_loss: 28.3351, val_MinusLogProbMetric: 28.3351

Epoch 240: val_loss did not improve from 28.28609
196/196 - 35s - loss: 27.4694 - MinusLogProbMetric: 27.4694 - val_loss: 28.3351 - val_MinusLogProbMetric: 28.3351 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 241/1000
2023-10-26 17:25:51.856 
Epoch 241/1000 
	 loss: 27.4392, MinusLogProbMetric: 27.4392, val_loss: 28.5312, val_MinusLogProbMetric: 28.5312

Epoch 241: val_loss did not improve from 28.28609
196/196 - 34s - loss: 27.4392 - MinusLogProbMetric: 27.4392 - val_loss: 28.5312 - val_MinusLogProbMetric: 28.5312 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 242/1000
2023-10-26 17:26:25.653 
Epoch 242/1000 
	 loss: 27.4928, MinusLogProbMetric: 27.4928, val_loss: 28.7566, val_MinusLogProbMetric: 28.7566

Epoch 242: val_loss did not improve from 28.28609
196/196 - 34s - loss: 27.4928 - MinusLogProbMetric: 27.4928 - val_loss: 28.7566 - val_MinusLogProbMetric: 28.7566 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 243/1000
2023-10-26 17:26:57.912 
Epoch 243/1000 
	 loss: 27.4391, MinusLogProbMetric: 27.4391, val_loss: 28.2878, val_MinusLogProbMetric: 28.2878

Epoch 243: val_loss did not improve from 28.28609
196/196 - 32s - loss: 27.4391 - MinusLogProbMetric: 27.4391 - val_loss: 28.2878 - val_MinusLogProbMetric: 28.2878 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 244/1000
2023-10-26 17:27:29.664 
Epoch 244/1000 
	 loss: 27.4896, MinusLogProbMetric: 27.4896, val_loss: 28.4876, val_MinusLogProbMetric: 28.4876

Epoch 244: val_loss did not improve from 28.28609
196/196 - 32s - loss: 27.4896 - MinusLogProbMetric: 27.4896 - val_loss: 28.4876 - val_MinusLogProbMetric: 28.4876 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 245/1000
2023-10-26 17:28:04.034 
Epoch 245/1000 
	 loss: 27.4290, MinusLogProbMetric: 27.4290, val_loss: 28.5219, val_MinusLogProbMetric: 28.5219

Epoch 245: val_loss did not improve from 28.28609
196/196 - 34s - loss: 27.4290 - MinusLogProbMetric: 27.4290 - val_loss: 28.5219 - val_MinusLogProbMetric: 28.5219 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 246/1000
2023-10-26 17:28:38.067 
Epoch 246/1000 
	 loss: 27.4664, MinusLogProbMetric: 27.4664, val_loss: 28.4850, val_MinusLogProbMetric: 28.4850

Epoch 246: val_loss did not improve from 28.28609
196/196 - 34s - loss: 27.4664 - MinusLogProbMetric: 27.4664 - val_loss: 28.4850 - val_MinusLogProbMetric: 28.4850 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 247/1000
2023-10-26 17:29:11.522 
Epoch 247/1000 
	 loss: 27.4913, MinusLogProbMetric: 27.4913, val_loss: 28.3006, val_MinusLogProbMetric: 28.3006

Epoch 247: val_loss did not improve from 28.28609
196/196 - 33s - loss: 27.4913 - MinusLogProbMetric: 27.4913 - val_loss: 28.3006 - val_MinusLogProbMetric: 28.3006 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 248/1000
2023-10-26 17:29:45.951 
Epoch 248/1000 
	 loss: 27.4738, MinusLogProbMetric: 27.4738, val_loss: 28.4894, val_MinusLogProbMetric: 28.4894

Epoch 248: val_loss did not improve from 28.28609
196/196 - 34s - loss: 27.4738 - MinusLogProbMetric: 27.4738 - val_loss: 28.4894 - val_MinusLogProbMetric: 28.4894 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 249/1000
2023-10-26 17:30:21.050 
Epoch 249/1000 
	 loss: 27.4513, MinusLogProbMetric: 27.4513, val_loss: 28.5033, val_MinusLogProbMetric: 28.5033

Epoch 249: val_loss did not improve from 28.28609
196/196 - 35s - loss: 27.4513 - MinusLogProbMetric: 27.4513 - val_loss: 28.5033 - val_MinusLogProbMetric: 28.5033 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 250/1000
2023-10-26 17:30:56.071 
Epoch 250/1000 
	 loss: 27.4317, MinusLogProbMetric: 27.4317, val_loss: 28.4097, val_MinusLogProbMetric: 28.4097

Epoch 250: val_loss did not improve from 28.28609
196/196 - 35s - loss: 27.4317 - MinusLogProbMetric: 27.4317 - val_loss: 28.4097 - val_MinusLogProbMetric: 28.4097 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 251/1000
2023-10-26 17:31:29.536 
Epoch 251/1000 
	 loss: 27.4317, MinusLogProbMetric: 27.4317, val_loss: 28.3447, val_MinusLogProbMetric: 28.3447

Epoch 251: val_loss did not improve from 28.28609
196/196 - 33s - loss: 27.4317 - MinusLogProbMetric: 27.4317 - val_loss: 28.3447 - val_MinusLogProbMetric: 28.3447 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 252/1000
2023-10-26 17:32:03.819 
Epoch 252/1000 
	 loss: 27.4288, MinusLogProbMetric: 27.4288, val_loss: 28.3482, val_MinusLogProbMetric: 28.3482

Epoch 252: val_loss did not improve from 28.28609
196/196 - 34s - loss: 27.4288 - MinusLogProbMetric: 27.4288 - val_loss: 28.3482 - val_MinusLogProbMetric: 28.3482 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 253/1000
2023-10-26 17:32:38.222 
Epoch 253/1000 
	 loss: 27.4340, MinusLogProbMetric: 27.4340, val_loss: 28.3044, val_MinusLogProbMetric: 28.3044

Epoch 253: val_loss did not improve from 28.28609
196/196 - 34s - loss: 27.4340 - MinusLogProbMetric: 27.4340 - val_loss: 28.3044 - val_MinusLogProbMetric: 28.3044 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 254/1000
2023-10-26 17:33:13.051 
Epoch 254/1000 
	 loss: 27.4128, MinusLogProbMetric: 27.4128, val_loss: 28.2804, val_MinusLogProbMetric: 28.2804

Epoch 254: val_loss improved from 28.28609 to 28.28044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 27.4128 - MinusLogProbMetric: 27.4128 - val_loss: 28.2804 - val_MinusLogProbMetric: 28.2804 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 255/1000
2023-10-26 17:33:45.783 
Epoch 255/1000 
	 loss: 27.4813, MinusLogProbMetric: 27.4813, val_loss: 28.3721, val_MinusLogProbMetric: 28.3721

Epoch 255: val_loss did not improve from 28.28044
196/196 - 32s - loss: 27.4813 - MinusLogProbMetric: 27.4813 - val_loss: 28.3721 - val_MinusLogProbMetric: 28.3721 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 256/1000
2023-10-26 17:34:19.170 
Epoch 256/1000 
	 loss: 27.4852, MinusLogProbMetric: 27.4852, val_loss: 28.4826, val_MinusLogProbMetric: 28.4826

Epoch 256: val_loss did not improve from 28.28044
196/196 - 33s - loss: 27.4852 - MinusLogProbMetric: 27.4852 - val_loss: 28.4826 - val_MinusLogProbMetric: 28.4826 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 257/1000
2023-10-26 17:34:54.052 
Epoch 257/1000 
	 loss: 27.4483, MinusLogProbMetric: 27.4483, val_loss: 29.1065, val_MinusLogProbMetric: 29.1065

Epoch 257: val_loss did not improve from 28.28044
196/196 - 35s - loss: 27.4483 - MinusLogProbMetric: 27.4483 - val_loss: 29.1065 - val_MinusLogProbMetric: 29.1065 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 258/1000
2023-10-26 17:35:27.811 
Epoch 258/1000 
	 loss: 27.4540, MinusLogProbMetric: 27.4540, val_loss: 28.4168, val_MinusLogProbMetric: 28.4168

Epoch 258: val_loss did not improve from 28.28044
196/196 - 34s - loss: 27.4540 - MinusLogProbMetric: 27.4540 - val_loss: 28.4168 - val_MinusLogProbMetric: 28.4168 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 259/1000
2023-10-26 17:36:02.510 
Epoch 259/1000 
	 loss: 27.4408, MinusLogProbMetric: 27.4408, val_loss: 28.3625, val_MinusLogProbMetric: 28.3625

Epoch 259: val_loss did not improve from 28.28044
196/196 - 35s - loss: 27.4408 - MinusLogProbMetric: 27.4408 - val_loss: 28.3625 - val_MinusLogProbMetric: 28.3625 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 260/1000
2023-10-26 17:36:37.290 
Epoch 260/1000 
	 loss: 27.4856, MinusLogProbMetric: 27.4856, val_loss: 28.3711, val_MinusLogProbMetric: 28.3711

Epoch 260: val_loss did not improve from 28.28044
196/196 - 35s - loss: 27.4856 - MinusLogProbMetric: 27.4856 - val_loss: 28.3711 - val_MinusLogProbMetric: 28.3711 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 261/1000
2023-10-26 17:37:11.918 
Epoch 261/1000 
	 loss: 27.4417, MinusLogProbMetric: 27.4417, val_loss: 28.4052, val_MinusLogProbMetric: 28.4052

Epoch 261: val_loss did not improve from 28.28044
196/196 - 35s - loss: 27.4417 - MinusLogProbMetric: 27.4417 - val_loss: 28.4052 - val_MinusLogProbMetric: 28.4052 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 262/1000
2023-10-26 17:37:46.383 
Epoch 262/1000 
	 loss: 27.4124, MinusLogProbMetric: 27.4124, val_loss: 28.2752, val_MinusLogProbMetric: 28.2752

Epoch 262: val_loss improved from 28.28044 to 28.27519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 27.4124 - MinusLogProbMetric: 27.4124 - val_loss: 28.2752 - val_MinusLogProbMetric: 28.2752 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 263/1000
2023-10-26 17:38:21.847 
Epoch 263/1000 
	 loss: 27.4384, MinusLogProbMetric: 27.4384, val_loss: 28.3661, val_MinusLogProbMetric: 28.3661

Epoch 263: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4384 - MinusLogProbMetric: 27.4384 - val_loss: 28.3661 - val_MinusLogProbMetric: 28.3661 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 264/1000
2023-10-26 17:38:56.189 
Epoch 264/1000 
	 loss: 27.4292, MinusLogProbMetric: 27.4292, val_loss: 28.4424, val_MinusLogProbMetric: 28.4424

Epoch 264: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4292 - MinusLogProbMetric: 27.4292 - val_loss: 28.4424 - val_MinusLogProbMetric: 28.4424 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 265/1000
2023-10-26 17:39:30.293 
Epoch 265/1000 
	 loss: 27.4778, MinusLogProbMetric: 27.4778, val_loss: 28.3395, val_MinusLogProbMetric: 28.3395

Epoch 265: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4778 - MinusLogProbMetric: 27.4778 - val_loss: 28.3395 - val_MinusLogProbMetric: 28.3395 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 266/1000
2023-10-26 17:40:04.617 
Epoch 266/1000 
	 loss: 27.3962, MinusLogProbMetric: 27.3962, val_loss: 28.6079, val_MinusLogProbMetric: 28.6079

Epoch 266: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3962 - MinusLogProbMetric: 27.3962 - val_loss: 28.6079 - val_MinusLogProbMetric: 28.6079 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 267/1000
2023-10-26 17:40:39.215 
Epoch 267/1000 
	 loss: 27.4615, MinusLogProbMetric: 27.4615, val_loss: 28.3524, val_MinusLogProbMetric: 28.3524

Epoch 267: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4615 - MinusLogProbMetric: 27.4615 - val_loss: 28.3524 - val_MinusLogProbMetric: 28.3524 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 268/1000
2023-10-26 17:41:13.460 
Epoch 268/1000 
	 loss: 27.4087, MinusLogProbMetric: 27.4087, val_loss: 28.3495, val_MinusLogProbMetric: 28.3495

Epoch 268: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4087 - MinusLogProbMetric: 27.4087 - val_loss: 28.3495 - val_MinusLogProbMetric: 28.3495 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 269/1000
2023-10-26 17:41:48.640 
Epoch 269/1000 
	 loss: 27.4129, MinusLogProbMetric: 27.4129, val_loss: 28.5237, val_MinusLogProbMetric: 28.5237

Epoch 269: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4129 - MinusLogProbMetric: 27.4129 - val_loss: 28.5237 - val_MinusLogProbMetric: 28.5237 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 270/1000
2023-10-26 17:42:23.122 
Epoch 270/1000 
	 loss: 27.4634, MinusLogProbMetric: 27.4634, val_loss: 28.3163, val_MinusLogProbMetric: 28.3163

Epoch 270: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4634 - MinusLogProbMetric: 27.4634 - val_loss: 28.3163 - val_MinusLogProbMetric: 28.3163 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 271/1000
2023-10-26 17:42:57.624 
Epoch 271/1000 
	 loss: 27.3994, MinusLogProbMetric: 27.3994, val_loss: 28.5229, val_MinusLogProbMetric: 28.5229

Epoch 271: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3994 - MinusLogProbMetric: 27.3994 - val_loss: 28.5229 - val_MinusLogProbMetric: 28.5229 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 272/1000
2023-10-26 17:43:30.954 
Epoch 272/1000 
	 loss: 27.4432, MinusLogProbMetric: 27.4432, val_loss: 28.4386, val_MinusLogProbMetric: 28.4386

Epoch 272: val_loss did not improve from 28.27519
196/196 - 33s - loss: 27.4432 - MinusLogProbMetric: 27.4432 - val_loss: 28.4386 - val_MinusLogProbMetric: 28.4386 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 273/1000
2023-10-26 17:44:01.676 
Epoch 273/1000 
	 loss: 27.4221, MinusLogProbMetric: 27.4221, val_loss: 28.3231, val_MinusLogProbMetric: 28.3231

Epoch 273: val_loss did not improve from 28.27519
196/196 - 31s - loss: 27.4221 - MinusLogProbMetric: 27.4221 - val_loss: 28.3231 - val_MinusLogProbMetric: 28.3231 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 274/1000
2023-10-26 17:44:31.515 
Epoch 274/1000 
	 loss: 27.4058, MinusLogProbMetric: 27.4058, val_loss: 28.9527, val_MinusLogProbMetric: 28.9527

Epoch 274: val_loss did not improve from 28.27519
196/196 - 30s - loss: 27.4058 - MinusLogProbMetric: 27.4058 - val_loss: 28.9527 - val_MinusLogProbMetric: 28.9527 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 275/1000
2023-10-26 17:45:02.516 
Epoch 275/1000 
	 loss: 27.4867, MinusLogProbMetric: 27.4867, val_loss: 28.3413, val_MinusLogProbMetric: 28.3413

Epoch 275: val_loss did not improve from 28.27519
196/196 - 31s - loss: 27.4867 - MinusLogProbMetric: 27.4867 - val_loss: 28.3413 - val_MinusLogProbMetric: 28.3413 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 276/1000
2023-10-26 17:45:37.149 
Epoch 276/1000 
	 loss: 27.4171, MinusLogProbMetric: 27.4171, val_loss: 28.5321, val_MinusLogProbMetric: 28.5321

Epoch 276: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4171 - MinusLogProbMetric: 27.4171 - val_loss: 28.5321 - val_MinusLogProbMetric: 28.5321 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 277/1000
2023-10-26 17:46:11.759 
Epoch 277/1000 
	 loss: 27.4044, MinusLogProbMetric: 27.4044, val_loss: 28.4299, val_MinusLogProbMetric: 28.4299

Epoch 277: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4044 - MinusLogProbMetric: 27.4044 - val_loss: 28.4299 - val_MinusLogProbMetric: 28.4299 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 278/1000
2023-10-26 17:46:46.828 
Epoch 278/1000 
	 loss: 27.4378, MinusLogProbMetric: 27.4378, val_loss: 28.5169, val_MinusLogProbMetric: 28.5169

Epoch 278: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4378 - MinusLogProbMetric: 27.4378 - val_loss: 28.5169 - val_MinusLogProbMetric: 28.5169 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 279/1000
2023-10-26 17:47:20.819 
Epoch 279/1000 
	 loss: 27.4075, MinusLogProbMetric: 27.4075, val_loss: 28.3030, val_MinusLogProbMetric: 28.3030

Epoch 279: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4075 - MinusLogProbMetric: 27.4075 - val_loss: 28.3030 - val_MinusLogProbMetric: 28.3030 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 280/1000
2023-10-26 17:47:55.233 
Epoch 280/1000 
	 loss: 27.4946, MinusLogProbMetric: 27.4946, val_loss: 28.4684, val_MinusLogProbMetric: 28.4684

Epoch 280: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4946 - MinusLogProbMetric: 27.4946 - val_loss: 28.4684 - val_MinusLogProbMetric: 28.4684 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 281/1000
2023-10-26 17:48:29.316 
Epoch 281/1000 
	 loss: 27.4726, MinusLogProbMetric: 27.4726, val_loss: 28.4782, val_MinusLogProbMetric: 28.4782

Epoch 281: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4726 - MinusLogProbMetric: 27.4726 - val_loss: 28.4782 - val_MinusLogProbMetric: 28.4782 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 282/1000
2023-10-26 17:49:03.724 
Epoch 282/1000 
	 loss: 27.4315, MinusLogProbMetric: 27.4315, val_loss: 28.6025, val_MinusLogProbMetric: 28.6025

Epoch 282: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4315 - MinusLogProbMetric: 27.4315 - val_loss: 28.6025 - val_MinusLogProbMetric: 28.6025 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 283/1000
2023-10-26 17:49:38.311 
Epoch 283/1000 
	 loss: 27.3781, MinusLogProbMetric: 27.3781, val_loss: 28.6316, val_MinusLogProbMetric: 28.6316

Epoch 283: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.3781 - MinusLogProbMetric: 27.3781 - val_loss: 28.6316 - val_MinusLogProbMetric: 28.6316 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 284/1000
2023-10-26 17:50:12.408 
Epoch 284/1000 
	 loss: 27.4665, MinusLogProbMetric: 27.4665, val_loss: 28.3720, val_MinusLogProbMetric: 28.3720

Epoch 284: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4665 - MinusLogProbMetric: 27.4665 - val_loss: 28.3720 - val_MinusLogProbMetric: 28.3720 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 285/1000
2023-10-26 17:50:47.166 
Epoch 285/1000 
	 loss: 27.4272, MinusLogProbMetric: 27.4272, val_loss: 28.3496, val_MinusLogProbMetric: 28.3496

Epoch 285: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4272 - MinusLogProbMetric: 27.4272 - val_loss: 28.3496 - val_MinusLogProbMetric: 28.3496 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 286/1000
2023-10-26 17:51:21.641 
Epoch 286/1000 
	 loss: 27.3810, MinusLogProbMetric: 27.3810, val_loss: 28.5237, val_MinusLogProbMetric: 28.5237

Epoch 286: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3810 - MinusLogProbMetric: 27.3810 - val_loss: 28.5237 - val_MinusLogProbMetric: 28.5237 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 287/1000
2023-10-26 17:51:56.561 
Epoch 287/1000 
	 loss: 27.3617, MinusLogProbMetric: 27.3617, val_loss: 28.4393, val_MinusLogProbMetric: 28.4393

Epoch 287: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.3617 - MinusLogProbMetric: 27.3617 - val_loss: 28.4393 - val_MinusLogProbMetric: 28.4393 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 288/1000
2023-10-26 17:52:31.044 
Epoch 288/1000 
	 loss: 27.3823, MinusLogProbMetric: 27.3823, val_loss: 28.4056, val_MinusLogProbMetric: 28.4056

Epoch 288: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3823 - MinusLogProbMetric: 27.3823 - val_loss: 28.4056 - val_MinusLogProbMetric: 28.4056 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 289/1000
2023-10-26 17:53:05.351 
Epoch 289/1000 
	 loss: 27.4643, MinusLogProbMetric: 27.4643, val_loss: 28.7345, val_MinusLogProbMetric: 28.7345

Epoch 289: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4643 - MinusLogProbMetric: 27.4643 - val_loss: 28.7345 - val_MinusLogProbMetric: 28.7345 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 290/1000
2023-10-26 17:53:39.814 
Epoch 290/1000 
	 loss: 27.3915, MinusLogProbMetric: 27.3915, val_loss: 28.3979, val_MinusLogProbMetric: 28.3979

Epoch 290: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3915 - MinusLogProbMetric: 27.3915 - val_loss: 28.3979 - val_MinusLogProbMetric: 28.3979 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 291/1000
2023-10-26 17:54:14.591 
Epoch 291/1000 
	 loss: 27.4180, MinusLogProbMetric: 27.4180, val_loss: 28.4515, val_MinusLogProbMetric: 28.4515

Epoch 291: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4180 - MinusLogProbMetric: 27.4180 - val_loss: 28.4515 - val_MinusLogProbMetric: 28.4515 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 292/1000
2023-10-26 17:54:48.744 
Epoch 292/1000 
	 loss: 27.3621, MinusLogProbMetric: 27.3621, val_loss: 28.5836, val_MinusLogProbMetric: 28.5836

Epoch 292: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3621 - MinusLogProbMetric: 27.3621 - val_loss: 28.5836 - val_MinusLogProbMetric: 28.5836 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 293/1000
2023-10-26 17:55:22.674 
Epoch 293/1000 
	 loss: 27.4220, MinusLogProbMetric: 27.4220, val_loss: 28.4519, val_MinusLogProbMetric: 28.4519

Epoch 293: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4220 - MinusLogProbMetric: 27.4220 - val_loss: 28.4519 - val_MinusLogProbMetric: 28.4519 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 294/1000
2023-10-26 17:55:56.956 
Epoch 294/1000 
	 loss: 27.3524, MinusLogProbMetric: 27.3524, val_loss: 28.3601, val_MinusLogProbMetric: 28.3601

Epoch 294: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3524 - MinusLogProbMetric: 27.3524 - val_loss: 28.3601 - val_MinusLogProbMetric: 28.3601 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 295/1000
2023-10-26 17:56:31.369 
Epoch 295/1000 
	 loss: 27.4407, MinusLogProbMetric: 27.4407, val_loss: 28.3459, val_MinusLogProbMetric: 28.3459

Epoch 295: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4407 - MinusLogProbMetric: 27.4407 - val_loss: 28.3459 - val_MinusLogProbMetric: 28.3459 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 296/1000
2023-10-26 17:57:05.744 
Epoch 296/1000 
	 loss: 27.3658, MinusLogProbMetric: 27.3658, val_loss: 28.4798, val_MinusLogProbMetric: 28.4798

Epoch 296: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3658 - MinusLogProbMetric: 27.3658 - val_loss: 28.4798 - val_MinusLogProbMetric: 28.4798 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 297/1000
2023-10-26 17:57:39.864 
Epoch 297/1000 
	 loss: 27.3753, MinusLogProbMetric: 27.3753, val_loss: 28.3408, val_MinusLogProbMetric: 28.3408

Epoch 297: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3753 - MinusLogProbMetric: 27.3753 - val_loss: 28.3408 - val_MinusLogProbMetric: 28.3408 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 298/1000
2023-10-26 17:58:14.062 
Epoch 298/1000 
	 loss: 27.3644, MinusLogProbMetric: 27.3644, val_loss: 28.4339, val_MinusLogProbMetric: 28.4339

Epoch 298: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3644 - MinusLogProbMetric: 27.3644 - val_loss: 28.4339 - val_MinusLogProbMetric: 28.4339 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 299/1000
2023-10-26 17:58:48.489 
Epoch 299/1000 
	 loss: 27.4089, MinusLogProbMetric: 27.4089, val_loss: 28.3858, val_MinusLogProbMetric: 28.3858

Epoch 299: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4089 - MinusLogProbMetric: 27.4089 - val_loss: 28.3858 - val_MinusLogProbMetric: 28.3858 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 300/1000
2023-10-26 17:59:22.938 
Epoch 300/1000 
	 loss: 27.3531, MinusLogProbMetric: 27.3531, val_loss: 28.4764, val_MinusLogProbMetric: 28.4764

Epoch 300: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3531 - MinusLogProbMetric: 27.3531 - val_loss: 28.4764 - val_MinusLogProbMetric: 28.4764 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 301/1000
2023-10-26 17:59:57.250 
Epoch 301/1000 
	 loss: 27.3618, MinusLogProbMetric: 27.3618, val_loss: 28.6685, val_MinusLogProbMetric: 28.6685

Epoch 301: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3618 - MinusLogProbMetric: 27.3618 - val_loss: 28.6685 - val_MinusLogProbMetric: 28.6685 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 302/1000
2023-10-26 18:00:32.038 
Epoch 302/1000 
	 loss: 27.3886, MinusLogProbMetric: 27.3886, val_loss: 28.3856, val_MinusLogProbMetric: 28.3856

Epoch 302: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.3886 - MinusLogProbMetric: 27.3886 - val_loss: 28.3856 - val_MinusLogProbMetric: 28.3856 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 303/1000
2023-10-26 18:01:06.433 
Epoch 303/1000 
	 loss: 27.3537, MinusLogProbMetric: 27.3537, val_loss: 28.2848, val_MinusLogProbMetric: 28.2848

Epoch 303: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3537 - MinusLogProbMetric: 27.3537 - val_loss: 28.2848 - val_MinusLogProbMetric: 28.2848 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 304/1000
2023-10-26 18:01:41.014 
Epoch 304/1000 
	 loss: 27.3617, MinusLogProbMetric: 27.3617, val_loss: 28.4861, val_MinusLogProbMetric: 28.4861

Epoch 304: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.3617 - MinusLogProbMetric: 27.3617 - val_loss: 28.4861 - val_MinusLogProbMetric: 28.4861 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 305/1000
2023-10-26 18:02:15.659 
Epoch 305/1000 
	 loss: 27.3540, MinusLogProbMetric: 27.3540, val_loss: 28.7983, val_MinusLogProbMetric: 28.7983

Epoch 305: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.3540 - MinusLogProbMetric: 27.3540 - val_loss: 28.7983 - val_MinusLogProbMetric: 28.7983 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 306/1000
2023-10-26 18:02:50.222 
Epoch 306/1000 
	 loss: 27.4255, MinusLogProbMetric: 27.4255, val_loss: 28.3840, val_MinusLogProbMetric: 28.3840

Epoch 306: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4255 - MinusLogProbMetric: 27.4255 - val_loss: 28.3840 - val_MinusLogProbMetric: 28.3840 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 307/1000
2023-10-26 18:03:25.063 
Epoch 307/1000 
	 loss: 27.4090, MinusLogProbMetric: 27.4090, val_loss: 28.4028, val_MinusLogProbMetric: 28.4028

Epoch 307: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.4090 - MinusLogProbMetric: 27.4090 - val_loss: 28.4028 - val_MinusLogProbMetric: 28.4028 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 308/1000
2023-10-26 18:03:59.638 
Epoch 308/1000 
	 loss: 27.3578, MinusLogProbMetric: 27.3578, val_loss: 28.5194, val_MinusLogProbMetric: 28.5194

Epoch 308: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.3578 - MinusLogProbMetric: 27.3578 - val_loss: 28.5194 - val_MinusLogProbMetric: 28.5194 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 309/1000
2023-10-26 18:04:34.168 
Epoch 309/1000 
	 loss: 27.3900, MinusLogProbMetric: 27.3900, val_loss: 28.5290, val_MinusLogProbMetric: 28.5290

Epoch 309: val_loss did not improve from 28.27519
196/196 - 35s - loss: 27.3900 - MinusLogProbMetric: 27.3900 - val_loss: 28.5290 - val_MinusLogProbMetric: 28.5290 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 310/1000
2023-10-26 18:05:07.864 
Epoch 310/1000 
	 loss: 27.4036, MinusLogProbMetric: 27.4036, val_loss: 28.3881, val_MinusLogProbMetric: 28.3881

Epoch 310: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.4036 - MinusLogProbMetric: 27.4036 - val_loss: 28.3881 - val_MinusLogProbMetric: 28.3881 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 311/1000
2023-10-26 18:05:41.804 
Epoch 311/1000 
	 loss: 27.3272, MinusLogProbMetric: 27.3272, val_loss: 28.3500, val_MinusLogProbMetric: 28.3500

Epoch 311: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3272 - MinusLogProbMetric: 27.3272 - val_loss: 28.3500 - val_MinusLogProbMetric: 28.3500 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 312/1000
2023-10-26 18:06:16.153 
Epoch 312/1000 
	 loss: 27.3600, MinusLogProbMetric: 27.3600, val_loss: 28.4768, val_MinusLogProbMetric: 28.4768

Epoch 312: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.3600 - MinusLogProbMetric: 27.3600 - val_loss: 28.4768 - val_MinusLogProbMetric: 28.4768 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 313/1000
2023-10-26 18:06:49.761 
Epoch 313/1000 
	 loss: 27.1902, MinusLogProbMetric: 27.1902, val_loss: 28.2877, val_MinusLogProbMetric: 28.2877

Epoch 313: val_loss did not improve from 28.27519
196/196 - 34s - loss: 27.1902 - MinusLogProbMetric: 27.1902 - val_loss: 28.2877 - val_MinusLogProbMetric: 28.2877 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 314/1000
2023-10-26 18:07:19.771 
Epoch 314/1000 
	 loss: 27.1838, MinusLogProbMetric: 27.1838, val_loss: 28.2553, val_MinusLogProbMetric: 28.2553

Epoch 314: val_loss improved from 28.27519 to 28.25530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 31s - loss: 27.1838 - MinusLogProbMetric: 27.1838 - val_loss: 28.2553 - val_MinusLogProbMetric: 28.2553 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 315/1000
2023-10-26 18:07:51.907 
Epoch 315/1000 
	 loss: 27.1710, MinusLogProbMetric: 27.1710, val_loss: 28.2871, val_MinusLogProbMetric: 28.2871

Epoch 315: val_loss did not improve from 28.25530
196/196 - 32s - loss: 27.1710 - MinusLogProbMetric: 27.1710 - val_loss: 28.2871 - val_MinusLogProbMetric: 28.2871 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 316/1000
2023-10-26 18:08:26.315 
Epoch 316/1000 
	 loss: 27.1715, MinusLogProbMetric: 27.1715, val_loss: 28.2897, val_MinusLogProbMetric: 28.2897

Epoch 316: val_loss did not improve from 28.25530
196/196 - 34s - loss: 27.1715 - MinusLogProbMetric: 27.1715 - val_loss: 28.2897 - val_MinusLogProbMetric: 28.2897 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 317/1000
2023-10-26 18:09:00.229 
Epoch 317/1000 
	 loss: 27.1727, MinusLogProbMetric: 27.1727, val_loss: 28.3531, val_MinusLogProbMetric: 28.3531

Epoch 317: val_loss did not improve from 28.25530
196/196 - 34s - loss: 27.1727 - MinusLogProbMetric: 27.1727 - val_loss: 28.3531 - val_MinusLogProbMetric: 28.3531 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 318/1000
2023-10-26 18:09:34.495 
Epoch 318/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 318: val_loss did not improve from 28.25530
196/196 - 34s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 319/1000
2023-10-26 18:10:09.181 
Epoch 319/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.2527, val_MinusLogProbMetric: 28.2527

Epoch 319: val_loss improved from 28.25530 to 28.25267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.2527 - val_MinusLogProbMetric: 28.2527 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 320/1000
2023-10-26 18:10:44.157 
Epoch 320/1000 
	 loss: 27.1826, MinusLogProbMetric: 27.1826, val_loss: 28.2362, val_MinusLogProbMetric: 28.2362

Epoch 320: val_loss improved from 28.25267 to 28.23620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 27.1826 - MinusLogProbMetric: 27.1826 - val_loss: 28.2362 - val_MinusLogProbMetric: 28.2362 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 321/1000
2023-10-26 18:11:19.038 
Epoch 321/1000 
	 loss: 27.1918, MinusLogProbMetric: 27.1918, val_loss: 28.2485, val_MinusLogProbMetric: 28.2485

Epoch 321: val_loss did not improve from 28.23620
196/196 - 34s - loss: 27.1918 - MinusLogProbMetric: 27.1918 - val_loss: 28.2485 - val_MinusLogProbMetric: 28.2485 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 322/1000
2023-10-26 18:11:52.959 
Epoch 322/1000 
	 loss: 27.1724, MinusLogProbMetric: 27.1724, val_loss: 28.2304, val_MinusLogProbMetric: 28.2304

Epoch 322: val_loss improved from 28.23620 to 28.23040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 34s - loss: 27.1724 - MinusLogProbMetric: 27.1724 - val_loss: 28.2304 - val_MinusLogProbMetric: 28.2304 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 323/1000
2023-10-26 18:12:28.316 
Epoch 323/1000 
	 loss: 27.1724, MinusLogProbMetric: 27.1724, val_loss: 28.2310, val_MinusLogProbMetric: 28.2310

Epoch 323: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1724 - MinusLogProbMetric: 27.1724 - val_loss: 28.2310 - val_MinusLogProbMetric: 28.2310 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 324/1000
2023-10-26 18:13:02.577 
Epoch 324/1000 
	 loss: 27.1701, MinusLogProbMetric: 27.1701, val_loss: 28.2772, val_MinusLogProbMetric: 28.2772

Epoch 324: val_loss did not improve from 28.23040
196/196 - 34s - loss: 27.1701 - MinusLogProbMetric: 27.1701 - val_loss: 28.2772 - val_MinusLogProbMetric: 28.2772 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 325/1000
2023-10-26 18:13:37.247 
Epoch 325/1000 
	 loss: 27.1647, MinusLogProbMetric: 27.1647, val_loss: 28.2615, val_MinusLogProbMetric: 28.2615

Epoch 325: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1647 - MinusLogProbMetric: 27.1647 - val_loss: 28.2615 - val_MinusLogProbMetric: 28.2615 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 326/1000
2023-10-26 18:14:11.807 
Epoch 326/1000 
	 loss: 27.1723, MinusLogProbMetric: 27.1723, val_loss: 28.2558, val_MinusLogProbMetric: 28.2558

Epoch 326: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1723 - MinusLogProbMetric: 27.1723 - val_loss: 28.2558 - val_MinusLogProbMetric: 28.2558 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 327/1000
2023-10-26 18:14:46.128 
Epoch 327/1000 
	 loss: 27.1629, MinusLogProbMetric: 27.1629, val_loss: 28.2664, val_MinusLogProbMetric: 28.2664

Epoch 327: val_loss did not improve from 28.23040
196/196 - 34s - loss: 27.1629 - MinusLogProbMetric: 27.1629 - val_loss: 28.2664 - val_MinusLogProbMetric: 28.2664 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 328/1000
2023-10-26 18:15:20.597 
Epoch 328/1000 
	 loss: 27.1831, MinusLogProbMetric: 27.1831, val_loss: 28.2762, val_MinusLogProbMetric: 28.2762

Epoch 328: val_loss did not improve from 28.23040
196/196 - 34s - loss: 27.1831 - MinusLogProbMetric: 27.1831 - val_loss: 28.2762 - val_MinusLogProbMetric: 28.2762 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 329/1000
2023-10-26 18:15:54.944 
Epoch 329/1000 
	 loss: 27.1725, MinusLogProbMetric: 27.1725, val_loss: 28.2444, val_MinusLogProbMetric: 28.2444

Epoch 329: val_loss did not improve from 28.23040
196/196 - 34s - loss: 27.1725 - MinusLogProbMetric: 27.1725 - val_loss: 28.2444 - val_MinusLogProbMetric: 28.2444 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 330/1000
2023-10-26 18:16:28.284 
Epoch 330/1000 
	 loss: 27.1571, MinusLogProbMetric: 27.1571, val_loss: 28.2508, val_MinusLogProbMetric: 28.2508

Epoch 330: val_loss did not improve from 28.23040
196/196 - 33s - loss: 27.1571 - MinusLogProbMetric: 27.1571 - val_loss: 28.2508 - val_MinusLogProbMetric: 28.2508 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 331/1000
2023-10-26 18:16:58.353 
Epoch 331/1000 
	 loss: 27.1600, MinusLogProbMetric: 27.1600, val_loss: 28.2795, val_MinusLogProbMetric: 28.2795

Epoch 331: val_loss did not improve from 28.23040
196/196 - 30s - loss: 27.1600 - MinusLogProbMetric: 27.1600 - val_loss: 28.2795 - val_MinusLogProbMetric: 28.2795 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 332/1000
2023-10-26 18:17:33.043 
Epoch 332/1000 
	 loss: 27.1754, MinusLogProbMetric: 27.1754, val_loss: 28.2404, val_MinusLogProbMetric: 28.2404

Epoch 332: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1754 - MinusLogProbMetric: 27.1754 - val_loss: 28.2404 - val_MinusLogProbMetric: 28.2404 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 333/1000
2023-10-26 18:18:07.056 
Epoch 333/1000 
	 loss: 27.1695, MinusLogProbMetric: 27.1695, val_loss: 28.2516, val_MinusLogProbMetric: 28.2516

Epoch 333: val_loss did not improve from 28.23040
196/196 - 34s - loss: 27.1695 - MinusLogProbMetric: 27.1695 - val_loss: 28.2516 - val_MinusLogProbMetric: 28.2516 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 334/1000
2023-10-26 18:18:34.517 
Epoch 334/1000 
	 loss: 27.1587, MinusLogProbMetric: 27.1587, val_loss: 28.2462, val_MinusLogProbMetric: 28.2462

Epoch 334: val_loss did not improve from 28.23040
196/196 - 27s - loss: 27.1587 - MinusLogProbMetric: 27.1587 - val_loss: 28.2462 - val_MinusLogProbMetric: 28.2462 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 335/1000
2023-10-26 18:19:03.572 
Epoch 335/1000 
	 loss: 27.1608, MinusLogProbMetric: 27.1608, val_loss: 28.2843, val_MinusLogProbMetric: 28.2843

Epoch 335: val_loss did not improve from 28.23040
196/196 - 29s - loss: 27.1608 - MinusLogProbMetric: 27.1608 - val_loss: 28.2843 - val_MinusLogProbMetric: 28.2843 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 336/1000
2023-10-26 18:19:36.478 
Epoch 336/1000 
	 loss: 27.1621, MinusLogProbMetric: 27.1621, val_loss: 28.2653, val_MinusLogProbMetric: 28.2653

Epoch 336: val_loss did not improve from 28.23040
196/196 - 33s - loss: 27.1621 - MinusLogProbMetric: 27.1621 - val_loss: 28.2653 - val_MinusLogProbMetric: 28.2653 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 337/1000
2023-10-26 18:20:11.375 
Epoch 337/1000 
	 loss: 27.1512, MinusLogProbMetric: 27.1512, val_loss: 28.3474, val_MinusLogProbMetric: 28.3474

Epoch 337: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1512 - MinusLogProbMetric: 27.1512 - val_loss: 28.3474 - val_MinusLogProbMetric: 28.3474 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 338/1000
2023-10-26 18:20:45.966 
Epoch 338/1000 
	 loss: 27.1659, MinusLogProbMetric: 27.1659, val_loss: 28.2460, val_MinusLogProbMetric: 28.2460

Epoch 338: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1659 - MinusLogProbMetric: 27.1659 - val_loss: 28.2460 - val_MinusLogProbMetric: 28.2460 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 339/1000
2023-10-26 18:21:20.723 
Epoch 339/1000 
	 loss: 27.1530, MinusLogProbMetric: 27.1530, val_loss: 28.2903, val_MinusLogProbMetric: 28.2903

Epoch 339: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1530 - MinusLogProbMetric: 27.1530 - val_loss: 28.2903 - val_MinusLogProbMetric: 28.2903 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 340/1000
2023-10-26 18:21:55.841 
Epoch 340/1000 
	 loss: 27.1523, MinusLogProbMetric: 27.1523, val_loss: 28.2983, val_MinusLogProbMetric: 28.2983

Epoch 340: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1523 - MinusLogProbMetric: 27.1523 - val_loss: 28.2983 - val_MinusLogProbMetric: 28.2983 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 341/1000
2023-10-26 18:22:30.656 
Epoch 341/1000 
	 loss: 27.1612, MinusLogProbMetric: 27.1612, val_loss: 28.2651, val_MinusLogProbMetric: 28.2651

Epoch 341: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1612 - MinusLogProbMetric: 27.1612 - val_loss: 28.2651 - val_MinusLogProbMetric: 28.2651 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 342/1000
2023-10-26 18:23:05.334 
Epoch 342/1000 
	 loss: 27.1485, MinusLogProbMetric: 27.1485, val_loss: 28.2488, val_MinusLogProbMetric: 28.2488

Epoch 342: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1485 - MinusLogProbMetric: 27.1485 - val_loss: 28.2488 - val_MinusLogProbMetric: 28.2488 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 343/1000
2023-10-26 18:23:39.936 
Epoch 343/1000 
	 loss: 27.1626, MinusLogProbMetric: 27.1626, val_loss: 28.2455, val_MinusLogProbMetric: 28.2455

Epoch 343: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1626 - MinusLogProbMetric: 27.1626 - val_loss: 28.2455 - val_MinusLogProbMetric: 28.2455 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 344/1000
2023-10-26 18:24:14.441 
Epoch 344/1000 
	 loss: 27.1492, MinusLogProbMetric: 27.1492, val_loss: 28.2321, val_MinusLogProbMetric: 28.2321

Epoch 344: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1492 - MinusLogProbMetric: 27.1492 - val_loss: 28.2321 - val_MinusLogProbMetric: 28.2321 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 345/1000
2023-10-26 18:24:49.243 
Epoch 345/1000 
	 loss: 27.1484, MinusLogProbMetric: 27.1484, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 345: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1484 - MinusLogProbMetric: 27.1484 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 346/1000
2023-10-26 18:25:23.997 
Epoch 346/1000 
	 loss: 27.1590, MinusLogProbMetric: 27.1590, val_loss: 28.2527, val_MinusLogProbMetric: 28.2527

Epoch 346: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1590 - MinusLogProbMetric: 27.1590 - val_loss: 28.2527 - val_MinusLogProbMetric: 28.2527 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 347/1000
2023-10-26 18:25:58.615 
Epoch 347/1000 
	 loss: 27.1538, MinusLogProbMetric: 27.1538, val_loss: 28.3160, val_MinusLogProbMetric: 28.3160

Epoch 347: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1538 - MinusLogProbMetric: 27.1538 - val_loss: 28.3160 - val_MinusLogProbMetric: 28.3160 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 348/1000
2023-10-26 18:26:33.120 
Epoch 348/1000 
	 loss: 27.1608, MinusLogProbMetric: 27.1608, val_loss: 28.3145, val_MinusLogProbMetric: 28.3145

Epoch 348: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1608 - MinusLogProbMetric: 27.1608 - val_loss: 28.3145 - val_MinusLogProbMetric: 28.3145 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 349/1000
2023-10-26 18:27:07.792 
Epoch 349/1000 
	 loss: 27.1569, MinusLogProbMetric: 27.1569, val_loss: 28.2686, val_MinusLogProbMetric: 28.2686

Epoch 349: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1569 - MinusLogProbMetric: 27.1569 - val_loss: 28.2686 - val_MinusLogProbMetric: 28.2686 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 350/1000
2023-10-26 18:27:42.309 
Epoch 350/1000 
	 loss: 27.1557, MinusLogProbMetric: 27.1557, val_loss: 28.4219, val_MinusLogProbMetric: 28.4219

Epoch 350: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1557 - MinusLogProbMetric: 27.1557 - val_loss: 28.4219 - val_MinusLogProbMetric: 28.4219 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 351/1000
2023-10-26 18:28:16.824 
Epoch 351/1000 
	 loss: 27.1725, MinusLogProbMetric: 27.1725, val_loss: 28.2793, val_MinusLogProbMetric: 28.2793

Epoch 351: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1725 - MinusLogProbMetric: 27.1725 - val_loss: 28.2793 - val_MinusLogProbMetric: 28.2793 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 352/1000
2023-10-26 18:28:51.466 
Epoch 352/1000 
	 loss: 27.1621, MinusLogProbMetric: 27.1621, val_loss: 28.2701, val_MinusLogProbMetric: 28.2701

Epoch 352: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1621 - MinusLogProbMetric: 27.1621 - val_loss: 28.2701 - val_MinusLogProbMetric: 28.2701 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 353/1000
2023-10-26 18:29:25.957 
Epoch 353/1000 
	 loss: 27.1426, MinusLogProbMetric: 27.1426, val_loss: 28.3087, val_MinusLogProbMetric: 28.3087

Epoch 353: val_loss did not improve from 28.23040
196/196 - 34s - loss: 27.1426 - MinusLogProbMetric: 27.1426 - val_loss: 28.3087 - val_MinusLogProbMetric: 28.3087 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 354/1000
2023-10-26 18:30:00.832 
Epoch 354/1000 
	 loss: 27.1561, MinusLogProbMetric: 27.1561, val_loss: 28.2846, val_MinusLogProbMetric: 28.2846

Epoch 354: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1561 - MinusLogProbMetric: 27.1561 - val_loss: 28.2846 - val_MinusLogProbMetric: 28.2846 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 355/1000
2023-10-26 18:30:35.444 
Epoch 355/1000 
	 loss: 27.1668, MinusLogProbMetric: 27.1668, val_loss: 28.4060, val_MinusLogProbMetric: 28.4060

Epoch 355: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1668 - MinusLogProbMetric: 27.1668 - val_loss: 28.4060 - val_MinusLogProbMetric: 28.4060 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 356/1000
2023-10-26 18:31:09.633 
Epoch 356/1000 
	 loss: 27.1463, MinusLogProbMetric: 27.1463, val_loss: 28.2555, val_MinusLogProbMetric: 28.2555

Epoch 356: val_loss did not improve from 28.23040
196/196 - 34s - loss: 27.1463 - MinusLogProbMetric: 27.1463 - val_loss: 28.2555 - val_MinusLogProbMetric: 28.2555 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 357/1000
2023-10-26 18:31:44.313 
Epoch 357/1000 
	 loss: 27.1399, MinusLogProbMetric: 27.1399, val_loss: 28.2773, val_MinusLogProbMetric: 28.2773

Epoch 357: val_loss did not improve from 28.23040
196/196 - 35s - loss: 27.1399 - MinusLogProbMetric: 27.1399 - val_loss: 28.2773 - val_MinusLogProbMetric: 28.2773 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 358/1000
2023-10-26 18:32:18.923 
Epoch 358/1000 
	 loss: 27.1452, MinusLogProbMetric: 27.1452, val_loss: 28.2245, val_MinusLogProbMetric: 28.2245

Epoch 358: val_loss improved from 28.23040 to 28.22447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_386/weights/best_weights.h5
196/196 - 35s - loss: 27.1452 - MinusLogProbMetric: 27.1452 - val_loss: 28.2245 - val_MinusLogProbMetric: 28.2245 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 359/1000
2023-10-26 18:32:54.130 
Epoch 359/1000 
	 loss: 27.1488, MinusLogProbMetric: 27.1488, val_loss: 28.2537, val_MinusLogProbMetric: 28.2537

Epoch 359: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1488 - MinusLogProbMetric: 27.1488 - val_loss: 28.2537 - val_MinusLogProbMetric: 28.2537 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 360/1000
2023-10-26 18:33:28.726 
Epoch 360/1000 
	 loss: 27.1504, MinusLogProbMetric: 27.1504, val_loss: 28.2906, val_MinusLogProbMetric: 28.2906

Epoch 360: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1504 - MinusLogProbMetric: 27.1504 - val_loss: 28.2906 - val_MinusLogProbMetric: 28.2906 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 361/1000
2023-10-26 18:34:03.242 
Epoch 361/1000 
	 loss: 27.1564, MinusLogProbMetric: 27.1564, val_loss: 28.4210, val_MinusLogProbMetric: 28.4210

Epoch 361: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1564 - MinusLogProbMetric: 27.1564 - val_loss: 28.4210 - val_MinusLogProbMetric: 28.4210 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 362/1000
2023-10-26 18:34:37.515 
Epoch 362/1000 
	 loss: 27.1497, MinusLogProbMetric: 27.1497, val_loss: 28.3549, val_MinusLogProbMetric: 28.3549

Epoch 362: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1497 - MinusLogProbMetric: 27.1497 - val_loss: 28.3549 - val_MinusLogProbMetric: 28.3549 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 363/1000
2023-10-26 18:35:12.024 
Epoch 363/1000 
	 loss: 27.1446, MinusLogProbMetric: 27.1446, val_loss: 28.2892, val_MinusLogProbMetric: 28.2892

Epoch 363: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1446 - MinusLogProbMetric: 27.1446 - val_loss: 28.2892 - val_MinusLogProbMetric: 28.2892 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 364/1000
2023-10-26 18:35:46.439 
Epoch 364/1000 
	 loss: 27.1486, MinusLogProbMetric: 27.1486, val_loss: 28.3046, val_MinusLogProbMetric: 28.3046

Epoch 364: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1486 - MinusLogProbMetric: 27.1486 - val_loss: 28.3046 - val_MinusLogProbMetric: 28.3046 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 365/1000
2023-10-26 18:36:21.143 
Epoch 365/1000 
	 loss: 27.1579, MinusLogProbMetric: 27.1579, val_loss: 28.2460, val_MinusLogProbMetric: 28.2460

Epoch 365: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1579 - MinusLogProbMetric: 27.1579 - val_loss: 28.2460 - val_MinusLogProbMetric: 28.2460 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 366/1000
2023-10-26 18:36:55.930 
Epoch 366/1000 
	 loss: 27.1357, MinusLogProbMetric: 27.1357, val_loss: 28.3151, val_MinusLogProbMetric: 28.3151

Epoch 366: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1357 - MinusLogProbMetric: 27.1357 - val_loss: 28.3151 - val_MinusLogProbMetric: 28.3151 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 367/1000
2023-10-26 18:37:30.346 
Epoch 367/1000 
	 loss: 27.1255, MinusLogProbMetric: 27.1255, val_loss: 28.2852, val_MinusLogProbMetric: 28.2852

Epoch 367: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1255 - MinusLogProbMetric: 27.1255 - val_loss: 28.2852 - val_MinusLogProbMetric: 28.2852 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 368/1000
2023-10-26 18:38:05.304 
Epoch 368/1000 
	 loss: 27.1363, MinusLogProbMetric: 27.1363, val_loss: 28.2581, val_MinusLogProbMetric: 28.2581

Epoch 368: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1363 - MinusLogProbMetric: 27.1363 - val_loss: 28.2581 - val_MinusLogProbMetric: 28.2581 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 369/1000
2023-10-26 18:38:40.101 
Epoch 369/1000 
	 loss: 27.1501, MinusLogProbMetric: 27.1501, val_loss: 28.2910, val_MinusLogProbMetric: 28.2910

Epoch 369: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1501 - MinusLogProbMetric: 27.1501 - val_loss: 28.2910 - val_MinusLogProbMetric: 28.2910 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 370/1000
2023-10-26 18:39:14.786 
Epoch 370/1000 
	 loss: 27.1438, MinusLogProbMetric: 27.1438, val_loss: 28.3560, val_MinusLogProbMetric: 28.3560

Epoch 370: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1438 - MinusLogProbMetric: 27.1438 - val_loss: 28.3560 - val_MinusLogProbMetric: 28.3560 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 371/1000
2023-10-26 18:39:49.264 
Epoch 371/1000 
	 loss: 27.1395, MinusLogProbMetric: 27.1395, val_loss: 28.2837, val_MinusLogProbMetric: 28.2837

Epoch 371: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1395 - MinusLogProbMetric: 27.1395 - val_loss: 28.2837 - val_MinusLogProbMetric: 28.2837 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 372/1000
2023-10-26 18:40:24.066 
Epoch 372/1000 
	 loss: 27.1284, MinusLogProbMetric: 27.1284, val_loss: 28.2991, val_MinusLogProbMetric: 28.2991

Epoch 372: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1284 - MinusLogProbMetric: 27.1284 - val_loss: 28.2991 - val_MinusLogProbMetric: 28.2991 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 373/1000
2023-10-26 18:40:58.629 
Epoch 373/1000 
	 loss: 27.1319, MinusLogProbMetric: 27.1319, val_loss: 28.4542, val_MinusLogProbMetric: 28.4542

Epoch 373: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1319 - MinusLogProbMetric: 27.1319 - val_loss: 28.4542 - val_MinusLogProbMetric: 28.4542 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 374/1000
2023-10-26 18:41:33.201 
Epoch 374/1000 
	 loss: 27.1381, MinusLogProbMetric: 27.1381, val_loss: 28.2850, val_MinusLogProbMetric: 28.2850

Epoch 374: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1381 - MinusLogProbMetric: 27.1381 - val_loss: 28.2850 - val_MinusLogProbMetric: 28.2850 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 375/1000
2023-10-26 18:42:07.725 
Epoch 375/1000 
	 loss: 27.1289, MinusLogProbMetric: 27.1289, val_loss: 28.2574, val_MinusLogProbMetric: 28.2574

Epoch 375: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1289 - MinusLogProbMetric: 27.1289 - val_loss: 28.2574 - val_MinusLogProbMetric: 28.2574 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 376/1000
2023-10-26 18:42:42.288 
Epoch 376/1000 
	 loss: 27.1248, MinusLogProbMetric: 27.1248, val_loss: 28.3171, val_MinusLogProbMetric: 28.3171

Epoch 376: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1248 - MinusLogProbMetric: 27.1248 - val_loss: 28.3171 - val_MinusLogProbMetric: 28.3171 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 377/1000
2023-10-26 18:43:17.036 
Epoch 377/1000 
	 loss: 27.1371, MinusLogProbMetric: 27.1371, val_loss: 28.2968, val_MinusLogProbMetric: 28.2968

Epoch 377: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1371 - MinusLogProbMetric: 27.1371 - val_loss: 28.2968 - val_MinusLogProbMetric: 28.2968 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 378/1000
2023-10-26 18:43:51.473 
Epoch 378/1000 
	 loss: 27.1379, MinusLogProbMetric: 27.1379, val_loss: 28.2720, val_MinusLogProbMetric: 28.2720

Epoch 378: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1379 - MinusLogProbMetric: 27.1379 - val_loss: 28.2720 - val_MinusLogProbMetric: 28.2720 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 379/1000
2023-10-26 18:44:26.394 
Epoch 379/1000 
	 loss: 27.1402, MinusLogProbMetric: 27.1402, val_loss: 28.3164, val_MinusLogProbMetric: 28.3164

Epoch 379: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1402 - MinusLogProbMetric: 27.1402 - val_loss: 28.3164 - val_MinusLogProbMetric: 28.3164 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 380/1000
2023-10-26 18:45:01.713 
Epoch 380/1000 
	 loss: 27.1350, MinusLogProbMetric: 27.1350, val_loss: 28.2869, val_MinusLogProbMetric: 28.2869

Epoch 380: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1350 - MinusLogProbMetric: 27.1350 - val_loss: 28.2869 - val_MinusLogProbMetric: 28.2869 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 381/1000
2023-10-26 18:45:36.049 
Epoch 381/1000 
	 loss: 27.1219, MinusLogProbMetric: 27.1219, val_loss: 28.4342, val_MinusLogProbMetric: 28.4342

Epoch 381: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1219 - MinusLogProbMetric: 27.1219 - val_loss: 28.4342 - val_MinusLogProbMetric: 28.4342 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 382/1000
2023-10-26 18:46:10.376 
Epoch 382/1000 
	 loss: 27.1324, MinusLogProbMetric: 27.1324, val_loss: 28.2664, val_MinusLogProbMetric: 28.2664

Epoch 382: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1324 - MinusLogProbMetric: 27.1324 - val_loss: 28.2664 - val_MinusLogProbMetric: 28.2664 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 383/1000
2023-10-26 18:46:44.952 
Epoch 383/1000 
	 loss: 27.1226, MinusLogProbMetric: 27.1226, val_loss: 28.2760, val_MinusLogProbMetric: 28.2760

Epoch 383: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1226 - MinusLogProbMetric: 27.1226 - val_loss: 28.2760 - val_MinusLogProbMetric: 28.2760 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 384/1000
2023-10-26 18:47:19.012 
Epoch 384/1000 
	 loss: 27.1198, MinusLogProbMetric: 27.1198, val_loss: 28.2640, val_MinusLogProbMetric: 28.2640

Epoch 384: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1198 - MinusLogProbMetric: 27.1198 - val_loss: 28.2640 - val_MinusLogProbMetric: 28.2640 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 385/1000
2023-10-26 18:47:53.523 
Epoch 385/1000 
	 loss: 27.1201, MinusLogProbMetric: 27.1201, val_loss: 28.3092, val_MinusLogProbMetric: 28.3092

Epoch 385: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1201 - MinusLogProbMetric: 27.1201 - val_loss: 28.3092 - val_MinusLogProbMetric: 28.3092 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 386/1000
2023-10-26 18:48:27.802 
Epoch 386/1000 
	 loss: 27.1258, MinusLogProbMetric: 27.1258, val_loss: 28.3234, val_MinusLogProbMetric: 28.3234

Epoch 386: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1258 - MinusLogProbMetric: 27.1258 - val_loss: 28.3234 - val_MinusLogProbMetric: 28.3234 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 387/1000
2023-10-26 18:49:02.022 
Epoch 387/1000 
	 loss: 27.1372, MinusLogProbMetric: 27.1372, val_loss: 28.2518, val_MinusLogProbMetric: 28.2518

Epoch 387: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1372 - MinusLogProbMetric: 27.1372 - val_loss: 28.2518 - val_MinusLogProbMetric: 28.2518 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 388/1000
2023-10-26 18:49:36.003 
Epoch 388/1000 
	 loss: 27.1160, MinusLogProbMetric: 27.1160, val_loss: 28.3212, val_MinusLogProbMetric: 28.3212

Epoch 388: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1160 - MinusLogProbMetric: 27.1160 - val_loss: 28.3212 - val_MinusLogProbMetric: 28.3212 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 389/1000
2023-10-26 18:50:10.365 
Epoch 389/1000 
	 loss: 27.1107, MinusLogProbMetric: 27.1107, val_loss: 28.3020, val_MinusLogProbMetric: 28.3020

Epoch 389: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1107 - MinusLogProbMetric: 27.1107 - val_loss: 28.3020 - val_MinusLogProbMetric: 28.3020 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 390/1000
2023-10-26 18:50:45.189 
Epoch 390/1000 
	 loss: 27.1342, MinusLogProbMetric: 27.1342, val_loss: 28.2993, val_MinusLogProbMetric: 28.2993

Epoch 390: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1342 - MinusLogProbMetric: 27.1342 - val_loss: 28.2993 - val_MinusLogProbMetric: 28.2993 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 391/1000
2023-10-26 18:51:19.536 
Epoch 391/1000 
	 loss: 27.1229, MinusLogProbMetric: 27.1229, val_loss: 28.3486, val_MinusLogProbMetric: 28.3486

Epoch 391: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1229 - MinusLogProbMetric: 27.1229 - val_loss: 28.3486 - val_MinusLogProbMetric: 28.3486 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 392/1000
2023-10-26 18:51:54.139 
Epoch 392/1000 
	 loss: 27.1257, MinusLogProbMetric: 27.1257, val_loss: 28.2537, val_MinusLogProbMetric: 28.2537

Epoch 392: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1257 - MinusLogProbMetric: 27.1257 - val_loss: 28.2537 - val_MinusLogProbMetric: 28.2537 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 393/1000
2023-10-26 18:52:28.818 
Epoch 393/1000 
	 loss: 27.1114, MinusLogProbMetric: 27.1114, val_loss: 28.3036, val_MinusLogProbMetric: 28.3036

Epoch 393: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1114 - MinusLogProbMetric: 27.1114 - val_loss: 28.3036 - val_MinusLogProbMetric: 28.3036 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 394/1000
2023-10-26 18:53:03.221 
Epoch 394/1000 
	 loss: 27.1146, MinusLogProbMetric: 27.1146, val_loss: 28.2553, val_MinusLogProbMetric: 28.2553

Epoch 394: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1146 - MinusLogProbMetric: 27.1146 - val_loss: 28.2553 - val_MinusLogProbMetric: 28.2553 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 395/1000
2023-10-26 18:53:37.695 
Epoch 395/1000 
	 loss: 27.1348, MinusLogProbMetric: 27.1348, val_loss: 28.3371, val_MinusLogProbMetric: 28.3371

Epoch 395: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1348 - MinusLogProbMetric: 27.1348 - val_loss: 28.3371 - val_MinusLogProbMetric: 28.3371 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 396/1000
2023-10-26 18:54:12.274 
Epoch 396/1000 
	 loss: 27.1060, MinusLogProbMetric: 27.1060, val_loss: 28.2678, val_MinusLogProbMetric: 28.2678

Epoch 396: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1060 - MinusLogProbMetric: 27.1060 - val_loss: 28.2678 - val_MinusLogProbMetric: 28.2678 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 397/1000
2023-10-26 18:54:46.585 
Epoch 397/1000 
	 loss: 27.1062, MinusLogProbMetric: 27.1062, val_loss: 28.3130, val_MinusLogProbMetric: 28.3130

Epoch 397: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1062 - MinusLogProbMetric: 27.1062 - val_loss: 28.3130 - val_MinusLogProbMetric: 28.3130 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 398/1000
2023-10-26 18:55:20.991 
Epoch 398/1000 
	 loss: 27.1204, MinusLogProbMetric: 27.1204, val_loss: 28.2835, val_MinusLogProbMetric: 28.2835

Epoch 398: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1204 - MinusLogProbMetric: 27.1204 - val_loss: 28.2835 - val_MinusLogProbMetric: 28.2835 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 399/1000
2023-10-26 18:55:55.260 
Epoch 399/1000 
	 loss: 27.1221, MinusLogProbMetric: 27.1221, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 399: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1221 - MinusLogProbMetric: 27.1221 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 400/1000
2023-10-26 18:56:29.702 
Epoch 400/1000 
	 loss: 27.1172, MinusLogProbMetric: 27.1172, val_loss: 28.3882, val_MinusLogProbMetric: 28.3882

Epoch 400: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1172 - MinusLogProbMetric: 27.1172 - val_loss: 28.3882 - val_MinusLogProbMetric: 28.3882 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 401/1000
2023-10-26 18:57:04.262 
Epoch 401/1000 
	 loss: 27.1248, MinusLogProbMetric: 27.1248, val_loss: 28.3841, val_MinusLogProbMetric: 28.3841

Epoch 401: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.1248 - MinusLogProbMetric: 27.1248 - val_loss: 28.3841 - val_MinusLogProbMetric: 28.3841 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 402/1000
2023-10-26 18:57:38.744 
Epoch 402/1000 
	 loss: 27.1216, MinusLogProbMetric: 27.1216, val_loss: 28.2620, val_MinusLogProbMetric: 28.2620

Epoch 402: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1216 - MinusLogProbMetric: 27.1216 - val_loss: 28.2620 - val_MinusLogProbMetric: 28.2620 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 403/1000
2023-10-26 18:58:12.867 
Epoch 403/1000 
	 loss: 27.1055, MinusLogProbMetric: 27.1055, val_loss: 28.2958, val_MinusLogProbMetric: 28.2958

Epoch 403: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1055 - MinusLogProbMetric: 27.1055 - val_loss: 28.2958 - val_MinusLogProbMetric: 28.2958 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 404/1000
2023-10-26 18:58:46.916 
Epoch 404/1000 
	 loss: 27.1211, MinusLogProbMetric: 27.1211, val_loss: 28.3238, val_MinusLogProbMetric: 28.3238

Epoch 404: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1211 - MinusLogProbMetric: 27.1211 - val_loss: 28.3238 - val_MinusLogProbMetric: 28.3238 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 405/1000
2023-10-26 18:59:21.086 
Epoch 405/1000 
	 loss: 27.1107, MinusLogProbMetric: 27.1107, val_loss: 28.2889, val_MinusLogProbMetric: 28.2889

Epoch 405: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1107 - MinusLogProbMetric: 27.1107 - val_loss: 28.2889 - val_MinusLogProbMetric: 28.2889 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 406/1000
2023-10-26 18:59:55.355 
Epoch 406/1000 
	 loss: 27.1097, MinusLogProbMetric: 27.1097, val_loss: 28.3368, val_MinusLogProbMetric: 28.3368

Epoch 406: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1097 - MinusLogProbMetric: 27.1097 - val_loss: 28.3368 - val_MinusLogProbMetric: 28.3368 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 407/1000
2023-10-26 19:00:29.753 
Epoch 407/1000 
	 loss: 27.1153, MinusLogProbMetric: 27.1153, val_loss: 28.2767, val_MinusLogProbMetric: 28.2767

Epoch 407: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1153 - MinusLogProbMetric: 27.1153 - val_loss: 28.2767 - val_MinusLogProbMetric: 28.2767 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 408/1000
2023-10-26 19:01:04.026 
Epoch 408/1000 
	 loss: 27.1113, MinusLogProbMetric: 27.1113, val_loss: 28.3139, val_MinusLogProbMetric: 28.3139

Epoch 408: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.1113 - MinusLogProbMetric: 27.1113 - val_loss: 28.3139 - val_MinusLogProbMetric: 28.3139 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 409/1000
2023-10-26 19:01:38.617 
Epoch 409/1000 
	 loss: 27.0406, MinusLogProbMetric: 27.0406, val_loss: 28.2369, val_MinusLogProbMetric: 28.2369

Epoch 409: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0406 - MinusLogProbMetric: 27.0406 - val_loss: 28.2369 - val_MinusLogProbMetric: 28.2369 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 410/1000
2023-10-26 19:02:12.984 
Epoch 410/1000 
	 loss: 27.0355, MinusLogProbMetric: 27.0355, val_loss: 28.2762, val_MinusLogProbMetric: 28.2762

Epoch 410: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0355 - MinusLogProbMetric: 27.0355 - val_loss: 28.2762 - val_MinusLogProbMetric: 28.2762 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 411/1000
2023-10-26 19:02:47.425 
Epoch 411/1000 
	 loss: 27.0373, MinusLogProbMetric: 27.0373, val_loss: 28.2273, val_MinusLogProbMetric: 28.2273

Epoch 411: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0373 - MinusLogProbMetric: 27.0373 - val_loss: 28.2273 - val_MinusLogProbMetric: 28.2273 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 412/1000
2023-10-26 19:03:21.578 
Epoch 412/1000 
	 loss: 27.0396, MinusLogProbMetric: 27.0396, val_loss: 28.2286, val_MinusLogProbMetric: 28.2286

Epoch 412: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0396 - MinusLogProbMetric: 27.0396 - val_loss: 28.2286 - val_MinusLogProbMetric: 28.2286 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 413/1000
2023-10-26 19:03:56.056 
Epoch 413/1000 
	 loss: 27.0348, MinusLogProbMetric: 27.0348, val_loss: 28.2602, val_MinusLogProbMetric: 28.2602

Epoch 413: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0348 - MinusLogProbMetric: 27.0348 - val_loss: 28.2602 - val_MinusLogProbMetric: 28.2602 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 414/1000
2023-10-26 19:04:30.456 
Epoch 414/1000 
	 loss: 27.0374, MinusLogProbMetric: 27.0374, val_loss: 28.2426, val_MinusLogProbMetric: 28.2426

Epoch 414: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0374 - MinusLogProbMetric: 27.0374 - val_loss: 28.2426 - val_MinusLogProbMetric: 28.2426 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 415/1000
2023-10-26 19:05:04.950 
Epoch 415/1000 
	 loss: 27.0288, MinusLogProbMetric: 27.0288, val_loss: 28.2339, val_MinusLogProbMetric: 28.2339

Epoch 415: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0288 - MinusLogProbMetric: 27.0288 - val_loss: 28.2339 - val_MinusLogProbMetric: 28.2339 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 416/1000
2023-10-26 19:05:39.196 
Epoch 416/1000 
	 loss: 27.0435, MinusLogProbMetric: 27.0435, val_loss: 28.2294, val_MinusLogProbMetric: 28.2294

Epoch 416: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0435 - MinusLogProbMetric: 27.0435 - val_loss: 28.2294 - val_MinusLogProbMetric: 28.2294 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 417/1000
2023-10-26 19:06:13.725 
Epoch 417/1000 
	 loss: 27.0372, MinusLogProbMetric: 27.0372, val_loss: 28.2494, val_MinusLogProbMetric: 28.2494

Epoch 417: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0372 - MinusLogProbMetric: 27.0372 - val_loss: 28.2494 - val_MinusLogProbMetric: 28.2494 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 418/1000
2023-10-26 19:06:47.857 
Epoch 418/1000 
	 loss: 27.0308, MinusLogProbMetric: 27.0308, val_loss: 28.2497, val_MinusLogProbMetric: 28.2497

Epoch 418: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0308 - MinusLogProbMetric: 27.0308 - val_loss: 28.2497 - val_MinusLogProbMetric: 28.2497 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 419/1000
2023-10-26 19:07:20.684 
Epoch 419/1000 
	 loss: 27.0386, MinusLogProbMetric: 27.0386, val_loss: 28.2327, val_MinusLogProbMetric: 28.2327

Epoch 419: val_loss did not improve from 28.22447
196/196 - 33s - loss: 27.0386 - MinusLogProbMetric: 27.0386 - val_loss: 28.2327 - val_MinusLogProbMetric: 28.2327 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 420/1000
2023-10-26 19:07:54.726 
Epoch 420/1000 
	 loss: 27.0317, MinusLogProbMetric: 27.0317, val_loss: 28.2374, val_MinusLogProbMetric: 28.2374

Epoch 420: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0317 - MinusLogProbMetric: 27.0317 - val_loss: 28.2374 - val_MinusLogProbMetric: 28.2374 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 421/1000
2023-10-26 19:08:24.696 
Epoch 421/1000 
	 loss: 27.0336, MinusLogProbMetric: 27.0336, val_loss: 28.2518, val_MinusLogProbMetric: 28.2518

Epoch 421: val_loss did not improve from 28.22447
196/196 - 30s - loss: 27.0336 - MinusLogProbMetric: 27.0336 - val_loss: 28.2518 - val_MinusLogProbMetric: 28.2518 - lr: 1.2500e-04 - 30s/epoch - 153ms/step
Epoch 422/1000
2023-10-26 19:08:56.894 
Epoch 422/1000 
	 loss: 27.0319, MinusLogProbMetric: 27.0319, val_loss: 28.2600, val_MinusLogProbMetric: 28.2600

Epoch 422: val_loss did not improve from 28.22447
196/196 - 32s - loss: 27.0319 - MinusLogProbMetric: 27.0319 - val_loss: 28.2600 - val_MinusLogProbMetric: 28.2600 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 423/1000
2023-10-26 19:09:30.488 
Epoch 423/1000 
	 loss: 27.0306, MinusLogProbMetric: 27.0306, val_loss: 28.2324, val_MinusLogProbMetric: 28.2324

Epoch 423: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0306 - MinusLogProbMetric: 27.0306 - val_loss: 28.2324 - val_MinusLogProbMetric: 28.2324 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 424/1000
2023-10-26 19:10:04.845 
Epoch 424/1000 
	 loss: 27.0311, MinusLogProbMetric: 27.0311, val_loss: 28.2632, val_MinusLogProbMetric: 28.2632

Epoch 424: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0311 - MinusLogProbMetric: 27.0311 - val_loss: 28.2632 - val_MinusLogProbMetric: 28.2632 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 425/1000
2023-10-26 19:10:39.480 
Epoch 425/1000 
	 loss: 27.0297, MinusLogProbMetric: 27.0297, val_loss: 28.2435, val_MinusLogProbMetric: 28.2435

Epoch 425: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0297 - MinusLogProbMetric: 27.0297 - val_loss: 28.2435 - val_MinusLogProbMetric: 28.2435 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 426/1000
2023-10-26 19:11:13.730 
Epoch 426/1000 
	 loss: 27.0284, MinusLogProbMetric: 27.0284, val_loss: 28.2468, val_MinusLogProbMetric: 28.2468

Epoch 426: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0284 - MinusLogProbMetric: 27.0284 - val_loss: 28.2468 - val_MinusLogProbMetric: 28.2468 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 427/1000
2023-10-26 19:11:47.058 
Epoch 427/1000 
	 loss: 27.0261, MinusLogProbMetric: 27.0261, val_loss: 28.2430, val_MinusLogProbMetric: 28.2430

Epoch 427: val_loss did not improve from 28.22447
196/196 - 33s - loss: 27.0261 - MinusLogProbMetric: 27.0261 - val_loss: 28.2430 - val_MinusLogProbMetric: 28.2430 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 428/1000
2023-10-26 19:12:21.747 
Epoch 428/1000 
	 loss: 27.0283, MinusLogProbMetric: 27.0283, val_loss: 28.2408, val_MinusLogProbMetric: 28.2408

Epoch 428: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0283 - MinusLogProbMetric: 27.0283 - val_loss: 28.2408 - val_MinusLogProbMetric: 28.2408 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 429/1000
2023-10-26 19:12:56.466 
Epoch 429/1000 
	 loss: 27.0315, MinusLogProbMetric: 27.0315, val_loss: 28.2458, val_MinusLogProbMetric: 28.2458

Epoch 429: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0315 - MinusLogProbMetric: 27.0315 - val_loss: 28.2458 - val_MinusLogProbMetric: 28.2458 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 430/1000
2023-10-26 19:13:31.257 
Epoch 430/1000 
	 loss: 27.0292, MinusLogProbMetric: 27.0292, val_loss: 28.2665, val_MinusLogProbMetric: 28.2665

Epoch 430: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0292 - MinusLogProbMetric: 27.0292 - val_loss: 28.2665 - val_MinusLogProbMetric: 28.2665 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 431/1000
2023-10-26 19:14:05.788 
Epoch 431/1000 
	 loss: 27.0254, MinusLogProbMetric: 27.0254, val_loss: 28.2509, val_MinusLogProbMetric: 28.2509

Epoch 431: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0254 - MinusLogProbMetric: 27.0254 - val_loss: 28.2509 - val_MinusLogProbMetric: 28.2509 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 432/1000
2023-10-26 19:14:40.405 
Epoch 432/1000 
	 loss: 27.0284, MinusLogProbMetric: 27.0284, val_loss: 28.2632, val_MinusLogProbMetric: 28.2632

Epoch 432: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0284 - MinusLogProbMetric: 27.0284 - val_loss: 28.2632 - val_MinusLogProbMetric: 28.2632 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 433/1000
2023-10-26 19:15:15.366 
Epoch 433/1000 
	 loss: 27.0249, MinusLogProbMetric: 27.0249, val_loss: 28.2533, val_MinusLogProbMetric: 28.2533

Epoch 433: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0249 - MinusLogProbMetric: 27.0249 - val_loss: 28.2533 - val_MinusLogProbMetric: 28.2533 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 434/1000
2023-10-26 19:15:49.955 
Epoch 434/1000 
	 loss: 27.0300, MinusLogProbMetric: 27.0300, val_loss: 28.2508, val_MinusLogProbMetric: 28.2508

Epoch 434: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0300 - MinusLogProbMetric: 27.0300 - val_loss: 28.2508 - val_MinusLogProbMetric: 28.2508 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 435/1000
2023-10-26 19:16:24.537 
Epoch 435/1000 
	 loss: 27.0298, MinusLogProbMetric: 27.0298, val_loss: 28.2286, val_MinusLogProbMetric: 28.2286

Epoch 435: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0298 - MinusLogProbMetric: 27.0298 - val_loss: 28.2286 - val_MinusLogProbMetric: 28.2286 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 436/1000
2023-10-26 19:17:00.011 
Epoch 436/1000 
	 loss: 27.0263, MinusLogProbMetric: 27.0263, val_loss: 28.2543, val_MinusLogProbMetric: 28.2543

Epoch 436: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0263 - MinusLogProbMetric: 27.0263 - val_loss: 28.2543 - val_MinusLogProbMetric: 28.2543 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 437/1000
2023-10-26 19:17:34.596 
Epoch 437/1000 
	 loss: 27.0346, MinusLogProbMetric: 27.0346, val_loss: 28.2342, val_MinusLogProbMetric: 28.2342

Epoch 437: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0346 - MinusLogProbMetric: 27.0346 - val_loss: 28.2342 - val_MinusLogProbMetric: 28.2342 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 438/1000
2023-10-26 19:18:09.374 
Epoch 438/1000 
	 loss: 27.0244, MinusLogProbMetric: 27.0244, val_loss: 28.2507, val_MinusLogProbMetric: 28.2507

Epoch 438: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0244 - MinusLogProbMetric: 27.0244 - val_loss: 28.2507 - val_MinusLogProbMetric: 28.2507 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 439/1000
2023-10-26 19:18:44.051 
Epoch 439/1000 
	 loss: 27.0312, MinusLogProbMetric: 27.0312, val_loss: 28.2617, val_MinusLogProbMetric: 28.2617

Epoch 439: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0312 - MinusLogProbMetric: 27.0312 - val_loss: 28.2617 - val_MinusLogProbMetric: 28.2617 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 440/1000
2023-10-26 19:19:19.094 
Epoch 440/1000 
	 loss: 27.0255, MinusLogProbMetric: 27.0255, val_loss: 28.2362, val_MinusLogProbMetric: 28.2362

Epoch 440: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0255 - MinusLogProbMetric: 27.0255 - val_loss: 28.2362 - val_MinusLogProbMetric: 28.2362 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 441/1000
2023-10-26 19:19:53.636 
Epoch 441/1000 
	 loss: 27.0256, MinusLogProbMetric: 27.0256, val_loss: 28.2708, val_MinusLogProbMetric: 28.2708

Epoch 441: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0256 - MinusLogProbMetric: 27.0256 - val_loss: 28.2708 - val_MinusLogProbMetric: 28.2708 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 442/1000
2023-10-26 19:20:28.068 
Epoch 442/1000 
	 loss: 27.0211, MinusLogProbMetric: 27.0211, val_loss: 28.2447, val_MinusLogProbMetric: 28.2447

Epoch 442: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0211 - MinusLogProbMetric: 27.0211 - val_loss: 28.2447 - val_MinusLogProbMetric: 28.2447 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 443/1000
2023-10-26 19:21:03.015 
Epoch 443/1000 
	 loss: 27.0247, MinusLogProbMetric: 27.0247, val_loss: 28.2948, val_MinusLogProbMetric: 28.2948

Epoch 443: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0247 - MinusLogProbMetric: 27.0247 - val_loss: 28.2948 - val_MinusLogProbMetric: 28.2948 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 444/1000
2023-10-26 19:21:38.254 
Epoch 444/1000 
	 loss: 27.0268, MinusLogProbMetric: 27.0268, val_loss: 28.2452, val_MinusLogProbMetric: 28.2452

Epoch 444: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0268 - MinusLogProbMetric: 27.0268 - val_loss: 28.2452 - val_MinusLogProbMetric: 28.2452 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 445/1000
2023-10-26 19:22:13.169 
Epoch 445/1000 
	 loss: 27.0264, MinusLogProbMetric: 27.0264, val_loss: 28.2650, val_MinusLogProbMetric: 28.2650

Epoch 445: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0264 - MinusLogProbMetric: 27.0264 - val_loss: 28.2650 - val_MinusLogProbMetric: 28.2650 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 446/1000
2023-10-26 19:22:47.968 
Epoch 446/1000 
	 loss: 27.0265, MinusLogProbMetric: 27.0265, val_loss: 28.2941, val_MinusLogProbMetric: 28.2941

Epoch 446: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0265 - MinusLogProbMetric: 27.0265 - val_loss: 28.2941 - val_MinusLogProbMetric: 28.2941 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 447/1000
2023-10-26 19:23:22.477 
Epoch 447/1000 
	 loss: 27.0269, MinusLogProbMetric: 27.0269, val_loss: 28.2592, val_MinusLogProbMetric: 28.2592

Epoch 447: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0269 - MinusLogProbMetric: 27.0269 - val_loss: 28.2592 - val_MinusLogProbMetric: 28.2592 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 448/1000
2023-10-26 19:23:57.349 
Epoch 448/1000 
	 loss: 27.0209, MinusLogProbMetric: 27.0209, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 448: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0209 - MinusLogProbMetric: 27.0209 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 449/1000
2023-10-26 19:24:32.216 
Epoch 449/1000 
	 loss: 27.0276, MinusLogProbMetric: 27.0276, val_loss: 28.2426, val_MinusLogProbMetric: 28.2426

Epoch 449: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0276 - MinusLogProbMetric: 27.0276 - val_loss: 28.2426 - val_MinusLogProbMetric: 28.2426 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 450/1000
2023-10-26 19:25:06.816 
Epoch 450/1000 
	 loss: 27.0194, MinusLogProbMetric: 27.0194, val_loss: 28.2344, val_MinusLogProbMetric: 28.2344

Epoch 450: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0194 - MinusLogProbMetric: 27.0194 - val_loss: 28.2344 - val_MinusLogProbMetric: 28.2344 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 451/1000
2023-10-26 19:25:41.673 
Epoch 451/1000 
	 loss: 27.0206, MinusLogProbMetric: 27.0206, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 451: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0206 - MinusLogProbMetric: 27.0206 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 452/1000
2023-10-26 19:26:16.477 
Epoch 452/1000 
	 loss: 27.0267, MinusLogProbMetric: 27.0267, val_loss: 28.2394, val_MinusLogProbMetric: 28.2394

Epoch 452: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0267 - MinusLogProbMetric: 27.0267 - val_loss: 28.2394 - val_MinusLogProbMetric: 28.2394 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 453/1000
2023-10-26 19:26:50.757 
Epoch 453/1000 
	 loss: 27.0207, MinusLogProbMetric: 27.0207, val_loss: 28.2390, val_MinusLogProbMetric: 28.2390

Epoch 453: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0207 - MinusLogProbMetric: 27.0207 - val_loss: 28.2390 - val_MinusLogProbMetric: 28.2390 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 454/1000
2023-10-26 19:27:25.178 
Epoch 454/1000 
	 loss: 27.0209, MinusLogProbMetric: 27.0209, val_loss: 28.2639, val_MinusLogProbMetric: 28.2639

Epoch 454: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0209 - MinusLogProbMetric: 27.0209 - val_loss: 28.2639 - val_MinusLogProbMetric: 28.2639 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 455/1000
2023-10-26 19:27:59.665 
Epoch 455/1000 
	 loss: 27.0217, MinusLogProbMetric: 27.0217, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 455: val_loss did not improve from 28.22447
196/196 - 34s - loss: 27.0217 - MinusLogProbMetric: 27.0217 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 456/1000
2023-10-26 19:28:34.558 
Epoch 456/1000 
	 loss: 27.0177, MinusLogProbMetric: 27.0177, val_loss: 28.2448, val_MinusLogProbMetric: 28.2448

Epoch 456: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0177 - MinusLogProbMetric: 27.0177 - val_loss: 28.2448 - val_MinusLogProbMetric: 28.2448 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 457/1000
2023-10-26 19:29:09.933 
Epoch 457/1000 
	 loss: 27.0185, MinusLogProbMetric: 27.0185, val_loss: 28.2596, val_MinusLogProbMetric: 28.2596

Epoch 457: val_loss did not improve from 28.22447
196/196 - 35s - loss: 27.0185 - MinusLogProbMetric: 27.0185 - val_loss: 28.2596 - val_MinusLogProbMetric: 28.2596 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 458/1000
2023-10-26 19:29:44.445 
Epoch 458/1000 
	 loss: 27.0256, MinusLogProbMetric: 27.0256, val_loss: 28.2423, val_MinusLogProbMetric: 28.2423

Epoch 458: val_loss did not improve from 28.22447
Restoring model weights from the end of the best epoch: 358.
196/196 - 35s - loss: 27.0256 - MinusLogProbMetric: 27.0256 - val_loss: 28.2423 - val_MinusLogProbMetric: 28.2423 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 458: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 926.
Model trained in 15639.23 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.81 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.01 s.
===========
Run 386/720 done in 15645.03 s.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

===========
Generating train data for run 394.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_394/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_394/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_394/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_394
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_343"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_344 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7feef5b545b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feef57b3df0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feef57b3df0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feef5608850>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feef56a4bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feef56a5120>, <keras.callbacks.ModelCheckpoint object at 0x7feef56a51e0>, <keras.callbacks.EarlyStopping object at 0x7feef56a5450>, <keras.callbacks.ReduceLROnPlateau object at 0x7feef56a5480>, <keras.callbacks.TerminateOnNaN object at 0x7feef56a50c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_394/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 394/720 with hyperparameters:
timestamp = 2023-10-26 19:29:50.145184
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
2023-10-26 19:31:42.530 
Epoch 1/1000 
	 loss: 503.5201, MinusLogProbMetric: 503.5201, val_loss: 107.6377, val_MinusLogProbMetric: 107.6377

Epoch 1: val_loss improved from inf to 107.63768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 113s - loss: 503.5201 - MinusLogProbMetric: 503.5201 - val_loss: 107.6377 - val_MinusLogProbMetric: 107.6377 - lr: 0.0010 - 113s/epoch - 574ms/step
Epoch 2/1000
2023-10-26 19:32:18.980 
Epoch 2/1000 
	 loss: 83.3259, MinusLogProbMetric: 83.3259, val_loss: 67.6348, val_MinusLogProbMetric: 67.6348

Epoch 2: val_loss improved from 107.63768 to 67.63481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 83.3259 - MinusLogProbMetric: 83.3259 - val_loss: 67.6348 - val_MinusLogProbMetric: 67.6348 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 3/1000
2023-10-26 19:32:55.606 
Epoch 3/1000 
	 loss: 61.1866, MinusLogProbMetric: 61.1866, val_loss: 56.9087, val_MinusLogProbMetric: 56.9087

Epoch 3: val_loss improved from 67.63481 to 56.90874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 61.1866 - MinusLogProbMetric: 61.1866 - val_loss: 56.9087 - val_MinusLogProbMetric: 56.9087 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 4/1000
2023-10-26 19:33:32.082 
Epoch 4/1000 
	 loss: 52.8352, MinusLogProbMetric: 52.8352, val_loss: 49.5813, val_MinusLogProbMetric: 49.5813

Epoch 4: val_loss improved from 56.90874 to 49.58130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 52.8352 - MinusLogProbMetric: 52.8352 - val_loss: 49.5813 - val_MinusLogProbMetric: 49.5813 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 5/1000
2023-10-26 19:34:09.003 
Epoch 5/1000 
	 loss: 48.9755, MinusLogProbMetric: 48.9755, val_loss: 47.2146, val_MinusLogProbMetric: 47.2146

Epoch 5: val_loss improved from 49.58130 to 47.21465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 48.9755 - MinusLogProbMetric: 48.9755 - val_loss: 47.2146 - val_MinusLogProbMetric: 47.2146 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 6/1000
2023-10-26 19:34:45.749 
Epoch 6/1000 
	 loss: 45.4341, MinusLogProbMetric: 45.4341, val_loss: 42.8110, val_MinusLogProbMetric: 42.8110

Epoch 6: val_loss improved from 47.21465 to 42.81099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 45.4341 - MinusLogProbMetric: 45.4341 - val_loss: 42.8110 - val_MinusLogProbMetric: 42.8110 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 7/1000
2023-10-26 19:35:22.262 
Epoch 7/1000 
	 loss: 43.9073, MinusLogProbMetric: 43.9073, val_loss: 45.1273, val_MinusLogProbMetric: 45.1273

Epoch 7: val_loss did not improve from 42.81099
196/196 - 36s - loss: 43.9073 - MinusLogProbMetric: 43.9073 - val_loss: 45.1273 - val_MinusLogProbMetric: 45.1273 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 8/1000
2023-10-26 19:35:58.506 
Epoch 8/1000 
	 loss: 42.0415, MinusLogProbMetric: 42.0415, val_loss: 42.9375, val_MinusLogProbMetric: 42.9375

Epoch 8: val_loss did not improve from 42.81099
196/196 - 36s - loss: 42.0415 - MinusLogProbMetric: 42.0415 - val_loss: 42.9375 - val_MinusLogProbMetric: 42.9375 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 9/1000
2023-10-26 19:36:34.402 
Epoch 9/1000 
	 loss: 40.3712, MinusLogProbMetric: 40.3712, val_loss: 39.6510, val_MinusLogProbMetric: 39.6510

Epoch 9: val_loss improved from 42.81099 to 39.65096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 40.3712 - MinusLogProbMetric: 40.3712 - val_loss: 39.6510 - val_MinusLogProbMetric: 39.6510 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 10/1000
2023-10-26 19:37:11.107 
Epoch 10/1000 
	 loss: 39.9916, MinusLogProbMetric: 39.9916, val_loss: 39.8791, val_MinusLogProbMetric: 39.8791

Epoch 10: val_loss did not improve from 39.65096
196/196 - 36s - loss: 39.9916 - MinusLogProbMetric: 39.9916 - val_loss: 39.8791 - val_MinusLogProbMetric: 39.8791 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 11/1000
2023-10-26 19:37:47.020 
Epoch 11/1000 
	 loss: 38.8743, MinusLogProbMetric: 38.8743, val_loss: 39.3161, val_MinusLogProbMetric: 39.3161

Epoch 11: val_loss improved from 39.65096 to 39.31605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 38.8743 - MinusLogProbMetric: 38.8743 - val_loss: 39.3161 - val_MinusLogProbMetric: 39.3161 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 12/1000
2023-10-26 19:38:23.471 
Epoch 12/1000 
	 loss: 38.5581, MinusLogProbMetric: 38.5581, val_loss: 36.6273, val_MinusLogProbMetric: 36.6273

Epoch 12: val_loss improved from 39.31605 to 36.62730, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 38.5581 - MinusLogProbMetric: 38.5581 - val_loss: 36.6273 - val_MinusLogProbMetric: 36.6273 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 13/1000
2023-10-26 19:39:00.225 
Epoch 13/1000 
	 loss: 37.5786, MinusLogProbMetric: 37.5786, val_loss: 38.2474, val_MinusLogProbMetric: 38.2474

Epoch 13: val_loss did not improve from 36.62730
196/196 - 36s - loss: 37.5786 - MinusLogProbMetric: 37.5786 - val_loss: 38.2474 - val_MinusLogProbMetric: 38.2474 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 14/1000
2023-10-26 19:39:36.220 
Epoch 14/1000 
	 loss: 37.1537, MinusLogProbMetric: 37.1537, val_loss: 35.7224, val_MinusLogProbMetric: 35.7224

Epoch 14: val_loss improved from 36.62730 to 35.72237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 37.1537 - MinusLogProbMetric: 37.1537 - val_loss: 35.7224 - val_MinusLogProbMetric: 35.7224 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 15/1000
2023-10-26 19:40:12.498 
Epoch 15/1000 
	 loss: 37.0878, MinusLogProbMetric: 37.0878, val_loss: 37.2385, val_MinusLogProbMetric: 37.2385

Epoch 15: val_loss did not improve from 35.72237
196/196 - 36s - loss: 37.0878 - MinusLogProbMetric: 37.0878 - val_loss: 37.2385 - val_MinusLogProbMetric: 37.2385 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 16/1000
2023-10-26 19:40:48.378 
Epoch 16/1000 
	 loss: 36.1749, MinusLogProbMetric: 36.1749, val_loss: 39.1458, val_MinusLogProbMetric: 39.1458

Epoch 16: val_loss did not improve from 35.72237
196/196 - 36s - loss: 36.1749 - MinusLogProbMetric: 36.1749 - val_loss: 39.1458 - val_MinusLogProbMetric: 39.1458 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 17/1000
2023-10-26 19:41:24.161 
Epoch 17/1000 
	 loss: 35.8648, MinusLogProbMetric: 35.8648, val_loss: 36.7714, val_MinusLogProbMetric: 36.7714

Epoch 17: val_loss did not improve from 35.72237
196/196 - 36s - loss: 35.8648 - MinusLogProbMetric: 35.8648 - val_loss: 36.7714 - val_MinusLogProbMetric: 36.7714 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 18/1000
2023-10-26 19:41:59.423 
Epoch 18/1000 
	 loss: 35.3470, MinusLogProbMetric: 35.3470, val_loss: 35.1892, val_MinusLogProbMetric: 35.1892

Epoch 18: val_loss improved from 35.72237 to 35.18923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 35.3470 - MinusLogProbMetric: 35.3470 - val_loss: 35.1892 - val_MinusLogProbMetric: 35.1892 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 19/1000
2023-10-26 19:42:35.857 
Epoch 19/1000 
	 loss: 35.4723, MinusLogProbMetric: 35.4723, val_loss: 34.9142, val_MinusLogProbMetric: 34.9142

Epoch 19: val_loss improved from 35.18923 to 34.91418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 35.4723 - MinusLogProbMetric: 35.4723 - val_loss: 34.9142 - val_MinusLogProbMetric: 34.9142 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 20/1000
2023-10-26 19:43:11.877 
Epoch 20/1000 
	 loss: 34.7522, MinusLogProbMetric: 34.7522, val_loss: 36.0895, val_MinusLogProbMetric: 36.0895

Epoch 20: val_loss did not improve from 34.91418
196/196 - 35s - loss: 34.7522 - MinusLogProbMetric: 34.7522 - val_loss: 36.0895 - val_MinusLogProbMetric: 36.0895 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 21/1000
2023-10-26 19:43:47.682 
Epoch 21/1000 
	 loss: 34.4796, MinusLogProbMetric: 34.4796, val_loss: 33.7913, val_MinusLogProbMetric: 33.7913

Epoch 21: val_loss improved from 34.91418 to 33.79126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 34.4796 - MinusLogProbMetric: 34.4796 - val_loss: 33.7913 - val_MinusLogProbMetric: 33.7913 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 22/1000
2023-10-26 19:44:23.943 
Epoch 22/1000 
	 loss: 34.1239, MinusLogProbMetric: 34.1239, val_loss: 35.3901, val_MinusLogProbMetric: 35.3901

Epoch 22: val_loss did not improve from 33.79126
196/196 - 36s - loss: 34.1239 - MinusLogProbMetric: 34.1239 - val_loss: 35.3901 - val_MinusLogProbMetric: 35.3901 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 23/1000
2023-10-26 19:44:59.404 
Epoch 23/1000 
	 loss: 34.1295, MinusLogProbMetric: 34.1295, val_loss: 34.4851, val_MinusLogProbMetric: 34.4851

Epoch 23: val_loss did not improve from 33.79126
196/196 - 35s - loss: 34.1295 - MinusLogProbMetric: 34.1295 - val_loss: 34.4851 - val_MinusLogProbMetric: 34.4851 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 24/1000
2023-10-26 19:45:35.094 
Epoch 24/1000 
	 loss: 34.2062, MinusLogProbMetric: 34.2062, val_loss: 33.3098, val_MinusLogProbMetric: 33.3098

Epoch 24: val_loss improved from 33.79126 to 33.30983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 34.2062 - MinusLogProbMetric: 34.2062 - val_loss: 33.3098 - val_MinusLogProbMetric: 33.3098 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 25/1000
2023-10-26 19:46:11.726 
Epoch 25/1000 
	 loss: 33.8697, MinusLogProbMetric: 33.8697, val_loss: 32.7658, val_MinusLogProbMetric: 32.7658

Epoch 25: val_loss improved from 33.30983 to 32.76579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 33.8697 - MinusLogProbMetric: 33.8697 - val_loss: 32.7658 - val_MinusLogProbMetric: 32.7658 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 26/1000
2023-10-26 19:46:48.902 
Epoch 26/1000 
	 loss: 33.6960, MinusLogProbMetric: 33.6960, val_loss: 33.1979, val_MinusLogProbMetric: 33.1979

Epoch 26: val_loss did not improve from 32.76579
196/196 - 37s - loss: 33.6960 - MinusLogProbMetric: 33.6960 - val_loss: 33.1979 - val_MinusLogProbMetric: 33.1979 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 27/1000
2023-10-26 19:47:25.059 
Epoch 27/1000 
	 loss: 33.6028, MinusLogProbMetric: 33.6028, val_loss: 33.6005, val_MinusLogProbMetric: 33.6005

Epoch 27: val_loss did not improve from 32.76579
196/196 - 36s - loss: 33.6028 - MinusLogProbMetric: 33.6028 - val_loss: 33.6005 - val_MinusLogProbMetric: 33.6005 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 28/1000
2023-10-26 19:48:01.073 
Epoch 28/1000 
	 loss: 33.3739, MinusLogProbMetric: 33.3739, val_loss: 33.8143, val_MinusLogProbMetric: 33.8143

Epoch 28: val_loss did not improve from 32.76579
196/196 - 36s - loss: 33.3739 - MinusLogProbMetric: 33.3739 - val_loss: 33.8143 - val_MinusLogProbMetric: 33.8143 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 29/1000
2023-10-26 19:48:36.961 
Epoch 29/1000 
	 loss: 32.8482, MinusLogProbMetric: 32.8482, val_loss: 33.6988, val_MinusLogProbMetric: 33.6988

Epoch 29: val_loss did not improve from 32.76579
196/196 - 36s - loss: 32.8482 - MinusLogProbMetric: 32.8482 - val_loss: 33.6988 - val_MinusLogProbMetric: 33.6988 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 30/1000
2023-10-26 19:49:12.535 
Epoch 30/1000 
	 loss: 32.9878, MinusLogProbMetric: 32.9878, val_loss: 34.0525, val_MinusLogProbMetric: 34.0525

Epoch 30: val_loss did not improve from 32.76579
196/196 - 36s - loss: 32.9878 - MinusLogProbMetric: 32.9878 - val_loss: 34.0525 - val_MinusLogProbMetric: 34.0525 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 31/1000
2023-10-26 19:49:48.502 
Epoch 31/1000 
	 loss: 32.8934, MinusLogProbMetric: 32.8934, val_loss: 34.8002, val_MinusLogProbMetric: 34.8002

Epoch 31: val_loss did not improve from 32.76579
196/196 - 36s - loss: 32.8934 - MinusLogProbMetric: 32.8934 - val_loss: 34.8002 - val_MinusLogProbMetric: 34.8002 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 32/1000
2023-10-26 19:50:24.069 
Epoch 32/1000 
	 loss: 32.6922, MinusLogProbMetric: 32.6922, val_loss: 31.6888, val_MinusLogProbMetric: 31.6888

Epoch 32: val_loss improved from 32.76579 to 31.68876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 32.6922 - MinusLogProbMetric: 32.6922 - val_loss: 31.6888 - val_MinusLogProbMetric: 31.6888 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 33/1000
2023-10-26 19:51:00.571 
Epoch 33/1000 
	 loss: 32.8268, MinusLogProbMetric: 32.8268, val_loss: 34.0786, val_MinusLogProbMetric: 34.0786

Epoch 33: val_loss did not improve from 31.68876
196/196 - 36s - loss: 32.8268 - MinusLogProbMetric: 32.8268 - val_loss: 34.0786 - val_MinusLogProbMetric: 34.0786 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 34/1000
2023-10-26 19:51:36.724 
Epoch 34/1000 
	 loss: 32.5462, MinusLogProbMetric: 32.5462, val_loss: 32.7695, val_MinusLogProbMetric: 32.7695

Epoch 34: val_loss did not improve from 31.68876
196/196 - 36s - loss: 32.5462 - MinusLogProbMetric: 32.5462 - val_loss: 32.7695 - val_MinusLogProbMetric: 32.7695 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 35/1000
2023-10-26 19:52:12.408 
Epoch 35/1000 
	 loss: 32.3880, MinusLogProbMetric: 32.3880, val_loss: 32.8694, val_MinusLogProbMetric: 32.8694

Epoch 35: val_loss did not improve from 31.68876
196/196 - 36s - loss: 32.3880 - MinusLogProbMetric: 32.3880 - val_loss: 32.8694 - val_MinusLogProbMetric: 32.8694 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 36/1000
2023-10-26 19:52:48.140 
Epoch 36/1000 
	 loss: 32.2997, MinusLogProbMetric: 32.2997, val_loss: 33.7092, val_MinusLogProbMetric: 33.7092

Epoch 36: val_loss did not improve from 31.68876
196/196 - 36s - loss: 32.2997 - MinusLogProbMetric: 32.2997 - val_loss: 33.7092 - val_MinusLogProbMetric: 33.7092 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 37/1000
2023-10-26 19:53:23.879 
Epoch 37/1000 
	 loss: 32.3894, MinusLogProbMetric: 32.3894, val_loss: 31.9892, val_MinusLogProbMetric: 31.9892

Epoch 37: val_loss did not improve from 31.68876
196/196 - 36s - loss: 32.3894 - MinusLogProbMetric: 32.3894 - val_loss: 31.9892 - val_MinusLogProbMetric: 31.9892 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 38/1000
2023-10-26 19:53:59.834 
Epoch 38/1000 
	 loss: 32.1358, MinusLogProbMetric: 32.1358, val_loss: 32.3962, val_MinusLogProbMetric: 32.3962

Epoch 38: val_loss did not improve from 31.68876
196/196 - 36s - loss: 32.1358 - MinusLogProbMetric: 32.1358 - val_loss: 32.3962 - val_MinusLogProbMetric: 32.3962 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 39/1000
2023-10-26 19:54:35.212 
Epoch 39/1000 
	 loss: 32.2507, MinusLogProbMetric: 32.2507, val_loss: 31.5647, val_MinusLogProbMetric: 31.5647

Epoch 39: val_loss improved from 31.68876 to 31.56470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 32.2507 - MinusLogProbMetric: 32.2507 - val_loss: 31.5647 - val_MinusLogProbMetric: 31.5647 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 40/1000
2023-10-26 19:55:11.688 
Epoch 40/1000 
	 loss: 31.9295, MinusLogProbMetric: 31.9295, val_loss: 31.7728, val_MinusLogProbMetric: 31.7728

Epoch 40: val_loss did not improve from 31.56470
196/196 - 36s - loss: 31.9295 - MinusLogProbMetric: 31.9295 - val_loss: 31.7728 - val_MinusLogProbMetric: 31.7728 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 41/1000
2023-10-26 19:55:47.530 
Epoch 41/1000 
	 loss: 31.9462, MinusLogProbMetric: 31.9462, val_loss: 33.5858, val_MinusLogProbMetric: 33.5858

Epoch 41: val_loss did not improve from 31.56470
196/196 - 36s - loss: 31.9462 - MinusLogProbMetric: 31.9462 - val_loss: 33.5858 - val_MinusLogProbMetric: 33.5858 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 42/1000
2023-10-26 19:56:23.226 
Epoch 42/1000 
	 loss: 32.0133, MinusLogProbMetric: 32.0133, val_loss: 33.6932, val_MinusLogProbMetric: 33.6932

Epoch 42: val_loss did not improve from 31.56470
196/196 - 36s - loss: 32.0133 - MinusLogProbMetric: 32.0133 - val_loss: 33.6932 - val_MinusLogProbMetric: 33.6932 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 43/1000
2023-10-26 19:56:59.135 
Epoch 43/1000 
	 loss: 31.8276, MinusLogProbMetric: 31.8276, val_loss: 33.0493, val_MinusLogProbMetric: 33.0493

Epoch 43: val_loss did not improve from 31.56470
196/196 - 36s - loss: 31.8276 - MinusLogProbMetric: 31.8276 - val_loss: 33.0493 - val_MinusLogProbMetric: 33.0493 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 44/1000
2023-10-26 19:57:34.747 
Epoch 44/1000 
	 loss: 31.8185, MinusLogProbMetric: 31.8185, val_loss: 32.1095, val_MinusLogProbMetric: 32.1095

Epoch 44: val_loss did not improve from 31.56470
196/196 - 36s - loss: 31.8185 - MinusLogProbMetric: 31.8185 - val_loss: 32.1095 - val_MinusLogProbMetric: 32.1095 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 45/1000
2023-10-26 19:58:10.490 
Epoch 45/1000 
	 loss: 31.6706, MinusLogProbMetric: 31.6706, val_loss: 31.6689, val_MinusLogProbMetric: 31.6689

Epoch 45: val_loss did not improve from 31.56470
196/196 - 36s - loss: 31.6706 - MinusLogProbMetric: 31.6706 - val_loss: 31.6689 - val_MinusLogProbMetric: 31.6689 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 46/1000
2023-10-26 19:58:46.167 
Epoch 46/1000 
	 loss: 31.8864, MinusLogProbMetric: 31.8864, val_loss: 31.7207, val_MinusLogProbMetric: 31.7207

Epoch 46: val_loss did not improve from 31.56470
196/196 - 36s - loss: 31.8864 - MinusLogProbMetric: 31.8864 - val_loss: 31.7207 - val_MinusLogProbMetric: 31.7207 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 47/1000
2023-10-26 19:59:21.601 
Epoch 47/1000 
	 loss: 31.3793, MinusLogProbMetric: 31.3793, val_loss: 32.2176, val_MinusLogProbMetric: 32.2176

Epoch 47: val_loss did not improve from 31.56470
196/196 - 35s - loss: 31.3793 - MinusLogProbMetric: 31.3793 - val_loss: 32.2176 - val_MinusLogProbMetric: 32.2176 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 48/1000
2023-10-26 19:59:57.337 
Epoch 48/1000 
	 loss: 31.3855, MinusLogProbMetric: 31.3855, val_loss: 32.5162, val_MinusLogProbMetric: 32.5162

Epoch 48: val_loss did not improve from 31.56470
196/196 - 36s - loss: 31.3855 - MinusLogProbMetric: 31.3855 - val_loss: 32.5162 - val_MinusLogProbMetric: 32.5162 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 49/1000
2023-10-26 20:00:33.061 
Epoch 49/1000 
	 loss: 31.6230, MinusLogProbMetric: 31.6230, val_loss: 31.2458, val_MinusLogProbMetric: 31.2458

Epoch 49: val_loss improved from 31.56470 to 31.24581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 31.6230 - MinusLogProbMetric: 31.6230 - val_loss: 31.2458 - val_MinusLogProbMetric: 31.2458 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 50/1000
2023-10-26 20:01:09.088 
Epoch 50/1000 
	 loss: 31.3874, MinusLogProbMetric: 31.3874, val_loss: 31.7951, val_MinusLogProbMetric: 31.7951

Epoch 50: val_loss did not improve from 31.24581
196/196 - 35s - loss: 31.3874 - MinusLogProbMetric: 31.3874 - val_loss: 31.7951 - val_MinusLogProbMetric: 31.7951 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 51/1000
2023-10-26 20:01:44.846 
Epoch 51/1000 
	 loss: 31.5104, MinusLogProbMetric: 31.5104, val_loss: 32.0829, val_MinusLogProbMetric: 32.0829

Epoch 51: val_loss did not improve from 31.24581
196/196 - 36s - loss: 31.5104 - MinusLogProbMetric: 31.5104 - val_loss: 32.0829 - val_MinusLogProbMetric: 32.0829 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 52/1000
2023-10-26 20:02:20.710 
Epoch 52/1000 
	 loss: 31.4149, MinusLogProbMetric: 31.4149, val_loss: 31.8447, val_MinusLogProbMetric: 31.8447

Epoch 52: val_loss did not improve from 31.24581
196/196 - 36s - loss: 31.4149 - MinusLogProbMetric: 31.4149 - val_loss: 31.8447 - val_MinusLogProbMetric: 31.8447 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 53/1000
2023-10-26 20:02:56.584 
Epoch 53/1000 
	 loss: 31.4295, MinusLogProbMetric: 31.4295, val_loss: 32.1417, val_MinusLogProbMetric: 32.1417

Epoch 53: val_loss did not improve from 31.24581
196/196 - 36s - loss: 31.4295 - MinusLogProbMetric: 31.4295 - val_loss: 32.1417 - val_MinusLogProbMetric: 32.1417 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 54/1000
2023-10-26 20:03:32.170 
Epoch 54/1000 
	 loss: 31.2520, MinusLogProbMetric: 31.2520, val_loss: 31.7167, val_MinusLogProbMetric: 31.7167

Epoch 54: val_loss did not improve from 31.24581
196/196 - 36s - loss: 31.2520 - MinusLogProbMetric: 31.2520 - val_loss: 31.7167 - val_MinusLogProbMetric: 31.7167 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 55/1000
2023-10-26 20:04:08.075 
Epoch 55/1000 
	 loss: 31.5083, MinusLogProbMetric: 31.5083, val_loss: 31.4367, val_MinusLogProbMetric: 31.4367

Epoch 55: val_loss did not improve from 31.24581
196/196 - 36s - loss: 31.5083 - MinusLogProbMetric: 31.5083 - val_loss: 31.4367 - val_MinusLogProbMetric: 31.4367 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 56/1000
2023-10-26 20:04:43.700 
Epoch 56/1000 
	 loss: 31.0141, MinusLogProbMetric: 31.0141, val_loss: 31.1810, val_MinusLogProbMetric: 31.1810

Epoch 56: val_loss improved from 31.24581 to 31.18102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 31.0141 - MinusLogProbMetric: 31.0141 - val_loss: 31.1810 - val_MinusLogProbMetric: 31.1810 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 57/1000
2023-10-26 20:05:19.463 
Epoch 57/1000 
	 loss: 31.2517, MinusLogProbMetric: 31.2517, val_loss: 32.3576, val_MinusLogProbMetric: 32.3576

Epoch 57: val_loss did not improve from 31.18102
196/196 - 35s - loss: 31.2517 - MinusLogProbMetric: 31.2517 - val_loss: 32.3576 - val_MinusLogProbMetric: 32.3576 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 58/1000
2023-10-26 20:05:55.303 
Epoch 58/1000 
	 loss: 31.1502, MinusLogProbMetric: 31.1502, val_loss: 31.1971, val_MinusLogProbMetric: 31.1971

Epoch 58: val_loss did not improve from 31.18102
196/196 - 36s - loss: 31.1502 - MinusLogProbMetric: 31.1502 - val_loss: 31.1971 - val_MinusLogProbMetric: 31.1971 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 59/1000
2023-10-26 20:06:30.732 
Epoch 59/1000 
	 loss: 31.2639, MinusLogProbMetric: 31.2639, val_loss: 32.5316, val_MinusLogProbMetric: 32.5316

Epoch 59: val_loss did not improve from 31.18102
196/196 - 35s - loss: 31.2639 - MinusLogProbMetric: 31.2639 - val_loss: 32.5316 - val_MinusLogProbMetric: 32.5316 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 60/1000
2023-10-26 20:07:06.789 
Epoch 60/1000 
	 loss: 31.0490, MinusLogProbMetric: 31.0490, val_loss: 30.9637, val_MinusLogProbMetric: 30.9637

Epoch 60: val_loss improved from 31.18102 to 30.96374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 31.0490 - MinusLogProbMetric: 31.0490 - val_loss: 30.9637 - val_MinusLogProbMetric: 30.9637 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 61/1000
2023-10-26 20:07:43.523 
Epoch 61/1000 
	 loss: 30.9182, MinusLogProbMetric: 30.9182, val_loss: 30.9371, val_MinusLogProbMetric: 30.9371

Epoch 61: val_loss improved from 30.96374 to 30.93705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 30.9182 - MinusLogProbMetric: 30.9182 - val_loss: 30.9371 - val_MinusLogProbMetric: 30.9371 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 62/1000
2023-10-26 20:08:19.558 
Epoch 62/1000 
	 loss: 30.8225, MinusLogProbMetric: 30.8225, val_loss: 31.1298, val_MinusLogProbMetric: 31.1298

Epoch 62: val_loss did not improve from 30.93705
196/196 - 36s - loss: 30.8225 - MinusLogProbMetric: 30.8225 - val_loss: 31.1298 - val_MinusLogProbMetric: 31.1298 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 63/1000
2023-10-26 20:08:54.956 
Epoch 63/1000 
	 loss: 31.2071, MinusLogProbMetric: 31.2071, val_loss: 30.5022, val_MinusLogProbMetric: 30.5022

Epoch 63: val_loss improved from 30.93705 to 30.50222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 31.2071 - MinusLogProbMetric: 31.2071 - val_loss: 30.5022 - val_MinusLogProbMetric: 30.5022 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 64/1000
2023-10-26 20:09:31.563 
Epoch 64/1000 
	 loss: 30.6716, MinusLogProbMetric: 30.6716, val_loss: 30.2506, val_MinusLogProbMetric: 30.2506

Epoch 64: val_loss improved from 30.50222 to 30.25061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 30.6716 - MinusLogProbMetric: 30.6716 - val_loss: 30.2506 - val_MinusLogProbMetric: 30.2506 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 65/1000
2023-10-26 20:10:08.041 
Epoch 65/1000 
	 loss: 30.7821, MinusLogProbMetric: 30.7821, val_loss: 31.9636, val_MinusLogProbMetric: 31.9636

Epoch 65: val_loss did not improve from 30.25061
196/196 - 36s - loss: 30.7821 - MinusLogProbMetric: 30.7821 - val_loss: 31.9636 - val_MinusLogProbMetric: 31.9636 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 66/1000
2023-10-26 20:10:44.183 
Epoch 66/1000 
	 loss: 30.7867, MinusLogProbMetric: 30.7867, val_loss: 30.2373, val_MinusLogProbMetric: 30.2373

Epoch 66: val_loss improved from 30.25061 to 30.23734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 30.7867 - MinusLogProbMetric: 30.7867 - val_loss: 30.2373 - val_MinusLogProbMetric: 30.2373 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 67/1000
2023-10-26 20:11:19.753 
Epoch 67/1000 
	 loss: 30.7921, MinusLogProbMetric: 30.7921, val_loss: 30.8270, val_MinusLogProbMetric: 30.8270

Epoch 67: val_loss did not improve from 30.23734
196/196 - 35s - loss: 30.7921 - MinusLogProbMetric: 30.7921 - val_loss: 30.8270 - val_MinusLogProbMetric: 30.8270 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 68/1000
2023-10-26 20:11:55.815 
Epoch 68/1000 
	 loss: 30.7948, MinusLogProbMetric: 30.7948, val_loss: 32.8543, val_MinusLogProbMetric: 32.8543

Epoch 68: val_loss did not improve from 30.23734
196/196 - 36s - loss: 30.7948 - MinusLogProbMetric: 30.7948 - val_loss: 32.8543 - val_MinusLogProbMetric: 32.8543 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 69/1000
2023-10-26 20:12:31.586 
Epoch 69/1000 
	 loss: 30.6188, MinusLogProbMetric: 30.6188, val_loss: 30.6091, val_MinusLogProbMetric: 30.6091

Epoch 69: val_loss did not improve from 30.23734
196/196 - 36s - loss: 30.6188 - MinusLogProbMetric: 30.6188 - val_loss: 30.6091 - val_MinusLogProbMetric: 30.6091 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 70/1000
2023-10-26 20:13:07.750 
Epoch 70/1000 
	 loss: 30.5877, MinusLogProbMetric: 30.5877, val_loss: 30.0187, val_MinusLogProbMetric: 30.0187

Epoch 70: val_loss improved from 30.23734 to 30.01870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 30.5877 - MinusLogProbMetric: 30.5877 - val_loss: 30.0187 - val_MinusLogProbMetric: 30.0187 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 71/1000
2023-10-26 20:13:44.423 
Epoch 71/1000 
	 loss: 30.4253, MinusLogProbMetric: 30.4253, val_loss: 30.4807, val_MinusLogProbMetric: 30.4807

Epoch 71: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.4253 - MinusLogProbMetric: 30.4253 - val_loss: 30.4807 - val_MinusLogProbMetric: 30.4807 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 72/1000
2023-10-26 20:14:20.446 
Epoch 72/1000 
	 loss: 30.5213, MinusLogProbMetric: 30.5213, val_loss: 31.0281, val_MinusLogProbMetric: 31.0281

Epoch 72: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.5213 - MinusLogProbMetric: 30.5213 - val_loss: 31.0281 - val_MinusLogProbMetric: 31.0281 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 73/1000
2023-10-26 20:14:56.125 
Epoch 73/1000 
	 loss: 30.6082, MinusLogProbMetric: 30.6082, val_loss: 30.7282, val_MinusLogProbMetric: 30.7282

Epoch 73: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.6082 - MinusLogProbMetric: 30.6082 - val_loss: 30.7282 - val_MinusLogProbMetric: 30.7282 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 74/1000
2023-10-26 20:15:31.988 
Epoch 74/1000 
	 loss: 30.6094, MinusLogProbMetric: 30.6094, val_loss: 31.6565, val_MinusLogProbMetric: 31.6565

Epoch 74: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.6094 - MinusLogProbMetric: 30.6094 - val_loss: 31.6565 - val_MinusLogProbMetric: 31.6565 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 75/1000
2023-10-26 20:16:07.602 
Epoch 75/1000 
	 loss: 30.4963, MinusLogProbMetric: 30.4963, val_loss: 30.0676, val_MinusLogProbMetric: 30.0676

Epoch 75: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.4963 - MinusLogProbMetric: 30.4963 - val_loss: 30.0676 - val_MinusLogProbMetric: 30.0676 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 76/1000
2023-10-26 20:16:43.443 
Epoch 76/1000 
	 loss: 30.2287, MinusLogProbMetric: 30.2287, val_loss: 31.5844, val_MinusLogProbMetric: 31.5844

Epoch 76: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.2287 - MinusLogProbMetric: 30.2287 - val_loss: 31.5844 - val_MinusLogProbMetric: 31.5844 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 77/1000
2023-10-26 20:17:19.060 
Epoch 77/1000 
	 loss: 30.3764, MinusLogProbMetric: 30.3764, val_loss: 30.4228, val_MinusLogProbMetric: 30.4228

Epoch 77: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.3764 - MinusLogProbMetric: 30.3764 - val_loss: 30.4228 - val_MinusLogProbMetric: 30.4228 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 78/1000
2023-10-26 20:17:54.572 
Epoch 78/1000 
	 loss: 30.2568, MinusLogProbMetric: 30.2568, val_loss: 30.3539, val_MinusLogProbMetric: 30.3539

Epoch 78: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.2568 - MinusLogProbMetric: 30.2568 - val_loss: 30.3539 - val_MinusLogProbMetric: 30.3539 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 79/1000
2023-10-26 20:18:30.235 
Epoch 79/1000 
	 loss: 30.2731, MinusLogProbMetric: 30.2731, val_loss: 31.6988, val_MinusLogProbMetric: 31.6988

Epoch 79: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.2731 - MinusLogProbMetric: 30.2731 - val_loss: 31.6988 - val_MinusLogProbMetric: 31.6988 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 80/1000
2023-10-26 20:19:05.435 
Epoch 80/1000 
	 loss: 30.2417, MinusLogProbMetric: 30.2417, val_loss: 30.6717, val_MinusLogProbMetric: 30.6717

Epoch 80: val_loss did not improve from 30.01870
196/196 - 35s - loss: 30.2417 - MinusLogProbMetric: 30.2417 - val_loss: 30.6717 - val_MinusLogProbMetric: 30.6717 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 81/1000
2023-10-26 20:19:41.152 
Epoch 81/1000 
	 loss: 30.0530, MinusLogProbMetric: 30.0530, val_loss: 31.4886, val_MinusLogProbMetric: 31.4886

Epoch 81: val_loss did not improve from 30.01870
196/196 - 36s - loss: 30.0530 - MinusLogProbMetric: 30.0530 - val_loss: 31.4886 - val_MinusLogProbMetric: 31.4886 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 82/1000
2023-10-26 20:20:17.046 
Epoch 82/1000 
	 loss: 30.4913, MinusLogProbMetric: 30.4913, val_loss: 29.9872, val_MinusLogProbMetric: 29.9872

Epoch 82: val_loss improved from 30.01870 to 29.98715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 30.4913 - MinusLogProbMetric: 30.4913 - val_loss: 29.9872 - val_MinusLogProbMetric: 29.9872 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 83/1000
2023-10-26 20:20:53.513 
Epoch 83/1000 
	 loss: 30.0677, MinusLogProbMetric: 30.0677, val_loss: 30.2135, val_MinusLogProbMetric: 30.2135

Epoch 83: val_loss did not improve from 29.98715
196/196 - 36s - loss: 30.0677 - MinusLogProbMetric: 30.0677 - val_loss: 30.2135 - val_MinusLogProbMetric: 30.2135 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 84/1000
2023-10-26 20:21:29.107 
Epoch 84/1000 
	 loss: 30.1063, MinusLogProbMetric: 30.1063, val_loss: 30.2981, val_MinusLogProbMetric: 30.2981

Epoch 84: val_loss did not improve from 29.98715
196/196 - 36s - loss: 30.1063 - MinusLogProbMetric: 30.1063 - val_loss: 30.2981 - val_MinusLogProbMetric: 30.2981 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 85/1000
2023-10-26 20:22:04.685 
Epoch 85/1000 
	 loss: 30.2085, MinusLogProbMetric: 30.2085, val_loss: 31.1036, val_MinusLogProbMetric: 31.1036

Epoch 85: val_loss did not improve from 29.98715
196/196 - 36s - loss: 30.2085 - MinusLogProbMetric: 30.2085 - val_loss: 31.1036 - val_MinusLogProbMetric: 31.1036 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 86/1000
2023-10-26 20:22:40.527 
Epoch 86/1000 
	 loss: 30.0247, MinusLogProbMetric: 30.0247, val_loss: 30.4223, val_MinusLogProbMetric: 30.4223

Epoch 86: val_loss did not improve from 29.98715
196/196 - 36s - loss: 30.0247 - MinusLogProbMetric: 30.0247 - val_loss: 30.4223 - val_MinusLogProbMetric: 30.4223 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 87/1000
2023-10-26 20:23:16.413 
Epoch 87/1000 
	 loss: 29.9293, MinusLogProbMetric: 29.9293, val_loss: 30.2759, val_MinusLogProbMetric: 30.2759

Epoch 87: val_loss did not improve from 29.98715
196/196 - 36s - loss: 29.9293 - MinusLogProbMetric: 29.9293 - val_loss: 30.2759 - val_MinusLogProbMetric: 30.2759 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 88/1000
2023-10-26 20:23:51.717 
Epoch 88/1000 
	 loss: 30.1441, MinusLogProbMetric: 30.1441, val_loss: 29.8275, val_MinusLogProbMetric: 29.8275

Epoch 88: val_loss improved from 29.98715 to 29.82752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 30.1441 - MinusLogProbMetric: 30.1441 - val_loss: 29.8275 - val_MinusLogProbMetric: 29.8275 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 89/1000
2023-10-26 20:24:28.040 
Epoch 89/1000 
	 loss: 29.9319, MinusLogProbMetric: 29.9319, val_loss: 29.7115, val_MinusLogProbMetric: 29.7115

Epoch 89: val_loss improved from 29.82752 to 29.71151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 29.9319 - MinusLogProbMetric: 29.9319 - val_loss: 29.7115 - val_MinusLogProbMetric: 29.7115 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 90/1000
2023-10-26 20:25:04.302 
Epoch 90/1000 
	 loss: 30.1836, MinusLogProbMetric: 30.1836, val_loss: 30.0905, val_MinusLogProbMetric: 30.0905

Epoch 90: val_loss did not improve from 29.71151
196/196 - 36s - loss: 30.1836 - MinusLogProbMetric: 30.1836 - val_loss: 30.0905 - val_MinusLogProbMetric: 30.0905 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 91/1000
2023-10-26 20:25:40.460 
Epoch 91/1000 
	 loss: 29.8859, MinusLogProbMetric: 29.8859, val_loss: 29.6485, val_MinusLogProbMetric: 29.6485

Epoch 91: val_loss improved from 29.71151 to 29.64850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 29.8859 - MinusLogProbMetric: 29.8859 - val_loss: 29.6485 - val_MinusLogProbMetric: 29.6485 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 92/1000
2023-10-26 20:26:16.769 
Epoch 92/1000 
	 loss: 29.8055, MinusLogProbMetric: 29.8055, val_loss: 31.2091, val_MinusLogProbMetric: 31.2091

Epoch 92: val_loss did not improve from 29.64850
196/196 - 36s - loss: 29.8055 - MinusLogProbMetric: 29.8055 - val_loss: 31.2091 - val_MinusLogProbMetric: 31.2091 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 93/1000
2023-10-26 20:26:52.374 
Epoch 93/1000 
	 loss: 30.0023, MinusLogProbMetric: 30.0023, val_loss: 29.7601, val_MinusLogProbMetric: 29.7601

Epoch 93: val_loss did not improve from 29.64850
196/196 - 36s - loss: 30.0023 - MinusLogProbMetric: 30.0023 - val_loss: 29.7601 - val_MinusLogProbMetric: 29.7601 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 94/1000
2023-10-26 20:27:27.841 
Epoch 94/1000 
	 loss: 30.0658, MinusLogProbMetric: 30.0658, val_loss: 30.0359, val_MinusLogProbMetric: 30.0359

Epoch 94: val_loss did not improve from 29.64850
196/196 - 35s - loss: 30.0658 - MinusLogProbMetric: 30.0658 - val_loss: 30.0359 - val_MinusLogProbMetric: 30.0359 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 95/1000
2023-10-26 20:28:02.995 
Epoch 95/1000 
	 loss: 29.7598, MinusLogProbMetric: 29.7598, val_loss: 30.4936, val_MinusLogProbMetric: 30.4936

Epoch 95: val_loss did not improve from 29.64850
196/196 - 35s - loss: 29.7598 - MinusLogProbMetric: 29.7598 - val_loss: 30.4936 - val_MinusLogProbMetric: 30.4936 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 96/1000
2023-10-26 20:28:38.271 
Epoch 96/1000 
	 loss: 29.9761, MinusLogProbMetric: 29.9761, val_loss: 30.9038, val_MinusLogProbMetric: 30.9038

Epoch 96: val_loss did not improve from 29.64850
196/196 - 35s - loss: 29.9761 - MinusLogProbMetric: 29.9761 - val_loss: 30.9038 - val_MinusLogProbMetric: 30.9038 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 97/1000
2023-10-26 20:29:13.720 
Epoch 97/1000 
	 loss: 30.2415, MinusLogProbMetric: 30.2415, val_loss: 30.2055, val_MinusLogProbMetric: 30.2055

Epoch 97: val_loss did not improve from 29.64850
196/196 - 35s - loss: 30.2415 - MinusLogProbMetric: 30.2415 - val_loss: 30.2055 - val_MinusLogProbMetric: 30.2055 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 98/1000
2023-10-26 20:29:49.676 
Epoch 98/1000 
	 loss: 30.1032, MinusLogProbMetric: 30.1032, val_loss: 29.7190, val_MinusLogProbMetric: 29.7190

Epoch 98: val_loss did not improve from 29.64850
196/196 - 36s - loss: 30.1032 - MinusLogProbMetric: 30.1032 - val_loss: 29.7190 - val_MinusLogProbMetric: 29.7190 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 99/1000
2023-10-26 20:30:25.121 
Epoch 99/1000 
	 loss: 29.8492, MinusLogProbMetric: 29.8492, val_loss: 29.9744, val_MinusLogProbMetric: 29.9744

Epoch 99: val_loss did not improve from 29.64850
196/196 - 35s - loss: 29.8492 - MinusLogProbMetric: 29.8492 - val_loss: 29.9744 - val_MinusLogProbMetric: 29.9744 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 100/1000
2023-10-26 20:31:01.496 
Epoch 100/1000 
	 loss: 29.8752, MinusLogProbMetric: 29.8752, val_loss: 30.6283, val_MinusLogProbMetric: 30.6283

Epoch 100: val_loss did not improve from 29.64850
196/196 - 36s - loss: 29.8752 - MinusLogProbMetric: 29.8752 - val_loss: 30.6283 - val_MinusLogProbMetric: 30.6283 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 101/1000
2023-10-26 20:31:36.766 
Epoch 101/1000 
	 loss: 29.7988, MinusLogProbMetric: 29.7988, val_loss: 30.0001, val_MinusLogProbMetric: 30.0001

Epoch 101: val_loss did not improve from 29.64850
196/196 - 35s - loss: 29.7988 - MinusLogProbMetric: 29.7988 - val_loss: 30.0001 - val_MinusLogProbMetric: 30.0001 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 102/1000
2023-10-26 20:32:12.754 
Epoch 102/1000 
	 loss: 29.7924, MinusLogProbMetric: 29.7924, val_loss: 31.0347, val_MinusLogProbMetric: 31.0347

Epoch 102: val_loss did not improve from 29.64850
196/196 - 36s - loss: 29.7924 - MinusLogProbMetric: 29.7924 - val_loss: 31.0347 - val_MinusLogProbMetric: 31.0347 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 103/1000
2023-10-26 20:32:48.545 
Epoch 103/1000 
	 loss: 29.7654, MinusLogProbMetric: 29.7654, val_loss: 29.7311, val_MinusLogProbMetric: 29.7311

Epoch 103: val_loss did not improve from 29.64850
196/196 - 36s - loss: 29.7654 - MinusLogProbMetric: 29.7654 - val_loss: 29.7311 - val_MinusLogProbMetric: 29.7311 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 104/1000
2023-10-26 20:33:24.273 
Epoch 104/1000 
	 loss: 29.7375, MinusLogProbMetric: 29.7375, val_loss: 29.8079, val_MinusLogProbMetric: 29.8079

Epoch 104: val_loss did not improve from 29.64850
196/196 - 36s - loss: 29.7375 - MinusLogProbMetric: 29.7375 - val_loss: 29.8079 - val_MinusLogProbMetric: 29.8079 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 105/1000
2023-10-26 20:33:59.783 
Epoch 105/1000 
	 loss: 29.5719, MinusLogProbMetric: 29.5719, val_loss: 29.9306, val_MinusLogProbMetric: 29.9306

Epoch 105: val_loss did not improve from 29.64850
196/196 - 36s - loss: 29.5719 - MinusLogProbMetric: 29.5719 - val_loss: 29.9306 - val_MinusLogProbMetric: 29.9306 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 106/1000
2023-10-26 20:34:35.078 
Epoch 106/1000 
	 loss: 29.6977, MinusLogProbMetric: 29.6977, val_loss: 30.0382, val_MinusLogProbMetric: 30.0382

Epoch 106: val_loss did not improve from 29.64850
196/196 - 35s - loss: 29.6977 - MinusLogProbMetric: 29.6977 - val_loss: 30.0382 - val_MinusLogProbMetric: 30.0382 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 107/1000
2023-10-26 20:35:10.585 
Epoch 107/1000 
	 loss: 29.5859, MinusLogProbMetric: 29.5859, val_loss: 30.7704, val_MinusLogProbMetric: 30.7704

Epoch 107: val_loss did not improve from 29.64850
196/196 - 36s - loss: 29.5859 - MinusLogProbMetric: 29.5859 - val_loss: 30.7704 - val_MinusLogProbMetric: 30.7704 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 108/1000
2023-10-26 20:35:45.940 
Epoch 108/1000 
	 loss: 29.5313, MinusLogProbMetric: 29.5313, val_loss: 29.7694, val_MinusLogProbMetric: 29.7694

Epoch 108: val_loss did not improve from 29.64850
196/196 - 35s - loss: 29.5313 - MinusLogProbMetric: 29.5313 - val_loss: 29.7694 - val_MinusLogProbMetric: 29.7694 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 109/1000
2023-10-26 20:36:21.868 
Epoch 109/1000 
	 loss: 29.6406, MinusLogProbMetric: 29.6406, val_loss: 29.9536, val_MinusLogProbMetric: 29.9536

Epoch 109: val_loss did not improve from 29.64850
196/196 - 36s - loss: 29.6406 - MinusLogProbMetric: 29.6406 - val_loss: 29.9536 - val_MinusLogProbMetric: 29.9536 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 110/1000
2023-10-26 20:36:57.293 
Epoch 110/1000 
	 loss: 29.7041, MinusLogProbMetric: 29.7041, val_loss: 29.7456, val_MinusLogProbMetric: 29.7456

Epoch 110: val_loss did not improve from 29.64850
196/196 - 35s - loss: 29.7041 - MinusLogProbMetric: 29.7041 - val_loss: 29.7456 - val_MinusLogProbMetric: 29.7456 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 111/1000
2023-10-26 20:37:32.895 
Epoch 111/1000 
	 loss: 29.5665, MinusLogProbMetric: 29.5665, val_loss: 29.2916, val_MinusLogProbMetric: 29.2916

Epoch 111: val_loss improved from 29.64850 to 29.29160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 29.5665 - MinusLogProbMetric: 29.5665 - val_loss: 29.2916 - val_MinusLogProbMetric: 29.2916 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 112/1000
2023-10-26 20:38:08.888 
Epoch 112/1000 
	 loss: 29.6530, MinusLogProbMetric: 29.6530, val_loss: 30.0179, val_MinusLogProbMetric: 30.0179

Epoch 112: val_loss did not improve from 29.29160
196/196 - 35s - loss: 29.6530 - MinusLogProbMetric: 29.6530 - val_loss: 30.0179 - val_MinusLogProbMetric: 30.0179 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 113/1000
2023-10-26 20:38:44.354 
Epoch 113/1000 
	 loss: 29.7552, MinusLogProbMetric: 29.7552, val_loss: 31.2075, val_MinusLogProbMetric: 31.2075

Epoch 113: val_loss did not improve from 29.29160
196/196 - 35s - loss: 29.7552 - MinusLogProbMetric: 29.7552 - val_loss: 31.2075 - val_MinusLogProbMetric: 31.2075 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 114/1000
2023-10-26 20:39:20.201 
Epoch 114/1000 
	 loss: 29.4051, MinusLogProbMetric: 29.4051, val_loss: 29.4029, val_MinusLogProbMetric: 29.4029

Epoch 114: val_loss did not improve from 29.29160
196/196 - 36s - loss: 29.4051 - MinusLogProbMetric: 29.4051 - val_loss: 29.4029 - val_MinusLogProbMetric: 29.4029 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 115/1000
2023-10-26 20:39:56.074 
Epoch 115/1000 
	 loss: 29.4308, MinusLogProbMetric: 29.4308, val_loss: 29.9653, val_MinusLogProbMetric: 29.9653

Epoch 115: val_loss did not improve from 29.29160
196/196 - 36s - loss: 29.4308 - MinusLogProbMetric: 29.4308 - val_loss: 29.9653 - val_MinusLogProbMetric: 29.9653 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 116/1000
2023-10-26 20:40:32.021 
Epoch 116/1000 
	 loss: 29.6367, MinusLogProbMetric: 29.6367, val_loss: 29.7893, val_MinusLogProbMetric: 29.7893

Epoch 116: val_loss did not improve from 29.29160
196/196 - 36s - loss: 29.6367 - MinusLogProbMetric: 29.6367 - val_loss: 29.7893 - val_MinusLogProbMetric: 29.7893 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 117/1000
2023-10-26 20:41:07.495 
Epoch 117/1000 
	 loss: 29.4387, MinusLogProbMetric: 29.4387, val_loss: 30.7006, val_MinusLogProbMetric: 30.7006

Epoch 117: val_loss did not improve from 29.29160
196/196 - 35s - loss: 29.4387 - MinusLogProbMetric: 29.4387 - val_loss: 30.7006 - val_MinusLogProbMetric: 30.7006 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 118/1000
2023-10-26 20:41:43.034 
Epoch 118/1000 
	 loss: 29.5571, MinusLogProbMetric: 29.5571, val_loss: 29.9115, val_MinusLogProbMetric: 29.9115

Epoch 118: val_loss did not improve from 29.29160
196/196 - 36s - loss: 29.5571 - MinusLogProbMetric: 29.5571 - val_loss: 29.9115 - val_MinusLogProbMetric: 29.9115 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 119/1000
2023-10-26 20:42:18.367 
Epoch 119/1000 
	 loss: 29.5329, MinusLogProbMetric: 29.5329, val_loss: 29.7509, val_MinusLogProbMetric: 29.7509

Epoch 119: val_loss did not improve from 29.29160
196/196 - 35s - loss: 29.5329 - MinusLogProbMetric: 29.5329 - val_loss: 29.7509 - val_MinusLogProbMetric: 29.7509 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 120/1000
2023-10-26 20:42:53.896 
Epoch 120/1000 
	 loss: 29.4103, MinusLogProbMetric: 29.4103, val_loss: 29.9874, val_MinusLogProbMetric: 29.9874

Epoch 120: val_loss did not improve from 29.29160
196/196 - 36s - loss: 29.4103 - MinusLogProbMetric: 29.4103 - val_loss: 29.9874 - val_MinusLogProbMetric: 29.9874 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 121/1000
2023-10-26 20:43:29.225 
Epoch 121/1000 
	 loss: 29.3265, MinusLogProbMetric: 29.3265, val_loss: 30.4410, val_MinusLogProbMetric: 30.4410

Epoch 121: val_loss did not improve from 29.29160
196/196 - 35s - loss: 29.3265 - MinusLogProbMetric: 29.3265 - val_loss: 30.4410 - val_MinusLogProbMetric: 30.4410 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 122/1000
2023-10-26 20:44:05.004 
Epoch 122/1000 
	 loss: 29.4328, MinusLogProbMetric: 29.4328, val_loss: 29.2009, val_MinusLogProbMetric: 29.2009

Epoch 122: val_loss improved from 29.29160 to 29.20090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 29.4328 - MinusLogProbMetric: 29.4328 - val_loss: 29.2009 - val_MinusLogProbMetric: 29.2009 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 123/1000
2023-10-26 20:44:41.281 
Epoch 123/1000 
	 loss: 29.2488, MinusLogProbMetric: 29.2488, val_loss: 29.2675, val_MinusLogProbMetric: 29.2675

Epoch 123: val_loss did not improve from 29.20090
196/196 - 36s - loss: 29.2488 - MinusLogProbMetric: 29.2488 - val_loss: 29.2675 - val_MinusLogProbMetric: 29.2675 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 124/1000
2023-10-26 20:45:16.925 
Epoch 124/1000 
	 loss: 29.4706, MinusLogProbMetric: 29.4706, val_loss: 29.3298, val_MinusLogProbMetric: 29.3298

Epoch 124: val_loss did not improve from 29.20090
196/196 - 36s - loss: 29.4706 - MinusLogProbMetric: 29.4706 - val_loss: 29.3298 - val_MinusLogProbMetric: 29.3298 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 125/1000
2023-10-26 20:45:52.335 
Epoch 125/1000 
	 loss: 29.3263, MinusLogProbMetric: 29.3263, val_loss: 29.8567, val_MinusLogProbMetric: 29.8567

Epoch 125: val_loss did not improve from 29.20090
196/196 - 35s - loss: 29.3263 - MinusLogProbMetric: 29.3263 - val_loss: 29.8567 - val_MinusLogProbMetric: 29.8567 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 126/1000
2023-10-26 20:46:28.042 
Epoch 126/1000 
	 loss: 29.3610, MinusLogProbMetric: 29.3610, val_loss: 29.2698, val_MinusLogProbMetric: 29.2698

Epoch 126: val_loss did not improve from 29.20090
196/196 - 36s - loss: 29.3610 - MinusLogProbMetric: 29.3610 - val_loss: 29.2698 - val_MinusLogProbMetric: 29.2698 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 127/1000
2023-10-26 20:47:03.770 
Epoch 127/1000 
	 loss: 29.3189, MinusLogProbMetric: 29.3189, val_loss: 29.6796, val_MinusLogProbMetric: 29.6796

Epoch 127: val_loss did not improve from 29.20090
196/196 - 36s - loss: 29.3189 - MinusLogProbMetric: 29.3189 - val_loss: 29.6796 - val_MinusLogProbMetric: 29.6796 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 128/1000
2023-10-26 20:47:39.978 
Epoch 128/1000 
	 loss: 29.1909, MinusLogProbMetric: 29.1909, val_loss: 29.6118, val_MinusLogProbMetric: 29.6118

Epoch 128: val_loss did not improve from 29.20090
196/196 - 36s - loss: 29.1909 - MinusLogProbMetric: 29.1909 - val_loss: 29.6118 - val_MinusLogProbMetric: 29.6118 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 129/1000
2023-10-26 20:48:15.622 
Epoch 129/1000 
	 loss: 29.2519, MinusLogProbMetric: 29.2519, val_loss: 29.8077, val_MinusLogProbMetric: 29.8077

Epoch 129: val_loss did not improve from 29.20090
196/196 - 36s - loss: 29.2519 - MinusLogProbMetric: 29.2519 - val_loss: 29.8077 - val_MinusLogProbMetric: 29.8077 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 130/1000
2023-10-26 20:48:51.167 
Epoch 130/1000 
	 loss: 29.3176, MinusLogProbMetric: 29.3176, val_loss: 29.9450, val_MinusLogProbMetric: 29.9450

Epoch 130: val_loss did not improve from 29.20090
196/196 - 36s - loss: 29.3176 - MinusLogProbMetric: 29.3176 - val_loss: 29.9450 - val_MinusLogProbMetric: 29.9450 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 131/1000
2023-10-26 20:49:26.763 
Epoch 131/1000 
	 loss: 29.1513, MinusLogProbMetric: 29.1513, val_loss: 29.1212, val_MinusLogProbMetric: 29.1212

Epoch 131: val_loss improved from 29.20090 to 29.12122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 29.1513 - MinusLogProbMetric: 29.1513 - val_loss: 29.1212 - val_MinusLogProbMetric: 29.1212 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 132/1000
2023-10-26 20:50:02.861 
Epoch 132/1000 
	 loss: 29.3470, MinusLogProbMetric: 29.3470, val_loss: 29.3080, val_MinusLogProbMetric: 29.3080

Epoch 132: val_loss did not improve from 29.12122
196/196 - 35s - loss: 29.3470 - MinusLogProbMetric: 29.3470 - val_loss: 29.3080 - val_MinusLogProbMetric: 29.3080 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 133/1000
2023-10-26 20:50:38.305 
Epoch 133/1000 
	 loss: 29.2340, MinusLogProbMetric: 29.2340, val_loss: 30.0905, val_MinusLogProbMetric: 30.0905

Epoch 133: val_loss did not improve from 29.12122
196/196 - 35s - loss: 29.2340 - MinusLogProbMetric: 29.2340 - val_loss: 30.0905 - val_MinusLogProbMetric: 30.0905 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 134/1000
2023-10-26 20:51:14.214 
Epoch 134/1000 
	 loss: 29.2252, MinusLogProbMetric: 29.2252, val_loss: 29.4955, val_MinusLogProbMetric: 29.4955

Epoch 134: val_loss did not improve from 29.12122
196/196 - 36s - loss: 29.2252 - MinusLogProbMetric: 29.2252 - val_loss: 29.4955 - val_MinusLogProbMetric: 29.4955 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 135/1000
2023-10-26 20:51:50.424 
Epoch 135/1000 
	 loss: 29.1868, MinusLogProbMetric: 29.1868, val_loss: 29.4525, val_MinusLogProbMetric: 29.4525

Epoch 135: val_loss did not improve from 29.12122
196/196 - 36s - loss: 29.1868 - MinusLogProbMetric: 29.1868 - val_loss: 29.4525 - val_MinusLogProbMetric: 29.4525 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 136/1000
2023-10-26 20:52:25.873 
Epoch 136/1000 
	 loss: 29.2233, MinusLogProbMetric: 29.2233, val_loss: 29.0997, val_MinusLogProbMetric: 29.0997

Epoch 136: val_loss improved from 29.12122 to 29.09967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 29.2233 - MinusLogProbMetric: 29.2233 - val_loss: 29.0997 - val_MinusLogProbMetric: 29.0997 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 137/1000
2023-10-26 20:53:02.229 
Epoch 137/1000 
	 loss: 29.1318, MinusLogProbMetric: 29.1318, val_loss: 29.5703, val_MinusLogProbMetric: 29.5703

Epoch 137: val_loss did not improve from 29.09967
196/196 - 36s - loss: 29.1318 - MinusLogProbMetric: 29.1318 - val_loss: 29.5703 - val_MinusLogProbMetric: 29.5703 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 138/1000
2023-10-26 20:53:37.989 
Epoch 138/1000 
	 loss: 29.3409, MinusLogProbMetric: 29.3409, val_loss: 28.9183, val_MinusLogProbMetric: 28.9183

Epoch 138: val_loss improved from 29.09967 to 28.91832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 29.3409 - MinusLogProbMetric: 29.3409 - val_loss: 28.9183 - val_MinusLogProbMetric: 28.9183 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 139/1000
2023-10-26 20:54:14.755 
Epoch 139/1000 
	 loss: 29.1133, MinusLogProbMetric: 29.1133, val_loss: 30.2452, val_MinusLogProbMetric: 30.2452

Epoch 139: val_loss did not improve from 28.91832
196/196 - 36s - loss: 29.1133 - MinusLogProbMetric: 29.1133 - val_loss: 30.2452 - val_MinusLogProbMetric: 30.2452 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 140/1000
2023-10-26 20:54:50.193 
Epoch 140/1000 
	 loss: 29.2538, MinusLogProbMetric: 29.2538, val_loss: 29.6662, val_MinusLogProbMetric: 29.6662

Epoch 140: val_loss did not improve from 28.91832
196/196 - 35s - loss: 29.2538 - MinusLogProbMetric: 29.2538 - val_loss: 29.6662 - val_MinusLogProbMetric: 29.6662 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 141/1000
2023-10-26 20:55:26.376 
Epoch 141/1000 
	 loss: 29.0664, MinusLogProbMetric: 29.0664, val_loss: 29.1615, val_MinusLogProbMetric: 29.1615

Epoch 141: val_loss did not improve from 28.91832
196/196 - 36s - loss: 29.0664 - MinusLogProbMetric: 29.0664 - val_loss: 29.1615 - val_MinusLogProbMetric: 29.1615 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 142/1000
2023-10-26 20:56:01.885 
Epoch 142/1000 
	 loss: 29.1733, MinusLogProbMetric: 29.1733, val_loss: 30.0957, val_MinusLogProbMetric: 30.0957

Epoch 142: val_loss did not improve from 28.91832
196/196 - 36s - loss: 29.1733 - MinusLogProbMetric: 29.1733 - val_loss: 30.0957 - val_MinusLogProbMetric: 30.0957 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 143/1000
2023-10-26 20:56:37.580 
Epoch 143/1000 
	 loss: 29.0290, MinusLogProbMetric: 29.0290, val_loss: 29.9021, val_MinusLogProbMetric: 29.9021

Epoch 143: val_loss did not improve from 28.91832
196/196 - 36s - loss: 29.0290 - MinusLogProbMetric: 29.0290 - val_loss: 29.9021 - val_MinusLogProbMetric: 29.9021 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 144/1000
2023-10-26 20:57:12.855 
Epoch 144/1000 
	 loss: 29.1995, MinusLogProbMetric: 29.1995, val_loss: 28.8292, val_MinusLogProbMetric: 28.8292

Epoch 144: val_loss improved from 28.91832 to 28.82916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 29.1995 - MinusLogProbMetric: 29.1995 - val_loss: 28.8292 - val_MinusLogProbMetric: 28.8292 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 145/1000
2023-10-26 20:57:49.084 
Epoch 145/1000 
	 loss: 29.0672, MinusLogProbMetric: 29.0672, val_loss: 29.2011, val_MinusLogProbMetric: 29.2011

Epoch 145: val_loss did not improve from 28.82916
196/196 - 36s - loss: 29.0672 - MinusLogProbMetric: 29.0672 - val_loss: 29.2011 - val_MinusLogProbMetric: 29.2011 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 146/1000
2023-10-26 20:58:25.028 
Epoch 146/1000 
	 loss: 29.1024, MinusLogProbMetric: 29.1024, val_loss: 29.7763, val_MinusLogProbMetric: 29.7763

Epoch 146: val_loss did not improve from 28.82916
196/196 - 36s - loss: 29.1024 - MinusLogProbMetric: 29.1024 - val_loss: 29.7763 - val_MinusLogProbMetric: 29.7763 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 147/1000
2023-10-26 20:59:01.045 
Epoch 147/1000 
	 loss: 29.1076, MinusLogProbMetric: 29.1076, val_loss: 29.4673, val_MinusLogProbMetric: 29.4673

Epoch 147: val_loss did not improve from 28.82916
196/196 - 36s - loss: 29.1076 - MinusLogProbMetric: 29.1076 - val_loss: 29.4673 - val_MinusLogProbMetric: 29.4673 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 148/1000
2023-10-26 20:59:36.676 
Epoch 148/1000 
	 loss: 29.1355, MinusLogProbMetric: 29.1355, val_loss: 29.7221, val_MinusLogProbMetric: 29.7221

Epoch 148: val_loss did not improve from 28.82916
196/196 - 36s - loss: 29.1355 - MinusLogProbMetric: 29.1355 - val_loss: 29.7221 - val_MinusLogProbMetric: 29.7221 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 149/1000
2023-10-26 21:00:12.023 
Epoch 149/1000 
	 loss: 29.0991, MinusLogProbMetric: 29.0991, val_loss: 29.2784, val_MinusLogProbMetric: 29.2784

Epoch 149: val_loss did not improve from 28.82916
196/196 - 35s - loss: 29.0991 - MinusLogProbMetric: 29.0991 - val_loss: 29.2784 - val_MinusLogProbMetric: 29.2784 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 150/1000
2023-10-26 21:00:47.406 
Epoch 150/1000 
	 loss: 28.9450, MinusLogProbMetric: 28.9450, val_loss: 29.0878, val_MinusLogProbMetric: 29.0878

Epoch 150: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.9450 - MinusLogProbMetric: 28.9450 - val_loss: 29.0878 - val_MinusLogProbMetric: 29.0878 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 151/1000
2023-10-26 21:01:23.076 
Epoch 151/1000 
	 loss: 28.9295, MinusLogProbMetric: 28.9295, val_loss: 29.6020, val_MinusLogProbMetric: 29.6020

Epoch 151: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.9295 - MinusLogProbMetric: 28.9295 - val_loss: 29.6020 - val_MinusLogProbMetric: 29.6020 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 152/1000
2023-10-26 21:01:58.212 
Epoch 152/1000 
	 loss: 29.0101, MinusLogProbMetric: 29.0101, val_loss: 29.2120, val_MinusLogProbMetric: 29.2120

Epoch 152: val_loss did not improve from 28.82916
196/196 - 35s - loss: 29.0101 - MinusLogProbMetric: 29.0101 - val_loss: 29.2120 - val_MinusLogProbMetric: 29.2120 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 153/1000
2023-10-26 21:02:33.724 
Epoch 153/1000 
	 loss: 28.9670, MinusLogProbMetric: 28.9670, val_loss: 30.5820, val_MinusLogProbMetric: 30.5820

Epoch 153: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.9670 - MinusLogProbMetric: 28.9670 - val_loss: 30.5820 - val_MinusLogProbMetric: 30.5820 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 154/1000
2023-10-26 21:03:09.361 
Epoch 154/1000 
	 loss: 29.1521, MinusLogProbMetric: 29.1521, val_loss: 29.1595, val_MinusLogProbMetric: 29.1595

Epoch 154: val_loss did not improve from 28.82916
196/196 - 36s - loss: 29.1521 - MinusLogProbMetric: 29.1521 - val_loss: 29.1595 - val_MinusLogProbMetric: 29.1595 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 155/1000
2023-10-26 21:03:44.897 
Epoch 155/1000 
	 loss: 28.8379, MinusLogProbMetric: 28.8379, val_loss: 30.2191, val_MinusLogProbMetric: 30.2191

Epoch 155: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8379 - MinusLogProbMetric: 28.8379 - val_loss: 30.2191 - val_MinusLogProbMetric: 30.2191 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 156/1000
2023-10-26 21:04:20.194 
Epoch 156/1000 
	 loss: 29.0217, MinusLogProbMetric: 29.0217, val_loss: 29.0442, val_MinusLogProbMetric: 29.0442

Epoch 156: val_loss did not improve from 28.82916
196/196 - 35s - loss: 29.0217 - MinusLogProbMetric: 29.0217 - val_loss: 29.0442 - val_MinusLogProbMetric: 29.0442 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 157/1000
2023-10-26 21:04:55.920 
Epoch 157/1000 
	 loss: 28.8970, MinusLogProbMetric: 28.8970, val_loss: 29.3364, val_MinusLogProbMetric: 29.3364

Epoch 157: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8970 - MinusLogProbMetric: 28.8970 - val_loss: 29.3364 - val_MinusLogProbMetric: 29.3364 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 158/1000
2023-10-26 21:05:31.688 
Epoch 158/1000 
	 loss: 28.9893, MinusLogProbMetric: 28.9893, val_loss: 29.1720, val_MinusLogProbMetric: 29.1720

Epoch 158: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.9893 - MinusLogProbMetric: 28.9893 - val_loss: 29.1720 - val_MinusLogProbMetric: 29.1720 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 159/1000
2023-10-26 21:06:07.306 
Epoch 159/1000 
	 loss: 28.9427, MinusLogProbMetric: 28.9427, val_loss: 29.2065, val_MinusLogProbMetric: 29.2065

Epoch 159: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.9427 - MinusLogProbMetric: 28.9427 - val_loss: 29.2065 - val_MinusLogProbMetric: 29.2065 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 160/1000
2023-10-26 21:06:42.693 
Epoch 160/1000 
	 loss: 28.9518, MinusLogProbMetric: 28.9518, val_loss: 29.1554, val_MinusLogProbMetric: 29.1554

Epoch 160: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.9518 - MinusLogProbMetric: 28.9518 - val_loss: 29.1554 - val_MinusLogProbMetric: 29.1554 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 161/1000
2023-10-26 21:07:18.229 
Epoch 161/1000 
	 loss: 28.8345, MinusLogProbMetric: 28.8345, val_loss: 29.3960, val_MinusLogProbMetric: 29.3960

Epoch 161: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8345 - MinusLogProbMetric: 28.8345 - val_loss: 29.3960 - val_MinusLogProbMetric: 29.3960 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 162/1000
2023-10-26 21:07:53.594 
Epoch 162/1000 
	 loss: 28.9170, MinusLogProbMetric: 28.9170, val_loss: 30.6016, val_MinusLogProbMetric: 30.6016

Epoch 162: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.9170 - MinusLogProbMetric: 28.9170 - val_loss: 30.6016 - val_MinusLogProbMetric: 30.6016 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 163/1000
2023-10-26 21:08:28.992 
Epoch 163/1000 
	 loss: 28.9009, MinusLogProbMetric: 28.9009, val_loss: 29.5025, val_MinusLogProbMetric: 29.5025

Epoch 163: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.9009 - MinusLogProbMetric: 28.9009 - val_loss: 29.5025 - val_MinusLogProbMetric: 29.5025 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 164/1000
2023-10-26 21:09:04.396 
Epoch 164/1000 
	 loss: 28.8577, MinusLogProbMetric: 28.8577, val_loss: 29.1217, val_MinusLogProbMetric: 29.1217

Epoch 164: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.8577 - MinusLogProbMetric: 28.8577 - val_loss: 29.1217 - val_MinusLogProbMetric: 29.1217 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 165/1000
2023-10-26 21:09:39.947 
Epoch 165/1000 
	 loss: 28.9248, MinusLogProbMetric: 28.9248, val_loss: 29.3592, val_MinusLogProbMetric: 29.3592

Epoch 165: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.9248 - MinusLogProbMetric: 28.9248 - val_loss: 29.3592 - val_MinusLogProbMetric: 29.3592 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 166/1000
2023-10-26 21:10:15.354 
Epoch 166/1000 
	 loss: 28.8550, MinusLogProbMetric: 28.8550, val_loss: 29.2446, val_MinusLogProbMetric: 29.2446

Epoch 166: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.8550 - MinusLogProbMetric: 28.8550 - val_loss: 29.2446 - val_MinusLogProbMetric: 29.2446 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 167/1000
2023-10-26 21:10:51.151 
Epoch 167/1000 
	 loss: 28.8140, MinusLogProbMetric: 28.8140, val_loss: 28.9862, val_MinusLogProbMetric: 28.9862

Epoch 167: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8140 - MinusLogProbMetric: 28.8140 - val_loss: 28.9862 - val_MinusLogProbMetric: 28.9862 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 168/1000
2023-10-26 21:11:26.872 
Epoch 168/1000 
	 loss: 28.8353, MinusLogProbMetric: 28.8353, val_loss: 29.4463, val_MinusLogProbMetric: 29.4463

Epoch 168: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8353 - MinusLogProbMetric: 28.8353 - val_loss: 29.4463 - val_MinusLogProbMetric: 29.4463 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 169/1000
2023-10-26 21:12:02.801 
Epoch 169/1000 
	 loss: 28.7855, MinusLogProbMetric: 28.7855, val_loss: 29.7960, val_MinusLogProbMetric: 29.7960

Epoch 169: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7855 - MinusLogProbMetric: 28.7855 - val_loss: 29.7960 - val_MinusLogProbMetric: 29.7960 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 170/1000
2023-10-26 21:12:38.393 
Epoch 170/1000 
	 loss: 28.8373, MinusLogProbMetric: 28.8373, val_loss: 29.4075, val_MinusLogProbMetric: 29.4075

Epoch 170: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8373 - MinusLogProbMetric: 28.8373 - val_loss: 29.4075 - val_MinusLogProbMetric: 29.4075 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 171/1000
2023-10-26 21:13:14.044 
Epoch 171/1000 
	 loss: 28.8708, MinusLogProbMetric: 28.8708, val_loss: 29.3883, val_MinusLogProbMetric: 29.3883

Epoch 171: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8708 - MinusLogProbMetric: 28.8708 - val_loss: 29.3883 - val_MinusLogProbMetric: 29.3883 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 172/1000
2023-10-26 21:13:49.113 
Epoch 172/1000 
	 loss: 28.8510, MinusLogProbMetric: 28.8510, val_loss: 28.9422, val_MinusLogProbMetric: 28.9422

Epoch 172: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.8510 - MinusLogProbMetric: 28.8510 - val_loss: 28.9422 - val_MinusLogProbMetric: 28.9422 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 173/1000
2023-10-26 21:14:24.679 
Epoch 173/1000 
	 loss: 28.7516, MinusLogProbMetric: 28.7516, val_loss: 29.1084, val_MinusLogProbMetric: 29.1084

Epoch 173: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7516 - MinusLogProbMetric: 28.7516 - val_loss: 29.1084 - val_MinusLogProbMetric: 29.1084 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 174/1000
2023-10-26 21:15:00.327 
Epoch 174/1000 
	 loss: 28.8174, MinusLogProbMetric: 28.8174, val_loss: 29.2570, val_MinusLogProbMetric: 29.2570

Epoch 174: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8174 - MinusLogProbMetric: 28.8174 - val_loss: 29.2570 - val_MinusLogProbMetric: 29.2570 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 175/1000
2023-10-26 21:15:35.674 
Epoch 175/1000 
	 loss: 28.7143, MinusLogProbMetric: 28.7143, val_loss: 29.4371, val_MinusLogProbMetric: 29.4371

Epoch 175: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.7143 - MinusLogProbMetric: 28.7143 - val_loss: 29.4371 - val_MinusLogProbMetric: 29.4371 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 176/1000
2023-10-26 21:16:11.474 
Epoch 176/1000 
	 loss: 28.8267, MinusLogProbMetric: 28.8267, val_loss: 29.3397, val_MinusLogProbMetric: 29.3397

Epoch 176: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8267 - MinusLogProbMetric: 28.8267 - val_loss: 29.3397 - val_MinusLogProbMetric: 29.3397 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 177/1000
2023-10-26 21:16:47.143 
Epoch 177/1000 
	 loss: 28.7824, MinusLogProbMetric: 28.7824, val_loss: 29.2550, val_MinusLogProbMetric: 29.2550

Epoch 177: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7824 - MinusLogProbMetric: 28.7824 - val_loss: 29.2550 - val_MinusLogProbMetric: 29.2550 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 178/1000
2023-10-26 21:17:22.505 
Epoch 178/1000 
	 loss: 28.8444, MinusLogProbMetric: 28.8444, val_loss: 28.9520, val_MinusLogProbMetric: 28.9520

Epoch 178: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.8444 - MinusLogProbMetric: 28.8444 - val_loss: 28.9520 - val_MinusLogProbMetric: 28.9520 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 179/1000
2023-10-26 21:17:58.228 
Epoch 179/1000 
	 loss: 28.7320, MinusLogProbMetric: 28.7320, val_loss: 29.0967, val_MinusLogProbMetric: 29.0967

Epoch 179: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7320 - MinusLogProbMetric: 28.7320 - val_loss: 29.0967 - val_MinusLogProbMetric: 29.0967 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 180/1000
2023-10-26 21:18:34.102 
Epoch 180/1000 
	 loss: 28.7752, MinusLogProbMetric: 28.7752, val_loss: 29.0105, val_MinusLogProbMetric: 29.0105

Epoch 180: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7752 - MinusLogProbMetric: 28.7752 - val_loss: 29.0105 - val_MinusLogProbMetric: 29.0105 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 181/1000
2023-10-26 21:19:09.732 
Epoch 181/1000 
	 loss: 28.7495, MinusLogProbMetric: 28.7495, val_loss: 28.9522, val_MinusLogProbMetric: 28.9522

Epoch 181: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7495 - MinusLogProbMetric: 28.7495 - val_loss: 28.9522 - val_MinusLogProbMetric: 28.9522 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 182/1000
2023-10-26 21:19:45.255 
Epoch 182/1000 
	 loss: 28.7571, MinusLogProbMetric: 28.7571, val_loss: 29.3517, val_MinusLogProbMetric: 29.3517

Epoch 182: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7571 - MinusLogProbMetric: 28.7571 - val_loss: 29.3517 - val_MinusLogProbMetric: 29.3517 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 183/1000
2023-10-26 21:20:20.839 
Epoch 183/1000 
	 loss: 28.7638, MinusLogProbMetric: 28.7638, val_loss: 28.8514, val_MinusLogProbMetric: 28.8514

Epoch 183: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7638 - MinusLogProbMetric: 28.7638 - val_loss: 28.8514 - val_MinusLogProbMetric: 28.8514 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 184/1000
2023-10-26 21:20:56.109 
Epoch 184/1000 
	 loss: 28.8718, MinusLogProbMetric: 28.8718, val_loss: 29.4867, val_MinusLogProbMetric: 29.4867

Epoch 184: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.8718 - MinusLogProbMetric: 28.8718 - val_loss: 29.4867 - val_MinusLogProbMetric: 29.4867 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 185/1000
2023-10-26 21:21:32.246 
Epoch 185/1000 
	 loss: 28.6883, MinusLogProbMetric: 28.6883, val_loss: 28.9301, val_MinusLogProbMetric: 28.9301

Epoch 185: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.6883 - MinusLogProbMetric: 28.6883 - val_loss: 28.9301 - val_MinusLogProbMetric: 28.9301 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 186/1000
2023-10-26 21:22:07.580 
Epoch 186/1000 
	 loss: 28.6932, MinusLogProbMetric: 28.6932, val_loss: 29.1116, val_MinusLogProbMetric: 29.1116

Epoch 186: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.6932 - MinusLogProbMetric: 28.6932 - val_loss: 29.1116 - val_MinusLogProbMetric: 29.1116 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 187/1000
2023-10-26 21:22:43.379 
Epoch 187/1000 
	 loss: 28.8145, MinusLogProbMetric: 28.8145, val_loss: 29.7426, val_MinusLogProbMetric: 29.7426

Epoch 187: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.8145 - MinusLogProbMetric: 28.8145 - val_loss: 29.7426 - val_MinusLogProbMetric: 29.7426 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 188/1000
2023-10-26 21:23:18.735 
Epoch 188/1000 
	 loss: 28.7805, MinusLogProbMetric: 28.7805, val_loss: 29.0929, val_MinusLogProbMetric: 29.0929

Epoch 188: val_loss did not improve from 28.82916
196/196 - 35s - loss: 28.7805 - MinusLogProbMetric: 28.7805 - val_loss: 29.0929 - val_MinusLogProbMetric: 29.0929 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 189/1000
2023-10-26 21:23:54.553 
Epoch 189/1000 
	 loss: 28.7787, MinusLogProbMetric: 28.7787, val_loss: 28.8984, val_MinusLogProbMetric: 28.8984

Epoch 189: val_loss did not improve from 28.82916
196/196 - 36s - loss: 28.7787 - MinusLogProbMetric: 28.7787 - val_loss: 28.8984 - val_MinusLogProbMetric: 28.8984 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 190/1000
2023-10-26 21:24:30.334 
Epoch 190/1000 
	 loss: 28.7197, MinusLogProbMetric: 28.7197, val_loss: 28.6845, val_MinusLogProbMetric: 28.6845

Epoch 190: val_loss improved from 28.82916 to 28.68446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 28.7197 - MinusLogProbMetric: 28.7197 - val_loss: 28.6845 - val_MinusLogProbMetric: 28.6845 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 191/1000
2023-10-26 21:25:06.664 
Epoch 191/1000 
	 loss: 28.7876, MinusLogProbMetric: 28.7876, val_loss: 29.2866, val_MinusLogProbMetric: 29.2866

Epoch 191: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.7876 - MinusLogProbMetric: 28.7876 - val_loss: 29.2866 - val_MinusLogProbMetric: 29.2866 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 192/1000
2023-10-26 21:25:42.715 
Epoch 192/1000 
	 loss: 28.5724, MinusLogProbMetric: 28.5724, val_loss: 28.9074, val_MinusLogProbMetric: 28.9074

Epoch 192: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.5724 - MinusLogProbMetric: 28.5724 - val_loss: 28.9074 - val_MinusLogProbMetric: 28.9074 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 193/1000
2023-10-26 21:26:18.673 
Epoch 193/1000 
	 loss: 28.7961, MinusLogProbMetric: 28.7961, val_loss: 29.2362, val_MinusLogProbMetric: 29.2362

Epoch 193: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.7961 - MinusLogProbMetric: 28.7961 - val_loss: 29.2362 - val_MinusLogProbMetric: 29.2362 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 194/1000
2023-10-26 21:26:54.546 
Epoch 194/1000 
	 loss: 28.6839, MinusLogProbMetric: 28.6839, val_loss: 28.9859, val_MinusLogProbMetric: 28.9859

Epoch 194: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.6839 - MinusLogProbMetric: 28.6839 - val_loss: 28.9859 - val_MinusLogProbMetric: 28.9859 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 195/1000
2023-10-26 21:27:30.631 
Epoch 195/1000 
	 loss: 28.7574, MinusLogProbMetric: 28.7574, val_loss: 28.8814, val_MinusLogProbMetric: 28.8814

Epoch 195: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.7574 - MinusLogProbMetric: 28.7574 - val_loss: 28.8814 - val_MinusLogProbMetric: 28.8814 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 196/1000
2023-10-26 21:28:06.367 
Epoch 196/1000 
	 loss: 28.7652, MinusLogProbMetric: 28.7652, val_loss: 28.8822, val_MinusLogProbMetric: 28.8822

Epoch 196: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.7652 - MinusLogProbMetric: 28.7652 - val_loss: 28.8822 - val_MinusLogProbMetric: 28.8822 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 197/1000
2023-10-26 21:28:42.131 
Epoch 197/1000 
	 loss: 28.6614, MinusLogProbMetric: 28.6614, val_loss: 29.6397, val_MinusLogProbMetric: 29.6397

Epoch 197: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.6614 - MinusLogProbMetric: 28.6614 - val_loss: 29.6397 - val_MinusLogProbMetric: 29.6397 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 198/1000
2023-10-26 21:29:17.707 
Epoch 198/1000 
	 loss: 28.5881, MinusLogProbMetric: 28.5881, val_loss: 28.9883, val_MinusLogProbMetric: 28.9883

Epoch 198: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.5881 - MinusLogProbMetric: 28.5881 - val_loss: 28.9883 - val_MinusLogProbMetric: 28.9883 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 199/1000
2023-10-26 21:29:53.181 
Epoch 199/1000 
	 loss: 28.6969, MinusLogProbMetric: 28.6969, val_loss: 28.9532, val_MinusLogProbMetric: 28.9532

Epoch 199: val_loss did not improve from 28.68446
196/196 - 35s - loss: 28.6969 - MinusLogProbMetric: 28.6969 - val_loss: 28.9532 - val_MinusLogProbMetric: 28.9532 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 200/1000
2023-10-26 21:30:28.692 
Epoch 200/1000 
	 loss: 28.6342, MinusLogProbMetric: 28.6342, val_loss: 29.4652, val_MinusLogProbMetric: 29.4652

Epoch 200: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.6342 - MinusLogProbMetric: 28.6342 - val_loss: 29.4652 - val_MinusLogProbMetric: 29.4652 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 201/1000
2023-10-26 21:31:04.116 
Epoch 201/1000 
	 loss: 28.6732, MinusLogProbMetric: 28.6732, val_loss: 28.7070, val_MinusLogProbMetric: 28.7070

Epoch 201: val_loss did not improve from 28.68446
196/196 - 35s - loss: 28.6732 - MinusLogProbMetric: 28.6732 - val_loss: 28.7070 - val_MinusLogProbMetric: 28.7070 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 202/1000
2023-10-26 21:31:40.163 
Epoch 202/1000 
	 loss: 28.6599, MinusLogProbMetric: 28.6599, val_loss: 29.6488, val_MinusLogProbMetric: 29.6488

Epoch 202: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.6599 - MinusLogProbMetric: 28.6599 - val_loss: 29.6488 - val_MinusLogProbMetric: 29.6488 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 203/1000
2023-10-26 21:32:15.789 
Epoch 203/1000 
	 loss: 28.6559, MinusLogProbMetric: 28.6559, val_loss: 29.3198, val_MinusLogProbMetric: 29.3198

Epoch 203: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.6559 - MinusLogProbMetric: 28.6559 - val_loss: 29.3198 - val_MinusLogProbMetric: 29.3198 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 204/1000
2023-10-26 21:32:51.511 
Epoch 204/1000 
	 loss: 28.7041, MinusLogProbMetric: 28.7041, val_loss: 29.0651, val_MinusLogProbMetric: 29.0651

Epoch 204: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.7041 - MinusLogProbMetric: 28.7041 - val_loss: 29.0651 - val_MinusLogProbMetric: 29.0651 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 205/1000
2023-10-26 21:33:27.167 
Epoch 205/1000 
	 loss: 28.5743, MinusLogProbMetric: 28.5743, val_loss: 28.7056, val_MinusLogProbMetric: 28.7056

Epoch 205: val_loss did not improve from 28.68446
196/196 - 36s - loss: 28.5743 - MinusLogProbMetric: 28.5743 - val_loss: 28.7056 - val_MinusLogProbMetric: 28.7056 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 206/1000
2023-10-26 21:34:02.778 
Epoch 206/1000 
	 loss: 28.5109, MinusLogProbMetric: 28.5109, val_loss: 28.6562, val_MinusLogProbMetric: 28.6562

Epoch 206: val_loss improved from 28.68446 to 28.65621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 28.5109 - MinusLogProbMetric: 28.5109 - val_loss: 28.6562 - val_MinusLogProbMetric: 28.6562 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 207/1000
2023-10-26 21:34:38.869 
Epoch 207/1000 
	 loss: 28.5384, MinusLogProbMetric: 28.5384, val_loss: 30.6514, val_MinusLogProbMetric: 30.6514

Epoch 207: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.5384 - MinusLogProbMetric: 28.5384 - val_loss: 30.6514 - val_MinusLogProbMetric: 30.6514 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 208/1000
2023-10-26 21:35:14.891 
Epoch 208/1000 
	 loss: 28.6191, MinusLogProbMetric: 28.6191, val_loss: 29.2661, val_MinusLogProbMetric: 29.2661

Epoch 208: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.6191 - MinusLogProbMetric: 28.6191 - val_loss: 29.2661 - val_MinusLogProbMetric: 29.2661 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 209/1000
2023-10-26 21:35:50.424 
Epoch 209/1000 
	 loss: 28.5646, MinusLogProbMetric: 28.5646, val_loss: 29.3775, val_MinusLogProbMetric: 29.3775

Epoch 209: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.5646 - MinusLogProbMetric: 28.5646 - val_loss: 29.3775 - val_MinusLogProbMetric: 29.3775 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 210/1000
2023-10-26 21:36:25.989 
Epoch 210/1000 
	 loss: 28.5582, MinusLogProbMetric: 28.5582, val_loss: 28.9026, val_MinusLogProbMetric: 28.9026

Epoch 210: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.5582 - MinusLogProbMetric: 28.5582 - val_loss: 28.9026 - val_MinusLogProbMetric: 28.9026 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 211/1000
2023-10-26 21:37:02.005 
Epoch 211/1000 
	 loss: 28.4921, MinusLogProbMetric: 28.4921, val_loss: 29.3808, val_MinusLogProbMetric: 29.3808

Epoch 211: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.4921 - MinusLogProbMetric: 28.4921 - val_loss: 29.3808 - val_MinusLogProbMetric: 29.3808 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 212/1000
2023-10-26 21:37:37.249 
Epoch 212/1000 
	 loss: 28.5938, MinusLogProbMetric: 28.5938, val_loss: 29.0310, val_MinusLogProbMetric: 29.0310

Epoch 212: val_loss did not improve from 28.65621
196/196 - 35s - loss: 28.5938 - MinusLogProbMetric: 28.5938 - val_loss: 29.0310 - val_MinusLogProbMetric: 29.0310 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 213/1000
2023-10-26 21:38:12.819 
Epoch 213/1000 
	 loss: 28.4982, MinusLogProbMetric: 28.4982, val_loss: 28.7031, val_MinusLogProbMetric: 28.7031

Epoch 213: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.4982 - MinusLogProbMetric: 28.4982 - val_loss: 28.7031 - val_MinusLogProbMetric: 28.7031 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 214/1000
2023-10-26 21:38:48.555 
Epoch 214/1000 
	 loss: 28.5509, MinusLogProbMetric: 28.5509, val_loss: 28.7059, val_MinusLogProbMetric: 28.7059

Epoch 214: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.5509 - MinusLogProbMetric: 28.5509 - val_loss: 28.7059 - val_MinusLogProbMetric: 28.7059 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 215/1000
2023-10-26 21:39:24.262 
Epoch 215/1000 
	 loss: 28.5102, MinusLogProbMetric: 28.5102, val_loss: 29.6537, val_MinusLogProbMetric: 29.6537

Epoch 215: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.5102 - MinusLogProbMetric: 28.5102 - val_loss: 29.6537 - val_MinusLogProbMetric: 29.6537 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 216/1000
2023-10-26 21:39:59.841 
Epoch 216/1000 
	 loss: 28.5149, MinusLogProbMetric: 28.5149, val_loss: 29.5700, val_MinusLogProbMetric: 29.5700

Epoch 216: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.5149 - MinusLogProbMetric: 28.5149 - val_loss: 29.5700 - val_MinusLogProbMetric: 29.5700 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 217/1000
2023-10-26 21:40:35.522 
Epoch 217/1000 
	 loss: 28.5149, MinusLogProbMetric: 28.5149, val_loss: 28.9980, val_MinusLogProbMetric: 28.9980

Epoch 217: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.5149 - MinusLogProbMetric: 28.5149 - val_loss: 28.9980 - val_MinusLogProbMetric: 28.9980 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 218/1000
2023-10-26 21:41:11.198 
Epoch 218/1000 
	 loss: 28.4290, MinusLogProbMetric: 28.4290, val_loss: 29.7434, val_MinusLogProbMetric: 29.7434

Epoch 218: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.4290 - MinusLogProbMetric: 28.4290 - val_loss: 29.7434 - val_MinusLogProbMetric: 29.7434 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 219/1000
2023-10-26 21:41:46.480 
Epoch 219/1000 
	 loss: 28.4924, MinusLogProbMetric: 28.4924, val_loss: 29.0484, val_MinusLogProbMetric: 29.0484

Epoch 219: val_loss did not improve from 28.65621
196/196 - 35s - loss: 28.4924 - MinusLogProbMetric: 28.4924 - val_loss: 29.0484 - val_MinusLogProbMetric: 29.0484 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 220/1000
2023-10-26 21:42:22.371 
Epoch 220/1000 
	 loss: 28.4340, MinusLogProbMetric: 28.4340, val_loss: 28.8413, val_MinusLogProbMetric: 28.8413

Epoch 220: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.4340 - MinusLogProbMetric: 28.4340 - val_loss: 28.8413 - val_MinusLogProbMetric: 28.8413 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 221/1000
2023-10-26 21:42:58.283 
Epoch 221/1000 
	 loss: 28.4577, MinusLogProbMetric: 28.4577, val_loss: 28.6937, val_MinusLogProbMetric: 28.6937

Epoch 221: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.4577 - MinusLogProbMetric: 28.4577 - val_loss: 28.6937 - val_MinusLogProbMetric: 28.6937 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 222/1000
2023-10-26 21:43:34.092 
Epoch 222/1000 
	 loss: 28.4270, MinusLogProbMetric: 28.4270, val_loss: 28.8277, val_MinusLogProbMetric: 28.8277

Epoch 222: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.4270 - MinusLogProbMetric: 28.4270 - val_loss: 28.8277 - val_MinusLogProbMetric: 28.8277 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 223/1000
2023-10-26 21:44:10.101 
Epoch 223/1000 
	 loss: 28.4917, MinusLogProbMetric: 28.4917, val_loss: 28.7720, val_MinusLogProbMetric: 28.7720

Epoch 223: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.4917 - MinusLogProbMetric: 28.4917 - val_loss: 28.7720 - val_MinusLogProbMetric: 28.7720 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 224/1000
2023-10-26 21:44:45.884 
Epoch 224/1000 
	 loss: 28.3820, MinusLogProbMetric: 28.3820, val_loss: 29.2591, val_MinusLogProbMetric: 29.2591

Epoch 224: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.3820 - MinusLogProbMetric: 28.3820 - val_loss: 29.2591 - val_MinusLogProbMetric: 29.2591 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 225/1000
2023-10-26 21:45:21.404 
Epoch 225/1000 
	 loss: 28.3692, MinusLogProbMetric: 28.3692, val_loss: 29.0575, val_MinusLogProbMetric: 29.0575

Epoch 225: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.3692 - MinusLogProbMetric: 28.3692 - val_loss: 29.0575 - val_MinusLogProbMetric: 29.0575 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 226/1000
2023-10-26 21:45:57.628 
Epoch 226/1000 
	 loss: 28.3697, MinusLogProbMetric: 28.3697, val_loss: 28.7397, val_MinusLogProbMetric: 28.7397

Epoch 226: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.3697 - MinusLogProbMetric: 28.3697 - val_loss: 28.7397 - val_MinusLogProbMetric: 28.7397 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 227/1000
2023-10-26 21:46:33.456 
Epoch 227/1000 
	 loss: 28.3334, MinusLogProbMetric: 28.3334, val_loss: 28.7646, val_MinusLogProbMetric: 28.7646

Epoch 227: val_loss did not improve from 28.65621
196/196 - 36s - loss: 28.3334 - MinusLogProbMetric: 28.3334 - val_loss: 28.7646 - val_MinusLogProbMetric: 28.7646 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 228/1000
2023-10-26 21:47:09.359 
Epoch 228/1000 
	 loss: 28.4019, MinusLogProbMetric: 28.4019, val_loss: 28.6334, val_MinusLogProbMetric: 28.6334

Epoch 228: val_loss improved from 28.65621 to 28.63344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 28.4019 - MinusLogProbMetric: 28.4019 - val_loss: 28.6334 - val_MinusLogProbMetric: 28.6334 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 229/1000
2023-10-26 21:47:45.481 
Epoch 229/1000 
	 loss: 28.3454, MinusLogProbMetric: 28.3454, val_loss: 28.7571, val_MinusLogProbMetric: 28.7571

Epoch 229: val_loss did not improve from 28.63344
196/196 - 35s - loss: 28.3454 - MinusLogProbMetric: 28.3454 - val_loss: 28.7571 - val_MinusLogProbMetric: 28.7571 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 230/1000
2023-10-26 21:48:21.109 
Epoch 230/1000 
	 loss: 28.3843, MinusLogProbMetric: 28.3843, val_loss: 28.4861, val_MinusLogProbMetric: 28.4861

Epoch 230: val_loss improved from 28.63344 to 28.48609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 28.3843 - MinusLogProbMetric: 28.3843 - val_loss: 28.4861 - val_MinusLogProbMetric: 28.4861 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 231/1000
2023-10-26 21:48:57.436 
Epoch 231/1000 
	 loss: 28.3676, MinusLogProbMetric: 28.3676, val_loss: 28.8598, val_MinusLogProbMetric: 28.8598

Epoch 231: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.3676 - MinusLogProbMetric: 28.3676 - val_loss: 28.8598 - val_MinusLogProbMetric: 28.8598 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 232/1000
2023-10-26 21:49:33.241 
Epoch 232/1000 
	 loss: 28.4516, MinusLogProbMetric: 28.4516, val_loss: 29.1898, val_MinusLogProbMetric: 29.1898

Epoch 232: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.4516 - MinusLogProbMetric: 28.4516 - val_loss: 29.1898 - val_MinusLogProbMetric: 29.1898 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 233/1000
2023-10-26 21:50:08.921 
Epoch 233/1000 
	 loss: 28.3791, MinusLogProbMetric: 28.3791, val_loss: 29.1400, val_MinusLogProbMetric: 29.1400

Epoch 233: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.3791 - MinusLogProbMetric: 28.3791 - val_loss: 29.1400 - val_MinusLogProbMetric: 29.1400 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 234/1000
2023-10-26 21:50:45.087 
Epoch 234/1000 
	 loss: 28.4281, MinusLogProbMetric: 28.4281, val_loss: 28.6265, val_MinusLogProbMetric: 28.6265

Epoch 234: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.4281 - MinusLogProbMetric: 28.4281 - val_loss: 28.6265 - val_MinusLogProbMetric: 28.6265 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 235/1000
2023-10-26 21:51:20.243 
Epoch 235/1000 
	 loss: 28.3607, MinusLogProbMetric: 28.3607, val_loss: 28.7367, val_MinusLogProbMetric: 28.7367

Epoch 235: val_loss did not improve from 28.48609
196/196 - 35s - loss: 28.3607 - MinusLogProbMetric: 28.3607 - val_loss: 28.7367 - val_MinusLogProbMetric: 28.7367 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 236/1000
2023-10-26 21:51:56.490 
Epoch 236/1000 
	 loss: 28.3137, MinusLogProbMetric: 28.3137, val_loss: 28.9016, val_MinusLogProbMetric: 28.9016

Epoch 236: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.3137 - MinusLogProbMetric: 28.3137 - val_loss: 28.9016 - val_MinusLogProbMetric: 28.9016 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 237/1000
2023-10-26 21:52:31.910 
Epoch 237/1000 
	 loss: 28.3203, MinusLogProbMetric: 28.3203, val_loss: 28.8717, val_MinusLogProbMetric: 28.8717

Epoch 237: val_loss did not improve from 28.48609
196/196 - 35s - loss: 28.3203 - MinusLogProbMetric: 28.3203 - val_loss: 28.8717 - val_MinusLogProbMetric: 28.8717 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 238/1000
2023-10-26 21:53:07.610 
Epoch 238/1000 
	 loss: 28.2881, MinusLogProbMetric: 28.2881, val_loss: 29.0368, val_MinusLogProbMetric: 29.0368

Epoch 238: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.2881 - MinusLogProbMetric: 28.2881 - val_loss: 29.0368 - val_MinusLogProbMetric: 29.0368 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 239/1000
2023-10-26 21:53:43.325 
Epoch 239/1000 
	 loss: 28.4438, MinusLogProbMetric: 28.4438, val_loss: 28.8811, val_MinusLogProbMetric: 28.8811

Epoch 239: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.4438 - MinusLogProbMetric: 28.4438 - val_loss: 28.8811 - val_MinusLogProbMetric: 28.8811 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 240/1000
2023-10-26 21:54:18.598 
Epoch 240/1000 
	 loss: 28.3522, MinusLogProbMetric: 28.3522, val_loss: 28.9151, val_MinusLogProbMetric: 28.9151

Epoch 240: val_loss did not improve from 28.48609
196/196 - 35s - loss: 28.3522 - MinusLogProbMetric: 28.3522 - val_loss: 28.9151 - val_MinusLogProbMetric: 28.9151 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 241/1000
2023-10-26 21:54:54.103 
Epoch 241/1000 
	 loss: 28.2825, MinusLogProbMetric: 28.2825, val_loss: 29.6896, val_MinusLogProbMetric: 29.6896

Epoch 241: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.2825 - MinusLogProbMetric: 28.2825 - val_loss: 29.6896 - val_MinusLogProbMetric: 29.6896 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 242/1000
2023-10-26 21:55:29.715 
Epoch 242/1000 
	 loss: 28.3443, MinusLogProbMetric: 28.3443, val_loss: 28.8539, val_MinusLogProbMetric: 28.8539

Epoch 242: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.3443 - MinusLogProbMetric: 28.3443 - val_loss: 28.8539 - val_MinusLogProbMetric: 28.8539 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 243/1000
2023-10-26 21:56:05.593 
Epoch 243/1000 
	 loss: 28.3225, MinusLogProbMetric: 28.3225, val_loss: 29.3062, val_MinusLogProbMetric: 29.3062

Epoch 243: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.3225 - MinusLogProbMetric: 28.3225 - val_loss: 29.3062 - val_MinusLogProbMetric: 29.3062 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 244/1000
2023-10-26 21:56:41.132 
Epoch 244/1000 
	 loss: 28.3298, MinusLogProbMetric: 28.3298, val_loss: 29.0351, val_MinusLogProbMetric: 29.0351

Epoch 244: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.3298 - MinusLogProbMetric: 28.3298 - val_loss: 29.0351 - val_MinusLogProbMetric: 29.0351 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 245/1000
2023-10-26 21:57:16.755 
Epoch 245/1000 
	 loss: 28.3231, MinusLogProbMetric: 28.3231, val_loss: 28.6768, val_MinusLogProbMetric: 28.6768

Epoch 245: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.3231 - MinusLogProbMetric: 28.3231 - val_loss: 28.6768 - val_MinusLogProbMetric: 28.6768 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 246/1000
2023-10-26 21:57:49.031 
Epoch 246/1000 
	 loss: 28.3821, MinusLogProbMetric: 28.3821, val_loss: 28.7621, val_MinusLogProbMetric: 28.7621

Epoch 246: val_loss did not improve from 28.48609
196/196 - 32s - loss: 28.3821 - MinusLogProbMetric: 28.3821 - val_loss: 28.7621 - val_MinusLogProbMetric: 28.7621 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 247/1000
2023-10-26 21:58:17.801 
Epoch 247/1000 
	 loss: 28.2771, MinusLogProbMetric: 28.2771, val_loss: 29.1073, val_MinusLogProbMetric: 29.1073

Epoch 247: val_loss did not improve from 28.48609
196/196 - 29s - loss: 28.2771 - MinusLogProbMetric: 28.2771 - val_loss: 29.1073 - val_MinusLogProbMetric: 29.1073 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 248/1000
2023-10-26 21:58:46.243 
Epoch 248/1000 
	 loss: 28.3647, MinusLogProbMetric: 28.3647, val_loss: 28.7819, val_MinusLogProbMetric: 28.7819

Epoch 248: val_loss did not improve from 28.48609
196/196 - 28s - loss: 28.3647 - MinusLogProbMetric: 28.3647 - val_loss: 28.7819 - val_MinusLogProbMetric: 28.7819 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 249/1000
2023-10-26 21:59:17.798 
Epoch 249/1000 
	 loss: 28.2863, MinusLogProbMetric: 28.2863, val_loss: 29.0316, val_MinusLogProbMetric: 29.0316

Epoch 249: val_loss did not improve from 28.48609
196/196 - 32s - loss: 28.2863 - MinusLogProbMetric: 28.2863 - val_loss: 29.0316 - val_MinusLogProbMetric: 29.0316 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 250/1000
2023-10-26 21:59:53.505 
Epoch 250/1000 
	 loss: 28.2640, MinusLogProbMetric: 28.2640, val_loss: 29.1924, val_MinusLogProbMetric: 29.1924

Epoch 250: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.2640 - MinusLogProbMetric: 28.2640 - val_loss: 29.1924 - val_MinusLogProbMetric: 29.1924 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 251/1000
2023-10-26 22:00:29.408 
Epoch 251/1000 
	 loss: 28.3470, MinusLogProbMetric: 28.3470, val_loss: 29.0283, val_MinusLogProbMetric: 29.0283

Epoch 251: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.3470 - MinusLogProbMetric: 28.3470 - val_loss: 29.0283 - val_MinusLogProbMetric: 29.0283 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 252/1000
2023-10-26 22:01:03.552 
Epoch 252/1000 
	 loss: 28.1622, MinusLogProbMetric: 28.1622, val_loss: 28.6126, val_MinusLogProbMetric: 28.6126

Epoch 252: val_loss did not improve from 28.48609
196/196 - 34s - loss: 28.1622 - MinusLogProbMetric: 28.1622 - val_loss: 28.6126 - val_MinusLogProbMetric: 28.6126 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 253/1000
2023-10-26 22:01:39.381 
Epoch 253/1000 
	 loss: 28.2492, MinusLogProbMetric: 28.2492, val_loss: 28.5814, val_MinusLogProbMetric: 28.5814

Epoch 253: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.2492 - MinusLogProbMetric: 28.2492 - val_loss: 28.5814 - val_MinusLogProbMetric: 28.5814 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 254/1000
2023-10-26 22:02:15.695 
Epoch 254/1000 
	 loss: 28.2140, MinusLogProbMetric: 28.2140, val_loss: 28.7498, val_MinusLogProbMetric: 28.7498

Epoch 254: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.2140 - MinusLogProbMetric: 28.2140 - val_loss: 28.7498 - val_MinusLogProbMetric: 28.7498 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 255/1000
2023-10-26 22:02:51.913 
Epoch 255/1000 
	 loss: 28.2572, MinusLogProbMetric: 28.2572, val_loss: 28.5745, val_MinusLogProbMetric: 28.5745

Epoch 255: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.2572 - MinusLogProbMetric: 28.2572 - val_loss: 28.5745 - val_MinusLogProbMetric: 28.5745 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 256/1000
2023-10-26 22:03:27.737 
Epoch 256/1000 
	 loss: 28.2051, MinusLogProbMetric: 28.2051, val_loss: 28.6802, val_MinusLogProbMetric: 28.6802

Epoch 256: val_loss did not improve from 28.48609
196/196 - 36s - loss: 28.2051 - MinusLogProbMetric: 28.2051 - val_loss: 28.6802 - val_MinusLogProbMetric: 28.6802 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 257/1000
2023-10-26 22:04:03.509 
Epoch 257/1000 
	 loss: 28.2221, MinusLogProbMetric: 28.2221, val_loss: 28.4600, val_MinusLogProbMetric: 28.4600

Epoch 257: val_loss improved from 28.48609 to 28.45996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 28.2221 - MinusLogProbMetric: 28.2221 - val_loss: 28.4600 - val_MinusLogProbMetric: 28.4600 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 258/1000
2023-10-26 22:04:40.075 
Epoch 258/1000 
	 loss: 28.2157, MinusLogProbMetric: 28.2157, val_loss: 29.2599, val_MinusLogProbMetric: 29.2599

Epoch 258: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.2157 - MinusLogProbMetric: 28.2157 - val_loss: 29.2599 - val_MinusLogProbMetric: 29.2599 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 259/1000
2023-10-26 22:05:16.031 
Epoch 259/1000 
	 loss: 28.2355, MinusLogProbMetric: 28.2355, val_loss: 28.6311, val_MinusLogProbMetric: 28.6311

Epoch 259: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.2355 - MinusLogProbMetric: 28.2355 - val_loss: 28.6311 - val_MinusLogProbMetric: 28.6311 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 260/1000
2023-10-26 22:05:51.603 
Epoch 260/1000 
	 loss: 28.1512, MinusLogProbMetric: 28.1512, val_loss: 28.5283, val_MinusLogProbMetric: 28.5283

Epoch 260: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1512 - MinusLogProbMetric: 28.1512 - val_loss: 28.5283 - val_MinusLogProbMetric: 28.5283 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 261/1000
2023-10-26 22:06:27.400 
Epoch 261/1000 
	 loss: 28.1674, MinusLogProbMetric: 28.1674, val_loss: 29.0617, val_MinusLogProbMetric: 29.0617

Epoch 261: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1674 - MinusLogProbMetric: 28.1674 - val_loss: 29.0617 - val_MinusLogProbMetric: 29.0617 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 262/1000
2023-10-26 22:07:03.567 
Epoch 262/1000 
	 loss: 28.2504, MinusLogProbMetric: 28.2504, val_loss: 28.5669, val_MinusLogProbMetric: 28.5669

Epoch 262: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.2504 - MinusLogProbMetric: 28.2504 - val_loss: 28.5669 - val_MinusLogProbMetric: 28.5669 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 263/1000
2023-10-26 22:07:39.323 
Epoch 263/1000 
	 loss: 28.1640, MinusLogProbMetric: 28.1640, val_loss: 28.8159, val_MinusLogProbMetric: 28.8159

Epoch 263: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1640 - MinusLogProbMetric: 28.1640 - val_loss: 28.8159 - val_MinusLogProbMetric: 28.8159 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 264/1000
2023-10-26 22:08:15.313 
Epoch 264/1000 
	 loss: 28.2144, MinusLogProbMetric: 28.2144, val_loss: 28.7537, val_MinusLogProbMetric: 28.7537

Epoch 264: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.2144 - MinusLogProbMetric: 28.2144 - val_loss: 28.7537 - val_MinusLogProbMetric: 28.7537 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 265/1000
2023-10-26 22:08:50.776 
Epoch 265/1000 
	 loss: 28.1795, MinusLogProbMetric: 28.1795, val_loss: 29.0117, val_MinusLogProbMetric: 29.0117

Epoch 265: val_loss did not improve from 28.45996
196/196 - 35s - loss: 28.1795 - MinusLogProbMetric: 28.1795 - val_loss: 29.0117 - val_MinusLogProbMetric: 29.0117 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 266/1000
2023-10-26 22:09:26.318 
Epoch 266/1000 
	 loss: 28.1715, MinusLogProbMetric: 28.1715, val_loss: 28.9255, val_MinusLogProbMetric: 28.9255

Epoch 266: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1715 - MinusLogProbMetric: 28.1715 - val_loss: 28.9255 - val_MinusLogProbMetric: 28.9255 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 267/1000
2023-10-26 22:10:01.691 
Epoch 267/1000 
	 loss: 28.2078, MinusLogProbMetric: 28.2078, val_loss: 28.6079, val_MinusLogProbMetric: 28.6079

Epoch 267: val_loss did not improve from 28.45996
196/196 - 35s - loss: 28.2078 - MinusLogProbMetric: 28.2078 - val_loss: 28.6079 - val_MinusLogProbMetric: 28.6079 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 268/1000
2023-10-26 22:10:37.272 
Epoch 268/1000 
	 loss: 28.1144, MinusLogProbMetric: 28.1144, val_loss: 28.6899, val_MinusLogProbMetric: 28.6899

Epoch 268: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1144 - MinusLogProbMetric: 28.1144 - val_loss: 28.6899 - val_MinusLogProbMetric: 28.6899 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 269/1000
2023-10-26 22:11:13.133 
Epoch 269/1000 
	 loss: 28.2592, MinusLogProbMetric: 28.2592, val_loss: 28.8121, val_MinusLogProbMetric: 28.8121

Epoch 269: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.2592 - MinusLogProbMetric: 28.2592 - val_loss: 28.8121 - val_MinusLogProbMetric: 28.8121 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 270/1000
2023-10-26 22:11:48.970 
Epoch 270/1000 
	 loss: 28.1746, MinusLogProbMetric: 28.1746, val_loss: 28.4907, val_MinusLogProbMetric: 28.4907

Epoch 270: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1746 - MinusLogProbMetric: 28.1746 - val_loss: 28.4907 - val_MinusLogProbMetric: 28.4907 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 271/1000
2023-10-26 22:12:24.522 
Epoch 271/1000 
	 loss: 28.2160, MinusLogProbMetric: 28.2160, val_loss: 28.7678, val_MinusLogProbMetric: 28.7678

Epoch 271: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.2160 - MinusLogProbMetric: 28.2160 - val_loss: 28.7678 - val_MinusLogProbMetric: 28.7678 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 272/1000
2023-10-26 22:13:00.248 
Epoch 272/1000 
	 loss: 28.0992, MinusLogProbMetric: 28.0992, val_loss: 28.6108, val_MinusLogProbMetric: 28.6108

Epoch 272: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0992 - MinusLogProbMetric: 28.0992 - val_loss: 28.6108 - val_MinusLogProbMetric: 28.6108 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 273/1000
2023-10-26 22:13:36.313 
Epoch 273/1000 
	 loss: 28.0927, MinusLogProbMetric: 28.0927, val_loss: 28.5863, val_MinusLogProbMetric: 28.5863

Epoch 273: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0927 - MinusLogProbMetric: 28.0927 - val_loss: 28.5863 - val_MinusLogProbMetric: 28.5863 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 274/1000
2023-10-26 22:14:12.221 
Epoch 274/1000 
	 loss: 28.2446, MinusLogProbMetric: 28.2446, val_loss: 28.5638, val_MinusLogProbMetric: 28.5638

Epoch 274: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.2446 - MinusLogProbMetric: 28.2446 - val_loss: 28.5638 - val_MinusLogProbMetric: 28.5638 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 275/1000
2023-10-26 22:14:48.055 
Epoch 275/1000 
	 loss: 28.1374, MinusLogProbMetric: 28.1374, val_loss: 28.9434, val_MinusLogProbMetric: 28.9434

Epoch 275: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1374 - MinusLogProbMetric: 28.1374 - val_loss: 28.9434 - val_MinusLogProbMetric: 28.9434 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 276/1000
2023-10-26 22:15:23.739 
Epoch 276/1000 
	 loss: 28.1193, MinusLogProbMetric: 28.1193, val_loss: 28.5850, val_MinusLogProbMetric: 28.5850

Epoch 276: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1193 - MinusLogProbMetric: 28.1193 - val_loss: 28.5850 - val_MinusLogProbMetric: 28.5850 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 277/1000
2023-10-26 22:15:59.367 
Epoch 277/1000 
	 loss: 28.1004, MinusLogProbMetric: 28.1004, val_loss: 28.6804, val_MinusLogProbMetric: 28.6804

Epoch 277: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1004 - MinusLogProbMetric: 28.1004 - val_loss: 28.6804 - val_MinusLogProbMetric: 28.6804 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 278/1000
2023-10-26 22:16:34.862 
Epoch 278/1000 
	 loss: 28.1648, MinusLogProbMetric: 28.1648, val_loss: 29.0572, val_MinusLogProbMetric: 29.0572

Epoch 278: val_loss did not improve from 28.45996
196/196 - 35s - loss: 28.1648 - MinusLogProbMetric: 28.1648 - val_loss: 29.0572 - val_MinusLogProbMetric: 29.0572 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 279/1000
2023-10-26 22:17:10.177 
Epoch 279/1000 
	 loss: 28.0884, MinusLogProbMetric: 28.0884, val_loss: 28.8906, val_MinusLogProbMetric: 28.8906

Epoch 279: val_loss did not improve from 28.45996
196/196 - 35s - loss: 28.0884 - MinusLogProbMetric: 28.0884 - val_loss: 28.8906 - val_MinusLogProbMetric: 28.8906 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 280/1000
2023-10-26 22:17:46.147 
Epoch 280/1000 
	 loss: 28.1754, MinusLogProbMetric: 28.1754, val_loss: 28.6628, val_MinusLogProbMetric: 28.6628

Epoch 280: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1754 - MinusLogProbMetric: 28.1754 - val_loss: 28.6628 - val_MinusLogProbMetric: 28.6628 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 281/1000
2023-10-26 22:18:22.100 
Epoch 281/1000 
	 loss: 28.0850, MinusLogProbMetric: 28.0850, val_loss: 28.8799, val_MinusLogProbMetric: 28.8799

Epoch 281: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0850 - MinusLogProbMetric: 28.0850 - val_loss: 28.8799 - val_MinusLogProbMetric: 28.8799 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 282/1000
2023-10-26 22:18:57.515 
Epoch 282/1000 
	 loss: 28.1332, MinusLogProbMetric: 28.1332, val_loss: 29.1061, val_MinusLogProbMetric: 29.1061

Epoch 282: val_loss did not improve from 28.45996
196/196 - 35s - loss: 28.1332 - MinusLogProbMetric: 28.1332 - val_loss: 29.1061 - val_MinusLogProbMetric: 29.1061 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 283/1000
2023-10-26 22:19:33.505 
Epoch 283/1000 
	 loss: 28.1308, MinusLogProbMetric: 28.1308, val_loss: 28.4962, val_MinusLogProbMetric: 28.4962

Epoch 283: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1308 - MinusLogProbMetric: 28.1308 - val_loss: 28.4962 - val_MinusLogProbMetric: 28.4962 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 284/1000
2023-10-26 22:20:09.829 
Epoch 284/1000 
	 loss: 27.9997, MinusLogProbMetric: 27.9997, val_loss: 28.7012, val_MinusLogProbMetric: 28.7012

Epoch 284: val_loss did not improve from 28.45996
196/196 - 36s - loss: 27.9997 - MinusLogProbMetric: 27.9997 - val_loss: 28.7012 - val_MinusLogProbMetric: 28.7012 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 285/1000
2023-10-26 22:20:45.764 
Epoch 285/1000 
	 loss: 28.1141, MinusLogProbMetric: 28.1141, val_loss: 29.9463, val_MinusLogProbMetric: 29.9463

Epoch 285: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1141 - MinusLogProbMetric: 28.1141 - val_loss: 29.9463 - val_MinusLogProbMetric: 29.9463 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 286/1000
2023-10-26 22:21:21.591 
Epoch 286/1000 
	 loss: 28.1406, MinusLogProbMetric: 28.1406, val_loss: 28.6371, val_MinusLogProbMetric: 28.6371

Epoch 286: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1406 - MinusLogProbMetric: 28.1406 - val_loss: 28.6371 - val_MinusLogProbMetric: 28.6371 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 287/1000
2023-10-26 22:21:57.129 
Epoch 287/1000 
	 loss: 28.0599, MinusLogProbMetric: 28.0599, val_loss: 28.6110, val_MinusLogProbMetric: 28.6110

Epoch 287: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0599 - MinusLogProbMetric: 28.0599 - val_loss: 28.6110 - val_MinusLogProbMetric: 28.6110 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 288/1000
2023-10-26 22:22:32.417 
Epoch 288/1000 
	 loss: 28.1255, MinusLogProbMetric: 28.1255, val_loss: 28.6177, val_MinusLogProbMetric: 28.6177

Epoch 288: val_loss did not improve from 28.45996
196/196 - 35s - loss: 28.1255 - MinusLogProbMetric: 28.1255 - val_loss: 28.6177 - val_MinusLogProbMetric: 28.6177 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 289/1000
2023-10-26 22:23:08.280 
Epoch 289/1000 
	 loss: 28.1148, MinusLogProbMetric: 28.1148, val_loss: 29.0508, val_MinusLogProbMetric: 29.0508

Epoch 289: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1148 - MinusLogProbMetric: 28.1148 - val_loss: 29.0508 - val_MinusLogProbMetric: 29.0508 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 290/1000
2023-10-26 22:23:44.483 
Epoch 290/1000 
	 loss: 28.1031, MinusLogProbMetric: 28.1031, val_loss: 28.5861, val_MinusLogProbMetric: 28.5861

Epoch 290: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.1031 - MinusLogProbMetric: 28.1031 - val_loss: 28.5861 - val_MinusLogProbMetric: 28.5861 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 291/1000
2023-10-26 22:24:20.435 
Epoch 291/1000 
	 loss: 28.0470, MinusLogProbMetric: 28.0470, val_loss: 28.8276, val_MinusLogProbMetric: 28.8276

Epoch 291: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0470 - MinusLogProbMetric: 28.0470 - val_loss: 28.8276 - val_MinusLogProbMetric: 28.8276 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 292/1000
2023-10-26 22:24:56.592 
Epoch 292/1000 
	 loss: 28.0710, MinusLogProbMetric: 28.0710, val_loss: 28.8165, val_MinusLogProbMetric: 28.8165

Epoch 292: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0710 - MinusLogProbMetric: 28.0710 - val_loss: 28.8165 - val_MinusLogProbMetric: 28.8165 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 293/1000
2023-10-26 22:25:32.150 
Epoch 293/1000 
	 loss: 28.0587, MinusLogProbMetric: 28.0587, val_loss: 28.9370, val_MinusLogProbMetric: 28.9370

Epoch 293: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0587 - MinusLogProbMetric: 28.0587 - val_loss: 28.9370 - val_MinusLogProbMetric: 28.9370 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 294/1000
2023-10-26 22:26:07.995 
Epoch 294/1000 
	 loss: 28.0718, MinusLogProbMetric: 28.0718, val_loss: 28.6331, val_MinusLogProbMetric: 28.6331

Epoch 294: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0718 - MinusLogProbMetric: 28.0718 - val_loss: 28.6331 - val_MinusLogProbMetric: 28.6331 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 295/1000
2023-10-26 22:26:43.862 
Epoch 295/1000 
	 loss: 28.0846, MinusLogProbMetric: 28.0846, val_loss: 28.8524, val_MinusLogProbMetric: 28.8524

Epoch 295: val_loss did not improve from 28.45996
196/196 - 36s - loss: 28.0846 - MinusLogProbMetric: 28.0846 - val_loss: 28.8524 - val_MinusLogProbMetric: 28.8524 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 296/1000
2023-10-26 22:27:19.853 
Epoch 296/1000 
	 loss: 28.0248, MinusLogProbMetric: 28.0248, val_loss: 28.4148, val_MinusLogProbMetric: 28.4148

Epoch 296: val_loss improved from 28.45996 to 28.41477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 28.0248 - MinusLogProbMetric: 28.0248 - val_loss: 28.4148 - val_MinusLogProbMetric: 28.4148 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 297/1000
2023-10-26 22:27:55.870 
Epoch 297/1000 
	 loss: 28.0341, MinusLogProbMetric: 28.0341, val_loss: 28.7253, val_MinusLogProbMetric: 28.7253

Epoch 297: val_loss did not improve from 28.41477
196/196 - 35s - loss: 28.0341 - MinusLogProbMetric: 28.0341 - val_loss: 28.7253 - val_MinusLogProbMetric: 28.7253 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 298/1000
2023-10-26 22:28:30.826 
Epoch 298/1000 
	 loss: 28.0814, MinusLogProbMetric: 28.0814, val_loss: 28.6544, val_MinusLogProbMetric: 28.6544

Epoch 298: val_loss did not improve from 28.41477
196/196 - 35s - loss: 28.0814 - MinusLogProbMetric: 28.0814 - val_loss: 28.6544 - val_MinusLogProbMetric: 28.6544 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 299/1000
2023-10-26 22:29:06.807 
Epoch 299/1000 
	 loss: 28.0821, MinusLogProbMetric: 28.0821, val_loss: 28.7026, val_MinusLogProbMetric: 28.7026

Epoch 299: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0821 - MinusLogProbMetric: 28.0821 - val_loss: 28.7026 - val_MinusLogProbMetric: 28.7026 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 300/1000
2023-10-26 22:29:42.537 
Epoch 300/1000 
	 loss: 28.0571, MinusLogProbMetric: 28.0571, val_loss: 28.7446, val_MinusLogProbMetric: 28.7446

Epoch 300: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0571 - MinusLogProbMetric: 28.0571 - val_loss: 28.7446 - val_MinusLogProbMetric: 28.7446 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 301/1000
2023-10-26 22:30:18.683 
Epoch 301/1000 
	 loss: 28.0505, MinusLogProbMetric: 28.0505, val_loss: 28.5779, val_MinusLogProbMetric: 28.5779

Epoch 301: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0505 - MinusLogProbMetric: 28.0505 - val_loss: 28.5779 - val_MinusLogProbMetric: 28.5779 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 302/1000
2023-10-26 22:30:54.527 
Epoch 302/1000 
	 loss: 27.9988, MinusLogProbMetric: 27.9988, val_loss: 28.5019, val_MinusLogProbMetric: 28.5019

Epoch 302: val_loss did not improve from 28.41477
196/196 - 36s - loss: 27.9988 - MinusLogProbMetric: 27.9988 - val_loss: 28.5019 - val_MinusLogProbMetric: 28.5019 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 303/1000
2023-10-26 22:31:30.654 
Epoch 303/1000 
	 loss: 28.0030, MinusLogProbMetric: 28.0030, val_loss: 28.7232, val_MinusLogProbMetric: 28.7232

Epoch 303: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0030 - MinusLogProbMetric: 28.0030 - val_loss: 28.7232 - val_MinusLogProbMetric: 28.7232 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 304/1000
2023-10-26 22:32:06.405 
Epoch 304/1000 
	 loss: 28.0150, MinusLogProbMetric: 28.0150, val_loss: 28.7679, val_MinusLogProbMetric: 28.7679

Epoch 304: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0150 - MinusLogProbMetric: 28.0150 - val_loss: 28.7679 - val_MinusLogProbMetric: 28.7679 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 305/1000
2023-10-26 22:32:41.898 
Epoch 305/1000 
	 loss: 28.0685, MinusLogProbMetric: 28.0685, val_loss: 28.5914, val_MinusLogProbMetric: 28.5914

Epoch 305: val_loss did not improve from 28.41477
196/196 - 35s - loss: 28.0685 - MinusLogProbMetric: 28.0685 - val_loss: 28.5914 - val_MinusLogProbMetric: 28.5914 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 306/1000
2023-10-26 22:33:17.631 
Epoch 306/1000 
	 loss: 28.0320, MinusLogProbMetric: 28.0320, val_loss: 28.5945, val_MinusLogProbMetric: 28.5945

Epoch 306: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0320 - MinusLogProbMetric: 28.0320 - val_loss: 28.5945 - val_MinusLogProbMetric: 28.5945 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 307/1000
2023-10-26 22:33:52.871 
Epoch 307/1000 
	 loss: 27.9318, MinusLogProbMetric: 27.9318, val_loss: 28.8727, val_MinusLogProbMetric: 28.8727

Epoch 307: val_loss did not improve from 28.41477
196/196 - 35s - loss: 27.9318 - MinusLogProbMetric: 27.9318 - val_loss: 28.8727 - val_MinusLogProbMetric: 28.8727 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 308/1000
2023-10-26 22:34:28.415 
Epoch 308/1000 
	 loss: 28.0443, MinusLogProbMetric: 28.0443, val_loss: 28.5176, val_MinusLogProbMetric: 28.5176

Epoch 308: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0443 - MinusLogProbMetric: 28.0443 - val_loss: 28.5176 - val_MinusLogProbMetric: 28.5176 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 309/1000
2023-10-26 22:35:04.228 
Epoch 309/1000 
	 loss: 28.0155, MinusLogProbMetric: 28.0155, val_loss: 28.5505, val_MinusLogProbMetric: 28.5505

Epoch 309: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0155 - MinusLogProbMetric: 28.0155 - val_loss: 28.5505 - val_MinusLogProbMetric: 28.5505 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 310/1000
2023-10-26 22:35:40.088 
Epoch 310/1000 
	 loss: 28.0433, MinusLogProbMetric: 28.0433, val_loss: 29.2966, val_MinusLogProbMetric: 29.2966

Epoch 310: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0433 - MinusLogProbMetric: 28.0433 - val_loss: 29.2966 - val_MinusLogProbMetric: 29.2966 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 311/1000
2023-10-26 22:36:15.609 
Epoch 311/1000 
	 loss: 27.9875, MinusLogProbMetric: 27.9875, val_loss: 28.6821, val_MinusLogProbMetric: 28.6821

Epoch 311: val_loss did not improve from 28.41477
196/196 - 36s - loss: 27.9875 - MinusLogProbMetric: 27.9875 - val_loss: 28.6821 - val_MinusLogProbMetric: 28.6821 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 312/1000
2023-10-26 22:36:51.666 
Epoch 312/1000 
	 loss: 28.0084, MinusLogProbMetric: 28.0084, val_loss: 28.9116, val_MinusLogProbMetric: 28.9116

Epoch 312: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0084 - MinusLogProbMetric: 28.0084 - val_loss: 28.9116 - val_MinusLogProbMetric: 28.9116 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 313/1000
2023-10-26 22:37:27.830 
Epoch 313/1000 
	 loss: 28.0255, MinusLogProbMetric: 28.0255, val_loss: 28.9475, val_MinusLogProbMetric: 28.9475

Epoch 313: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0255 - MinusLogProbMetric: 28.0255 - val_loss: 28.9475 - val_MinusLogProbMetric: 28.9475 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 314/1000
2023-10-26 22:38:03.259 
Epoch 314/1000 
	 loss: 28.0455, MinusLogProbMetric: 28.0455, val_loss: 28.8766, val_MinusLogProbMetric: 28.8766

Epoch 314: val_loss did not improve from 28.41477
196/196 - 35s - loss: 28.0455 - MinusLogProbMetric: 28.0455 - val_loss: 28.8766 - val_MinusLogProbMetric: 28.8766 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 315/1000
2023-10-26 22:38:38.864 
Epoch 315/1000 
	 loss: 27.9580, MinusLogProbMetric: 27.9580, val_loss: 28.7239, val_MinusLogProbMetric: 28.7239

Epoch 315: val_loss did not improve from 28.41477
196/196 - 36s - loss: 27.9580 - MinusLogProbMetric: 27.9580 - val_loss: 28.7239 - val_MinusLogProbMetric: 28.7239 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 316/1000
2023-10-26 22:39:14.300 
Epoch 316/1000 
	 loss: 27.9957, MinusLogProbMetric: 27.9957, val_loss: 28.7739, val_MinusLogProbMetric: 28.7739

Epoch 316: val_loss did not improve from 28.41477
196/196 - 35s - loss: 27.9957 - MinusLogProbMetric: 27.9957 - val_loss: 28.7739 - val_MinusLogProbMetric: 28.7739 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 317/1000
2023-10-26 22:39:49.914 
Epoch 317/1000 
	 loss: 27.9327, MinusLogProbMetric: 27.9327, val_loss: 28.4295, val_MinusLogProbMetric: 28.4295

Epoch 317: val_loss did not improve from 28.41477
196/196 - 36s - loss: 27.9327 - MinusLogProbMetric: 27.9327 - val_loss: 28.4295 - val_MinusLogProbMetric: 28.4295 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 318/1000
2023-10-26 22:40:25.476 
Epoch 318/1000 
	 loss: 27.9276, MinusLogProbMetric: 27.9276, val_loss: 28.9935, val_MinusLogProbMetric: 28.9935

Epoch 318: val_loss did not improve from 28.41477
196/196 - 36s - loss: 27.9276 - MinusLogProbMetric: 27.9276 - val_loss: 28.9935 - val_MinusLogProbMetric: 28.9935 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 319/1000
2023-10-26 22:41:01.172 
Epoch 319/1000 
	 loss: 28.0043, MinusLogProbMetric: 28.0043, val_loss: 28.6443, val_MinusLogProbMetric: 28.6443

Epoch 319: val_loss did not improve from 28.41477
196/196 - 36s - loss: 28.0043 - MinusLogProbMetric: 28.0043 - val_loss: 28.6443 - val_MinusLogProbMetric: 28.6443 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 320/1000
2023-10-26 22:41:36.375 
Epoch 320/1000 
	 loss: 27.9691, MinusLogProbMetric: 27.9691, val_loss: 28.6857, val_MinusLogProbMetric: 28.6857

Epoch 320: val_loss did not improve from 28.41477
196/196 - 35s - loss: 27.9691 - MinusLogProbMetric: 27.9691 - val_loss: 28.6857 - val_MinusLogProbMetric: 28.6857 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 321/1000
2023-10-26 22:42:12.245 
Epoch 321/1000 
	 loss: 27.9692, MinusLogProbMetric: 27.9692, val_loss: 28.4144, val_MinusLogProbMetric: 28.4144

Epoch 321: val_loss improved from 28.41477 to 28.41440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 27.9692 - MinusLogProbMetric: 27.9692 - val_loss: 28.4144 - val_MinusLogProbMetric: 28.4144 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 322/1000
2023-10-26 22:42:48.566 
Epoch 322/1000 
	 loss: 27.9131, MinusLogProbMetric: 27.9131, val_loss: 28.6397, val_MinusLogProbMetric: 28.6397

Epoch 322: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9131 - MinusLogProbMetric: 27.9131 - val_loss: 28.6397 - val_MinusLogProbMetric: 28.6397 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 323/1000
2023-10-26 22:43:24.121 
Epoch 323/1000 
	 loss: 27.9973, MinusLogProbMetric: 27.9973, val_loss: 28.6670, val_MinusLogProbMetric: 28.6670

Epoch 323: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9973 - MinusLogProbMetric: 27.9973 - val_loss: 28.6670 - val_MinusLogProbMetric: 28.6670 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 324/1000
2023-10-26 22:44:00.045 
Epoch 324/1000 
	 loss: 27.9743, MinusLogProbMetric: 27.9743, val_loss: 28.5416, val_MinusLogProbMetric: 28.5416

Epoch 324: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9743 - MinusLogProbMetric: 27.9743 - val_loss: 28.5416 - val_MinusLogProbMetric: 28.5416 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 325/1000
2023-10-26 22:44:35.566 
Epoch 325/1000 
	 loss: 27.9452, MinusLogProbMetric: 27.9452, val_loss: 28.8427, val_MinusLogProbMetric: 28.8427

Epoch 325: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9452 - MinusLogProbMetric: 27.9452 - val_loss: 28.8427 - val_MinusLogProbMetric: 28.8427 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 326/1000
2023-10-26 22:45:11.223 
Epoch 326/1000 
	 loss: 27.9615, MinusLogProbMetric: 27.9615, val_loss: 28.9082, val_MinusLogProbMetric: 28.9082

Epoch 326: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9615 - MinusLogProbMetric: 27.9615 - val_loss: 28.9082 - val_MinusLogProbMetric: 28.9082 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 327/1000
2023-10-26 22:45:47.174 
Epoch 327/1000 
	 loss: 27.9688, MinusLogProbMetric: 27.9688, val_loss: 28.5814, val_MinusLogProbMetric: 28.5814

Epoch 327: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9688 - MinusLogProbMetric: 27.9688 - val_loss: 28.5814 - val_MinusLogProbMetric: 28.5814 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 328/1000
2023-10-26 22:46:23.006 
Epoch 328/1000 
	 loss: 27.9255, MinusLogProbMetric: 27.9255, val_loss: 28.4664, val_MinusLogProbMetric: 28.4664

Epoch 328: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9255 - MinusLogProbMetric: 27.9255 - val_loss: 28.4664 - val_MinusLogProbMetric: 28.4664 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 329/1000
2023-10-26 22:46:59.217 
Epoch 329/1000 
	 loss: 27.9227, MinusLogProbMetric: 27.9227, val_loss: 28.7029, val_MinusLogProbMetric: 28.7029

Epoch 329: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9227 - MinusLogProbMetric: 27.9227 - val_loss: 28.7029 - val_MinusLogProbMetric: 28.7029 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 330/1000
2023-10-26 22:47:34.894 
Epoch 330/1000 
	 loss: 27.9380, MinusLogProbMetric: 27.9380, val_loss: 28.6810, val_MinusLogProbMetric: 28.6810

Epoch 330: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9380 - MinusLogProbMetric: 27.9380 - val_loss: 28.6810 - val_MinusLogProbMetric: 28.6810 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 331/1000
2023-10-26 22:48:10.522 
Epoch 331/1000 
	 loss: 28.0139, MinusLogProbMetric: 28.0139, val_loss: 28.7623, val_MinusLogProbMetric: 28.7623

Epoch 331: val_loss did not improve from 28.41440
196/196 - 36s - loss: 28.0139 - MinusLogProbMetric: 28.0139 - val_loss: 28.7623 - val_MinusLogProbMetric: 28.7623 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 332/1000
2023-10-26 22:48:46.213 
Epoch 332/1000 
	 loss: 27.9198, MinusLogProbMetric: 27.9198, val_loss: 28.6506, val_MinusLogProbMetric: 28.6506

Epoch 332: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9198 - MinusLogProbMetric: 27.9198 - val_loss: 28.6506 - val_MinusLogProbMetric: 28.6506 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 333/1000
2023-10-26 22:49:21.825 
Epoch 333/1000 
	 loss: 27.8716, MinusLogProbMetric: 27.8716, val_loss: 28.6005, val_MinusLogProbMetric: 28.6005

Epoch 333: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.8716 - MinusLogProbMetric: 27.8716 - val_loss: 28.6005 - val_MinusLogProbMetric: 28.6005 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 334/1000
2023-10-26 22:49:57.925 
Epoch 334/1000 
	 loss: 27.9376, MinusLogProbMetric: 27.9376, val_loss: 28.4678, val_MinusLogProbMetric: 28.4678

Epoch 334: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.9376 - MinusLogProbMetric: 27.9376 - val_loss: 28.4678 - val_MinusLogProbMetric: 28.4678 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 335/1000
2023-10-26 22:50:33.380 
Epoch 335/1000 
	 loss: 27.8520, MinusLogProbMetric: 27.8520, val_loss: 28.4778, val_MinusLogProbMetric: 28.4778

Epoch 335: val_loss did not improve from 28.41440
196/196 - 35s - loss: 27.8520 - MinusLogProbMetric: 27.8520 - val_loss: 28.4778 - val_MinusLogProbMetric: 28.4778 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 336/1000
2023-10-26 22:51:09.327 
Epoch 336/1000 
	 loss: 27.8691, MinusLogProbMetric: 27.8691, val_loss: 28.6223, val_MinusLogProbMetric: 28.6223

Epoch 336: val_loss did not improve from 28.41440
196/196 - 36s - loss: 27.8691 - MinusLogProbMetric: 27.8691 - val_loss: 28.6223 - val_MinusLogProbMetric: 28.6223 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 337/1000
2023-10-26 22:51:45.479 
Epoch 337/1000 
	 loss: 27.9287, MinusLogProbMetric: 27.9287, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 337: val_loss improved from 28.41440 to 28.26375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 27.9287 - MinusLogProbMetric: 27.9287 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 338/1000
2023-10-26 22:52:22.184 
Epoch 338/1000 
	 loss: 27.8751, MinusLogProbMetric: 27.8751, val_loss: 28.5919, val_MinusLogProbMetric: 28.5919

Epoch 338: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8751 - MinusLogProbMetric: 27.8751 - val_loss: 28.5919 - val_MinusLogProbMetric: 28.5919 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 339/1000
2023-10-26 22:52:58.414 
Epoch 339/1000 
	 loss: 27.8996, MinusLogProbMetric: 27.8996, val_loss: 28.7516, val_MinusLogProbMetric: 28.7516

Epoch 339: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8996 - MinusLogProbMetric: 27.8996 - val_loss: 28.7516 - val_MinusLogProbMetric: 28.7516 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 340/1000
2023-10-26 22:53:34.445 
Epoch 340/1000 
	 loss: 27.8680, MinusLogProbMetric: 27.8680, val_loss: 28.6068, val_MinusLogProbMetric: 28.6068

Epoch 340: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8680 - MinusLogProbMetric: 27.8680 - val_loss: 28.6068 - val_MinusLogProbMetric: 28.6068 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 341/1000
2023-10-26 22:54:10.093 
Epoch 341/1000 
	 loss: 27.8912, MinusLogProbMetric: 27.8912, val_loss: 28.7455, val_MinusLogProbMetric: 28.7455

Epoch 341: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8912 - MinusLogProbMetric: 27.8912 - val_loss: 28.7455 - val_MinusLogProbMetric: 28.7455 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 342/1000
2023-10-26 22:54:46.040 
Epoch 342/1000 
	 loss: 27.9294, MinusLogProbMetric: 27.9294, val_loss: 28.8218, val_MinusLogProbMetric: 28.8218

Epoch 342: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.9294 - MinusLogProbMetric: 27.9294 - val_loss: 28.8218 - val_MinusLogProbMetric: 28.8218 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 343/1000
2023-10-26 22:55:22.024 
Epoch 343/1000 
	 loss: 27.8421, MinusLogProbMetric: 27.8421, val_loss: 29.3127, val_MinusLogProbMetric: 29.3127

Epoch 343: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8421 - MinusLogProbMetric: 27.8421 - val_loss: 29.3127 - val_MinusLogProbMetric: 29.3127 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 344/1000
2023-10-26 22:55:57.456 
Epoch 344/1000 
	 loss: 27.9147, MinusLogProbMetric: 27.9147, val_loss: 28.4081, val_MinusLogProbMetric: 28.4081

Epoch 344: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.9147 - MinusLogProbMetric: 27.9147 - val_loss: 28.4081 - val_MinusLogProbMetric: 28.4081 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 345/1000
2023-10-26 22:56:33.093 
Epoch 345/1000 
	 loss: 27.9057, MinusLogProbMetric: 27.9057, val_loss: 28.3079, val_MinusLogProbMetric: 28.3079

Epoch 345: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.9057 - MinusLogProbMetric: 27.9057 - val_loss: 28.3079 - val_MinusLogProbMetric: 28.3079 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 346/1000
2023-10-26 22:57:08.707 
Epoch 346/1000 
	 loss: 27.8864, MinusLogProbMetric: 27.8864, val_loss: 28.7650, val_MinusLogProbMetric: 28.7650

Epoch 346: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8864 - MinusLogProbMetric: 27.8864 - val_loss: 28.7650 - val_MinusLogProbMetric: 28.7650 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 347/1000
2023-10-26 22:57:44.634 
Epoch 347/1000 
	 loss: 27.8687, MinusLogProbMetric: 27.8687, val_loss: 28.8607, val_MinusLogProbMetric: 28.8607

Epoch 347: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8687 - MinusLogProbMetric: 27.8687 - val_loss: 28.8607 - val_MinusLogProbMetric: 28.8607 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 348/1000
2023-10-26 22:58:20.105 
Epoch 348/1000 
	 loss: 27.8632, MinusLogProbMetric: 27.8632, val_loss: 28.8644, val_MinusLogProbMetric: 28.8644

Epoch 348: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.8632 - MinusLogProbMetric: 27.8632 - val_loss: 28.8644 - val_MinusLogProbMetric: 28.8644 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 349/1000
2023-10-26 22:58:55.801 
Epoch 349/1000 
	 loss: 27.8952, MinusLogProbMetric: 27.8952, val_loss: 28.4022, val_MinusLogProbMetric: 28.4022

Epoch 349: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8952 - MinusLogProbMetric: 27.8952 - val_loss: 28.4022 - val_MinusLogProbMetric: 28.4022 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 350/1000
2023-10-26 22:59:31.270 
Epoch 350/1000 
	 loss: 27.8894, MinusLogProbMetric: 27.8894, val_loss: 28.3056, val_MinusLogProbMetric: 28.3056

Epoch 350: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.8894 - MinusLogProbMetric: 27.8894 - val_loss: 28.3056 - val_MinusLogProbMetric: 28.3056 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 351/1000
2023-10-26 23:00:06.857 
Epoch 351/1000 
	 loss: 27.8069, MinusLogProbMetric: 27.8069, val_loss: 28.7279, val_MinusLogProbMetric: 28.7279

Epoch 351: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8069 - MinusLogProbMetric: 27.8069 - val_loss: 28.7279 - val_MinusLogProbMetric: 28.7279 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 352/1000
2023-10-26 23:00:42.421 
Epoch 352/1000 
	 loss: 27.8650, MinusLogProbMetric: 27.8650, val_loss: 28.5968, val_MinusLogProbMetric: 28.5968

Epoch 352: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8650 - MinusLogProbMetric: 27.8650 - val_loss: 28.5968 - val_MinusLogProbMetric: 28.5968 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 353/1000
2023-10-26 23:01:18.094 
Epoch 353/1000 
	 loss: 27.8237, MinusLogProbMetric: 27.8237, val_loss: 28.6021, val_MinusLogProbMetric: 28.6021

Epoch 353: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8237 - MinusLogProbMetric: 27.8237 - val_loss: 28.6021 - val_MinusLogProbMetric: 28.6021 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 354/1000
2023-10-26 23:01:54.090 
Epoch 354/1000 
	 loss: 27.9041, MinusLogProbMetric: 27.9041, val_loss: 28.5210, val_MinusLogProbMetric: 28.5210

Epoch 354: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.9041 - MinusLogProbMetric: 27.9041 - val_loss: 28.5210 - val_MinusLogProbMetric: 28.5210 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 355/1000
2023-10-26 23:02:30.064 
Epoch 355/1000 
	 loss: 27.8559, MinusLogProbMetric: 27.8559, val_loss: 28.5936, val_MinusLogProbMetric: 28.5936

Epoch 355: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8559 - MinusLogProbMetric: 27.8559 - val_loss: 28.5936 - val_MinusLogProbMetric: 28.5936 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 356/1000
2023-10-26 23:03:05.375 
Epoch 356/1000 
	 loss: 27.8421, MinusLogProbMetric: 27.8421, val_loss: 28.6343, val_MinusLogProbMetric: 28.6343

Epoch 356: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.8421 - MinusLogProbMetric: 27.8421 - val_loss: 28.6343 - val_MinusLogProbMetric: 28.6343 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 357/1000
2023-10-26 23:03:41.213 
Epoch 357/1000 
	 loss: 27.8423, MinusLogProbMetric: 27.8423, val_loss: 28.3688, val_MinusLogProbMetric: 28.3688

Epoch 357: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8423 - MinusLogProbMetric: 27.8423 - val_loss: 28.3688 - val_MinusLogProbMetric: 28.3688 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 358/1000
2023-10-26 23:04:16.956 
Epoch 358/1000 
	 loss: 27.8690, MinusLogProbMetric: 27.8690, val_loss: 28.6378, val_MinusLogProbMetric: 28.6378

Epoch 358: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8690 - MinusLogProbMetric: 27.8690 - val_loss: 28.6378 - val_MinusLogProbMetric: 28.6378 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 359/1000
2023-10-26 23:04:53.130 
Epoch 359/1000 
	 loss: 27.7711, MinusLogProbMetric: 27.7711, val_loss: 29.0966, val_MinusLogProbMetric: 29.0966

Epoch 359: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7711 - MinusLogProbMetric: 27.7711 - val_loss: 29.0966 - val_MinusLogProbMetric: 29.0966 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 360/1000
2023-10-26 23:05:28.902 
Epoch 360/1000 
	 loss: 27.8747, MinusLogProbMetric: 27.8747, val_loss: 28.6368, val_MinusLogProbMetric: 28.6368

Epoch 360: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8747 - MinusLogProbMetric: 27.8747 - val_loss: 28.6368 - val_MinusLogProbMetric: 28.6368 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 361/1000
2023-10-26 23:06:04.724 
Epoch 361/1000 
	 loss: 27.8383, MinusLogProbMetric: 27.8383, val_loss: 28.6348, val_MinusLogProbMetric: 28.6348

Epoch 361: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8383 - MinusLogProbMetric: 27.8383 - val_loss: 28.6348 - val_MinusLogProbMetric: 28.6348 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 362/1000
2023-10-26 23:06:40.330 
Epoch 362/1000 
	 loss: 27.8548, MinusLogProbMetric: 27.8548, val_loss: 28.8315, val_MinusLogProbMetric: 28.8315

Epoch 362: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8548 - MinusLogProbMetric: 27.8548 - val_loss: 28.8315 - val_MinusLogProbMetric: 28.8315 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 363/1000
2023-10-26 23:07:16.175 
Epoch 363/1000 
	 loss: 27.8800, MinusLogProbMetric: 27.8800, val_loss: 28.6456, val_MinusLogProbMetric: 28.6456

Epoch 363: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8800 - MinusLogProbMetric: 27.8800 - val_loss: 28.6456 - val_MinusLogProbMetric: 28.6456 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 364/1000
2023-10-26 23:07:51.608 
Epoch 364/1000 
	 loss: 27.7847, MinusLogProbMetric: 27.7847, val_loss: 28.5487, val_MinusLogProbMetric: 28.5487

Epoch 364: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.7847 - MinusLogProbMetric: 27.7847 - val_loss: 28.5487 - val_MinusLogProbMetric: 28.5487 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 365/1000
2023-10-26 23:08:26.877 
Epoch 365/1000 
	 loss: 27.8561, MinusLogProbMetric: 27.8561, val_loss: 28.9938, val_MinusLogProbMetric: 28.9938

Epoch 365: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.8561 - MinusLogProbMetric: 27.8561 - val_loss: 28.9938 - val_MinusLogProbMetric: 28.9938 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 366/1000
2023-10-26 23:09:02.256 
Epoch 366/1000 
	 loss: 27.7522, MinusLogProbMetric: 27.7522, val_loss: 28.3624, val_MinusLogProbMetric: 28.3624

Epoch 366: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.7522 - MinusLogProbMetric: 27.7522 - val_loss: 28.3624 - val_MinusLogProbMetric: 28.3624 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 367/1000
2023-10-26 23:09:37.628 
Epoch 367/1000 
	 loss: 27.8365, MinusLogProbMetric: 27.8365, val_loss: 28.4438, val_MinusLogProbMetric: 28.4438

Epoch 367: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.8365 - MinusLogProbMetric: 27.8365 - val_loss: 28.4438 - val_MinusLogProbMetric: 28.4438 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 368/1000
2023-10-26 23:10:13.510 
Epoch 368/1000 
	 loss: 27.8008, MinusLogProbMetric: 27.8008, val_loss: 28.5396, val_MinusLogProbMetric: 28.5396

Epoch 368: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8008 - MinusLogProbMetric: 27.8008 - val_loss: 28.5396 - val_MinusLogProbMetric: 28.5396 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 369/1000
2023-10-26 23:10:49.210 
Epoch 369/1000 
	 loss: 27.7847, MinusLogProbMetric: 27.7847, val_loss: 28.5026, val_MinusLogProbMetric: 28.5026

Epoch 369: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7847 - MinusLogProbMetric: 27.7847 - val_loss: 28.5026 - val_MinusLogProbMetric: 28.5026 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 370/1000
2023-10-26 23:11:25.511 
Epoch 370/1000 
	 loss: 27.7873, MinusLogProbMetric: 27.7873, val_loss: 28.4241, val_MinusLogProbMetric: 28.4241

Epoch 370: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7873 - MinusLogProbMetric: 27.7873 - val_loss: 28.4241 - val_MinusLogProbMetric: 28.4241 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 371/1000
2023-10-26 23:12:01.148 
Epoch 371/1000 
	 loss: 27.8237, MinusLogProbMetric: 27.8237, val_loss: 28.6508, val_MinusLogProbMetric: 28.6508

Epoch 371: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8237 - MinusLogProbMetric: 27.8237 - val_loss: 28.6508 - val_MinusLogProbMetric: 28.6508 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 372/1000
2023-10-26 23:12:36.776 
Epoch 372/1000 
	 loss: 27.8207, MinusLogProbMetric: 27.8207, val_loss: 28.5096, val_MinusLogProbMetric: 28.5096

Epoch 372: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8207 - MinusLogProbMetric: 27.8207 - val_loss: 28.5096 - val_MinusLogProbMetric: 28.5096 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 373/1000
2023-10-26 23:13:12.662 
Epoch 373/1000 
	 loss: 27.8266, MinusLogProbMetric: 27.8266, val_loss: 28.6291, val_MinusLogProbMetric: 28.6291

Epoch 373: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8266 - MinusLogProbMetric: 27.8266 - val_loss: 28.6291 - val_MinusLogProbMetric: 28.6291 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 374/1000
2023-10-26 23:13:48.366 
Epoch 374/1000 
	 loss: 27.8150, MinusLogProbMetric: 27.8150, val_loss: 28.4858, val_MinusLogProbMetric: 28.4858

Epoch 374: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8150 - MinusLogProbMetric: 27.8150 - val_loss: 28.4858 - val_MinusLogProbMetric: 28.4858 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 375/1000
2023-10-26 23:14:23.880 
Epoch 375/1000 
	 loss: 27.7917, MinusLogProbMetric: 27.7917, val_loss: 28.5171, val_MinusLogProbMetric: 28.5171

Epoch 375: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7917 - MinusLogProbMetric: 27.7917 - val_loss: 28.5171 - val_MinusLogProbMetric: 28.5171 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 376/1000
2023-10-26 23:14:59.599 
Epoch 376/1000 
	 loss: 27.7503, MinusLogProbMetric: 27.7503, val_loss: 28.6571, val_MinusLogProbMetric: 28.6571

Epoch 376: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7503 - MinusLogProbMetric: 27.7503 - val_loss: 28.6571 - val_MinusLogProbMetric: 28.6571 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 377/1000
2023-10-26 23:15:35.333 
Epoch 377/1000 
	 loss: 27.7945, MinusLogProbMetric: 27.7945, val_loss: 28.6725, val_MinusLogProbMetric: 28.6725

Epoch 377: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7945 - MinusLogProbMetric: 27.7945 - val_loss: 28.6725 - val_MinusLogProbMetric: 28.6725 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 378/1000
2023-10-26 23:16:10.710 
Epoch 378/1000 
	 loss: 27.7772, MinusLogProbMetric: 27.7772, val_loss: 28.5921, val_MinusLogProbMetric: 28.5921

Epoch 378: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.7772 - MinusLogProbMetric: 27.7772 - val_loss: 28.5921 - val_MinusLogProbMetric: 28.5921 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 379/1000
2023-10-26 23:16:45.947 
Epoch 379/1000 
	 loss: 27.8004, MinusLogProbMetric: 27.8004, val_loss: 29.6063, val_MinusLogProbMetric: 29.6063

Epoch 379: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.8004 - MinusLogProbMetric: 27.8004 - val_loss: 29.6063 - val_MinusLogProbMetric: 29.6063 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 380/1000
2023-10-26 23:17:21.472 
Epoch 380/1000 
	 loss: 27.7914, MinusLogProbMetric: 27.7914, val_loss: 28.6362, val_MinusLogProbMetric: 28.6362

Epoch 380: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7914 - MinusLogProbMetric: 27.7914 - val_loss: 28.6362 - val_MinusLogProbMetric: 28.6362 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 381/1000
2023-10-26 23:17:57.056 
Epoch 381/1000 
	 loss: 27.8077, MinusLogProbMetric: 27.8077, val_loss: 28.6211, val_MinusLogProbMetric: 28.6211

Epoch 381: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8077 - MinusLogProbMetric: 27.8077 - val_loss: 28.6211 - val_MinusLogProbMetric: 28.6211 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 382/1000
2023-10-26 23:18:33.048 
Epoch 382/1000 
	 loss: 27.7583, MinusLogProbMetric: 27.7583, val_loss: 28.7455, val_MinusLogProbMetric: 28.7455

Epoch 382: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7583 - MinusLogProbMetric: 27.7583 - val_loss: 28.7455 - val_MinusLogProbMetric: 28.7455 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 383/1000
2023-10-26 23:19:08.695 
Epoch 383/1000 
	 loss: 27.7834, MinusLogProbMetric: 27.7834, val_loss: 28.9638, val_MinusLogProbMetric: 28.9638

Epoch 383: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7834 - MinusLogProbMetric: 27.7834 - val_loss: 28.9638 - val_MinusLogProbMetric: 28.9638 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 384/1000
2023-10-26 23:19:44.739 
Epoch 384/1000 
	 loss: 27.8426, MinusLogProbMetric: 27.8426, val_loss: 28.5774, val_MinusLogProbMetric: 28.5774

Epoch 384: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8426 - MinusLogProbMetric: 27.8426 - val_loss: 28.5774 - val_MinusLogProbMetric: 28.5774 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 385/1000
2023-10-26 23:20:20.462 
Epoch 385/1000 
	 loss: 27.7261, MinusLogProbMetric: 27.7261, val_loss: 28.8505, val_MinusLogProbMetric: 28.8505

Epoch 385: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.7261 - MinusLogProbMetric: 27.7261 - val_loss: 28.8505 - val_MinusLogProbMetric: 28.8505 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 386/1000
2023-10-26 23:20:55.797 
Epoch 386/1000 
	 loss: 27.7539, MinusLogProbMetric: 27.7539, val_loss: 28.2829, val_MinusLogProbMetric: 28.2829

Epoch 386: val_loss did not improve from 28.26375
196/196 - 35s - loss: 27.7539 - MinusLogProbMetric: 27.7539 - val_loss: 28.2829 - val_MinusLogProbMetric: 28.2829 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 387/1000
2023-10-26 23:21:31.606 
Epoch 387/1000 
	 loss: 27.8249, MinusLogProbMetric: 27.8249, val_loss: 28.4245, val_MinusLogProbMetric: 28.4245

Epoch 387: val_loss did not improve from 28.26375
196/196 - 36s - loss: 27.8249 - MinusLogProbMetric: 27.8249 - val_loss: 28.4245 - val_MinusLogProbMetric: 28.4245 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 388/1000
2023-10-26 23:22:07.444 
Epoch 388/1000 
	 loss: 27.3765, MinusLogProbMetric: 27.3765, val_loss: 28.1497, val_MinusLogProbMetric: 28.1497

Epoch 388: val_loss improved from 28.26375 to 28.14966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 27.3765 - MinusLogProbMetric: 27.3765 - val_loss: 28.1497 - val_MinusLogProbMetric: 28.1497 - lr: 5.0000e-04 - 37s/epoch - 186ms/step
Epoch 389/1000
2023-10-26 23:22:43.544 
Epoch 389/1000 
	 loss: 27.3817, MinusLogProbMetric: 27.3817, val_loss: 28.2812, val_MinusLogProbMetric: 28.2812

Epoch 389: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3817 - MinusLogProbMetric: 27.3817 - val_loss: 28.2812 - val_MinusLogProbMetric: 28.2812 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 390/1000
2023-10-26 23:23:19.059 
Epoch 390/1000 
	 loss: 27.3818, MinusLogProbMetric: 27.3818, val_loss: 28.1724, val_MinusLogProbMetric: 28.1724

Epoch 390: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3818 - MinusLogProbMetric: 27.3818 - val_loss: 28.1724 - val_MinusLogProbMetric: 28.1724 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 391/1000
2023-10-26 23:23:54.622 
Epoch 391/1000 
	 loss: 27.3715, MinusLogProbMetric: 27.3715, val_loss: 28.1621, val_MinusLogProbMetric: 28.1621

Epoch 391: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3715 - MinusLogProbMetric: 27.3715 - val_loss: 28.1621 - val_MinusLogProbMetric: 28.1621 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 392/1000
2023-10-26 23:24:30.125 
Epoch 392/1000 
	 loss: 27.3645, MinusLogProbMetric: 27.3645, val_loss: 28.1806, val_MinusLogProbMetric: 28.1806

Epoch 392: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3645 - MinusLogProbMetric: 27.3645 - val_loss: 28.1806 - val_MinusLogProbMetric: 28.1806 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 393/1000
2023-10-26 23:25:05.645 
Epoch 393/1000 
	 loss: 27.3581, MinusLogProbMetric: 27.3581, val_loss: 28.2145, val_MinusLogProbMetric: 28.2145

Epoch 393: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3581 - MinusLogProbMetric: 27.3581 - val_loss: 28.2145 - val_MinusLogProbMetric: 28.2145 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 394/1000
2023-10-26 23:25:41.486 
Epoch 394/1000 
	 loss: 27.3847, MinusLogProbMetric: 27.3847, val_loss: 28.2215, val_MinusLogProbMetric: 28.2215

Epoch 394: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3847 - MinusLogProbMetric: 27.3847 - val_loss: 28.2215 - val_MinusLogProbMetric: 28.2215 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 395/1000
2023-10-26 23:26:17.270 
Epoch 395/1000 
	 loss: 27.3625, MinusLogProbMetric: 27.3625, val_loss: 28.1935, val_MinusLogProbMetric: 28.1935

Epoch 395: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3625 - MinusLogProbMetric: 27.3625 - val_loss: 28.1935 - val_MinusLogProbMetric: 28.1935 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 396/1000
2023-10-26 23:26:52.982 
Epoch 396/1000 
	 loss: 27.3896, MinusLogProbMetric: 27.3896, val_loss: 28.3019, val_MinusLogProbMetric: 28.3019

Epoch 396: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3896 - MinusLogProbMetric: 27.3896 - val_loss: 28.3019 - val_MinusLogProbMetric: 28.3019 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 397/1000
2023-10-26 23:27:28.935 
Epoch 397/1000 
	 loss: 27.3739, MinusLogProbMetric: 27.3739, val_loss: 28.1686, val_MinusLogProbMetric: 28.1686

Epoch 397: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3739 - MinusLogProbMetric: 27.3739 - val_loss: 28.1686 - val_MinusLogProbMetric: 28.1686 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 398/1000
2023-10-26 23:28:04.546 
Epoch 398/1000 
	 loss: 27.3641, MinusLogProbMetric: 27.3641, val_loss: 28.1806, val_MinusLogProbMetric: 28.1806

Epoch 398: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3641 - MinusLogProbMetric: 27.3641 - val_loss: 28.1806 - val_MinusLogProbMetric: 28.1806 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 399/1000
2023-10-26 23:28:39.926 
Epoch 399/1000 
	 loss: 27.3516, MinusLogProbMetric: 27.3516, val_loss: 28.1778, val_MinusLogProbMetric: 28.1778

Epoch 399: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3516 - MinusLogProbMetric: 27.3516 - val_loss: 28.1778 - val_MinusLogProbMetric: 28.1778 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 400/1000
2023-10-26 23:29:15.355 
Epoch 400/1000 
	 loss: 27.3697, MinusLogProbMetric: 27.3697, val_loss: 28.3551, val_MinusLogProbMetric: 28.3551

Epoch 400: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3697 - MinusLogProbMetric: 27.3697 - val_loss: 28.3551 - val_MinusLogProbMetric: 28.3551 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 401/1000
2023-10-26 23:29:50.789 
Epoch 401/1000 
	 loss: 27.3857, MinusLogProbMetric: 27.3857, val_loss: 28.1710, val_MinusLogProbMetric: 28.1710

Epoch 401: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3857 - MinusLogProbMetric: 27.3857 - val_loss: 28.1710 - val_MinusLogProbMetric: 28.1710 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 402/1000
2023-10-26 23:30:25.949 
Epoch 402/1000 
	 loss: 27.3654, MinusLogProbMetric: 27.3654, val_loss: 28.1937, val_MinusLogProbMetric: 28.1937

Epoch 402: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3654 - MinusLogProbMetric: 27.3654 - val_loss: 28.1937 - val_MinusLogProbMetric: 28.1937 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 403/1000
2023-10-26 23:31:01.259 
Epoch 403/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.1853, val_MinusLogProbMetric: 28.1853

Epoch 403: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.1853 - val_MinusLogProbMetric: 28.1853 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 404/1000
2023-10-26 23:31:37.308 
Epoch 404/1000 
	 loss: 27.3660, MinusLogProbMetric: 27.3660, val_loss: 28.3547, val_MinusLogProbMetric: 28.3547

Epoch 404: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3660 - MinusLogProbMetric: 27.3660 - val_loss: 28.3547 - val_MinusLogProbMetric: 28.3547 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 405/1000
2023-10-26 23:32:13.094 
Epoch 405/1000 
	 loss: 27.3781, MinusLogProbMetric: 27.3781, val_loss: 28.2223, val_MinusLogProbMetric: 28.2223

Epoch 405: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3781 - MinusLogProbMetric: 27.3781 - val_loss: 28.2223 - val_MinusLogProbMetric: 28.2223 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 406/1000
2023-10-26 23:32:48.928 
Epoch 406/1000 
	 loss: 27.3491, MinusLogProbMetric: 27.3491, val_loss: 28.2980, val_MinusLogProbMetric: 28.2980

Epoch 406: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3491 - MinusLogProbMetric: 27.3491 - val_loss: 28.2980 - val_MinusLogProbMetric: 28.2980 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 407/1000
2023-10-26 23:33:24.736 
Epoch 407/1000 
	 loss: 27.3701, MinusLogProbMetric: 27.3701, val_loss: 28.2353, val_MinusLogProbMetric: 28.2353

Epoch 407: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3701 - MinusLogProbMetric: 27.3701 - val_loss: 28.2353 - val_MinusLogProbMetric: 28.2353 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 408/1000
2023-10-26 23:34:00.570 
Epoch 408/1000 
	 loss: 27.3386, MinusLogProbMetric: 27.3386, val_loss: 28.2558, val_MinusLogProbMetric: 28.2558

Epoch 408: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3386 - MinusLogProbMetric: 27.3386 - val_loss: 28.2558 - val_MinusLogProbMetric: 28.2558 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 409/1000
2023-10-26 23:34:35.959 
Epoch 409/1000 
	 loss: 27.3607, MinusLogProbMetric: 27.3607, val_loss: 28.1757, val_MinusLogProbMetric: 28.1757

Epoch 409: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3607 - MinusLogProbMetric: 27.3607 - val_loss: 28.1757 - val_MinusLogProbMetric: 28.1757 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 410/1000
2023-10-26 23:35:11.391 
Epoch 410/1000 
	 loss: 27.3353, MinusLogProbMetric: 27.3353, val_loss: 28.1965, val_MinusLogProbMetric: 28.1965

Epoch 410: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3353 - MinusLogProbMetric: 27.3353 - val_loss: 28.1965 - val_MinusLogProbMetric: 28.1965 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 411/1000
2023-10-26 23:35:47.113 
Epoch 411/1000 
	 loss: 27.3851, MinusLogProbMetric: 27.3851, val_loss: 28.1498, val_MinusLogProbMetric: 28.1498

Epoch 411: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3851 - MinusLogProbMetric: 27.3851 - val_loss: 28.1498 - val_MinusLogProbMetric: 28.1498 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 412/1000
2023-10-26 23:36:22.842 
Epoch 412/1000 
	 loss: 27.3428, MinusLogProbMetric: 27.3428, val_loss: 28.1797, val_MinusLogProbMetric: 28.1797

Epoch 412: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3428 - MinusLogProbMetric: 27.3428 - val_loss: 28.1797 - val_MinusLogProbMetric: 28.1797 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 413/1000
2023-10-26 23:36:58.508 
Epoch 413/1000 
	 loss: 27.3488, MinusLogProbMetric: 27.3488, val_loss: 28.1671, val_MinusLogProbMetric: 28.1671

Epoch 413: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3488 - MinusLogProbMetric: 27.3488 - val_loss: 28.1671 - val_MinusLogProbMetric: 28.1671 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 414/1000
2023-10-26 23:37:34.094 
Epoch 414/1000 
	 loss: 27.3575, MinusLogProbMetric: 27.3575, val_loss: 28.2728, val_MinusLogProbMetric: 28.2728

Epoch 414: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3575 - MinusLogProbMetric: 27.3575 - val_loss: 28.2728 - val_MinusLogProbMetric: 28.2728 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 415/1000
2023-10-26 23:38:09.680 
Epoch 415/1000 
	 loss: 27.3447, MinusLogProbMetric: 27.3447, val_loss: 28.2072, val_MinusLogProbMetric: 28.2072

Epoch 415: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3447 - MinusLogProbMetric: 27.3447 - val_loss: 28.2072 - val_MinusLogProbMetric: 28.2072 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 416/1000
2023-10-26 23:38:45.146 
Epoch 416/1000 
	 loss: 27.3542, MinusLogProbMetric: 27.3542, val_loss: 28.2127, val_MinusLogProbMetric: 28.2127

Epoch 416: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3542 - MinusLogProbMetric: 27.3542 - val_loss: 28.2127 - val_MinusLogProbMetric: 28.2127 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 417/1000
2023-10-26 23:39:21.152 
Epoch 417/1000 
	 loss: 27.3693, MinusLogProbMetric: 27.3693, val_loss: 28.2359, val_MinusLogProbMetric: 28.2359

Epoch 417: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3693 - MinusLogProbMetric: 27.3693 - val_loss: 28.2359 - val_MinusLogProbMetric: 28.2359 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 418/1000
2023-10-26 23:39:56.915 
Epoch 418/1000 
	 loss: 27.3456, MinusLogProbMetric: 27.3456, val_loss: 28.2222, val_MinusLogProbMetric: 28.2222

Epoch 418: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3456 - MinusLogProbMetric: 27.3456 - val_loss: 28.2222 - val_MinusLogProbMetric: 28.2222 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 419/1000
2023-10-26 23:40:32.955 
Epoch 419/1000 
	 loss: 27.3619, MinusLogProbMetric: 27.3619, val_loss: 28.2267, val_MinusLogProbMetric: 28.2267

Epoch 419: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3619 - MinusLogProbMetric: 27.3619 - val_loss: 28.2267 - val_MinusLogProbMetric: 28.2267 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 420/1000
2023-10-26 23:41:08.396 
Epoch 420/1000 
	 loss: 27.3560, MinusLogProbMetric: 27.3560, val_loss: 28.3448, val_MinusLogProbMetric: 28.3448

Epoch 420: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3560 - MinusLogProbMetric: 27.3560 - val_loss: 28.3448 - val_MinusLogProbMetric: 28.3448 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 421/1000
2023-10-26 23:41:44.098 
Epoch 421/1000 
	 loss: 27.3488, MinusLogProbMetric: 27.3488, val_loss: 28.1604, val_MinusLogProbMetric: 28.1604

Epoch 421: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3488 - MinusLogProbMetric: 27.3488 - val_loss: 28.1604 - val_MinusLogProbMetric: 28.1604 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 422/1000
2023-10-26 23:42:19.349 
Epoch 422/1000 
	 loss: 27.3526, MinusLogProbMetric: 27.3526, val_loss: 28.2456, val_MinusLogProbMetric: 28.2456

Epoch 422: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3526 - MinusLogProbMetric: 27.3526 - val_loss: 28.2456 - val_MinusLogProbMetric: 28.2456 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 423/1000
2023-10-26 23:42:54.883 
Epoch 423/1000 
	 loss: 27.3242, MinusLogProbMetric: 27.3242, val_loss: 28.1681, val_MinusLogProbMetric: 28.1681

Epoch 423: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3242 - MinusLogProbMetric: 27.3242 - val_loss: 28.1681 - val_MinusLogProbMetric: 28.1681 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 424/1000
2023-10-26 23:43:30.457 
Epoch 424/1000 
	 loss: 27.3421, MinusLogProbMetric: 27.3421, val_loss: 28.3633, val_MinusLogProbMetric: 28.3633

Epoch 424: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3421 - MinusLogProbMetric: 27.3421 - val_loss: 28.3633 - val_MinusLogProbMetric: 28.3633 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 425/1000
2023-10-26 23:44:06.360 
Epoch 425/1000 
	 loss: 27.3393, MinusLogProbMetric: 27.3393, val_loss: 28.5243, val_MinusLogProbMetric: 28.5243

Epoch 425: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3393 - MinusLogProbMetric: 27.3393 - val_loss: 28.5243 - val_MinusLogProbMetric: 28.5243 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 426/1000
2023-10-26 23:44:41.843 
Epoch 426/1000 
	 loss: 27.3479, MinusLogProbMetric: 27.3479, val_loss: 28.2273, val_MinusLogProbMetric: 28.2273

Epoch 426: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3479 - MinusLogProbMetric: 27.3479 - val_loss: 28.2273 - val_MinusLogProbMetric: 28.2273 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 427/1000
2023-10-26 23:45:17.471 
Epoch 427/1000 
	 loss: 27.3554, MinusLogProbMetric: 27.3554, val_loss: 28.1944, val_MinusLogProbMetric: 28.1944

Epoch 427: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3554 - MinusLogProbMetric: 27.3554 - val_loss: 28.1944 - val_MinusLogProbMetric: 28.1944 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 428/1000
2023-10-26 23:45:53.204 
Epoch 428/1000 
	 loss: 27.3399, MinusLogProbMetric: 27.3399, val_loss: 28.1687, val_MinusLogProbMetric: 28.1687

Epoch 428: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3399 - MinusLogProbMetric: 27.3399 - val_loss: 28.1687 - val_MinusLogProbMetric: 28.1687 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 429/1000
2023-10-26 23:46:28.811 
Epoch 429/1000 
	 loss: 27.3422, MinusLogProbMetric: 27.3422, val_loss: 28.2277, val_MinusLogProbMetric: 28.2277

Epoch 429: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3422 - MinusLogProbMetric: 27.3422 - val_loss: 28.2277 - val_MinusLogProbMetric: 28.2277 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 430/1000
2023-10-26 23:47:04.644 
Epoch 430/1000 
	 loss: 27.3185, MinusLogProbMetric: 27.3185, val_loss: 28.2860, val_MinusLogProbMetric: 28.2860

Epoch 430: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3185 - MinusLogProbMetric: 27.3185 - val_loss: 28.2860 - val_MinusLogProbMetric: 28.2860 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 431/1000
2023-10-26 23:47:40.442 
Epoch 431/1000 
	 loss: 27.3323, MinusLogProbMetric: 27.3323, val_loss: 28.3587, val_MinusLogProbMetric: 28.3587

Epoch 431: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3323 - MinusLogProbMetric: 27.3323 - val_loss: 28.3587 - val_MinusLogProbMetric: 28.3587 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 432/1000
2023-10-26 23:48:15.948 
Epoch 432/1000 
	 loss: 27.3315, MinusLogProbMetric: 27.3315, val_loss: 28.1571, val_MinusLogProbMetric: 28.1571

Epoch 432: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3315 - MinusLogProbMetric: 27.3315 - val_loss: 28.1571 - val_MinusLogProbMetric: 28.1571 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 433/1000
2023-10-26 23:48:51.755 
Epoch 433/1000 
	 loss: 27.3237, MinusLogProbMetric: 27.3237, val_loss: 28.3069, val_MinusLogProbMetric: 28.3069

Epoch 433: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3237 - MinusLogProbMetric: 27.3237 - val_loss: 28.3069 - val_MinusLogProbMetric: 28.3069 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 434/1000
2023-10-26 23:49:27.392 
Epoch 434/1000 
	 loss: 27.3299, MinusLogProbMetric: 27.3299, val_loss: 28.1727, val_MinusLogProbMetric: 28.1727

Epoch 434: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3299 - MinusLogProbMetric: 27.3299 - val_loss: 28.1727 - val_MinusLogProbMetric: 28.1727 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 435/1000
2023-10-26 23:50:02.979 
Epoch 435/1000 
	 loss: 27.3109, MinusLogProbMetric: 27.3109, val_loss: 28.2546, val_MinusLogProbMetric: 28.2546

Epoch 435: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3109 - MinusLogProbMetric: 27.3109 - val_loss: 28.2546 - val_MinusLogProbMetric: 28.2546 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 436/1000
2023-10-26 23:50:38.851 
Epoch 436/1000 
	 loss: 27.3342, MinusLogProbMetric: 27.3342, val_loss: 28.2580, val_MinusLogProbMetric: 28.2580

Epoch 436: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3342 - MinusLogProbMetric: 27.3342 - val_loss: 28.2580 - val_MinusLogProbMetric: 28.2580 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 437/1000
2023-10-26 23:51:14.819 
Epoch 437/1000 
	 loss: 27.3339, MinusLogProbMetric: 27.3339, val_loss: 28.1973, val_MinusLogProbMetric: 28.1973

Epoch 437: val_loss did not improve from 28.14966
196/196 - 36s - loss: 27.3339 - MinusLogProbMetric: 27.3339 - val_loss: 28.1973 - val_MinusLogProbMetric: 28.1973 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 438/1000
2023-10-26 23:51:50.063 
Epoch 438/1000 
	 loss: 27.3330, MinusLogProbMetric: 27.3330, val_loss: 28.2618, val_MinusLogProbMetric: 28.2618

Epoch 438: val_loss did not improve from 28.14966
196/196 - 35s - loss: 27.3330 - MinusLogProbMetric: 27.3330 - val_loss: 28.2618 - val_MinusLogProbMetric: 28.2618 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 439/1000
2023-10-26 23:52:25.953 
Epoch 439/1000 
	 loss: 27.2048, MinusLogProbMetric: 27.2048, val_loss: 28.0696, val_MinusLogProbMetric: 28.0696

Epoch 439: val_loss improved from 28.14966 to 28.06962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 27.2048 - MinusLogProbMetric: 27.2048 - val_loss: 28.0696 - val_MinusLogProbMetric: 28.0696 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 440/1000
2023-10-26 23:53:02.329 
Epoch 440/1000 
	 loss: 27.1917, MinusLogProbMetric: 27.1917, val_loss: 28.1676, val_MinusLogProbMetric: 28.1676

Epoch 440: val_loss did not improve from 28.06962
196/196 - 36s - loss: 27.1917 - MinusLogProbMetric: 27.1917 - val_loss: 28.1676 - val_MinusLogProbMetric: 28.1676 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 441/1000
2023-10-26 23:53:37.949 
Epoch 441/1000 
	 loss: 27.1878, MinusLogProbMetric: 27.1878, val_loss: 28.1211, val_MinusLogProbMetric: 28.1211

Epoch 441: val_loss did not improve from 28.06962
196/196 - 36s - loss: 27.1878 - MinusLogProbMetric: 27.1878 - val_loss: 28.1211 - val_MinusLogProbMetric: 28.1211 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 442/1000
2023-10-26 23:54:13.689 
Epoch 442/1000 
	 loss: 27.1885, MinusLogProbMetric: 27.1885, val_loss: 28.0972, val_MinusLogProbMetric: 28.0972

Epoch 442: val_loss did not improve from 28.06962
196/196 - 36s - loss: 27.1885 - MinusLogProbMetric: 27.1885 - val_loss: 28.0972 - val_MinusLogProbMetric: 28.0972 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 443/1000
2023-10-26 23:54:49.539 
Epoch 443/1000 
	 loss: 27.1856, MinusLogProbMetric: 27.1856, val_loss: 28.0651, val_MinusLogProbMetric: 28.0651

Epoch 443: val_loss improved from 28.06962 to 28.06508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 36s - loss: 27.1856 - MinusLogProbMetric: 27.1856 - val_loss: 28.0651 - val_MinusLogProbMetric: 28.0651 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 444/1000
2023-10-26 23:55:25.323 
Epoch 444/1000 
	 loss: 27.1870, MinusLogProbMetric: 27.1870, val_loss: 28.0834, val_MinusLogProbMetric: 28.0834

Epoch 444: val_loss did not improve from 28.06508
196/196 - 35s - loss: 27.1870 - MinusLogProbMetric: 27.1870 - val_loss: 28.0834 - val_MinusLogProbMetric: 28.0834 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 445/1000
2023-10-26 23:56:01.175 
Epoch 445/1000 
	 loss: 27.1866, MinusLogProbMetric: 27.1866, val_loss: 28.1155, val_MinusLogProbMetric: 28.1155

Epoch 445: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1866 - MinusLogProbMetric: 27.1866 - val_loss: 28.1155 - val_MinusLogProbMetric: 28.1155 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 446/1000
2023-10-26 23:56:36.892 
Epoch 446/1000 
	 loss: 27.1891, MinusLogProbMetric: 27.1891, val_loss: 28.1639, val_MinusLogProbMetric: 28.1639

Epoch 446: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1891 - MinusLogProbMetric: 27.1891 - val_loss: 28.1639 - val_MinusLogProbMetric: 28.1639 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 447/1000
2023-10-26 23:57:12.501 
Epoch 447/1000 
	 loss: 27.1948, MinusLogProbMetric: 27.1948, val_loss: 28.1328, val_MinusLogProbMetric: 28.1328

Epoch 447: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1948 - MinusLogProbMetric: 27.1948 - val_loss: 28.1328 - val_MinusLogProbMetric: 28.1328 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 448/1000
2023-10-26 23:57:48.084 
Epoch 448/1000 
	 loss: 27.1844, MinusLogProbMetric: 27.1844, val_loss: 28.1266, val_MinusLogProbMetric: 28.1266

Epoch 448: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1844 - MinusLogProbMetric: 27.1844 - val_loss: 28.1266 - val_MinusLogProbMetric: 28.1266 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 449/1000
2023-10-26 23:58:24.294 
Epoch 449/1000 
	 loss: 27.1899, MinusLogProbMetric: 27.1899, val_loss: 28.0848, val_MinusLogProbMetric: 28.0848

Epoch 449: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1899 - MinusLogProbMetric: 27.1899 - val_loss: 28.0848 - val_MinusLogProbMetric: 28.0848 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 450/1000
2023-10-26 23:59:00.289 
Epoch 450/1000 
	 loss: 27.1893, MinusLogProbMetric: 27.1893, val_loss: 28.1076, val_MinusLogProbMetric: 28.1076

Epoch 450: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1893 - MinusLogProbMetric: 27.1893 - val_loss: 28.1076 - val_MinusLogProbMetric: 28.1076 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 451/1000
2023-10-26 23:59:36.205 
Epoch 451/1000 
	 loss: 27.1889, MinusLogProbMetric: 27.1889, val_loss: 28.1262, val_MinusLogProbMetric: 28.1262

Epoch 451: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1889 - MinusLogProbMetric: 27.1889 - val_loss: 28.1262 - val_MinusLogProbMetric: 28.1262 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 452/1000
2023-10-27 00:00:11.835 
Epoch 452/1000 
	 loss: 27.1805, MinusLogProbMetric: 27.1805, val_loss: 28.1417, val_MinusLogProbMetric: 28.1417

Epoch 452: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1805 - MinusLogProbMetric: 27.1805 - val_loss: 28.1417 - val_MinusLogProbMetric: 28.1417 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 453/1000
2023-10-27 00:00:47.503 
Epoch 453/1000 
	 loss: 27.1950, MinusLogProbMetric: 27.1950, val_loss: 28.2145, val_MinusLogProbMetric: 28.2145

Epoch 453: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1950 - MinusLogProbMetric: 27.1950 - val_loss: 28.2145 - val_MinusLogProbMetric: 28.2145 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 454/1000
2023-10-27 00:01:23.058 
Epoch 454/1000 
	 loss: 27.1900, MinusLogProbMetric: 27.1900, val_loss: 28.0926, val_MinusLogProbMetric: 28.0926

Epoch 454: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1900 - MinusLogProbMetric: 27.1900 - val_loss: 28.0926 - val_MinusLogProbMetric: 28.0926 - lr: 2.5000e-04 - 36s/epoch - 181ms/step
Epoch 455/1000
2023-10-27 00:01:58.932 
Epoch 455/1000 
	 loss: 27.1839, MinusLogProbMetric: 27.1839, val_loss: 28.1073, val_MinusLogProbMetric: 28.1073

Epoch 455: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1839 - MinusLogProbMetric: 27.1839 - val_loss: 28.1073 - val_MinusLogProbMetric: 28.1073 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 456/1000
2023-10-27 00:02:34.620 
Epoch 456/1000 
	 loss: 27.1817, MinusLogProbMetric: 27.1817, val_loss: 28.1025, val_MinusLogProbMetric: 28.1025

Epoch 456: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1817 - MinusLogProbMetric: 27.1817 - val_loss: 28.1025 - val_MinusLogProbMetric: 28.1025 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 457/1000
2023-10-27 00:03:10.157 
Epoch 457/1000 
	 loss: 27.1788, MinusLogProbMetric: 27.1788, val_loss: 28.0756, val_MinusLogProbMetric: 28.0756

Epoch 457: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1788 - MinusLogProbMetric: 27.1788 - val_loss: 28.0756 - val_MinusLogProbMetric: 28.0756 - lr: 2.5000e-04 - 36s/epoch - 181ms/step
Epoch 458/1000
2023-10-27 00:03:45.779 
Epoch 458/1000 
	 loss: 27.1812, MinusLogProbMetric: 27.1812, val_loss: 28.1726, val_MinusLogProbMetric: 28.1726

Epoch 458: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1812 - MinusLogProbMetric: 27.1812 - val_loss: 28.1726 - val_MinusLogProbMetric: 28.1726 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 459/1000
2023-10-27 00:04:21.831 
Epoch 459/1000 
	 loss: 27.1756, MinusLogProbMetric: 27.1756, val_loss: 28.1004, val_MinusLogProbMetric: 28.1004

Epoch 459: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1756 - MinusLogProbMetric: 27.1756 - val_loss: 28.1004 - val_MinusLogProbMetric: 28.1004 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 460/1000
2023-10-27 00:04:57.347 
Epoch 460/1000 
	 loss: 27.1826, MinusLogProbMetric: 27.1826, val_loss: 28.1051, val_MinusLogProbMetric: 28.1051

Epoch 460: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1826 - MinusLogProbMetric: 27.1826 - val_loss: 28.1051 - val_MinusLogProbMetric: 28.1051 - lr: 2.5000e-04 - 36s/epoch - 181ms/step
Epoch 461/1000
2023-10-27 00:05:33.163 
Epoch 461/1000 
	 loss: 27.1819, MinusLogProbMetric: 27.1819, val_loss: 28.1091, val_MinusLogProbMetric: 28.1091

Epoch 461: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1819 - MinusLogProbMetric: 27.1819 - val_loss: 28.1091 - val_MinusLogProbMetric: 28.1091 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 462/1000
2023-10-27 00:06:08.889 
Epoch 462/1000 
	 loss: 27.1691, MinusLogProbMetric: 27.1691, val_loss: 28.1316, val_MinusLogProbMetric: 28.1316

Epoch 462: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1691 - MinusLogProbMetric: 27.1691 - val_loss: 28.1316 - val_MinusLogProbMetric: 28.1316 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 463/1000
2023-10-27 00:06:44.578 
Epoch 463/1000 
	 loss: 27.1827, MinusLogProbMetric: 27.1827, val_loss: 28.1544, val_MinusLogProbMetric: 28.1544

Epoch 463: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1827 - MinusLogProbMetric: 27.1827 - val_loss: 28.1544 - val_MinusLogProbMetric: 28.1544 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 464/1000
2023-10-27 00:07:20.223 
Epoch 464/1000 
	 loss: 27.1772, MinusLogProbMetric: 27.1772, val_loss: 28.1386, val_MinusLogProbMetric: 28.1386

Epoch 464: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1772 - MinusLogProbMetric: 27.1772 - val_loss: 28.1386 - val_MinusLogProbMetric: 28.1386 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 465/1000
2023-10-27 00:07:55.975 
Epoch 465/1000 
	 loss: 27.1802, MinusLogProbMetric: 27.1802, val_loss: 28.1083, val_MinusLogProbMetric: 28.1083

Epoch 465: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1802 - MinusLogProbMetric: 27.1802 - val_loss: 28.1083 - val_MinusLogProbMetric: 28.1083 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 466/1000
2023-10-27 00:08:31.304 
Epoch 466/1000 
	 loss: 27.1820, MinusLogProbMetric: 27.1820, val_loss: 28.1308, val_MinusLogProbMetric: 28.1308

Epoch 466: val_loss did not improve from 28.06508
196/196 - 35s - loss: 27.1820 - MinusLogProbMetric: 27.1820 - val_loss: 28.1308 - val_MinusLogProbMetric: 28.1308 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 467/1000
2023-10-27 00:09:06.664 
Epoch 467/1000 
	 loss: 27.1783, MinusLogProbMetric: 27.1783, val_loss: 28.1163, val_MinusLogProbMetric: 28.1163

Epoch 467: val_loss did not improve from 28.06508
196/196 - 35s - loss: 27.1783 - MinusLogProbMetric: 27.1783 - val_loss: 28.1163 - val_MinusLogProbMetric: 28.1163 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 468/1000
2023-10-27 00:09:42.482 
Epoch 468/1000 
	 loss: 27.1771, MinusLogProbMetric: 27.1771, val_loss: 28.1046, val_MinusLogProbMetric: 28.1046

Epoch 468: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1771 - MinusLogProbMetric: 27.1771 - val_loss: 28.1046 - val_MinusLogProbMetric: 28.1046 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 469/1000
2023-10-27 00:10:18.132 
Epoch 469/1000 
	 loss: 27.1710, MinusLogProbMetric: 27.1710, val_loss: 28.1193, val_MinusLogProbMetric: 28.1193

Epoch 469: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1710 - MinusLogProbMetric: 27.1710 - val_loss: 28.1193 - val_MinusLogProbMetric: 28.1193 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 470/1000
2023-10-27 00:10:53.955 
Epoch 470/1000 
	 loss: 27.1770, MinusLogProbMetric: 27.1770, val_loss: 28.1364, val_MinusLogProbMetric: 28.1364

Epoch 470: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1770 - MinusLogProbMetric: 27.1770 - val_loss: 28.1364 - val_MinusLogProbMetric: 28.1364 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 471/1000
2023-10-27 00:11:29.705 
Epoch 471/1000 
	 loss: 27.1682, MinusLogProbMetric: 27.1682, val_loss: 28.0989, val_MinusLogProbMetric: 28.0989

Epoch 471: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1682 - MinusLogProbMetric: 27.1682 - val_loss: 28.0989 - val_MinusLogProbMetric: 28.0989 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 472/1000
2023-10-27 00:12:06.020 
Epoch 472/1000 
	 loss: 27.1784, MinusLogProbMetric: 27.1784, val_loss: 28.1352, val_MinusLogProbMetric: 28.1352

Epoch 472: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1784 - MinusLogProbMetric: 27.1784 - val_loss: 28.1352 - val_MinusLogProbMetric: 28.1352 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 473/1000
2023-10-27 00:12:42.160 
Epoch 473/1000 
	 loss: 27.1726, MinusLogProbMetric: 27.1726, val_loss: 28.0856, val_MinusLogProbMetric: 28.0856

Epoch 473: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1726 - MinusLogProbMetric: 27.1726 - val_loss: 28.0856 - val_MinusLogProbMetric: 28.0856 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 474/1000
2023-10-27 00:13:18.241 
Epoch 474/1000 
	 loss: 27.1745, MinusLogProbMetric: 27.1745, val_loss: 28.1023, val_MinusLogProbMetric: 28.1023

Epoch 474: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1745 - MinusLogProbMetric: 27.1745 - val_loss: 28.1023 - val_MinusLogProbMetric: 28.1023 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 475/1000
2023-10-27 00:13:54.300 
Epoch 475/1000 
	 loss: 27.1675, MinusLogProbMetric: 27.1675, val_loss: 28.1185, val_MinusLogProbMetric: 28.1185

Epoch 475: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1675 - MinusLogProbMetric: 27.1675 - val_loss: 28.1185 - val_MinusLogProbMetric: 28.1185 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 476/1000
2023-10-27 00:14:29.489 
Epoch 476/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.1479, val_MinusLogProbMetric: 28.1479

Epoch 476: val_loss did not improve from 28.06508
196/196 - 35s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.1479 - val_MinusLogProbMetric: 28.1479 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 477/1000
2023-10-27 00:14:58.711 
Epoch 477/1000 
	 loss: 27.1724, MinusLogProbMetric: 27.1724, val_loss: 28.1037, val_MinusLogProbMetric: 28.1037

Epoch 477: val_loss did not improve from 28.06508
196/196 - 29s - loss: 27.1724 - MinusLogProbMetric: 27.1724 - val_loss: 28.1037 - val_MinusLogProbMetric: 28.1037 - lr: 2.5000e-04 - 29s/epoch - 149ms/step
Epoch 478/1000
2023-10-27 00:15:31.258 
Epoch 478/1000 
	 loss: 27.1622, MinusLogProbMetric: 27.1622, val_loss: 28.1127, val_MinusLogProbMetric: 28.1127

Epoch 478: val_loss did not improve from 28.06508
196/196 - 33s - loss: 27.1622 - MinusLogProbMetric: 27.1622 - val_loss: 28.1127 - val_MinusLogProbMetric: 28.1127 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 479/1000
2023-10-27 00:16:07.516 
Epoch 479/1000 
	 loss: 27.1765, MinusLogProbMetric: 27.1765, val_loss: 28.1478, val_MinusLogProbMetric: 28.1478

Epoch 479: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1765 - MinusLogProbMetric: 27.1765 - val_loss: 28.1478 - val_MinusLogProbMetric: 28.1478 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 480/1000
2023-10-27 00:16:43.591 
Epoch 480/1000 
	 loss: 27.1658, MinusLogProbMetric: 27.1658, val_loss: 28.1186, val_MinusLogProbMetric: 28.1186

Epoch 480: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1658 - MinusLogProbMetric: 27.1658 - val_loss: 28.1186 - val_MinusLogProbMetric: 28.1186 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 481/1000
2023-10-27 00:17:19.886 
Epoch 481/1000 
	 loss: 27.1786, MinusLogProbMetric: 27.1786, val_loss: 28.0959, val_MinusLogProbMetric: 28.0959

Epoch 481: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1786 - MinusLogProbMetric: 27.1786 - val_loss: 28.0959 - val_MinusLogProbMetric: 28.0959 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 482/1000
2023-10-27 00:17:56.113 
Epoch 482/1000 
	 loss: 27.1672, MinusLogProbMetric: 27.1672, val_loss: 28.1211, val_MinusLogProbMetric: 28.1211

Epoch 482: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1672 - MinusLogProbMetric: 27.1672 - val_loss: 28.1211 - val_MinusLogProbMetric: 28.1211 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 483/1000
2023-10-27 00:18:32.399 
Epoch 483/1000 
	 loss: 27.1610, MinusLogProbMetric: 27.1610, val_loss: 28.0975, val_MinusLogProbMetric: 28.0975

Epoch 483: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1610 - MinusLogProbMetric: 27.1610 - val_loss: 28.0975 - val_MinusLogProbMetric: 28.0975 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 484/1000
2023-10-27 00:19:08.602 
Epoch 484/1000 
	 loss: 27.1689, MinusLogProbMetric: 27.1689, val_loss: 28.0881, val_MinusLogProbMetric: 28.0881

Epoch 484: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1689 - MinusLogProbMetric: 27.1689 - val_loss: 28.0881 - val_MinusLogProbMetric: 28.0881 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 485/1000
2023-10-27 00:19:44.584 
Epoch 485/1000 
	 loss: 27.1736, MinusLogProbMetric: 27.1736, val_loss: 28.1486, val_MinusLogProbMetric: 28.1486

Epoch 485: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1736 - MinusLogProbMetric: 27.1736 - val_loss: 28.1486 - val_MinusLogProbMetric: 28.1486 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 486/1000
2023-10-27 00:20:20.661 
Epoch 486/1000 
	 loss: 27.1612, MinusLogProbMetric: 27.1612, val_loss: 28.1112, val_MinusLogProbMetric: 28.1112

Epoch 486: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1612 - MinusLogProbMetric: 27.1612 - val_loss: 28.1112 - val_MinusLogProbMetric: 28.1112 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 487/1000
2023-10-27 00:20:56.594 
Epoch 487/1000 
	 loss: 27.1611, MinusLogProbMetric: 27.1611, val_loss: 28.1546, val_MinusLogProbMetric: 28.1546

Epoch 487: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1611 - MinusLogProbMetric: 27.1611 - val_loss: 28.1546 - val_MinusLogProbMetric: 28.1546 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 488/1000
2023-10-27 00:21:32.579 
Epoch 488/1000 
	 loss: 27.1722, MinusLogProbMetric: 27.1722, val_loss: 28.1334, val_MinusLogProbMetric: 28.1334

Epoch 488: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1722 - MinusLogProbMetric: 27.1722 - val_loss: 28.1334 - val_MinusLogProbMetric: 28.1334 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 489/1000
2023-10-27 00:22:08.817 
Epoch 489/1000 
	 loss: 27.1646, MinusLogProbMetric: 27.1646, val_loss: 28.1296, val_MinusLogProbMetric: 28.1296

Epoch 489: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1646 - MinusLogProbMetric: 27.1646 - val_loss: 28.1296 - val_MinusLogProbMetric: 28.1296 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 490/1000
2023-10-27 00:22:44.733 
Epoch 490/1000 
	 loss: 27.1657, MinusLogProbMetric: 27.1657, val_loss: 28.1044, val_MinusLogProbMetric: 28.1044

Epoch 490: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1657 - MinusLogProbMetric: 27.1657 - val_loss: 28.1044 - val_MinusLogProbMetric: 28.1044 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 491/1000
2023-10-27 00:23:20.331 
Epoch 491/1000 
	 loss: 27.1627, MinusLogProbMetric: 27.1627, val_loss: 28.1052, val_MinusLogProbMetric: 28.1052

Epoch 491: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1627 - MinusLogProbMetric: 27.1627 - val_loss: 28.1052 - val_MinusLogProbMetric: 28.1052 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 492/1000
2023-10-27 00:23:56.051 
Epoch 492/1000 
	 loss: 27.1631, MinusLogProbMetric: 27.1631, val_loss: 28.1420, val_MinusLogProbMetric: 28.1420

Epoch 492: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1631 - MinusLogProbMetric: 27.1631 - val_loss: 28.1420 - val_MinusLogProbMetric: 28.1420 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 493/1000
2023-10-27 00:24:31.471 
Epoch 493/1000 
	 loss: 27.1705, MinusLogProbMetric: 27.1705, val_loss: 28.1114, val_MinusLogProbMetric: 28.1114

Epoch 493: val_loss did not improve from 28.06508
196/196 - 35s - loss: 27.1705 - MinusLogProbMetric: 27.1705 - val_loss: 28.1114 - val_MinusLogProbMetric: 28.1114 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 494/1000
2023-10-27 00:25:07.095 
Epoch 494/1000 
	 loss: 27.1071, MinusLogProbMetric: 27.1071, val_loss: 28.0742, val_MinusLogProbMetric: 28.0742

Epoch 494: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1071 - MinusLogProbMetric: 27.1071 - val_loss: 28.0742 - val_MinusLogProbMetric: 28.0742 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 495/1000
2023-10-27 00:25:42.795 
Epoch 495/1000 
	 loss: 27.1054, MinusLogProbMetric: 27.1054, val_loss: 28.0697, val_MinusLogProbMetric: 28.0697

Epoch 495: val_loss did not improve from 28.06508
196/196 - 36s - loss: 27.1054 - MinusLogProbMetric: 27.1054 - val_loss: 28.0697 - val_MinusLogProbMetric: 28.0697 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 496/1000
2023-10-27 00:26:18.740 
Epoch 496/1000 
	 loss: 27.1085, MinusLogProbMetric: 27.1085, val_loss: 28.0641, val_MinusLogProbMetric: 28.0641

Epoch 496: val_loss improved from 28.06508 to 28.06408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 27.1085 - MinusLogProbMetric: 27.1085 - val_loss: 28.0641 - val_MinusLogProbMetric: 28.0641 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 497/1000
2023-10-27 00:26:55.392 
Epoch 497/1000 
	 loss: 27.1033, MinusLogProbMetric: 27.1033, val_loss: 28.0825, val_MinusLogProbMetric: 28.0825

Epoch 497: val_loss did not improve from 28.06408
196/196 - 36s - loss: 27.1033 - MinusLogProbMetric: 27.1033 - val_loss: 28.0825 - val_MinusLogProbMetric: 28.0825 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 498/1000
2023-10-27 00:27:31.660 
Epoch 498/1000 
	 loss: 27.1033, MinusLogProbMetric: 27.1033, val_loss: 28.0747, val_MinusLogProbMetric: 28.0747

Epoch 498: val_loss did not improve from 28.06408
196/196 - 36s - loss: 27.1033 - MinusLogProbMetric: 27.1033 - val_loss: 28.0747 - val_MinusLogProbMetric: 28.0747 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 499/1000
2023-10-27 00:28:07.706 
Epoch 499/1000 
	 loss: 27.1037, MinusLogProbMetric: 27.1037, val_loss: 28.0898, val_MinusLogProbMetric: 28.0898

Epoch 499: val_loss did not improve from 28.06408
196/196 - 36s - loss: 27.1037 - MinusLogProbMetric: 27.1037 - val_loss: 28.0898 - val_MinusLogProbMetric: 28.0898 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 500/1000
2023-10-27 00:28:43.768 
Epoch 500/1000 
	 loss: 27.1032, MinusLogProbMetric: 27.1032, val_loss: 28.0751, val_MinusLogProbMetric: 28.0751

Epoch 500: val_loss did not improve from 28.06408
196/196 - 36s - loss: 27.1032 - MinusLogProbMetric: 27.1032 - val_loss: 28.0751 - val_MinusLogProbMetric: 28.0751 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 501/1000
2023-10-27 00:29:19.956 
Epoch 501/1000 
	 loss: 27.1007, MinusLogProbMetric: 27.1007, val_loss: 28.0959, val_MinusLogProbMetric: 28.0959

Epoch 501: val_loss did not improve from 28.06408
196/196 - 36s - loss: 27.1007 - MinusLogProbMetric: 27.1007 - val_loss: 28.0959 - val_MinusLogProbMetric: 28.0959 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 502/1000
2023-10-27 00:29:55.597 
Epoch 502/1000 
	 loss: 27.1012, MinusLogProbMetric: 27.1012, val_loss: 28.0760, val_MinusLogProbMetric: 28.0760

Epoch 502: val_loss did not improve from 28.06408
196/196 - 36s - loss: 27.1012 - MinusLogProbMetric: 27.1012 - val_loss: 28.0760 - val_MinusLogProbMetric: 28.0760 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 503/1000
2023-10-27 00:30:31.170 
Epoch 503/1000 
	 loss: 27.1007, MinusLogProbMetric: 27.1007, val_loss: 28.0706, val_MinusLogProbMetric: 28.0706

Epoch 503: val_loss did not improve from 28.06408
196/196 - 36s - loss: 27.1007 - MinusLogProbMetric: 27.1007 - val_loss: 28.0706 - val_MinusLogProbMetric: 28.0706 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 504/1000
2023-10-27 00:31:07.207 
Epoch 504/1000 
	 loss: 27.1042, MinusLogProbMetric: 27.1042, val_loss: 28.0620, val_MinusLogProbMetric: 28.0620

Epoch 504: val_loss improved from 28.06408 to 28.06202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 27.1042 - MinusLogProbMetric: 27.1042 - val_loss: 28.0620 - val_MinusLogProbMetric: 28.0620 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 505/1000
2023-10-27 00:31:43.910 
Epoch 505/1000 
	 loss: 27.1024, MinusLogProbMetric: 27.1024, val_loss: 28.0839, val_MinusLogProbMetric: 28.0839

Epoch 505: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1024 - MinusLogProbMetric: 27.1024 - val_loss: 28.0839 - val_MinusLogProbMetric: 28.0839 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 506/1000
2023-10-27 00:32:19.902 
Epoch 506/1000 
	 loss: 27.1034, MinusLogProbMetric: 27.1034, val_loss: 28.0826, val_MinusLogProbMetric: 28.0826

Epoch 506: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1034 - MinusLogProbMetric: 27.1034 - val_loss: 28.0826 - val_MinusLogProbMetric: 28.0826 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 507/1000
2023-10-27 00:32:55.651 
Epoch 507/1000 
	 loss: 27.1068, MinusLogProbMetric: 27.1068, val_loss: 28.0973, val_MinusLogProbMetric: 28.0973

Epoch 507: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1068 - MinusLogProbMetric: 27.1068 - val_loss: 28.0973 - val_MinusLogProbMetric: 28.0973 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 508/1000
2023-10-27 00:33:31.328 
Epoch 508/1000 
	 loss: 27.1009, MinusLogProbMetric: 27.1009, val_loss: 28.0822, val_MinusLogProbMetric: 28.0822

Epoch 508: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1009 - MinusLogProbMetric: 27.1009 - val_loss: 28.0822 - val_MinusLogProbMetric: 28.0822 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 509/1000
2023-10-27 00:34:07.336 
Epoch 509/1000 
	 loss: 27.1015, MinusLogProbMetric: 27.1015, val_loss: 28.0664, val_MinusLogProbMetric: 28.0664

Epoch 509: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1015 - MinusLogProbMetric: 27.1015 - val_loss: 28.0664 - val_MinusLogProbMetric: 28.0664 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 510/1000
2023-10-27 00:34:43.237 
Epoch 510/1000 
	 loss: 27.1037, MinusLogProbMetric: 27.1037, val_loss: 28.0744, val_MinusLogProbMetric: 28.0744

Epoch 510: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1037 - MinusLogProbMetric: 27.1037 - val_loss: 28.0744 - val_MinusLogProbMetric: 28.0744 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 511/1000
2023-10-27 00:35:19.022 
Epoch 511/1000 
	 loss: 27.1010, MinusLogProbMetric: 27.1010, val_loss: 28.0840, val_MinusLogProbMetric: 28.0840

Epoch 511: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1010 - MinusLogProbMetric: 27.1010 - val_loss: 28.0840 - val_MinusLogProbMetric: 28.0840 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 512/1000
2023-10-27 00:35:54.206 
Epoch 512/1000 
	 loss: 27.0990, MinusLogProbMetric: 27.0990, val_loss: 28.0841, val_MinusLogProbMetric: 28.0841

Epoch 512: val_loss did not improve from 28.06202
196/196 - 35s - loss: 27.0990 - MinusLogProbMetric: 27.0990 - val_loss: 28.0841 - val_MinusLogProbMetric: 28.0841 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 513/1000
2023-10-27 00:36:29.968 
Epoch 513/1000 
	 loss: 27.1025, MinusLogProbMetric: 27.1025, val_loss: 28.0806, val_MinusLogProbMetric: 28.0806

Epoch 513: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1025 - MinusLogProbMetric: 27.1025 - val_loss: 28.0806 - val_MinusLogProbMetric: 28.0806 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 514/1000
2023-10-27 00:37:05.873 
Epoch 514/1000 
	 loss: 27.0995, MinusLogProbMetric: 27.0995, val_loss: 28.0709, val_MinusLogProbMetric: 28.0709

Epoch 514: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0995 - MinusLogProbMetric: 27.0995 - val_loss: 28.0709 - val_MinusLogProbMetric: 28.0709 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 515/1000
2023-10-27 00:37:41.527 
Epoch 515/1000 
	 loss: 27.1003, MinusLogProbMetric: 27.1003, val_loss: 28.0704, val_MinusLogProbMetric: 28.0704

Epoch 515: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1003 - MinusLogProbMetric: 27.1003 - val_loss: 28.0704 - val_MinusLogProbMetric: 28.0704 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 516/1000
2023-10-27 00:38:17.900 
Epoch 516/1000 
	 loss: 27.0952, MinusLogProbMetric: 27.0952, val_loss: 28.0785, val_MinusLogProbMetric: 28.0785

Epoch 516: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0952 - MinusLogProbMetric: 27.0952 - val_loss: 28.0785 - val_MinusLogProbMetric: 28.0785 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 517/1000
2023-10-27 00:38:53.625 
Epoch 517/1000 
	 loss: 27.0978, MinusLogProbMetric: 27.0978, val_loss: 28.0848, val_MinusLogProbMetric: 28.0848

Epoch 517: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0978 - MinusLogProbMetric: 27.0978 - val_loss: 28.0848 - val_MinusLogProbMetric: 28.0848 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 518/1000
2023-10-27 00:39:29.802 
Epoch 518/1000 
	 loss: 27.0986, MinusLogProbMetric: 27.0986, val_loss: 28.0947, val_MinusLogProbMetric: 28.0947

Epoch 518: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0986 - MinusLogProbMetric: 27.0986 - val_loss: 28.0947 - val_MinusLogProbMetric: 28.0947 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 519/1000
2023-10-27 00:40:05.723 
Epoch 519/1000 
	 loss: 27.0971, MinusLogProbMetric: 27.0971, val_loss: 28.1037, val_MinusLogProbMetric: 28.1037

Epoch 519: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0971 - MinusLogProbMetric: 27.0971 - val_loss: 28.1037 - val_MinusLogProbMetric: 28.1037 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 520/1000
2023-10-27 00:40:41.638 
Epoch 520/1000 
	 loss: 27.0975, MinusLogProbMetric: 27.0975, val_loss: 28.0923, val_MinusLogProbMetric: 28.0923

Epoch 520: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0975 - MinusLogProbMetric: 27.0975 - val_loss: 28.0923 - val_MinusLogProbMetric: 28.0923 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 521/1000
2023-10-27 00:41:17.521 
Epoch 521/1000 
	 loss: 27.1019, MinusLogProbMetric: 27.1019, val_loss: 28.0709, val_MinusLogProbMetric: 28.0709

Epoch 521: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1019 - MinusLogProbMetric: 27.1019 - val_loss: 28.0709 - val_MinusLogProbMetric: 28.0709 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 522/1000
2023-10-27 00:41:53.470 
Epoch 522/1000 
	 loss: 27.0967, MinusLogProbMetric: 27.0967, val_loss: 28.0975, val_MinusLogProbMetric: 28.0975

Epoch 522: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0967 - MinusLogProbMetric: 27.0967 - val_loss: 28.0975 - val_MinusLogProbMetric: 28.0975 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 523/1000
2023-10-27 00:42:29.325 
Epoch 523/1000 
	 loss: 27.1001, MinusLogProbMetric: 27.1001, val_loss: 28.0736, val_MinusLogProbMetric: 28.0736

Epoch 523: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1001 - MinusLogProbMetric: 27.1001 - val_loss: 28.0736 - val_MinusLogProbMetric: 28.0736 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 524/1000
2023-10-27 00:43:05.017 
Epoch 524/1000 
	 loss: 27.0946, MinusLogProbMetric: 27.0946, val_loss: 28.0861, val_MinusLogProbMetric: 28.0861

Epoch 524: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0946 - MinusLogProbMetric: 27.0946 - val_loss: 28.0861 - val_MinusLogProbMetric: 28.0861 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 525/1000
2023-10-27 00:43:41.064 
Epoch 525/1000 
	 loss: 27.1001, MinusLogProbMetric: 27.1001, val_loss: 28.0746, val_MinusLogProbMetric: 28.0746

Epoch 525: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.1001 - MinusLogProbMetric: 27.1001 - val_loss: 28.0746 - val_MinusLogProbMetric: 28.0746 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 526/1000
2023-10-27 00:44:17.204 
Epoch 526/1000 
	 loss: 27.0963, MinusLogProbMetric: 27.0963, val_loss: 28.0919, val_MinusLogProbMetric: 28.0919

Epoch 526: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0963 - MinusLogProbMetric: 27.0963 - val_loss: 28.0919 - val_MinusLogProbMetric: 28.0919 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 527/1000
2023-10-27 00:44:53.117 
Epoch 527/1000 
	 loss: 27.0982, MinusLogProbMetric: 27.0982, val_loss: 28.0963, val_MinusLogProbMetric: 28.0963

Epoch 527: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0982 - MinusLogProbMetric: 27.0982 - val_loss: 28.0963 - val_MinusLogProbMetric: 28.0963 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 528/1000
2023-10-27 00:45:28.964 
Epoch 528/1000 
	 loss: 27.0979, MinusLogProbMetric: 27.0979, val_loss: 28.0983, val_MinusLogProbMetric: 28.0983

Epoch 528: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0979 - MinusLogProbMetric: 27.0979 - val_loss: 28.0983 - val_MinusLogProbMetric: 28.0983 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 529/1000
2023-10-27 00:46:05.040 
Epoch 529/1000 
	 loss: 27.0946, MinusLogProbMetric: 27.0946, val_loss: 28.0872, val_MinusLogProbMetric: 28.0872

Epoch 529: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0946 - MinusLogProbMetric: 27.0946 - val_loss: 28.0872 - val_MinusLogProbMetric: 28.0872 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 530/1000
2023-10-27 00:46:41.172 
Epoch 530/1000 
	 loss: 27.0974, MinusLogProbMetric: 27.0974, val_loss: 28.0843, val_MinusLogProbMetric: 28.0843

Epoch 530: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0974 - MinusLogProbMetric: 27.0974 - val_loss: 28.0843 - val_MinusLogProbMetric: 28.0843 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 531/1000
2023-10-27 00:47:17.490 
Epoch 531/1000 
	 loss: 27.0973, MinusLogProbMetric: 27.0973, val_loss: 28.0815, val_MinusLogProbMetric: 28.0815

Epoch 531: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0973 - MinusLogProbMetric: 27.0973 - val_loss: 28.0815 - val_MinusLogProbMetric: 28.0815 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 532/1000
2023-10-27 00:47:52.982 
Epoch 532/1000 
	 loss: 27.0937, MinusLogProbMetric: 27.0937, val_loss: 28.0796, val_MinusLogProbMetric: 28.0796

Epoch 532: val_loss did not improve from 28.06202
196/196 - 35s - loss: 27.0937 - MinusLogProbMetric: 27.0937 - val_loss: 28.0796 - val_MinusLogProbMetric: 28.0796 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 533/1000
2023-10-27 00:48:28.665 
Epoch 533/1000 
	 loss: 27.0915, MinusLogProbMetric: 27.0915, val_loss: 28.0836, val_MinusLogProbMetric: 28.0836

Epoch 533: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0915 - MinusLogProbMetric: 27.0915 - val_loss: 28.0836 - val_MinusLogProbMetric: 28.0836 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 534/1000
2023-10-27 00:49:04.970 
Epoch 534/1000 
	 loss: 27.0894, MinusLogProbMetric: 27.0894, val_loss: 28.0814, val_MinusLogProbMetric: 28.0814

Epoch 534: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0894 - MinusLogProbMetric: 27.0894 - val_loss: 28.0814 - val_MinusLogProbMetric: 28.0814 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 535/1000
2023-10-27 00:49:41.364 
Epoch 535/1000 
	 loss: 27.0972, MinusLogProbMetric: 27.0972, val_loss: 28.0928, val_MinusLogProbMetric: 28.0928

Epoch 535: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0972 - MinusLogProbMetric: 27.0972 - val_loss: 28.0928 - val_MinusLogProbMetric: 28.0928 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 536/1000
2023-10-27 00:50:16.839 
Epoch 536/1000 
	 loss: 27.0949, MinusLogProbMetric: 27.0949, val_loss: 28.0689, val_MinusLogProbMetric: 28.0689

Epoch 536: val_loss did not improve from 28.06202
196/196 - 35s - loss: 27.0949 - MinusLogProbMetric: 27.0949 - val_loss: 28.0689 - val_MinusLogProbMetric: 28.0689 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 537/1000
2023-10-27 00:50:52.894 
Epoch 537/1000 
	 loss: 27.0934, MinusLogProbMetric: 27.0934, val_loss: 28.0923, val_MinusLogProbMetric: 28.0923

Epoch 537: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0934 - MinusLogProbMetric: 27.0934 - val_loss: 28.0923 - val_MinusLogProbMetric: 28.0923 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 538/1000
2023-10-27 00:51:28.701 
Epoch 538/1000 
	 loss: 27.0933, MinusLogProbMetric: 27.0933, val_loss: 28.0808, val_MinusLogProbMetric: 28.0808

Epoch 538: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0933 - MinusLogProbMetric: 27.0933 - val_loss: 28.0808 - val_MinusLogProbMetric: 28.0808 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 539/1000
2023-10-27 00:52:04.865 
Epoch 539/1000 
	 loss: 27.0985, MinusLogProbMetric: 27.0985, val_loss: 28.0767, val_MinusLogProbMetric: 28.0767

Epoch 539: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0985 - MinusLogProbMetric: 27.0985 - val_loss: 28.0767 - val_MinusLogProbMetric: 28.0767 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 540/1000
2023-10-27 00:52:40.394 
Epoch 540/1000 
	 loss: 27.0964, MinusLogProbMetric: 27.0964, val_loss: 28.0807, val_MinusLogProbMetric: 28.0807

Epoch 540: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0964 - MinusLogProbMetric: 27.0964 - val_loss: 28.0807 - val_MinusLogProbMetric: 28.0807 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 541/1000
2023-10-27 00:53:16.458 
Epoch 541/1000 
	 loss: 27.0932, MinusLogProbMetric: 27.0932, val_loss: 28.0860, val_MinusLogProbMetric: 28.0860

Epoch 541: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0932 - MinusLogProbMetric: 27.0932 - val_loss: 28.0860 - val_MinusLogProbMetric: 28.0860 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 542/1000
2023-10-27 00:53:52.532 
Epoch 542/1000 
	 loss: 27.0887, MinusLogProbMetric: 27.0887, val_loss: 28.0765, val_MinusLogProbMetric: 28.0765

Epoch 542: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0887 - MinusLogProbMetric: 27.0887 - val_loss: 28.0765 - val_MinusLogProbMetric: 28.0765 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 543/1000
2023-10-27 00:54:28.415 
Epoch 543/1000 
	 loss: 27.0933, MinusLogProbMetric: 27.0933, val_loss: 28.0913, val_MinusLogProbMetric: 28.0913

Epoch 543: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0933 - MinusLogProbMetric: 27.0933 - val_loss: 28.0913 - val_MinusLogProbMetric: 28.0913 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 544/1000
2023-10-27 00:55:04.618 
Epoch 544/1000 
	 loss: 27.0924, MinusLogProbMetric: 27.0924, val_loss: 28.0913, val_MinusLogProbMetric: 28.0913

Epoch 544: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0924 - MinusLogProbMetric: 27.0924 - val_loss: 28.0913 - val_MinusLogProbMetric: 28.0913 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 545/1000
2023-10-27 00:55:40.903 
Epoch 545/1000 
	 loss: 27.0899, MinusLogProbMetric: 27.0899, val_loss: 28.0996, val_MinusLogProbMetric: 28.0996

Epoch 545: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0899 - MinusLogProbMetric: 27.0899 - val_loss: 28.0996 - val_MinusLogProbMetric: 28.0996 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 546/1000
2023-10-27 00:56:16.823 
Epoch 546/1000 
	 loss: 27.0931, MinusLogProbMetric: 27.0931, val_loss: 28.1010, val_MinusLogProbMetric: 28.1010

Epoch 546: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0931 - MinusLogProbMetric: 27.0931 - val_loss: 28.1010 - val_MinusLogProbMetric: 28.1010 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 547/1000
2023-10-27 00:56:52.571 
Epoch 547/1000 
	 loss: 27.0953, MinusLogProbMetric: 27.0953, val_loss: 28.0917, val_MinusLogProbMetric: 28.0917

Epoch 547: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0953 - MinusLogProbMetric: 27.0953 - val_loss: 28.0917 - val_MinusLogProbMetric: 28.0917 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 548/1000
2023-10-27 00:57:28.671 
Epoch 548/1000 
	 loss: 27.0925, MinusLogProbMetric: 27.0925, val_loss: 28.0926, val_MinusLogProbMetric: 28.0926

Epoch 548: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0925 - MinusLogProbMetric: 27.0925 - val_loss: 28.0926 - val_MinusLogProbMetric: 28.0926 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 549/1000
2023-10-27 00:58:04.555 
Epoch 549/1000 
	 loss: 27.0922, MinusLogProbMetric: 27.0922, val_loss: 28.0842, val_MinusLogProbMetric: 28.0842

Epoch 549: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0922 - MinusLogProbMetric: 27.0922 - val_loss: 28.0842 - val_MinusLogProbMetric: 28.0842 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 550/1000
2023-10-27 00:58:40.134 
Epoch 550/1000 
	 loss: 27.0889, MinusLogProbMetric: 27.0889, val_loss: 28.0968, val_MinusLogProbMetric: 28.0968

Epoch 550: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0889 - MinusLogProbMetric: 27.0889 - val_loss: 28.0968 - val_MinusLogProbMetric: 28.0968 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 551/1000
2023-10-27 00:59:16.148 
Epoch 551/1000 
	 loss: 27.0908, MinusLogProbMetric: 27.0908, val_loss: 28.0710, val_MinusLogProbMetric: 28.0710

Epoch 551: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0908 - MinusLogProbMetric: 27.0908 - val_loss: 28.0710 - val_MinusLogProbMetric: 28.0710 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 552/1000
2023-10-27 00:59:52.238 
Epoch 552/1000 
	 loss: 27.0918, MinusLogProbMetric: 27.0918, val_loss: 28.0769, val_MinusLogProbMetric: 28.0769

Epoch 552: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0918 - MinusLogProbMetric: 27.0918 - val_loss: 28.0769 - val_MinusLogProbMetric: 28.0769 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 553/1000
2023-10-27 01:00:28.305 
Epoch 553/1000 
	 loss: 27.0904, MinusLogProbMetric: 27.0904, val_loss: 28.0736, val_MinusLogProbMetric: 28.0736

Epoch 553: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0904 - MinusLogProbMetric: 27.0904 - val_loss: 28.0736 - val_MinusLogProbMetric: 28.0736 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 554/1000
2023-10-27 01:01:04.168 
Epoch 554/1000 
	 loss: 27.0911, MinusLogProbMetric: 27.0911, val_loss: 28.0870, val_MinusLogProbMetric: 28.0870

Epoch 554: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0911 - MinusLogProbMetric: 27.0911 - val_loss: 28.0870 - val_MinusLogProbMetric: 28.0870 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 555/1000
2023-10-27 01:01:40.153 
Epoch 555/1000 
	 loss: 27.0623, MinusLogProbMetric: 27.0623, val_loss: 28.0621, val_MinusLogProbMetric: 28.0621

Epoch 555: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0623 - MinusLogProbMetric: 27.0623 - val_loss: 28.0621 - val_MinusLogProbMetric: 28.0621 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 556/1000
2023-10-27 01:02:16.454 
Epoch 556/1000 
	 loss: 27.0590, MinusLogProbMetric: 27.0590, val_loss: 28.0722, val_MinusLogProbMetric: 28.0722

Epoch 556: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0590 - MinusLogProbMetric: 27.0590 - val_loss: 28.0722 - val_MinusLogProbMetric: 28.0722 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 557/1000
2023-10-27 01:02:52.891 
Epoch 557/1000 
	 loss: 27.0608, MinusLogProbMetric: 27.0608, val_loss: 28.0749, val_MinusLogProbMetric: 28.0749

Epoch 557: val_loss did not improve from 28.06202
196/196 - 36s - loss: 27.0608 - MinusLogProbMetric: 27.0608 - val_loss: 28.0749 - val_MinusLogProbMetric: 28.0749 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 558/1000
2023-10-27 01:03:28.822 
Epoch 558/1000 
	 loss: 27.0610, MinusLogProbMetric: 27.0610, val_loss: 28.0594, val_MinusLogProbMetric: 28.0594

Epoch 558: val_loss improved from 28.06202 to 28.05936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_394/weights/best_weights.h5
196/196 - 37s - loss: 27.0610 - MinusLogProbMetric: 27.0610 - val_loss: 28.0594 - val_MinusLogProbMetric: 28.0594 - lr: 6.2500e-05 - 37s/epoch - 187ms/step
Epoch 559/1000
2023-10-27 01:04:05.618 
Epoch 559/1000 
	 loss: 27.0602, MinusLogProbMetric: 27.0602, val_loss: 28.0716, val_MinusLogProbMetric: 28.0716

Epoch 559: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0602 - MinusLogProbMetric: 27.0602 - val_loss: 28.0716 - val_MinusLogProbMetric: 28.0716 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 560/1000
2023-10-27 01:04:41.946 
Epoch 560/1000 
	 loss: 27.0612, MinusLogProbMetric: 27.0612, val_loss: 28.0685, val_MinusLogProbMetric: 28.0685

Epoch 560: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0612 - MinusLogProbMetric: 27.0612 - val_loss: 28.0685 - val_MinusLogProbMetric: 28.0685 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 561/1000
2023-10-27 01:05:18.136 
Epoch 561/1000 
	 loss: 27.0589, MinusLogProbMetric: 27.0589, val_loss: 28.0732, val_MinusLogProbMetric: 28.0732

Epoch 561: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0589 - MinusLogProbMetric: 27.0589 - val_loss: 28.0732 - val_MinusLogProbMetric: 28.0732 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 562/1000
2023-10-27 01:05:53.708 
Epoch 562/1000 
	 loss: 27.0606, MinusLogProbMetric: 27.0606, val_loss: 28.0607, val_MinusLogProbMetric: 28.0607

Epoch 562: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0606 - MinusLogProbMetric: 27.0606 - val_loss: 28.0607 - val_MinusLogProbMetric: 28.0607 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 563/1000
2023-10-27 01:06:29.736 
Epoch 563/1000 
	 loss: 27.0579, MinusLogProbMetric: 27.0579, val_loss: 28.0756, val_MinusLogProbMetric: 28.0756

Epoch 563: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0579 - MinusLogProbMetric: 27.0579 - val_loss: 28.0756 - val_MinusLogProbMetric: 28.0756 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 564/1000
2023-10-27 01:07:05.506 
Epoch 564/1000 
	 loss: 27.0611, MinusLogProbMetric: 27.0611, val_loss: 28.0759, val_MinusLogProbMetric: 28.0759

Epoch 564: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0611 - MinusLogProbMetric: 27.0611 - val_loss: 28.0759 - val_MinusLogProbMetric: 28.0759 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 565/1000
2023-10-27 01:07:41.309 
Epoch 565/1000 
	 loss: 27.0572, MinusLogProbMetric: 27.0572, val_loss: 28.0619, val_MinusLogProbMetric: 28.0619

Epoch 565: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0572 - MinusLogProbMetric: 27.0572 - val_loss: 28.0619 - val_MinusLogProbMetric: 28.0619 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 566/1000
2023-10-27 01:08:17.799 
Epoch 566/1000 
	 loss: 27.0581, MinusLogProbMetric: 27.0581, val_loss: 28.0698, val_MinusLogProbMetric: 28.0698

Epoch 566: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0581 - MinusLogProbMetric: 27.0581 - val_loss: 28.0698 - val_MinusLogProbMetric: 28.0698 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 567/1000
2023-10-27 01:08:53.920 
Epoch 567/1000 
	 loss: 27.0587, MinusLogProbMetric: 27.0587, val_loss: 28.0685, val_MinusLogProbMetric: 28.0685

Epoch 567: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0587 - MinusLogProbMetric: 27.0587 - val_loss: 28.0685 - val_MinusLogProbMetric: 28.0685 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 568/1000
2023-10-27 01:09:30.577 
Epoch 568/1000 
	 loss: 27.0595, MinusLogProbMetric: 27.0595, val_loss: 28.0824, val_MinusLogProbMetric: 28.0824

Epoch 568: val_loss did not improve from 28.05936
196/196 - 37s - loss: 27.0595 - MinusLogProbMetric: 27.0595 - val_loss: 28.0824 - val_MinusLogProbMetric: 28.0824 - lr: 6.2500e-05 - 37s/epoch - 187ms/step
Epoch 569/1000
2023-10-27 01:10:06.161 
Epoch 569/1000 
	 loss: 27.0585, MinusLogProbMetric: 27.0585, val_loss: 28.0681, val_MinusLogProbMetric: 28.0681

Epoch 569: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0585 - MinusLogProbMetric: 27.0585 - val_loss: 28.0681 - val_MinusLogProbMetric: 28.0681 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 570/1000
2023-10-27 01:10:42.263 
Epoch 570/1000 
	 loss: 27.0573, MinusLogProbMetric: 27.0573, val_loss: 28.0704, val_MinusLogProbMetric: 28.0704

Epoch 570: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0573 - MinusLogProbMetric: 27.0573 - val_loss: 28.0704 - val_MinusLogProbMetric: 28.0704 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 571/1000
2023-10-27 01:11:18.622 
Epoch 571/1000 
	 loss: 27.0567, MinusLogProbMetric: 27.0567, val_loss: 28.0743, val_MinusLogProbMetric: 28.0743

Epoch 571: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0567 - MinusLogProbMetric: 27.0567 - val_loss: 28.0743 - val_MinusLogProbMetric: 28.0743 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 572/1000
2023-10-27 01:11:54.588 
Epoch 572/1000 
	 loss: 27.0606, MinusLogProbMetric: 27.0606, val_loss: 28.0675, val_MinusLogProbMetric: 28.0675

Epoch 572: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0606 - MinusLogProbMetric: 27.0606 - val_loss: 28.0675 - val_MinusLogProbMetric: 28.0675 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 573/1000
2023-10-27 01:12:30.192 
Epoch 573/1000 
	 loss: 27.0574, MinusLogProbMetric: 27.0574, val_loss: 28.0758, val_MinusLogProbMetric: 28.0758

Epoch 573: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0574 - MinusLogProbMetric: 27.0574 - val_loss: 28.0758 - val_MinusLogProbMetric: 28.0758 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 574/1000
2023-10-27 01:13:05.946 
Epoch 574/1000 
	 loss: 27.0561, MinusLogProbMetric: 27.0561, val_loss: 28.0670, val_MinusLogProbMetric: 28.0670

Epoch 574: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0561 - MinusLogProbMetric: 27.0561 - val_loss: 28.0670 - val_MinusLogProbMetric: 28.0670 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 575/1000
2023-10-27 01:13:42.188 
Epoch 575/1000 
	 loss: 27.0598, MinusLogProbMetric: 27.0598, val_loss: 28.0614, val_MinusLogProbMetric: 28.0614

Epoch 575: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0598 - MinusLogProbMetric: 27.0598 - val_loss: 28.0614 - val_MinusLogProbMetric: 28.0614 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 576/1000
2023-10-27 01:14:17.570 
Epoch 576/1000 
	 loss: 27.0574, MinusLogProbMetric: 27.0574, val_loss: 28.0714, val_MinusLogProbMetric: 28.0714

Epoch 576: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0574 - MinusLogProbMetric: 27.0574 - val_loss: 28.0714 - val_MinusLogProbMetric: 28.0714 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 577/1000
2023-10-27 01:14:53.546 
Epoch 577/1000 
	 loss: 27.0566, MinusLogProbMetric: 27.0566, val_loss: 28.0634, val_MinusLogProbMetric: 28.0634

Epoch 577: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0566 - MinusLogProbMetric: 27.0566 - val_loss: 28.0634 - val_MinusLogProbMetric: 28.0634 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 578/1000
2023-10-27 01:15:29.708 
Epoch 578/1000 
	 loss: 27.0582, MinusLogProbMetric: 27.0582, val_loss: 28.0755, val_MinusLogProbMetric: 28.0755

Epoch 578: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0582 - MinusLogProbMetric: 27.0582 - val_loss: 28.0755 - val_MinusLogProbMetric: 28.0755 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 579/1000
2023-10-27 01:16:05.832 
Epoch 579/1000 
	 loss: 27.0579, MinusLogProbMetric: 27.0579, val_loss: 28.0723, val_MinusLogProbMetric: 28.0723

Epoch 579: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0579 - MinusLogProbMetric: 27.0579 - val_loss: 28.0723 - val_MinusLogProbMetric: 28.0723 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 580/1000
2023-10-27 01:16:41.908 
Epoch 580/1000 
	 loss: 27.0577, MinusLogProbMetric: 27.0577, val_loss: 28.0830, val_MinusLogProbMetric: 28.0830

Epoch 580: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0577 - MinusLogProbMetric: 27.0577 - val_loss: 28.0830 - val_MinusLogProbMetric: 28.0830 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 581/1000
2023-10-27 01:17:18.133 
Epoch 581/1000 
	 loss: 27.0570, MinusLogProbMetric: 27.0570, val_loss: 28.0767, val_MinusLogProbMetric: 28.0767

Epoch 581: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0570 - MinusLogProbMetric: 27.0570 - val_loss: 28.0767 - val_MinusLogProbMetric: 28.0767 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 582/1000
2023-10-27 01:17:54.022 
Epoch 582/1000 
	 loss: 27.0557, MinusLogProbMetric: 27.0557, val_loss: 28.0639, val_MinusLogProbMetric: 28.0639

Epoch 582: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0557 - MinusLogProbMetric: 27.0557 - val_loss: 28.0639 - val_MinusLogProbMetric: 28.0639 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 583/1000
2023-10-27 01:18:30.210 
Epoch 583/1000 
	 loss: 27.0569, MinusLogProbMetric: 27.0569, val_loss: 28.0791, val_MinusLogProbMetric: 28.0791

Epoch 583: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0569 - MinusLogProbMetric: 27.0569 - val_loss: 28.0791 - val_MinusLogProbMetric: 28.0791 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 584/1000
2023-10-27 01:19:06.070 
Epoch 584/1000 
	 loss: 27.0557, MinusLogProbMetric: 27.0557, val_loss: 28.0696, val_MinusLogProbMetric: 28.0696

Epoch 584: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0557 - MinusLogProbMetric: 27.0557 - val_loss: 28.0696 - val_MinusLogProbMetric: 28.0696 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 585/1000
2023-10-27 01:19:41.390 
Epoch 585/1000 
	 loss: 27.0576, MinusLogProbMetric: 27.0576, val_loss: 28.0810, val_MinusLogProbMetric: 28.0810

Epoch 585: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0576 - MinusLogProbMetric: 27.0576 - val_loss: 28.0810 - val_MinusLogProbMetric: 28.0810 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 586/1000
2023-10-27 01:20:17.676 
Epoch 586/1000 
	 loss: 27.0571, MinusLogProbMetric: 27.0571, val_loss: 28.0728, val_MinusLogProbMetric: 28.0728

Epoch 586: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0571 - MinusLogProbMetric: 27.0571 - val_loss: 28.0728 - val_MinusLogProbMetric: 28.0728 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 587/1000
2023-10-27 01:20:53.669 
Epoch 587/1000 
	 loss: 27.0546, MinusLogProbMetric: 27.0546, val_loss: 28.0679, val_MinusLogProbMetric: 28.0679

Epoch 587: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0546 - MinusLogProbMetric: 27.0546 - val_loss: 28.0679 - val_MinusLogProbMetric: 28.0679 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 588/1000
2023-10-27 01:21:29.063 
Epoch 588/1000 
	 loss: 27.0559, MinusLogProbMetric: 27.0559, val_loss: 28.0657, val_MinusLogProbMetric: 28.0657

Epoch 588: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0559 - MinusLogProbMetric: 27.0559 - val_loss: 28.0657 - val_MinusLogProbMetric: 28.0657 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 589/1000
2023-10-27 01:22:05.182 
Epoch 589/1000 
	 loss: 27.0545, MinusLogProbMetric: 27.0545, val_loss: 28.0704, val_MinusLogProbMetric: 28.0704

Epoch 589: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0545 - MinusLogProbMetric: 27.0545 - val_loss: 28.0704 - val_MinusLogProbMetric: 28.0704 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 590/1000
2023-10-27 01:22:41.019 
Epoch 590/1000 
	 loss: 27.0538, MinusLogProbMetric: 27.0538, val_loss: 28.0757, val_MinusLogProbMetric: 28.0757

Epoch 590: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0538 - MinusLogProbMetric: 27.0538 - val_loss: 28.0757 - val_MinusLogProbMetric: 28.0757 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 591/1000
2023-10-27 01:23:17.150 
Epoch 591/1000 
	 loss: 27.0572, MinusLogProbMetric: 27.0572, val_loss: 28.0869, val_MinusLogProbMetric: 28.0869

Epoch 591: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0572 - MinusLogProbMetric: 27.0572 - val_loss: 28.0869 - val_MinusLogProbMetric: 28.0869 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 592/1000
2023-10-27 01:23:52.817 
Epoch 592/1000 
	 loss: 27.0541, MinusLogProbMetric: 27.0541, val_loss: 28.0724, val_MinusLogProbMetric: 28.0724

Epoch 592: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0541 - MinusLogProbMetric: 27.0541 - val_loss: 28.0724 - val_MinusLogProbMetric: 28.0724 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 593/1000
2023-10-27 01:24:28.659 
Epoch 593/1000 
	 loss: 27.0551, MinusLogProbMetric: 27.0551, val_loss: 28.0730, val_MinusLogProbMetric: 28.0730

Epoch 593: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0551 - MinusLogProbMetric: 27.0551 - val_loss: 28.0730 - val_MinusLogProbMetric: 28.0730 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 594/1000
2023-10-27 01:25:04.558 
Epoch 594/1000 
	 loss: 27.0539, MinusLogProbMetric: 27.0539, val_loss: 28.0804, val_MinusLogProbMetric: 28.0804

Epoch 594: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0539 - MinusLogProbMetric: 27.0539 - val_loss: 28.0804 - val_MinusLogProbMetric: 28.0804 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 595/1000
2023-10-27 01:25:40.580 
Epoch 595/1000 
	 loss: 27.0534, MinusLogProbMetric: 27.0534, val_loss: 28.0700, val_MinusLogProbMetric: 28.0700

Epoch 595: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0534 - MinusLogProbMetric: 27.0534 - val_loss: 28.0700 - val_MinusLogProbMetric: 28.0700 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 596/1000
2023-10-27 01:26:16.645 
Epoch 596/1000 
	 loss: 27.0544, MinusLogProbMetric: 27.0544, val_loss: 28.0706, val_MinusLogProbMetric: 28.0706

Epoch 596: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0544 - MinusLogProbMetric: 27.0544 - val_loss: 28.0706 - val_MinusLogProbMetric: 28.0706 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 597/1000
2023-10-27 01:26:52.210 
Epoch 597/1000 
	 loss: 27.0549, MinusLogProbMetric: 27.0549, val_loss: 28.0704, val_MinusLogProbMetric: 28.0704

Epoch 597: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0549 - MinusLogProbMetric: 27.0549 - val_loss: 28.0704 - val_MinusLogProbMetric: 28.0704 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 598/1000
2023-10-27 01:27:28.189 
Epoch 598/1000 
	 loss: 27.0547, MinusLogProbMetric: 27.0547, val_loss: 28.0692, val_MinusLogProbMetric: 28.0692

Epoch 598: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0547 - MinusLogProbMetric: 27.0547 - val_loss: 28.0692 - val_MinusLogProbMetric: 28.0692 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 599/1000
2023-10-27 01:28:04.010 
Epoch 599/1000 
	 loss: 27.0541, MinusLogProbMetric: 27.0541, val_loss: 28.0741, val_MinusLogProbMetric: 28.0741

Epoch 599: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0541 - MinusLogProbMetric: 27.0541 - val_loss: 28.0741 - val_MinusLogProbMetric: 28.0741 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 600/1000
2023-10-27 01:28:39.759 
Epoch 600/1000 
	 loss: 27.0539, MinusLogProbMetric: 27.0539, val_loss: 28.0742, val_MinusLogProbMetric: 28.0742

Epoch 600: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0539 - MinusLogProbMetric: 27.0539 - val_loss: 28.0742 - val_MinusLogProbMetric: 28.0742 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 601/1000
2023-10-27 01:29:15.457 
Epoch 601/1000 
	 loss: 27.0537, MinusLogProbMetric: 27.0537, val_loss: 28.0736, val_MinusLogProbMetric: 28.0736

Epoch 601: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0537 - MinusLogProbMetric: 27.0537 - val_loss: 28.0736 - val_MinusLogProbMetric: 28.0736 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 602/1000
2023-10-27 01:29:51.263 
Epoch 602/1000 
	 loss: 27.0541, MinusLogProbMetric: 27.0541, val_loss: 28.0701, val_MinusLogProbMetric: 28.0701

Epoch 602: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0541 - MinusLogProbMetric: 27.0541 - val_loss: 28.0701 - val_MinusLogProbMetric: 28.0701 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 603/1000
2023-10-27 01:30:26.730 
Epoch 603/1000 
	 loss: 27.0522, MinusLogProbMetric: 27.0522, val_loss: 28.0804, val_MinusLogProbMetric: 28.0804

Epoch 603: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0522 - MinusLogProbMetric: 27.0522 - val_loss: 28.0804 - val_MinusLogProbMetric: 28.0804 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 604/1000
2023-10-27 01:31:02.420 
Epoch 604/1000 
	 loss: 27.0539, MinusLogProbMetric: 27.0539, val_loss: 28.0686, val_MinusLogProbMetric: 28.0686

Epoch 604: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0539 - MinusLogProbMetric: 27.0539 - val_loss: 28.0686 - val_MinusLogProbMetric: 28.0686 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 605/1000
2023-10-27 01:31:38.360 
Epoch 605/1000 
	 loss: 27.0546, MinusLogProbMetric: 27.0546, val_loss: 28.0724, val_MinusLogProbMetric: 28.0724

Epoch 605: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0546 - MinusLogProbMetric: 27.0546 - val_loss: 28.0724 - val_MinusLogProbMetric: 28.0724 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 606/1000
2023-10-27 01:32:14.507 
Epoch 606/1000 
	 loss: 27.0524, MinusLogProbMetric: 27.0524, val_loss: 28.0757, val_MinusLogProbMetric: 28.0757

Epoch 606: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0524 - MinusLogProbMetric: 27.0524 - val_loss: 28.0757 - val_MinusLogProbMetric: 28.0757 - lr: 6.2500e-05 - 36s/epoch - 184ms/step
Epoch 607/1000
2023-10-27 01:32:50.367 
Epoch 607/1000 
	 loss: 27.0545, MinusLogProbMetric: 27.0545, val_loss: 28.0711, val_MinusLogProbMetric: 28.0711

Epoch 607: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0545 - MinusLogProbMetric: 27.0545 - val_loss: 28.0711 - val_MinusLogProbMetric: 28.0711 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 608/1000
2023-10-27 01:33:25.819 
Epoch 608/1000 
	 loss: 27.0535, MinusLogProbMetric: 27.0535, val_loss: 28.0710, val_MinusLogProbMetric: 28.0710

Epoch 608: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0535 - MinusLogProbMetric: 27.0535 - val_loss: 28.0710 - val_MinusLogProbMetric: 28.0710 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 609/1000
2023-10-27 01:34:02.158 
Epoch 609/1000 
	 loss: 27.0390, MinusLogProbMetric: 27.0390, val_loss: 28.0654, val_MinusLogProbMetric: 28.0654

Epoch 609: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0390 - MinusLogProbMetric: 27.0390 - val_loss: 28.0654 - val_MinusLogProbMetric: 28.0654 - lr: 3.1250e-05 - 36s/epoch - 185ms/step
Epoch 610/1000
2023-10-27 01:34:37.814 
Epoch 610/1000 
	 loss: 27.0387, MinusLogProbMetric: 27.0387, val_loss: 28.0694, val_MinusLogProbMetric: 28.0694

Epoch 610: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0387 - MinusLogProbMetric: 27.0387 - val_loss: 28.0694 - val_MinusLogProbMetric: 28.0694 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 611/1000
2023-10-27 01:35:13.858 
Epoch 611/1000 
	 loss: 27.0378, MinusLogProbMetric: 27.0378, val_loss: 28.0623, val_MinusLogProbMetric: 28.0623

Epoch 611: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0378 - MinusLogProbMetric: 27.0378 - val_loss: 28.0623 - val_MinusLogProbMetric: 28.0623 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 612/1000
2023-10-27 01:35:50.270 
Epoch 612/1000 
	 loss: 27.0389, MinusLogProbMetric: 27.0389, val_loss: 28.0610, val_MinusLogProbMetric: 28.0610

Epoch 612: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0389 - MinusLogProbMetric: 27.0389 - val_loss: 28.0610 - val_MinusLogProbMetric: 28.0610 - lr: 3.1250e-05 - 36s/epoch - 186ms/step
Epoch 613/1000
2023-10-27 01:36:25.844 
Epoch 613/1000 
	 loss: 27.0397, MinusLogProbMetric: 27.0397, val_loss: 28.0628, val_MinusLogProbMetric: 28.0628

Epoch 613: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0397 - MinusLogProbMetric: 27.0397 - val_loss: 28.0628 - val_MinusLogProbMetric: 28.0628 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 614/1000
2023-10-27 01:37:02.101 
Epoch 614/1000 
	 loss: 27.0388, MinusLogProbMetric: 27.0388, val_loss: 28.0636, val_MinusLogProbMetric: 28.0636

Epoch 614: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0388 - MinusLogProbMetric: 27.0388 - val_loss: 28.0636 - val_MinusLogProbMetric: 28.0636 - lr: 3.1250e-05 - 36s/epoch - 185ms/step
Epoch 615/1000
2023-10-27 01:37:37.643 
Epoch 615/1000 
	 loss: 27.0391, MinusLogProbMetric: 27.0391, val_loss: 28.0650, val_MinusLogProbMetric: 28.0650

Epoch 615: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0391 - MinusLogProbMetric: 27.0391 - val_loss: 28.0650 - val_MinusLogProbMetric: 28.0650 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 616/1000
2023-10-27 01:38:13.202 
Epoch 616/1000 
	 loss: 27.0392, MinusLogProbMetric: 27.0392, val_loss: 28.0650, val_MinusLogProbMetric: 28.0650

Epoch 616: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0392 - MinusLogProbMetric: 27.0392 - val_loss: 28.0650 - val_MinusLogProbMetric: 28.0650 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 617/1000
2023-10-27 01:38:49.224 
Epoch 617/1000 
	 loss: 27.0395, MinusLogProbMetric: 27.0395, val_loss: 28.0635, val_MinusLogProbMetric: 28.0635

Epoch 617: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0395 - MinusLogProbMetric: 27.0395 - val_loss: 28.0635 - val_MinusLogProbMetric: 28.0635 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 618/1000
2023-10-27 01:39:25.271 
Epoch 618/1000 
	 loss: 27.0378, MinusLogProbMetric: 27.0378, val_loss: 28.0615, val_MinusLogProbMetric: 28.0615

Epoch 618: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0378 - MinusLogProbMetric: 27.0378 - val_loss: 28.0615 - val_MinusLogProbMetric: 28.0615 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 619/1000
2023-10-27 01:40:01.055 
Epoch 619/1000 
	 loss: 27.0382, MinusLogProbMetric: 27.0382, val_loss: 28.0657, val_MinusLogProbMetric: 28.0657

Epoch 619: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0382 - MinusLogProbMetric: 27.0382 - val_loss: 28.0657 - val_MinusLogProbMetric: 28.0657 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 620/1000
2023-10-27 01:40:36.491 
Epoch 620/1000 
	 loss: 27.0396, MinusLogProbMetric: 27.0396, val_loss: 28.0646, val_MinusLogProbMetric: 28.0646

Epoch 620: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0396 - MinusLogProbMetric: 27.0396 - val_loss: 28.0646 - val_MinusLogProbMetric: 28.0646 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 621/1000
2023-10-27 01:41:11.675 
Epoch 621/1000 
	 loss: 27.0393, MinusLogProbMetric: 27.0393, val_loss: 28.0644, val_MinusLogProbMetric: 28.0644

Epoch 621: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0393 - MinusLogProbMetric: 27.0393 - val_loss: 28.0644 - val_MinusLogProbMetric: 28.0644 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 622/1000
2023-10-27 01:41:47.272 
Epoch 622/1000 
	 loss: 27.0391, MinusLogProbMetric: 27.0391, val_loss: 28.0646, val_MinusLogProbMetric: 28.0646

Epoch 622: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0391 - MinusLogProbMetric: 27.0391 - val_loss: 28.0646 - val_MinusLogProbMetric: 28.0646 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 623/1000
2023-10-27 01:42:23.088 
Epoch 623/1000 
	 loss: 27.0399, MinusLogProbMetric: 27.0399, val_loss: 28.0715, val_MinusLogProbMetric: 28.0715

Epoch 623: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0399 - MinusLogProbMetric: 27.0399 - val_loss: 28.0715 - val_MinusLogProbMetric: 28.0715 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 624/1000
2023-10-27 01:42:58.886 
Epoch 624/1000 
	 loss: 27.0394, MinusLogProbMetric: 27.0394, val_loss: 28.0668, val_MinusLogProbMetric: 28.0668

Epoch 624: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0394 - MinusLogProbMetric: 27.0394 - val_loss: 28.0668 - val_MinusLogProbMetric: 28.0668 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 625/1000
2023-10-27 01:43:34.571 
Epoch 625/1000 
	 loss: 27.0388, MinusLogProbMetric: 27.0388, val_loss: 28.0655, val_MinusLogProbMetric: 28.0655

Epoch 625: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0388 - MinusLogProbMetric: 27.0388 - val_loss: 28.0655 - val_MinusLogProbMetric: 28.0655 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 626/1000
2023-10-27 01:44:10.066 
Epoch 626/1000 
	 loss: 27.0382, MinusLogProbMetric: 27.0382, val_loss: 28.0697, val_MinusLogProbMetric: 28.0697

Epoch 626: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0382 - MinusLogProbMetric: 27.0382 - val_loss: 28.0697 - val_MinusLogProbMetric: 28.0697 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 627/1000
2023-10-27 01:44:45.593 
Epoch 627/1000 
	 loss: 27.0394, MinusLogProbMetric: 27.0394, val_loss: 28.0651, val_MinusLogProbMetric: 28.0651

Epoch 627: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0394 - MinusLogProbMetric: 27.0394 - val_loss: 28.0651 - val_MinusLogProbMetric: 28.0651 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 628/1000
2023-10-27 01:45:21.181 
Epoch 628/1000 
	 loss: 27.0385, MinusLogProbMetric: 27.0385, val_loss: 28.0709, val_MinusLogProbMetric: 28.0709

Epoch 628: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0385 - MinusLogProbMetric: 27.0385 - val_loss: 28.0709 - val_MinusLogProbMetric: 28.0709 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 629/1000
2023-10-27 01:45:55.350 
Epoch 629/1000 
	 loss: 27.0386, MinusLogProbMetric: 27.0386, val_loss: 28.0627, val_MinusLogProbMetric: 28.0627

Epoch 629: val_loss did not improve from 28.05936
196/196 - 34s - loss: 27.0386 - MinusLogProbMetric: 27.0386 - val_loss: 28.0627 - val_MinusLogProbMetric: 28.0627 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 630/1000
2023-10-27 01:46:31.025 
Epoch 630/1000 
	 loss: 27.0379, MinusLogProbMetric: 27.0379, val_loss: 28.0732, val_MinusLogProbMetric: 28.0732

Epoch 630: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0379 - MinusLogProbMetric: 27.0379 - val_loss: 28.0732 - val_MinusLogProbMetric: 28.0732 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 631/1000
2023-10-27 01:47:05.370 
Epoch 631/1000 
	 loss: 27.0372, MinusLogProbMetric: 27.0372, val_loss: 28.0695, val_MinusLogProbMetric: 28.0695

Epoch 631: val_loss did not improve from 28.05936
196/196 - 34s - loss: 27.0372 - MinusLogProbMetric: 27.0372 - val_loss: 28.0695 - val_MinusLogProbMetric: 28.0695 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 632/1000
2023-10-27 01:47:40.677 
Epoch 632/1000 
	 loss: 27.0388, MinusLogProbMetric: 27.0388, val_loss: 28.0639, val_MinusLogProbMetric: 28.0639

Epoch 632: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0388 - MinusLogProbMetric: 27.0388 - val_loss: 28.0639 - val_MinusLogProbMetric: 28.0639 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 633/1000
2023-10-27 01:48:14.313 
Epoch 633/1000 
	 loss: 27.0385, MinusLogProbMetric: 27.0385, val_loss: 28.0634, val_MinusLogProbMetric: 28.0634

Epoch 633: val_loss did not improve from 28.05936
196/196 - 34s - loss: 27.0385 - MinusLogProbMetric: 27.0385 - val_loss: 28.0634 - val_MinusLogProbMetric: 28.0634 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 634/1000
2023-10-27 01:48:49.991 
Epoch 634/1000 
	 loss: 27.0375, MinusLogProbMetric: 27.0375, val_loss: 28.0636, val_MinusLogProbMetric: 28.0636

Epoch 634: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0375 - MinusLogProbMetric: 27.0375 - val_loss: 28.0636 - val_MinusLogProbMetric: 28.0636 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 635/1000
2023-10-27 01:49:25.582 
Epoch 635/1000 
	 loss: 27.0386, MinusLogProbMetric: 27.0386, val_loss: 28.0654, val_MinusLogProbMetric: 28.0654

Epoch 635: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0386 - MinusLogProbMetric: 27.0386 - val_loss: 28.0654 - val_MinusLogProbMetric: 28.0654 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 636/1000
2023-10-27 01:50:00.951 
Epoch 636/1000 
	 loss: 27.0372, MinusLogProbMetric: 27.0372, val_loss: 28.0670, val_MinusLogProbMetric: 28.0670

Epoch 636: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0372 - MinusLogProbMetric: 27.0372 - val_loss: 28.0670 - val_MinusLogProbMetric: 28.0670 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 637/1000
2023-10-27 01:50:36.842 
Epoch 637/1000 
	 loss: 27.0380, MinusLogProbMetric: 27.0380, val_loss: 28.0693, val_MinusLogProbMetric: 28.0693

Epoch 637: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0380 - MinusLogProbMetric: 27.0380 - val_loss: 28.0693 - val_MinusLogProbMetric: 28.0693 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 638/1000
2023-10-27 01:51:12.283 
Epoch 638/1000 
	 loss: 27.0375, MinusLogProbMetric: 27.0375, val_loss: 28.0678, val_MinusLogProbMetric: 28.0678

Epoch 638: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0375 - MinusLogProbMetric: 27.0375 - val_loss: 28.0678 - val_MinusLogProbMetric: 28.0678 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 639/1000
2023-10-27 01:51:48.047 
Epoch 639/1000 
	 loss: 27.0373, MinusLogProbMetric: 27.0373, val_loss: 28.0650, val_MinusLogProbMetric: 28.0650

Epoch 639: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0373 - MinusLogProbMetric: 27.0373 - val_loss: 28.0650 - val_MinusLogProbMetric: 28.0650 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 640/1000
2023-10-27 01:52:22.976 
Epoch 640/1000 
	 loss: 27.0369, MinusLogProbMetric: 27.0369, val_loss: 28.0649, val_MinusLogProbMetric: 28.0649

Epoch 640: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0369 - MinusLogProbMetric: 27.0369 - val_loss: 28.0649 - val_MinusLogProbMetric: 28.0649 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 641/1000
2023-10-27 01:52:58.681 
Epoch 641/1000 
	 loss: 27.0380, MinusLogProbMetric: 27.0380, val_loss: 28.0606, val_MinusLogProbMetric: 28.0606

Epoch 641: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0380 - MinusLogProbMetric: 27.0380 - val_loss: 28.0606 - val_MinusLogProbMetric: 28.0606 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 642/1000
2023-10-27 01:53:34.694 
Epoch 642/1000 
	 loss: 27.0369, MinusLogProbMetric: 27.0369, val_loss: 28.0653, val_MinusLogProbMetric: 28.0653

Epoch 642: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0369 - MinusLogProbMetric: 27.0369 - val_loss: 28.0653 - val_MinusLogProbMetric: 28.0653 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 643/1000
2023-10-27 01:54:10.086 
Epoch 643/1000 
	 loss: 27.0378, MinusLogProbMetric: 27.0378, val_loss: 28.0648, val_MinusLogProbMetric: 28.0648

Epoch 643: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0378 - MinusLogProbMetric: 27.0378 - val_loss: 28.0648 - val_MinusLogProbMetric: 28.0648 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 644/1000
2023-10-27 01:54:46.077 
Epoch 644/1000 
	 loss: 27.0367, MinusLogProbMetric: 27.0367, val_loss: 28.0648, val_MinusLogProbMetric: 28.0648

Epoch 644: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0367 - MinusLogProbMetric: 27.0367 - val_loss: 28.0648 - val_MinusLogProbMetric: 28.0648 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 645/1000
2023-10-27 01:55:21.761 
Epoch 645/1000 
	 loss: 27.0369, MinusLogProbMetric: 27.0369, val_loss: 28.0615, val_MinusLogProbMetric: 28.0615

Epoch 645: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0369 - MinusLogProbMetric: 27.0369 - val_loss: 28.0615 - val_MinusLogProbMetric: 28.0615 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 646/1000
2023-10-27 01:55:57.587 
Epoch 646/1000 
	 loss: 27.0370, MinusLogProbMetric: 27.0370, val_loss: 28.0690, val_MinusLogProbMetric: 28.0690

Epoch 646: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0370 - MinusLogProbMetric: 27.0370 - val_loss: 28.0690 - val_MinusLogProbMetric: 28.0690 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 647/1000
2023-10-27 01:56:33.124 
Epoch 647/1000 
	 loss: 27.0371, MinusLogProbMetric: 27.0371, val_loss: 28.0696, val_MinusLogProbMetric: 28.0696

Epoch 647: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0371 - MinusLogProbMetric: 27.0371 - val_loss: 28.0696 - val_MinusLogProbMetric: 28.0696 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 648/1000
2023-10-27 01:57:08.598 
Epoch 648/1000 
	 loss: 27.0372, MinusLogProbMetric: 27.0372, val_loss: 28.0681, val_MinusLogProbMetric: 28.0681

Epoch 648: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0372 - MinusLogProbMetric: 27.0372 - val_loss: 28.0681 - val_MinusLogProbMetric: 28.0681 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 649/1000
2023-10-27 01:57:44.303 
Epoch 649/1000 
	 loss: 27.0368, MinusLogProbMetric: 27.0368, val_loss: 28.0687, val_MinusLogProbMetric: 28.0687

Epoch 649: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0368 - MinusLogProbMetric: 27.0368 - val_loss: 28.0687 - val_MinusLogProbMetric: 28.0687 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 650/1000
2023-10-27 01:58:19.501 
Epoch 650/1000 
	 loss: 27.0373, MinusLogProbMetric: 27.0373, val_loss: 28.0639, val_MinusLogProbMetric: 28.0639

Epoch 650: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0373 - MinusLogProbMetric: 27.0373 - val_loss: 28.0639 - val_MinusLogProbMetric: 28.0639 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 651/1000
2023-10-27 01:58:55.175 
Epoch 651/1000 
	 loss: 27.0376, MinusLogProbMetric: 27.0376, val_loss: 28.0701, val_MinusLogProbMetric: 28.0701

Epoch 651: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0376 - MinusLogProbMetric: 27.0376 - val_loss: 28.0701 - val_MinusLogProbMetric: 28.0701 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 652/1000
2023-10-27 01:59:30.309 
Epoch 652/1000 
	 loss: 27.0365, MinusLogProbMetric: 27.0365, val_loss: 28.0694, val_MinusLogProbMetric: 28.0694

Epoch 652: val_loss did not improve from 28.05936
196/196 - 35s - loss: 27.0365 - MinusLogProbMetric: 27.0365 - val_loss: 28.0694 - val_MinusLogProbMetric: 28.0694 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 653/1000
2023-10-27 02:00:05.951 
Epoch 653/1000 
	 loss: 27.0371, MinusLogProbMetric: 27.0371, val_loss: 28.0700, val_MinusLogProbMetric: 28.0700

Epoch 653: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0371 - MinusLogProbMetric: 27.0371 - val_loss: 28.0700 - val_MinusLogProbMetric: 28.0700 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 654/1000
2023-10-27 02:00:41.631 
Epoch 654/1000 
	 loss: 27.0366, MinusLogProbMetric: 27.0366, val_loss: 28.0648, val_MinusLogProbMetric: 28.0648

Epoch 654: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0366 - MinusLogProbMetric: 27.0366 - val_loss: 28.0648 - val_MinusLogProbMetric: 28.0648 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 655/1000
2023-10-27 02:01:17.268 
Epoch 655/1000 
	 loss: 27.0355, MinusLogProbMetric: 27.0355, val_loss: 28.0709, val_MinusLogProbMetric: 28.0709

Epoch 655: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0355 - MinusLogProbMetric: 27.0355 - val_loss: 28.0709 - val_MinusLogProbMetric: 28.0709 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 656/1000
2023-10-27 02:01:52.882 
Epoch 656/1000 
	 loss: 27.0366, MinusLogProbMetric: 27.0366, val_loss: 28.0629, val_MinusLogProbMetric: 28.0629

Epoch 656: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0366 - MinusLogProbMetric: 27.0366 - val_loss: 28.0629 - val_MinusLogProbMetric: 28.0629 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 657/1000
2023-10-27 02:02:28.433 
Epoch 657/1000 
	 loss: 27.0366, MinusLogProbMetric: 27.0366, val_loss: 28.0668, val_MinusLogProbMetric: 28.0668

Epoch 657: val_loss did not improve from 28.05936
196/196 - 36s - loss: 27.0366 - MinusLogProbMetric: 27.0366 - val_loss: 28.0668 - val_MinusLogProbMetric: 28.0668 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 658/1000
2023-10-27 02:03:03.979 
Epoch 658/1000 
	 loss: 27.0357, MinusLogProbMetric: 27.0357, val_loss: 28.0657, val_MinusLogProbMetric: 28.0657

Epoch 658: val_loss did not improve from 28.05936
Restoring model weights from the end of the best epoch: 558.
196/196 - 36s - loss: 27.0357 - MinusLogProbMetric: 27.0357 - val_loss: 28.0657 - val_MinusLogProbMetric: 28.0657 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 658: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 933.
Model trained in 23594.17 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.06 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.32 s.
===========
Run 394/720 done in 23599.83 s.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

===========
Generating train data for run 396.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_396/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_396/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_396/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_396
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_349"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_350 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7fef4f0bae30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff761945360>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff761945360>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef00360190>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fefe7529a80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fefe75286a0>, <keras.callbacks.ModelCheckpoint object at 0x7fefe752b880>, <keras.callbacks.EarlyStopping object at 0x7fefe7528670>, <keras.callbacks.ReduceLROnPlateau object at 0x7fefe7529c00>, <keras.callbacks.TerminateOnNaN object at 0x7fefe752b640>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_396/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 396/720 with hyperparameters:
timestamp = 2023-10-27 02:03:12.339618
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
2023-10-27 02:05:13.452 
Epoch 1/1000 
	 loss: 725.7590, MinusLogProbMetric: 725.7590, val_loss: 183.8584, val_MinusLogProbMetric: 183.8584

Epoch 1: val_loss improved from inf to 183.85840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 122s - loss: 725.7590 - MinusLogProbMetric: 725.7590 - val_loss: 183.8584 - val_MinusLogProbMetric: 183.8584 - lr: 0.0010 - 122s/epoch - 621ms/step
Epoch 2/1000
2023-10-27 02:05:56.981 
Epoch 2/1000 
	 loss: 153.6074, MinusLogProbMetric: 153.6074, val_loss: 120.5096, val_MinusLogProbMetric: 120.5096

Epoch 2: val_loss improved from 183.85840 to 120.50964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 153.6074 - MinusLogProbMetric: 153.6074 - val_loss: 120.5096 - val_MinusLogProbMetric: 120.5096 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 3/1000
2023-10-27 02:06:40.513 
Epoch 3/1000 
	 loss: 101.8646, MinusLogProbMetric: 101.8646, val_loss: 82.8760, val_MinusLogProbMetric: 82.8760

Epoch 3: val_loss improved from 120.50964 to 82.87595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 101.8646 - MinusLogProbMetric: 101.8646 - val_loss: 82.8760 - val_MinusLogProbMetric: 82.8760 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 4/1000
2023-10-27 02:07:24.315 
Epoch 4/1000 
	 loss: 76.7160, MinusLogProbMetric: 76.7160, val_loss: 69.1866, val_MinusLogProbMetric: 69.1866

Epoch 4: val_loss improved from 82.87595 to 69.18663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 76.7160 - MinusLogProbMetric: 76.7160 - val_loss: 69.1866 - val_MinusLogProbMetric: 69.1866 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 5/1000
2023-10-27 02:08:08.151 
Epoch 5/1000 
	 loss: 66.3208, MinusLogProbMetric: 66.3208, val_loss: 62.0259, val_MinusLogProbMetric: 62.0259

Epoch 5: val_loss improved from 69.18663 to 62.02592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 66.3208 - MinusLogProbMetric: 66.3208 - val_loss: 62.0259 - val_MinusLogProbMetric: 62.0259 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 6/1000
2023-10-27 02:08:51.487 
Epoch 6/1000 
	 loss: 60.9846, MinusLogProbMetric: 60.9846, val_loss: 56.0326, val_MinusLogProbMetric: 56.0326

Epoch 6: val_loss improved from 62.02592 to 56.03265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 60.9846 - MinusLogProbMetric: 60.9846 - val_loss: 56.0326 - val_MinusLogProbMetric: 56.0326 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 7/1000
2023-10-27 02:09:34.665 
Epoch 7/1000 
	 loss: 55.3267, MinusLogProbMetric: 55.3267, val_loss: 56.0411, val_MinusLogProbMetric: 56.0411

Epoch 7: val_loss did not improve from 56.03265
196/196 - 42s - loss: 55.3267 - MinusLogProbMetric: 55.3267 - val_loss: 56.0411 - val_MinusLogProbMetric: 56.0411 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 8/1000
2023-10-27 02:10:17.582 
Epoch 8/1000 
	 loss: 53.3789, MinusLogProbMetric: 53.3789, val_loss: 49.9543, val_MinusLogProbMetric: 49.9543

Epoch 8: val_loss improved from 56.03265 to 49.95433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 53.3789 - MinusLogProbMetric: 53.3789 - val_loss: 49.9543 - val_MinusLogProbMetric: 49.9543 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 9/1000
2023-10-27 02:11:01.134 
Epoch 9/1000 
	 loss: 48.4259, MinusLogProbMetric: 48.4259, val_loss: 48.0857, val_MinusLogProbMetric: 48.0857

Epoch 9: val_loss improved from 49.95433 to 48.08566, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 48.4259 - MinusLogProbMetric: 48.4259 - val_loss: 48.0857 - val_MinusLogProbMetric: 48.0857 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 10/1000
2023-10-27 02:11:44.797 
Epoch 10/1000 
	 loss: 46.8492, MinusLogProbMetric: 46.8492, val_loss: 46.8596, val_MinusLogProbMetric: 46.8596

Epoch 10: val_loss improved from 48.08566 to 46.85963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 46.8492 - MinusLogProbMetric: 46.8492 - val_loss: 46.8596 - val_MinusLogProbMetric: 46.8596 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 11/1000
2023-10-27 02:12:28.332 
Epoch 11/1000 
	 loss: 47.4651, MinusLogProbMetric: 47.4651, val_loss: 48.3961, val_MinusLogProbMetric: 48.3961

Epoch 11: val_loss did not improve from 46.85963
196/196 - 43s - loss: 47.4651 - MinusLogProbMetric: 47.4651 - val_loss: 48.3961 - val_MinusLogProbMetric: 48.3961 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 12/1000
2023-10-27 02:13:11.215 
Epoch 12/1000 
	 loss: 44.6646, MinusLogProbMetric: 44.6646, val_loss: 43.4061, val_MinusLogProbMetric: 43.4061

Epoch 12: val_loss improved from 46.85963 to 43.40610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 44.6646 - MinusLogProbMetric: 44.6646 - val_loss: 43.4061 - val_MinusLogProbMetric: 43.4061 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 13/1000
2023-10-27 02:13:54.305 
Epoch 13/1000 
	 loss: 43.0167, MinusLogProbMetric: 43.0167, val_loss: 40.8933, val_MinusLogProbMetric: 40.8933

Epoch 13: val_loss improved from 43.40610 to 40.89325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 43.0167 - MinusLogProbMetric: 43.0167 - val_loss: 40.8933 - val_MinusLogProbMetric: 40.8933 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 14/1000
2023-10-27 02:14:37.371 
Epoch 14/1000 
	 loss: 42.0924, MinusLogProbMetric: 42.0924, val_loss: 43.9530, val_MinusLogProbMetric: 43.9530

Epoch 14: val_loss did not improve from 40.89325
196/196 - 42s - loss: 42.0924 - MinusLogProbMetric: 42.0924 - val_loss: 43.9530 - val_MinusLogProbMetric: 43.9530 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 15/1000
2023-10-27 02:15:20.497 
Epoch 15/1000 
	 loss: 41.0690, MinusLogProbMetric: 41.0690, val_loss: 44.3142, val_MinusLogProbMetric: 44.3142

Epoch 15: val_loss did not improve from 40.89325
196/196 - 43s - loss: 41.0690 - MinusLogProbMetric: 41.0690 - val_loss: 44.3142 - val_MinusLogProbMetric: 44.3142 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 16/1000
2023-10-27 02:16:03.018 
Epoch 16/1000 
	 loss: 40.3609, MinusLogProbMetric: 40.3609, val_loss: 39.1093, val_MinusLogProbMetric: 39.1093

Epoch 16: val_loss improved from 40.89325 to 39.10933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 40.3609 - MinusLogProbMetric: 40.3609 - val_loss: 39.1093 - val_MinusLogProbMetric: 39.1093 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 17/1000
2023-10-27 02:16:46.563 
Epoch 17/1000 
	 loss: 38.9516, MinusLogProbMetric: 38.9516, val_loss: 38.9173, val_MinusLogProbMetric: 38.9173

Epoch 17: val_loss improved from 39.10933 to 38.91728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 38.9516 - MinusLogProbMetric: 38.9516 - val_loss: 38.9173 - val_MinusLogProbMetric: 38.9173 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 18/1000
2023-10-27 02:17:29.631 
Epoch 18/1000 
	 loss: 38.9884, MinusLogProbMetric: 38.9884, val_loss: 44.8820, val_MinusLogProbMetric: 44.8820

Epoch 18: val_loss did not improve from 38.91728
196/196 - 42s - loss: 38.9884 - MinusLogProbMetric: 38.9884 - val_loss: 44.8820 - val_MinusLogProbMetric: 44.8820 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 19/1000
2023-10-27 02:18:12.184 
Epoch 19/1000 
	 loss: 38.2097, MinusLogProbMetric: 38.2097, val_loss: 36.6035, val_MinusLogProbMetric: 36.6035

Epoch 19: val_loss improved from 38.91728 to 36.60347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 38.2097 - MinusLogProbMetric: 38.2097 - val_loss: 36.6035 - val_MinusLogProbMetric: 36.6035 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 20/1000
2023-10-27 02:18:55.015 
Epoch 20/1000 
	 loss: 37.5177, MinusLogProbMetric: 37.5177, val_loss: 37.0905, val_MinusLogProbMetric: 37.0905

Epoch 20: val_loss did not improve from 36.60347
196/196 - 42s - loss: 37.5177 - MinusLogProbMetric: 37.5177 - val_loss: 37.0905 - val_MinusLogProbMetric: 37.0905 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 21/1000
2023-10-27 02:19:37.073 
Epoch 21/1000 
	 loss: 37.6198, MinusLogProbMetric: 37.6198, val_loss: 38.2497, val_MinusLogProbMetric: 38.2497

Epoch 21: val_loss did not improve from 36.60347
196/196 - 42s - loss: 37.6198 - MinusLogProbMetric: 37.6198 - val_loss: 38.2497 - val_MinusLogProbMetric: 38.2497 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 22/1000
2023-10-27 02:20:19.318 
Epoch 22/1000 
	 loss: 37.0443, MinusLogProbMetric: 37.0443, val_loss: 40.3240, val_MinusLogProbMetric: 40.3240

Epoch 22: val_loss did not improve from 36.60347
196/196 - 42s - loss: 37.0443 - MinusLogProbMetric: 37.0443 - val_loss: 40.3240 - val_MinusLogProbMetric: 40.3240 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 23/1000
2023-10-27 02:21:02.319 
Epoch 23/1000 
	 loss: 37.0324, MinusLogProbMetric: 37.0324, val_loss: 37.0597, val_MinusLogProbMetric: 37.0597

Epoch 23: val_loss did not improve from 36.60347
196/196 - 43s - loss: 37.0324 - MinusLogProbMetric: 37.0324 - val_loss: 37.0597 - val_MinusLogProbMetric: 37.0597 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 24/1000
2023-10-27 02:21:44.691 
Epoch 24/1000 
	 loss: 36.2414, MinusLogProbMetric: 36.2414, val_loss: 37.9985, val_MinusLogProbMetric: 37.9985

Epoch 24: val_loss did not improve from 36.60347
196/196 - 42s - loss: 36.2414 - MinusLogProbMetric: 36.2414 - val_loss: 37.9985 - val_MinusLogProbMetric: 37.9985 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 25/1000
2023-10-27 02:22:27.716 
Epoch 25/1000 
	 loss: 36.5286, MinusLogProbMetric: 36.5286, val_loss: 36.4026, val_MinusLogProbMetric: 36.4026

Epoch 25: val_loss improved from 36.60347 to 36.40255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 36.5286 - MinusLogProbMetric: 36.5286 - val_loss: 36.4026 - val_MinusLogProbMetric: 36.4026 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 26/1000
2023-10-27 02:23:11.598 
Epoch 26/1000 
	 loss: 36.0248, MinusLogProbMetric: 36.0248, val_loss: 36.3353, val_MinusLogProbMetric: 36.3353

Epoch 26: val_loss improved from 36.40255 to 36.33526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 36.0248 - MinusLogProbMetric: 36.0248 - val_loss: 36.3353 - val_MinusLogProbMetric: 36.3353 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 27/1000
2023-10-27 02:23:54.884 
Epoch 27/1000 
	 loss: 35.6706, MinusLogProbMetric: 35.6706, val_loss: 35.0154, val_MinusLogProbMetric: 35.0154

Epoch 27: val_loss improved from 36.33526 to 35.01542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 35.6706 - MinusLogProbMetric: 35.6706 - val_loss: 35.0154 - val_MinusLogProbMetric: 35.0154 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 28/1000
2023-10-27 02:24:38.780 
Epoch 28/1000 
	 loss: 34.9784, MinusLogProbMetric: 34.9784, val_loss: 36.2473, val_MinusLogProbMetric: 36.2473

Epoch 28: val_loss did not improve from 35.01542
196/196 - 43s - loss: 34.9784 - MinusLogProbMetric: 34.9784 - val_loss: 36.2473 - val_MinusLogProbMetric: 36.2473 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 29/1000
2023-10-27 02:25:21.402 
Epoch 29/1000 
	 loss: 35.1699, MinusLogProbMetric: 35.1699, val_loss: 35.8028, val_MinusLogProbMetric: 35.8028

Epoch 29: val_loss did not improve from 35.01542
196/196 - 43s - loss: 35.1699 - MinusLogProbMetric: 35.1699 - val_loss: 35.8028 - val_MinusLogProbMetric: 35.8028 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 30/1000
2023-10-27 02:26:03.533 
Epoch 30/1000 
	 loss: 34.7708, MinusLogProbMetric: 34.7708, val_loss: 35.5770, val_MinusLogProbMetric: 35.5770

Epoch 30: val_loss did not improve from 35.01542
196/196 - 42s - loss: 34.7708 - MinusLogProbMetric: 34.7708 - val_loss: 35.5770 - val_MinusLogProbMetric: 35.5770 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 31/1000
2023-10-27 02:26:45.995 
Epoch 31/1000 
	 loss: 34.7434, MinusLogProbMetric: 34.7434, val_loss: 34.6079, val_MinusLogProbMetric: 34.6079

Epoch 31: val_loss improved from 35.01542 to 34.60792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 34.7434 - MinusLogProbMetric: 34.7434 - val_loss: 34.6079 - val_MinusLogProbMetric: 34.6079 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 32/1000
2023-10-27 02:27:29.656 
Epoch 32/1000 
	 loss: 34.5367, MinusLogProbMetric: 34.5367, val_loss: 36.7774, val_MinusLogProbMetric: 36.7774

Epoch 32: val_loss did not improve from 34.60792
196/196 - 43s - loss: 34.5367 - MinusLogProbMetric: 34.5367 - val_loss: 36.7774 - val_MinusLogProbMetric: 36.7774 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 33/1000
2023-10-27 02:28:12.799 
Epoch 33/1000 
	 loss: 34.5800, MinusLogProbMetric: 34.5800, val_loss: 33.8103, val_MinusLogProbMetric: 33.8103

Epoch 33: val_loss improved from 34.60792 to 33.81025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 34.5800 - MinusLogProbMetric: 34.5800 - val_loss: 33.8103 - val_MinusLogProbMetric: 33.8103 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 34/1000
2023-10-27 02:28:56.301 
Epoch 34/1000 
	 loss: 34.0183, MinusLogProbMetric: 34.0183, val_loss: 33.6281, val_MinusLogProbMetric: 33.6281

Epoch 34: val_loss improved from 33.81025 to 33.62809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 34.0183 - MinusLogProbMetric: 34.0183 - val_loss: 33.6281 - val_MinusLogProbMetric: 33.6281 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 35/1000
2023-10-27 02:29:40.029 
Epoch 35/1000 
	 loss: 33.8046, MinusLogProbMetric: 33.8046, val_loss: 33.8513, val_MinusLogProbMetric: 33.8513

Epoch 35: val_loss did not improve from 33.62809
196/196 - 43s - loss: 33.8046 - MinusLogProbMetric: 33.8046 - val_loss: 33.8513 - val_MinusLogProbMetric: 33.8513 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 36/1000
2023-10-27 02:30:22.638 
Epoch 36/1000 
	 loss: 33.9667, MinusLogProbMetric: 33.9667, val_loss: 33.5447, val_MinusLogProbMetric: 33.5447

Epoch 36: val_loss improved from 33.62809 to 33.54475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 33.9667 - MinusLogProbMetric: 33.9667 - val_loss: 33.5447 - val_MinusLogProbMetric: 33.5447 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 37/1000
2023-10-27 02:31:06.129 
Epoch 37/1000 
	 loss: 33.6719, MinusLogProbMetric: 33.6719, val_loss: 34.2845, val_MinusLogProbMetric: 34.2845

Epoch 37: val_loss did not improve from 33.54475
196/196 - 43s - loss: 33.6719 - MinusLogProbMetric: 33.6719 - val_loss: 34.2845 - val_MinusLogProbMetric: 34.2845 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 38/1000
2023-10-27 02:31:48.615 
Epoch 38/1000 
	 loss: 33.5637, MinusLogProbMetric: 33.5637, val_loss: 33.8495, val_MinusLogProbMetric: 33.8495

Epoch 38: val_loss did not improve from 33.54475
196/196 - 42s - loss: 33.5637 - MinusLogProbMetric: 33.5637 - val_loss: 33.8495 - val_MinusLogProbMetric: 33.8495 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 39/1000
2023-10-27 02:32:30.808 
Epoch 39/1000 
	 loss: 33.6608, MinusLogProbMetric: 33.6608, val_loss: 34.1650, val_MinusLogProbMetric: 34.1650

Epoch 39: val_loss did not improve from 33.54475
196/196 - 42s - loss: 33.6608 - MinusLogProbMetric: 33.6608 - val_loss: 34.1650 - val_MinusLogProbMetric: 34.1650 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 40/1000
2023-10-27 02:33:13.302 
Epoch 40/1000 
	 loss: 33.4131, MinusLogProbMetric: 33.4131, val_loss: 34.8087, val_MinusLogProbMetric: 34.8087

Epoch 40: val_loss did not improve from 33.54475
196/196 - 42s - loss: 33.4131 - MinusLogProbMetric: 33.4131 - val_loss: 34.8087 - val_MinusLogProbMetric: 34.8087 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 41/1000
2023-10-27 02:33:55.961 
Epoch 41/1000 
	 loss: 33.3909, MinusLogProbMetric: 33.3909, val_loss: 33.1462, val_MinusLogProbMetric: 33.1462

Epoch 41: val_loss improved from 33.54475 to 33.14621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 33.3909 - MinusLogProbMetric: 33.3909 - val_loss: 33.1462 - val_MinusLogProbMetric: 33.1462 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 42/1000
2023-10-27 02:34:39.582 
Epoch 42/1000 
	 loss: 33.3096, MinusLogProbMetric: 33.3096, val_loss: 35.5267, val_MinusLogProbMetric: 35.5267

Epoch 42: val_loss did not improve from 33.14621
196/196 - 43s - loss: 33.3096 - MinusLogProbMetric: 33.3096 - val_loss: 35.5267 - val_MinusLogProbMetric: 35.5267 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 43/1000
2023-10-27 02:35:22.240 
Epoch 43/1000 
	 loss: 33.3577, MinusLogProbMetric: 33.3577, val_loss: 33.4846, val_MinusLogProbMetric: 33.4846

Epoch 43: val_loss did not improve from 33.14621
196/196 - 43s - loss: 33.3577 - MinusLogProbMetric: 33.3577 - val_loss: 33.4846 - val_MinusLogProbMetric: 33.4846 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 44/1000
2023-10-27 02:36:04.851 
Epoch 44/1000 
	 loss: 33.2363, MinusLogProbMetric: 33.2363, val_loss: 32.5254, val_MinusLogProbMetric: 32.5254

Epoch 44: val_loss improved from 33.14621 to 32.52542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 33.2363 - MinusLogProbMetric: 33.2363 - val_loss: 32.5254 - val_MinusLogProbMetric: 32.5254 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 45/1000
2023-10-27 02:36:48.064 
Epoch 45/1000 
	 loss: 32.9381, MinusLogProbMetric: 32.9381, val_loss: 33.7584, val_MinusLogProbMetric: 33.7584

Epoch 45: val_loss did not improve from 32.52542
196/196 - 42s - loss: 32.9381 - MinusLogProbMetric: 32.9381 - val_loss: 33.7584 - val_MinusLogProbMetric: 33.7584 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 46/1000
2023-10-27 02:37:30.401 
Epoch 46/1000 
	 loss: 33.1335, MinusLogProbMetric: 33.1335, val_loss: 35.0055, val_MinusLogProbMetric: 35.0055

Epoch 46: val_loss did not improve from 32.52542
196/196 - 42s - loss: 33.1335 - MinusLogProbMetric: 33.1335 - val_loss: 35.0055 - val_MinusLogProbMetric: 35.0055 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 47/1000
2023-10-27 02:38:13.353 
Epoch 47/1000 
	 loss: 32.8618, MinusLogProbMetric: 32.8618, val_loss: 32.8298, val_MinusLogProbMetric: 32.8298

Epoch 47: val_loss did not improve from 32.52542
196/196 - 43s - loss: 32.8618 - MinusLogProbMetric: 32.8618 - val_loss: 32.8298 - val_MinusLogProbMetric: 32.8298 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 48/1000
2023-10-27 02:38:55.907 
Epoch 48/1000 
	 loss: 32.6304, MinusLogProbMetric: 32.6304, val_loss: 32.0541, val_MinusLogProbMetric: 32.0541

Epoch 48: val_loss improved from 32.52542 to 32.05406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 32.6304 - MinusLogProbMetric: 32.6304 - val_loss: 32.0541 - val_MinusLogProbMetric: 32.0541 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 49/1000
2023-10-27 02:39:39.292 
Epoch 49/1000 
	 loss: 32.8221, MinusLogProbMetric: 32.8221, val_loss: 33.4769, val_MinusLogProbMetric: 33.4769

Epoch 49: val_loss did not improve from 32.05406
196/196 - 42s - loss: 32.8221 - MinusLogProbMetric: 32.8221 - val_loss: 33.4769 - val_MinusLogProbMetric: 33.4769 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 50/1000
2023-10-27 02:40:21.954 
Epoch 50/1000 
	 loss: 32.5771, MinusLogProbMetric: 32.5771, val_loss: 32.5964, val_MinusLogProbMetric: 32.5964

Epoch 50: val_loss did not improve from 32.05406
196/196 - 43s - loss: 32.5771 - MinusLogProbMetric: 32.5771 - val_loss: 32.5964 - val_MinusLogProbMetric: 32.5964 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 51/1000
2023-10-27 02:41:04.911 
Epoch 51/1000 
	 loss: 32.5878, MinusLogProbMetric: 32.5878, val_loss: 31.5544, val_MinusLogProbMetric: 31.5544

Epoch 51: val_loss improved from 32.05406 to 31.55436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 44s - loss: 32.5878 - MinusLogProbMetric: 32.5878 - val_loss: 31.5544 - val_MinusLogProbMetric: 31.5544 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 52/1000
2023-10-27 02:41:48.332 
Epoch 52/1000 
	 loss: 32.4397, MinusLogProbMetric: 32.4397, val_loss: 33.1229, val_MinusLogProbMetric: 33.1229

Epoch 52: val_loss did not improve from 31.55436
196/196 - 43s - loss: 32.4397 - MinusLogProbMetric: 32.4397 - val_loss: 33.1229 - val_MinusLogProbMetric: 33.1229 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 53/1000
2023-10-27 02:42:30.954 
Epoch 53/1000 
	 loss: 32.4693, MinusLogProbMetric: 32.4693, val_loss: 32.2055, val_MinusLogProbMetric: 32.2055

Epoch 53: val_loss did not improve from 31.55436
196/196 - 43s - loss: 32.4693 - MinusLogProbMetric: 32.4693 - val_loss: 32.2055 - val_MinusLogProbMetric: 32.2055 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 54/1000
2023-10-27 02:43:13.146 
Epoch 54/1000 
	 loss: 32.3950, MinusLogProbMetric: 32.3950, val_loss: 32.8707, val_MinusLogProbMetric: 32.8707

Epoch 54: val_loss did not improve from 31.55436
196/196 - 42s - loss: 32.3950 - MinusLogProbMetric: 32.3950 - val_loss: 32.8707 - val_MinusLogProbMetric: 32.8707 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 55/1000
2023-10-27 02:43:55.845 
Epoch 55/1000 
	 loss: 32.1542, MinusLogProbMetric: 32.1542, val_loss: 33.5203, val_MinusLogProbMetric: 33.5203

Epoch 55: val_loss did not improve from 31.55436
196/196 - 43s - loss: 32.1542 - MinusLogProbMetric: 32.1542 - val_loss: 33.5203 - val_MinusLogProbMetric: 33.5203 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 56/1000
2023-10-27 02:44:37.971 
Epoch 56/1000 
	 loss: 32.1317, MinusLogProbMetric: 32.1317, val_loss: 32.4138, val_MinusLogProbMetric: 32.4138

Epoch 56: val_loss did not improve from 31.55436
196/196 - 42s - loss: 32.1317 - MinusLogProbMetric: 32.1317 - val_loss: 32.4138 - val_MinusLogProbMetric: 32.4138 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 57/1000
2023-10-27 02:45:20.678 
Epoch 57/1000 
	 loss: 32.3552, MinusLogProbMetric: 32.3552, val_loss: 32.4483, val_MinusLogProbMetric: 32.4483

Epoch 57: val_loss did not improve from 31.55436
196/196 - 43s - loss: 32.3552 - MinusLogProbMetric: 32.3552 - val_loss: 32.4483 - val_MinusLogProbMetric: 32.4483 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 58/1000
2023-10-27 02:46:01.797 
Epoch 58/1000 
	 loss: 32.0136, MinusLogProbMetric: 32.0136, val_loss: 31.8823, val_MinusLogProbMetric: 31.8823

Epoch 58: val_loss did not improve from 31.55436
196/196 - 41s - loss: 32.0136 - MinusLogProbMetric: 32.0136 - val_loss: 31.8823 - val_MinusLogProbMetric: 31.8823 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 59/1000
2023-10-27 02:46:44.725 
Epoch 59/1000 
	 loss: 31.8727, MinusLogProbMetric: 31.8727, val_loss: 33.0750, val_MinusLogProbMetric: 33.0750

Epoch 59: val_loss did not improve from 31.55436
196/196 - 43s - loss: 31.8727 - MinusLogProbMetric: 31.8727 - val_loss: 33.0750 - val_MinusLogProbMetric: 33.0750 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 60/1000
2023-10-27 02:47:26.518 
Epoch 60/1000 
	 loss: 31.8441, MinusLogProbMetric: 31.8441, val_loss: 31.8481, val_MinusLogProbMetric: 31.8481

Epoch 60: val_loss did not improve from 31.55436
196/196 - 42s - loss: 31.8441 - MinusLogProbMetric: 31.8441 - val_loss: 31.8481 - val_MinusLogProbMetric: 31.8481 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 61/1000
2023-10-27 02:48:07.939 
Epoch 61/1000 
	 loss: 31.9266, MinusLogProbMetric: 31.9266, val_loss: 31.7010, val_MinusLogProbMetric: 31.7010

Epoch 61: val_loss did not improve from 31.55436
196/196 - 41s - loss: 31.9266 - MinusLogProbMetric: 31.9266 - val_loss: 31.7010 - val_MinusLogProbMetric: 31.7010 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 62/1000
2023-10-27 02:48:50.891 
Epoch 62/1000 
	 loss: 31.9119, MinusLogProbMetric: 31.9119, val_loss: 33.3622, val_MinusLogProbMetric: 33.3622

Epoch 62: val_loss did not improve from 31.55436
196/196 - 43s - loss: 31.9119 - MinusLogProbMetric: 31.9119 - val_loss: 33.3622 - val_MinusLogProbMetric: 33.3622 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 63/1000
2023-10-27 02:49:33.447 
Epoch 63/1000 
	 loss: 31.7123, MinusLogProbMetric: 31.7123, val_loss: 31.9874, val_MinusLogProbMetric: 31.9874

Epoch 63: val_loss did not improve from 31.55436
196/196 - 43s - loss: 31.7123 - MinusLogProbMetric: 31.7123 - val_loss: 31.9874 - val_MinusLogProbMetric: 31.9874 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 64/1000
2023-10-27 02:50:14.861 
Epoch 64/1000 
	 loss: 31.7336, MinusLogProbMetric: 31.7336, val_loss: 32.2890, val_MinusLogProbMetric: 32.2890

Epoch 64: val_loss did not improve from 31.55436
196/196 - 41s - loss: 31.7336 - MinusLogProbMetric: 31.7336 - val_loss: 32.2890 - val_MinusLogProbMetric: 32.2890 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 65/1000
2023-10-27 02:50:56.169 
Epoch 65/1000 
	 loss: 31.5428, MinusLogProbMetric: 31.5428, val_loss: 31.6417, val_MinusLogProbMetric: 31.6417

Epoch 65: val_loss did not improve from 31.55436
196/196 - 41s - loss: 31.5428 - MinusLogProbMetric: 31.5428 - val_loss: 31.6417 - val_MinusLogProbMetric: 31.6417 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 66/1000
2023-10-27 02:51:30.994 
Epoch 66/1000 
	 loss: 31.4429, MinusLogProbMetric: 31.4429, val_loss: 31.8601, val_MinusLogProbMetric: 31.8601

Epoch 66: val_loss did not improve from 31.55436
196/196 - 35s - loss: 31.4429 - MinusLogProbMetric: 31.4429 - val_loss: 31.8601 - val_MinusLogProbMetric: 31.8601 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 67/1000
2023-10-27 02:52:05.001 
Epoch 67/1000 
	 loss: 31.5947, MinusLogProbMetric: 31.5947, val_loss: 31.6378, val_MinusLogProbMetric: 31.6378

Epoch 67: val_loss did not improve from 31.55436
196/196 - 34s - loss: 31.5947 - MinusLogProbMetric: 31.5947 - val_loss: 31.6378 - val_MinusLogProbMetric: 31.6378 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 68/1000
2023-10-27 02:52:39.727 
Epoch 68/1000 
	 loss: 31.3285, MinusLogProbMetric: 31.3285, val_loss: 33.6458, val_MinusLogProbMetric: 33.6458

Epoch 68: val_loss did not improve from 31.55436
196/196 - 35s - loss: 31.3285 - MinusLogProbMetric: 31.3285 - val_loss: 33.6458 - val_MinusLogProbMetric: 33.6458 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 69/1000
2023-10-27 02:53:17.060 
Epoch 69/1000 
	 loss: 31.4636, MinusLogProbMetric: 31.4636, val_loss: 32.1650, val_MinusLogProbMetric: 32.1650

Epoch 69: val_loss did not improve from 31.55436
196/196 - 37s - loss: 31.4636 - MinusLogProbMetric: 31.4636 - val_loss: 32.1650 - val_MinusLogProbMetric: 32.1650 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 70/1000
2023-10-27 02:53:59.062 
Epoch 70/1000 
	 loss: 31.5507, MinusLogProbMetric: 31.5507, val_loss: 31.3571, val_MinusLogProbMetric: 31.3571

Epoch 70: val_loss improved from 31.55436 to 31.35710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 31.5507 - MinusLogProbMetric: 31.5507 - val_loss: 31.3571 - val_MinusLogProbMetric: 31.3571 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 71/1000
2023-10-27 02:54:36.636 
Epoch 71/1000 
	 loss: 31.3440, MinusLogProbMetric: 31.3440, val_loss: 31.0407, val_MinusLogProbMetric: 31.0407

Epoch 71: val_loss improved from 31.35710 to 31.04066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 37s - loss: 31.3440 - MinusLogProbMetric: 31.3440 - val_loss: 31.0407 - val_MinusLogProbMetric: 31.0407 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 72/1000
2023-10-27 02:55:13.716 
Epoch 72/1000 
	 loss: 31.3024, MinusLogProbMetric: 31.3024, val_loss: 31.4530, val_MinusLogProbMetric: 31.4530

Epoch 72: val_loss did not improve from 31.04066
196/196 - 36s - loss: 31.3024 - MinusLogProbMetric: 31.3024 - val_loss: 31.4530 - val_MinusLogProbMetric: 31.4530 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 73/1000
2023-10-27 02:55:49.455 
Epoch 73/1000 
	 loss: 31.2894, MinusLogProbMetric: 31.2894, val_loss: 31.5339, val_MinusLogProbMetric: 31.5339

Epoch 73: val_loss did not improve from 31.04066
196/196 - 36s - loss: 31.2894 - MinusLogProbMetric: 31.2894 - val_loss: 31.5339 - val_MinusLogProbMetric: 31.5339 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 74/1000
2023-10-27 02:56:29.479 
Epoch 74/1000 
	 loss: 31.2038, MinusLogProbMetric: 31.2038, val_loss: 32.5971, val_MinusLogProbMetric: 32.5971

Epoch 74: val_loss did not improve from 31.04066
196/196 - 40s - loss: 31.2038 - MinusLogProbMetric: 31.2038 - val_loss: 32.5971 - val_MinusLogProbMetric: 32.5971 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 75/1000
2023-10-27 02:57:09.721 
Epoch 75/1000 
	 loss: 31.2006, MinusLogProbMetric: 31.2006, val_loss: 31.0802, val_MinusLogProbMetric: 31.0802

Epoch 75: val_loss did not improve from 31.04066
196/196 - 40s - loss: 31.2006 - MinusLogProbMetric: 31.2006 - val_loss: 31.0802 - val_MinusLogProbMetric: 31.0802 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 76/1000
2023-10-27 02:57:44.848 
Epoch 76/1000 
	 loss: 31.0979, MinusLogProbMetric: 31.0979, val_loss: 32.7046, val_MinusLogProbMetric: 32.7046

Epoch 76: val_loss did not improve from 31.04066
196/196 - 35s - loss: 31.0979 - MinusLogProbMetric: 31.0979 - val_loss: 32.7046 - val_MinusLogProbMetric: 32.7046 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 77/1000
2023-10-27 02:58:24.290 
Epoch 77/1000 
	 loss: 31.1572, MinusLogProbMetric: 31.1572, val_loss: 30.9305, val_MinusLogProbMetric: 30.9305

Epoch 77: val_loss improved from 31.04066 to 30.93049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 40s - loss: 31.1572 - MinusLogProbMetric: 31.1572 - val_loss: 30.9305 - val_MinusLogProbMetric: 30.9305 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 78/1000
2023-10-27 02:59:04.740 
Epoch 78/1000 
	 loss: 31.0797, MinusLogProbMetric: 31.0797, val_loss: 32.0432, val_MinusLogProbMetric: 32.0432

Epoch 78: val_loss did not improve from 30.93049
196/196 - 40s - loss: 31.0797 - MinusLogProbMetric: 31.0797 - val_loss: 32.0432 - val_MinusLogProbMetric: 32.0432 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 79/1000
2023-10-27 02:59:44.226 
Epoch 79/1000 
	 loss: 30.9727, MinusLogProbMetric: 30.9727, val_loss: 30.8714, val_MinusLogProbMetric: 30.8714

Epoch 79: val_loss improved from 30.93049 to 30.87138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 40s - loss: 30.9727 - MinusLogProbMetric: 30.9727 - val_loss: 30.8714 - val_MinusLogProbMetric: 30.8714 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 80/1000
2023-10-27 03:00:19.438 
Epoch 80/1000 
	 loss: 31.0626, MinusLogProbMetric: 31.0626, val_loss: 32.3056, val_MinusLogProbMetric: 32.3056

Epoch 80: val_loss did not improve from 30.87138
196/196 - 35s - loss: 31.0626 - MinusLogProbMetric: 31.0626 - val_loss: 32.3056 - val_MinusLogProbMetric: 32.3056 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 81/1000
2023-10-27 03:00:54.208 
Epoch 81/1000 
	 loss: 31.0859, MinusLogProbMetric: 31.0859, val_loss: 32.0371, val_MinusLogProbMetric: 32.0371

Epoch 81: val_loss did not improve from 30.87138
196/196 - 35s - loss: 31.0859 - MinusLogProbMetric: 31.0859 - val_loss: 32.0371 - val_MinusLogProbMetric: 32.0371 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 82/1000
2023-10-27 03:01:30.525 
Epoch 82/1000 
	 loss: 30.9656, MinusLogProbMetric: 30.9656, val_loss: 30.8349, val_MinusLogProbMetric: 30.8349

Epoch 82: val_loss improved from 30.87138 to 30.83494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 37s - loss: 30.9656 - MinusLogProbMetric: 30.9656 - val_loss: 30.8349 - val_MinusLogProbMetric: 30.8349 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 83/1000
2023-10-27 03:02:12.848 
Epoch 83/1000 
	 loss: 30.8615, MinusLogProbMetric: 30.8615, val_loss: 30.5166, val_MinusLogProbMetric: 30.5166

Epoch 83: val_loss improved from 30.83494 to 30.51665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 42s - loss: 30.8615 - MinusLogProbMetric: 30.8615 - val_loss: 30.5166 - val_MinusLogProbMetric: 30.5166 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 84/1000
2023-10-27 03:02:48.708 
Epoch 84/1000 
	 loss: 30.8499, MinusLogProbMetric: 30.8499, val_loss: 30.9799, val_MinusLogProbMetric: 30.9799

Epoch 84: val_loss did not improve from 30.51665
196/196 - 35s - loss: 30.8499 - MinusLogProbMetric: 30.8499 - val_loss: 30.9799 - val_MinusLogProbMetric: 30.9799 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 85/1000
2023-10-27 03:03:24.586 
Epoch 85/1000 
	 loss: 30.8575, MinusLogProbMetric: 30.8575, val_loss: 30.8713, val_MinusLogProbMetric: 30.8713

Epoch 85: val_loss did not improve from 30.51665
196/196 - 36s - loss: 30.8575 - MinusLogProbMetric: 30.8575 - val_loss: 30.8713 - val_MinusLogProbMetric: 30.8713 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 86/1000
2023-10-27 03:04:02.876 
Epoch 86/1000 
	 loss: 30.6898, MinusLogProbMetric: 30.6898, val_loss: 31.4110, val_MinusLogProbMetric: 31.4110

Epoch 86: val_loss did not improve from 30.51665
196/196 - 38s - loss: 30.6898 - MinusLogProbMetric: 30.6898 - val_loss: 31.4110 - val_MinusLogProbMetric: 31.4110 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 87/1000
2023-10-27 03:04:43.127 
Epoch 87/1000 
	 loss: 30.7559, MinusLogProbMetric: 30.7559, val_loss: 31.3035, val_MinusLogProbMetric: 31.3035

Epoch 87: val_loss did not improve from 30.51665
196/196 - 40s - loss: 30.7559 - MinusLogProbMetric: 30.7559 - val_loss: 31.3035 - val_MinusLogProbMetric: 31.3035 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 88/1000
2023-10-27 03:05:17.461 
Epoch 88/1000 
	 loss: 30.7268, MinusLogProbMetric: 30.7268, val_loss: 31.4044, val_MinusLogProbMetric: 31.4044

Epoch 88: val_loss did not improve from 30.51665
196/196 - 34s - loss: 30.7268 - MinusLogProbMetric: 30.7268 - val_loss: 31.4044 - val_MinusLogProbMetric: 31.4044 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 89/1000
2023-10-27 03:05:51.819 
Epoch 89/1000 
	 loss: 30.6617, MinusLogProbMetric: 30.6617, val_loss: 30.5022, val_MinusLogProbMetric: 30.5022

Epoch 89: val_loss improved from 30.51665 to 30.50221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 35s - loss: 30.6617 - MinusLogProbMetric: 30.6617 - val_loss: 30.5022 - val_MinusLogProbMetric: 30.5022 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 90/1000
2023-10-27 03:06:33.249 
Epoch 90/1000 
	 loss: 30.7923, MinusLogProbMetric: 30.7923, val_loss: 30.6025, val_MinusLogProbMetric: 30.6025

Epoch 90: val_loss did not improve from 30.50221
196/196 - 41s - loss: 30.7923 - MinusLogProbMetric: 30.7923 - val_loss: 30.6025 - val_MinusLogProbMetric: 30.6025 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 91/1000
2023-10-27 03:07:14.830 
Epoch 91/1000 
	 loss: 30.6545, MinusLogProbMetric: 30.6545, val_loss: 30.0269, val_MinusLogProbMetric: 30.0269

Epoch 91: val_loss improved from 30.50221 to 30.02694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 42s - loss: 30.6545 - MinusLogProbMetric: 30.6545 - val_loss: 30.0269 - val_MinusLogProbMetric: 30.0269 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 92/1000
2023-10-27 03:07:50.017 
Epoch 92/1000 
	 loss: 30.6289, MinusLogProbMetric: 30.6289, val_loss: 30.8654, val_MinusLogProbMetric: 30.8654

Epoch 92: val_loss did not improve from 30.02694
196/196 - 34s - loss: 30.6289 - MinusLogProbMetric: 30.6289 - val_loss: 30.8654 - val_MinusLogProbMetric: 30.8654 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 93/1000
2023-10-27 03:08:24.058 
Epoch 93/1000 
	 loss: 30.6943, MinusLogProbMetric: 30.6943, val_loss: 30.6709, val_MinusLogProbMetric: 30.6709

Epoch 93: val_loss did not improve from 30.02694
196/196 - 34s - loss: 30.6943 - MinusLogProbMetric: 30.6943 - val_loss: 30.6709 - val_MinusLogProbMetric: 30.6709 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 94/1000
2023-10-27 03:09:02.318 
Epoch 94/1000 
	 loss: 30.6611, MinusLogProbMetric: 30.6611, val_loss: 31.3143, val_MinusLogProbMetric: 31.3143

Epoch 94: val_loss did not improve from 30.02694
196/196 - 38s - loss: 30.6611 - MinusLogProbMetric: 30.6611 - val_loss: 31.3143 - val_MinusLogProbMetric: 31.3143 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 95/1000
2023-10-27 03:09:41.290 
Epoch 95/1000 
	 loss: 30.4590, MinusLogProbMetric: 30.4590, val_loss: 31.3044, val_MinusLogProbMetric: 31.3044

Epoch 95: val_loss did not improve from 30.02694
196/196 - 39s - loss: 30.4590 - MinusLogProbMetric: 30.4590 - val_loss: 31.3044 - val_MinusLogProbMetric: 31.3044 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 96/1000
2023-10-27 03:10:18.483 
Epoch 96/1000 
	 loss: 30.4571, MinusLogProbMetric: 30.4571, val_loss: 29.9402, val_MinusLogProbMetric: 29.9402

Epoch 96: val_loss improved from 30.02694 to 29.94021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 38s - loss: 30.4571 - MinusLogProbMetric: 30.4571 - val_loss: 29.9402 - val_MinusLogProbMetric: 29.9402 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 97/1000
2023-10-27 03:10:53.245 
Epoch 97/1000 
	 loss: 30.5853, MinusLogProbMetric: 30.5853, val_loss: 31.2323, val_MinusLogProbMetric: 31.2323

Epoch 97: val_loss did not improve from 29.94021
196/196 - 34s - loss: 30.5853 - MinusLogProbMetric: 30.5853 - val_loss: 31.2323 - val_MinusLogProbMetric: 31.2323 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 98/1000
2023-10-27 03:11:31.828 
Epoch 98/1000 
	 loss: 30.5688, MinusLogProbMetric: 30.5688, val_loss: 30.5341, val_MinusLogProbMetric: 30.5341

Epoch 98: val_loss did not improve from 29.94021
196/196 - 39s - loss: 30.5688 - MinusLogProbMetric: 30.5688 - val_loss: 30.5341 - val_MinusLogProbMetric: 30.5341 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 99/1000
2023-10-27 03:12:10.246 
Epoch 99/1000 
	 loss: 30.4331, MinusLogProbMetric: 30.4331, val_loss: 32.1791, val_MinusLogProbMetric: 32.1791

Epoch 99: val_loss did not improve from 29.94021
196/196 - 38s - loss: 30.4331 - MinusLogProbMetric: 30.4331 - val_loss: 32.1791 - val_MinusLogProbMetric: 32.1791 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 100/1000
2023-10-27 03:12:44.661 
Epoch 100/1000 
	 loss: 30.5201, MinusLogProbMetric: 30.5201, val_loss: 31.1967, val_MinusLogProbMetric: 31.1967

Epoch 100: val_loss did not improve from 29.94021
196/196 - 34s - loss: 30.5201 - MinusLogProbMetric: 30.5201 - val_loss: 31.1967 - val_MinusLogProbMetric: 31.1967 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 101/1000
2023-10-27 03:13:18.820 
Epoch 101/1000 
	 loss: 30.4573, MinusLogProbMetric: 30.4573, val_loss: 31.5441, val_MinusLogProbMetric: 31.5441

Epoch 101: val_loss did not improve from 29.94021
196/196 - 34s - loss: 30.4573 - MinusLogProbMetric: 30.4573 - val_loss: 31.5441 - val_MinusLogProbMetric: 31.5441 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 102/1000
2023-10-27 03:13:57.233 
Epoch 102/1000 
	 loss: 30.4021, MinusLogProbMetric: 30.4021, val_loss: 29.9526, val_MinusLogProbMetric: 29.9526

Epoch 102: val_loss did not improve from 29.94021
196/196 - 38s - loss: 30.4021 - MinusLogProbMetric: 30.4021 - val_loss: 29.9526 - val_MinusLogProbMetric: 29.9526 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 103/1000
2023-10-27 03:14:37.894 
Epoch 103/1000 
	 loss: 30.3335, MinusLogProbMetric: 30.3335, val_loss: 31.0659, val_MinusLogProbMetric: 31.0659

Epoch 103: val_loss did not improve from 29.94021
196/196 - 41s - loss: 30.3335 - MinusLogProbMetric: 30.3335 - val_loss: 31.0659 - val_MinusLogProbMetric: 31.0659 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 104/1000
2023-10-27 03:15:11.541 
Epoch 104/1000 
	 loss: 30.2756, MinusLogProbMetric: 30.2756, val_loss: 30.0855, val_MinusLogProbMetric: 30.0855

Epoch 104: val_loss did not improve from 29.94021
196/196 - 34s - loss: 30.2756 - MinusLogProbMetric: 30.2756 - val_loss: 30.0855 - val_MinusLogProbMetric: 30.0855 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 105/1000
2023-10-27 03:15:45.613 
Epoch 105/1000 
	 loss: 30.3556, MinusLogProbMetric: 30.3556, val_loss: 30.6603, val_MinusLogProbMetric: 30.6603

Epoch 105: val_loss did not improve from 29.94021
196/196 - 34s - loss: 30.3556 - MinusLogProbMetric: 30.3556 - val_loss: 30.6603 - val_MinusLogProbMetric: 30.6603 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 106/1000
2023-10-27 03:16:23.022 
Epoch 106/1000 
	 loss: 30.2795, MinusLogProbMetric: 30.2795, val_loss: 30.2182, val_MinusLogProbMetric: 30.2182

Epoch 106: val_loss did not improve from 29.94021
196/196 - 37s - loss: 30.2795 - MinusLogProbMetric: 30.2795 - val_loss: 30.2182 - val_MinusLogProbMetric: 30.2182 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 107/1000
2023-10-27 03:17:02.410 
Epoch 107/1000 
	 loss: 30.3062, MinusLogProbMetric: 30.3062, val_loss: 30.9371, val_MinusLogProbMetric: 30.9371

Epoch 107: val_loss did not improve from 29.94021
196/196 - 39s - loss: 30.3062 - MinusLogProbMetric: 30.3062 - val_loss: 30.9371 - val_MinusLogProbMetric: 30.9371 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 108/1000
2023-10-27 03:17:37.018 
Epoch 108/1000 
	 loss: 30.2450, MinusLogProbMetric: 30.2450, val_loss: 29.6720, val_MinusLogProbMetric: 29.6720

Epoch 108: val_loss improved from 29.94021 to 29.67196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 35s - loss: 30.2450 - MinusLogProbMetric: 30.2450 - val_loss: 29.6720 - val_MinusLogProbMetric: 29.6720 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 109/1000
2023-10-27 03:18:12.034 
Epoch 109/1000 
	 loss: 30.2642, MinusLogProbMetric: 30.2642, val_loss: 31.8522, val_MinusLogProbMetric: 31.8522

Epoch 109: val_loss did not improve from 29.67196
196/196 - 34s - loss: 30.2642 - MinusLogProbMetric: 30.2642 - val_loss: 31.8522 - val_MinusLogProbMetric: 31.8522 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 110/1000
2023-10-27 03:18:46.669 
Epoch 110/1000 
	 loss: 30.1606, MinusLogProbMetric: 30.1606, val_loss: 31.2228, val_MinusLogProbMetric: 31.2228

Epoch 110: val_loss did not improve from 29.67196
196/196 - 35s - loss: 30.1606 - MinusLogProbMetric: 30.1606 - val_loss: 31.2228 - val_MinusLogProbMetric: 31.2228 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 111/1000
2023-10-27 03:19:27.891 
Epoch 111/1000 
	 loss: 30.2110, MinusLogProbMetric: 30.2110, val_loss: 30.9596, val_MinusLogProbMetric: 30.9596

Epoch 111: val_loss did not improve from 29.67196
196/196 - 41s - loss: 30.2110 - MinusLogProbMetric: 30.2110 - val_loss: 30.9596 - val_MinusLogProbMetric: 30.9596 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 112/1000
2023-10-27 03:20:02.495 
Epoch 112/1000 
	 loss: 30.1863, MinusLogProbMetric: 30.1863, val_loss: 30.8930, val_MinusLogProbMetric: 30.8930

Epoch 112: val_loss did not improve from 29.67196
196/196 - 35s - loss: 30.1863 - MinusLogProbMetric: 30.1863 - val_loss: 30.8930 - val_MinusLogProbMetric: 30.8930 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 113/1000
2023-10-27 03:20:36.811 
Epoch 113/1000 
	 loss: 30.2941, MinusLogProbMetric: 30.2941, val_loss: 30.3474, val_MinusLogProbMetric: 30.3474

Epoch 113: val_loss did not improve from 29.67196
196/196 - 34s - loss: 30.2941 - MinusLogProbMetric: 30.2941 - val_loss: 30.3474 - val_MinusLogProbMetric: 30.3474 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 114/1000
2023-10-27 03:21:16.246 
Epoch 114/1000 
	 loss: 30.1837, MinusLogProbMetric: 30.1837, val_loss: 30.2233, val_MinusLogProbMetric: 30.2233

Epoch 114: val_loss did not improve from 29.67196
196/196 - 39s - loss: 30.1837 - MinusLogProbMetric: 30.1837 - val_loss: 30.2233 - val_MinusLogProbMetric: 30.2233 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 115/1000
2023-10-27 03:21:57.375 
Epoch 115/1000 
	 loss: 30.1457, MinusLogProbMetric: 30.1457, val_loss: 30.6775, val_MinusLogProbMetric: 30.6775

Epoch 115: val_loss did not improve from 29.67196
196/196 - 41s - loss: 30.1457 - MinusLogProbMetric: 30.1457 - val_loss: 30.6775 - val_MinusLogProbMetric: 30.6775 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 116/1000
2023-10-27 03:22:30.680 
Epoch 116/1000 
	 loss: 30.1242, MinusLogProbMetric: 30.1242, val_loss: 29.7130, val_MinusLogProbMetric: 29.7130

Epoch 116: val_loss did not improve from 29.67196
196/196 - 33s - loss: 30.1242 - MinusLogProbMetric: 30.1242 - val_loss: 29.7130 - val_MinusLogProbMetric: 29.7130 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 117/1000
2023-10-27 03:23:05.948 
Epoch 117/1000 
	 loss: 30.0531, MinusLogProbMetric: 30.0531, val_loss: 31.0439, val_MinusLogProbMetric: 31.0439

Epoch 117: val_loss did not improve from 29.67196
196/196 - 35s - loss: 30.0531 - MinusLogProbMetric: 30.0531 - val_loss: 31.0439 - val_MinusLogProbMetric: 31.0439 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 118/1000
2023-10-27 03:23:42.459 
Epoch 118/1000 
	 loss: 30.1779, MinusLogProbMetric: 30.1779, val_loss: 30.3216, val_MinusLogProbMetric: 30.3216

Epoch 118: val_loss did not improve from 29.67196
196/196 - 37s - loss: 30.1779 - MinusLogProbMetric: 30.1779 - val_loss: 30.3216 - val_MinusLogProbMetric: 30.3216 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 119/1000
2023-10-27 03:24:24.880 
Epoch 119/1000 
	 loss: 30.0872, MinusLogProbMetric: 30.0872, val_loss: 30.1059, val_MinusLogProbMetric: 30.1059

Epoch 119: val_loss did not improve from 29.67196
196/196 - 42s - loss: 30.0872 - MinusLogProbMetric: 30.0872 - val_loss: 30.1059 - val_MinusLogProbMetric: 30.1059 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 120/1000
2023-10-27 03:25:07.007 
Epoch 120/1000 
	 loss: 30.0908, MinusLogProbMetric: 30.0908, val_loss: 30.5261, val_MinusLogProbMetric: 30.5261

Epoch 120: val_loss did not improve from 29.67196
196/196 - 42s - loss: 30.0908 - MinusLogProbMetric: 30.0908 - val_loss: 30.5261 - val_MinusLogProbMetric: 30.5261 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 121/1000
2023-10-27 03:25:48.197 
Epoch 121/1000 
	 loss: 30.0561, MinusLogProbMetric: 30.0561, val_loss: 29.9711, val_MinusLogProbMetric: 29.9711

Epoch 121: val_loss did not improve from 29.67196
196/196 - 41s - loss: 30.0561 - MinusLogProbMetric: 30.0561 - val_loss: 29.9711 - val_MinusLogProbMetric: 29.9711 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 122/1000
2023-10-27 03:26:28.492 
Epoch 122/1000 
	 loss: 29.9317, MinusLogProbMetric: 29.9317, val_loss: 30.3137, val_MinusLogProbMetric: 30.3137

Epoch 122: val_loss did not improve from 29.67196
196/196 - 40s - loss: 29.9317 - MinusLogProbMetric: 29.9317 - val_loss: 30.3137 - val_MinusLogProbMetric: 30.3137 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 123/1000
2023-10-27 03:27:03.357 
Epoch 123/1000 
	 loss: 29.9756, MinusLogProbMetric: 29.9756, val_loss: 31.4083, val_MinusLogProbMetric: 31.4083

Epoch 123: val_loss did not improve from 29.67196
196/196 - 35s - loss: 29.9756 - MinusLogProbMetric: 29.9756 - val_loss: 31.4083 - val_MinusLogProbMetric: 31.4083 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 124/1000
2023-10-27 03:27:38.024 
Epoch 124/1000 
	 loss: 29.9531, MinusLogProbMetric: 29.9531, val_loss: 29.9165, val_MinusLogProbMetric: 29.9165

Epoch 124: val_loss did not improve from 29.67196
196/196 - 35s - loss: 29.9531 - MinusLogProbMetric: 29.9531 - val_loss: 29.9165 - val_MinusLogProbMetric: 29.9165 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 125/1000
2023-10-27 03:28:18.980 
Epoch 125/1000 
	 loss: 29.9463, MinusLogProbMetric: 29.9463, val_loss: 31.2551, val_MinusLogProbMetric: 31.2551

Epoch 125: val_loss did not improve from 29.67196
196/196 - 41s - loss: 29.9463 - MinusLogProbMetric: 29.9463 - val_loss: 31.2551 - val_MinusLogProbMetric: 31.2551 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 126/1000
2023-10-27 03:29:00.856 
Epoch 126/1000 
	 loss: 29.8826, MinusLogProbMetric: 29.8826, val_loss: 29.9765, val_MinusLogProbMetric: 29.9765

Epoch 126: val_loss did not improve from 29.67196
196/196 - 42s - loss: 29.8826 - MinusLogProbMetric: 29.8826 - val_loss: 29.9765 - val_MinusLogProbMetric: 29.9765 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 127/1000
2023-10-27 03:29:43.006 
Epoch 127/1000 
	 loss: 29.8840, MinusLogProbMetric: 29.8840, val_loss: 30.4945, val_MinusLogProbMetric: 30.4945

Epoch 127: val_loss did not improve from 29.67196
196/196 - 42s - loss: 29.8840 - MinusLogProbMetric: 29.8840 - val_loss: 30.4945 - val_MinusLogProbMetric: 30.4945 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 128/1000
2023-10-27 03:30:25.196 
Epoch 128/1000 
	 loss: 29.9649, MinusLogProbMetric: 29.9649, val_loss: 30.2949, val_MinusLogProbMetric: 30.2949

Epoch 128: val_loss did not improve from 29.67196
196/196 - 42s - loss: 29.9649 - MinusLogProbMetric: 29.9649 - val_loss: 30.2949 - val_MinusLogProbMetric: 30.2949 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 129/1000
2023-10-27 03:31:07.806 
Epoch 129/1000 
	 loss: 29.8683, MinusLogProbMetric: 29.8683, val_loss: 29.8047, val_MinusLogProbMetric: 29.8047

Epoch 129: val_loss did not improve from 29.67196
196/196 - 43s - loss: 29.8683 - MinusLogProbMetric: 29.8683 - val_loss: 29.8047 - val_MinusLogProbMetric: 29.8047 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 130/1000
2023-10-27 03:31:50.283 
Epoch 130/1000 
	 loss: 29.8841, MinusLogProbMetric: 29.8841, val_loss: 29.7048, val_MinusLogProbMetric: 29.7048

Epoch 130: val_loss did not improve from 29.67196
196/196 - 42s - loss: 29.8841 - MinusLogProbMetric: 29.8841 - val_loss: 29.7048 - val_MinusLogProbMetric: 29.7048 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 131/1000
2023-10-27 03:32:32.325 
Epoch 131/1000 
	 loss: 29.7718, MinusLogProbMetric: 29.7718, val_loss: 29.6506, val_MinusLogProbMetric: 29.6506

Epoch 131: val_loss improved from 29.67196 to 29.65063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 29.7718 - MinusLogProbMetric: 29.7718 - val_loss: 29.6506 - val_MinusLogProbMetric: 29.6506 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 132/1000
2023-10-27 03:33:14.196 
Epoch 132/1000 
	 loss: 29.7904, MinusLogProbMetric: 29.7904, val_loss: 30.0554, val_MinusLogProbMetric: 30.0554

Epoch 132: val_loss did not improve from 29.65063
196/196 - 41s - loss: 29.7904 - MinusLogProbMetric: 29.7904 - val_loss: 30.0554 - val_MinusLogProbMetric: 30.0554 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 133/1000
2023-10-27 03:33:55.602 
Epoch 133/1000 
	 loss: 29.7451, MinusLogProbMetric: 29.7451, val_loss: 30.5740, val_MinusLogProbMetric: 30.5740

Epoch 133: val_loss did not improve from 29.65063
196/196 - 41s - loss: 29.7451 - MinusLogProbMetric: 29.7451 - val_loss: 30.5740 - val_MinusLogProbMetric: 30.5740 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 134/1000
2023-10-27 03:34:37.448 
Epoch 134/1000 
	 loss: 29.7410, MinusLogProbMetric: 29.7410, val_loss: 30.5875, val_MinusLogProbMetric: 30.5875

Epoch 134: val_loss did not improve from 29.65063
196/196 - 42s - loss: 29.7410 - MinusLogProbMetric: 29.7410 - val_loss: 30.5875 - val_MinusLogProbMetric: 30.5875 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 135/1000
2023-10-27 03:35:19.875 
Epoch 135/1000 
	 loss: 29.7197, MinusLogProbMetric: 29.7197, val_loss: 29.8956, val_MinusLogProbMetric: 29.8956

Epoch 135: val_loss did not improve from 29.65063
196/196 - 42s - loss: 29.7197 - MinusLogProbMetric: 29.7197 - val_loss: 29.8956 - val_MinusLogProbMetric: 29.8956 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 136/1000
2023-10-27 03:36:01.690 
Epoch 136/1000 
	 loss: 29.6973, MinusLogProbMetric: 29.6973, val_loss: 29.5898, val_MinusLogProbMetric: 29.5898

Epoch 136: val_loss improved from 29.65063 to 29.58975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 29.6973 - MinusLogProbMetric: 29.6973 - val_loss: 29.5898 - val_MinusLogProbMetric: 29.5898 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 137/1000
2023-10-27 03:36:44.588 
Epoch 137/1000 
	 loss: 29.9929, MinusLogProbMetric: 29.9929, val_loss: 30.4010, val_MinusLogProbMetric: 30.4010

Epoch 137: val_loss did not improve from 29.58975
196/196 - 42s - loss: 29.9929 - MinusLogProbMetric: 29.9929 - val_loss: 30.4010 - val_MinusLogProbMetric: 30.4010 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 138/1000
2023-10-27 03:37:26.897 
Epoch 138/1000 
	 loss: 29.7282, MinusLogProbMetric: 29.7282, val_loss: 30.4208, val_MinusLogProbMetric: 30.4208

Epoch 138: val_loss did not improve from 29.58975
196/196 - 42s - loss: 29.7282 - MinusLogProbMetric: 29.7282 - val_loss: 30.4208 - val_MinusLogProbMetric: 30.4208 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 139/1000
2023-10-27 03:38:09.312 
Epoch 139/1000 
	 loss: 29.6730, MinusLogProbMetric: 29.6730, val_loss: 32.0393, val_MinusLogProbMetric: 32.0393

Epoch 139: val_loss did not improve from 29.58975
196/196 - 42s - loss: 29.6730 - MinusLogProbMetric: 29.6730 - val_loss: 32.0393 - val_MinusLogProbMetric: 32.0393 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 140/1000
2023-10-27 03:38:51.902 
Epoch 140/1000 
	 loss: 29.9338, MinusLogProbMetric: 29.9338, val_loss: 29.7209, val_MinusLogProbMetric: 29.7209

Epoch 140: val_loss did not improve from 29.58975
196/196 - 43s - loss: 29.9338 - MinusLogProbMetric: 29.9338 - val_loss: 29.7209 - val_MinusLogProbMetric: 29.7209 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 141/1000
2023-10-27 03:39:33.753 
Epoch 141/1000 
	 loss: 29.6844, MinusLogProbMetric: 29.6844, val_loss: 30.1067, val_MinusLogProbMetric: 30.1067

Epoch 141: val_loss did not improve from 29.58975
196/196 - 42s - loss: 29.6844 - MinusLogProbMetric: 29.6844 - val_loss: 30.1067 - val_MinusLogProbMetric: 30.1067 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 142/1000
2023-10-27 03:40:16.035 
Epoch 142/1000 
	 loss: 29.7041, MinusLogProbMetric: 29.7041, val_loss: 29.4896, val_MinusLogProbMetric: 29.4896

Epoch 142: val_loss improved from 29.58975 to 29.48962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 29.7041 - MinusLogProbMetric: 29.7041 - val_loss: 29.4896 - val_MinusLogProbMetric: 29.4896 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 143/1000
2023-10-27 03:40:59.092 
Epoch 143/1000 
	 loss: 29.6930, MinusLogProbMetric: 29.6930, val_loss: 29.8576, val_MinusLogProbMetric: 29.8576

Epoch 143: val_loss did not improve from 29.48962
196/196 - 42s - loss: 29.6930 - MinusLogProbMetric: 29.6930 - val_loss: 29.8576 - val_MinusLogProbMetric: 29.8576 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 144/1000
2023-10-27 03:41:41.938 
Epoch 144/1000 
	 loss: 29.6291, MinusLogProbMetric: 29.6291, val_loss: 29.5871, val_MinusLogProbMetric: 29.5871

Epoch 144: val_loss did not improve from 29.48962
196/196 - 43s - loss: 29.6291 - MinusLogProbMetric: 29.6291 - val_loss: 29.5871 - val_MinusLogProbMetric: 29.5871 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 145/1000
2023-10-27 03:42:23.836 
Epoch 145/1000 
	 loss: 29.5752, MinusLogProbMetric: 29.5752, val_loss: 30.9718, val_MinusLogProbMetric: 30.9718

Epoch 145: val_loss did not improve from 29.48962
196/196 - 42s - loss: 29.5752 - MinusLogProbMetric: 29.5752 - val_loss: 30.9718 - val_MinusLogProbMetric: 30.9718 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 146/1000
2023-10-27 03:43:05.873 
Epoch 146/1000 
	 loss: 29.6227, MinusLogProbMetric: 29.6227, val_loss: 29.8694, val_MinusLogProbMetric: 29.8694

Epoch 146: val_loss did not improve from 29.48962
196/196 - 42s - loss: 29.6227 - MinusLogProbMetric: 29.6227 - val_loss: 29.8694 - val_MinusLogProbMetric: 29.8694 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 147/1000
2023-10-27 03:43:48.661 
Epoch 147/1000 
	 loss: 29.6630, MinusLogProbMetric: 29.6630, val_loss: 29.9253, val_MinusLogProbMetric: 29.9253

Epoch 147: val_loss did not improve from 29.48962
196/196 - 43s - loss: 29.6630 - MinusLogProbMetric: 29.6630 - val_loss: 29.9253 - val_MinusLogProbMetric: 29.9253 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 148/1000
2023-10-27 03:44:30.956 
Epoch 148/1000 
	 loss: 29.6375, MinusLogProbMetric: 29.6375, val_loss: 29.6529, val_MinusLogProbMetric: 29.6529

Epoch 148: val_loss did not improve from 29.48962
196/196 - 42s - loss: 29.6375 - MinusLogProbMetric: 29.6375 - val_loss: 29.6529 - val_MinusLogProbMetric: 29.6529 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 149/1000
2023-10-27 03:45:13.062 
Epoch 149/1000 
	 loss: 29.5740, MinusLogProbMetric: 29.5740, val_loss: 30.3284, val_MinusLogProbMetric: 30.3284

Epoch 149: val_loss did not improve from 29.48962
196/196 - 42s - loss: 29.5740 - MinusLogProbMetric: 29.5740 - val_loss: 30.3284 - val_MinusLogProbMetric: 30.3284 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 150/1000
2023-10-27 03:45:54.454 
Epoch 150/1000 
	 loss: 29.5437, MinusLogProbMetric: 29.5437, val_loss: 29.7584, val_MinusLogProbMetric: 29.7584

Epoch 150: val_loss did not improve from 29.48962
196/196 - 41s - loss: 29.5437 - MinusLogProbMetric: 29.5437 - val_loss: 29.7584 - val_MinusLogProbMetric: 29.7584 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 151/1000
2023-10-27 03:46:34.748 
Epoch 151/1000 
	 loss: 29.5034, MinusLogProbMetric: 29.5034, val_loss: 29.6430, val_MinusLogProbMetric: 29.6430

Epoch 151: val_loss did not improve from 29.48962
196/196 - 40s - loss: 29.5034 - MinusLogProbMetric: 29.5034 - val_loss: 29.6430 - val_MinusLogProbMetric: 29.6430 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 152/1000
2023-10-27 03:47:10.478 
Epoch 152/1000 
	 loss: 29.5949, MinusLogProbMetric: 29.5949, val_loss: 29.4550, val_MinusLogProbMetric: 29.4550

Epoch 152: val_loss improved from 29.48962 to 29.45501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 36s - loss: 29.5949 - MinusLogProbMetric: 29.5949 - val_loss: 29.4550 - val_MinusLogProbMetric: 29.4550 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 153/1000
2023-10-27 03:47:46.595 
Epoch 153/1000 
	 loss: 29.5568, MinusLogProbMetric: 29.5568, val_loss: 30.6321, val_MinusLogProbMetric: 30.6321

Epoch 153: val_loss did not improve from 29.45501
196/196 - 36s - loss: 29.5568 - MinusLogProbMetric: 29.5568 - val_loss: 30.6321 - val_MinusLogProbMetric: 30.6321 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 154/1000
2023-10-27 03:48:24.286 
Epoch 154/1000 
	 loss: 29.5875, MinusLogProbMetric: 29.5875, val_loss: 29.6625, val_MinusLogProbMetric: 29.6625

Epoch 154: val_loss did not improve from 29.45501
196/196 - 38s - loss: 29.5875 - MinusLogProbMetric: 29.5875 - val_loss: 29.6625 - val_MinusLogProbMetric: 29.6625 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 155/1000
2023-10-27 03:49:04.053 
Epoch 155/1000 
	 loss: 29.5653, MinusLogProbMetric: 29.5653, val_loss: 29.7790, val_MinusLogProbMetric: 29.7790

Epoch 155: val_loss did not improve from 29.45501
196/196 - 40s - loss: 29.5653 - MinusLogProbMetric: 29.5653 - val_loss: 29.7790 - val_MinusLogProbMetric: 29.7790 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 156/1000
2023-10-27 03:49:42.736 
Epoch 156/1000 
	 loss: 29.5329, MinusLogProbMetric: 29.5329, val_loss: 29.9454, val_MinusLogProbMetric: 29.9454

Epoch 156: val_loss did not improve from 29.45501
196/196 - 39s - loss: 29.5329 - MinusLogProbMetric: 29.5329 - val_loss: 29.9454 - val_MinusLogProbMetric: 29.9454 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 157/1000
2023-10-27 03:50:18.299 
Epoch 157/1000 
	 loss: 29.6106, MinusLogProbMetric: 29.6106, val_loss: 29.3960, val_MinusLogProbMetric: 29.3960

Epoch 157: val_loss improved from 29.45501 to 29.39598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 36s - loss: 29.6106 - MinusLogProbMetric: 29.6106 - val_loss: 29.3960 - val_MinusLogProbMetric: 29.3960 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 158/1000
2023-10-27 03:50:53.515 
Epoch 158/1000 
	 loss: 29.5115, MinusLogProbMetric: 29.5115, val_loss: 29.9938, val_MinusLogProbMetric: 29.9938

Epoch 158: val_loss did not improve from 29.39598
196/196 - 35s - loss: 29.5115 - MinusLogProbMetric: 29.5115 - val_loss: 29.9938 - val_MinusLogProbMetric: 29.9938 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 159/1000
2023-10-27 03:51:32.148 
Epoch 159/1000 
	 loss: 29.5982, MinusLogProbMetric: 29.5982, val_loss: 30.7027, val_MinusLogProbMetric: 30.7027

Epoch 159: val_loss did not improve from 29.39598
196/196 - 39s - loss: 29.5982 - MinusLogProbMetric: 29.5982 - val_loss: 30.7027 - val_MinusLogProbMetric: 30.7027 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 160/1000
2023-10-27 03:52:13.616 
Epoch 160/1000 
	 loss: 29.5565, MinusLogProbMetric: 29.5565, val_loss: 29.5271, val_MinusLogProbMetric: 29.5271

Epoch 160: val_loss did not improve from 29.39598
196/196 - 41s - loss: 29.5565 - MinusLogProbMetric: 29.5565 - val_loss: 29.5271 - val_MinusLogProbMetric: 29.5271 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 161/1000
2023-10-27 03:52:49.998 
Epoch 161/1000 
	 loss: 29.4923, MinusLogProbMetric: 29.4923, val_loss: 29.7995, val_MinusLogProbMetric: 29.7995

Epoch 161: val_loss did not improve from 29.39598
196/196 - 36s - loss: 29.4923 - MinusLogProbMetric: 29.4923 - val_loss: 29.7995 - val_MinusLogProbMetric: 29.7995 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 162/1000
2023-10-27 03:53:27.905 
Epoch 162/1000 
	 loss: 29.4678, MinusLogProbMetric: 29.4678, val_loss: 30.8704, val_MinusLogProbMetric: 30.8704

Epoch 162: val_loss did not improve from 29.39598
196/196 - 38s - loss: 29.4678 - MinusLogProbMetric: 29.4678 - val_loss: 30.8704 - val_MinusLogProbMetric: 30.8704 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 163/1000
2023-10-27 03:54:03.970 
Epoch 163/1000 
	 loss: 29.5640, MinusLogProbMetric: 29.5640, val_loss: 29.7928, val_MinusLogProbMetric: 29.7928

Epoch 163: val_loss did not improve from 29.39598
196/196 - 36s - loss: 29.5640 - MinusLogProbMetric: 29.5640 - val_loss: 29.7928 - val_MinusLogProbMetric: 29.7928 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 164/1000
2023-10-27 03:54:39.518 
Epoch 164/1000 
	 loss: 29.4004, MinusLogProbMetric: 29.4004, val_loss: 29.6627, val_MinusLogProbMetric: 29.6627

Epoch 164: val_loss did not improve from 29.39598
196/196 - 36s - loss: 29.4004 - MinusLogProbMetric: 29.4004 - val_loss: 29.6627 - val_MinusLogProbMetric: 29.6627 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 165/1000
2023-10-27 03:55:14.916 
Epoch 165/1000 
	 loss: 29.4334, MinusLogProbMetric: 29.4334, val_loss: 30.0727, val_MinusLogProbMetric: 30.0727

Epoch 165: val_loss did not improve from 29.39598
196/196 - 35s - loss: 29.4334 - MinusLogProbMetric: 29.4334 - val_loss: 30.0727 - val_MinusLogProbMetric: 30.0727 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 166/1000
2023-10-27 03:55:50.042 
Epoch 166/1000 
	 loss: 29.3684, MinusLogProbMetric: 29.3684, val_loss: 29.5685, val_MinusLogProbMetric: 29.5685

Epoch 166: val_loss did not improve from 29.39598
196/196 - 35s - loss: 29.3684 - MinusLogProbMetric: 29.3684 - val_loss: 29.5685 - val_MinusLogProbMetric: 29.5685 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 167/1000
2023-10-27 03:56:25.211 
Epoch 167/1000 
	 loss: 29.5748, MinusLogProbMetric: 29.5748, val_loss: 29.6628, val_MinusLogProbMetric: 29.6628

Epoch 167: val_loss did not improve from 29.39598
196/196 - 35s - loss: 29.5748 - MinusLogProbMetric: 29.5748 - val_loss: 29.6628 - val_MinusLogProbMetric: 29.6628 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 168/1000
2023-10-27 03:57:00.437 
Epoch 168/1000 
	 loss: 29.3318, MinusLogProbMetric: 29.3318, val_loss: 30.2600, val_MinusLogProbMetric: 30.2600

Epoch 168: val_loss did not improve from 29.39598
196/196 - 35s - loss: 29.3318 - MinusLogProbMetric: 29.3318 - val_loss: 30.2600 - val_MinusLogProbMetric: 30.2600 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 169/1000
2023-10-27 03:57:35.621 
Epoch 169/1000 
	 loss: 29.4893, MinusLogProbMetric: 29.4893, val_loss: 30.0836, val_MinusLogProbMetric: 30.0836

Epoch 169: val_loss did not improve from 29.39598
196/196 - 35s - loss: 29.4893 - MinusLogProbMetric: 29.4893 - val_loss: 30.0836 - val_MinusLogProbMetric: 30.0836 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 170/1000
2023-10-27 03:58:10.735 
Epoch 170/1000 
	 loss: 29.4147, MinusLogProbMetric: 29.4147, val_loss: 29.2959, val_MinusLogProbMetric: 29.2959

Epoch 170: val_loss improved from 29.39598 to 29.29586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 36s - loss: 29.4147 - MinusLogProbMetric: 29.4147 - val_loss: 29.2959 - val_MinusLogProbMetric: 29.2959 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 171/1000
2023-10-27 03:58:46.100 
Epoch 171/1000 
	 loss: 29.3767, MinusLogProbMetric: 29.3767, val_loss: 29.4117, val_MinusLogProbMetric: 29.4117

Epoch 171: val_loss did not improve from 29.29586
196/196 - 35s - loss: 29.3767 - MinusLogProbMetric: 29.3767 - val_loss: 29.4117 - val_MinusLogProbMetric: 29.4117 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 172/1000
2023-10-27 03:59:21.341 
Epoch 172/1000 
	 loss: 29.3955, MinusLogProbMetric: 29.3955, val_loss: 29.9158, val_MinusLogProbMetric: 29.9158

Epoch 172: val_loss did not improve from 29.29586
196/196 - 35s - loss: 29.3955 - MinusLogProbMetric: 29.3955 - val_loss: 29.9158 - val_MinusLogProbMetric: 29.9158 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 173/1000
2023-10-27 03:59:56.682 
Epoch 173/1000 
	 loss: 29.2465, MinusLogProbMetric: 29.2465, val_loss: 30.3946, val_MinusLogProbMetric: 30.3946

Epoch 173: val_loss did not improve from 29.29586
196/196 - 35s - loss: 29.2465 - MinusLogProbMetric: 29.2465 - val_loss: 30.3946 - val_MinusLogProbMetric: 30.3946 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 174/1000
2023-10-27 04:00:32.584 
Epoch 174/1000 
	 loss: 29.3734, MinusLogProbMetric: 29.3734, val_loss: 29.9644, val_MinusLogProbMetric: 29.9644

Epoch 174: val_loss did not improve from 29.29586
196/196 - 36s - loss: 29.3734 - MinusLogProbMetric: 29.3734 - val_loss: 29.9644 - val_MinusLogProbMetric: 29.9644 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 175/1000
2023-10-27 04:01:07.075 
Epoch 175/1000 
	 loss: 29.2581, MinusLogProbMetric: 29.2581, val_loss: 30.1311, val_MinusLogProbMetric: 30.1311

Epoch 175: val_loss did not improve from 29.29586
196/196 - 34s - loss: 29.2581 - MinusLogProbMetric: 29.2581 - val_loss: 30.1311 - val_MinusLogProbMetric: 30.1311 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 176/1000
2023-10-27 04:01:41.929 
Epoch 176/1000 
	 loss: 29.3529, MinusLogProbMetric: 29.3529, val_loss: 29.5003, val_MinusLogProbMetric: 29.5003

Epoch 176: val_loss did not improve from 29.29586
196/196 - 35s - loss: 29.3529 - MinusLogProbMetric: 29.3529 - val_loss: 29.5003 - val_MinusLogProbMetric: 29.5003 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 177/1000
2023-10-27 04:02:16.484 
Epoch 177/1000 
	 loss: 29.3397, MinusLogProbMetric: 29.3397, val_loss: 29.4205, val_MinusLogProbMetric: 29.4205

Epoch 177: val_loss did not improve from 29.29586
196/196 - 35s - loss: 29.3397 - MinusLogProbMetric: 29.3397 - val_loss: 29.4205 - val_MinusLogProbMetric: 29.4205 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 178/1000
2023-10-27 04:02:52.521 
Epoch 178/1000 
	 loss: 29.2381, MinusLogProbMetric: 29.2381, val_loss: 29.3738, val_MinusLogProbMetric: 29.3738

Epoch 178: val_loss did not improve from 29.29586
196/196 - 36s - loss: 29.2381 - MinusLogProbMetric: 29.2381 - val_loss: 29.3738 - val_MinusLogProbMetric: 29.3738 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 179/1000
2023-10-27 04:03:28.365 
Epoch 179/1000 
	 loss: 29.2922, MinusLogProbMetric: 29.2922, val_loss: 30.9701, val_MinusLogProbMetric: 30.9701

Epoch 179: val_loss did not improve from 29.29586
196/196 - 36s - loss: 29.2922 - MinusLogProbMetric: 29.2922 - val_loss: 30.9701 - val_MinusLogProbMetric: 30.9701 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 180/1000
2023-10-27 04:04:05.222 
Epoch 180/1000 
	 loss: 29.2662, MinusLogProbMetric: 29.2662, val_loss: 29.0842, val_MinusLogProbMetric: 29.0842

Epoch 180: val_loss improved from 29.29586 to 29.08424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 37s - loss: 29.2662 - MinusLogProbMetric: 29.2662 - val_loss: 29.0842 - val_MinusLogProbMetric: 29.0842 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 181/1000
2023-10-27 04:04:40.668 
Epoch 181/1000 
	 loss: 29.1802, MinusLogProbMetric: 29.1802, val_loss: 29.5710, val_MinusLogProbMetric: 29.5710

Epoch 181: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.1802 - MinusLogProbMetric: 29.1802 - val_loss: 29.5710 - val_MinusLogProbMetric: 29.5710 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 182/1000
2023-10-27 04:05:16.112 
Epoch 182/1000 
	 loss: 29.3463, MinusLogProbMetric: 29.3463, val_loss: 30.1409, val_MinusLogProbMetric: 30.1409

Epoch 182: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.3463 - MinusLogProbMetric: 29.3463 - val_loss: 30.1409 - val_MinusLogProbMetric: 30.1409 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 183/1000
2023-10-27 04:05:51.590 
Epoch 183/1000 
	 loss: 29.2458, MinusLogProbMetric: 29.2458, val_loss: 29.6713, val_MinusLogProbMetric: 29.6713

Epoch 183: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.2458 - MinusLogProbMetric: 29.2458 - val_loss: 29.6713 - val_MinusLogProbMetric: 29.6713 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 184/1000
2023-10-27 04:06:28.428 
Epoch 184/1000 
	 loss: 29.2587, MinusLogProbMetric: 29.2587, val_loss: 30.3151, val_MinusLogProbMetric: 30.3151

Epoch 184: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.2587 - MinusLogProbMetric: 29.2587 - val_loss: 30.3151 - val_MinusLogProbMetric: 30.3151 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 185/1000
2023-10-27 04:07:04.303 
Epoch 185/1000 
	 loss: 29.2130, MinusLogProbMetric: 29.2130, val_loss: 29.7996, val_MinusLogProbMetric: 29.7996

Epoch 185: val_loss did not improve from 29.08424
196/196 - 36s - loss: 29.2130 - MinusLogProbMetric: 29.2130 - val_loss: 29.7996 - val_MinusLogProbMetric: 29.7996 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 186/1000
2023-10-27 04:07:39.002 
Epoch 186/1000 
	 loss: 29.2527, MinusLogProbMetric: 29.2527, val_loss: 29.3331, val_MinusLogProbMetric: 29.3331

Epoch 186: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.2527 - MinusLogProbMetric: 29.2527 - val_loss: 29.3331 - val_MinusLogProbMetric: 29.3331 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 187/1000
2023-10-27 04:08:14.391 
Epoch 187/1000 
	 loss: 29.2036, MinusLogProbMetric: 29.2036, val_loss: 30.2598, val_MinusLogProbMetric: 30.2598

Epoch 187: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.2036 - MinusLogProbMetric: 29.2036 - val_loss: 30.2598 - val_MinusLogProbMetric: 30.2598 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 188/1000
2023-10-27 04:08:49.471 
Epoch 188/1000 
	 loss: 29.2934, MinusLogProbMetric: 29.2934, val_loss: 30.1794, val_MinusLogProbMetric: 30.1794

Epoch 188: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.2934 - MinusLogProbMetric: 29.2934 - val_loss: 30.1794 - val_MinusLogProbMetric: 30.1794 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 189/1000
2023-10-27 04:09:26.466 
Epoch 189/1000 
	 loss: 29.1458, MinusLogProbMetric: 29.1458, val_loss: 29.4119, val_MinusLogProbMetric: 29.4119

Epoch 189: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.1458 - MinusLogProbMetric: 29.1458 - val_loss: 29.4119 - val_MinusLogProbMetric: 29.4119 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 190/1000
2023-10-27 04:10:05.909 
Epoch 190/1000 
	 loss: 29.2644, MinusLogProbMetric: 29.2644, val_loss: 29.8881, val_MinusLogProbMetric: 29.8881

Epoch 190: val_loss did not improve from 29.08424
196/196 - 39s - loss: 29.2644 - MinusLogProbMetric: 29.2644 - val_loss: 29.8881 - val_MinusLogProbMetric: 29.8881 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 191/1000
2023-10-27 04:10:41.287 
Epoch 191/1000 
	 loss: 29.2299, MinusLogProbMetric: 29.2299, val_loss: 29.4848, val_MinusLogProbMetric: 29.4848

Epoch 191: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.2299 - MinusLogProbMetric: 29.2299 - val_loss: 29.4848 - val_MinusLogProbMetric: 29.4848 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 192/1000
2023-10-27 04:11:16.695 
Epoch 192/1000 
	 loss: 29.2143, MinusLogProbMetric: 29.2143, val_loss: 29.1719, val_MinusLogProbMetric: 29.1719

Epoch 192: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.2143 - MinusLogProbMetric: 29.2143 - val_loss: 29.1719 - val_MinusLogProbMetric: 29.1719 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 193/1000
2023-10-27 04:11:53.917 
Epoch 193/1000 
	 loss: 29.2500, MinusLogProbMetric: 29.2500, val_loss: 30.2444, val_MinusLogProbMetric: 30.2444

Epoch 193: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.2500 - MinusLogProbMetric: 29.2500 - val_loss: 30.2444 - val_MinusLogProbMetric: 30.2444 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 194/1000
2023-10-27 04:12:34.444 
Epoch 194/1000 
	 loss: 29.2421, MinusLogProbMetric: 29.2421, val_loss: 30.2461, val_MinusLogProbMetric: 30.2461

Epoch 194: val_loss did not improve from 29.08424
196/196 - 41s - loss: 29.2421 - MinusLogProbMetric: 29.2421 - val_loss: 30.2461 - val_MinusLogProbMetric: 30.2461 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 195/1000
2023-10-27 04:13:11.599 
Epoch 195/1000 
	 loss: 29.2006, MinusLogProbMetric: 29.2006, val_loss: 29.7973, val_MinusLogProbMetric: 29.7973

Epoch 195: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.2006 - MinusLogProbMetric: 29.2006 - val_loss: 29.7973 - val_MinusLogProbMetric: 29.7973 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 196/1000
2023-10-27 04:13:46.616 
Epoch 196/1000 
	 loss: 29.1000, MinusLogProbMetric: 29.1000, val_loss: 29.7185, val_MinusLogProbMetric: 29.7185

Epoch 196: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.1000 - MinusLogProbMetric: 29.1000 - val_loss: 29.7185 - val_MinusLogProbMetric: 29.7185 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 197/1000
2023-10-27 04:14:21.374 
Epoch 197/1000 
	 loss: 29.2027, MinusLogProbMetric: 29.2027, val_loss: 29.3649, val_MinusLogProbMetric: 29.3649

Epoch 197: val_loss did not improve from 29.08424
196/196 - 35s - loss: 29.2027 - MinusLogProbMetric: 29.2027 - val_loss: 29.3649 - val_MinusLogProbMetric: 29.3649 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 198/1000
2023-10-27 04:14:58.712 
Epoch 198/1000 
	 loss: 29.2002, MinusLogProbMetric: 29.2002, val_loss: 30.0154, val_MinusLogProbMetric: 30.0154

Epoch 198: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.2002 - MinusLogProbMetric: 29.2002 - val_loss: 30.0154 - val_MinusLogProbMetric: 30.0154 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 199/1000
2023-10-27 04:15:39.533 
Epoch 199/1000 
	 loss: 29.1369, MinusLogProbMetric: 29.1369, val_loss: 29.8021, val_MinusLogProbMetric: 29.8021

Epoch 199: val_loss did not improve from 29.08424
196/196 - 41s - loss: 29.1369 - MinusLogProbMetric: 29.1369 - val_loss: 29.8021 - val_MinusLogProbMetric: 29.8021 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 200/1000
2023-10-27 04:16:16.064 
Epoch 200/1000 
	 loss: 29.1982, MinusLogProbMetric: 29.1982, val_loss: 30.3006, val_MinusLogProbMetric: 30.3006

Epoch 200: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.1982 - MinusLogProbMetric: 29.1982 - val_loss: 30.3006 - val_MinusLogProbMetric: 30.3006 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 201/1000
2023-10-27 04:16:52.699 
Epoch 201/1000 
	 loss: 29.2199, MinusLogProbMetric: 29.2199, val_loss: 29.2289, val_MinusLogProbMetric: 29.2289

Epoch 201: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.2199 - MinusLogProbMetric: 29.2199 - val_loss: 29.2289 - val_MinusLogProbMetric: 29.2289 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 202/1000
2023-10-27 04:17:28.452 
Epoch 202/1000 
	 loss: 29.1058, MinusLogProbMetric: 29.1058, val_loss: 29.1450, val_MinusLogProbMetric: 29.1450

Epoch 202: val_loss did not improve from 29.08424
196/196 - 36s - loss: 29.1058 - MinusLogProbMetric: 29.1058 - val_loss: 29.1450 - val_MinusLogProbMetric: 29.1450 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 203/1000
2023-10-27 04:18:07.481 
Epoch 203/1000 
	 loss: 29.1276, MinusLogProbMetric: 29.1276, val_loss: 29.6491, val_MinusLogProbMetric: 29.6491

Epoch 203: val_loss did not improve from 29.08424
196/196 - 39s - loss: 29.1276 - MinusLogProbMetric: 29.1276 - val_loss: 29.6491 - val_MinusLogProbMetric: 29.6491 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 204/1000
2023-10-27 04:18:48.261 
Epoch 204/1000 
	 loss: 29.1095, MinusLogProbMetric: 29.1095, val_loss: 29.9185, val_MinusLogProbMetric: 29.9185

Epoch 204: val_loss did not improve from 29.08424
196/196 - 41s - loss: 29.1095 - MinusLogProbMetric: 29.1095 - val_loss: 29.9185 - val_MinusLogProbMetric: 29.9185 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 205/1000
2023-10-27 04:19:24.862 
Epoch 205/1000 
	 loss: 29.1235, MinusLogProbMetric: 29.1235, val_loss: 30.0564, val_MinusLogProbMetric: 30.0564

Epoch 205: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.1235 - MinusLogProbMetric: 29.1235 - val_loss: 30.0564 - val_MinusLogProbMetric: 30.0564 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 206/1000
2023-10-27 04:20:01.625 
Epoch 206/1000 
	 loss: 29.0750, MinusLogProbMetric: 29.0750, val_loss: 29.5576, val_MinusLogProbMetric: 29.5576

Epoch 206: val_loss did not improve from 29.08424
196/196 - 37s - loss: 29.0750 - MinusLogProbMetric: 29.0750 - val_loss: 29.5576 - val_MinusLogProbMetric: 29.5576 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 207/1000
2023-10-27 04:20:39.240 
Epoch 207/1000 
	 loss: 29.1629, MinusLogProbMetric: 29.1629, val_loss: 29.4276, val_MinusLogProbMetric: 29.4276

Epoch 207: val_loss did not improve from 29.08424
196/196 - 38s - loss: 29.1629 - MinusLogProbMetric: 29.1629 - val_loss: 29.4276 - val_MinusLogProbMetric: 29.4276 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 208/1000
2023-10-27 04:21:14.926 
Epoch 208/1000 
	 loss: 29.0909, MinusLogProbMetric: 29.0909, val_loss: 29.4406, val_MinusLogProbMetric: 29.4406

Epoch 208: val_loss did not improve from 29.08424
196/196 - 36s - loss: 29.0909 - MinusLogProbMetric: 29.0909 - val_loss: 29.4406 - val_MinusLogProbMetric: 29.4406 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 209/1000
2023-10-27 04:21:56.100 
Epoch 209/1000 
	 loss: 29.2129, MinusLogProbMetric: 29.2129, val_loss: 29.3315, val_MinusLogProbMetric: 29.3315

Epoch 209: val_loss did not improve from 29.08424
196/196 - 41s - loss: 29.2129 - MinusLogProbMetric: 29.2129 - val_loss: 29.3315 - val_MinusLogProbMetric: 29.3315 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 210/1000
2023-10-27 04:22:32.815 
Epoch 210/1000 
	 loss: 29.0220, MinusLogProbMetric: 29.0220, val_loss: 28.9735, val_MinusLogProbMetric: 28.9735

Epoch 210: val_loss improved from 29.08424 to 28.97347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 37s - loss: 29.0220 - MinusLogProbMetric: 29.0220 - val_loss: 28.9735 - val_MinusLogProbMetric: 28.9735 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 211/1000
2023-10-27 04:23:12.202 
Epoch 211/1000 
	 loss: 29.0923, MinusLogProbMetric: 29.0923, val_loss: 30.2337, val_MinusLogProbMetric: 30.2337

Epoch 211: val_loss did not improve from 28.97347
196/196 - 39s - loss: 29.0923 - MinusLogProbMetric: 29.0923 - val_loss: 30.2337 - val_MinusLogProbMetric: 30.2337 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 212/1000
2023-10-27 04:23:45.362 
Epoch 212/1000 
	 loss: 29.1104, MinusLogProbMetric: 29.1104, val_loss: 29.6637, val_MinusLogProbMetric: 29.6637

Epoch 212: val_loss did not improve from 28.97347
196/196 - 33s - loss: 29.1104 - MinusLogProbMetric: 29.1104 - val_loss: 29.6637 - val_MinusLogProbMetric: 29.6637 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 213/1000
2023-10-27 04:24:19.161 
Epoch 213/1000 
	 loss: 29.0960, MinusLogProbMetric: 29.0960, val_loss: 30.4209, val_MinusLogProbMetric: 30.4209

Epoch 213: val_loss did not improve from 28.97347
196/196 - 34s - loss: 29.0960 - MinusLogProbMetric: 29.0960 - val_loss: 30.4209 - val_MinusLogProbMetric: 30.4209 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 214/1000
2023-10-27 04:24:52.374 
Epoch 214/1000 
	 loss: 29.1763, MinusLogProbMetric: 29.1763, val_loss: 29.9280, val_MinusLogProbMetric: 29.9280

Epoch 214: val_loss did not improve from 28.97347
196/196 - 33s - loss: 29.1763 - MinusLogProbMetric: 29.1763 - val_loss: 29.9280 - val_MinusLogProbMetric: 29.9280 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 215/1000
2023-10-27 04:25:25.665 
Epoch 215/1000 
	 loss: 29.0084, MinusLogProbMetric: 29.0084, val_loss: 29.6287, val_MinusLogProbMetric: 29.6287

Epoch 215: val_loss did not improve from 28.97347
196/196 - 33s - loss: 29.0084 - MinusLogProbMetric: 29.0084 - val_loss: 29.6287 - val_MinusLogProbMetric: 29.6287 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 216/1000
2023-10-27 04:26:03.886 
Epoch 216/1000 
	 loss: 29.0687, MinusLogProbMetric: 29.0687, val_loss: 29.3060, val_MinusLogProbMetric: 29.3060

Epoch 216: val_loss did not improve from 28.97347
196/196 - 38s - loss: 29.0687 - MinusLogProbMetric: 29.0687 - val_loss: 29.3060 - val_MinusLogProbMetric: 29.3060 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 217/1000
2023-10-27 04:26:43.094 
Epoch 217/1000 
	 loss: 29.0790, MinusLogProbMetric: 29.0790, val_loss: 29.2912, val_MinusLogProbMetric: 29.2912

Epoch 217: val_loss did not improve from 28.97347
196/196 - 39s - loss: 29.0790 - MinusLogProbMetric: 29.0790 - val_loss: 29.2912 - val_MinusLogProbMetric: 29.2912 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 218/1000
2023-10-27 04:27:21.451 
Epoch 218/1000 
	 loss: 29.0618, MinusLogProbMetric: 29.0618, val_loss: 29.8633, val_MinusLogProbMetric: 29.8633

Epoch 218: val_loss did not improve from 28.97347
196/196 - 38s - loss: 29.0618 - MinusLogProbMetric: 29.0618 - val_loss: 29.8633 - val_MinusLogProbMetric: 29.8633 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 219/1000
2023-10-27 04:28:01.964 
Epoch 219/1000 
	 loss: 29.1024, MinusLogProbMetric: 29.1024, val_loss: 29.1344, val_MinusLogProbMetric: 29.1344

Epoch 219: val_loss did not improve from 28.97347
196/196 - 41s - loss: 29.1024 - MinusLogProbMetric: 29.1024 - val_loss: 29.1344 - val_MinusLogProbMetric: 29.1344 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 220/1000
2023-10-27 04:28:38.078 
Epoch 220/1000 
	 loss: 29.0710, MinusLogProbMetric: 29.0710, val_loss: 30.4108, val_MinusLogProbMetric: 30.4108

Epoch 220: val_loss did not improve from 28.97347
196/196 - 36s - loss: 29.0710 - MinusLogProbMetric: 29.0710 - val_loss: 30.4108 - val_MinusLogProbMetric: 30.4108 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 221/1000
2023-10-27 04:29:18.946 
Epoch 221/1000 
	 loss: 29.0207, MinusLogProbMetric: 29.0207, val_loss: 29.5377, val_MinusLogProbMetric: 29.5377

Epoch 221: val_loss did not improve from 28.97347
196/196 - 41s - loss: 29.0207 - MinusLogProbMetric: 29.0207 - val_loss: 29.5377 - val_MinusLogProbMetric: 29.5377 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 222/1000
2023-10-27 04:29:56.300 
Epoch 222/1000 
	 loss: 28.9875, MinusLogProbMetric: 28.9875, val_loss: 29.3671, val_MinusLogProbMetric: 29.3671

Epoch 222: val_loss did not improve from 28.97347
196/196 - 37s - loss: 28.9875 - MinusLogProbMetric: 28.9875 - val_loss: 29.3671 - val_MinusLogProbMetric: 29.3671 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 223/1000
2023-10-27 04:30:33.967 
Epoch 223/1000 
	 loss: 29.0207, MinusLogProbMetric: 29.0207, val_loss: 29.6150, val_MinusLogProbMetric: 29.6150

Epoch 223: val_loss did not improve from 28.97347
196/196 - 38s - loss: 29.0207 - MinusLogProbMetric: 29.0207 - val_loss: 29.6150 - val_MinusLogProbMetric: 29.6150 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 224/1000
2023-10-27 04:31:12.218 
Epoch 224/1000 
	 loss: 28.9719, MinusLogProbMetric: 28.9719, val_loss: 28.9001, val_MinusLogProbMetric: 28.9001

Epoch 224: val_loss improved from 28.97347 to 28.90006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 39s - loss: 28.9719 - MinusLogProbMetric: 28.9719 - val_loss: 28.9001 - val_MinusLogProbMetric: 28.9001 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 225/1000
2023-10-27 04:31:51.252 
Epoch 225/1000 
	 loss: 28.9893, MinusLogProbMetric: 28.9893, val_loss: 29.5778, val_MinusLogProbMetric: 29.5778

Epoch 225: val_loss did not improve from 28.90006
196/196 - 38s - loss: 28.9893 - MinusLogProbMetric: 28.9893 - val_loss: 29.5778 - val_MinusLogProbMetric: 29.5778 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 226/1000
2023-10-27 04:32:32.007 
Epoch 226/1000 
	 loss: 29.0019, MinusLogProbMetric: 29.0019, val_loss: 29.7475, val_MinusLogProbMetric: 29.7475

Epoch 226: val_loss did not improve from 28.90006
196/196 - 41s - loss: 29.0019 - MinusLogProbMetric: 29.0019 - val_loss: 29.7475 - val_MinusLogProbMetric: 29.7475 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 227/1000
2023-10-27 04:33:08.854 
Epoch 227/1000 
	 loss: 28.9651, MinusLogProbMetric: 28.9651, val_loss: 29.3454, val_MinusLogProbMetric: 29.3454

Epoch 227: val_loss did not improve from 28.90006
196/196 - 37s - loss: 28.9651 - MinusLogProbMetric: 28.9651 - val_loss: 29.3454 - val_MinusLogProbMetric: 29.3454 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 228/1000
2023-10-27 04:33:46.490 
Epoch 228/1000 
	 loss: 28.9824, MinusLogProbMetric: 28.9824, val_loss: 30.0577, val_MinusLogProbMetric: 30.0577

Epoch 228: val_loss did not improve from 28.90006
196/196 - 38s - loss: 28.9824 - MinusLogProbMetric: 28.9824 - val_loss: 30.0577 - val_MinusLogProbMetric: 30.0577 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 229/1000
2023-10-27 04:34:22.497 
Epoch 229/1000 
	 loss: 28.9656, MinusLogProbMetric: 28.9656, val_loss: 29.0970, val_MinusLogProbMetric: 29.0970

Epoch 229: val_loss did not improve from 28.90006
196/196 - 36s - loss: 28.9656 - MinusLogProbMetric: 28.9656 - val_loss: 29.0970 - val_MinusLogProbMetric: 29.0970 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 230/1000
2023-10-27 04:34:59.304 
Epoch 230/1000 
	 loss: 28.8853, MinusLogProbMetric: 28.8853, val_loss: 29.0196, val_MinusLogProbMetric: 29.0196

Epoch 230: val_loss did not improve from 28.90006
196/196 - 37s - loss: 28.8853 - MinusLogProbMetric: 28.8853 - val_loss: 29.0196 - val_MinusLogProbMetric: 29.0196 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 231/1000
2023-10-27 04:35:38.537 
Epoch 231/1000 
	 loss: 28.9604, MinusLogProbMetric: 28.9604, val_loss: 29.1439, val_MinusLogProbMetric: 29.1439

Epoch 231: val_loss did not improve from 28.90006
196/196 - 39s - loss: 28.9604 - MinusLogProbMetric: 28.9604 - val_loss: 29.1439 - val_MinusLogProbMetric: 29.1439 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 232/1000
2023-10-27 04:36:15.578 
Epoch 232/1000 
	 loss: 28.8289, MinusLogProbMetric: 28.8289, val_loss: 29.5615, val_MinusLogProbMetric: 29.5615

Epoch 232: val_loss did not improve from 28.90006
196/196 - 37s - loss: 28.8289 - MinusLogProbMetric: 28.8289 - val_loss: 29.5615 - val_MinusLogProbMetric: 29.5615 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 233/1000
2023-10-27 04:36:55.410 
Epoch 233/1000 
	 loss: 28.9080, MinusLogProbMetric: 28.9080, val_loss: 29.3066, val_MinusLogProbMetric: 29.3066

Epoch 233: val_loss did not improve from 28.90006
196/196 - 40s - loss: 28.9080 - MinusLogProbMetric: 28.9080 - val_loss: 29.3066 - val_MinusLogProbMetric: 29.3066 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 234/1000
2023-10-27 04:37:30.875 
Epoch 234/1000 
	 loss: 28.9458, MinusLogProbMetric: 28.9458, val_loss: 29.1957, val_MinusLogProbMetric: 29.1957

Epoch 234: val_loss did not improve from 28.90006
196/196 - 35s - loss: 28.9458 - MinusLogProbMetric: 28.9458 - val_loss: 29.1957 - val_MinusLogProbMetric: 29.1957 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 235/1000
2023-10-27 04:38:09.137 
Epoch 235/1000 
	 loss: 28.9686, MinusLogProbMetric: 28.9686, val_loss: 29.3593, val_MinusLogProbMetric: 29.3593

Epoch 235: val_loss did not improve from 28.90006
196/196 - 38s - loss: 28.9686 - MinusLogProbMetric: 28.9686 - val_loss: 29.3593 - val_MinusLogProbMetric: 29.3593 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 236/1000
2023-10-27 04:38:51.798 
Epoch 236/1000 
	 loss: 28.9158, MinusLogProbMetric: 28.9158, val_loss: 29.3038, val_MinusLogProbMetric: 29.3038

Epoch 236: val_loss did not improve from 28.90006
196/196 - 43s - loss: 28.9158 - MinusLogProbMetric: 28.9158 - val_loss: 29.3038 - val_MinusLogProbMetric: 29.3038 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 237/1000
2023-10-27 04:39:29.515 
Epoch 237/1000 
	 loss: 28.8820, MinusLogProbMetric: 28.8820, val_loss: 29.9211, val_MinusLogProbMetric: 29.9211

Epoch 237: val_loss did not improve from 28.90006
196/196 - 38s - loss: 28.8820 - MinusLogProbMetric: 28.8820 - val_loss: 29.9211 - val_MinusLogProbMetric: 29.9211 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 238/1000
2023-10-27 04:40:07.609 
Epoch 238/1000 
	 loss: 29.0570, MinusLogProbMetric: 29.0570, val_loss: 29.5974, val_MinusLogProbMetric: 29.5974

Epoch 238: val_loss did not improve from 28.90006
196/196 - 38s - loss: 29.0570 - MinusLogProbMetric: 29.0570 - val_loss: 29.5974 - val_MinusLogProbMetric: 29.5974 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 239/1000
2023-10-27 04:40:50.074 
Epoch 239/1000 
	 loss: 28.9195, MinusLogProbMetric: 28.9195, val_loss: 29.1373, val_MinusLogProbMetric: 29.1373

Epoch 239: val_loss did not improve from 28.90006
196/196 - 42s - loss: 28.9195 - MinusLogProbMetric: 28.9195 - val_loss: 29.1373 - val_MinusLogProbMetric: 29.1373 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 240/1000
2023-10-27 04:41:31.477 
Epoch 240/1000 
	 loss: 28.9232, MinusLogProbMetric: 28.9232, val_loss: 29.3664, val_MinusLogProbMetric: 29.3664

Epoch 240: val_loss did not improve from 28.90006
196/196 - 41s - loss: 28.9232 - MinusLogProbMetric: 28.9232 - val_loss: 29.3664 - val_MinusLogProbMetric: 29.3664 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 241/1000
2023-10-27 04:42:14.369 
Epoch 241/1000 
	 loss: 28.9400, MinusLogProbMetric: 28.9400, val_loss: 29.2295, val_MinusLogProbMetric: 29.2295

Epoch 241: val_loss did not improve from 28.90006
196/196 - 43s - loss: 28.9400 - MinusLogProbMetric: 28.9400 - val_loss: 29.2295 - val_MinusLogProbMetric: 29.2295 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 242/1000
2023-10-27 04:42:56.289 
Epoch 242/1000 
	 loss: 28.9017, MinusLogProbMetric: 28.9017, val_loss: 29.3337, val_MinusLogProbMetric: 29.3337

Epoch 242: val_loss did not improve from 28.90006
196/196 - 42s - loss: 28.9017 - MinusLogProbMetric: 28.9017 - val_loss: 29.3337 - val_MinusLogProbMetric: 29.3337 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 243/1000
2023-10-27 04:43:39.032 
Epoch 243/1000 
	 loss: 28.8720, MinusLogProbMetric: 28.8720, val_loss: 29.1902, val_MinusLogProbMetric: 29.1902

Epoch 243: val_loss did not improve from 28.90006
196/196 - 43s - loss: 28.8720 - MinusLogProbMetric: 28.8720 - val_loss: 29.1902 - val_MinusLogProbMetric: 29.1902 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 244/1000
2023-10-27 04:44:20.902 
Epoch 244/1000 
	 loss: 28.8409, MinusLogProbMetric: 28.8409, val_loss: 29.0240, val_MinusLogProbMetric: 29.0240

Epoch 244: val_loss did not improve from 28.90006
196/196 - 42s - loss: 28.8409 - MinusLogProbMetric: 28.8409 - val_loss: 29.0240 - val_MinusLogProbMetric: 29.0240 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 245/1000
2023-10-27 04:45:03.449 
Epoch 245/1000 
	 loss: 28.8476, MinusLogProbMetric: 28.8476, val_loss: 29.1299, val_MinusLogProbMetric: 29.1299

Epoch 245: val_loss did not improve from 28.90006
196/196 - 43s - loss: 28.8476 - MinusLogProbMetric: 28.8476 - val_loss: 29.1299 - val_MinusLogProbMetric: 29.1299 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 246/1000
2023-10-27 04:45:45.291 
Epoch 246/1000 
	 loss: 28.9248, MinusLogProbMetric: 28.9248, val_loss: 29.3824, val_MinusLogProbMetric: 29.3824

Epoch 246: val_loss did not improve from 28.90006
196/196 - 42s - loss: 28.9248 - MinusLogProbMetric: 28.9248 - val_loss: 29.3824 - val_MinusLogProbMetric: 29.3824 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 247/1000
2023-10-27 04:46:27.278 
Epoch 247/1000 
	 loss: 28.9027, MinusLogProbMetric: 28.9027, val_loss: 29.0339, val_MinusLogProbMetric: 29.0339

Epoch 247: val_loss did not improve from 28.90006
196/196 - 42s - loss: 28.9027 - MinusLogProbMetric: 28.9027 - val_loss: 29.0339 - val_MinusLogProbMetric: 29.0339 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 248/1000
2023-10-27 04:47:08.729 
Epoch 248/1000 
	 loss: 28.8260, MinusLogProbMetric: 28.8260, val_loss: 29.2086, val_MinusLogProbMetric: 29.2086

Epoch 248: val_loss did not improve from 28.90006
196/196 - 41s - loss: 28.8260 - MinusLogProbMetric: 28.8260 - val_loss: 29.2086 - val_MinusLogProbMetric: 29.2086 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 249/1000
2023-10-27 04:47:51.077 
Epoch 249/1000 
	 loss: 28.8701, MinusLogProbMetric: 28.8701, val_loss: 28.9646, val_MinusLogProbMetric: 28.9646

Epoch 249: val_loss did not improve from 28.90006
196/196 - 42s - loss: 28.8701 - MinusLogProbMetric: 28.8701 - val_loss: 28.9646 - val_MinusLogProbMetric: 28.9646 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 250/1000
2023-10-27 04:48:32.075 
Epoch 250/1000 
	 loss: 28.8170, MinusLogProbMetric: 28.8170, val_loss: 29.2245, val_MinusLogProbMetric: 29.2245

Epoch 250: val_loss did not improve from 28.90006
196/196 - 41s - loss: 28.8170 - MinusLogProbMetric: 28.8170 - val_loss: 29.2245 - val_MinusLogProbMetric: 29.2245 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 251/1000
2023-10-27 04:49:13.539 
Epoch 251/1000 
	 loss: 28.8588, MinusLogProbMetric: 28.8588, val_loss: 29.8167, val_MinusLogProbMetric: 29.8167

Epoch 251: val_loss did not improve from 28.90006
196/196 - 41s - loss: 28.8588 - MinusLogProbMetric: 28.8588 - val_loss: 29.8167 - val_MinusLogProbMetric: 29.8167 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 252/1000
2023-10-27 04:49:55.019 
Epoch 252/1000 
	 loss: 28.9260, MinusLogProbMetric: 28.9260, val_loss: 29.3910, val_MinusLogProbMetric: 29.3910

Epoch 252: val_loss did not improve from 28.90006
196/196 - 41s - loss: 28.9260 - MinusLogProbMetric: 28.9260 - val_loss: 29.3910 - val_MinusLogProbMetric: 29.3910 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 253/1000
2023-10-27 04:50:37.055 
Epoch 253/1000 
	 loss: 28.7700, MinusLogProbMetric: 28.7700, val_loss: 28.8833, val_MinusLogProbMetric: 28.8833

Epoch 253: val_loss improved from 28.90006 to 28.88327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 28.7700 - MinusLogProbMetric: 28.7700 - val_loss: 28.8833 - val_MinusLogProbMetric: 28.8833 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 254/1000
2023-10-27 04:51:19.807 
Epoch 254/1000 
	 loss: 28.8603, MinusLogProbMetric: 28.8603, val_loss: 29.3037, val_MinusLogProbMetric: 29.3037

Epoch 254: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.8603 - MinusLogProbMetric: 28.8603 - val_loss: 29.3037 - val_MinusLogProbMetric: 29.3037 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 255/1000
2023-10-27 04:52:00.370 
Epoch 255/1000 
	 loss: 28.8290, MinusLogProbMetric: 28.8290, val_loss: 29.0907, val_MinusLogProbMetric: 29.0907

Epoch 255: val_loss did not improve from 28.88327
196/196 - 41s - loss: 28.8290 - MinusLogProbMetric: 28.8290 - val_loss: 29.0907 - val_MinusLogProbMetric: 29.0907 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 256/1000
2023-10-27 04:52:42.496 
Epoch 256/1000 
	 loss: 28.7560, MinusLogProbMetric: 28.7560, val_loss: 29.3991, val_MinusLogProbMetric: 29.3991

Epoch 256: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7560 - MinusLogProbMetric: 28.7560 - val_loss: 29.3991 - val_MinusLogProbMetric: 29.3991 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 257/1000
2023-10-27 04:53:24.953 
Epoch 257/1000 
	 loss: 28.8630, MinusLogProbMetric: 28.8630, val_loss: 29.4817, val_MinusLogProbMetric: 29.4817

Epoch 257: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.8630 - MinusLogProbMetric: 28.8630 - val_loss: 29.4817 - val_MinusLogProbMetric: 29.4817 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 258/1000
2023-10-27 04:54:06.932 
Epoch 258/1000 
	 loss: 28.8375, MinusLogProbMetric: 28.8375, val_loss: 30.1232, val_MinusLogProbMetric: 30.1232

Epoch 258: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.8375 - MinusLogProbMetric: 28.8375 - val_loss: 30.1232 - val_MinusLogProbMetric: 30.1232 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 259/1000
2023-10-27 04:54:49.512 
Epoch 259/1000 
	 loss: 28.8270, MinusLogProbMetric: 28.8270, val_loss: 29.5415, val_MinusLogProbMetric: 29.5415

Epoch 259: val_loss did not improve from 28.88327
196/196 - 43s - loss: 28.8270 - MinusLogProbMetric: 28.8270 - val_loss: 29.5415 - val_MinusLogProbMetric: 29.5415 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 260/1000
2023-10-27 04:55:32.180 
Epoch 260/1000 
	 loss: 28.7310, MinusLogProbMetric: 28.7310, val_loss: 28.9484, val_MinusLogProbMetric: 28.9484

Epoch 260: val_loss did not improve from 28.88327
196/196 - 43s - loss: 28.7310 - MinusLogProbMetric: 28.7310 - val_loss: 28.9484 - val_MinusLogProbMetric: 28.9484 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 261/1000
2023-10-27 04:56:14.699 
Epoch 261/1000 
	 loss: 28.8923, MinusLogProbMetric: 28.8923, val_loss: 29.2975, val_MinusLogProbMetric: 29.2975

Epoch 261: val_loss did not improve from 28.88327
196/196 - 43s - loss: 28.8923 - MinusLogProbMetric: 28.8923 - val_loss: 29.2975 - val_MinusLogProbMetric: 29.2975 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 262/1000
2023-10-27 04:56:57.090 
Epoch 262/1000 
	 loss: 28.8022, MinusLogProbMetric: 28.8022, val_loss: 29.4736, val_MinusLogProbMetric: 29.4736

Epoch 262: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.8022 - MinusLogProbMetric: 28.8022 - val_loss: 29.4736 - val_MinusLogProbMetric: 29.4736 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 263/1000
2023-10-27 04:57:39.686 
Epoch 263/1000 
	 loss: 28.8321, MinusLogProbMetric: 28.8321, val_loss: 29.5041, val_MinusLogProbMetric: 29.5041

Epoch 263: val_loss did not improve from 28.88327
196/196 - 43s - loss: 28.8321 - MinusLogProbMetric: 28.8321 - val_loss: 29.5041 - val_MinusLogProbMetric: 29.5041 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 264/1000
2023-10-27 04:58:21.217 
Epoch 264/1000 
	 loss: 28.7906, MinusLogProbMetric: 28.7906, val_loss: 29.1803, val_MinusLogProbMetric: 29.1803

Epoch 264: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7906 - MinusLogProbMetric: 28.7906 - val_loss: 29.1803 - val_MinusLogProbMetric: 29.1803 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 265/1000
2023-10-27 04:59:01.851 
Epoch 265/1000 
	 loss: 28.7569, MinusLogProbMetric: 28.7569, val_loss: 29.8242, val_MinusLogProbMetric: 29.8242

Epoch 265: val_loss did not improve from 28.88327
196/196 - 41s - loss: 28.7569 - MinusLogProbMetric: 28.7569 - val_loss: 29.8242 - val_MinusLogProbMetric: 29.8242 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 266/1000
2023-10-27 04:59:44.128 
Epoch 266/1000 
	 loss: 28.7486, MinusLogProbMetric: 28.7486, val_loss: 30.8013, val_MinusLogProbMetric: 30.8013

Epoch 266: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7486 - MinusLogProbMetric: 28.7486 - val_loss: 30.8013 - val_MinusLogProbMetric: 30.8013 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 267/1000
2023-10-27 05:00:25.881 
Epoch 267/1000 
	 loss: 28.7606, MinusLogProbMetric: 28.7606, val_loss: 28.9857, val_MinusLogProbMetric: 28.9857

Epoch 267: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7606 - MinusLogProbMetric: 28.7606 - val_loss: 28.9857 - val_MinusLogProbMetric: 28.9857 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 268/1000
2023-10-27 05:01:07.277 
Epoch 268/1000 
	 loss: 28.6963, MinusLogProbMetric: 28.6963, val_loss: 29.4787, val_MinusLogProbMetric: 29.4787

Epoch 268: val_loss did not improve from 28.88327
196/196 - 41s - loss: 28.6963 - MinusLogProbMetric: 28.6963 - val_loss: 29.4787 - val_MinusLogProbMetric: 29.4787 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 269/1000
2023-10-27 05:01:48.488 
Epoch 269/1000 
	 loss: 28.7863, MinusLogProbMetric: 28.7863, val_loss: 29.0340, val_MinusLogProbMetric: 29.0340

Epoch 269: val_loss did not improve from 28.88327
196/196 - 41s - loss: 28.7863 - MinusLogProbMetric: 28.7863 - val_loss: 29.0340 - val_MinusLogProbMetric: 29.0340 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 270/1000
2023-10-27 05:02:30.779 
Epoch 270/1000 
	 loss: 28.8283, MinusLogProbMetric: 28.8283, val_loss: 28.9776, val_MinusLogProbMetric: 28.9776

Epoch 270: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.8283 - MinusLogProbMetric: 28.8283 - val_loss: 28.9776 - val_MinusLogProbMetric: 28.9776 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 271/1000
2023-10-27 05:03:12.747 
Epoch 271/1000 
	 loss: 28.7921, MinusLogProbMetric: 28.7921, val_loss: 29.2132, val_MinusLogProbMetric: 29.2132

Epoch 271: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7921 - MinusLogProbMetric: 28.7921 - val_loss: 29.2132 - val_MinusLogProbMetric: 29.2132 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 272/1000
2023-10-27 05:03:54.928 
Epoch 272/1000 
	 loss: 28.7548, MinusLogProbMetric: 28.7548, val_loss: 29.5072, val_MinusLogProbMetric: 29.5072

Epoch 272: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7548 - MinusLogProbMetric: 28.7548 - val_loss: 29.5072 - val_MinusLogProbMetric: 29.5072 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 273/1000
2023-10-27 05:04:36.857 
Epoch 273/1000 
	 loss: 28.7189, MinusLogProbMetric: 28.7189, val_loss: 29.0254, val_MinusLogProbMetric: 29.0254

Epoch 273: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7189 - MinusLogProbMetric: 28.7189 - val_loss: 29.0254 - val_MinusLogProbMetric: 29.0254 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 274/1000
2023-10-27 05:05:18.738 
Epoch 274/1000 
	 loss: 28.6787, MinusLogProbMetric: 28.6787, val_loss: 29.4915, val_MinusLogProbMetric: 29.4915

Epoch 274: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.6787 - MinusLogProbMetric: 28.6787 - val_loss: 29.4915 - val_MinusLogProbMetric: 29.4915 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 275/1000
2023-10-27 05:06:00.601 
Epoch 275/1000 
	 loss: 28.7252, MinusLogProbMetric: 28.7252, val_loss: 29.3688, val_MinusLogProbMetric: 29.3688

Epoch 275: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7252 - MinusLogProbMetric: 28.7252 - val_loss: 29.3688 - val_MinusLogProbMetric: 29.3688 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 276/1000
2023-10-27 05:06:42.922 
Epoch 276/1000 
	 loss: 28.7798, MinusLogProbMetric: 28.7798, val_loss: 29.0538, val_MinusLogProbMetric: 29.0538

Epoch 276: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7798 - MinusLogProbMetric: 28.7798 - val_loss: 29.0538 - val_MinusLogProbMetric: 29.0538 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 277/1000
2023-10-27 05:07:25.326 
Epoch 277/1000 
	 loss: 28.6935, MinusLogProbMetric: 28.6935, val_loss: 29.0170, val_MinusLogProbMetric: 29.0170

Epoch 277: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.6935 - MinusLogProbMetric: 28.6935 - val_loss: 29.0170 - val_MinusLogProbMetric: 29.0170 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 278/1000
2023-10-27 05:08:07.463 
Epoch 278/1000 
	 loss: 28.6827, MinusLogProbMetric: 28.6827, val_loss: 29.7410, val_MinusLogProbMetric: 29.7410

Epoch 278: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.6827 - MinusLogProbMetric: 28.6827 - val_loss: 29.7410 - val_MinusLogProbMetric: 29.7410 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 279/1000
2023-10-27 05:08:49.255 
Epoch 279/1000 
	 loss: 28.7410, MinusLogProbMetric: 28.7410, val_loss: 28.8983, val_MinusLogProbMetric: 28.8983

Epoch 279: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.7410 - MinusLogProbMetric: 28.7410 - val_loss: 28.8983 - val_MinusLogProbMetric: 28.8983 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 280/1000
2023-10-27 05:09:31.467 
Epoch 280/1000 
	 loss: 28.6868, MinusLogProbMetric: 28.6868, val_loss: 29.0884, val_MinusLogProbMetric: 29.0884

Epoch 280: val_loss did not improve from 28.88327
196/196 - 42s - loss: 28.6868 - MinusLogProbMetric: 28.6868 - val_loss: 29.0884 - val_MinusLogProbMetric: 29.0884 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 281/1000
2023-10-27 05:10:14.121 
Epoch 281/1000 
	 loss: 28.7288, MinusLogProbMetric: 28.7288, val_loss: 28.8804, val_MinusLogProbMetric: 28.8804

Epoch 281: val_loss improved from 28.88327 to 28.88037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 28.7288 - MinusLogProbMetric: 28.7288 - val_loss: 28.8804 - val_MinusLogProbMetric: 28.8804 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 282/1000
2023-10-27 05:10:56.416 
Epoch 282/1000 
	 loss: 28.6880, MinusLogProbMetric: 28.6880, val_loss: 30.2971, val_MinusLogProbMetric: 30.2971

Epoch 282: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6880 - MinusLogProbMetric: 28.6880 - val_loss: 30.2971 - val_MinusLogProbMetric: 30.2971 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 283/1000
2023-10-27 05:11:38.654 
Epoch 283/1000 
	 loss: 28.7211, MinusLogProbMetric: 28.7211, val_loss: 28.9481, val_MinusLogProbMetric: 28.9481

Epoch 283: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.7211 - MinusLogProbMetric: 28.7211 - val_loss: 28.9481 - val_MinusLogProbMetric: 28.9481 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 284/1000
2023-10-27 05:12:20.863 
Epoch 284/1000 
	 loss: 28.6829, MinusLogProbMetric: 28.6829, val_loss: 29.0337, val_MinusLogProbMetric: 29.0337

Epoch 284: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6829 - MinusLogProbMetric: 28.6829 - val_loss: 29.0337 - val_MinusLogProbMetric: 29.0337 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 285/1000
2023-10-27 05:13:03.363 
Epoch 285/1000 
	 loss: 28.6704, MinusLogProbMetric: 28.6704, val_loss: 29.8743, val_MinusLogProbMetric: 29.8743

Epoch 285: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6704 - MinusLogProbMetric: 28.6704 - val_loss: 29.8743 - val_MinusLogProbMetric: 29.8743 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 286/1000
2023-10-27 05:13:45.662 
Epoch 286/1000 
	 loss: 28.7136, MinusLogProbMetric: 28.7136, val_loss: 29.7661, val_MinusLogProbMetric: 29.7661

Epoch 286: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.7136 - MinusLogProbMetric: 28.7136 - val_loss: 29.7661 - val_MinusLogProbMetric: 29.7661 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 287/1000
2023-10-27 05:14:28.058 
Epoch 287/1000 
	 loss: 28.7070, MinusLogProbMetric: 28.7070, val_loss: 29.3416, val_MinusLogProbMetric: 29.3416

Epoch 287: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.7070 - MinusLogProbMetric: 28.7070 - val_loss: 29.3416 - val_MinusLogProbMetric: 29.3416 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 288/1000
2023-10-27 05:15:10.451 
Epoch 288/1000 
	 loss: 28.6564, MinusLogProbMetric: 28.6564, val_loss: 29.5446, val_MinusLogProbMetric: 29.5446

Epoch 288: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6564 - MinusLogProbMetric: 28.6564 - val_loss: 29.5446 - val_MinusLogProbMetric: 29.5446 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 289/1000
2023-10-27 05:15:52.594 
Epoch 289/1000 
	 loss: 28.7437, MinusLogProbMetric: 28.7437, val_loss: 29.3420, val_MinusLogProbMetric: 29.3420

Epoch 289: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.7437 - MinusLogProbMetric: 28.7437 - val_loss: 29.3420 - val_MinusLogProbMetric: 29.3420 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 290/1000
2023-10-27 05:16:34.507 
Epoch 290/1000 
	 loss: 28.6781, MinusLogProbMetric: 28.6781, val_loss: 30.1687, val_MinusLogProbMetric: 30.1687

Epoch 290: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6781 - MinusLogProbMetric: 28.6781 - val_loss: 30.1687 - val_MinusLogProbMetric: 30.1687 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 291/1000
2023-10-27 05:17:16.477 
Epoch 291/1000 
	 loss: 28.6973, MinusLogProbMetric: 28.6973, val_loss: 29.2824, val_MinusLogProbMetric: 29.2824

Epoch 291: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6973 - MinusLogProbMetric: 28.6973 - val_loss: 29.2824 - val_MinusLogProbMetric: 29.2824 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 292/1000
2023-10-27 05:17:58.867 
Epoch 292/1000 
	 loss: 28.6618, MinusLogProbMetric: 28.6618, val_loss: 29.0163, val_MinusLogProbMetric: 29.0163

Epoch 292: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6618 - MinusLogProbMetric: 28.6618 - val_loss: 29.0163 - val_MinusLogProbMetric: 29.0163 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 293/1000
2023-10-27 05:18:41.136 
Epoch 293/1000 
	 loss: 28.6430, MinusLogProbMetric: 28.6430, val_loss: 29.1848, val_MinusLogProbMetric: 29.1848

Epoch 293: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6430 - MinusLogProbMetric: 28.6430 - val_loss: 29.1848 - val_MinusLogProbMetric: 29.1848 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 294/1000
2023-10-27 05:19:23.215 
Epoch 294/1000 
	 loss: 28.6315, MinusLogProbMetric: 28.6315, val_loss: 28.9588, val_MinusLogProbMetric: 28.9588

Epoch 294: val_loss did not improve from 28.88037
196/196 - 42s - loss: 28.6315 - MinusLogProbMetric: 28.6315 - val_loss: 28.9588 - val_MinusLogProbMetric: 28.9588 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 295/1000
2023-10-27 05:20:04.421 
Epoch 295/1000 
	 loss: 28.6626, MinusLogProbMetric: 28.6626, val_loss: 28.8473, val_MinusLogProbMetric: 28.8473

Epoch 295: val_loss improved from 28.88037 to 28.84733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 42s - loss: 28.6626 - MinusLogProbMetric: 28.6626 - val_loss: 28.8473 - val_MinusLogProbMetric: 28.8473 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 296/1000
2023-10-27 05:20:46.837 
Epoch 296/1000 
	 loss: 28.7500, MinusLogProbMetric: 28.7500, val_loss: 29.5732, val_MinusLogProbMetric: 29.5732

Epoch 296: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.7500 - MinusLogProbMetric: 28.7500 - val_loss: 29.5732 - val_MinusLogProbMetric: 29.5732 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 297/1000
2023-10-27 05:21:28.782 
Epoch 297/1000 
	 loss: 28.6108, MinusLogProbMetric: 28.6108, val_loss: 29.0049, val_MinusLogProbMetric: 29.0049

Epoch 297: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.6108 - MinusLogProbMetric: 28.6108 - val_loss: 29.0049 - val_MinusLogProbMetric: 29.0049 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 298/1000
2023-10-27 05:22:09.849 
Epoch 298/1000 
	 loss: 28.6066, MinusLogProbMetric: 28.6066, val_loss: 28.9449, val_MinusLogProbMetric: 28.9449

Epoch 298: val_loss did not improve from 28.84733
196/196 - 41s - loss: 28.6066 - MinusLogProbMetric: 28.6066 - val_loss: 28.9449 - val_MinusLogProbMetric: 28.9449 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 299/1000
2023-10-27 05:22:52.003 
Epoch 299/1000 
	 loss: 28.6373, MinusLogProbMetric: 28.6373, val_loss: 29.0458, val_MinusLogProbMetric: 29.0458

Epoch 299: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.6373 - MinusLogProbMetric: 28.6373 - val_loss: 29.0458 - val_MinusLogProbMetric: 29.0458 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 300/1000
2023-10-27 05:23:34.288 
Epoch 300/1000 
	 loss: 28.6295, MinusLogProbMetric: 28.6295, val_loss: 29.4094, val_MinusLogProbMetric: 29.4094

Epoch 300: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.6295 - MinusLogProbMetric: 28.6295 - val_loss: 29.4094 - val_MinusLogProbMetric: 29.4094 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 301/1000
2023-10-27 05:24:15.979 
Epoch 301/1000 
	 loss: 28.6397, MinusLogProbMetric: 28.6397, val_loss: 28.9839, val_MinusLogProbMetric: 28.9839

Epoch 301: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.6397 - MinusLogProbMetric: 28.6397 - val_loss: 28.9839 - val_MinusLogProbMetric: 28.9839 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 302/1000
2023-10-27 05:24:57.444 
Epoch 302/1000 
	 loss: 28.5834, MinusLogProbMetric: 28.5834, val_loss: 29.1747, val_MinusLogProbMetric: 29.1747

Epoch 302: val_loss did not improve from 28.84733
196/196 - 41s - loss: 28.5834 - MinusLogProbMetric: 28.5834 - val_loss: 29.1747 - val_MinusLogProbMetric: 29.1747 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 303/1000
2023-10-27 05:25:39.447 
Epoch 303/1000 
	 loss: 28.6383, MinusLogProbMetric: 28.6383, val_loss: 29.1463, val_MinusLogProbMetric: 29.1463

Epoch 303: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.6383 - MinusLogProbMetric: 28.6383 - val_loss: 29.1463 - val_MinusLogProbMetric: 29.1463 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 304/1000
2023-10-27 05:26:15.311 
Epoch 304/1000 
	 loss: 28.5390, MinusLogProbMetric: 28.5390, val_loss: 29.4214, val_MinusLogProbMetric: 29.4214

Epoch 304: val_loss did not improve from 28.84733
196/196 - 36s - loss: 28.5390 - MinusLogProbMetric: 28.5390 - val_loss: 29.4214 - val_MinusLogProbMetric: 29.4214 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 305/1000
2023-10-27 05:26:50.529 
Epoch 305/1000 
	 loss: 28.7246, MinusLogProbMetric: 28.7246, val_loss: 30.1086, val_MinusLogProbMetric: 30.1086

Epoch 305: val_loss did not improve from 28.84733
196/196 - 35s - loss: 28.7246 - MinusLogProbMetric: 28.7246 - val_loss: 30.1086 - val_MinusLogProbMetric: 30.1086 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 306/1000
2023-10-27 05:27:26.336 
Epoch 306/1000 
	 loss: 28.6560, MinusLogProbMetric: 28.6560, val_loss: 29.6040, val_MinusLogProbMetric: 29.6040

Epoch 306: val_loss did not improve from 28.84733
196/196 - 36s - loss: 28.6560 - MinusLogProbMetric: 28.6560 - val_loss: 29.6040 - val_MinusLogProbMetric: 29.6040 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 307/1000
2023-10-27 05:28:08.193 
Epoch 307/1000 
	 loss: 28.5928, MinusLogProbMetric: 28.5928, val_loss: 29.0258, val_MinusLogProbMetric: 29.0258

Epoch 307: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5928 - MinusLogProbMetric: 28.5928 - val_loss: 29.0258 - val_MinusLogProbMetric: 29.0258 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 308/1000
2023-10-27 05:28:50.407 
Epoch 308/1000 
	 loss: 28.5620, MinusLogProbMetric: 28.5620, val_loss: 28.9273, val_MinusLogProbMetric: 28.9273

Epoch 308: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5620 - MinusLogProbMetric: 28.5620 - val_loss: 28.9273 - val_MinusLogProbMetric: 28.9273 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 309/1000
2023-10-27 05:29:31.132 
Epoch 309/1000 
	 loss: 28.5865, MinusLogProbMetric: 28.5865, val_loss: 28.9458, val_MinusLogProbMetric: 28.9458

Epoch 309: val_loss did not improve from 28.84733
196/196 - 41s - loss: 28.5865 - MinusLogProbMetric: 28.5865 - val_loss: 28.9458 - val_MinusLogProbMetric: 28.9458 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 310/1000
2023-10-27 05:30:13.578 
Epoch 310/1000 
	 loss: 28.6456, MinusLogProbMetric: 28.6456, val_loss: 29.2541, val_MinusLogProbMetric: 29.2541

Epoch 310: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.6456 - MinusLogProbMetric: 28.6456 - val_loss: 29.2541 - val_MinusLogProbMetric: 29.2541 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 311/1000
2023-10-27 05:30:55.410 
Epoch 311/1000 
	 loss: 28.5803, MinusLogProbMetric: 28.5803, val_loss: 29.9260, val_MinusLogProbMetric: 29.9260

Epoch 311: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5803 - MinusLogProbMetric: 28.5803 - val_loss: 29.9260 - val_MinusLogProbMetric: 29.9260 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 312/1000
2023-10-27 05:31:37.717 
Epoch 312/1000 
	 loss: 28.6269, MinusLogProbMetric: 28.6269, val_loss: 29.1896, val_MinusLogProbMetric: 29.1896

Epoch 312: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.6269 - MinusLogProbMetric: 28.6269 - val_loss: 29.1896 - val_MinusLogProbMetric: 29.1896 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 313/1000
2023-10-27 05:32:20.177 
Epoch 313/1000 
	 loss: 28.5530, MinusLogProbMetric: 28.5530, val_loss: 29.5038, val_MinusLogProbMetric: 29.5038

Epoch 313: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5530 - MinusLogProbMetric: 28.5530 - val_loss: 29.5038 - val_MinusLogProbMetric: 29.5038 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 314/1000
2023-10-27 05:33:01.958 
Epoch 314/1000 
	 loss: 28.5453, MinusLogProbMetric: 28.5453, val_loss: 28.9080, val_MinusLogProbMetric: 28.9080

Epoch 314: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5453 - MinusLogProbMetric: 28.5453 - val_loss: 28.9080 - val_MinusLogProbMetric: 28.9080 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 315/1000
2023-10-27 05:33:43.315 
Epoch 315/1000 
	 loss: 28.5052, MinusLogProbMetric: 28.5052, val_loss: 28.9229, val_MinusLogProbMetric: 28.9229

Epoch 315: val_loss did not improve from 28.84733
196/196 - 41s - loss: 28.5052 - MinusLogProbMetric: 28.5052 - val_loss: 28.9229 - val_MinusLogProbMetric: 28.9229 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 316/1000
2023-10-27 05:34:25.135 
Epoch 316/1000 
	 loss: 28.5311, MinusLogProbMetric: 28.5311, val_loss: 28.9172, val_MinusLogProbMetric: 28.9172

Epoch 316: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5311 - MinusLogProbMetric: 28.5311 - val_loss: 28.9172 - val_MinusLogProbMetric: 28.9172 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 317/1000
2023-10-27 05:35:07.470 
Epoch 317/1000 
	 loss: 28.5406, MinusLogProbMetric: 28.5406, val_loss: 29.3710, val_MinusLogProbMetric: 29.3710

Epoch 317: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5406 - MinusLogProbMetric: 28.5406 - val_loss: 29.3710 - val_MinusLogProbMetric: 29.3710 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 318/1000
2023-10-27 05:35:49.771 
Epoch 318/1000 
	 loss: 28.5439, MinusLogProbMetric: 28.5439, val_loss: 29.4770, val_MinusLogProbMetric: 29.4770

Epoch 318: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5439 - MinusLogProbMetric: 28.5439 - val_loss: 29.4770 - val_MinusLogProbMetric: 29.4770 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 319/1000
2023-10-27 05:36:32.249 
Epoch 319/1000 
	 loss: 28.5179, MinusLogProbMetric: 28.5179, val_loss: 29.2811, val_MinusLogProbMetric: 29.2811

Epoch 319: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5179 - MinusLogProbMetric: 28.5179 - val_loss: 29.2811 - val_MinusLogProbMetric: 29.2811 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 320/1000
2023-10-27 05:37:14.494 
Epoch 320/1000 
	 loss: 28.5020, MinusLogProbMetric: 28.5020, val_loss: 28.8738, val_MinusLogProbMetric: 28.8738

Epoch 320: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5020 - MinusLogProbMetric: 28.5020 - val_loss: 28.8738 - val_MinusLogProbMetric: 28.8738 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 321/1000
2023-10-27 05:37:56.623 
Epoch 321/1000 
	 loss: 28.5770, MinusLogProbMetric: 28.5770, val_loss: 29.5044, val_MinusLogProbMetric: 29.5044

Epoch 321: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5770 - MinusLogProbMetric: 28.5770 - val_loss: 29.5044 - val_MinusLogProbMetric: 29.5044 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 322/1000
2023-10-27 05:38:38.843 
Epoch 322/1000 
	 loss: 28.5065, MinusLogProbMetric: 28.5065, val_loss: 28.9301, val_MinusLogProbMetric: 28.9301

Epoch 322: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5065 - MinusLogProbMetric: 28.5065 - val_loss: 28.9301 - val_MinusLogProbMetric: 28.9301 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 323/1000
2023-10-27 05:39:21.142 
Epoch 323/1000 
	 loss: 28.5346, MinusLogProbMetric: 28.5346, val_loss: 29.3732, val_MinusLogProbMetric: 29.3732

Epoch 323: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5346 - MinusLogProbMetric: 28.5346 - val_loss: 29.3732 - val_MinusLogProbMetric: 29.3732 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 324/1000
2023-10-27 05:40:03.081 
Epoch 324/1000 
	 loss: 28.5552, MinusLogProbMetric: 28.5552, val_loss: 29.0077, val_MinusLogProbMetric: 29.0077

Epoch 324: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5552 - MinusLogProbMetric: 28.5552 - val_loss: 29.0077 - val_MinusLogProbMetric: 29.0077 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 325/1000
2023-10-27 05:40:45.052 
Epoch 325/1000 
	 loss: 28.5025, MinusLogProbMetric: 28.5025, val_loss: 29.0245, val_MinusLogProbMetric: 29.0245

Epoch 325: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.5025 - MinusLogProbMetric: 28.5025 - val_loss: 29.0245 - val_MinusLogProbMetric: 29.0245 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 326/1000
2023-10-27 05:41:27.260 
Epoch 326/1000 
	 loss: 28.4889, MinusLogProbMetric: 28.4889, val_loss: 29.1023, val_MinusLogProbMetric: 29.1023

Epoch 326: val_loss did not improve from 28.84733
196/196 - 42s - loss: 28.4889 - MinusLogProbMetric: 28.4889 - val_loss: 29.1023 - val_MinusLogProbMetric: 29.1023 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 327/1000
2023-10-27 05:42:08.420 
Epoch 327/1000 
	 loss: 28.4758, MinusLogProbMetric: 28.4758, val_loss: 28.6877, val_MinusLogProbMetric: 28.6877

Epoch 327: val_loss improved from 28.84733 to 28.68771, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 42s - loss: 28.4758 - MinusLogProbMetric: 28.4758 - val_loss: 28.6877 - val_MinusLogProbMetric: 28.6877 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 328/1000
2023-10-27 05:42:50.687 
Epoch 328/1000 
	 loss: 28.5268, MinusLogProbMetric: 28.5268, val_loss: 29.2120, val_MinusLogProbMetric: 29.2120

Epoch 328: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5268 - MinusLogProbMetric: 28.5268 - val_loss: 29.2120 - val_MinusLogProbMetric: 29.2120 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 329/1000
2023-10-27 05:43:32.457 
Epoch 329/1000 
	 loss: 28.5092, MinusLogProbMetric: 28.5092, val_loss: 29.8745, val_MinusLogProbMetric: 29.8745

Epoch 329: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5092 - MinusLogProbMetric: 28.5092 - val_loss: 29.8745 - val_MinusLogProbMetric: 29.8745 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 330/1000
2023-10-27 05:44:14.434 
Epoch 330/1000 
	 loss: 28.5225, MinusLogProbMetric: 28.5225, val_loss: 29.5695, val_MinusLogProbMetric: 29.5695

Epoch 330: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5225 - MinusLogProbMetric: 28.5225 - val_loss: 29.5695 - val_MinusLogProbMetric: 29.5695 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 331/1000
2023-10-27 05:44:55.593 
Epoch 331/1000 
	 loss: 28.5076, MinusLogProbMetric: 28.5076, val_loss: 28.9205, val_MinusLogProbMetric: 28.9205

Epoch 331: val_loss did not improve from 28.68771
196/196 - 41s - loss: 28.5076 - MinusLogProbMetric: 28.5076 - val_loss: 28.9205 - val_MinusLogProbMetric: 28.9205 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 332/1000
2023-10-27 05:45:37.586 
Epoch 332/1000 
	 loss: 28.4722, MinusLogProbMetric: 28.4722, val_loss: 28.8752, val_MinusLogProbMetric: 28.8752

Epoch 332: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4722 - MinusLogProbMetric: 28.4722 - val_loss: 28.8752 - val_MinusLogProbMetric: 28.8752 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 333/1000
2023-10-27 05:46:19.709 
Epoch 333/1000 
	 loss: 28.5569, MinusLogProbMetric: 28.5569, val_loss: 29.0127, val_MinusLogProbMetric: 29.0127

Epoch 333: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5569 - MinusLogProbMetric: 28.5569 - val_loss: 29.0127 - val_MinusLogProbMetric: 29.0127 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 334/1000
2023-10-27 05:47:02.313 
Epoch 334/1000 
	 loss: 28.4891, MinusLogProbMetric: 28.4891, val_loss: 29.1656, val_MinusLogProbMetric: 29.1656

Epoch 334: val_loss did not improve from 28.68771
196/196 - 43s - loss: 28.4891 - MinusLogProbMetric: 28.4891 - val_loss: 29.1656 - val_MinusLogProbMetric: 29.1656 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 335/1000
2023-10-27 05:47:45.142 
Epoch 335/1000 
	 loss: 28.4455, MinusLogProbMetric: 28.4455, val_loss: 29.5243, val_MinusLogProbMetric: 29.5243

Epoch 335: val_loss did not improve from 28.68771
196/196 - 43s - loss: 28.4455 - MinusLogProbMetric: 28.4455 - val_loss: 29.5243 - val_MinusLogProbMetric: 29.5243 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 336/1000
2023-10-27 05:48:26.620 
Epoch 336/1000 
	 loss: 28.5212, MinusLogProbMetric: 28.5212, val_loss: 29.3965, val_MinusLogProbMetric: 29.3965

Epoch 336: val_loss did not improve from 28.68771
196/196 - 41s - loss: 28.5212 - MinusLogProbMetric: 28.5212 - val_loss: 29.3965 - val_MinusLogProbMetric: 29.3965 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 337/1000
2023-10-27 05:49:08.457 
Epoch 337/1000 
	 loss: 28.6244, MinusLogProbMetric: 28.6244, val_loss: 29.7142, val_MinusLogProbMetric: 29.7142

Epoch 337: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.6244 - MinusLogProbMetric: 28.6244 - val_loss: 29.7142 - val_MinusLogProbMetric: 29.7142 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 338/1000
2023-10-27 05:49:50.634 
Epoch 338/1000 
	 loss: 28.5002, MinusLogProbMetric: 28.5002, val_loss: 29.8081, val_MinusLogProbMetric: 29.8081

Epoch 338: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5002 - MinusLogProbMetric: 28.5002 - val_loss: 29.8081 - val_MinusLogProbMetric: 29.8081 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 339/1000
2023-10-27 05:50:33.623 
Epoch 339/1000 
	 loss: 28.4361, MinusLogProbMetric: 28.4361, val_loss: 29.1623, val_MinusLogProbMetric: 29.1623

Epoch 339: val_loss did not improve from 28.68771
196/196 - 43s - loss: 28.4361 - MinusLogProbMetric: 28.4361 - val_loss: 29.1623 - val_MinusLogProbMetric: 29.1623 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 340/1000
2023-10-27 05:51:15.887 
Epoch 340/1000 
	 loss: 28.5171, MinusLogProbMetric: 28.5171, val_loss: 28.8074, val_MinusLogProbMetric: 28.8074

Epoch 340: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5171 - MinusLogProbMetric: 28.5171 - val_loss: 28.8074 - val_MinusLogProbMetric: 28.8074 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 341/1000
2023-10-27 05:51:57.613 
Epoch 341/1000 
	 loss: 28.4369, MinusLogProbMetric: 28.4369, val_loss: 29.1792, val_MinusLogProbMetric: 29.1792

Epoch 341: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4369 - MinusLogProbMetric: 28.4369 - val_loss: 29.1792 - val_MinusLogProbMetric: 29.1792 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 342/1000
2023-10-27 05:52:39.839 
Epoch 342/1000 
	 loss: 28.4297, MinusLogProbMetric: 28.4297, val_loss: 28.7624, val_MinusLogProbMetric: 28.7624

Epoch 342: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4297 - MinusLogProbMetric: 28.4297 - val_loss: 28.7624 - val_MinusLogProbMetric: 28.7624 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 343/1000
2023-10-27 05:53:22.120 
Epoch 343/1000 
	 loss: 28.4460, MinusLogProbMetric: 28.4460, val_loss: 29.7657, val_MinusLogProbMetric: 29.7657

Epoch 343: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4460 - MinusLogProbMetric: 28.4460 - val_loss: 29.7657 - val_MinusLogProbMetric: 29.7657 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 344/1000
2023-10-27 05:54:04.170 
Epoch 344/1000 
	 loss: 28.5081, MinusLogProbMetric: 28.5081, val_loss: 28.8368, val_MinusLogProbMetric: 28.8368

Epoch 344: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5081 - MinusLogProbMetric: 28.5081 - val_loss: 28.8368 - val_MinusLogProbMetric: 28.8368 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 345/1000
2023-10-27 05:54:46.564 
Epoch 345/1000 
	 loss: 28.5155, MinusLogProbMetric: 28.5155, val_loss: 28.8477, val_MinusLogProbMetric: 28.8477

Epoch 345: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5155 - MinusLogProbMetric: 28.5155 - val_loss: 28.8477 - val_MinusLogProbMetric: 28.8477 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 346/1000
2023-10-27 05:55:28.301 
Epoch 346/1000 
	 loss: 28.5049, MinusLogProbMetric: 28.5049, val_loss: 29.3153, val_MinusLogProbMetric: 29.3153

Epoch 346: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5049 - MinusLogProbMetric: 28.5049 - val_loss: 29.3153 - val_MinusLogProbMetric: 29.3153 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 347/1000
2023-10-27 05:56:09.770 
Epoch 347/1000 
	 loss: 28.4520, MinusLogProbMetric: 28.4520, val_loss: 28.9289, val_MinusLogProbMetric: 28.9289

Epoch 347: val_loss did not improve from 28.68771
196/196 - 41s - loss: 28.4520 - MinusLogProbMetric: 28.4520 - val_loss: 28.9289 - val_MinusLogProbMetric: 28.9289 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 348/1000
2023-10-27 05:56:51.862 
Epoch 348/1000 
	 loss: 28.4627, MinusLogProbMetric: 28.4627, val_loss: 29.1555, val_MinusLogProbMetric: 29.1555

Epoch 348: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4627 - MinusLogProbMetric: 28.4627 - val_loss: 29.1555 - val_MinusLogProbMetric: 29.1555 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 349/1000
2023-10-27 05:57:34.326 
Epoch 349/1000 
	 loss: 28.4545, MinusLogProbMetric: 28.4545, val_loss: 29.2477, val_MinusLogProbMetric: 29.2477

Epoch 349: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4545 - MinusLogProbMetric: 28.4545 - val_loss: 29.2477 - val_MinusLogProbMetric: 29.2477 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 350/1000
2023-10-27 05:58:16.637 
Epoch 350/1000 
	 loss: 28.4270, MinusLogProbMetric: 28.4270, val_loss: 28.7130, val_MinusLogProbMetric: 28.7130

Epoch 350: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4270 - MinusLogProbMetric: 28.4270 - val_loss: 28.7130 - val_MinusLogProbMetric: 28.7130 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 351/1000
2023-10-27 05:58:58.823 
Epoch 351/1000 
	 loss: 28.3822, MinusLogProbMetric: 28.3822, val_loss: 29.5825, val_MinusLogProbMetric: 29.5825

Epoch 351: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.3822 - MinusLogProbMetric: 28.3822 - val_loss: 29.5825 - val_MinusLogProbMetric: 29.5825 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 352/1000
2023-10-27 05:59:40.994 
Epoch 352/1000 
	 loss: 28.4279, MinusLogProbMetric: 28.4279, val_loss: 29.0273, val_MinusLogProbMetric: 29.0273

Epoch 352: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4279 - MinusLogProbMetric: 28.4279 - val_loss: 29.0273 - val_MinusLogProbMetric: 29.0273 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 353/1000
2023-10-27 06:00:23.263 
Epoch 353/1000 
	 loss: 28.3848, MinusLogProbMetric: 28.3848, val_loss: 29.1708, val_MinusLogProbMetric: 29.1708

Epoch 353: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.3848 - MinusLogProbMetric: 28.3848 - val_loss: 29.1708 - val_MinusLogProbMetric: 29.1708 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 354/1000
2023-10-27 06:01:05.119 
Epoch 354/1000 
	 loss: 28.4695, MinusLogProbMetric: 28.4695, val_loss: 29.0340, val_MinusLogProbMetric: 29.0340

Epoch 354: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4695 - MinusLogProbMetric: 28.4695 - val_loss: 29.0340 - val_MinusLogProbMetric: 29.0340 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 355/1000
2023-10-27 06:01:46.201 
Epoch 355/1000 
	 loss: 28.5046, MinusLogProbMetric: 28.5046, val_loss: 29.7084, val_MinusLogProbMetric: 29.7084

Epoch 355: val_loss did not improve from 28.68771
196/196 - 41s - loss: 28.5046 - MinusLogProbMetric: 28.5046 - val_loss: 29.7084 - val_MinusLogProbMetric: 29.7084 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 356/1000
2023-10-27 06:02:28.312 
Epoch 356/1000 
	 loss: 28.5001, MinusLogProbMetric: 28.5001, val_loss: 29.7472, val_MinusLogProbMetric: 29.7472

Epoch 356: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.5001 - MinusLogProbMetric: 28.5001 - val_loss: 29.7472 - val_MinusLogProbMetric: 29.7472 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 357/1000
2023-10-27 06:03:10.129 
Epoch 357/1000 
	 loss: 28.4210, MinusLogProbMetric: 28.4210, val_loss: 29.0747, val_MinusLogProbMetric: 29.0747

Epoch 357: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4210 - MinusLogProbMetric: 28.4210 - val_loss: 29.0747 - val_MinusLogProbMetric: 29.0747 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 358/1000
2023-10-27 06:03:51.978 
Epoch 358/1000 
	 loss: 28.4226, MinusLogProbMetric: 28.4226, val_loss: 29.3023, val_MinusLogProbMetric: 29.3023

Epoch 358: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.4226 - MinusLogProbMetric: 28.4226 - val_loss: 29.3023 - val_MinusLogProbMetric: 29.3023 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 359/1000
2023-10-27 06:04:33.456 
Epoch 359/1000 
	 loss: 28.4044, MinusLogProbMetric: 28.4044, val_loss: 29.0061, val_MinusLogProbMetric: 29.0061

Epoch 359: val_loss did not improve from 28.68771
196/196 - 41s - loss: 28.4044 - MinusLogProbMetric: 28.4044 - val_loss: 29.0061 - val_MinusLogProbMetric: 29.0061 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 360/1000
2023-10-27 06:05:16.018 
Epoch 360/1000 
	 loss: 28.4195, MinusLogProbMetric: 28.4195, val_loss: 28.9689, val_MinusLogProbMetric: 28.9689

Epoch 360: val_loss did not improve from 28.68771
196/196 - 43s - loss: 28.4195 - MinusLogProbMetric: 28.4195 - val_loss: 28.9689 - val_MinusLogProbMetric: 28.9689 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 361/1000
2023-10-27 06:05:58.144 
Epoch 361/1000 
	 loss: 28.3410, MinusLogProbMetric: 28.3410, val_loss: 29.2818, val_MinusLogProbMetric: 29.2818

Epoch 361: val_loss did not improve from 28.68771
196/196 - 42s - loss: 28.3410 - MinusLogProbMetric: 28.3410 - val_loss: 29.2818 - val_MinusLogProbMetric: 29.2818 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 362/1000
2023-10-27 06:06:39.542 
Epoch 362/1000 
	 loss: 28.4702, MinusLogProbMetric: 28.4702, val_loss: 29.1678, val_MinusLogProbMetric: 29.1678

Epoch 362: val_loss did not improve from 28.68771
196/196 - 41s - loss: 28.4702 - MinusLogProbMetric: 28.4702 - val_loss: 29.1678 - val_MinusLogProbMetric: 29.1678 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 363/1000
2023-10-27 06:07:21.524 
Epoch 363/1000 
	 loss: 28.3861, MinusLogProbMetric: 28.3861, val_loss: 28.6441, val_MinusLogProbMetric: 28.6441

Epoch 363: val_loss improved from 28.68771 to 28.64411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 28.3861 - MinusLogProbMetric: 28.3861 - val_loss: 28.6441 - val_MinusLogProbMetric: 28.6441 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 364/1000
2023-10-27 06:08:03.253 
Epoch 364/1000 
	 loss: 28.3628, MinusLogProbMetric: 28.3628, val_loss: 29.4929, val_MinusLogProbMetric: 29.4929

Epoch 364: val_loss did not improve from 28.64411
196/196 - 41s - loss: 28.3628 - MinusLogProbMetric: 28.3628 - val_loss: 29.4929 - val_MinusLogProbMetric: 29.4929 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 365/1000
2023-10-27 06:08:45.379 
Epoch 365/1000 
	 loss: 28.4494, MinusLogProbMetric: 28.4494, val_loss: 29.1995, val_MinusLogProbMetric: 29.1995

Epoch 365: val_loss did not improve from 28.64411
196/196 - 42s - loss: 28.4494 - MinusLogProbMetric: 28.4494 - val_loss: 29.1995 - val_MinusLogProbMetric: 29.1995 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 366/1000
2023-10-27 06:09:27.733 
Epoch 366/1000 
	 loss: 28.3163, MinusLogProbMetric: 28.3163, val_loss: 28.7740, val_MinusLogProbMetric: 28.7740

Epoch 366: val_loss did not improve from 28.64411
196/196 - 42s - loss: 28.3163 - MinusLogProbMetric: 28.3163 - val_loss: 28.7740 - val_MinusLogProbMetric: 28.7740 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 367/1000
2023-10-27 06:10:09.665 
Epoch 367/1000 
	 loss: 28.4247, MinusLogProbMetric: 28.4247, val_loss: 28.7512, val_MinusLogProbMetric: 28.7512

Epoch 367: val_loss did not improve from 28.64411
196/196 - 42s - loss: 28.4247 - MinusLogProbMetric: 28.4247 - val_loss: 28.7512 - val_MinusLogProbMetric: 28.7512 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 368/1000
2023-10-27 06:10:49.468 
Epoch 368/1000 
	 loss: 28.3775, MinusLogProbMetric: 28.3775, val_loss: 28.7922, val_MinusLogProbMetric: 28.7922

Epoch 368: val_loss did not improve from 28.64411
196/196 - 40s - loss: 28.3775 - MinusLogProbMetric: 28.3775 - val_loss: 28.7922 - val_MinusLogProbMetric: 28.7922 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 369/1000
2023-10-27 06:11:32.246 
Epoch 369/1000 
	 loss: 28.3434, MinusLogProbMetric: 28.3434, val_loss: 28.9054, val_MinusLogProbMetric: 28.9054

Epoch 369: val_loss did not improve from 28.64411
196/196 - 43s - loss: 28.3434 - MinusLogProbMetric: 28.3434 - val_loss: 28.9054 - val_MinusLogProbMetric: 28.9054 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 370/1000
2023-10-27 06:12:14.352 
Epoch 370/1000 
	 loss: 28.3469, MinusLogProbMetric: 28.3469, val_loss: 29.8748, val_MinusLogProbMetric: 29.8748

Epoch 370: val_loss did not improve from 28.64411
196/196 - 42s - loss: 28.3469 - MinusLogProbMetric: 28.3469 - val_loss: 29.8748 - val_MinusLogProbMetric: 29.8748 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 371/1000
2023-10-27 06:12:56.551 
Epoch 371/1000 
	 loss: 28.3427, MinusLogProbMetric: 28.3427, val_loss: 28.9285, val_MinusLogProbMetric: 28.9285

Epoch 371: val_loss did not improve from 28.64411
196/196 - 42s - loss: 28.3427 - MinusLogProbMetric: 28.3427 - val_loss: 28.9285 - val_MinusLogProbMetric: 28.9285 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 372/1000
2023-10-27 06:13:38.637 
Epoch 372/1000 
	 loss: 28.3928, MinusLogProbMetric: 28.3928, val_loss: 29.0166, val_MinusLogProbMetric: 29.0166

Epoch 372: val_loss did not improve from 28.64411
196/196 - 42s - loss: 28.3928 - MinusLogProbMetric: 28.3928 - val_loss: 29.0166 - val_MinusLogProbMetric: 29.0166 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 373/1000
2023-10-27 06:14:21.028 
Epoch 373/1000 
	 loss: 28.3946, MinusLogProbMetric: 28.3946, val_loss: 28.6149, val_MinusLogProbMetric: 28.6149

Epoch 373: val_loss improved from 28.64411 to 28.61488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 28.3946 - MinusLogProbMetric: 28.3946 - val_loss: 28.6149 - val_MinusLogProbMetric: 28.6149 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 374/1000
2023-10-27 06:15:03.664 
Epoch 374/1000 
	 loss: 28.2910, MinusLogProbMetric: 28.2910, val_loss: 29.0449, val_MinusLogProbMetric: 29.0449

Epoch 374: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.2910 - MinusLogProbMetric: 28.2910 - val_loss: 29.0449 - val_MinusLogProbMetric: 29.0449 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 375/1000
2023-10-27 06:15:45.734 
Epoch 375/1000 
	 loss: 28.3553, MinusLogProbMetric: 28.3553, val_loss: 28.9234, val_MinusLogProbMetric: 28.9234

Epoch 375: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3553 - MinusLogProbMetric: 28.3553 - val_loss: 28.9234 - val_MinusLogProbMetric: 28.9234 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 376/1000
2023-10-27 06:16:27.109 
Epoch 376/1000 
	 loss: 28.3706, MinusLogProbMetric: 28.3706, val_loss: 29.4988, val_MinusLogProbMetric: 29.4988

Epoch 376: val_loss did not improve from 28.61488
196/196 - 41s - loss: 28.3706 - MinusLogProbMetric: 28.3706 - val_loss: 29.4988 - val_MinusLogProbMetric: 29.4988 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 377/1000
2023-10-27 06:17:09.467 
Epoch 377/1000 
	 loss: 28.3321, MinusLogProbMetric: 28.3321, val_loss: 28.8054, val_MinusLogProbMetric: 28.8054

Epoch 377: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3321 - MinusLogProbMetric: 28.3321 - val_loss: 28.8054 - val_MinusLogProbMetric: 28.8054 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 378/1000
2023-10-27 06:17:51.997 
Epoch 378/1000 
	 loss: 28.4100, MinusLogProbMetric: 28.4100, val_loss: 28.9691, val_MinusLogProbMetric: 28.9691

Epoch 378: val_loss did not improve from 28.61488
196/196 - 43s - loss: 28.4100 - MinusLogProbMetric: 28.4100 - val_loss: 28.9691 - val_MinusLogProbMetric: 28.9691 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 379/1000
2023-10-27 06:18:34.621 
Epoch 379/1000 
	 loss: 28.3732, MinusLogProbMetric: 28.3732, val_loss: 29.5224, val_MinusLogProbMetric: 29.5224

Epoch 379: val_loss did not improve from 28.61488
196/196 - 43s - loss: 28.3732 - MinusLogProbMetric: 28.3732 - val_loss: 29.5224 - val_MinusLogProbMetric: 29.5224 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 380/1000
2023-10-27 06:19:16.644 
Epoch 380/1000 
	 loss: 28.3368, MinusLogProbMetric: 28.3368, val_loss: 29.0933, val_MinusLogProbMetric: 29.0933

Epoch 380: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3368 - MinusLogProbMetric: 28.3368 - val_loss: 29.0933 - val_MinusLogProbMetric: 29.0933 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 381/1000
2023-10-27 06:19:58.749 
Epoch 381/1000 
	 loss: 28.3392, MinusLogProbMetric: 28.3392, val_loss: 29.2109, val_MinusLogProbMetric: 29.2109

Epoch 381: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3392 - MinusLogProbMetric: 28.3392 - val_loss: 29.2109 - val_MinusLogProbMetric: 29.2109 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 382/1000
2023-10-27 06:20:40.538 
Epoch 382/1000 
	 loss: 28.3589, MinusLogProbMetric: 28.3589, val_loss: 28.9960, val_MinusLogProbMetric: 28.9960

Epoch 382: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3589 - MinusLogProbMetric: 28.3589 - val_loss: 28.9960 - val_MinusLogProbMetric: 28.9960 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 383/1000
2023-10-27 06:21:22.757 
Epoch 383/1000 
	 loss: 28.3334, MinusLogProbMetric: 28.3334, val_loss: 28.6709, val_MinusLogProbMetric: 28.6709

Epoch 383: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3334 - MinusLogProbMetric: 28.3334 - val_loss: 28.6709 - val_MinusLogProbMetric: 28.6709 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 384/1000
2023-10-27 06:22:05.428 
Epoch 384/1000 
	 loss: 28.3241, MinusLogProbMetric: 28.3241, val_loss: 28.9740, val_MinusLogProbMetric: 28.9740

Epoch 384: val_loss did not improve from 28.61488
196/196 - 43s - loss: 28.3241 - MinusLogProbMetric: 28.3241 - val_loss: 28.9740 - val_MinusLogProbMetric: 28.9740 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 385/1000
2023-10-27 06:22:47.745 
Epoch 385/1000 
	 loss: 28.3156, MinusLogProbMetric: 28.3156, val_loss: 29.2112, val_MinusLogProbMetric: 29.2112

Epoch 385: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3156 - MinusLogProbMetric: 28.3156 - val_loss: 29.2112 - val_MinusLogProbMetric: 29.2112 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 386/1000
2023-10-27 06:23:30.056 
Epoch 386/1000 
	 loss: 28.3321, MinusLogProbMetric: 28.3321, val_loss: 28.8414, val_MinusLogProbMetric: 28.8414

Epoch 386: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3321 - MinusLogProbMetric: 28.3321 - val_loss: 28.8414 - val_MinusLogProbMetric: 28.8414 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 387/1000
2023-10-27 06:24:12.374 
Epoch 387/1000 
	 loss: 28.3688, MinusLogProbMetric: 28.3688, val_loss: 28.8429, val_MinusLogProbMetric: 28.8429

Epoch 387: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3688 - MinusLogProbMetric: 28.3688 - val_loss: 28.8429 - val_MinusLogProbMetric: 28.8429 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 388/1000
2023-10-27 06:24:54.835 
Epoch 388/1000 
	 loss: 28.2869, MinusLogProbMetric: 28.2869, val_loss: 29.2444, val_MinusLogProbMetric: 29.2444

Epoch 388: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.2869 - MinusLogProbMetric: 28.2869 - val_loss: 29.2444 - val_MinusLogProbMetric: 29.2444 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 389/1000
2023-10-27 06:25:36.984 
Epoch 389/1000 
	 loss: 28.3011, MinusLogProbMetric: 28.3011, val_loss: 28.6958, val_MinusLogProbMetric: 28.6958

Epoch 389: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3011 - MinusLogProbMetric: 28.3011 - val_loss: 28.6958 - val_MinusLogProbMetric: 28.6958 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 390/1000
2023-10-27 06:26:19.132 
Epoch 390/1000 
	 loss: 28.3714, MinusLogProbMetric: 28.3714, val_loss: 29.8215, val_MinusLogProbMetric: 29.8215

Epoch 390: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3714 - MinusLogProbMetric: 28.3714 - val_loss: 29.8215 - val_MinusLogProbMetric: 29.8215 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 391/1000
2023-10-27 06:27:01.183 
Epoch 391/1000 
	 loss: 28.3239, MinusLogProbMetric: 28.3239, val_loss: 29.0005, val_MinusLogProbMetric: 29.0005

Epoch 391: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3239 - MinusLogProbMetric: 28.3239 - val_loss: 29.0005 - val_MinusLogProbMetric: 29.0005 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 392/1000
2023-10-27 06:27:43.424 
Epoch 392/1000 
	 loss: 28.2488, MinusLogProbMetric: 28.2488, val_loss: 29.0032, val_MinusLogProbMetric: 29.0032

Epoch 392: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.2488 - MinusLogProbMetric: 28.2488 - val_loss: 29.0032 - val_MinusLogProbMetric: 29.0032 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 393/1000
2023-10-27 06:28:23.049 
Epoch 393/1000 
	 loss: 28.3023, MinusLogProbMetric: 28.3023, val_loss: 29.6261, val_MinusLogProbMetric: 29.6261

Epoch 393: val_loss did not improve from 28.61488
196/196 - 40s - loss: 28.3023 - MinusLogProbMetric: 28.3023 - val_loss: 29.6261 - val_MinusLogProbMetric: 29.6261 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 394/1000
2023-10-27 06:28:59.657 
Epoch 394/1000 
	 loss: 28.3278, MinusLogProbMetric: 28.3278, val_loss: 28.9018, val_MinusLogProbMetric: 28.9018

Epoch 394: val_loss did not improve from 28.61488
196/196 - 37s - loss: 28.3278 - MinusLogProbMetric: 28.3278 - val_loss: 28.9018 - val_MinusLogProbMetric: 28.9018 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 395/1000
2023-10-27 06:29:41.265 
Epoch 395/1000 
	 loss: 28.3449, MinusLogProbMetric: 28.3449, val_loss: 28.9719, val_MinusLogProbMetric: 28.9719

Epoch 395: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3449 - MinusLogProbMetric: 28.3449 - val_loss: 28.9719 - val_MinusLogProbMetric: 28.9719 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 396/1000
2023-10-27 06:30:17.013 
Epoch 396/1000 
	 loss: 28.2248, MinusLogProbMetric: 28.2248, val_loss: 29.3708, val_MinusLogProbMetric: 29.3708

Epoch 396: val_loss did not improve from 28.61488
196/196 - 36s - loss: 28.2248 - MinusLogProbMetric: 28.2248 - val_loss: 29.3708 - val_MinusLogProbMetric: 29.3708 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 397/1000
2023-10-27 06:30:56.581 
Epoch 397/1000 
	 loss: 28.2818, MinusLogProbMetric: 28.2818, val_loss: 29.6012, val_MinusLogProbMetric: 29.6012

Epoch 397: val_loss did not improve from 28.61488
196/196 - 40s - loss: 28.2818 - MinusLogProbMetric: 28.2818 - val_loss: 29.6012 - val_MinusLogProbMetric: 29.6012 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 398/1000
2023-10-27 06:31:38.465 
Epoch 398/1000 
	 loss: 28.3095, MinusLogProbMetric: 28.3095, val_loss: 28.7382, val_MinusLogProbMetric: 28.7382

Epoch 398: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3095 - MinusLogProbMetric: 28.3095 - val_loss: 28.7382 - val_MinusLogProbMetric: 28.7382 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 399/1000
2023-10-27 06:32:20.521 
Epoch 399/1000 
	 loss: 28.3455, MinusLogProbMetric: 28.3455, val_loss: 29.0694, val_MinusLogProbMetric: 29.0694

Epoch 399: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3455 - MinusLogProbMetric: 28.3455 - val_loss: 29.0694 - val_MinusLogProbMetric: 29.0694 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 400/1000
2023-10-27 06:33:02.781 
Epoch 400/1000 
	 loss: 28.2794, MinusLogProbMetric: 28.2794, val_loss: 29.2191, val_MinusLogProbMetric: 29.2191

Epoch 400: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.2794 - MinusLogProbMetric: 28.2794 - val_loss: 29.2191 - val_MinusLogProbMetric: 29.2191 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 401/1000
2023-10-27 06:33:43.631 
Epoch 401/1000 
	 loss: 28.2953, MinusLogProbMetric: 28.2953, val_loss: 29.6214, val_MinusLogProbMetric: 29.6214

Epoch 401: val_loss did not improve from 28.61488
196/196 - 41s - loss: 28.2953 - MinusLogProbMetric: 28.2953 - val_loss: 29.6214 - val_MinusLogProbMetric: 29.6214 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 402/1000
2023-10-27 06:34:25.683 
Epoch 402/1000 
	 loss: 28.3533, MinusLogProbMetric: 28.3533, val_loss: 29.2912, val_MinusLogProbMetric: 29.2912

Epoch 402: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3533 - MinusLogProbMetric: 28.3533 - val_loss: 29.2912 - val_MinusLogProbMetric: 29.2912 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 403/1000
2023-10-27 06:35:07.038 
Epoch 403/1000 
	 loss: 28.2537, MinusLogProbMetric: 28.2537, val_loss: 28.7726, val_MinusLogProbMetric: 28.7726

Epoch 403: val_loss did not improve from 28.61488
196/196 - 41s - loss: 28.2537 - MinusLogProbMetric: 28.2537 - val_loss: 28.7726 - val_MinusLogProbMetric: 28.7726 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 404/1000
2023-10-27 06:35:48.157 
Epoch 404/1000 
	 loss: 28.3019, MinusLogProbMetric: 28.3019, val_loss: 28.8746, val_MinusLogProbMetric: 28.8746

Epoch 404: val_loss did not improve from 28.61488
196/196 - 41s - loss: 28.3019 - MinusLogProbMetric: 28.3019 - val_loss: 28.8746 - val_MinusLogProbMetric: 28.8746 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 405/1000
2023-10-27 06:36:30.452 
Epoch 405/1000 
	 loss: 28.2420, MinusLogProbMetric: 28.2420, val_loss: 29.1081, val_MinusLogProbMetric: 29.1081

Epoch 405: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.2420 - MinusLogProbMetric: 28.2420 - val_loss: 29.1081 - val_MinusLogProbMetric: 29.1081 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 406/1000
2023-10-27 06:37:12.840 
Epoch 406/1000 
	 loss: 28.3038, MinusLogProbMetric: 28.3038, val_loss: 29.1446, val_MinusLogProbMetric: 29.1446

Epoch 406: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.3038 - MinusLogProbMetric: 28.3038 - val_loss: 29.1446 - val_MinusLogProbMetric: 29.1446 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 407/1000
2023-10-27 06:37:55.433 
Epoch 407/1000 
	 loss: 28.2482, MinusLogProbMetric: 28.2482, val_loss: 28.8457, val_MinusLogProbMetric: 28.8457

Epoch 407: val_loss did not improve from 28.61488
196/196 - 43s - loss: 28.2482 - MinusLogProbMetric: 28.2482 - val_loss: 28.8457 - val_MinusLogProbMetric: 28.8457 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 408/1000
2023-10-27 06:38:36.600 
Epoch 408/1000 
	 loss: 28.3382, MinusLogProbMetric: 28.3382, val_loss: 29.2562, val_MinusLogProbMetric: 29.2562

Epoch 408: val_loss did not improve from 28.61488
196/196 - 41s - loss: 28.3382 - MinusLogProbMetric: 28.3382 - val_loss: 29.2562 - val_MinusLogProbMetric: 29.2562 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 409/1000
2023-10-27 06:39:18.885 
Epoch 409/1000 
	 loss: 28.2663, MinusLogProbMetric: 28.2663, val_loss: 28.7557, val_MinusLogProbMetric: 28.7557

Epoch 409: val_loss did not improve from 28.61488
196/196 - 42s - loss: 28.2663 - MinusLogProbMetric: 28.2663 - val_loss: 28.7557 - val_MinusLogProbMetric: 28.7557 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 410/1000
2023-10-27 06:40:00.764 
Epoch 410/1000 
	 loss: 28.2837, MinusLogProbMetric: 28.2837, val_loss: 28.5732, val_MinusLogProbMetric: 28.5732

Epoch 410: val_loss improved from 28.61488 to 28.57323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 28.2837 - MinusLogProbMetric: 28.2837 - val_loss: 28.5732 - val_MinusLogProbMetric: 28.5732 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 411/1000
2023-10-27 06:40:43.632 
Epoch 411/1000 
	 loss: 28.3127, MinusLogProbMetric: 28.3127, val_loss: 29.4708, val_MinusLogProbMetric: 29.4708

Epoch 411: val_loss did not improve from 28.57323
196/196 - 42s - loss: 28.3127 - MinusLogProbMetric: 28.3127 - val_loss: 29.4708 - val_MinusLogProbMetric: 29.4708 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 412/1000
2023-10-27 06:41:25.638 
Epoch 412/1000 
	 loss: 28.2360, MinusLogProbMetric: 28.2360, val_loss: 29.4455, val_MinusLogProbMetric: 29.4455

Epoch 412: val_loss did not improve from 28.57323
196/196 - 42s - loss: 28.2360 - MinusLogProbMetric: 28.2360 - val_loss: 29.4455 - val_MinusLogProbMetric: 29.4455 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 413/1000
2023-10-27 06:42:07.539 
Epoch 413/1000 
	 loss: 28.2932, MinusLogProbMetric: 28.2932, val_loss: 28.8545, val_MinusLogProbMetric: 28.8545

Epoch 413: val_loss did not improve from 28.57323
196/196 - 42s - loss: 28.2932 - MinusLogProbMetric: 28.2932 - val_loss: 28.8545 - val_MinusLogProbMetric: 28.8545 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 414/1000
2023-10-27 06:42:48.324 
Epoch 414/1000 
	 loss: 28.3251, MinusLogProbMetric: 28.3251, val_loss: 29.5680, val_MinusLogProbMetric: 29.5680

Epoch 414: val_loss did not improve from 28.57323
196/196 - 41s - loss: 28.3251 - MinusLogProbMetric: 28.3251 - val_loss: 29.5680 - val_MinusLogProbMetric: 29.5680 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 415/1000
2023-10-27 06:43:30.778 
Epoch 415/1000 
	 loss: 28.2552, MinusLogProbMetric: 28.2552, val_loss: 28.5016, val_MinusLogProbMetric: 28.5016

Epoch 415: val_loss improved from 28.57323 to 28.50162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 28.2552 - MinusLogProbMetric: 28.2552 - val_loss: 28.5016 - val_MinusLogProbMetric: 28.5016 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 416/1000
2023-10-27 06:44:13.715 
Epoch 416/1000 
	 loss: 28.3285, MinusLogProbMetric: 28.3285, val_loss: 28.8235, val_MinusLogProbMetric: 28.8235

Epoch 416: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.3285 - MinusLogProbMetric: 28.3285 - val_loss: 28.8235 - val_MinusLogProbMetric: 28.8235 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 417/1000
2023-10-27 06:44:55.660 
Epoch 417/1000 
	 loss: 28.3315, MinusLogProbMetric: 28.3315, val_loss: 29.0434, val_MinusLogProbMetric: 29.0434

Epoch 417: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.3315 - MinusLogProbMetric: 28.3315 - val_loss: 29.0434 - val_MinusLogProbMetric: 29.0434 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 418/1000
2023-10-27 06:45:38.423 
Epoch 418/1000 
	 loss: 28.2425, MinusLogProbMetric: 28.2425, val_loss: 28.7846, val_MinusLogProbMetric: 28.7846

Epoch 418: val_loss did not improve from 28.50162
196/196 - 43s - loss: 28.2425 - MinusLogProbMetric: 28.2425 - val_loss: 28.7846 - val_MinusLogProbMetric: 28.7846 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 419/1000
2023-10-27 06:46:20.743 
Epoch 419/1000 
	 loss: 28.3034, MinusLogProbMetric: 28.3034, val_loss: 29.1048, val_MinusLogProbMetric: 29.1048

Epoch 419: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.3034 - MinusLogProbMetric: 28.3034 - val_loss: 29.1048 - val_MinusLogProbMetric: 29.1048 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 420/1000
2023-10-27 06:47:03.157 
Epoch 420/1000 
	 loss: 28.2501, MinusLogProbMetric: 28.2501, val_loss: 29.5211, val_MinusLogProbMetric: 29.5211

Epoch 420: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2501 - MinusLogProbMetric: 28.2501 - val_loss: 29.5211 - val_MinusLogProbMetric: 29.5211 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 421/1000
2023-10-27 06:47:45.964 
Epoch 421/1000 
	 loss: 28.2478, MinusLogProbMetric: 28.2478, val_loss: 28.7910, val_MinusLogProbMetric: 28.7910

Epoch 421: val_loss did not improve from 28.50162
196/196 - 43s - loss: 28.2478 - MinusLogProbMetric: 28.2478 - val_loss: 28.7910 - val_MinusLogProbMetric: 28.7910 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 422/1000
2023-10-27 06:48:28.452 
Epoch 422/1000 
	 loss: 28.2288, MinusLogProbMetric: 28.2288, val_loss: 28.8818, val_MinusLogProbMetric: 28.8818

Epoch 422: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2288 - MinusLogProbMetric: 28.2288 - val_loss: 28.8818 - val_MinusLogProbMetric: 28.8818 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 423/1000
2023-10-27 06:49:10.683 
Epoch 423/1000 
	 loss: 28.2368, MinusLogProbMetric: 28.2368, val_loss: 29.3196, val_MinusLogProbMetric: 29.3196

Epoch 423: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2368 - MinusLogProbMetric: 28.2368 - val_loss: 29.3196 - val_MinusLogProbMetric: 29.3196 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 424/1000
2023-10-27 06:49:52.724 
Epoch 424/1000 
	 loss: 28.2674, MinusLogProbMetric: 28.2674, val_loss: 28.8408, val_MinusLogProbMetric: 28.8408

Epoch 424: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2674 - MinusLogProbMetric: 28.2674 - val_loss: 28.8408 - val_MinusLogProbMetric: 28.8408 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 425/1000
2023-10-27 06:50:34.444 
Epoch 425/1000 
	 loss: 28.1633, MinusLogProbMetric: 28.1633, val_loss: 28.8080, val_MinusLogProbMetric: 28.8080

Epoch 425: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1633 - MinusLogProbMetric: 28.1633 - val_loss: 28.8080 - val_MinusLogProbMetric: 28.8080 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 426/1000
2023-10-27 06:51:16.134 
Epoch 426/1000 
	 loss: 28.1784, MinusLogProbMetric: 28.1784, val_loss: 29.3655, val_MinusLogProbMetric: 29.3655

Epoch 426: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1784 - MinusLogProbMetric: 28.1784 - val_loss: 29.3655 - val_MinusLogProbMetric: 29.3655 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 427/1000
2023-10-27 06:51:57.852 
Epoch 427/1000 
	 loss: 28.2560, MinusLogProbMetric: 28.2560, val_loss: 29.1361, val_MinusLogProbMetric: 29.1361

Epoch 427: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2560 - MinusLogProbMetric: 28.2560 - val_loss: 29.1361 - val_MinusLogProbMetric: 29.1361 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 428/1000
2023-10-27 06:52:38.530 
Epoch 428/1000 
	 loss: 28.2516, MinusLogProbMetric: 28.2516, val_loss: 28.7933, val_MinusLogProbMetric: 28.7933

Epoch 428: val_loss did not improve from 28.50162
196/196 - 41s - loss: 28.2516 - MinusLogProbMetric: 28.2516 - val_loss: 28.7933 - val_MinusLogProbMetric: 28.7933 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 429/1000
2023-10-27 06:53:20.213 
Epoch 429/1000 
	 loss: 28.2041, MinusLogProbMetric: 28.2041, val_loss: 29.4424, val_MinusLogProbMetric: 29.4424

Epoch 429: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2041 - MinusLogProbMetric: 28.2041 - val_loss: 29.4424 - val_MinusLogProbMetric: 29.4424 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 430/1000
2023-10-27 06:54:01.421 
Epoch 430/1000 
	 loss: 28.2184, MinusLogProbMetric: 28.2184, val_loss: 28.7464, val_MinusLogProbMetric: 28.7464

Epoch 430: val_loss did not improve from 28.50162
196/196 - 41s - loss: 28.2184 - MinusLogProbMetric: 28.2184 - val_loss: 28.7464 - val_MinusLogProbMetric: 28.7464 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 431/1000
2023-10-27 06:54:43.180 
Epoch 431/1000 
	 loss: 28.2827, MinusLogProbMetric: 28.2827, val_loss: 29.0738, val_MinusLogProbMetric: 29.0738

Epoch 431: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2827 - MinusLogProbMetric: 28.2827 - val_loss: 29.0738 - val_MinusLogProbMetric: 29.0738 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 432/1000
2023-10-27 06:55:25.232 
Epoch 432/1000 
	 loss: 28.1836, MinusLogProbMetric: 28.1836, val_loss: 28.7796, val_MinusLogProbMetric: 28.7796

Epoch 432: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1836 - MinusLogProbMetric: 28.1836 - val_loss: 28.7796 - val_MinusLogProbMetric: 28.7796 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 433/1000
2023-10-27 06:56:07.780 
Epoch 433/1000 
	 loss: 28.2127, MinusLogProbMetric: 28.2127, val_loss: 28.9530, val_MinusLogProbMetric: 28.9530

Epoch 433: val_loss did not improve from 28.50162
196/196 - 43s - loss: 28.2127 - MinusLogProbMetric: 28.2127 - val_loss: 28.9530 - val_MinusLogProbMetric: 28.9530 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 434/1000
2023-10-27 06:56:49.903 
Epoch 434/1000 
	 loss: 28.1842, MinusLogProbMetric: 28.1842, val_loss: 28.7900, val_MinusLogProbMetric: 28.7900

Epoch 434: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1842 - MinusLogProbMetric: 28.1842 - val_loss: 28.7900 - val_MinusLogProbMetric: 28.7900 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 435/1000
2023-10-27 06:57:31.799 
Epoch 435/1000 
	 loss: 28.1808, MinusLogProbMetric: 28.1808, val_loss: 29.4097, val_MinusLogProbMetric: 29.4097

Epoch 435: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1808 - MinusLogProbMetric: 28.1808 - val_loss: 29.4097 - val_MinusLogProbMetric: 29.4097 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 436/1000
2023-10-27 06:58:13.465 
Epoch 436/1000 
	 loss: 28.2012, MinusLogProbMetric: 28.2012, val_loss: 28.8871, val_MinusLogProbMetric: 28.8871

Epoch 436: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2012 - MinusLogProbMetric: 28.2012 - val_loss: 28.8871 - val_MinusLogProbMetric: 28.8871 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 437/1000
2023-10-27 06:58:54.362 
Epoch 437/1000 
	 loss: 28.1546, MinusLogProbMetric: 28.1546, val_loss: 29.4116, val_MinusLogProbMetric: 29.4116

Epoch 437: val_loss did not improve from 28.50162
196/196 - 41s - loss: 28.1546 - MinusLogProbMetric: 28.1546 - val_loss: 29.4116 - val_MinusLogProbMetric: 29.4116 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 438/1000
2023-10-27 06:59:35.714 
Epoch 438/1000 
	 loss: 28.1924, MinusLogProbMetric: 28.1924, val_loss: 28.8236, val_MinusLogProbMetric: 28.8236

Epoch 438: val_loss did not improve from 28.50162
196/196 - 41s - loss: 28.1924 - MinusLogProbMetric: 28.1924 - val_loss: 28.8236 - val_MinusLogProbMetric: 28.8236 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 439/1000
2023-10-27 07:00:18.023 
Epoch 439/1000 
	 loss: 28.1667, MinusLogProbMetric: 28.1667, val_loss: 29.1459, val_MinusLogProbMetric: 29.1459

Epoch 439: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1667 - MinusLogProbMetric: 28.1667 - val_loss: 29.1459 - val_MinusLogProbMetric: 29.1459 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 440/1000
2023-10-27 07:01:00.523 
Epoch 440/1000 
	 loss: 28.1938, MinusLogProbMetric: 28.1938, val_loss: 28.5995, val_MinusLogProbMetric: 28.5995

Epoch 440: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1938 - MinusLogProbMetric: 28.1938 - val_loss: 28.5995 - val_MinusLogProbMetric: 28.5995 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 441/1000
2023-10-27 07:01:42.598 
Epoch 441/1000 
	 loss: 28.1009, MinusLogProbMetric: 28.1009, val_loss: 29.0187, val_MinusLogProbMetric: 29.0187

Epoch 441: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1009 - MinusLogProbMetric: 28.1009 - val_loss: 29.0187 - val_MinusLogProbMetric: 29.0187 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 442/1000
2023-10-27 07:02:24.718 
Epoch 442/1000 
	 loss: 28.2449, MinusLogProbMetric: 28.2449, val_loss: 29.2383, val_MinusLogProbMetric: 29.2383

Epoch 442: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2449 - MinusLogProbMetric: 28.2449 - val_loss: 29.2383 - val_MinusLogProbMetric: 29.2383 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 443/1000
2023-10-27 07:03:05.734 
Epoch 443/1000 
	 loss: 28.1772, MinusLogProbMetric: 28.1772, val_loss: 30.4628, val_MinusLogProbMetric: 30.4628

Epoch 443: val_loss did not improve from 28.50162
196/196 - 41s - loss: 28.1772 - MinusLogProbMetric: 28.1772 - val_loss: 30.4628 - val_MinusLogProbMetric: 30.4628 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 444/1000
2023-10-27 07:03:47.301 
Epoch 444/1000 
	 loss: 28.2248, MinusLogProbMetric: 28.2248, val_loss: 29.0143, val_MinusLogProbMetric: 29.0143

Epoch 444: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2248 - MinusLogProbMetric: 28.2248 - val_loss: 29.0143 - val_MinusLogProbMetric: 29.0143 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 445/1000
2023-10-27 07:04:29.233 
Epoch 445/1000 
	 loss: 28.2203, MinusLogProbMetric: 28.2203, val_loss: 28.8410, val_MinusLogProbMetric: 28.8410

Epoch 445: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2203 - MinusLogProbMetric: 28.2203 - val_loss: 28.8410 - val_MinusLogProbMetric: 28.8410 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 446/1000
2023-10-27 07:05:11.517 
Epoch 446/1000 
	 loss: 28.2156, MinusLogProbMetric: 28.2156, val_loss: 28.9652, val_MinusLogProbMetric: 28.9652

Epoch 446: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2156 - MinusLogProbMetric: 28.2156 - val_loss: 28.9652 - val_MinusLogProbMetric: 28.9652 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 447/1000
2023-10-27 07:05:53.213 
Epoch 447/1000 
	 loss: 28.1879, MinusLogProbMetric: 28.1879, val_loss: 28.8333, val_MinusLogProbMetric: 28.8333

Epoch 447: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1879 - MinusLogProbMetric: 28.1879 - val_loss: 28.8333 - val_MinusLogProbMetric: 28.8333 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 448/1000
2023-10-27 07:06:28.766 
Epoch 448/1000 
	 loss: 28.1962, MinusLogProbMetric: 28.1962, val_loss: 28.8426, val_MinusLogProbMetric: 28.8426

Epoch 448: val_loss did not improve from 28.50162
196/196 - 36s - loss: 28.1962 - MinusLogProbMetric: 28.1962 - val_loss: 28.8426 - val_MinusLogProbMetric: 28.8426 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 449/1000
2023-10-27 07:07:03.986 
Epoch 449/1000 
	 loss: 28.1792, MinusLogProbMetric: 28.1792, val_loss: 28.5815, val_MinusLogProbMetric: 28.5815

Epoch 449: val_loss did not improve from 28.50162
196/196 - 35s - loss: 28.1792 - MinusLogProbMetric: 28.1792 - val_loss: 28.5815 - val_MinusLogProbMetric: 28.5815 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 450/1000
2023-10-27 07:07:38.173 
Epoch 450/1000 
	 loss: 28.2530, MinusLogProbMetric: 28.2530, val_loss: 29.1711, val_MinusLogProbMetric: 29.1711

Epoch 450: val_loss did not improve from 28.50162
196/196 - 34s - loss: 28.2530 - MinusLogProbMetric: 28.2530 - val_loss: 29.1711 - val_MinusLogProbMetric: 29.1711 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 451/1000
2023-10-27 07:08:16.262 
Epoch 451/1000 
	 loss: 28.1590, MinusLogProbMetric: 28.1590, val_loss: 28.7981, val_MinusLogProbMetric: 28.7981

Epoch 451: val_loss did not improve from 28.50162
196/196 - 38s - loss: 28.1590 - MinusLogProbMetric: 28.1590 - val_loss: 28.7981 - val_MinusLogProbMetric: 28.7981 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 452/1000
2023-10-27 07:08:58.611 
Epoch 452/1000 
	 loss: 28.2169, MinusLogProbMetric: 28.2169, val_loss: 28.9893, val_MinusLogProbMetric: 28.9893

Epoch 452: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.2169 - MinusLogProbMetric: 28.2169 - val_loss: 28.9893 - val_MinusLogProbMetric: 28.9893 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 453/1000
2023-10-27 07:09:40.734 
Epoch 453/1000 
	 loss: 28.1811, MinusLogProbMetric: 28.1811, val_loss: 28.9409, val_MinusLogProbMetric: 28.9409

Epoch 453: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1811 - MinusLogProbMetric: 28.1811 - val_loss: 28.9409 - val_MinusLogProbMetric: 28.9409 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 454/1000
2023-10-27 07:10:21.670 
Epoch 454/1000 
	 loss: 28.1361, MinusLogProbMetric: 28.1361, val_loss: 29.0021, val_MinusLogProbMetric: 29.0021

Epoch 454: val_loss did not improve from 28.50162
196/196 - 41s - loss: 28.1361 - MinusLogProbMetric: 28.1361 - val_loss: 29.0021 - val_MinusLogProbMetric: 29.0021 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 455/1000
2023-10-27 07:11:03.841 
Epoch 455/1000 
	 loss: 28.1489, MinusLogProbMetric: 28.1489, val_loss: 28.6695, val_MinusLogProbMetric: 28.6695

Epoch 455: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1489 - MinusLogProbMetric: 28.1489 - val_loss: 28.6695 - val_MinusLogProbMetric: 28.6695 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 456/1000
2023-10-27 07:11:44.791 
Epoch 456/1000 
	 loss: 28.1393, MinusLogProbMetric: 28.1393, val_loss: 28.8736, val_MinusLogProbMetric: 28.8736

Epoch 456: val_loss did not improve from 28.50162
196/196 - 41s - loss: 28.1393 - MinusLogProbMetric: 28.1393 - val_loss: 28.8736 - val_MinusLogProbMetric: 28.8736 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 457/1000
2023-10-27 07:12:27.235 
Epoch 457/1000 
	 loss: 28.1464, MinusLogProbMetric: 28.1464, val_loss: 28.9011, val_MinusLogProbMetric: 28.9011

Epoch 457: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.1464 - MinusLogProbMetric: 28.1464 - val_loss: 28.9011 - val_MinusLogProbMetric: 28.9011 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 458/1000
2023-10-27 07:13:09.844 
Epoch 458/1000 
	 loss: 28.1181, MinusLogProbMetric: 28.1181, val_loss: 28.7247, val_MinusLogProbMetric: 28.7247

Epoch 458: val_loss did not improve from 28.50162
196/196 - 43s - loss: 28.1181 - MinusLogProbMetric: 28.1181 - val_loss: 28.7247 - val_MinusLogProbMetric: 28.7247 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 459/1000
2023-10-27 07:13:47.172 
Epoch 459/1000 
	 loss: 28.1678, MinusLogProbMetric: 28.1678, val_loss: 29.2827, val_MinusLogProbMetric: 29.2827

Epoch 459: val_loss did not improve from 28.50162
196/196 - 37s - loss: 28.1678 - MinusLogProbMetric: 28.1678 - val_loss: 29.2827 - val_MinusLogProbMetric: 29.2827 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 460/1000
2023-10-27 07:14:22.582 
Epoch 460/1000 
	 loss: 28.1699, MinusLogProbMetric: 28.1699, val_loss: 28.9729, val_MinusLogProbMetric: 28.9729

Epoch 460: val_loss did not improve from 28.50162
196/196 - 35s - loss: 28.1699 - MinusLogProbMetric: 28.1699 - val_loss: 28.9729 - val_MinusLogProbMetric: 28.9729 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 461/1000
2023-10-27 07:14:57.477 
Epoch 461/1000 
	 loss: 28.1747, MinusLogProbMetric: 28.1747, val_loss: 28.7871, val_MinusLogProbMetric: 28.7871

Epoch 461: val_loss did not improve from 28.50162
196/196 - 35s - loss: 28.1747 - MinusLogProbMetric: 28.1747 - val_loss: 28.7871 - val_MinusLogProbMetric: 28.7871 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 462/1000
2023-10-27 07:15:36.998 
Epoch 462/1000 
	 loss: 28.1036, MinusLogProbMetric: 28.1036, val_loss: 29.3165, val_MinusLogProbMetric: 29.3165

Epoch 462: val_loss did not improve from 28.50162
196/196 - 40s - loss: 28.1036 - MinusLogProbMetric: 28.1036 - val_loss: 29.3165 - val_MinusLogProbMetric: 29.3165 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 463/1000
2023-10-27 07:16:17.283 
Epoch 463/1000 
	 loss: 28.1488, MinusLogProbMetric: 28.1488, val_loss: 29.1893, val_MinusLogProbMetric: 29.1893

Epoch 463: val_loss did not improve from 28.50162
196/196 - 40s - loss: 28.1488 - MinusLogProbMetric: 28.1488 - val_loss: 29.1893 - val_MinusLogProbMetric: 29.1893 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 464/1000
2023-10-27 07:16:59.297 
Epoch 464/1000 
	 loss: 28.0902, MinusLogProbMetric: 28.0902, val_loss: 28.7656, val_MinusLogProbMetric: 28.7656

Epoch 464: val_loss did not improve from 28.50162
196/196 - 42s - loss: 28.0902 - MinusLogProbMetric: 28.0902 - val_loss: 28.7656 - val_MinusLogProbMetric: 28.7656 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 465/1000
2023-10-27 07:17:40.079 
Epoch 465/1000 
	 loss: 28.1397, MinusLogProbMetric: 28.1397, val_loss: 29.2098, val_MinusLogProbMetric: 29.2098

Epoch 465: val_loss did not improve from 28.50162
196/196 - 41s - loss: 28.1397 - MinusLogProbMetric: 28.1397 - val_loss: 29.2098 - val_MinusLogProbMetric: 29.2098 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 466/1000
2023-10-27 07:18:20.595 
Epoch 466/1000 
	 loss: 27.6322, MinusLogProbMetric: 27.6322, val_loss: 28.3022, val_MinusLogProbMetric: 28.3022

Epoch 466: val_loss improved from 28.50162 to 28.30218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 41s - loss: 27.6322 - MinusLogProbMetric: 27.6322 - val_loss: 28.3022 - val_MinusLogProbMetric: 28.3022 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 467/1000
2023-10-27 07:19:01.978 
Epoch 467/1000 
	 loss: 27.6128, MinusLogProbMetric: 27.6128, val_loss: 28.4284, val_MinusLogProbMetric: 28.4284

Epoch 467: val_loss did not improve from 28.30218
196/196 - 41s - loss: 27.6128 - MinusLogProbMetric: 27.6128 - val_loss: 28.4284 - val_MinusLogProbMetric: 28.4284 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 468/1000
2023-10-27 07:19:43.906 
Epoch 468/1000 
	 loss: 27.5994, MinusLogProbMetric: 27.5994, val_loss: 28.4222, val_MinusLogProbMetric: 28.4222

Epoch 468: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5994 - MinusLogProbMetric: 27.5994 - val_loss: 28.4222 - val_MinusLogProbMetric: 28.4222 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 469/1000
2023-10-27 07:20:25.500 
Epoch 469/1000 
	 loss: 27.6041, MinusLogProbMetric: 27.6041, val_loss: 28.5068, val_MinusLogProbMetric: 28.5068

Epoch 469: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6041 - MinusLogProbMetric: 27.6041 - val_loss: 28.5068 - val_MinusLogProbMetric: 28.5068 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 470/1000
2023-10-27 07:21:08.004 
Epoch 470/1000 
	 loss: 27.6219, MinusLogProbMetric: 27.6219, val_loss: 28.3484, val_MinusLogProbMetric: 28.3484

Epoch 470: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6219 - MinusLogProbMetric: 27.6219 - val_loss: 28.3484 - val_MinusLogProbMetric: 28.3484 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 471/1000
2023-10-27 07:21:49.121 
Epoch 471/1000 
	 loss: 27.5787, MinusLogProbMetric: 27.5787, val_loss: 28.3185, val_MinusLogProbMetric: 28.3185

Epoch 471: val_loss did not improve from 28.30218
196/196 - 41s - loss: 27.5787 - MinusLogProbMetric: 27.5787 - val_loss: 28.3185 - val_MinusLogProbMetric: 28.3185 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 472/1000
2023-10-27 07:22:31.979 
Epoch 472/1000 
	 loss: 27.6082, MinusLogProbMetric: 27.6082, val_loss: 28.3168, val_MinusLogProbMetric: 28.3168

Epoch 472: val_loss did not improve from 28.30218
196/196 - 43s - loss: 27.6082 - MinusLogProbMetric: 27.6082 - val_loss: 28.3168 - val_MinusLogProbMetric: 28.3168 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 473/1000
2023-10-27 07:23:14.594 
Epoch 473/1000 
	 loss: 27.6173, MinusLogProbMetric: 27.6173, val_loss: 28.4179, val_MinusLogProbMetric: 28.4179

Epoch 473: val_loss did not improve from 28.30218
196/196 - 43s - loss: 27.6173 - MinusLogProbMetric: 27.6173 - val_loss: 28.4179 - val_MinusLogProbMetric: 28.4179 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 474/1000
2023-10-27 07:23:56.484 
Epoch 474/1000 
	 loss: 27.6016, MinusLogProbMetric: 27.6016, val_loss: 28.4191, val_MinusLogProbMetric: 28.4191

Epoch 474: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6016 - MinusLogProbMetric: 27.6016 - val_loss: 28.4191 - val_MinusLogProbMetric: 28.4191 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 475/1000
2023-10-27 07:24:37.393 
Epoch 475/1000 
	 loss: 27.5844, MinusLogProbMetric: 27.5844, val_loss: 28.5162, val_MinusLogProbMetric: 28.5162

Epoch 475: val_loss did not improve from 28.30218
196/196 - 41s - loss: 27.5844 - MinusLogProbMetric: 27.5844 - val_loss: 28.5162 - val_MinusLogProbMetric: 28.5162 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 476/1000
2023-10-27 07:25:17.974 
Epoch 476/1000 
	 loss: 27.6047, MinusLogProbMetric: 27.6047, val_loss: 28.3847, val_MinusLogProbMetric: 28.3847

Epoch 476: val_loss did not improve from 28.30218
196/196 - 41s - loss: 27.6047 - MinusLogProbMetric: 27.6047 - val_loss: 28.3847 - val_MinusLogProbMetric: 28.3847 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 477/1000
2023-10-27 07:26:00.684 
Epoch 477/1000 
	 loss: 27.6229, MinusLogProbMetric: 27.6229, val_loss: 28.5859, val_MinusLogProbMetric: 28.5859

Epoch 477: val_loss did not improve from 28.30218
196/196 - 43s - loss: 27.6229 - MinusLogProbMetric: 27.6229 - val_loss: 28.5859 - val_MinusLogProbMetric: 28.5859 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 478/1000
2023-10-27 07:26:42.349 
Epoch 478/1000 
	 loss: 27.5871, MinusLogProbMetric: 27.5871, val_loss: 28.3360, val_MinusLogProbMetric: 28.3360

Epoch 478: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5871 - MinusLogProbMetric: 27.5871 - val_loss: 28.3360 - val_MinusLogProbMetric: 28.3360 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 479/1000
2023-10-27 07:27:24.366 
Epoch 479/1000 
	 loss: 27.6106, MinusLogProbMetric: 27.6106, val_loss: 28.3197, val_MinusLogProbMetric: 28.3197

Epoch 479: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6106 - MinusLogProbMetric: 27.6106 - val_loss: 28.3197 - val_MinusLogProbMetric: 28.3197 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 480/1000
2023-10-27 07:28:06.423 
Epoch 480/1000 
	 loss: 27.6110, MinusLogProbMetric: 27.6110, val_loss: 28.3560, val_MinusLogProbMetric: 28.3560

Epoch 480: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6110 - MinusLogProbMetric: 27.6110 - val_loss: 28.3560 - val_MinusLogProbMetric: 28.3560 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 481/1000
2023-10-27 07:28:47.462 
Epoch 481/1000 
	 loss: 27.6007, MinusLogProbMetric: 27.6007, val_loss: 28.4471, val_MinusLogProbMetric: 28.4471

Epoch 481: val_loss did not improve from 28.30218
196/196 - 41s - loss: 27.6007 - MinusLogProbMetric: 27.6007 - val_loss: 28.4471 - val_MinusLogProbMetric: 28.4471 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 482/1000
2023-10-27 07:29:29.521 
Epoch 482/1000 
	 loss: 27.6163, MinusLogProbMetric: 27.6163, val_loss: 28.3498, val_MinusLogProbMetric: 28.3498

Epoch 482: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6163 - MinusLogProbMetric: 27.6163 - val_loss: 28.3498 - val_MinusLogProbMetric: 28.3498 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 483/1000
2023-10-27 07:30:11.893 
Epoch 483/1000 
	 loss: 27.5894, MinusLogProbMetric: 27.5894, val_loss: 28.3977, val_MinusLogProbMetric: 28.3977

Epoch 483: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5894 - MinusLogProbMetric: 27.5894 - val_loss: 28.3977 - val_MinusLogProbMetric: 28.3977 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 484/1000
2023-10-27 07:30:53.858 
Epoch 484/1000 
	 loss: 27.5762, MinusLogProbMetric: 27.5762, val_loss: 28.7233, val_MinusLogProbMetric: 28.7233

Epoch 484: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5762 - MinusLogProbMetric: 27.5762 - val_loss: 28.7233 - val_MinusLogProbMetric: 28.7233 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 485/1000
2023-10-27 07:31:35.556 
Epoch 485/1000 
	 loss: 27.5879, MinusLogProbMetric: 27.5879, val_loss: 28.8125, val_MinusLogProbMetric: 28.8125

Epoch 485: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5879 - MinusLogProbMetric: 27.5879 - val_loss: 28.8125 - val_MinusLogProbMetric: 28.8125 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 486/1000
2023-10-27 07:32:17.606 
Epoch 486/1000 
	 loss: 27.6066, MinusLogProbMetric: 27.6066, val_loss: 28.4895, val_MinusLogProbMetric: 28.4895

Epoch 486: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6066 - MinusLogProbMetric: 27.6066 - val_loss: 28.4895 - val_MinusLogProbMetric: 28.4895 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 487/1000
2023-10-27 07:32:59.962 
Epoch 487/1000 
	 loss: 27.5930, MinusLogProbMetric: 27.5930, val_loss: 28.3344, val_MinusLogProbMetric: 28.3344

Epoch 487: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5930 - MinusLogProbMetric: 27.5930 - val_loss: 28.3344 - val_MinusLogProbMetric: 28.3344 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 488/1000
2023-10-27 07:33:42.034 
Epoch 488/1000 
	 loss: 27.5982, MinusLogProbMetric: 27.5982, val_loss: 28.4783, val_MinusLogProbMetric: 28.4783

Epoch 488: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5982 - MinusLogProbMetric: 27.5982 - val_loss: 28.4783 - val_MinusLogProbMetric: 28.4783 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 489/1000
2023-10-27 07:34:23.701 
Epoch 489/1000 
	 loss: 27.5899, MinusLogProbMetric: 27.5899, val_loss: 28.3133, val_MinusLogProbMetric: 28.3133

Epoch 489: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5899 - MinusLogProbMetric: 27.5899 - val_loss: 28.3133 - val_MinusLogProbMetric: 28.3133 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 490/1000
2023-10-27 07:35:05.684 
Epoch 490/1000 
	 loss: 27.5901, MinusLogProbMetric: 27.5901, val_loss: 28.3654, val_MinusLogProbMetric: 28.3654

Epoch 490: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5901 - MinusLogProbMetric: 27.5901 - val_loss: 28.3654 - val_MinusLogProbMetric: 28.3654 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 491/1000
2023-10-27 07:35:47.972 
Epoch 491/1000 
	 loss: 27.5828, MinusLogProbMetric: 27.5828, val_loss: 28.4392, val_MinusLogProbMetric: 28.4392

Epoch 491: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5828 - MinusLogProbMetric: 27.5828 - val_loss: 28.4392 - val_MinusLogProbMetric: 28.4392 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 492/1000
2023-10-27 07:36:29.792 
Epoch 492/1000 
	 loss: 27.6155, MinusLogProbMetric: 27.6155, val_loss: 28.5793, val_MinusLogProbMetric: 28.5793

Epoch 492: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6155 - MinusLogProbMetric: 27.6155 - val_loss: 28.5793 - val_MinusLogProbMetric: 28.5793 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 493/1000
2023-10-27 07:37:11.771 
Epoch 493/1000 
	 loss: 27.6067, MinusLogProbMetric: 27.6067, val_loss: 28.3987, val_MinusLogProbMetric: 28.3987

Epoch 493: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6067 - MinusLogProbMetric: 27.6067 - val_loss: 28.3987 - val_MinusLogProbMetric: 28.3987 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 494/1000
2023-10-27 07:37:53.641 
Epoch 494/1000 
	 loss: 27.5727, MinusLogProbMetric: 27.5727, val_loss: 28.3995, val_MinusLogProbMetric: 28.3995

Epoch 494: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5727 - MinusLogProbMetric: 27.5727 - val_loss: 28.3995 - val_MinusLogProbMetric: 28.3995 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 495/1000
2023-10-27 07:38:31.822 
Epoch 495/1000 
	 loss: 27.5877, MinusLogProbMetric: 27.5877, val_loss: 28.3969, val_MinusLogProbMetric: 28.3969

Epoch 495: val_loss did not improve from 28.30218
196/196 - 38s - loss: 27.5877 - MinusLogProbMetric: 27.5877 - val_loss: 28.3969 - val_MinusLogProbMetric: 28.3969 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 496/1000
2023-10-27 07:39:04.785 
Epoch 496/1000 
	 loss: 27.5671, MinusLogProbMetric: 27.5671, val_loss: 28.3690, val_MinusLogProbMetric: 28.3690

Epoch 496: val_loss did not improve from 28.30218
196/196 - 33s - loss: 27.5671 - MinusLogProbMetric: 27.5671 - val_loss: 28.3690 - val_MinusLogProbMetric: 28.3690 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 497/1000
2023-10-27 07:39:38.147 
Epoch 497/1000 
	 loss: 27.6253, MinusLogProbMetric: 27.6253, val_loss: 28.3426, val_MinusLogProbMetric: 28.3426

Epoch 497: val_loss did not improve from 28.30218
196/196 - 33s - loss: 27.6253 - MinusLogProbMetric: 27.6253 - val_loss: 28.3426 - val_MinusLogProbMetric: 28.3426 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 498/1000
2023-10-27 07:40:11.600 
Epoch 498/1000 
	 loss: 27.5986, MinusLogProbMetric: 27.5986, val_loss: 28.4188, val_MinusLogProbMetric: 28.4188

Epoch 498: val_loss did not improve from 28.30218
196/196 - 33s - loss: 27.5986 - MinusLogProbMetric: 27.5986 - val_loss: 28.4188 - val_MinusLogProbMetric: 28.4188 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 499/1000
2023-10-27 07:40:53.529 
Epoch 499/1000 
	 loss: 27.5804, MinusLogProbMetric: 27.5804, val_loss: 28.3323, val_MinusLogProbMetric: 28.3323

Epoch 499: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5804 - MinusLogProbMetric: 27.5804 - val_loss: 28.3323 - val_MinusLogProbMetric: 28.3323 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 500/1000
2023-10-27 07:41:35.663 
Epoch 500/1000 
	 loss: 27.6048, MinusLogProbMetric: 27.6048, val_loss: 28.3038, val_MinusLogProbMetric: 28.3038

Epoch 500: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6048 - MinusLogProbMetric: 27.6048 - val_loss: 28.3038 - val_MinusLogProbMetric: 28.3038 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 501/1000
2023-10-27 07:42:17.944 
Epoch 501/1000 
	 loss: 27.5931, MinusLogProbMetric: 27.5931, val_loss: 28.4096, val_MinusLogProbMetric: 28.4096

Epoch 501: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5931 - MinusLogProbMetric: 27.5931 - val_loss: 28.4096 - val_MinusLogProbMetric: 28.4096 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 502/1000
2023-10-27 07:42:59.585 
Epoch 502/1000 
	 loss: 27.5877, MinusLogProbMetric: 27.5877, val_loss: 28.3890, val_MinusLogProbMetric: 28.3890

Epoch 502: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5877 - MinusLogProbMetric: 27.5877 - val_loss: 28.3890 - val_MinusLogProbMetric: 28.3890 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 503/1000
2023-10-27 07:43:41.661 
Epoch 503/1000 
	 loss: 27.5814, MinusLogProbMetric: 27.5814, val_loss: 28.4474, val_MinusLogProbMetric: 28.4474

Epoch 503: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5814 - MinusLogProbMetric: 27.5814 - val_loss: 28.4474 - val_MinusLogProbMetric: 28.4474 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 504/1000
2023-10-27 07:44:23.499 
Epoch 504/1000 
	 loss: 27.5844, MinusLogProbMetric: 27.5844, val_loss: 28.3543, val_MinusLogProbMetric: 28.3543

Epoch 504: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5844 - MinusLogProbMetric: 27.5844 - val_loss: 28.3543 - val_MinusLogProbMetric: 28.3543 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 505/1000
2023-10-27 07:45:05.859 
Epoch 505/1000 
	 loss: 27.5665, MinusLogProbMetric: 27.5665, val_loss: 28.4306, val_MinusLogProbMetric: 28.4306

Epoch 505: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5665 - MinusLogProbMetric: 27.5665 - val_loss: 28.4306 - val_MinusLogProbMetric: 28.4306 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 506/1000
2023-10-27 07:45:47.718 
Epoch 506/1000 
	 loss: 27.5495, MinusLogProbMetric: 27.5495, val_loss: 28.3599, val_MinusLogProbMetric: 28.3599

Epoch 506: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5495 - MinusLogProbMetric: 27.5495 - val_loss: 28.3599 - val_MinusLogProbMetric: 28.3599 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 507/1000
2023-10-27 07:46:29.327 
Epoch 507/1000 
	 loss: 27.5700, MinusLogProbMetric: 27.5700, val_loss: 28.4200, val_MinusLogProbMetric: 28.4200

Epoch 507: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5700 - MinusLogProbMetric: 27.5700 - val_loss: 28.4200 - val_MinusLogProbMetric: 28.4200 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 508/1000
2023-10-27 07:47:10.639 
Epoch 508/1000 
	 loss: 27.6114, MinusLogProbMetric: 27.6114, val_loss: 28.3940, val_MinusLogProbMetric: 28.3940

Epoch 508: val_loss did not improve from 28.30218
196/196 - 41s - loss: 27.6114 - MinusLogProbMetric: 27.6114 - val_loss: 28.3940 - val_MinusLogProbMetric: 28.3940 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 509/1000
2023-10-27 07:47:52.688 
Epoch 509/1000 
	 loss: 27.5825, MinusLogProbMetric: 27.5825, val_loss: 28.3300, val_MinusLogProbMetric: 28.3300

Epoch 509: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5825 - MinusLogProbMetric: 27.5825 - val_loss: 28.3300 - val_MinusLogProbMetric: 28.3300 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 510/1000
2023-10-27 07:48:35.349 
Epoch 510/1000 
	 loss: 27.5685, MinusLogProbMetric: 27.5685, val_loss: 28.6091, val_MinusLogProbMetric: 28.6091

Epoch 510: val_loss did not improve from 28.30218
196/196 - 43s - loss: 27.5685 - MinusLogProbMetric: 27.5685 - val_loss: 28.6091 - val_MinusLogProbMetric: 28.6091 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 511/1000
2023-10-27 07:49:17.215 
Epoch 511/1000 
	 loss: 27.5588, MinusLogProbMetric: 27.5588, val_loss: 28.3679, val_MinusLogProbMetric: 28.3679

Epoch 511: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5588 - MinusLogProbMetric: 27.5588 - val_loss: 28.3679 - val_MinusLogProbMetric: 28.3679 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 512/1000
2023-10-27 07:49:59.352 
Epoch 512/1000 
	 loss: 27.5949, MinusLogProbMetric: 27.5949, val_loss: 28.3948, val_MinusLogProbMetric: 28.3948

Epoch 512: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5949 - MinusLogProbMetric: 27.5949 - val_loss: 28.3948 - val_MinusLogProbMetric: 28.3948 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 513/1000
2023-10-27 07:50:41.198 
Epoch 513/1000 
	 loss: 27.5576, MinusLogProbMetric: 27.5576, val_loss: 28.4689, val_MinusLogProbMetric: 28.4689

Epoch 513: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5576 - MinusLogProbMetric: 27.5576 - val_loss: 28.4689 - val_MinusLogProbMetric: 28.4689 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 514/1000
2023-10-27 07:51:23.174 
Epoch 514/1000 
	 loss: 27.6145, MinusLogProbMetric: 27.6145, val_loss: 28.3972, val_MinusLogProbMetric: 28.3972

Epoch 514: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.6145 - MinusLogProbMetric: 27.6145 - val_loss: 28.3972 - val_MinusLogProbMetric: 28.3972 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 515/1000
2023-10-27 07:52:05.486 
Epoch 515/1000 
	 loss: 27.5644, MinusLogProbMetric: 27.5644, val_loss: 28.3997, val_MinusLogProbMetric: 28.3997

Epoch 515: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5644 - MinusLogProbMetric: 27.5644 - val_loss: 28.3997 - val_MinusLogProbMetric: 28.3997 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 516/1000
2023-10-27 07:52:47.702 
Epoch 516/1000 
	 loss: 27.5938, MinusLogProbMetric: 27.5938, val_loss: 28.4289, val_MinusLogProbMetric: 28.4289

Epoch 516: val_loss did not improve from 28.30218
196/196 - 42s - loss: 27.5938 - MinusLogProbMetric: 27.5938 - val_loss: 28.4289 - val_MinusLogProbMetric: 28.4289 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 517/1000
2023-10-27 07:53:30.055 
Epoch 517/1000 
	 loss: 27.4011, MinusLogProbMetric: 27.4011, val_loss: 28.2703, val_MinusLogProbMetric: 28.2703

Epoch 517: val_loss improved from 28.30218 to 28.27034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 27.4011 - MinusLogProbMetric: 27.4011 - val_loss: 28.2703 - val_MinusLogProbMetric: 28.2703 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 518/1000
2023-10-27 07:54:13.149 
Epoch 518/1000 
	 loss: 27.3889, MinusLogProbMetric: 27.3889, val_loss: 28.2555, val_MinusLogProbMetric: 28.2555

Epoch 518: val_loss improved from 28.27034 to 28.25546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 27.3889 - MinusLogProbMetric: 27.3889 - val_loss: 28.2555 - val_MinusLogProbMetric: 28.2555 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 519/1000
2023-10-27 07:54:55.894 
Epoch 519/1000 
	 loss: 27.3866, MinusLogProbMetric: 27.3866, val_loss: 28.2340, val_MinusLogProbMetric: 28.2340

Epoch 519: val_loss improved from 28.25546 to 28.23403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 27.3866 - MinusLogProbMetric: 27.3866 - val_loss: 28.2340 - val_MinusLogProbMetric: 28.2340 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 520/1000
2023-10-27 07:55:38.288 
Epoch 520/1000 
	 loss: 27.3858, MinusLogProbMetric: 27.3858, val_loss: 28.2784, val_MinusLogProbMetric: 28.2784

Epoch 520: val_loss did not improve from 28.23403
196/196 - 42s - loss: 27.3858 - MinusLogProbMetric: 27.3858 - val_loss: 28.2784 - val_MinusLogProbMetric: 28.2784 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 521/1000
2023-10-27 07:56:16.202 
Epoch 521/1000 
	 loss: 27.3908, MinusLogProbMetric: 27.3908, val_loss: 28.3220, val_MinusLogProbMetric: 28.3220

Epoch 521: val_loss did not improve from 28.23403
196/196 - 38s - loss: 27.3908 - MinusLogProbMetric: 27.3908 - val_loss: 28.3220 - val_MinusLogProbMetric: 28.3220 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 522/1000
2023-10-27 07:56:50.771 
Epoch 522/1000 
	 loss: 27.3825, MinusLogProbMetric: 27.3825, val_loss: 28.2643, val_MinusLogProbMetric: 28.2643

Epoch 522: val_loss did not improve from 28.23403
196/196 - 35s - loss: 27.3825 - MinusLogProbMetric: 27.3825 - val_loss: 28.2643 - val_MinusLogProbMetric: 28.2643 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 523/1000
2023-10-27 07:57:24.098 
Epoch 523/1000 
	 loss: 27.3844, MinusLogProbMetric: 27.3844, val_loss: 28.3590, val_MinusLogProbMetric: 28.3590

Epoch 523: val_loss did not improve from 28.23403
196/196 - 33s - loss: 27.3844 - MinusLogProbMetric: 27.3844 - val_loss: 28.3590 - val_MinusLogProbMetric: 28.3590 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 524/1000
2023-10-27 07:58:02.409 
Epoch 524/1000 
	 loss: 27.3939, MinusLogProbMetric: 27.3939, val_loss: 28.2326, val_MinusLogProbMetric: 28.2326

Epoch 524: val_loss improved from 28.23403 to 28.23257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 39s - loss: 27.3939 - MinusLogProbMetric: 27.3939 - val_loss: 28.2326 - val_MinusLogProbMetric: 28.2326 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 525/1000
2023-10-27 07:58:43.495 
Epoch 525/1000 
	 loss: 27.3809, MinusLogProbMetric: 27.3809, val_loss: 28.2577, val_MinusLogProbMetric: 28.2577

Epoch 525: val_loss did not improve from 28.23257
196/196 - 40s - loss: 27.3809 - MinusLogProbMetric: 27.3809 - val_loss: 28.2577 - val_MinusLogProbMetric: 28.2577 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 526/1000
2023-10-27 07:59:16.859 
Epoch 526/1000 
	 loss: 27.3787, MinusLogProbMetric: 27.3787, val_loss: 28.2774, val_MinusLogProbMetric: 28.2774

Epoch 526: val_loss did not improve from 28.23257
196/196 - 33s - loss: 27.3787 - MinusLogProbMetric: 27.3787 - val_loss: 28.2774 - val_MinusLogProbMetric: 28.2774 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 527/1000
2023-10-27 07:59:51.648 
Epoch 527/1000 
	 loss: 27.3891, MinusLogProbMetric: 27.3891, val_loss: 28.2034, val_MinusLogProbMetric: 28.2034

Epoch 527: val_loss improved from 28.23257 to 28.20337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 35s - loss: 27.3891 - MinusLogProbMetric: 27.3891 - val_loss: 28.2034 - val_MinusLogProbMetric: 28.2034 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 528/1000
2023-10-27 08:00:30.637 
Epoch 528/1000 
	 loss: 27.4016, MinusLogProbMetric: 27.4016, val_loss: 28.2189, val_MinusLogProbMetric: 28.2189

Epoch 528: val_loss did not improve from 28.20337
196/196 - 38s - loss: 27.4016 - MinusLogProbMetric: 27.4016 - val_loss: 28.2189 - val_MinusLogProbMetric: 28.2189 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 529/1000
2023-10-27 08:01:06.137 
Epoch 529/1000 
	 loss: 27.3786, MinusLogProbMetric: 27.3786, val_loss: 28.2252, val_MinusLogProbMetric: 28.2252

Epoch 529: val_loss did not improve from 28.20337
196/196 - 35s - loss: 27.3786 - MinusLogProbMetric: 27.3786 - val_loss: 28.2252 - val_MinusLogProbMetric: 28.2252 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 530/1000
2023-10-27 08:01:39.039 
Epoch 530/1000 
	 loss: 27.3796, MinusLogProbMetric: 27.3796, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 530: val_loss did not improve from 28.20337
196/196 - 33s - loss: 27.3796 - MinusLogProbMetric: 27.3796 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 531/1000
2023-10-27 08:02:13.113 
Epoch 531/1000 
	 loss: 27.3733, MinusLogProbMetric: 27.3733, val_loss: 28.2462, val_MinusLogProbMetric: 28.2462

Epoch 531: val_loss did not improve from 28.20337
196/196 - 34s - loss: 27.3733 - MinusLogProbMetric: 27.3733 - val_loss: 28.2462 - val_MinusLogProbMetric: 28.2462 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 532/1000
2023-10-27 08:02:51.711 
Epoch 532/1000 
	 loss: 27.3716, MinusLogProbMetric: 27.3716, val_loss: 28.2158, val_MinusLogProbMetric: 28.2158

Epoch 532: val_loss did not improve from 28.20337
196/196 - 39s - loss: 27.3716 - MinusLogProbMetric: 27.3716 - val_loss: 28.2158 - val_MinusLogProbMetric: 28.2158 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 533/1000
2023-10-27 08:03:30.787 
Epoch 533/1000 
	 loss: 27.3659, MinusLogProbMetric: 27.3659, val_loss: 28.2296, val_MinusLogProbMetric: 28.2296

Epoch 533: val_loss did not improve from 28.20337
196/196 - 39s - loss: 27.3659 - MinusLogProbMetric: 27.3659 - val_loss: 28.2296 - val_MinusLogProbMetric: 28.2296 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 534/1000
2023-10-27 08:04:06.950 
Epoch 534/1000 
	 loss: 27.3744, MinusLogProbMetric: 27.3744, val_loss: 28.2377, val_MinusLogProbMetric: 28.2377

Epoch 534: val_loss did not improve from 28.20337
196/196 - 36s - loss: 27.3744 - MinusLogProbMetric: 27.3744 - val_loss: 28.2377 - val_MinusLogProbMetric: 28.2377 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 535/1000
2023-10-27 08:04:40.369 
Epoch 535/1000 
	 loss: 27.3725, MinusLogProbMetric: 27.3725, val_loss: 28.2089, val_MinusLogProbMetric: 28.2089

Epoch 535: val_loss did not improve from 28.20337
196/196 - 33s - loss: 27.3725 - MinusLogProbMetric: 27.3725 - val_loss: 28.2089 - val_MinusLogProbMetric: 28.2089 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 536/1000
2023-10-27 08:05:13.879 
Epoch 536/1000 
	 loss: 27.3652, MinusLogProbMetric: 27.3652, val_loss: 28.2084, val_MinusLogProbMetric: 28.2084

Epoch 536: val_loss did not improve from 28.20337
196/196 - 34s - loss: 27.3652 - MinusLogProbMetric: 27.3652 - val_loss: 28.2084 - val_MinusLogProbMetric: 28.2084 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 537/1000
2023-10-27 08:05:54.164 
Epoch 537/1000 
	 loss: 27.3762, MinusLogProbMetric: 27.3762, val_loss: 28.2259, val_MinusLogProbMetric: 28.2259

Epoch 537: val_loss did not improve from 28.20337
196/196 - 40s - loss: 27.3762 - MinusLogProbMetric: 27.3762 - val_loss: 28.2259 - val_MinusLogProbMetric: 28.2259 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 538/1000
2023-10-27 08:06:30.580 
Epoch 538/1000 
	 loss: 27.3815, MinusLogProbMetric: 27.3815, val_loss: 28.2322, val_MinusLogProbMetric: 28.2322

Epoch 538: val_loss did not improve from 28.20337
196/196 - 36s - loss: 27.3815 - MinusLogProbMetric: 27.3815 - val_loss: 28.2322 - val_MinusLogProbMetric: 28.2322 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 539/1000
2023-10-27 08:07:03.641 
Epoch 539/1000 
	 loss: 27.3765, MinusLogProbMetric: 27.3765, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 539: val_loss did not improve from 28.20337
196/196 - 33s - loss: 27.3765 - MinusLogProbMetric: 27.3765 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 540/1000
2023-10-27 08:07:38.617 
Epoch 540/1000 
	 loss: 27.3761, MinusLogProbMetric: 27.3761, val_loss: 28.2110, val_MinusLogProbMetric: 28.2110

Epoch 540: val_loss did not improve from 28.20337
196/196 - 35s - loss: 27.3761 - MinusLogProbMetric: 27.3761 - val_loss: 28.2110 - val_MinusLogProbMetric: 28.2110 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 541/1000
2023-10-27 08:08:21.444 
Epoch 541/1000 
	 loss: 27.3880, MinusLogProbMetric: 27.3880, val_loss: 28.2373, val_MinusLogProbMetric: 28.2373

Epoch 541: val_loss did not improve from 28.20337
196/196 - 43s - loss: 27.3880 - MinusLogProbMetric: 27.3880 - val_loss: 28.2373 - val_MinusLogProbMetric: 28.2373 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 542/1000
2023-10-27 08:08:58.037 
Epoch 542/1000 
	 loss: 27.3730, MinusLogProbMetric: 27.3730, val_loss: 28.1990, val_MinusLogProbMetric: 28.1990

Epoch 542: val_loss improved from 28.20337 to 28.19902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 37s - loss: 27.3730 - MinusLogProbMetric: 27.3730 - val_loss: 28.1990 - val_MinusLogProbMetric: 28.1990 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 543/1000
2023-10-27 08:09:37.520 
Epoch 543/1000 
	 loss: 27.3749, MinusLogProbMetric: 27.3749, val_loss: 28.2248, val_MinusLogProbMetric: 28.2248

Epoch 543: val_loss did not improve from 28.19902
196/196 - 39s - loss: 27.3749 - MinusLogProbMetric: 27.3749 - val_loss: 28.2248 - val_MinusLogProbMetric: 28.2248 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 544/1000
2023-10-27 08:10:11.991 
Epoch 544/1000 
	 loss: 27.3714, MinusLogProbMetric: 27.3714, val_loss: 28.2030, val_MinusLogProbMetric: 28.2030

Epoch 544: val_loss did not improve from 28.19902
196/196 - 34s - loss: 27.3714 - MinusLogProbMetric: 27.3714 - val_loss: 28.2030 - val_MinusLogProbMetric: 28.2030 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 545/1000
2023-10-27 08:10:51.714 
Epoch 545/1000 
	 loss: 27.3719, MinusLogProbMetric: 27.3719, val_loss: 28.2388, val_MinusLogProbMetric: 28.2388

Epoch 545: val_loss did not improve from 28.19902
196/196 - 40s - loss: 27.3719 - MinusLogProbMetric: 27.3719 - val_loss: 28.2388 - val_MinusLogProbMetric: 28.2388 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 546/1000
2023-10-27 08:11:30.187 
Epoch 546/1000 
	 loss: 27.3718, MinusLogProbMetric: 27.3718, val_loss: 28.2107, val_MinusLogProbMetric: 28.2107

Epoch 546: val_loss did not improve from 28.19902
196/196 - 38s - loss: 27.3718 - MinusLogProbMetric: 27.3718 - val_loss: 28.2107 - val_MinusLogProbMetric: 28.2107 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 547/1000
2023-10-27 08:12:07.144 
Epoch 547/1000 
	 loss: 27.3592, MinusLogProbMetric: 27.3592, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 547: val_loss did not improve from 28.19902
196/196 - 37s - loss: 27.3592 - MinusLogProbMetric: 27.3592 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 548/1000
2023-10-27 08:12:42.182 
Epoch 548/1000 
	 loss: 27.3694, MinusLogProbMetric: 27.3694, val_loss: 28.2635, val_MinusLogProbMetric: 28.2635

Epoch 548: val_loss did not improve from 28.19902
196/196 - 35s - loss: 27.3694 - MinusLogProbMetric: 27.3694 - val_loss: 28.2635 - val_MinusLogProbMetric: 28.2635 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 549/1000
2023-10-27 08:13:22.803 
Epoch 549/1000 
	 loss: 27.3697, MinusLogProbMetric: 27.3697, val_loss: 28.2372, val_MinusLogProbMetric: 28.2372

Epoch 549: val_loss did not improve from 28.19902
196/196 - 41s - loss: 27.3697 - MinusLogProbMetric: 27.3697 - val_loss: 28.2372 - val_MinusLogProbMetric: 28.2372 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 550/1000
2023-10-27 08:14:02.077 
Epoch 550/1000 
	 loss: 27.3670, MinusLogProbMetric: 27.3670, val_loss: 28.2456, val_MinusLogProbMetric: 28.2456

Epoch 550: val_loss did not improve from 28.19902
196/196 - 39s - loss: 27.3670 - MinusLogProbMetric: 27.3670 - val_loss: 28.2456 - val_MinusLogProbMetric: 28.2456 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 551/1000
2023-10-27 08:14:36.088 
Epoch 551/1000 
	 loss: 27.3670, MinusLogProbMetric: 27.3670, val_loss: 28.2283, val_MinusLogProbMetric: 28.2283

Epoch 551: val_loss did not improve from 28.19902
196/196 - 34s - loss: 27.3670 - MinusLogProbMetric: 27.3670 - val_loss: 28.2283 - val_MinusLogProbMetric: 28.2283 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 552/1000
2023-10-27 08:15:09.523 
Epoch 552/1000 
	 loss: 27.3618, MinusLogProbMetric: 27.3618, val_loss: 28.2339, val_MinusLogProbMetric: 28.2339

Epoch 552: val_loss did not improve from 28.19902
196/196 - 33s - loss: 27.3618 - MinusLogProbMetric: 27.3618 - val_loss: 28.2339 - val_MinusLogProbMetric: 28.2339 - lr: 2.5000e-04 - 33s/epoch - 171ms/step
Epoch 553/1000
2023-10-27 08:15:46.135 
Epoch 553/1000 
	 loss: 27.3692, MinusLogProbMetric: 27.3692, val_loss: 28.3477, val_MinusLogProbMetric: 28.3477

Epoch 553: val_loss did not improve from 28.19902
196/196 - 37s - loss: 27.3692 - MinusLogProbMetric: 27.3692 - val_loss: 28.3477 - val_MinusLogProbMetric: 28.3477 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 554/1000
2023-10-27 08:16:26.858 
Epoch 554/1000 
	 loss: 27.3708, MinusLogProbMetric: 27.3708, val_loss: 28.3358, val_MinusLogProbMetric: 28.3358

Epoch 554: val_loss did not improve from 28.19902
196/196 - 41s - loss: 27.3708 - MinusLogProbMetric: 27.3708 - val_loss: 28.3358 - val_MinusLogProbMetric: 28.3358 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 555/1000
2023-10-27 08:17:02.842 
Epoch 555/1000 
	 loss: 27.3651, MinusLogProbMetric: 27.3651, val_loss: 28.2820, val_MinusLogProbMetric: 28.2820

Epoch 555: val_loss did not improve from 28.19902
196/196 - 36s - loss: 27.3651 - MinusLogProbMetric: 27.3651 - val_loss: 28.2820 - val_MinusLogProbMetric: 28.2820 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 556/1000
2023-10-27 08:17:40.522 
Epoch 556/1000 
	 loss: 27.3751, MinusLogProbMetric: 27.3751, val_loss: 28.2517, val_MinusLogProbMetric: 28.2517

Epoch 556: val_loss did not improve from 28.19902
196/196 - 38s - loss: 27.3751 - MinusLogProbMetric: 27.3751 - val_loss: 28.2517 - val_MinusLogProbMetric: 28.2517 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 557/1000
2023-10-27 08:18:19.349 
Epoch 557/1000 
	 loss: 27.3731, MinusLogProbMetric: 27.3731, val_loss: 28.2555, val_MinusLogProbMetric: 28.2555

Epoch 557: val_loss did not improve from 28.19902
196/196 - 39s - loss: 27.3731 - MinusLogProbMetric: 27.3731 - val_loss: 28.2555 - val_MinusLogProbMetric: 28.2555 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 558/1000
2023-10-27 08:18:59.863 
Epoch 558/1000 
	 loss: 27.3801, MinusLogProbMetric: 27.3801, val_loss: 28.2867, val_MinusLogProbMetric: 28.2867

Epoch 558: val_loss did not improve from 28.19902
196/196 - 41s - loss: 27.3801 - MinusLogProbMetric: 27.3801 - val_loss: 28.2867 - val_MinusLogProbMetric: 28.2867 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 559/1000
2023-10-27 08:19:40.224 
Epoch 559/1000 
	 loss: 27.3760, MinusLogProbMetric: 27.3760, val_loss: 28.2610, val_MinusLogProbMetric: 28.2610

Epoch 559: val_loss did not improve from 28.19902
196/196 - 40s - loss: 27.3760 - MinusLogProbMetric: 27.3760 - val_loss: 28.2610 - val_MinusLogProbMetric: 28.2610 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 560/1000
2023-10-27 08:20:17.516 
Epoch 560/1000 
	 loss: 27.3654, MinusLogProbMetric: 27.3654, val_loss: 28.2393, val_MinusLogProbMetric: 28.2393

Epoch 560: val_loss did not improve from 28.19902
196/196 - 37s - loss: 27.3654 - MinusLogProbMetric: 27.3654 - val_loss: 28.2393 - val_MinusLogProbMetric: 28.2393 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 561/1000
2023-10-27 08:20:55.978 
Epoch 561/1000 
	 loss: 27.3679, MinusLogProbMetric: 27.3679, val_loss: 28.3083, val_MinusLogProbMetric: 28.3083

Epoch 561: val_loss did not improve from 28.19902
196/196 - 38s - loss: 27.3679 - MinusLogProbMetric: 27.3679 - val_loss: 28.3083 - val_MinusLogProbMetric: 28.3083 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 562/1000
2023-10-27 08:21:32.008 
Epoch 562/1000 
	 loss: 27.3670, MinusLogProbMetric: 27.3670, val_loss: 28.3055, val_MinusLogProbMetric: 28.3055

Epoch 562: val_loss did not improve from 28.19902
196/196 - 36s - loss: 27.3670 - MinusLogProbMetric: 27.3670 - val_loss: 28.3055 - val_MinusLogProbMetric: 28.3055 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 563/1000
2023-10-27 08:22:14.325 
Epoch 563/1000 
	 loss: 27.3516, MinusLogProbMetric: 27.3516, val_loss: 28.2125, val_MinusLogProbMetric: 28.2125

Epoch 563: val_loss did not improve from 28.19902
196/196 - 42s - loss: 27.3516 - MinusLogProbMetric: 27.3516 - val_loss: 28.2125 - val_MinusLogProbMetric: 28.2125 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 564/1000
2023-10-27 08:22:52.184 
Epoch 564/1000 
	 loss: 27.3689, MinusLogProbMetric: 27.3689, val_loss: 28.2044, val_MinusLogProbMetric: 28.2044

Epoch 564: val_loss did not improve from 28.19902
196/196 - 38s - loss: 27.3689 - MinusLogProbMetric: 27.3689 - val_loss: 28.2044 - val_MinusLogProbMetric: 28.2044 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 565/1000
2023-10-27 08:23:30.852 
Epoch 565/1000 
	 loss: 27.3760, MinusLogProbMetric: 27.3760, val_loss: 28.1966, val_MinusLogProbMetric: 28.1966

Epoch 565: val_loss improved from 28.19902 to 28.19663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 39s - loss: 27.3760 - MinusLogProbMetric: 27.3760 - val_loss: 28.1966 - val_MinusLogProbMetric: 28.1966 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 566/1000
2023-10-27 08:24:08.451 
Epoch 566/1000 
	 loss: 27.3597, MinusLogProbMetric: 27.3597, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 566: val_loss did not improve from 28.19663
196/196 - 37s - loss: 27.3597 - MinusLogProbMetric: 27.3597 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 567/1000
2023-10-27 08:24:47.746 
Epoch 567/1000 
	 loss: 27.3550, MinusLogProbMetric: 27.3550, val_loss: 28.2582, val_MinusLogProbMetric: 28.2582

Epoch 567: val_loss did not improve from 28.19663
196/196 - 39s - loss: 27.3550 - MinusLogProbMetric: 27.3550 - val_loss: 28.2582 - val_MinusLogProbMetric: 28.2582 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 568/1000
2023-10-27 08:25:28.950 
Epoch 568/1000 
	 loss: 27.3664, MinusLogProbMetric: 27.3664, val_loss: 28.2207, val_MinusLogProbMetric: 28.2207

Epoch 568: val_loss did not improve from 28.19663
196/196 - 41s - loss: 27.3664 - MinusLogProbMetric: 27.3664 - val_loss: 28.2207 - val_MinusLogProbMetric: 28.2207 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 569/1000
2023-10-27 08:26:05.001 
Epoch 569/1000 
	 loss: 27.3558, MinusLogProbMetric: 27.3558, val_loss: 28.2379, val_MinusLogProbMetric: 28.2379

Epoch 569: val_loss did not improve from 28.19663
196/196 - 36s - loss: 27.3558 - MinusLogProbMetric: 27.3558 - val_loss: 28.2379 - val_MinusLogProbMetric: 28.2379 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 570/1000
2023-10-27 08:26:43.708 
Epoch 570/1000 
	 loss: 27.3605, MinusLogProbMetric: 27.3605, val_loss: 28.2671, val_MinusLogProbMetric: 28.2671

Epoch 570: val_loss did not improve from 28.19663
196/196 - 39s - loss: 27.3605 - MinusLogProbMetric: 27.3605 - val_loss: 28.2671 - val_MinusLogProbMetric: 28.2671 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 571/1000
2023-10-27 08:27:22.510 
Epoch 571/1000 
	 loss: 27.3564, MinusLogProbMetric: 27.3564, val_loss: 28.2123, val_MinusLogProbMetric: 28.2123

Epoch 571: val_loss did not improve from 28.19663
196/196 - 39s - loss: 27.3564 - MinusLogProbMetric: 27.3564 - val_loss: 28.2123 - val_MinusLogProbMetric: 28.2123 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 572/1000
2023-10-27 08:28:04.357 
Epoch 572/1000 
	 loss: 27.3620, MinusLogProbMetric: 27.3620, val_loss: 28.2072, val_MinusLogProbMetric: 28.2072

Epoch 572: val_loss did not improve from 28.19663
196/196 - 42s - loss: 27.3620 - MinusLogProbMetric: 27.3620 - val_loss: 28.2072 - val_MinusLogProbMetric: 28.2072 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 573/1000
2023-10-27 08:28:44.756 
Epoch 573/1000 
	 loss: 27.3583, MinusLogProbMetric: 27.3583, val_loss: 28.2531, val_MinusLogProbMetric: 28.2531

Epoch 573: val_loss did not improve from 28.19663
196/196 - 40s - loss: 27.3583 - MinusLogProbMetric: 27.3583 - val_loss: 28.2531 - val_MinusLogProbMetric: 28.2531 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 574/1000
2023-10-27 08:29:20.040 
Epoch 574/1000 
	 loss: 27.3488, MinusLogProbMetric: 27.3488, val_loss: 28.2792, val_MinusLogProbMetric: 28.2792

Epoch 574: val_loss did not improve from 28.19663
196/196 - 35s - loss: 27.3488 - MinusLogProbMetric: 27.3488 - val_loss: 28.2792 - val_MinusLogProbMetric: 28.2792 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 575/1000
2023-10-27 08:29:59.991 
Epoch 575/1000 
	 loss: 27.3675, MinusLogProbMetric: 27.3675, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 575: val_loss did not improve from 28.19663
196/196 - 40s - loss: 27.3675 - MinusLogProbMetric: 27.3675 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 576/1000
2023-10-27 08:30:39.494 
Epoch 576/1000 
	 loss: 27.3641, MinusLogProbMetric: 27.3641, val_loss: 28.2304, val_MinusLogProbMetric: 28.2304

Epoch 576: val_loss did not improve from 28.19663
196/196 - 39s - loss: 27.3641 - MinusLogProbMetric: 27.3641 - val_loss: 28.2304 - val_MinusLogProbMetric: 28.2304 - lr: 2.5000e-04 - 39s/epoch - 202ms/step
Epoch 577/1000
2023-10-27 08:31:21.507 
Epoch 577/1000 
	 loss: 27.3506, MinusLogProbMetric: 27.3506, val_loss: 28.2401, val_MinusLogProbMetric: 28.2401

Epoch 577: val_loss did not improve from 28.19663
196/196 - 42s - loss: 27.3506 - MinusLogProbMetric: 27.3506 - val_loss: 28.2401 - val_MinusLogProbMetric: 28.2401 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 578/1000
2023-10-27 08:31:55.042 
Epoch 578/1000 
	 loss: 27.3529, MinusLogProbMetric: 27.3529, val_loss: 28.2090, val_MinusLogProbMetric: 28.2090

Epoch 578: val_loss did not improve from 28.19663
196/196 - 34s - loss: 27.3529 - MinusLogProbMetric: 27.3529 - val_loss: 28.2090 - val_MinusLogProbMetric: 28.2090 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 579/1000
2023-10-27 08:32:28.614 
Epoch 579/1000 
	 loss: 27.3608, MinusLogProbMetric: 27.3608, val_loss: 28.2419, val_MinusLogProbMetric: 28.2419

Epoch 579: val_loss did not improve from 28.19663
196/196 - 34s - loss: 27.3608 - MinusLogProbMetric: 27.3608 - val_loss: 28.2419 - val_MinusLogProbMetric: 28.2419 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 580/1000
2023-10-27 08:33:04.071 
Epoch 580/1000 
	 loss: 27.3468, MinusLogProbMetric: 27.3468, val_loss: 28.2614, val_MinusLogProbMetric: 28.2614

Epoch 580: val_loss did not improve from 28.19663
196/196 - 35s - loss: 27.3468 - MinusLogProbMetric: 27.3468 - val_loss: 28.2614 - val_MinusLogProbMetric: 28.2614 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 581/1000
2023-10-27 08:33:42.787 
Epoch 581/1000 
	 loss: 27.3637, MinusLogProbMetric: 27.3637, val_loss: 28.2465, val_MinusLogProbMetric: 28.2465

Epoch 581: val_loss did not improve from 28.19663
196/196 - 39s - loss: 27.3637 - MinusLogProbMetric: 27.3637 - val_loss: 28.2465 - val_MinusLogProbMetric: 28.2465 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 582/1000
2023-10-27 08:34:15.703 
Epoch 582/1000 
	 loss: 27.3526, MinusLogProbMetric: 27.3526, val_loss: 28.2174, val_MinusLogProbMetric: 28.2174

Epoch 582: val_loss did not improve from 28.19663
196/196 - 33s - loss: 27.3526 - MinusLogProbMetric: 27.3526 - val_loss: 28.2174 - val_MinusLogProbMetric: 28.2174 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 583/1000
2023-10-27 08:34:52.834 
Epoch 583/1000 
	 loss: 27.3540, MinusLogProbMetric: 27.3540, val_loss: 28.2335, val_MinusLogProbMetric: 28.2335

Epoch 583: val_loss did not improve from 28.19663
196/196 - 37s - loss: 27.3540 - MinusLogProbMetric: 27.3540 - val_loss: 28.2335 - val_MinusLogProbMetric: 28.2335 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 584/1000
2023-10-27 08:35:28.274 
Epoch 584/1000 
	 loss: 27.3599, MinusLogProbMetric: 27.3599, val_loss: 28.2754, val_MinusLogProbMetric: 28.2754

Epoch 584: val_loss did not improve from 28.19663
196/196 - 35s - loss: 27.3599 - MinusLogProbMetric: 27.3599 - val_loss: 28.2754 - val_MinusLogProbMetric: 28.2754 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 585/1000
2023-10-27 08:36:10.523 
Epoch 585/1000 
	 loss: 27.3624, MinusLogProbMetric: 27.3624, val_loss: 28.3009, val_MinusLogProbMetric: 28.3009

Epoch 585: val_loss did not improve from 28.19663
196/196 - 42s - loss: 27.3624 - MinusLogProbMetric: 27.3624 - val_loss: 28.3009 - val_MinusLogProbMetric: 28.3009 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 586/1000
2023-10-27 08:36:50.919 
Epoch 586/1000 
	 loss: 27.3621, MinusLogProbMetric: 27.3621, val_loss: 28.2235, val_MinusLogProbMetric: 28.2235

Epoch 586: val_loss did not improve from 28.19663
196/196 - 40s - loss: 27.3621 - MinusLogProbMetric: 27.3621 - val_loss: 28.2235 - val_MinusLogProbMetric: 28.2235 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 587/1000
2023-10-27 08:37:29.265 
Epoch 587/1000 
	 loss: 27.3597, MinusLogProbMetric: 27.3597, val_loss: 28.2463, val_MinusLogProbMetric: 28.2463

Epoch 587: val_loss did not improve from 28.19663
196/196 - 38s - loss: 27.3597 - MinusLogProbMetric: 27.3597 - val_loss: 28.2463 - val_MinusLogProbMetric: 28.2463 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 588/1000
2023-10-27 08:38:06.408 
Epoch 588/1000 
	 loss: 27.3561, MinusLogProbMetric: 27.3561, val_loss: 28.1913, val_MinusLogProbMetric: 28.1913

Epoch 588: val_loss improved from 28.19663 to 28.19127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 38s - loss: 27.3561 - MinusLogProbMetric: 27.3561 - val_loss: 28.1913 - val_MinusLogProbMetric: 28.1913 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 589/1000
2023-10-27 08:38:46.675 
Epoch 589/1000 
	 loss: 27.3366, MinusLogProbMetric: 27.3366, val_loss: 28.2448, val_MinusLogProbMetric: 28.2448

Epoch 589: val_loss did not improve from 28.19127
196/196 - 40s - loss: 27.3366 - MinusLogProbMetric: 27.3366 - val_loss: 28.2448 - val_MinusLogProbMetric: 28.2448 - lr: 2.5000e-04 - 40s/epoch - 202ms/step
Epoch 590/1000
2023-10-27 08:39:29.194 
Epoch 590/1000 
	 loss: 27.3498, MinusLogProbMetric: 27.3498, val_loss: 28.2193, val_MinusLogProbMetric: 28.2193

Epoch 590: val_loss did not improve from 28.19127
196/196 - 43s - loss: 27.3498 - MinusLogProbMetric: 27.3498 - val_loss: 28.2193 - val_MinusLogProbMetric: 28.2193 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 591/1000
2023-10-27 08:40:09.035 
Epoch 591/1000 
	 loss: 27.3547, MinusLogProbMetric: 27.3547, val_loss: 28.3621, val_MinusLogProbMetric: 28.3621

Epoch 591: val_loss did not improve from 28.19127
196/196 - 40s - loss: 27.3547 - MinusLogProbMetric: 27.3547 - val_loss: 28.3621 - val_MinusLogProbMetric: 28.3621 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 592/1000
2023-10-27 08:40:43.791 
Epoch 592/1000 
	 loss: 27.3555, MinusLogProbMetric: 27.3555, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 592: val_loss did not improve from 28.19127
196/196 - 35s - loss: 27.3555 - MinusLogProbMetric: 27.3555 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 593/1000
2023-10-27 08:41:19.118 
Epoch 593/1000 
	 loss: 27.3461, MinusLogProbMetric: 27.3461, val_loss: 28.2734, val_MinusLogProbMetric: 28.2734

Epoch 593: val_loss did not improve from 28.19127
196/196 - 35s - loss: 27.3461 - MinusLogProbMetric: 27.3461 - val_loss: 28.2734 - val_MinusLogProbMetric: 28.2734 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 594/1000
2023-10-27 08:41:59.671 
Epoch 594/1000 
	 loss: 27.3416, MinusLogProbMetric: 27.3416, val_loss: 28.2359, val_MinusLogProbMetric: 28.2359

Epoch 594: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3416 - MinusLogProbMetric: 27.3416 - val_loss: 28.2359 - val_MinusLogProbMetric: 28.2359 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 595/1000
2023-10-27 08:42:36.639 
Epoch 595/1000 
	 loss: 27.3509, MinusLogProbMetric: 27.3509, val_loss: 28.2242, val_MinusLogProbMetric: 28.2242

Epoch 595: val_loss did not improve from 28.19127
196/196 - 37s - loss: 27.3509 - MinusLogProbMetric: 27.3509 - val_loss: 28.2242 - val_MinusLogProbMetric: 28.2242 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 596/1000
2023-10-27 08:43:13.845 
Epoch 596/1000 
	 loss: 27.3429, MinusLogProbMetric: 27.3429, val_loss: 28.2993, val_MinusLogProbMetric: 28.2993

Epoch 596: val_loss did not improve from 28.19127
196/196 - 37s - loss: 27.3429 - MinusLogProbMetric: 27.3429 - val_loss: 28.2993 - val_MinusLogProbMetric: 28.2993 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 597/1000
2023-10-27 08:43:49.539 
Epoch 597/1000 
	 loss: 27.3595, MinusLogProbMetric: 27.3595, val_loss: 28.2561, val_MinusLogProbMetric: 28.2561

Epoch 597: val_loss did not improve from 28.19127
196/196 - 36s - loss: 27.3595 - MinusLogProbMetric: 27.3595 - val_loss: 28.2561 - val_MinusLogProbMetric: 28.2561 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 598/1000
2023-10-27 08:44:29.754 
Epoch 598/1000 
	 loss: 27.3477, MinusLogProbMetric: 27.3477, val_loss: 28.2176, val_MinusLogProbMetric: 28.2176

Epoch 598: val_loss did not improve from 28.19127
196/196 - 40s - loss: 27.3477 - MinusLogProbMetric: 27.3477 - val_loss: 28.2176 - val_MinusLogProbMetric: 28.2176 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 599/1000
2023-10-27 08:45:08.724 
Epoch 599/1000 
	 loss: 27.3367, MinusLogProbMetric: 27.3367, val_loss: 28.2292, val_MinusLogProbMetric: 28.2292

Epoch 599: val_loss did not improve from 28.19127
196/196 - 39s - loss: 27.3367 - MinusLogProbMetric: 27.3367 - val_loss: 28.2292 - val_MinusLogProbMetric: 28.2292 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 600/1000
2023-10-27 08:45:47.575 
Epoch 600/1000 
	 loss: 27.3514, MinusLogProbMetric: 27.3514, val_loss: 28.2462, val_MinusLogProbMetric: 28.2462

Epoch 600: val_loss did not improve from 28.19127
196/196 - 39s - loss: 27.3514 - MinusLogProbMetric: 27.3514 - val_loss: 28.2462 - val_MinusLogProbMetric: 28.2462 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 601/1000
2023-10-27 08:46:23.543 
Epoch 601/1000 
	 loss: 27.3394, MinusLogProbMetric: 27.3394, val_loss: 28.2414, val_MinusLogProbMetric: 28.2414

Epoch 601: val_loss did not improve from 28.19127
196/196 - 36s - loss: 27.3394 - MinusLogProbMetric: 27.3394 - val_loss: 28.2414 - val_MinusLogProbMetric: 28.2414 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 602/1000
2023-10-27 08:47:02.276 
Epoch 602/1000 
	 loss: 27.3476, MinusLogProbMetric: 27.3476, val_loss: 28.3355, val_MinusLogProbMetric: 28.3355

Epoch 602: val_loss did not improve from 28.19127
196/196 - 39s - loss: 27.3476 - MinusLogProbMetric: 27.3476 - val_loss: 28.3355 - val_MinusLogProbMetric: 28.3355 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 603/1000
2023-10-27 08:47:42.747 
Epoch 603/1000 
	 loss: 27.3534, MinusLogProbMetric: 27.3534, val_loss: 28.3219, val_MinusLogProbMetric: 28.3219

Epoch 603: val_loss did not improve from 28.19127
196/196 - 40s - loss: 27.3534 - MinusLogProbMetric: 27.3534 - val_loss: 28.3219 - val_MinusLogProbMetric: 28.3219 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 604/1000
2023-10-27 08:48:24.314 
Epoch 604/1000 
	 loss: 27.3432, MinusLogProbMetric: 27.3432, val_loss: 28.3006, val_MinusLogProbMetric: 28.3006

Epoch 604: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3432 - MinusLogProbMetric: 27.3432 - val_loss: 28.3006 - val_MinusLogProbMetric: 28.3006 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 605/1000
2023-10-27 08:48:58.262 
Epoch 605/1000 
	 loss: 27.3451, MinusLogProbMetric: 27.3451, val_loss: 28.2745, val_MinusLogProbMetric: 28.2745

Epoch 605: val_loss did not improve from 28.19127
196/196 - 34s - loss: 27.3451 - MinusLogProbMetric: 27.3451 - val_loss: 28.2745 - val_MinusLogProbMetric: 28.2745 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 606/1000
2023-10-27 08:49:32.458 
Epoch 606/1000 
	 loss: 27.3468, MinusLogProbMetric: 27.3468, val_loss: 28.2457, val_MinusLogProbMetric: 28.2457

Epoch 606: val_loss did not improve from 28.19127
196/196 - 34s - loss: 27.3468 - MinusLogProbMetric: 27.3468 - val_loss: 28.2457 - val_MinusLogProbMetric: 28.2457 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 607/1000
2023-10-27 08:50:10.956 
Epoch 607/1000 
	 loss: 27.3410, MinusLogProbMetric: 27.3410, val_loss: 28.3171, val_MinusLogProbMetric: 28.3171

Epoch 607: val_loss did not improve from 28.19127
196/196 - 38s - loss: 27.3410 - MinusLogProbMetric: 27.3410 - val_loss: 28.3171 - val_MinusLogProbMetric: 28.3171 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 608/1000
2023-10-27 08:50:51.602 
Epoch 608/1000 
	 loss: 27.3349, MinusLogProbMetric: 27.3349, val_loss: 28.2365, val_MinusLogProbMetric: 28.2365

Epoch 608: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3349 - MinusLogProbMetric: 27.3349 - val_loss: 28.2365 - val_MinusLogProbMetric: 28.2365 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 609/1000
2023-10-27 08:51:33.528 
Epoch 609/1000 
	 loss: 27.3463, MinusLogProbMetric: 27.3463, val_loss: 28.2390, val_MinusLogProbMetric: 28.2390

Epoch 609: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3463 - MinusLogProbMetric: 27.3463 - val_loss: 28.2390 - val_MinusLogProbMetric: 28.2390 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 610/1000
2023-10-27 08:52:15.640 
Epoch 610/1000 
	 loss: 27.3287, MinusLogProbMetric: 27.3287, val_loss: 28.2772, val_MinusLogProbMetric: 28.2772

Epoch 610: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3287 - MinusLogProbMetric: 27.3287 - val_loss: 28.2772 - val_MinusLogProbMetric: 28.2772 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 611/1000
2023-10-27 08:52:56.885 
Epoch 611/1000 
	 loss: 27.3402, MinusLogProbMetric: 27.3402, val_loss: 28.2301, val_MinusLogProbMetric: 28.2301

Epoch 611: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3402 - MinusLogProbMetric: 27.3402 - val_loss: 28.2301 - val_MinusLogProbMetric: 28.2301 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 612/1000
2023-10-27 08:53:38.704 
Epoch 612/1000 
	 loss: 27.3443, MinusLogProbMetric: 27.3443, val_loss: 28.2424, val_MinusLogProbMetric: 28.2424

Epoch 612: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3443 - MinusLogProbMetric: 27.3443 - val_loss: 28.2424 - val_MinusLogProbMetric: 28.2424 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 613/1000
2023-10-27 08:54:20.550 
Epoch 613/1000 
	 loss: 27.3478, MinusLogProbMetric: 27.3478, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 613: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3478 - MinusLogProbMetric: 27.3478 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 614/1000
2023-10-27 08:55:02.105 
Epoch 614/1000 
	 loss: 27.3390, MinusLogProbMetric: 27.3390, val_loss: 28.2826, val_MinusLogProbMetric: 28.2826

Epoch 614: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3390 - MinusLogProbMetric: 27.3390 - val_loss: 28.2826 - val_MinusLogProbMetric: 28.2826 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 615/1000
2023-10-27 08:55:42.440 
Epoch 615/1000 
	 loss: 27.3394, MinusLogProbMetric: 27.3394, val_loss: 28.2357, val_MinusLogProbMetric: 28.2357

Epoch 615: val_loss did not improve from 28.19127
196/196 - 40s - loss: 27.3394 - MinusLogProbMetric: 27.3394 - val_loss: 28.2357 - val_MinusLogProbMetric: 28.2357 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 616/1000
2023-10-27 08:56:24.417 
Epoch 616/1000 
	 loss: 27.3507, MinusLogProbMetric: 27.3507, val_loss: 28.3326, val_MinusLogProbMetric: 28.3326

Epoch 616: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3507 - MinusLogProbMetric: 27.3507 - val_loss: 28.3326 - val_MinusLogProbMetric: 28.3326 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 617/1000
2023-10-27 08:57:05.236 
Epoch 617/1000 
	 loss: 27.3332, MinusLogProbMetric: 27.3332, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 617: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3332 - MinusLogProbMetric: 27.3332 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 618/1000
2023-10-27 08:57:47.303 
Epoch 618/1000 
	 loss: 27.3474, MinusLogProbMetric: 27.3474, val_loss: 28.2183, val_MinusLogProbMetric: 28.2183

Epoch 618: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3474 - MinusLogProbMetric: 27.3474 - val_loss: 28.2183 - val_MinusLogProbMetric: 28.2183 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 619/1000
2023-10-27 08:58:27.402 
Epoch 619/1000 
	 loss: 27.3381, MinusLogProbMetric: 27.3381, val_loss: 28.2377, val_MinusLogProbMetric: 28.2377

Epoch 619: val_loss did not improve from 28.19127
196/196 - 40s - loss: 27.3381 - MinusLogProbMetric: 27.3381 - val_loss: 28.2377 - val_MinusLogProbMetric: 28.2377 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 620/1000
2023-10-27 08:59:07.504 
Epoch 620/1000 
	 loss: 27.3362, MinusLogProbMetric: 27.3362, val_loss: 28.3361, val_MinusLogProbMetric: 28.3361

Epoch 620: val_loss did not improve from 28.19127
196/196 - 40s - loss: 27.3362 - MinusLogProbMetric: 27.3362 - val_loss: 28.3361 - val_MinusLogProbMetric: 28.3361 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 621/1000
2023-10-27 08:59:49.367 
Epoch 621/1000 
	 loss: 27.3461, MinusLogProbMetric: 27.3461, val_loss: 28.2087, val_MinusLogProbMetric: 28.2087

Epoch 621: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3461 - MinusLogProbMetric: 27.3461 - val_loss: 28.2087 - val_MinusLogProbMetric: 28.2087 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 622/1000
2023-10-27 09:00:30.956 
Epoch 622/1000 
	 loss: 27.3399, MinusLogProbMetric: 27.3399, val_loss: 28.3093, val_MinusLogProbMetric: 28.3093

Epoch 622: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3399 - MinusLogProbMetric: 27.3399 - val_loss: 28.3093 - val_MinusLogProbMetric: 28.3093 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 623/1000
2023-10-27 09:01:11.641 
Epoch 623/1000 
	 loss: 27.3520, MinusLogProbMetric: 27.3520, val_loss: 28.3141, val_MinusLogProbMetric: 28.3141

Epoch 623: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3520 - MinusLogProbMetric: 27.3520 - val_loss: 28.3141 - val_MinusLogProbMetric: 28.3141 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 624/1000
2023-10-27 09:01:52.580 
Epoch 624/1000 
	 loss: 27.3422, MinusLogProbMetric: 27.3422, val_loss: 28.2247, val_MinusLogProbMetric: 28.2247

Epoch 624: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3422 - MinusLogProbMetric: 27.3422 - val_loss: 28.2247 - val_MinusLogProbMetric: 28.2247 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 625/1000
2023-10-27 09:02:34.784 
Epoch 625/1000 
	 loss: 27.3299, MinusLogProbMetric: 27.3299, val_loss: 28.2192, val_MinusLogProbMetric: 28.2192

Epoch 625: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3299 - MinusLogProbMetric: 27.3299 - val_loss: 28.2192 - val_MinusLogProbMetric: 28.2192 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 626/1000
2023-10-27 09:03:16.892 
Epoch 626/1000 
	 loss: 27.3393, MinusLogProbMetric: 27.3393, val_loss: 28.2763, val_MinusLogProbMetric: 28.2763

Epoch 626: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3393 - MinusLogProbMetric: 27.3393 - val_loss: 28.2763 - val_MinusLogProbMetric: 28.2763 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 627/1000
2023-10-27 09:03:57.562 
Epoch 627/1000 
	 loss: 27.3341, MinusLogProbMetric: 27.3341, val_loss: 28.3137, val_MinusLogProbMetric: 28.3137

Epoch 627: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3341 - MinusLogProbMetric: 27.3341 - val_loss: 28.3137 - val_MinusLogProbMetric: 28.3137 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 628/1000
2023-10-27 09:04:38.809 
Epoch 628/1000 
	 loss: 27.3459, MinusLogProbMetric: 27.3459, val_loss: 28.2295, val_MinusLogProbMetric: 28.2295

Epoch 628: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3459 - MinusLogProbMetric: 27.3459 - val_loss: 28.2295 - val_MinusLogProbMetric: 28.2295 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 629/1000
2023-10-27 09:05:20.636 
Epoch 629/1000 
	 loss: 27.3305, MinusLogProbMetric: 27.3305, val_loss: 28.2957, val_MinusLogProbMetric: 28.2957

Epoch 629: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3305 - MinusLogProbMetric: 27.3305 - val_loss: 28.2957 - val_MinusLogProbMetric: 28.2957 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 630/1000
2023-10-27 09:06:02.233 
Epoch 630/1000 
	 loss: 27.3331, MinusLogProbMetric: 27.3331, val_loss: 28.2675, val_MinusLogProbMetric: 28.2675

Epoch 630: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3331 - MinusLogProbMetric: 27.3331 - val_loss: 28.2675 - val_MinusLogProbMetric: 28.2675 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 631/1000
2023-10-27 09:06:43.110 
Epoch 631/1000 
	 loss: 27.3312, MinusLogProbMetric: 27.3312, val_loss: 28.2582, val_MinusLogProbMetric: 28.2582

Epoch 631: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3312 - MinusLogProbMetric: 27.3312 - val_loss: 28.2582 - val_MinusLogProbMetric: 28.2582 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 632/1000
2023-10-27 09:07:24.701 
Epoch 632/1000 
	 loss: 27.3386, MinusLogProbMetric: 27.3386, val_loss: 28.2263, val_MinusLogProbMetric: 28.2263

Epoch 632: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3386 - MinusLogProbMetric: 27.3386 - val_loss: 28.2263 - val_MinusLogProbMetric: 28.2263 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 633/1000
2023-10-27 09:08:07.396 
Epoch 633/1000 
	 loss: 27.3410, MinusLogProbMetric: 27.3410, val_loss: 28.2801, val_MinusLogProbMetric: 28.2801

Epoch 633: val_loss did not improve from 28.19127
196/196 - 43s - loss: 27.3410 - MinusLogProbMetric: 27.3410 - val_loss: 28.2801 - val_MinusLogProbMetric: 28.2801 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 634/1000
2023-10-27 09:08:49.210 
Epoch 634/1000 
	 loss: 27.3215, MinusLogProbMetric: 27.3215, val_loss: 28.2600, val_MinusLogProbMetric: 28.2600

Epoch 634: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.3215 - MinusLogProbMetric: 27.3215 - val_loss: 28.2600 - val_MinusLogProbMetric: 28.2600 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 635/1000
2023-10-27 09:09:30.233 
Epoch 635/1000 
	 loss: 27.3353, MinusLogProbMetric: 27.3353, val_loss: 28.2097, val_MinusLogProbMetric: 28.2097

Epoch 635: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3353 - MinusLogProbMetric: 27.3353 - val_loss: 28.2097 - val_MinusLogProbMetric: 28.2097 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 636/1000
2023-10-27 09:10:10.924 
Epoch 636/1000 
	 loss: 27.3261, MinusLogProbMetric: 27.3261, val_loss: 28.2240, val_MinusLogProbMetric: 28.2240

Epoch 636: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3261 - MinusLogProbMetric: 27.3261 - val_loss: 28.2240 - val_MinusLogProbMetric: 28.2240 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 637/1000
2023-10-27 09:10:51.443 
Epoch 637/1000 
	 loss: 27.3265, MinusLogProbMetric: 27.3265, val_loss: 28.2505, val_MinusLogProbMetric: 28.2505

Epoch 637: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3265 - MinusLogProbMetric: 27.3265 - val_loss: 28.2505 - val_MinusLogProbMetric: 28.2505 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 638/1000
2023-10-27 09:11:32.604 
Epoch 638/1000 
	 loss: 27.3223, MinusLogProbMetric: 27.3223, val_loss: 28.2223, val_MinusLogProbMetric: 28.2223

Epoch 638: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.3223 - MinusLogProbMetric: 27.3223 - val_loss: 28.2223 - val_MinusLogProbMetric: 28.2223 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 639/1000
2023-10-27 09:12:13.752 
Epoch 639/1000 
	 loss: 27.2615, MinusLogProbMetric: 27.2615, val_loss: 28.1934, val_MinusLogProbMetric: 28.1934

Epoch 639: val_loss did not improve from 28.19127
196/196 - 41s - loss: 27.2615 - MinusLogProbMetric: 27.2615 - val_loss: 28.1934 - val_MinusLogProbMetric: 28.1934 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 640/1000
2023-10-27 09:12:55.859 
Epoch 640/1000 
	 loss: 27.2554, MinusLogProbMetric: 27.2554, val_loss: 28.1993, val_MinusLogProbMetric: 28.1993

Epoch 640: val_loss did not improve from 28.19127
196/196 - 42s - loss: 27.2554 - MinusLogProbMetric: 27.2554 - val_loss: 28.1993 - val_MinusLogProbMetric: 28.1993 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 641/1000
2023-10-27 09:13:37.609 
Epoch 641/1000 
	 loss: 27.2492, MinusLogProbMetric: 27.2492, val_loss: 28.1764, val_MinusLogProbMetric: 28.1764

Epoch 641: val_loss improved from 28.19127 to 28.17644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 42s - loss: 27.2492 - MinusLogProbMetric: 27.2492 - val_loss: 28.1764 - val_MinusLogProbMetric: 28.1764 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 642/1000
2023-10-27 09:14:17.463 
Epoch 642/1000 
	 loss: 27.2560, MinusLogProbMetric: 27.2560, val_loss: 28.1854, val_MinusLogProbMetric: 28.1854

Epoch 642: val_loss did not improve from 28.17644
196/196 - 39s - loss: 27.2560 - MinusLogProbMetric: 27.2560 - val_loss: 28.1854 - val_MinusLogProbMetric: 28.1854 - lr: 1.2500e-04 - 39s/epoch - 200ms/step
Epoch 643/1000
2023-10-27 09:14:57.690 
Epoch 643/1000 
	 loss: 27.2511, MinusLogProbMetric: 27.2511, val_loss: 28.2135, val_MinusLogProbMetric: 28.2135

Epoch 643: val_loss did not improve from 28.17644
196/196 - 40s - loss: 27.2511 - MinusLogProbMetric: 27.2511 - val_loss: 28.2135 - val_MinusLogProbMetric: 28.2135 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 644/1000
2023-10-27 09:15:38.681 
Epoch 644/1000 
	 loss: 27.2554, MinusLogProbMetric: 27.2554, val_loss: 28.1948, val_MinusLogProbMetric: 28.1948

Epoch 644: val_loss did not improve from 28.17644
196/196 - 41s - loss: 27.2554 - MinusLogProbMetric: 27.2554 - val_loss: 28.1948 - val_MinusLogProbMetric: 28.1948 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 645/1000
2023-10-27 09:16:19.499 
Epoch 645/1000 
	 loss: 27.2493, MinusLogProbMetric: 27.2493, val_loss: 28.2004, val_MinusLogProbMetric: 28.2004

Epoch 645: val_loss did not improve from 28.17644
196/196 - 41s - loss: 27.2493 - MinusLogProbMetric: 27.2493 - val_loss: 28.2004 - val_MinusLogProbMetric: 28.2004 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 646/1000
2023-10-27 09:17:00.445 
Epoch 646/1000 
	 loss: 27.2512, MinusLogProbMetric: 27.2512, val_loss: 28.1977, val_MinusLogProbMetric: 28.1977

Epoch 646: val_loss did not improve from 28.17644
196/196 - 41s - loss: 27.2512 - MinusLogProbMetric: 27.2512 - val_loss: 28.1977 - val_MinusLogProbMetric: 28.1977 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 647/1000
2023-10-27 09:17:41.193 
Epoch 647/1000 
	 loss: 27.2512, MinusLogProbMetric: 27.2512, val_loss: 28.2145, val_MinusLogProbMetric: 28.2145

Epoch 647: val_loss did not improve from 28.17644
196/196 - 41s - loss: 27.2512 - MinusLogProbMetric: 27.2512 - val_loss: 28.2145 - val_MinusLogProbMetric: 28.2145 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 648/1000
2023-10-27 09:18:22.035 
Epoch 648/1000 
	 loss: 27.2488, MinusLogProbMetric: 27.2488, val_loss: 28.2017, val_MinusLogProbMetric: 28.2017

Epoch 648: val_loss did not improve from 28.17644
196/196 - 41s - loss: 27.2488 - MinusLogProbMetric: 27.2488 - val_loss: 28.2017 - val_MinusLogProbMetric: 28.2017 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 649/1000
2023-10-27 09:19:01.371 
Epoch 649/1000 
	 loss: 27.2497, MinusLogProbMetric: 27.2497, val_loss: 28.1920, val_MinusLogProbMetric: 28.1920

Epoch 649: val_loss did not improve from 28.17644
196/196 - 39s - loss: 27.2497 - MinusLogProbMetric: 27.2497 - val_loss: 28.1920 - val_MinusLogProbMetric: 28.1920 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 650/1000
2023-10-27 09:19:41.400 
Epoch 650/1000 
	 loss: 27.2496, MinusLogProbMetric: 27.2496, val_loss: 28.1986, val_MinusLogProbMetric: 28.1986

Epoch 650: val_loss did not improve from 28.17644
196/196 - 40s - loss: 27.2496 - MinusLogProbMetric: 27.2496 - val_loss: 28.1986 - val_MinusLogProbMetric: 28.1986 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 651/1000
2023-10-27 09:20:23.230 
Epoch 651/1000 
	 loss: 27.2511, MinusLogProbMetric: 27.2511, val_loss: 28.2248, val_MinusLogProbMetric: 28.2248

Epoch 651: val_loss did not improve from 28.17644
196/196 - 42s - loss: 27.2511 - MinusLogProbMetric: 27.2511 - val_loss: 28.2248 - val_MinusLogProbMetric: 28.2248 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 652/1000
2023-10-27 09:21:03.513 
Epoch 652/1000 
	 loss: 27.2543, MinusLogProbMetric: 27.2543, val_loss: 28.1896, val_MinusLogProbMetric: 28.1896

Epoch 652: val_loss did not improve from 28.17644
196/196 - 40s - loss: 27.2543 - MinusLogProbMetric: 27.2543 - val_loss: 28.1896 - val_MinusLogProbMetric: 28.1896 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 653/1000
2023-10-27 09:21:44.731 
Epoch 653/1000 
	 loss: 27.2477, MinusLogProbMetric: 27.2477, val_loss: 28.1975, val_MinusLogProbMetric: 28.1975

Epoch 653: val_loss did not improve from 28.17644
196/196 - 41s - loss: 27.2477 - MinusLogProbMetric: 27.2477 - val_loss: 28.1975 - val_MinusLogProbMetric: 28.1975 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 654/1000
2023-10-27 09:22:24.246 
Epoch 654/1000 
	 loss: 27.2541, MinusLogProbMetric: 27.2541, val_loss: 28.2142, val_MinusLogProbMetric: 28.2142

Epoch 654: val_loss did not improve from 28.17644
196/196 - 40s - loss: 27.2541 - MinusLogProbMetric: 27.2541 - val_loss: 28.2142 - val_MinusLogProbMetric: 28.2142 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 655/1000
2023-10-27 09:23:04.729 
Epoch 655/1000 
	 loss: 27.2495, MinusLogProbMetric: 27.2495, val_loss: 28.2116, val_MinusLogProbMetric: 28.2116

Epoch 655: val_loss did not improve from 28.17644
196/196 - 40s - loss: 27.2495 - MinusLogProbMetric: 27.2495 - val_loss: 28.2116 - val_MinusLogProbMetric: 28.2116 - lr: 1.2500e-04 - 40s/epoch - 207ms/step
Epoch 656/1000
2023-10-27 09:23:45.933 
Epoch 656/1000 
	 loss: 27.2507, MinusLogProbMetric: 27.2507, val_loss: 28.2411, val_MinusLogProbMetric: 28.2411

Epoch 656: val_loss did not improve from 28.17644
196/196 - 41s - loss: 27.2507 - MinusLogProbMetric: 27.2507 - val_loss: 28.2411 - val_MinusLogProbMetric: 28.2411 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 657/1000
2023-10-27 09:24:24.579 
Epoch 657/1000 
	 loss: 27.2544, MinusLogProbMetric: 27.2544, val_loss: 28.1773, val_MinusLogProbMetric: 28.1773

Epoch 657: val_loss did not improve from 28.17644
196/196 - 39s - loss: 27.2544 - MinusLogProbMetric: 27.2544 - val_loss: 28.1773 - val_MinusLogProbMetric: 28.1773 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 658/1000
2023-10-27 09:25:03.437 
Epoch 658/1000 
	 loss: 27.2463, MinusLogProbMetric: 27.2463, val_loss: 28.2014, val_MinusLogProbMetric: 28.2014

Epoch 658: val_loss did not improve from 28.17644
196/196 - 39s - loss: 27.2463 - MinusLogProbMetric: 27.2463 - val_loss: 28.2014 - val_MinusLogProbMetric: 28.2014 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 659/1000
2023-10-27 09:25:43.502 
Epoch 659/1000 
	 loss: 27.2512, MinusLogProbMetric: 27.2512, val_loss: 28.2058, val_MinusLogProbMetric: 28.2058

Epoch 659: val_loss did not improve from 28.17644
196/196 - 40s - loss: 27.2512 - MinusLogProbMetric: 27.2512 - val_loss: 28.2058 - val_MinusLogProbMetric: 28.2058 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 660/1000
2023-10-27 09:26:23.531 
Epoch 660/1000 
	 loss: 27.2506, MinusLogProbMetric: 27.2506, val_loss: 28.1725, val_MinusLogProbMetric: 28.1725

Epoch 660: val_loss improved from 28.17644 to 28.17251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 41s - loss: 27.2506 - MinusLogProbMetric: 27.2506 - val_loss: 28.1725 - val_MinusLogProbMetric: 28.1725 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 661/1000
2023-10-27 09:27:04.805 
Epoch 661/1000 
	 loss: 27.2520, MinusLogProbMetric: 27.2520, val_loss: 28.1739, val_MinusLogProbMetric: 28.1739

Epoch 661: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2520 - MinusLogProbMetric: 27.2520 - val_loss: 28.1739 - val_MinusLogProbMetric: 28.1739 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 662/1000
2023-10-27 09:27:44.706 
Epoch 662/1000 
	 loss: 27.2488, MinusLogProbMetric: 27.2488, val_loss: 28.2132, val_MinusLogProbMetric: 28.2132

Epoch 662: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2488 - MinusLogProbMetric: 27.2488 - val_loss: 28.2132 - val_MinusLogProbMetric: 28.2132 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 663/1000
2023-10-27 09:28:25.738 
Epoch 663/1000 
	 loss: 27.2503, MinusLogProbMetric: 27.2503, val_loss: 28.2286, val_MinusLogProbMetric: 28.2286

Epoch 663: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2503 - MinusLogProbMetric: 27.2503 - val_loss: 28.2286 - val_MinusLogProbMetric: 28.2286 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 664/1000
2023-10-27 09:29:06.161 
Epoch 664/1000 
	 loss: 27.2468, MinusLogProbMetric: 27.2468, val_loss: 28.1857, val_MinusLogProbMetric: 28.1857

Epoch 664: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2468 - MinusLogProbMetric: 27.2468 - val_loss: 28.1857 - val_MinusLogProbMetric: 28.1857 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 665/1000
2023-10-27 09:29:47.344 
Epoch 665/1000 
	 loss: 27.2480, MinusLogProbMetric: 27.2480, val_loss: 28.1933, val_MinusLogProbMetric: 28.1933

Epoch 665: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2480 - MinusLogProbMetric: 27.2480 - val_loss: 28.1933 - val_MinusLogProbMetric: 28.1933 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 666/1000
2023-10-27 09:30:28.747 
Epoch 666/1000 
	 loss: 27.2463, MinusLogProbMetric: 27.2463, val_loss: 28.1985, val_MinusLogProbMetric: 28.1985

Epoch 666: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2463 - MinusLogProbMetric: 27.2463 - val_loss: 28.1985 - val_MinusLogProbMetric: 28.1985 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 667/1000
2023-10-27 09:31:10.791 
Epoch 667/1000 
	 loss: 27.2464, MinusLogProbMetric: 27.2464, val_loss: 28.2750, val_MinusLogProbMetric: 28.2750

Epoch 667: val_loss did not improve from 28.17251
196/196 - 42s - loss: 27.2464 - MinusLogProbMetric: 27.2464 - val_loss: 28.2750 - val_MinusLogProbMetric: 28.2750 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 668/1000
2023-10-27 09:31:51.762 
Epoch 668/1000 
	 loss: 27.2491, MinusLogProbMetric: 27.2491, val_loss: 28.1919, val_MinusLogProbMetric: 28.1919

Epoch 668: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2491 - MinusLogProbMetric: 27.2491 - val_loss: 28.1919 - val_MinusLogProbMetric: 28.1919 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 669/1000
2023-10-27 09:32:31.800 
Epoch 669/1000 
	 loss: 27.2506, MinusLogProbMetric: 27.2506, val_loss: 28.2209, val_MinusLogProbMetric: 28.2209

Epoch 669: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2506 - MinusLogProbMetric: 27.2506 - val_loss: 28.2209 - val_MinusLogProbMetric: 28.2209 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 670/1000
2023-10-27 09:33:12.602 
Epoch 670/1000 
	 loss: 27.2468, MinusLogProbMetric: 27.2468, val_loss: 28.2073, val_MinusLogProbMetric: 28.2073

Epoch 670: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2468 - MinusLogProbMetric: 27.2468 - val_loss: 28.2073 - val_MinusLogProbMetric: 28.2073 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 671/1000
2023-10-27 09:33:53.724 
Epoch 671/1000 
	 loss: 27.2467, MinusLogProbMetric: 27.2467, val_loss: 28.1968, val_MinusLogProbMetric: 28.1968

Epoch 671: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2467 - MinusLogProbMetric: 27.2467 - val_loss: 28.1968 - val_MinusLogProbMetric: 28.1968 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 672/1000
2023-10-27 09:34:33.546 
Epoch 672/1000 
	 loss: 27.2435, MinusLogProbMetric: 27.2435, val_loss: 28.1954, val_MinusLogProbMetric: 28.1954

Epoch 672: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2435 - MinusLogProbMetric: 27.2435 - val_loss: 28.1954 - val_MinusLogProbMetric: 28.1954 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 673/1000
2023-10-27 09:35:15.253 
Epoch 673/1000 
	 loss: 27.2412, MinusLogProbMetric: 27.2412, val_loss: 28.2278, val_MinusLogProbMetric: 28.2278

Epoch 673: val_loss did not improve from 28.17251
196/196 - 42s - loss: 27.2412 - MinusLogProbMetric: 27.2412 - val_loss: 28.2278 - val_MinusLogProbMetric: 28.2278 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 674/1000
2023-10-27 09:35:56.392 
Epoch 674/1000 
	 loss: 27.2484, MinusLogProbMetric: 27.2484, val_loss: 28.1896, val_MinusLogProbMetric: 28.1896

Epoch 674: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2484 - MinusLogProbMetric: 27.2484 - val_loss: 28.1896 - val_MinusLogProbMetric: 28.1896 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 675/1000
2023-10-27 09:36:37.722 
Epoch 675/1000 
	 loss: 27.2388, MinusLogProbMetric: 27.2388, val_loss: 28.2018, val_MinusLogProbMetric: 28.2018

Epoch 675: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2388 - MinusLogProbMetric: 27.2388 - val_loss: 28.2018 - val_MinusLogProbMetric: 28.2018 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 676/1000
2023-10-27 09:37:17.644 
Epoch 676/1000 
	 loss: 27.2447, MinusLogProbMetric: 27.2447, val_loss: 28.1820, val_MinusLogProbMetric: 28.1820

Epoch 676: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2447 - MinusLogProbMetric: 27.2447 - val_loss: 28.1820 - val_MinusLogProbMetric: 28.1820 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 677/1000
2023-10-27 09:37:58.422 
Epoch 677/1000 
	 loss: 27.2496, MinusLogProbMetric: 27.2496, val_loss: 28.2261, val_MinusLogProbMetric: 28.2261

Epoch 677: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2496 - MinusLogProbMetric: 27.2496 - val_loss: 28.2261 - val_MinusLogProbMetric: 28.2261 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 678/1000
2023-10-27 09:38:38.951 
Epoch 678/1000 
	 loss: 27.2421, MinusLogProbMetric: 27.2421, val_loss: 28.1938, val_MinusLogProbMetric: 28.1938

Epoch 678: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2421 - MinusLogProbMetric: 27.2421 - val_loss: 28.1938 - val_MinusLogProbMetric: 28.1938 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 679/1000
2023-10-27 09:39:20.118 
Epoch 679/1000 
	 loss: 27.2439, MinusLogProbMetric: 27.2439, val_loss: 28.2098, val_MinusLogProbMetric: 28.2098

Epoch 679: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2439 - MinusLogProbMetric: 27.2439 - val_loss: 28.2098 - val_MinusLogProbMetric: 28.2098 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 680/1000
2023-10-27 09:40:00.056 
Epoch 680/1000 
	 loss: 27.2425, MinusLogProbMetric: 27.2425, val_loss: 28.1954, val_MinusLogProbMetric: 28.1954

Epoch 680: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2425 - MinusLogProbMetric: 27.2425 - val_loss: 28.1954 - val_MinusLogProbMetric: 28.1954 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 681/1000
2023-10-27 09:40:39.172 
Epoch 681/1000 
	 loss: 27.2452, MinusLogProbMetric: 27.2452, val_loss: 28.2029, val_MinusLogProbMetric: 28.2029

Epoch 681: val_loss did not improve from 28.17251
196/196 - 39s - loss: 27.2452 - MinusLogProbMetric: 27.2452 - val_loss: 28.2029 - val_MinusLogProbMetric: 28.2029 - lr: 1.2500e-04 - 39s/epoch - 200ms/step
Epoch 682/1000
2023-10-27 09:41:20.271 
Epoch 682/1000 
	 loss: 27.2489, MinusLogProbMetric: 27.2489, val_loss: 28.1997, val_MinusLogProbMetric: 28.1997

Epoch 682: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2489 - MinusLogProbMetric: 27.2489 - val_loss: 28.1997 - val_MinusLogProbMetric: 28.1997 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 683/1000
2023-10-27 09:41:59.723 
Epoch 683/1000 
	 loss: 27.2426, MinusLogProbMetric: 27.2426, val_loss: 28.1900, val_MinusLogProbMetric: 28.1900

Epoch 683: val_loss did not improve from 28.17251
196/196 - 39s - loss: 27.2426 - MinusLogProbMetric: 27.2426 - val_loss: 28.1900 - val_MinusLogProbMetric: 28.1900 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 684/1000
2023-10-27 09:42:41.127 
Epoch 684/1000 
	 loss: 27.2497, MinusLogProbMetric: 27.2497, val_loss: 28.2312, val_MinusLogProbMetric: 28.2312

Epoch 684: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2497 - MinusLogProbMetric: 27.2497 - val_loss: 28.2312 - val_MinusLogProbMetric: 28.2312 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 685/1000
2023-10-27 09:43:20.596 
Epoch 685/1000 
	 loss: 27.2394, MinusLogProbMetric: 27.2394, val_loss: 28.2020, val_MinusLogProbMetric: 28.2020

Epoch 685: val_loss did not improve from 28.17251
196/196 - 39s - loss: 27.2394 - MinusLogProbMetric: 27.2394 - val_loss: 28.2020 - val_MinusLogProbMetric: 28.2020 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 686/1000
2023-10-27 09:44:01.075 
Epoch 686/1000 
	 loss: 27.2405, MinusLogProbMetric: 27.2405, val_loss: 28.1921, val_MinusLogProbMetric: 28.1921

Epoch 686: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2405 - MinusLogProbMetric: 27.2405 - val_loss: 28.1921 - val_MinusLogProbMetric: 28.1921 - lr: 1.2500e-04 - 40s/epoch - 207ms/step
Epoch 687/1000
2023-10-27 09:44:42.806 
Epoch 687/1000 
	 loss: 27.2399, MinusLogProbMetric: 27.2399, val_loss: 28.2131, val_MinusLogProbMetric: 28.2131

Epoch 687: val_loss did not improve from 28.17251
196/196 - 42s - loss: 27.2399 - MinusLogProbMetric: 27.2399 - val_loss: 28.2131 - val_MinusLogProbMetric: 28.2131 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 688/1000
2023-10-27 09:45:23.166 
Epoch 688/1000 
	 loss: 27.2384, MinusLogProbMetric: 27.2384, val_loss: 28.1831, val_MinusLogProbMetric: 28.1831

Epoch 688: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2384 - MinusLogProbMetric: 27.2384 - val_loss: 28.1831 - val_MinusLogProbMetric: 28.1831 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 689/1000
2023-10-27 09:46:03.696 
Epoch 689/1000 
	 loss: 27.2436, MinusLogProbMetric: 27.2436, val_loss: 28.2197, val_MinusLogProbMetric: 28.2197

Epoch 689: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2436 - MinusLogProbMetric: 27.2436 - val_loss: 28.2197 - val_MinusLogProbMetric: 28.2197 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 690/1000
2023-10-27 09:46:45.157 
Epoch 690/1000 
	 loss: 27.2369, MinusLogProbMetric: 27.2369, val_loss: 28.1864, val_MinusLogProbMetric: 28.1864

Epoch 690: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2369 - MinusLogProbMetric: 27.2369 - val_loss: 28.1864 - val_MinusLogProbMetric: 28.1864 - lr: 1.2500e-04 - 41s/epoch - 212ms/step
Epoch 691/1000
2023-10-27 09:47:25.843 
Epoch 691/1000 
	 loss: 27.2375, MinusLogProbMetric: 27.2375, val_loss: 28.1980, val_MinusLogProbMetric: 28.1980

Epoch 691: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2375 - MinusLogProbMetric: 27.2375 - val_loss: 28.1980 - val_MinusLogProbMetric: 28.1980 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 692/1000
2023-10-27 09:48:05.684 
Epoch 692/1000 
	 loss: 27.2388, MinusLogProbMetric: 27.2388, val_loss: 28.2047, val_MinusLogProbMetric: 28.2047

Epoch 692: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2388 - MinusLogProbMetric: 27.2388 - val_loss: 28.2047 - val_MinusLogProbMetric: 28.2047 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 693/1000
2023-10-27 09:48:45.031 
Epoch 693/1000 
	 loss: 27.2391, MinusLogProbMetric: 27.2391, val_loss: 28.1849, val_MinusLogProbMetric: 28.1849

Epoch 693: val_loss did not improve from 28.17251
196/196 - 39s - loss: 27.2391 - MinusLogProbMetric: 27.2391 - val_loss: 28.1849 - val_MinusLogProbMetric: 28.1849 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 694/1000
2023-10-27 09:49:23.964 
Epoch 694/1000 
	 loss: 27.2406, MinusLogProbMetric: 27.2406, val_loss: 28.2332, val_MinusLogProbMetric: 28.2332

Epoch 694: val_loss did not improve from 28.17251
196/196 - 39s - loss: 27.2406 - MinusLogProbMetric: 27.2406 - val_loss: 28.2332 - val_MinusLogProbMetric: 28.2332 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 695/1000
2023-10-27 09:50:05.874 
Epoch 695/1000 
	 loss: 27.2377, MinusLogProbMetric: 27.2377, val_loss: 28.2028, val_MinusLogProbMetric: 28.2028

Epoch 695: val_loss did not improve from 28.17251
196/196 - 42s - loss: 27.2377 - MinusLogProbMetric: 27.2377 - val_loss: 28.2028 - val_MinusLogProbMetric: 28.2028 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 696/1000
2023-10-27 09:50:47.022 
Epoch 696/1000 
	 loss: 27.2399, MinusLogProbMetric: 27.2399, val_loss: 28.1854, val_MinusLogProbMetric: 28.1854

Epoch 696: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2399 - MinusLogProbMetric: 27.2399 - val_loss: 28.1854 - val_MinusLogProbMetric: 28.1854 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 697/1000
2023-10-27 09:51:26.942 
Epoch 697/1000 
	 loss: 27.2367, MinusLogProbMetric: 27.2367, val_loss: 28.1868, val_MinusLogProbMetric: 28.1868

Epoch 697: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2367 - MinusLogProbMetric: 27.2367 - val_loss: 28.1868 - val_MinusLogProbMetric: 28.1868 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 698/1000
2023-10-27 09:52:05.176 
Epoch 698/1000 
	 loss: 27.2447, MinusLogProbMetric: 27.2447, val_loss: 28.1974, val_MinusLogProbMetric: 28.1974

Epoch 698: val_loss did not improve from 28.17251
196/196 - 38s - loss: 27.2447 - MinusLogProbMetric: 27.2447 - val_loss: 28.1974 - val_MinusLogProbMetric: 28.1974 - lr: 1.2500e-04 - 38s/epoch - 195ms/step
Epoch 699/1000
2023-10-27 09:52:43.677 
Epoch 699/1000 
	 loss: 27.2409, MinusLogProbMetric: 27.2409, val_loss: 28.1987, val_MinusLogProbMetric: 28.1987

Epoch 699: val_loss did not improve from 28.17251
196/196 - 38s - loss: 27.2409 - MinusLogProbMetric: 27.2409 - val_loss: 28.1987 - val_MinusLogProbMetric: 28.1987 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 700/1000
2023-10-27 09:53:22.494 
Epoch 700/1000 
	 loss: 27.2391, MinusLogProbMetric: 27.2391, val_loss: 28.2151, val_MinusLogProbMetric: 28.2151

Epoch 700: val_loss did not improve from 28.17251
196/196 - 39s - loss: 27.2391 - MinusLogProbMetric: 27.2391 - val_loss: 28.2151 - val_MinusLogProbMetric: 28.2151 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 701/1000
2023-10-27 09:54:03.403 
Epoch 701/1000 
	 loss: 27.2423, MinusLogProbMetric: 27.2423, val_loss: 28.1960, val_MinusLogProbMetric: 28.1960

Epoch 701: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2423 - MinusLogProbMetric: 27.2423 - val_loss: 28.1960 - val_MinusLogProbMetric: 28.1960 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 702/1000
2023-10-27 09:54:44.334 
Epoch 702/1000 
	 loss: 27.2406, MinusLogProbMetric: 27.2406, val_loss: 28.2145, val_MinusLogProbMetric: 28.2145

Epoch 702: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2406 - MinusLogProbMetric: 27.2406 - val_loss: 28.2145 - val_MinusLogProbMetric: 28.2145 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 703/1000
2023-10-27 09:55:23.885 
Epoch 703/1000 
	 loss: 27.2394, MinusLogProbMetric: 27.2394, val_loss: 28.2063, val_MinusLogProbMetric: 28.2063

Epoch 703: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2394 - MinusLogProbMetric: 27.2394 - val_loss: 28.2063 - val_MinusLogProbMetric: 28.2063 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 704/1000
2023-10-27 09:56:03.911 
Epoch 704/1000 
	 loss: 27.2409, MinusLogProbMetric: 27.2409, val_loss: 28.2056, val_MinusLogProbMetric: 28.2056

Epoch 704: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2409 - MinusLogProbMetric: 27.2409 - val_loss: 28.2056 - val_MinusLogProbMetric: 28.2056 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 705/1000
2023-10-27 09:56:45.189 
Epoch 705/1000 
	 loss: 27.2373, MinusLogProbMetric: 27.2373, val_loss: 28.2065, val_MinusLogProbMetric: 28.2065

Epoch 705: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2373 - MinusLogProbMetric: 27.2373 - val_loss: 28.2065 - val_MinusLogProbMetric: 28.2065 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 706/1000
2023-10-27 09:57:26.539 
Epoch 706/1000 
	 loss: 27.2405, MinusLogProbMetric: 27.2405, val_loss: 28.2160, val_MinusLogProbMetric: 28.2160

Epoch 706: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2405 - MinusLogProbMetric: 27.2405 - val_loss: 28.2160 - val_MinusLogProbMetric: 28.2160 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 707/1000
2023-10-27 09:58:07.620 
Epoch 707/1000 
	 loss: 27.2379, MinusLogProbMetric: 27.2379, val_loss: 28.1948, val_MinusLogProbMetric: 28.1948

Epoch 707: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2379 - MinusLogProbMetric: 27.2379 - val_loss: 28.1948 - val_MinusLogProbMetric: 28.1948 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 708/1000
2023-10-27 09:58:48.766 
Epoch 708/1000 
	 loss: 27.2393, MinusLogProbMetric: 27.2393, val_loss: 28.1940, val_MinusLogProbMetric: 28.1940

Epoch 708: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2393 - MinusLogProbMetric: 27.2393 - val_loss: 28.1940 - val_MinusLogProbMetric: 28.1940 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 709/1000
2023-10-27 09:59:29.608 
Epoch 709/1000 
	 loss: 27.2361, MinusLogProbMetric: 27.2361, val_loss: 28.2274, val_MinusLogProbMetric: 28.2274

Epoch 709: val_loss did not improve from 28.17251
196/196 - 41s - loss: 27.2361 - MinusLogProbMetric: 27.2361 - val_loss: 28.2274 - val_MinusLogProbMetric: 28.2274 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 710/1000
2023-10-27 10:00:09.317 
Epoch 710/1000 
	 loss: 27.2423, MinusLogProbMetric: 27.2423, val_loss: 28.2097, val_MinusLogProbMetric: 28.2097

Epoch 710: val_loss did not improve from 28.17251
196/196 - 40s - loss: 27.2423 - MinusLogProbMetric: 27.2423 - val_loss: 28.2097 - val_MinusLogProbMetric: 28.2097 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 711/1000
2023-10-27 10:00:49.301 
Epoch 711/1000 
	 loss: 27.2026, MinusLogProbMetric: 27.2026, val_loss: 28.1678, val_MinusLogProbMetric: 28.1678

Epoch 711: val_loss improved from 28.17251 to 28.16779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 41s - loss: 27.2026 - MinusLogProbMetric: 27.2026 - val_loss: 28.1678 - val_MinusLogProbMetric: 28.1678 - lr: 6.2500e-05 - 41s/epoch - 208ms/step
Epoch 712/1000
2023-10-27 10:01:29.299 
Epoch 712/1000 
	 loss: 27.2021, MinusLogProbMetric: 27.2021, val_loss: 28.1733, val_MinusLogProbMetric: 28.1733

Epoch 712: val_loss did not improve from 28.16779
196/196 - 39s - loss: 27.2021 - MinusLogProbMetric: 27.2021 - val_loss: 28.1733 - val_MinusLogProbMetric: 28.1733 - lr: 6.2500e-05 - 39s/epoch - 200ms/step
Epoch 713/1000
2023-10-27 10:02:09.535 
Epoch 713/1000 
	 loss: 27.2023, MinusLogProbMetric: 27.2023, val_loss: 28.1753, val_MinusLogProbMetric: 28.1753

Epoch 713: val_loss did not improve from 28.16779
196/196 - 40s - loss: 27.2023 - MinusLogProbMetric: 27.2023 - val_loss: 28.1753 - val_MinusLogProbMetric: 28.1753 - lr: 6.2500e-05 - 40s/epoch - 205ms/step
Epoch 714/1000
2023-10-27 10:02:49.740 
Epoch 714/1000 
	 loss: 27.2010, MinusLogProbMetric: 27.2010, val_loss: 28.1650, val_MinusLogProbMetric: 28.1650

Epoch 714: val_loss improved from 28.16779 to 28.16498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 41s - loss: 27.2010 - MinusLogProbMetric: 27.2010 - val_loss: 28.1650 - val_MinusLogProbMetric: 28.1650 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Epoch 715/1000
2023-10-27 10:03:31.442 
Epoch 715/1000 
	 loss: 27.2017, MinusLogProbMetric: 27.2017, val_loss: 28.1717, val_MinusLogProbMetric: 28.1717

Epoch 715: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.2017 - MinusLogProbMetric: 27.2017 - val_loss: 28.1717 - val_MinusLogProbMetric: 28.1717 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Epoch 716/1000
2023-10-27 10:04:12.766 
Epoch 716/1000 
	 loss: 27.2009, MinusLogProbMetric: 27.2009, val_loss: 28.1747, val_MinusLogProbMetric: 28.1747

Epoch 716: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.2009 - MinusLogProbMetric: 27.2009 - val_loss: 28.1747 - val_MinusLogProbMetric: 28.1747 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 717/1000
2023-10-27 10:04:54.146 
Epoch 717/1000 
	 loss: 27.1997, MinusLogProbMetric: 27.1997, val_loss: 28.1840, val_MinusLogProbMetric: 28.1840

Epoch 717: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1997 - MinusLogProbMetric: 27.1997 - val_loss: 28.1840 - val_MinusLogProbMetric: 28.1840 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 718/1000
2023-10-27 10:05:33.753 
Epoch 718/1000 
	 loss: 27.2006, MinusLogProbMetric: 27.2006, val_loss: 28.1767, val_MinusLogProbMetric: 28.1767

Epoch 718: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.2006 - MinusLogProbMetric: 27.2006 - val_loss: 28.1767 - val_MinusLogProbMetric: 28.1767 - lr: 6.2500e-05 - 40s/epoch - 202ms/step
Epoch 719/1000
2023-10-27 10:06:13.801 
Epoch 719/1000 
	 loss: 27.2011, MinusLogProbMetric: 27.2011, val_loss: 28.1740, val_MinusLogProbMetric: 28.1740

Epoch 719: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.2011 - MinusLogProbMetric: 27.2011 - val_loss: 28.1740 - val_MinusLogProbMetric: 28.1740 - lr: 6.2500e-05 - 40s/epoch - 204ms/step
Epoch 720/1000
2023-10-27 10:06:53.989 
Epoch 720/1000 
	 loss: 27.2027, MinusLogProbMetric: 27.2027, val_loss: 28.1753, val_MinusLogProbMetric: 28.1753

Epoch 720: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.2027 - MinusLogProbMetric: 27.2027 - val_loss: 28.1753 - val_MinusLogProbMetric: 28.1753 - lr: 6.2500e-05 - 40s/epoch - 205ms/step
Epoch 721/1000
2023-10-27 10:07:33.050 
Epoch 721/1000 
	 loss: 27.1978, MinusLogProbMetric: 27.1978, val_loss: 28.1809, val_MinusLogProbMetric: 28.1809

Epoch 721: val_loss did not improve from 28.16498
196/196 - 39s - loss: 27.1978 - MinusLogProbMetric: 27.1978 - val_loss: 28.1809 - val_MinusLogProbMetric: 28.1809 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 722/1000
2023-10-27 10:08:14.800 
Epoch 722/1000 
	 loss: 27.2011, MinusLogProbMetric: 27.2011, val_loss: 28.1780, val_MinusLogProbMetric: 28.1780

Epoch 722: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.2011 - MinusLogProbMetric: 27.2011 - val_loss: 28.1780 - val_MinusLogProbMetric: 28.1780 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 723/1000
2023-10-27 10:08:52.800 
Epoch 723/1000 
	 loss: 27.1998, MinusLogProbMetric: 27.1998, val_loss: 28.1746, val_MinusLogProbMetric: 28.1746

Epoch 723: val_loss did not improve from 28.16498
196/196 - 38s - loss: 27.1998 - MinusLogProbMetric: 27.1998 - val_loss: 28.1746 - val_MinusLogProbMetric: 28.1746 - lr: 6.2500e-05 - 38s/epoch - 194ms/step
Epoch 724/1000
2023-10-27 10:09:33.834 
Epoch 724/1000 
	 loss: 27.2021, MinusLogProbMetric: 27.2021, val_loss: 28.1786, val_MinusLogProbMetric: 28.1786

Epoch 724: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.2021 - MinusLogProbMetric: 27.2021 - val_loss: 28.1786 - val_MinusLogProbMetric: 28.1786 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Epoch 725/1000
2023-10-27 10:10:14.749 
Epoch 725/1000 
	 loss: 27.1991, MinusLogProbMetric: 27.1991, val_loss: 28.1711, val_MinusLogProbMetric: 28.1711

Epoch 725: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1991 - MinusLogProbMetric: 27.1991 - val_loss: 28.1711 - val_MinusLogProbMetric: 28.1711 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Epoch 726/1000
2023-10-27 10:10:54.670 
Epoch 726/1000 
	 loss: 27.1967, MinusLogProbMetric: 27.1967, val_loss: 28.1757, val_MinusLogProbMetric: 28.1757

Epoch 726: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.1967 - MinusLogProbMetric: 27.1967 - val_loss: 28.1757 - val_MinusLogProbMetric: 28.1757 - lr: 6.2500e-05 - 40s/epoch - 204ms/step
Epoch 727/1000
2023-10-27 10:11:35.785 
Epoch 727/1000 
	 loss: 27.1995, MinusLogProbMetric: 27.1995, val_loss: 28.1852, val_MinusLogProbMetric: 28.1852

Epoch 727: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1995 - MinusLogProbMetric: 27.1995 - val_loss: 28.1852 - val_MinusLogProbMetric: 28.1852 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 728/1000
2023-10-27 10:12:16.999 
Epoch 728/1000 
	 loss: 27.2009, MinusLogProbMetric: 27.2009, val_loss: 28.1860, val_MinusLogProbMetric: 28.1860

Epoch 728: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.2009 - MinusLogProbMetric: 27.2009 - val_loss: 28.1860 - val_MinusLogProbMetric: 28.1860 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 729/1000
2023-10-27 10:12:58.601 
Epoch 729/1000 
	 loss: 27.1970, MinusLogProbMetric: 27.1970, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 729: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1970 - MinusLogProbMetric: 27.1970 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 730/1000
2023-10-27 10:13:40.298 
Epoch 730/1000 
	 loss: 27.1978, MinusLogProbMetric: 27.1978, val_loss: 28.1746, val_MinusLogProbMetric: 28.1746

Epoch 730: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1978 - MinusLogProbMetric: 27.1978 - val_loss: 28.1746 - val_MinusLogProbMetric: 28.1746 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 731/1000
2023-10-27 10:14:24.742 
Epoch 731/1000 
	 loss: 27.1998, MinusLogProbMetric: 27.1998, val_loss: 28.1674, val_MinusLogProbMetric: 28.1674

Epoch 731: val_loss did not improve from 28.16498
196/196 - 44s - loss: 27.1998 - MinusLogProbMetric: 27.1998 - val_loss: 28.1674 - val_MinusLogProbMetric: 28.1674 - lr: 6.2500e-05 - 44s/epoch - 227ms/step
Epoch 732/1000
2023-10-27 10:15:08.095 
Epoch 732/1000 
	 loss: 27.2012, MinusLogProbMetric: 27.2012, val_loss: 28.1785, val_MinusLogProbMetric: 28.1785

Epoch 732: val_loss did not improve from 28.16498
196/196 - 43s - loss: 27.2012 - MinusLogProbMetric: 27.2012 - val_loss: 28.1785 - val_MinusLogProbMetric: 28.1785 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 733/1000
2023-10-27 10:15:51.729 
Epoch 733/1000 
	 loss: 27.2004, MinusLogProbMetric: 27.2004, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 733: val_loss did not improve from 28.16498
196/196 - 44s - loss: 27.2004 - MinusLogProbMetric: 27.2004 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 734/1000
2023-10-27 10:16:35.184 
Epoch 734/1000 
	 loss: 27.1997, MinusLogProbMetric: 27.1997, val_loss: 28.1763, val_MinusLogProbMetric: 28.1763

Epoch 734: val_loss did not improve from 28.16498
196/196 - 43s - loss: 27.1997 - MinusLogProbMetric: 27.1997 - val_loss: 28.1763 - val_MinusLogProbMetric: 28.1763 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 735/1000
2023-10-27 10:17:18.434 
Epoch 735/1000 
	 loss: 27.1990, MinusLogProbMetric: 27.1990, val_loss: 28.1776, val_MinusLogProbMetric: 28.1776

Epoch 735: val_loss did not improve from 28.16498
196/196 - 43s - loss: 27.1990 - MinusLogProbMetric: 27.1990 - val_loss: 28.1776 - val_MinusLogProbMetric: 28.1776 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 736/1000
2023-10-27 10:18:01.809 
Epoch 736/1000 
	 loss: 27.2000, MinusLogProbMetric: 27.2000, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 736: val_loss did not improve from 28.16498
196/196 - 43s - loss: 27.2000 - MinusLogProbMetric: 27.2000 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 737/1000
2023-10-27 10:18:45.328 
Epoch 737/1000 
	 loss: 27.1968, MinusLogProbMetric: 27.1968, val_loss: 28.1703, val_MinusLogProbMetric: 28.1703

Epoch 737: val_loss did not improve from 28.16498
196/196 - 44s - loss: 27.1968 - MinusLogProbMetric: 27.1968 - val_loss: 28.1703 - val_MinusLogProbMetric: 28.1703 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 738/1000
2023-10-27 10:19:28.510 
Epoch 738/1000 
	 loss: 27.1965, MinusLogProbMetric: 27.1965, val_loss: 28.1790, val_MinusLogProbMetric: 28.1790

Epoch 738: val_loss did not improve from 28.16498
196/196 - 43s - loss: 27.1965 - MinusLogProbMetric: 27.1965 - val_loss: 28.1790 - val_MinusLogProbMetric: 28.1790 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 739/1000
2023-10-27 10:20:11.483 
Epoch 739/1000 
	 loss: 27.1975, MinusLogProbMetric: 27.1975, val_loss: 28.1692, val_MinusLogProbMetric: 28.1692

Epoch 739: val_loss did not improve from 28.16498
196/196 - 43s - loss: 27.1975 - MinusLogProbMetric: 27.1975 - val_loss: 28.1692 - val_MinusLogProbMetric: 28.1692 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 740/1000
2023-10-27 10:20:54.755 
Epoch 740/1000 
	 loss: 27.1990, MinusLogProbMetric: 27.1990, val_loss: 28.1692, val_MinusLogProbMetric: 28.1692

Epoch 740: val_loss did not improve from 28.16498
196/196 - 43s - loss: 27.1990 - MinusLogProbMetric: 27.1990 - val_loss: 28.1692 - val_MinusLogProbMetric: 28.1692 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 741/1000
2023-10-27 10:21:38.647 
Epoch 741/1000 
	 loss: 27.1960, MinusLogProbMetric: 27.1960, val_loss: 28.2014, val_MinusLogProbMetric: 28.2014

Epoch 741: val_loss did not improve from 28.16498
196/196 - 44s - loss: 27.1960 - MinusLogProbMetric: 27.1960 - val_loss: 28.2014 - val_MinusLogProbMetric: 28.2014 - lr: 6.2500e-05 - 44s/epoch - 224ms/step
Epoch 742/1000
2023-10-27 10:22:22.234 
Epoch 742/1000 
	 loss: 27.1989, MinusLogProbMetric: 27.1989, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 742: val_loss did not improve from 28.16498
196/196 - 44s - loss: 27.1989 - MinusLogProbMetric: 27.1989 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 743/1000
2023-10-27 10:23:04.038 
Epoch 743/1000 
	 loss: 27.1979, MinusLogProbMetric: 27.1979, val_loss: 28.1817, val_MinusLogProbMetric: 28.1817

Epoch 743: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1979 - MinusLogProbMetric: 27.1979 - val_loss: 28.1817 - val_MinusLogProbMetric: 28.1817 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 744/1000
2023-10-27 10:23:46.026 
Epoch 744/1000 
	 loss: 27.1990, MinusLogProbMetric: 27.1990, val_loss: 28.1932, val_MinusLogProbMetric: 28.1932

Epoch 744: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1990 - MinusLogProbMetric: 27.1990 - val_loss: 28.1932 - val_MinusLogProbMetric: 28.1932 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 745/1000
2023-10-27 10:24:27.240 
Epoch 745/1000 
	 loss: 27.1982, MinusLogProbMetric: 27.1982, val_loss: 28.1807, val_MinusLogProbMetric: 28.1807

Epoch 745: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1982 - MinusLogProbMetric: 27.1982 - val_loss: 28.1807 - val_MinusLogProbMetric: 28.1807 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 746/1000
2023-10-27 10:25:08.774 
Epoch 746/1000 
	 loss: 27.1963, MinusLogProbMetric: 27.1963, val_loss: 28.1748, val_MinusLogProbMetric: 28.1748

Epoch 746: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1963 - MinusLogProbMetric: 27.1963 - val_loss: 28.1748 - val_MinusLogProbMetric: 28.1748 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 747/1000
2023-10-27 10:25:50.455 
Epoch 747/1000 
	 loss: 27.1961, MinusLogProbMetric: 27.1961, val_loss: 28.1821, val_MinusLogProbMetric: 28.1821

Epoch 747: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1961 - MinusLogProbMetric: 27.1961 - val_loss: 28.1821 - val_MinusLogProbMetric: 28.1821 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 748/1000
2023-10-27 10:26:31.842 
Epoch 748/1000 
	 loss: 27.1955, MinusLogProbMetric: 27.1955, val_loss: 28.1852, val_MinusLogProbMetric: 28.1852

Epoch 748: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1955 - MinusLogProbMetric: 27.1955 - val_loss: 28.1852 - val_MinusLogProbMetric: 28.1852 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 749/1000
2023-10-27 10:27:13.218 
Epoch 749/1000 
	 loss: 27.1950, MinusLogProbMetric: 27.1950, val_loss: 28.1764, val_MinusLogProbMetric: 28.1764

Epoch 749: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1950 - MinusLogProbMetric: 27.1950 - val_loss: 28.1764 - val_MinusLogProbMetric: 28.1764 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 750/1000
2023-10-27 10:27:53.658 
Epoch 750/1000 
	 loss: 27.1958, MinusLogProbMetric: 27.1958, val_loss: 28.1718, val_MinusLogProbMetric: 28.1718

Epoch 750: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.1958 - MinusLogProbMetric: 27.1958 - val_loss: 28.1718 - val_MinusLogProbMetric: 28.1718 - lr: 6.2500e-05 - 40s/epoch - 206ms/step
Epoch 751/1000
2023-10-27 10:28:33.748 
Epoch 751/1000 
	 loss: 27.1931, MinusLogProbMetric: 27.1931, val_loss: 28.1722, val_MinusLogProbMetric: 28.1722

Epoch 751: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.1931 - MinusLogProbMetric: 27.1931 - val_loss: 28.1722 - val_MinusLogProbMetric: 28.1722 - lr: 6.2500e-05 - 40s/epoch - 205ms/step
Epoch 752/1000
2023-10-27 10:29:14.254 
Epoch 752/1000 
	 loss: 27.1949, MinusLogProbMetric: 27.1949, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 752: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1949 - MinusLogProbMetric: 27.1949 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 6.2500e-05 - 41s/epoch - 207ms/step
Epoch 753/1000
2023-10-27 10:29:54.518 
Epoch 753/1000 
	 loss: 27.1932, MinusLogProbMetric: 27.1932, val_loss: 28.1782, val_MinusLogProbMetric: 28.1782

Epoch 753: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.1932 - MinusLogProbMetric: 27.1932 - val_loss: 28.1782 - val_MinusLogProbMetric: 28.1782 - lr: 6.2500e-05 - 40s/epoch - 205ms/step
Epoch 754/1000
2023-10-27 10:30:36.710 
Epoch 754/1000 
	 loss: 27.1943, MinusLogProbMetric: 27.1943, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 754: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1943 - MinusLogProbMetric: 27.1943 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 755/1000
2023-10-27 10:31:17.275 
Epoch 755/1000 
	 loss: 27.1986, MinusLogProbMetric: 27.1986, val_loss: 28.1727, val_MinusLogProbMetric: 28.1727

Epoch 755: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1986 - MinusLogProbMetric: 27.1986 - val_loss: 28.1727 - val_MinusLogProbMetric: 28.1727 - lr: 6.2500e-05 - 41s/epoch - 207ms/step
Epoch 756/1000
2023-10-27 10:31:57.072 
Epoch 756/1000 
	 loss: 27.1958, MinusLogProbMetric: 27.1958, val_loss: 28.1869, val_MinusLogProbMetric: 28.1869

Epoch 756: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.1958 - MinusLogProbMetric: 27.1958 - val_loss: 28.1869 - val_MinusLogProbMetric: 28.1869 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 757/1000
2023-10-27 10:32:38.355 
Epoch 757/1000 
	 loss: 27.1980, MinusLogProbMetric: 27.1980, val_loss: 28.1772, val_MinusLogProbMetric: 28.1772

Epoch 757: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1980 - MinusLogProbMetric: 27.1980 - val_loss: 28.1772 - val_MinusLogProbMetric: 28.1772 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 758/1000
2023-10-27 10:33:19.598 
Epoch 758/1000 
	 loss: 27.1931, MinusLogProbMetric: 27.1931, val_loss: 28.1831, val_MinusLogProbMetric: 28.1831

Epoch 758: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1931 - MinusLogProbMetric: 27.1931 - val_loss: 28.1831 - val_MinusLogProbMetric: 28.1831 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 759/1000
2023-10-27 10:34:01.729 
Epoch 759/1000 
	 loss: 27.1948, MinusLogProbMetric: 27.1948, val_loss: 28.1802, val_MinusLogProbMetric: 28.1802

Epoch 759: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1948 - MinusLogProbMetric: 27.1948 - val_loss: 28.1802 - val_MinusLogProbMetric: 28.1802 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 760/1000
2023-10-27 10:34:42.959 
Epoch 760/1000 
	 loss: 27.1936, MinusLogProbMetric: 27.1936, val_loss: 28.1796, val_MinusLogProbMetric: 28.1796

Epoch 760: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1936 - MinusLogProbMetric: 27.1936 - val_loss: 28.1796 - val_MinusLogProbMetric: 28.1796 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 761/1000
2023-10-27 10:35:24.615 
Epoch 761/1000 
	 loss: 27.1949, MinusLogProbMetric: 27.1949, val_loss: 28.1780, val_MinusLogProbMetric: 28.1780

Epoch 761: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1949 - MinusLogProbMetric: 27.1949 - val_loss: 28.1780 - val_MinusLogProbMetric: 28.1780 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 762/1000
2023-10-27 10:36:06.055 
Epoch 762/1000 
	 loss: 27.1967, MinusLogProbMetric: 27.1967, val_loss: 28.1795, val_MinusLogProbMetric: 28.1795

Epoch 762: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1967 - MinusLogProbMetric: 27.1967 - val_loss: 28.1795 - val_MinusLogProbMetric: 28.1795 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 763/1000
2023-10-27 10:36:47.352 
Epoch 763/1000 
	 loss: 27.1921, MinusLogProbMetric: 27.1921, val_loss: 28.1756, val_MinusLogProbMetric: 28.1756

Epoch 763: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1921 - MinusLogProbMetric: 27.1921 - val_loss: 28.1756 - val_MinusLogProbMetric: 28.1756 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 764/1000
2023-10-27 10:37:28.603 
Epoch 764/1000 
	 loss: 27.1941, MinusLogProbMetric: 27.1941, val_loss: 28.1841, val_MinusLogProbMetric: 28.1841

Epoch 764: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1941 - MinusLogProbMetric: 27.1941 - val_loss: 28.1841 - val_MinusLogProbMetric: 28.1841 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 765/1000
2023-10-27 10:38:08.814 
Epoch 765/1000 
	 loss: 27.1797, MinusLogProbMetric: 27.1797, val_loss: 28.1670, val_MinusLogProbMetric: 28.1670

Epoch 765: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.1797 - MinusLogProbMetric: 27.1797 - val_loss: 28.1670 - val_MinusLogProbMetric: 28.1670 - lr: 3.1250e-05 - 40s/epoch - 205ms/step
Epoch 766/1000
2023-10-27 10:38:49.768 
Epoch 766/1000 
	 loss: 27.1791, MinusLogProbMetric: 27.1791, val_loss: 28.1692, val_MinusLogProbMetric: 28.1692

Epoch 766: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1791 - MinusLogProbMetric: 27.1791 - val_loss: 28.1692 - val_MinusLogProbMetric: 28.1692 - lr: 3.1250e-05 - 41s/epoch - 209ms/step
Epoch 767/1000
2023-10-27 10:39:29.950 
Epoch 767/1000 
	 loss: 27.1772, MinusLogProbMetric: 27.1772, val_loss: 28.1662, val_MinusLogProbMetric: 28.1662

Epoch 767: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.1772 - MinusLogProbMetric: 27.1772 - val_loss: 28.1662 - val_MinusLogProbMetric: 28.1662 - lr: 3.1250e-05 - 40s/epoch - 205ms/step
Epoch 768/1000
2023-10-27 10:40:11.383 
Epoch 768/1000 
	 loss: 27.1760, MinusLogProbMetric: 27.1760, val_loss: 28.1659, val_MinusLogProbMetric: 28.1659

Epoch 768: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1760 - MinusLogProbMetric: 27.1760 - val_loss: 28.1659 - val_MinusLogProbMetric: 28.1659 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 769/1000
2023-10-27 10:40:53.242 
Epoch 769/1000 
	 loss: 27.1779, MinusLogProbMetric: 27.1779, val_loss: 28.1666, val_MinusLogProbMetric: 28.1666

Epoch 769: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1779 - MinusLogProbMetric: 27.1779 - val_loss: 28.1666 - val_MinusLogProbMetric: 28.1666 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 770/1000
2023-10-27 10:41:33.985 
Epoch 770/1000 
	 loss: 27.1773, MinusLogProbMetric: 27.1773, val_loss: 28.1660, val_MinusLogProbMetric: 28.1660

Epoch 770: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1773 - MinusLogProbMetric: 27.1773 - val_loss: 28.1660 - val_MinusLogProbMetric: 28.1660 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 771/1000
2023-10-27 10:42:14.856 
Epoch 771/1000 
	 loss: 27.1770, MinusLogProbMetric: 27.1770, val_loss: 28.1651, val_MinusLogProbMetric: 28.1651

Epoch 771: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1770 - MinusLogProbMetric: 27.1770 - val_loss: 28.1651 - val_MinusLogProbMetric: 28.1651 - lr: 3.1250e-05 - 41s/epoch - 209ms/step
Epoch 772/1000
2023-10-27 10:42:56.349 
Epoch 772/1000 
	 loss: 27.1769, MinusLogProbMetric: 27.1769, val_loss: 28.1778, val_MinusLogProbMetric: 28.1778

Epoch 772: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1769 - MinusLogProbMetric: 27.1769 - val_loss: 28.1778 - val_MinusLogProbMetric: 28.1778 - lr: 3.1250e-05 - 41s/epoch - 212ms/step
Epoch 773/1000
2023-10-27 10:43:38.709 
Epoch 773/1000 
	 loss: 27.1780, MinusLogProbMetric: 27.1780, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 773: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1780 - MinusLogProbMetric: 27.1780 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 774/1000
2023-10-27 10:44:20.748 
Epoch 774/1000 
	 loss: 27.1762, MinusLogProbMetric: 27.1762, val_loss: 28.1705, val_MinusLogProbMetric: 28.1705

Epoch 774: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1762 - MinusLogProbMetric: 27.1762 - val_loss: 28.1705 - val_MinusLogProbMetric: 28.1705 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 775/1000
2023-10-27 10:45:01.877 
Epoch 775/1000 
	 loss: 27.1769, MinusLogProbMetric: 27.1769, val_loss: 28.1666, val_MinusLogProbMetric: 28.1666

Epoch 775: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1769 - MinusLogProbMetric: 27.1769 - val_loss: 28.1666 - val_MinusLogProbMetric: 28.1666 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 776/1000
2023-10-27 10:45:43.559 
Epoch 776/1000 
	 loss: 27.1767, MinusLogProbMetric: 27.1767, val_loss: 28.1668, val_MinusLogProbMetric: 28.1668

Epoch 776: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1767 - MinusLogProbMetric: 27.1767 - val_loss: 28.1668 - val_MinusLogProbMetric: 28.1668 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 777/1000
2023-10-27 10:46:24.735 
Epoch 777/1000 
	 loss: 27.1752, MinusLogProbMetric: 27.1752, val_loss: 28.1673, val_MinusLogProbMetric: 28.1673

Epoch 777: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1752 - MinusLogProbMetric: 27.1752 - val_loss: 28.1673 - val_MinusLogProbMetric: 28.1673 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 778/1000
2023-10-27 10:47:05.954 
Epoch 778/1000 
	 loss: 27.1768, MinusLogProbMetric: 27.1768, val_loss: 28.1654, val_MinusLogProbMetric: 28.1654

Epoch 778: val_loss did not improve from 28.16498
196/196 - 41s - loss: 27.1768 - MinusLogProbMetric: 27.1768 - val_loss: 28.1654 - val_MinusLogProbMetric: 28.1654 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 779/1000
2023-10-27 10:47:46.401 
Epoch 779/1000 
	 loss: 27.1762, MinusLogProbMetric: 27.1762, val_loss: 28.1667, val_MinusLogProbMetric: 28.1667

Epoch 779: val_loss did not improve from 28.16498
196/196 - 40s - loss: 27.1762 - MinusLogProbMetric: 27.1762 - val_loss: 28.1667 - val_MinusLogProbMetric: 28.1667 - lr: 3.1250e-05 - 40s/epoch - 206ms/step
Epoch 780/1000
2023-10-27 10:48:28.231 
Epoch 780/1000 
	 loss: 27.1769, MinusLogProbMetric: 27.1769, val_loss: 28.1700, val_MinusLogProbMetric: 28.1700

Epoch 780: val_loss did not improve from 28.16498
196/196 - 42s - loss: 27.1769 - MinusLogProbMetric: 27.1769 - val_loss: 28.1700 - val_MinusLogProbMetric: 28.1700 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 781/1000
2023-10-27 10:49:10.017 
Epoch 781/1000 
	 loss: 27.1758, MinusLogProbMetric: 27.1758, val_loss: 28.1632, val_MinusLogProbMetric: 28.1632

Epoch 781: val_loss improved from 28.16498 to 28.16318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 42s - loss: 27.1758 - MinusLogProbMetric: 27.1758 - val_loss: 28.1632 - val_MinusLogProbMetric: 28.1632 - lr: 3.1250e-05 - 42s/epoch - 217ms/step
Epoch 782/1000
2023-10-27 10:49:52.685 
Epoch 782/1000 
	 loss: 27.1763, MinusLogProbMetric: 27.1763, val_loss: 28.1691, val_MinusLogProbMetric: 28.1691

Epoch 782: val_loss did not improve from 28.16318
196/196 - 42s - loss: 27.1763 - MinusLogProbMetric: 27.1763 - val_loss: 28.1691 - val_MinusLogProbMetric: 28.1691 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 783/1000
2023-10-27 10:50:33.091 
Epoch 783/1000 
	 loss: 27.1758, MinusLogProbMetric: 27.1758, val_loss: 28.1695, val_MinusLogProbMetric: 28.1695

Epoch 783: val_loss did not improve from 28.16318
196/196 - 40s - loss: 27.1758 - MinusLogProbMetric: 27.1758 - val_loss: 28.1695 - val_MinusLogProbMetric: 28.1695 - lr: 3.1250e-05 - 40s/epoch - 206ms/step
Epoch 784/1000
2023-10-27 10:51:15.112 
Epoch 784/1000 
	 loss: 27.1763, MinusLogProbMetric: 27.1763, val_loss: 28.1670, val_MinusLogProbMetric: 28.1670

Epoch 784: val_loss did not improve from 28.16318
196/196 - 42s - loss: 27.1763 - MinusLogProbMetric: 27.1763 - val_loss: 28.1670 - val_MinusLogProbMetric: 28.1670 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 785/1000
2023-10-27 10:51:55.946 
Epoch 785/1000 
	 loss: 27.1758, MinusLogProbMetric: 27.1758, val_loss: 28.1655, val_MinusLogProbMetric: 28.1655

Epoch 785: val_loss did not improve from 28.16318
196/196 - 41s - loss: 27.1758 - MinusLogProbMetric: 27.1758 - val_loss: 28.1655 - val_MinusLogProbMetric: 28.1655 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 786/1000
2023-10-27 10:52:37.827 
Epoch 786/1000 
	 loss: 27.1756, MinusLogProbMetric: 27.1756, val_loss: 28.1724, val_MinusLogProbMetric: 28.1724

Epoch 786: val_loss did not improve from 28.16318
196/196 - 42s - loss: 27.1756 - MinusLogProbMetric: 27.1756 - val_loss: 28.1724 - val_MinusLogProbMetric: 28.1724 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 787/1000
2023-10-27 10:53:18.331 
Epoch 787/1000 
	 loss: 27.1751, MinusLogProbMetric: 27.1751, val_loss: 28.1688, val_MinusLogProbMetric: 28.1688

Epoch 787: val_loss did not improve from 28.16318
196/196 - 41s - loss: 27.1751 - MinusLogProbMetric: 27.1751 - val_loss: 28.1688 - val_MinusLogProbMetric: 28.1688 - lr: 3.1250e-05 - 41s/epoch - 207ms/step
Epoch 788/1000
2023-10-27 10:53:59.717 
Epoch 788/1000 
	 loss: 27.1765, MinusLogProbMetric: 27.1765, val_loss: 28.1744, val_MinusLogProbMetric: 28.1744

Epoch 788: val_loss did not improve from 28.16318
196/196 - 41s - loss: 27.1765 - MinusLogProbMetric: 27.1765 - val_loss: 28.1744 - val_MinusLogProbMetric: 28.1744 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 789/1000
2023-10-27 10:54:41.260 
Epoch 789/1000 
	 loss: 27.1755, MinusLogProbMetric: 27.1755, val_loss: 28.1661, val_MinusLogProbMetric: 28.1661

Epoch 789: val_loss did not improve from 28.16318
196/196 - 42s - loss: 27.1755 - MinusLogProbMetric: 27.1755 - val_loss: 28.1661 - val_MinusLogProbMetric: 28.1661 - lr: 3.1250e-05 - 42s/epoch - 212ms/step
Epoch 790/1000
2023-10-27 10:55:22.603 
Epoch 790/1000 
	 loss: 27.1755, MinusLogProbMetric: 27.1755, val_loss: 28.1670, val_MinusLogProbMetric: 28.1670

Epoch 790: val_loss did not improve from 28.16318
196/196 - 41s - loss: 27.1755 - MinusLogProbMetric: 27.1755 - val_loss: 28.1670 - val_MinusLogProbMetric: 28.1670 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 791/1000
2023-10-27 10:56:04.953 
Epoch 791/1000 
	 loss: 27.1752, MinusLogProbMetric: 27.1752, val_loss: 28.1696, val_MinusLogProbMetric: 28.1696

Epoch 791: val_loss did not improve from 28.16318
196/196 - 42s - loss: 27.1752 - MinusLogProbMetric: 27.1752 - val_loss: 28.1696 - val_MinusLogProbMetric: 28.1696 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 792/1000
2023-10-27 10:56:46.350 
Epoch 792/1000 
	 loss: 27.1768, MinusLogProbMetric: 27.1768, val_loss: 28.1714, val_MinusLogProbMetric: 28.1714

Epoch 792: val_loss did not improve from 28.16318
196/196 - 41s - loss: 27.1768 - MinusLogProbMetric: 27.1768 - val_loss: 28.1714 - val_MinusLogProbMetric: 28.1714 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 793/1000
2023-10-27 10:57:26.701 
Epoch 793/1000 
	 loss: 27.1754, MinusLogProbMetric: 27.1754, val_loss: 28.1754, val_MinusLogProbMetric: 28.1754

Epoch 793: val_loss did not improve from 28.16318
196/196 - 40s - loss: 27.1754 - MinusLogProbMetric: 27.1754 - val_loss: 28.1754 - val_MinusLogProbMetric: 28.1754 - lr: 3.1250e-05 - 40s/epoch - 206ms/step
Epoch 794/1000
2023-10-27 10:58:07.426 
Epoch 794/1000 
	 loss: 27.1750, MinusLogProbMetric: 27.1750, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 794: val_loss did not improve from 28.16318
196/196 - 41s - loss: 27.1750 - MinusLogProbMetric: 27.1750 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 795/1000
2023-10-27 10:58:49.215 
Epoch 795/1000 
	 loss: 27.1759, MinusLogProbMetric: 27.1759, val_loss: 28.1670, val_MinusLogProbMetric: 28.1670

Epoch 795: val_loss did not improve from 28.16318
196/196 - 42s - loss: 27.1759 - MinusLogProbMetric: 27.1759 - val_loss: 28.1670 - val_MinusLogProbMetric: 28.1670 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 796/1000
2023-10-27 10:59:30.754 
Epoch 796/1000 
	 loss: 27.1774, MinusLogProbMetric: 27.1774, val_loss: 28.1660, val_MinusLogProbMetric: 28.1660

Epoch 796: val_loss did not improve from 28.16318
196/196 - 42s - loss: 27.1774 - MinusLogProbMetric: 27.1774 - val_loss: 28.1660 - val_MinusLogProbMetric: 28.1660 - lr: 3.1250e-05 - 42s/epoch - 212ms/step
Epoch 797/1000
2023-10-27 11:00:12.766 
Epoch 797/1000 
	 loss: 27.1749, MinusLogProbMetric: 27.1749, val_loss: 28.1671, val_MinusLogProbMetric: 28.1671

Epoch 797: val_loss did not improve from 28.16318
196/196 - 42s - loss: 27.1749 - MinusLogProbMetric: 27.1749 - val_loss: 28.1671 - val_MinusLogProbMetric: 28.1671 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 798/1000
2023-10-27 11:00:53.336 
Epoch 798/1000 
	 loss: 27.1760, MinusLogProbMetric: 27.1760, val_loss: 28.1677, val_MinusLogProbMetric: 28.1677

Epoch 798: val_loss did not improve from 28.16318
196/196 - 41s - loss: 27.1760 - MinusLogProbMetric: 27.1760 - val_loss: 28.1677 - val_MinusLogProbMetric: 28.1677 - lr: 3.1250e-05 - 41s/epoch - 207ms/step
Epoch 799/1000
2023-10-27 11:01:34.667 
Epoch 799/1000 
	 loss: 27.1758, MinusLogProbMetric: 27.1758, val_loss: 28.1691, val_MinusLogProbMetric: 28.1691

Epoch 799: val_loss did not improve from 28.16318
196/196 - 41s - loss: 27.1758 - MinusLogProbMetric: 27.1758 - val_loss: 28.1691 - val_MinusLogProbMetric: 28.1691 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 800/1000
2023-10-27 11:02:16.964 
Epoch 800/1000 
	 loss: 27.1750, MinusLogProbMetric: 27.1750, val_loss: 28.1630, val_MinusLogProbMetric: 28.1630

Epoch 800: val_loss improved from 28.16318 to 28.16296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 27.1750 - MinusLogProbMetric: 27.1750 - val_loss: 28.1630 - val_MinusLogProbMetric: 28.1630 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 801/1000
2023-10-27 11:03:00.269 
Epoch 801/1000 
	 loss: 27.1743, MinusLogProbMetric: 27.1743, val_loss: 28.1671, val_MinusLogProbMetric: 28.1671

Epoch 801: val_loss did not improve from 28.16296
196/196 - 42s - loss: 27.1743 - MinusLogProbMetric: 27.1743 - val_loss: 28.1671 - val_MinusLogProbMetric: 28.1671 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 802/1000
2023-10-27 11:03:41.703 
Epoch 802/1000 
	 loss: 27.1751, MinusLogProbMetric: 27.1751, val_loss: 28.1702, val_MinusLogProbMetric: 28.1702

Epoch 802: val_loss did not improve from 28.16296
196/196 - 41s - loss: 27.1751 - MinusLogProbMetric: 27.1751 - val_loss: 28.1702 - val_MinusLogProbMetric: 28.1702 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 803/1000
2023-10-27 11:04:22.634 
Epoch 803/1000 
	 loss: 27.1750, MinusLogProbMetric: 27.1750, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 803: val_loss did not improve from 28.16296
196/196 - 41s - loss: 27.1750 - MinusLogProbMetric: 27.1750 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 3.1250e-05 - 41s/epoch - 209ms/step
Epoch 804/1000
2023-10-27 11:05:03.521 
Epoch 804/1000 
	 loss: 27.1750, MinusLogProbMetric: 27.1750, val_loss: 28.1647, val_MinusLogProbMetric: 28.1647

Epoch 804: val_loss did not improve from 28.16296
196/196 - 41s - loss: 27.1750 - MinusLogProbMetric: 27.1750 - val_loss: 28.1647 - val_MinusLogProbMetric: 28.1647 - lr: 3.1250e-05 - 41s/epoch - 209ms/step
Epoch 805/1000
2023-10-27 11:05:44.356 
Epoch 805/1000 
	 loss: 27.1759, MinusLogProbMetric: 27.1759, val_loss: 28.1688, val_MinusLogProbMetric: 28.1688

Epoch 805: val_loss did not improve from 28.16296
196/196 - 41s - loss: 27.1759 - MinusLogProbMetric: 27.1759 - val_loss: 28.1688 - val_MinusLogProbMetric: 28.1688 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 806/1000
2023-10-27 11:06:24.486 
Epoch 806/1000 
	 loss: 27.1740, MinusLogProbMetric: 27.1740, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 806: val_loss did not improve from 28.16296
196/196 - 40s - loss: 27.1740 - MinusLogProbMetric: 27.1740 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 3.1250e-05 - 40s/epoch - 205ms/step
Epoch 807/1000
2023-10-27 11:07:06.370 
Epoch 807/1000 
	 loss: 27.1750, MinusLogProbMetric: 27.1750, val_loss: 28.1694, val_MinusLogProbMetric: 28.1694

Epoch 807: val_loss did not improve from 28.16296
196/196 - 42s - loss: 27.1750 - MinusLogProbMetric: 27.1750 - val_loss: 28.1694 - val_MinusLogProbMetric: 28.1694 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 808/1000
2023-10-27 11:07:47.927 
Epoch 808/1000 
	 loss: 27.1753, MinusLogProbMetric: 27.1753, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 808: val_loss did not improve from 28.16296
196/196 - 42s - loss: 27.1753 - MinusLogProbMetric: 27.1753 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 3.1250e-05 - 42s/epoch - 212ms/step
Epoch 809/1000
2023-10-27 11:08:29.587 
Epoch 809/1000 
	 loss: 27.1750, MinusLogProbMetric: 27.1750, val_loss: 28.1673, val_MinusLogProbMetric: 28.1673

Epoch 809: val_loss did not improve from 28.16296
196/196 - 42s - loss: 27.1750 - MinusLogProbMetric: 27.1750 - val_loss: 28.1673 - val_MinusLogProbMetric: 28.1673 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 810/1000
2023-10-27 11:09:08.874 
Epoch 810/1000 
	 loss: 27.1759, MinusLogProbMetric: 27.1759, val_loss: 28.1723, val_MinusLogProbMetric: 28.1723

Epoch 810: val_loss did not improve from 28.16296
196/196 - 39s - loss: 27.1759 - MinusLogProbMetric: 27.1759 - val_loss: 28.1723 - val_MinusLogProbMetric: 28.1723 - lr: 3.1250e-05 - 39s/epoch - 200ms/step
Epoch 811/1000
2023-10-27 11:09:49.703 
Epoch 811/1000 
	 loss: 27.1740, MinusLogProbMetric: 27.1740, val_loss: 28.1726, val_MinusLogProbMetric: 28.1726

Epoch 811: val_loss did not improve from 28.16296
196/196 - 41s - loss: 27.1740 - MinusLogProbMetric: 27.1740 - val_loss: 28.1726 - val_MinusLogProbMetric: 28.1726 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 812/1000
2023-10-27 11:10:30.945 
Epoch 812/1000 
	 loss: 27.1747, MinusLogProbMetric: 27.1747, val_loss: 28.1669, val_MinusLogProbMetric: 28.1669

Epoch 812: val_loss did not improve from 28.16296
196/196 - 41s - loss: 27.1747 - MinusLogProbMetric: 27.1747 - val_loss: 28.1669 - val_MinusLogProbMetric: 28.1669 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 813/1000
2023-10-27 11:11:12.066 
Epoch 813/1000 
	 loss: 27.1744, MinusLogProbMetric: 27.1744, val_loss: 28.1719, val_MinusLogProbMetric: 28.1719

Epoch 813: val_loss did not improve from 28.16296
196/196 - 41s - loss: 27.1744 - MinusLogProbMetric: 27.1744 - val_loss: 28.1719 - val_MinusLogProbMetric: 28.1719 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 814/1000
2023-10-27 11:11:53.790 
Epoch 814/1000 
	 loss: 27.1763, MinusLogProbMetric: 27.1763, val_loss: 28.1719, val_MinusLogProbMetric: 28.1719

Epoch 814: val_loss did not improve from 28.16296
196/196 - 42s - loss: 27.1763 - MinusLogProbMetric: 27.1763 - val_loss: 28.1719 - val_MinusLogProbMetric: 28.1719 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 815/1000
2023-10-27 11:12:35.884 
Epoch 815/1000 
	 loss: 27.1733, MinusLogProbMetric: 27.1733, val_loss: 28.1627, val_MinusLogProbMetric: 28.1627

Epoch 815: val_loss improved from 28.16296 to 28.16271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 43s - loss: 27.1733 - MinusLogProbMetric: 27.1733 - val_loss: 28.1627 - val_MinusLogProbMetric: 28.1627 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 816/1000
2023-10-27 11:13:18.303 
Epoch 816/1000 
	 loss: 27.1765, MinusLogProbMetric: 27.1765, val_loss: 28.1772, val_MinusLogProbMetric: 28.1772

Epoch 816: val_loss did not improve from 28.16271
196/196 - 42s - loss: 27.1765 - MinusLogProbMetric: 27.1765 - val_loss: 28.1772 - val_MinusLogProbMetric: 28.1772 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 817/1000
2023-10-27 11:13:59.792 
Epoch 817/1000 
	 loss: 27.1744, MinusLogProbMetric: 27.1744, val_loss: 28.1688, val_MinusLogProbMetric: 28.1688

Epoch 817: val_loss did not improve from 28.16271
196/196 - 41s - loss: 27.1744 - MinusLogProbMetric: 27.1744 - val_loss: 28.1688 - val_MinusLogProbMetric: 28.1688 - lr: 3.1250e-05 - 41s/epoch - 212ms/step
Epoch 818/1000
2023-10-27 11:14:39.576 
Epoch 818/1000 
	 loss: 27.1744, MinusLogProbMetric: 27.1744, val_loss: 28.1740, val_MinusLogProbMetric: 28.1740

Epoch 818: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1744 - MinusLogProbMetric: 27.1744 - val_loss: 28.1740 - val_MinusLogProbMetric: 28.1740 - lr: 3.1250e-05 - 40s/epoch - 203ms/step
Epoch 819/1000
2023-10-27 11:15:19.804 
Epoch 819/1000 
	 loss: 27.1754, MinusLogProbMetric: 27.1754, val_loss: 28.1666, val_MinusLogProbMetric: 28.1666

Epoch 819: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1754 - MinusLogProbMetric: 27.1754 - val_loss: 28.1666 - val_MinusLogProbMetric: 28.1666 - lr: 3.1250e-05 - 40s/epoch - 205ms/step
Epoch 820/1000
2023-10-27 11:16:00.305 
Epoch 820/1000 
	 loss: 27.1739, MinusLogProbMetric: 27.1739, val_loss: 28.1691, val_MinusLogProbMetric: 28.1691

Epoch 820: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1739 - MinusLogProbMetric: 27.1739 - val_loss: 28.1691 - val_MinusLogProbMetric: 28.1691 - lr: 3.1250e-05 - 40s/epoch - 207ms/step
Epoch 821/1000
2023-10-27 11:16:41.040 
Epoch 821/1000 
	 loss: 27.1742, MinusLogProbMetric: 27.1742, val_loss: 28.1737, val_MinusLogProbMetric: 28.1737

Epoch 821: val_loss did not improve from 28.16271
196/196 - 41s - loss: 27.1742 - MinusLogProbMetric: 27.1742 - val_loss: 28.1737 - val_MinusLogProbMetric: 28.1737 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 822/1000
2023-10-27 11:17:22.880 
Epoch 822/1000 
	 loss: 27.1750, MinusLogProbMetric: 27.1750, val_loss: 28.1730, val_MinusLogProbMetric: 28.1730

Epoch 822: val_loss did not improve from 28.16271
196/196 - 42s - loss: 27.1750 - MinusLogProbMetric: 27.1750 - val_loss: 28.1730 - val_MinusLogProbMetric: 28.1730 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 823/1000
2023-10-27 11:18:02.424 
Epoch 823/1000 
	 loss: 27.1747, MinusLogProbMetric: 27.1747, val_loss: 28.1778, val_MinusLogProbMetric: 28.1778

Epoch 823: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1747 - MinusLogProbMetric: 27.1747 - val_loss: 28.1778 - val_MinusLogProbMetric: 28.1778 - lr: 3.1250e-05 - 40s/epoch - 202ms/step
Epoch 824/1000
2023-10-27 11:18:42.297 
Epoch 824/1000 
	 loss: 27.1735, MinusLogProbMetric: 27.1735, val_loss: 28.1675, val_MinusLogProbMetric: 28.1675

Epoch 824: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1735 - MinusLogProbMetric: 27.1735 - val_loss: 28.1675 - val_MinusLogProbMetric: 28.1675 - lr: 3.1250e-05 - 40s/epoch - 203ms/step
Epoch 825/1000
2023-10-27 11:19:21.645 
Epoch 825/1000 
	 loss: 27.1723, MinusLogProbMetric: 27.1723, val_loss: 28.1653, val_MinusLogProbMetric: 28.1653

Epoch 825: val_loss did not improve from 28.16271
196/196 - 39s - loss: 27.1723 - MinusLogProbMetric: 27.1723 - val_loss: 28.1653 - val_MinusLogProbMetric: 28.1653 - lr: 3.1250e-05 - 39s/epoch - 201ms/step
Epoch 826/1000
2023-10-27 11:20:01.929 
Epoch 826/1000 
	 loss: 27.1719, MinusLogProbMetric: 27.1719, val_loss: 28.1660, val_MinusLogProbMetric: 28.1660

Epoch 826: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1719 - MinusLogProbMetric: 27.1719 - val_loss: 28.1660 - val_MinusLogProbMetric: 28.1660 - lr: 3.1250e-05 - 40s/epoch - 206ms/step
Epoch 827/1000
2023-10-27 11:20:41.958 
Epoch 827/1000 
	 loss: 27.1744, MinusLogProbMetric: 27.1744, val_loss: 28.1637, val_MinusLogProbMetric: 28.1637

Epoch 827: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1744 - MinusLogProbMetric: 27.1744 - val_loss: 28.1637 - val_MinusLogProbMetric: 28.1637 - lr: 3.1250e-05 - 40s/epoch - 204ms/step
Epoch 828/1000
2023-10-27 11:21:22.286 
Epoch 828/1000 
	 loss: 27.1722, MinusLogProbMetric: 27.1722, val_loss: 28.1643, val_MinusLogProbMetric: 28.1643

Epoch 828: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1722 - MinusLogProbMetric: 27.1722 - val_loss: 28.1643 - val_MinusLogProbMetric: 28.1643 - lr: 3.1250e-05 - 40s/epoch - 206ms/step
Epoch 829/1000
2023-10-27 11:22:02.404 
Epoch 829/1000 
	 loss: 27.1721, MinusLogProbMetric: 27.1721, val_loss: 28.1747, val_MinusLogProbMetric: 28.1747

Epoch 829: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1721 - MinusLogProbMetric: 27.1721 - val_loss: 28.1747 - val_MinusLogProbMetric: 28.1747 - lr: 3.1250e-05 - 40s/epoch - 205ms/step
Epoch 830/1000
2023-10-27 11:22:41.756 
Epoch 830/1000 
	 loss: 27.1740, MinusLogProbMetric: 27.1740, val_loss: 28.1689, val_MinusLogProbMetric: 28.1689

Epoch 830: val_loss did not improve from 28.16271
196/196 - 39s - loss: 27.1740 - MinusLogProbMetric: 27.1740 - val_loss: 28.1689 - val_MinusLogProbMetric: 28.1689 - lr: 3.1250e-05 - 39s/epoch - 201ms/step
Epoch 831/1000
2023-10-27 11:23:21.878 
Epoch 831/1000 
	 loss: 27.1729, MinusLogProbMetric: 27.1729, val_loss: 28.1763, val_MinusLogProbMetric: 28.1763

Epoch 831: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1729 - MinusLogProbMetric: 27.1729 - val_loss: 28.1763 - val_MinusLogProbMetric: 28.1763 - lr: 3.1250e-05 - 40s/epoch - 205ms/step
Epoch 832/1000
2023-10-27 11:24:03.058 
Epoch 832/1000 
	 loss: 27.1735, MinusLogProbMetric: 27.1735, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 832: val_loss did not improve from 28.16271
196/196 - 41s - loss: 27.1735 - MinusLogProbMetric: 27.1735 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 833/1000
2023-10-27 11:24:43.222 
Epoch 833/1000 
	 loss: 27.1725, MinusLogProbMetric: 27.1725, val_loss: 28.1666, val_MinusLogProbMetric: 28.1666

Epoch 833: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1725 - MinusLogProbMetric: 27.1725 - val_loss: 28.1666 - val_MinusLogProbMetric: 28.1666 - lr: 3.1250e-05 - 40s/epoch - 205ms/step
Epoch 834/1000
2023-10-27 11:25:22.578 
Epoch 834/1000 
	 loss: 27.1724, MinusLogProbMetric: 27.1724, val_loss: 28.1747, val_MinusLogProbMetric: 28.1747

Epoch 834: val_loss did not improve from 28.16271
196/196 - 39s - loss: 27.1724 - MinusLogProbMetric: 27.1724 - val_loss: 28.1747 - val_MinusLogProbMetric: 28.1747 - lr: 3.1250e-05 - 39s/epoch - 201ms/step
Epoch 835/1000
2023-10-27 11:26:02.476 
Epoch 835/1000 
	 loss: 27.1727, MinusLogProbMetric: 27.1727, val_loss: 28.1685, val_MinusLogProbMetric: 28.1685

Epoch 835: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1727 - MinusLogProbMetric: 27.1727 - val_loss: 28.1685 - val_MinusLogProbMetric: 28.1685 - lr: 3.1250e-05 - 40s/epoch - 204ms/step
Epoch 836/1000
2023-10-27 11:26:42.710 
Epoch 836/1000 
	 loss: 27.1715, MinusLogProbMetric: 27.1715, val_loss: 28.1703, val_MinusLogProbMetric: 28.1703

Epoch 836: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1715 - MinusLogProbMetric: 27.1715 - val_loss: 28.1703 - val_MinusLogProbMetric: 28.1703 - lr: 3.1250e-05 - 40s/epoch - 205ms/step
Epoch 837/1000
2023-10-27 11:27:21.488 
Epoch 837/1000 
	 loss: 27.1739, MinusLogProbMetric: 27.1739, val_loss: 28.1701, val_MinusLogProbMetric: 28.1701

Epoch 837: val_loss did not improve from 28.16271
196/196 - 39s - loss: 27.1739 - MinusLogProbMetric: 27.1739 - val_loss: 28.1701 - val_MinusLogProbMetric: 28.1701 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 838/1000
2023-10-27 11:28:02.628 
Epoch 838/1000 
	 loss: 27.1727, MinusLogProbMetric: 27.1727, val_loss: 28.1725, val_MinusLogProbMetric: 28.1725

Epoch 838: val_loss did not improve from 28.16271
196/196 - 41s - loss: 27.1727 - MinusLogProbMetric: 27.1727 - val_loss: 28.1725 - val_MinusLogProbMetric: 28.1725 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 839/1000
2023-10-27 11:28:43.372 
Epoch 839/1000 
	 loss: 27.1730, MinusLogProbMetric: 27.1730, val_loss: 28.1727, val_MinusLogProbMetric: 28.1727

Epoch 839: val_loss did not improve from 28.16271
196/196 - 41s - loss: 27.1730 - MinusLogProbMetric: 27.1730 - val_loss: 28.1727 - val_MinusLogProbMetric: 28.1727 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 840/1000
2023-10-27 11:29:24.537 
Epoch 840/1000 
	 loss: 27.1720, MinusLogProbMetric: 27.1720, val_loss: 28.1673, val_MinusLogProbMetric: 28.1673

Epoch 840: val_loss did not improve from 28.16271
196/196 - 41s - loss: 27.1720 - MinusLogProbMetric: 27.1720 - val_loss: 28.1673 - val_MinusLogProbMetric: 28.1673 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 841/1000
2023-10-27 11:30:06.925 
Epoch 841/1000 
	 loss: 27.1739, MinusLogProbMetric: 27.1739, val_loss: 28.1654, val_MinusLogProbMetric: 28.1654

Epoch 841: val_loss did not improve from 28.16271
196/196 - 42s - loss: 27.1739 - MinusLogProbMetric: 27.1739 - val_loss: 28.1654 - val_MinusLogProbMetric: 28.1654 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 842/1000
2023-10-27 11:30:46.637 
Epoch 842/1000 
	 loss: 27.1727, MinusLogProbMetric: 27.1727, val_loss: 28.1795, val_MinusLogProbMetric: 28.1795

Epoch 842: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1727 - MinusLogProbMetric: 27.1727 - val_loss: 28.1795 - val_MinusLogProbMetric: 28.1795 - lr: 3.1250e-05 - 40s/epoch - 203ms/step
Epoch 843/1000
2023-10-27 11:31:26.994 
Epoch 843/1000 
	 loss: 27.1721, MinusLogProbMetric: 27.1721, val_loss: 28.1710, val_MinusLogProbMetric: 28.1710

Epoch 843: val_loss did not improve from 28.16271
196/196 - 40s - loss: 27.1721 - MinusLogProbMetric: 27.1721 - val_loss: 28.1710 - val_MinusLogProbMetric: 28.1710 - lr: 3.1250e-05 - 40s/epoch - 206ms/step
Epoch 844/1000
2023-10-27 11:32:07.731 
Epoch 844/1000 
	 loss: 27.1715, MinusLogProbMetric: 27.1715, val_loss: 28.1738, val_MinusLogProbMetric: 28.1738

Epoch 844: val_loss did not improve from 28.16271
196/196 - 41s - loss: 27.1715 - MinusLogProbMetric: 27.1715 - val_loss: 28.1738 - val_MinusLogProbMetric: 28.1738 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 845/1000
2023-10-27 11:32:48.878 
Epoch 845/1000 
	 loss: 27.1724, MinusLogProbMetric: 27.1724, val_loss: 28.1683, val_MinusLogProbMetric: 28.1683

Epoch 845: val_loss did not improve from 28.16271
196/196 - 41s - loss: 27.1724 - MinusLogProbMetric: 27.1724 - val_loss: 28.1683 - val_MinusLogProbMetric: 28.1683 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 846/1000
2023-10-27 11:33:22.151 
Epoch 846/1000 
	 loss: 27.1726, MinusLogProbMetric: 27.1726, val_loss: 28.1665, val_MinusLogProbMetric: 28.1665

Epoch 846: val_loss did not improve from 28.16271
196/196 - 33s - loss: 27.1726 - MinusLogProbMetric: 27.1726 - val_loss: 28.1665 - val_MinusLogProbMetric: 28.1665 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 847/1000
2023-10-27 11:33:55.015 
Epoch 847/1000 
	 loss: 27.1709, MinusLogProbMetric: 27.1709, val_loss: 28.1717, val_MinusLogProbMetric: 28.1717

Epoch 847: val_loss did not improve from 28.16271
196/196 - 33s - loss: 27.1709 - MinusLogProbMetric: 27.1709 - val_loss: 28.1717 - val_MinusLogProbMetric: 28.1717 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 848/1000
2023-10-27 11:34:27.005 
Epoch 848/1000 
	 loss: 27.1724, MinusLogProbMetric: 27.1724, val_loss: 28.1682, val_MinusLogProbMetric: 28.1682

Epoch 848: val_loss did not improve from 28.16271
196/196 - 32s - loss: 27.1724 - MinusLogProbMetric: 27.1724 - val_loss: 28.1682 - val_MinusLogProbMetric: 28.1682 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 849/1000
2023-10-27 11:35:00.369 
Epoch 849/1000 
	 loss: 27.1726, MinusLogProbMetric: 27.1726, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 849: val_loss did not improve from 28.16271
196/196 - 33s - loss: 27.1726 - MinusLogProbMetric: 27.1726 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 850/1000
2023-10-27 11:35:35.471 
Epoch 850/1000 
	 loss: 27.1705, MinusLogProbMetric: 27.1705, val_loss: 28.1730, val_MinusLogProbMetric: 28.1730

Epoch 850: val_loss did not improve from 28.16271
196/196 - 35s - loss: 27.1705 - MinusLogProbMetric: 27.1705 - val_loss: 28.1730 - val_MinusLogProbMetric: 28.1730 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 851/1000
2023-10-27 11:36:09.587 
Epoch 851/1000 
	 loss: 27.1723, MinusLogProbMetric: 27.1723, val_loss: 28.1690, val_MinusLogProbMetric: 28.1690

Epoch 851: val_loss did not improve from 28.16271
196/196 - 34s - loss: 27.1723 - MinusLogProbMetric: 27.1723 - val_loss: 28.1690 - val_MinusLogProbMetric: 28.1690 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 852/1000
2023-10-27 11:36:42.189 
Epoch 852/1000 
	 loss: 27.1728, MinusLogProbMetric: 27.1728, val_loss: 28.1685, val_MinusLogProbMetric: 28.1685

Epoch 852: val_loss did not improve from 28.16271
196/196 - 33s - loss: 27.1728 - MinusLogProbMetric: 27.1728 - val_loss: 28.1685 - val_MinusLogProbMetric: 28.1685 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 853/1000
2023-10-27 11:37:15.085 
Epoch 853/1000 
	 loss: 27.1716, MinusLogProbMetric: 27.1716, val_loss: 28.1686, val_MinusLogProbMetric: 28.1686

Epoch 853: val_loss did not improve from 28.16271
196/196 - 33s - loss: 27.1716 - MinusLogProbMetric: 27.1716 - val_loss: 28.1686 - val_MinusLogProbMetric: 28.1686 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 854/1000
2023-10-27 11:37:49.843 
Epoch 854/1000 
	 loss: 27.1712, MinusLogProbMetric: 27.1712, val_loss: 28.1696, val_MinusLogProbMetric: 28.1696

Epoch 854: val_loss did not improve from 28.16271
196/196 - 35s - loss: 27.1712 - MinusLogProbMetric: 27.1712 - val_loss: 28.1696 - val_MinusLogProbMetric: 28.1696 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 855/1000
2023-10-27 11:38:25.041 
Epoch 855/1000 
	 loss: 27.1724, MinusLogProbMetric: 27.1724, val_loss: 28.1743, val_MinusLogProbMetric: 28.1743

Epoch 855: val_loss did not improve from 28.16271
196/196 - 35s - loss: 27.1724 - MinusLogProbMetric: 27.1724 - val_loss: 28.1743 - val_MinusLogProbMetric: 28.1743 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 856/1000
2023-10-27 11:38:57.994 
Epoch 856/1000 
	 loss: 27.1705, MinusLogProbMetric: 27.1705, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 856: val_loss did not improve from 28.16271
196/196 - 33s - loss: 27.1705 - MinusLogProbMetric: 27.1705 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 857/1000
2023-10-27 11:39:30.134 
Epoch 857/1000 
	 loss: 27.1714, MinusLogProbMetric: 27.1714, val_loss: 28.1703, val_MinusLogProbMetric: 28.1703

Epoch 857: val_loss did not improve from 28.16271
196/196 - 32s - loss: 27.1714 - MinusLogProbMetric: 27.1714 - val_loss: 28.1703 - val_MinusLogProbMetric: 28.1703 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 858/1000
2023-10-27 11:40:02.570 
Epoch 858/1000 
	 loss: 27.1710, MinusLogProbMetric: 27.1710, val_loss: 28.1692, val_MinusLogProbMetric: 28.1692

Epoch 858: val_loss did not improve from 28.16271
196/196 - 32s - loss: 27.1710 - MinusLogProbMetric: 27.1710 - val_loss: 28.1692 - val_MinusLogProbMetric: 28.1692 - lr: 3.1250e-05 - 32s/epoch - 165ms/step
Epoch 859/1000
2023-10-27 11:40:36.914 
Epoch 859/1000 
	 loss: 27.1728, MinusLogProbMetric: 27.1728, val_loss: 28.1776, val_MinusLogProbMetric: 28.1776

Epoch 859: val_loss did not improve from 28.16271
196/196 - 34s - loss: 27.1728 - MinusLogProbMetric: 27.1728 - val_loss: 28.1776 - val_MinusLogProbMetric: 28.1776 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 860/1000
2023-10-27 11:41:15.164 
Epoch 860/1000 
	 loss: 27.1721, MinusLogProbMetric: 27.1721, val_loss: 28.1664, val_MinusLogProbMetric: 28.1664

Epoch 860: val_loss did not improve from 28.16271
196/196 - 38s - loss: 27.1721 - MinusLogProbMetric: 27.1721 - val_loss: 28.1664 - val_MinusLogProbMetric: 28.1664 - lr: 3.1250e-05 - 38s/epoch - 195ms/step
Epoch 861/1000
2023-10-27 11:41:47.900 
Epoch 861/1000 
	 loss: 27.1722, MinusLogProbMetric: 27.1722, val_loss: 28.1653, val_MinusLogProbMetric: 28.1653

Epoch 861: val_loss did not improve from 28.16271
196/196 - 33s - loss: 27.1722 - MinusLogProbMetric: 27.1722 - val_loss: 28.1653 - val_MinusLogProbMetric: 28.1653 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 862/1000
2023-10-27 11:42:20.073 
Epoch 862/1000 
	 loss: 27.1697, MinusLogProbMetric: 27.1697, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 862: val_loss did not improve from 28.16271
196/196 - 32s - loss: 27.1697 - MinusLogProbMetric: 27.1697 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 863/1000
2023-10-27 11:42:52.644 
Epoch 863/1000 
	 loss: 27.1712, MinusLogProbMetric: 27.1712, val_loss: 28.1748, val_MinusLogProbMetric: 28.1748

Epoch 863: val_loss did not improve from 28.16271
196/196 - 33s - loss: 27.1712 - MinusLogProbMetric: 27.1712 - val_loss: 28.1748 - val_MinusLogProbMetric: 28.1748 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 864/1000
2023-10-27 11:43:29.333 
Epoch 864/1000 
	 loss: 27.1717, MinusLogProbMetric: 27.1717, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 864: val_loss did not improve from 28.16271
196/196 - 37s - loss: 27.1717 - MinusLogProbMetric: 27.1717 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 3.1250e-05 - 37s/epoch - 187ms/step
Epoch 865/1000
2023-10-27 11:44:06.194 
Epoch 865/1000 
	 loss: 27.1703, MinusLogProbMetric: 27.1703, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 865: val_loss did not improve from 28.16271
196/196 - 37s - loss: 27.1703 - MinusLogProbMetric: 27.1703 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 3.1250e-05 - 37s/epoch - 188ms/step
Epoch 866/1000
2023-10-27 11:44:38.449 
Epoch 866/1000 
	 loss: 27.1624, MinusLogProbMetric: 27.1624, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 866: val_loss did not improve from 28.16271
196/196 - 32s - loss: 27.1624 - MinusLogProbMetric: 27.1624 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 867/1000
2023-10-27 11:45:10.796 
Epoch 867/1000 
	 loss: 27.1620, MinusLogProbMetric: 27.1620, val_loss: 28.1620, val_MinusLogProbMetric: 28.1620

Epoch 867: val_loss improved from 28.16271 to 28.16199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 33s - loss: 27.1620 - MinusLogProbMetric: 27.1620 - val_loss: 28.1620 - val_MinusLogProbMetric: 28.1620 - lr: 1.5625e-05 - 33s/epoch - 168ms/step
Epoch 868/1000
2023-10-27 11:45:43.366 
Epoch 868/1000 
	 loss: 27.1626, MinusLogProbMetric: 27.1626, val_loss: 28.1645, val_MinusLogProbMetric: 28.1645

Epoch 868: val_loss did not improve from 28.16199
196/196 - 32s - loss: 27.1626 - MinusLogProbMetric: 27.1626 - val_loss: 28.1645 - val_MinusLogProbMetric: 28.1645 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 869/1000
2023-10-27 11:46:19.238 
Epoch 869/1000 
	 loss: 27.1620, MinusLogProbMetric: 27.1620, val_loss: 28.1646, val_MinusLogProbMetric: 28.1646

Epoch 869: val_loss did not improve from 28.16199
196/196 - 36s - loss: 27.1620 - MinusLogProbMetric: 27.1620 - val_loss: 28.1646 - val_MinusLogProbMetric: 28.1646 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 870/1000
2023-10-27 11:46:54.990 
Epoch 870/1000 
	 loss: 27.1619, MinusLogProbMetric: 27.1619, val_loss: 28.1620, val_MinusLogProbMetric: 28.1620

Epoch 870: val_loss improved from 28.16199 to 28.16197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 36s - loss: 27.1619 - MinusLogProbMetric: 27.1619 - val_loss: 28.1620 - val_MinusLogProbMetric: 28.1620 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 871/1000
2023-10-27 11:47:27.326 
Epoch 871/1000 
	 loss: 27.1622, MinusLogProbMetric: 27.1622, val_loss: 28.1672, val_MinusLogProbMetric: 28.1672

Epoch 871: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1622 - MinusLogProbMetric: 27.1622 - val_loss: 28.1672 - val_MinusLogProbMetric: 28.1672 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 872/1000
2023-10-27 11:47:59.087 
Epoch 872/1000 
	 loss: 27.1618, MinusLogProbMetric: 27.1618, val_loss: 28.1648, val_MinusLogProbMetric: 28.1648

Epoch 872: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1618 - MinusLogProbMetric: 27.1618 - val_loss: 28.1648 - val_MinusLogProbMetric: 28.1648 - lr: 1.5625e-05 - 32s/epoch - 162ms/step
Epoch 873/1000
2023-10-27 11:48:31.509 
Epoch 873/1000 
	 loss: 27.1623, MinusLogProbMetric: 27.1623, val_loss: 28.1632, val_MinusLogProbMetric: 28.1632

Epoch 873: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1623 - MinusLogProbMetric: 27.1623 - val_loss: 28.1632 - val_MinusLogProbMetric: 28.1632 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 874/1000
2023-10-27 11:49:04.883 
Epoch 874/1000 
	 loss: 27.1616, MinusLogProbMetric: 27.1616, val_loss: 28.1645, val_MinusLogProbMetric: 28.1645

Epoch 874: val_loss did not improve from 28.16197
196/196 - 33s - loss: 27.1616 - MinusLogProbMetric: 27.1616 - val_loss: 28.1645 - val_MinusLogProbMetric: 28.1645 - lr: 1.5625e-05 - 33s/epoch - 170ms/step
Epoch 875/1000
2023-10-27 11:49:39.566 
Epoch 875/1000 
	 loss: 27.1622, MinusLogProbMetric: 27.1622, val_loss: 28.1647, val_MinusLogProbMetric: 28.1647

Epoch 875: val_loss did not improve from 28.16197
196/196 - 35s - loss: 27.1622 - MinusLogProbMetric: 27.1622 - val_loss: 28.1647 - val_MinusLogProbMetric: 28.1647 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 876/1000
2023-10-27 11:50:11.697 
Epoch 876/1000 
	 loss: 27.1618, MinusLogProbMetric: 27.1618, val_loss: 28.1625, val_MinusLogProbMetric: 28.1625

Epoch 876: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1618 - MinusLogProbMetric: 27.1618 - val_loss: 28.1625 - val_MinusLogProbMetric: 28.1625 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 877/1000
2023-10-27 11:50:43.591 
Epoch 877/1000 
	 loss: 27.1621, MinusLogProbMetric: 27.1621, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 877: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1621 - MinusLogProbMetric: 27.1621 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 878/1000
2023-10-27 11:51:15.930 
Epoch 878/1000 
	 loss: 27.1616, MinusLogProbMetric: 27.1616, val_loss: 28.1642, val_MinusLogProbMetric: 28.1642

Epoch 878: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1616 - MinusLogProbMetric: 27.1616 - val_loss: 28.1642 - val_MinusLogProbMetric: 28.1642 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 879/1000
2023-10-27 11:51:51.820 
Epoch 879/1000 
	 loss: 27.1617, MinusLogProbMetric: 27.1617, val_loss: 28.1645, val_MinusLogProbMetric: 28.1645

Epoch 879: val_loss did not improve from 28.16197
196/196 - 36s - loss: 27.1617 - MinusLogProbMetric: 27.1617 - val_loss: 28.1645 - val_MinusLogProbMetric: 28.1645 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 880/1000
2023-10-27 11:52:28.110 
Epoch 880/1000 
	 loss: 27.1624, MinusLogProbMetric: 27.1624, val_loss: 28.1713, val_MinusLogProbMetric: 28.1713

Epoch 880: val_loss did not improve from 28.16197
196/196 - 36s - loss: 27.1624 - MinusLogProbMetric: 27.1624 - val_loss: 28.1713 - val_MinusLogProbMetric: 28.1713 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 881/1000
2023-10-27 11:53:00.048 
Epoch 881/1000 
	 loss: 27.1613, MinusLogProbMetric: 27.1613, val_loss: 28.1636, val_MinusLogProbMetric: 28.1636

Epoch 881: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1613 - MinusLogProbMetric: 27.1613 - val_loss: 28.1636 - val_MinusLogProbMetric: 28.1636 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 882/1000
2023-10-27 11:53:31.666 
Epoch 882/1000 
	 loss: 27.1608, MinusLogProbMetric: 27.1608, val_loss: 28.1642, val_MinusLogProbMetric: 28.1642

Epoch 882: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1608 - MinusLogProbMetric: 27.1608 - val_loss: 28.1642 - val_MinusLogProbMetric: 28.1642 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 883/1000
2023-10-27 11:54:03.716 
Epoch 883/1000 
	 loss: 27.1618, MinusLogProbMetric: 27.1618, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 883: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1618 - MinusLogProbMetric: 27.1618 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 884/1000
2023-10-27 11:54:39.089 
Epoch 884/1000 
	 loss: 27.1617, MinusLogProbMetric: 27.1617, val_loss: 28.1675, val_MinusLogProbMetric: 28.1675

Epoch 884: val_loss did not improve from 28.16197
196/196 - 35s - loss: 27.1617 - MinusLogProbMetric: 27.1617 - val_loss: 28.1675 - val_MinusLogProbMetric: 28.1675 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 885/1000
2023-10-27 11:55:13.711 
Epoch 885/1000 
	 loss: 27.1614, MinusLogProbMetric: 27.1614, val_loss: 28.1628, val_MinusLogProbMetric: 28.1628

Epoch 885: val_loss did not improve from 28.16197
196/196 - 35s - loss: 27.1614 - MinusLogProbMetric: 27.1614 - val_loss: 28.1628 - val_MinusLogProbMetric: 28.1628 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 886/1000
2023-10-27 11:55:46.088 
Epoch 886/1000 
	 loss: 27.1614, MinusLogProbMetric: 27.1614, val_loss: 28.1642, val_MinusLogProbMetric: 28.1642

Epoch 886: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1614 - MinusLogProbMetric: 27.1614 - val_loss: 28.1642 - val_MinusLogProbMetric: 28.1642 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 887/1000
2023-10-27 11:56:18.339 
Epoch 887/1000 
	 loss: 27.1611, MinusLogProbMetric: 27.1611, val_loss: 28.1657, val_MinusLogProbMetric: 28.1657

Epoch 887: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1611 - MinusLogProbMetric: 27.1611 - val_loss: 28.1657 - val_MinusLogProbMetric: 28.1657 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 888/1000
2023-10-27 11:56:50.401 
Epoch 888/1000 
	 loss: 27.1622, MinusLogProbMetric: 27.1622, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 888: val_loss did not improve from 28.16197
196/196 - 32s - loss: 27.1622 - MinusLogProbMetric: 27.1622 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 889/1000
2023-10-27 11:57:25.253 
Epoch 889/1000 
	 loss: 27.1622, MinusLogProbMetric: 27.1622, val_loss: 28.1691, val_MinusLogProbMetric: 28.1691

Epoch 889: val_loss did not improve from 28.16197
196/196 - 35s - loss: 27.1622 - MinusLogProbMetric: 27.1622 - val_loss: 28.1691 - val_MinusLogProbMetric: 28.1691 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 890/1000
2023-10-27 11:57:59.784 
Epoch 890/1000 
	 loss: 27.1623, MinusLogProbMetric: 27.1623, val_loss: 28.1633, val_MinusLogProbMetric: 28.1633

Epoch 890: val_loss did not improve from 28.16197
196/196 - 35s - loss: 27.1623 - MinusLogProbMetric: 27.1623 - val_loss: 28.1633 - val_MinusLogProbMetric: 28.1633 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 891/1000
2023-10-27 11:58:33.123 
Epoch 891/1000 
	 loss: 27.1619, MinusLogProbMetric: 27.1619, val_loss: 28.1674, val_MinusLogProbMetric: 28.1674

Epoch 891: val_loss did not improve from 28.16197
196/196 - 33s - loss: 27.1619 - MinusLogProbMetric: 27.1619 - val_loss: 28.1674 - val_MinusLogProbMetric: 28.1674 - lr: 1.5625e-05 - 33s/epoch - 170ms/step
Epoch 892/1000
2023-10-27 11:59:05.355 
Epoch 892/1000 
	 loss: 27.1611, MinusLogProbMetric: 27.1611, val_loss: 28.1618, val_MinusLogProbMetric: 28.1618

Epoch 892: val_loss improved from 28.16197 to 28.16180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 33s - loss: 27.1611 - MinusLogProbMetric: 27.1611 - val_loss: 28.1618 - val_MinusLogProbMetric: 28.1618 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 893/1000
2023-10-27 11:59:38.267 
Epoch 893/1000 
	 loss: 27.1612, MinusLogProbMetric: 27.1612, val_loss: 28.1634, val_MinusLogProbMetric: 28.1634

Epoch 893: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1612 - MinusLogProbMetric: 27.1612 - val_loss: 28.1634 - val_MinusLogProbMetric: 28.1634 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 894/1000
2023-10-27 12:00:13.902 
Epoch 894/1000 
	 loss: 27.1615, MinusLogProbMetric: 27.1615, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 894: val_loss did not improve from 28.16180
196/196 - 36s - loss: 27.1615 - MinusLogProbMetric: 27.1615 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 895/1000
2023-10-27 12:00:48.977 
Epoch 895/1000 
	 loss: 27.1612, MinusLogProbMetric: 27.1612, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 895: val_loss did not improve from 28.16180
196/196 - 35s - loss: 27.1612 - MinusLogProbMetric: 27.1612 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 896/1000
2023-10-27 12:01:21.626 
Epoch 896/1000 
	 loss: 27.1601, MinusLogProbMetric: 27.1601, val_loss: 28.1618, val_MinusLogProbMetric: 28.1618

Epoch 896: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1601 - MinusLogProbMetric: 27.1601 - val_loss: 28.1618 - val_MinusLogProbMetric: 28.1618 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 897/1000
2023-10-27 12:01:53.964 
Epoch 897/1000 
	 loss: 27.1605, MinusLogProbMetric: 27.1605, val_loss: 28.1667, val_MinusLogProbMetric: 28.1667

Epoch 897: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1605 - MinusLogProbMetric: 27.1605 - val_loss: 28.1667 - val_MinusLogProbMetric: 28.1667 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 898/1000
2023-10-27 12:02:27.251 
Epoch 898/1000 
	 loss: 27.1611, MinusLogProbMetric: 27.1611, val_loss: 28.1649, val_MinusLogProbMetric: 28.1649

Epoch 898: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1611 - MinusLogProbMetric: 27.1611 - val_loss: 28.1649 - val_MinusLogProbMetric: 28.1649 - lr: 1.5625e-05 - 33s/epoch - 170ms/step
Epoch 899/1000
2023-10-27 12:03:02.063 
Epoch 899/1000 
	 loss: 27.1610, MinusLogProbMetric: 27.1610, val_loss: 28.1650, val_MinusLogProbMetric: 28.1650

Epoch 899: val_loss did not improve from 28.16180
196/196 - 35s - loss: 27.1610 - MinusLogProbMetric: 27.1610 - val_loss: 28.1650 - val_MinusLogProbMetric: 28.1650 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 900/1000
2023-10-27 12:03:39.167 
Epoch 900/1000 
	 loss: 27.1614, MinusLogProbMetric: 27.1614, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 900: val_loss did not improve from 28.16180
196/196 - 37s - loss: 27.1614 - MinusLogProbMetric: 27.1614 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 1.5625e-05 - 37s/epoch - 189ms/step
Epoch 901/1000
2023-10-27 12:04:13.313 
Epoch 901/1000 
	 loss: 27.1610, MinusLogProbMetric: 27.1610, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 901: val_loss did not improve from 28.16180
196/196 - 34s - loss: 27.1610 - MinusLogProbMetric: 27.1610 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 1.5625e-05 - 34s/epoch - 174ms/step
Epoch 902/1000
2023-10-27 12:04:45.656 
Epoch 902/1000 
	 loss: 27.1609, MinusLogProbMetric: 27.1609, val_loss: 28.1643, val_MinusLogProbMetric: 28.1643

Epoch 902: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1609 - MinusLogProbMetric: 27.1609 - val_loss: 28.1643 - val_MinusLogProbMetric: 28.1643 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 903/1000
2023-10-27 12:05:18.111 
Epoch 903/1000 
	 loss: 27.1604, MinusLogProbMetric: 27.1604, val_loss: 28.1648, val_MinusLogProbMetric: 28.1648

Epoch 903: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1604 - MinusLogProbMetric: 27.1604 - val_loss: 28.1648 - val_MinusLogProbMetric: 28.1648 - lr: 1.5625e-05 - 32s/epoch - 166ms/step
Epoch 904/1000
2023-10-27 12:05:54.340 
Epoch 904/1000 
	 loss: 27.1603, MinusLogProbMetric: 27.1603, val_loss: 28.1674, val_MinusLogProbMetric: 28.1674

Epoch 904: val_loss did not improve from 28.16180
196/196 - 36s - loss: 27.1603 - MinusLogProbMetric: 27.1603 - val_loss: 28.1674 - val_MinusLogProbMetric: 28.1674 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 905/1000
2023-10-27 12:06:33.037 
Epoch 905/1000 
	 loss: 27.1602, MinusLogProbMetric: 27.1602, val_loss: 28.1672, val_MinusLogProbMetric: 28.1672

Epoch 905: val_loss did not improve from 28.16180
196/196 - 39s - loss: 27.1602 - MinusLogProbMetric: 27.1602 - val_loss: 28.1672 - val_MinusLogProbMetric: 28.1672 - lr: 1.5625e-05 - 39s/epoch - 197ms/step
Epoch 906/1000
2023-10-27 12:07:05.154 
Epoch 906/1000 
	 loss: 27.1607, MinusLogProbMetric: 27.1607, val_loss: 28.1631, val_MinusLogProbMetric: 28.1631

Epoch 906: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1607 - MinusLogProbMetric: 27.1607 - val_loss: 28.1631 - val_MinusLogProbMetric: 28.1631 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 907/1000
2023-10-27 12:07:37.438 
Epoch 907/1000 
	 loss: 27.1601, MinusLogProbMetric: 27.1601, val_loss: 28.1719, val_MinusLogProbMetric: 28.1719

Epoch 907: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1601 - MinusLogProbMetric: 27.1601 - val_loss: 28.1719 - val_MinusLogProbMetric: 28.1719 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 908/1000
2023-10-27 12:08:09.409 
Epoch 908/1000 
	 loss: 27.1610, MinusLogProbMetric: 27.1610, val_loss: 28.1627, val_MinusLogProbMetric: 28.1627

Epoch 908: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1610 - MinusLogProbMetric: 27.1610 - val_loss: 28.1627 - val_MinusLogProbMetric: 28.1627 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 909/1000
2023-10-27 12:08:46.894 
Epoch 909/1000 
	 loss: 27.1617, MinusLogProbMetric: 27.1617, val_loss: 28.1642, val_MinusLogProbMetric: 28.1642

Epoch 909: val_loss did not improve from 28.16180
196/196 - 37s - loss: 27.1617 - MinusLogProbMetric: 27.1617 - val_loss: 28.1642 - val_MinusLogProbMetric: 28.1642 - lr: 1.5625e-05 - 37s/epoch - 191ms/step
Epoch 910/1000
2023-10-27 12:09:23.317 
Epoch 910/1000 
	 loss: 27.1604, MinusLogProbMetric: 27.1604, val_loss: 28.1667, val_MinusLogProbMetric: 28.1667

Epoch 910: val_loss did not improve from 28.16180
196/196 - 36s - loss: 27.1604 - MinusLogProbMetric: 27.1604 - val_loss: 28.1667 - val_MinusLogProbMetric: 28.1667 - lr: 1.5625e-05 - 36s/epoch - 186ms/step
Epoch 911/1000
2023-10-27 12:09:55.398 
Epoch 911/1000 
	 loss: 27.1600, MinusLogProbMetric: 27.1600, val_loss: 28.1683, val_MinusLogProbMetric: 28.1683

Epoch 911: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1600 - MinusLogProbMetric: 27.1600 - val_loss: 28.1683 - val_MinusLogProbMetric: 28.1683 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 912/1000
2023-10-27 12:10:28.282 
Epoch 912/1000 
	 loss: 27.1602, MinusLogProbMetric: 27.1602, val_loss: 28.1644, val_MinusLogProbMetric: 28.1644

Epoch 912: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1602 - MinusLogProbMetric: 27.1602 - val_loss: 28.1644 - val_MinusLogProbMetric: 28.1644 - lr: 1.5625e-05 - 33s/epoch - 168ms/step
Epoch 913/1000
2023-10-27 12:11:00.448 
Epoch 913/1000 
	 loss: 27.1601, MinusLogProbMetric: 27.1601, val_loss: 28.1693, val_MinusLogProbMetric: 28.1693

Epoch 913: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1601 - MinusLogProbMetric: 27.1601 - val_loss: 28.1693 - val_MinusLogProbMetric: 28.1693 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 914/1000
2023-10-27 12:11:35.909 
Epoch 914/1000 
	 loss: 27.1595, MinusLogProbMetric: 27.1595, val_loss: 28.1627, val_MinusLogProbMetric: 28.1627

Epoch 914: val_loss did not improve from 28.16180
196/196 - 35s - loss: 27.1595 - MinusLogProbMetric: 27.1595 - val_loss: 28.1627 - val_MinusLogProbMetric: 28.1627 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 915/1000
2023-10-27 12:12:11.598 
Epoch 915/1000 
	 loss: 27.1607, MinusLogProbMetric: 27.1607, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 915: val_loss did not improve from 28.16180
196/196 - 36s - loss: 27.1607 - MinusLogProbMetric: 27.1607 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 1.5625e-05 - 36s/epoch - 182ms/step
Epoch 916/1000
2023-10-27 12:12:44.329 
Epoch 916/1000 
	 loss: 27.1604, MinusLogProbMetric: 27.1604, val_loss: 28.1692, val_MinusLogProbMetric: 28.1692

Epoch 916: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1604 - MinusLogProbMetric: 27.1604 - val_loss: 28.1692 - val_MinusLogProbMetric: 28.1692 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 917/1000
2023-10-27 12:13:16.620 
Epoch 917/1000 
	 loss: 27.1603, MinusLogProbMetric: 27.1603, val_loss: 28.1649, val_MinusLogProbMetric: 28.1649

Epoch 917: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1603 - MinusLogProbMetric: 27.1603 - val_loss: 28.1649 - val_MinusLogProbMetric: 28.1649 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 918/1000
2023-10-27 12:13:49.160 
Epoch 918/1000 
	 loss: 27.1602, MinusLogProbMetric: 27.1602, val_loss: 28.1667, val_MinusLogProbMetric: 28.1667

Epoch 918: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1602 - MinusLogProbMetric: 27.1602 - val_loss: 28.1667 - val_MinusLogProbMetric: 28.1667 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 919/1000
2023-10-27 12:14:26.973 
Epoch 919/1000 
	 loss: 27.1603, MinusLogProbMetric: 27.1603, val_loss: 28.1649, val_MinusLogProbMetric: 28.1649

Epoch 919: val_loss did not improve from 28.16180
196/196 - 38s - loss: 27.1603 - MinusLogProbMetric: 27.1603 - val_loss: 28.1649 - val_MinusLogProbMetric: 28.1649 - lr: 1.5625e-05 - 38s/epoch - 193ms/step
Epoch 920/1000
2023-10-27 12:15:05.295 
Epoch 920/1000 
	 loss: 27.1611, MinusLogProbMetric: 27.1611, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 920: val_loss did not improve from 28.16180
196/196 - 38s - loss: 27.1611 - MinusLogProbMetric: 27.1611 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 1.5625e-05 - 38s/epoch - 195ms/step
Epoch 921/1000
2023-10-27 12:15:37.833 
Epoch 921/1000 
	 loss: 27.1600, MinusLogProbMetric: 27.1600, val_loss: 28.1648, val_MinusLogProbMetric: 28.1648

Epoch 921: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1600 - MinusLogProbMetric: 27.1600 - val_loss: 28.1648 - val_MinusLogProbMetric: 28.1648 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 922/1000
2023-10-27 12:16:10.370 
Epoch 922/1000 
	 loss: 27.1600, MinusLogProbMetric: 27.1600, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 922: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1600 - MinusLogProbMetric: 27.1600 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 923/1000
2023-10-27 12:16:43.086 
Epoch 923/1000 
	 loss: 27.1592, MinusLogProbMetric: 27.1592, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 923: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1592 - MinusLogProbMetric: 27.1592 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 924/1000
2023-10-27 12:17:21.082 
Epoch 924/1000 
	 loss: 27.1599, MinusLogProbMetric: 27.1599, val_loss: 28.1664, val_MinusLogProbMetric: 28.1664

Epoch 924: val_loss did not improve from 28.16180
196/196 - 38s - loss: 27.1599 - MinusLogProbMetric: 27.1599 - val_loss: 28.1664 - val_MinusLogProbMetric: 28.1664 - lr: 1.5625e-05 - 38s/epoch - 194ms/step
Epoch 925/1000
2023-10-27 12:17:54.829 
Epoch 925/1000 
	 loss: 27.1601, MinusLogProbMetric: 27.1601, val_loss: 28.1675, val_MinusLogProbMetric: 28.1675

Epoch 925: val_loss did not improve from 28.16180
196/196 - 34s - loss: 27.1601 - MinusLogProbMetric: 27.1601 - val_loss: 28.1675 - val_MinusLogProbMetric: 28.1675 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 926/1000
2023-10-27 12:18:27.064 
Epoch 926/1000 
	 loss: 27.1606, MinusLogProbMetric: 27.1606, val_loss: 28.1630, val_MinusLogProbMetric: 28.1630

Epoch 926: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1606 - MinusLogProbMetric: 27.1606 - val_loss: 28.1630 - val_MinusLogProbMetric: 28.1630 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 927/1000
2023-10-27 12:18:59.207 
Epoch 927/1000 
	 loss: 27.1606, MinusLogProbMetric: 27.1606, val_loss: 28.1648, val_MinusLogProbMetric: 28.1648

Epoch 927: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1606 - MinusLogProbMetric: 27.1606 - val_loss: 28.1648 - val_MinusLogProbMetric: 28.1648 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 928/1000
2023-10-27 12:19:32.301 
Epoch 928/1000 
	 loss: 27.1597, MinusLogProbMetric: 27.1597, val_loss: 28.1637, val_MinusLogProbMetric: 28.1637

Epoch 928: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1597 - MinusLogProbMetric: 27.1597 - val_loss: 28.1637 - val_MinusLogProbMetric: 28.1637 - lr: 1.5625e-05 - 33s/epoch - 169ms/step
Epoch 929/1000
2023-10-27 12:20:08.144 
Epoch 929/1000 
	 loss: 27.1600, MinusLogProbMetric: 27.1600, val_loss: 28.1693, val_MinusLogProbMetric: 28.1693

Epoch 929: val_loss did not improve from 28.16180
196/196 - 36s - loss: 27.1600 - MinusLogProbMetric: 27.1600 - val_loss: 28.1693 - val_MinusLogProbMetric: 28.1693 - lr: 1.5625e-05 - 36s/epoch - 183ms/step
Epoch 930/1000
2023-10-27 12:20:45.244 
Epoch 930/1000 
	 loss: 27.1603, MinusLogProbMetric: 27.1603, val_loss: 28.1665, val_MinusLogProbMetric: 28.1665

Epoch 930: val_loss did not improve from 28.16180
196/196 - 37s - loss: 27.1603 - MinusLogProbMetric: 27.1603 - val_loss: 28.1665 - val_MinusLogProbMetric: 28.1665 - lr: 1.5625e-05 - 37s/epoch - 189ms/step
Epoch 931/1000
2023-10-27 12:21:17.285 
Epoch 931/1000 
	 loss: 27.1594, MinusLogProbMetric: 27.1594, val_loss: 28.1637, val_MinusLogProbMetric: 28.1637

Epoch 931: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1594 - MinusLogProbMetric: 27.1594 - val_loss: 28.1637 - val_MinusLogProbMetric: 28.1637 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 932/1000
2023-10-27 12:21:50.086 
Epoch 932/1000 
	 loss: 27.1600, MinusLogProbMetric: 27.1600, val_loss: 28.1703, val_MinusLogProbMetric: 28.1703

Epoch 932: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1600 - MinusLogProbMetric: 27.1600 - val_loss: 28.1703 - val_MinusLogProbMetric: 28.1703 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 933/1000
2023-10-27 12:22:22.504 
Epoch 933/1000 
	 loss: 27.1606, MinusLogProbMetric: 27.1606, val_loss: 28.1655, val_MinusLogProbMetric: 28.1655

Epoch 933: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1606 - MinusLogProbMetric: 27.1606 - val_loss: 28.1655 - val_MinusLogProbMetric: 28.1655 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 934/1000
2023-10-27 12:23:00.899 
Epoch 934/1000 
	 loss: 27.1593, MinusLogProbMetric: 27.1593, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 934: val_loss did not improve from 28.16180
196/196 - 38s - loss: 27.1593 - MinusLogProbMetric: 27.1593 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 1.5625e-05 - 38s/epoch - 196ms/step
Epoch 935/1000
2023-10-27 12:23:36.352 
Epoch 935/1000 
	 loss: 27.1596, MinusLogProbMetric: 27.1596, val_loss: 28.1684, val_MinusLogProbMetric: 28.1684

Epoch 935: val_loss did not improve from 28.16180
196/196 - 35s - loss: 27.1596 - MinusLogProbMetric: 27.1596 - val_loss: 28.1684 - val_MinusLogProbMetric: 28.1684 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 936/1000
2023-10-27 12:24:08.483 
Epoch 936/1000 
	 loss: 27.1597, MinusLogProbMetric: 27.1597, val_loss: 28.1647, val_MinusLogProbMetric: 28.1647

Epoch 936: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1597 - MinusLogProbMetric: 27.1597 - val_loss: 28.1647 - val_MinusLogProbMetric: 28.1647 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 937/1000
2023-10-27 12:24:40.651 
Epoch 937/1000 
	 loss: 27.1605, MinusLogProbMetric: 27.1605, val_loss: 28.1631, val_MinusLogProbMetric: 28.1631

Epoch 937: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1605 - MinusLogProbMetric: 27.1605 - val_loss: 28.1631 - val_MinusLogProbMetric: 28.1631 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 938/1000
2023-10-27 12:25:13.255 
Epoch 938/1000 
	 loss: 27.1601, MinusLogProbMetric: 27.1601, val_loss: 28.1639, val_MinusLogProbMetric: 28.1639

Epoch 938: val_loss did not improve from 28.16180
196/196 - 33s - loss: 27.1601 - MinusLogProbMetric: 27.1601 - val_loss: 28.1639 - val_MinusLogProbMetric: 28.1639 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 939/1000
2023-10-27 12:25:47.244 
Epoch 939/1000 
	 loss: 27.1591, MinusLogProbMetric: 27.1591, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 939: val_loss did not improve from 28.16180
196/196 - 34s - loss: 27.1591 - MinusLogProbMetric: 27.1591 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 940/1000
2023-10-27 12:26:24.449 
Epoch 940/1000 
	 loss: 27.1589, MinusLogProbMetric: 27.1589, val_loss: 28.1660, val_MinusLogProbMetric: 28.1660

Epoch 940: val_loss did not improve from 28.16180
196/196 - 37s - loss: 27.1589 - MinusLogProbMetric: 27.1589 - val_loss: 28.1660 - val_MinusLogProbMetric: 28.1660 - lr: 1.5625e-05 - 37s/epoch - 190ms/step
Epoch 941/1000
2023-10-27 12:26:56.780 
Epoch 941/1000 
	 loss: 27.1595, MinusLogProbMetric: 27.1595, val_loss: 28.1658, val_MinusLogProbMetric: 28.1658

Epoch 941: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1595 - MinusLogProbMetric: 27.1595 - val_loss: 28.1658 - val_MinusLogProbMetric: 28.1658 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 942/1000
2023-10-27 12:27:28.648 
Epoch 942/1000 
	 loss: 27.1603, MinusLogProbMetric: 27.1603, val_loss: 28.1638, val_MinusLogProbMetric: 28.1638

Epoch 942: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1603 - MinusLogProbMetric: 27.1603 - val_loss: 28.1638 - val_MinusLogProbMetric: 28.1638 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 943/1000
2023-10-27 12:28:00.770 
Epoch 943/1000 
	 loss: 27.1552, MinusLogProbMetric: 27.1552, val_loss: 28.1639, val_MinusLogProbMetric: 28.1639

Epoch 943: val_loss did not improve from 28.16180
196/196 - 32s - loss: 27.1552 - MinusLogProbMetric: 27.1552 - val_loss: 28.1639 - val_MinusLogProbMetric: 28.1639 - lr: 7.8125e-06 - 32s/epoch - 164ms/step
Epoch 944/1000
2023-10-27 12:28:36.405 
Epoch 944/1000 
	 loss: 27.1552, MinusLogProbMetric: 27.1552, val_loss: 28.1638, val_MinusLogProbMetric: 28.1638

Epoch 944: val_loss did not improve from 28.16180
196/196 - 36s - loss: 27.1552 - MinusLogProbMetric: 27.1552 - val_loss: 28.1638 - val_MinusLogProbMetric: 28.1638 - lr: 7.8125e-06 - 36s/epoch - 182ms/step
Epoch 945/1000
2023-10-27 12:29:12.430 
Epoch 945/1000 
	 loss: 27.1548, MinusLogProbMetric: 27.1548, val_loss: 28.1633, val_MinusLogProbMetric: 28.1633

Epoch 945: val_loss did not improve from 28.16180
196/196 - 36s - loss: 27.1548 - MinusLogProbMetric: 27.1548 - val_loss: 28.1633 - val_MinusLogProbMetric: 28.1633 - lr: 7.8125e-06 - 36s/epoch - 184ms/step
Epoch 946/1000
2023-10-27 12:29:44.494 
Epoch 946/1000 
	 loss: 27.1549, MinusLogProbMetric: 27.1549, val_loss: 28.1615, val_MinusLogProbMetric: 28.1615

Epoch 946: val_loss improved from 28.16180 to 28.16147, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 33s - loss: 27.1549 - MinusLogProbMetric: 27.1549 - val_loss: 28.1615 - val_MinusLogProbMetric: 28.1615 - lr: 7.8125e-06 - 33s/epoch - 167ms/step
Epoch 947/1000
2023-10-27 12:30:16.940 
Epoch 947/1000 
	 loss: 27.1550, MinusLogProbMetric: 27.1550, val_loss: 28.1628, val_MinusLogProbMetric: 28.1628

Epoch 947: val_loss did not improve from 28.16147
196/196 - 32s - loss: 27.1550 - MinusLogProbMetric: 27.1550 - val_loss: 28.1628 - val_MinusLogProbMetric: 28.1628 - lr: 7.8125e-06 - 32s/epoch - 163ms/step
Epoch 948/1000
2023-10-27 12:30:50.243 
Epoch 948/1000 
	 loss: 27.1550, MinusLogProbMetric: 27.1550, val_loss: 28.1628, val_MinusLogProbMetric: 28.1628

Epoch 948: val_loss did not improve from 28.16147
196/196 - 33s - loss: 27.1550 - MinusLogProbMetric: 27.1550 - val_loss: 28.1628 - val_MinusLogProbMetric: 28.1628 - lr: 7.8125e-06 - 33s/epoch - 170ms/step
Epoch 949/1000
2023-10-27 12:31:26.995 
Epoch 949/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1649, val_MinusLogProbMetric: 28.1649

Epoch 949: val_loss did not improve from 28.16147
196/196 - 37s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1649 - val_MinusLogProbMetric: 28.1649 - lr: 7.8125e-06 - 37s/epoch - 187ms/step
Epoch 950/1000
2023-10-27 12:32:02.544 
Epoch 950/1000 
	 loss: 27.1548, MinusLogProbMetric: 27.1548, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 950: val_loss did not improve from 28.16147
196/196 - 36s - loss: 27.1548 - MinusLogProbMetric: 27.1548 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 7.8125e-06 - 36s/epoch - 181ms/step
Epoch 951/1000
2023-10-27 12:32:36.667 
Epoch 951/1000 
	 loss: 27.1547, MinusLogProbMetric: 27.1547, val_loss: 28.1640, val_MinusLogProbMetric: 28.1640

Epoch 951: val_loss did not improve from 28.16147
196/196 - 34s - loss: 27.1547 - MinusLogProbMetric: 27.1547 - val_loss: 28.1640 - val_MinusLogProbMetric: 28.1640 - lr: 7.8125e-06 - 34s/epoch - 174ms/step
Epoch 952/1000
2023-10-27 12:33:08.888 
Epoch 952/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 952: val_loss did not improve from 28.16147
196/196 - 32s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 7.8125e-06 - 32s/epoch - 164ms/step
Epoch 953/1000
2023-10-27 12:33:41.478 
Epoch 953/1000 
	 loss: 27.1547, MinusLogProbMetric: 27.1547, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 953: val_loss did not improve from 28.16147
196/196 - 33s - loss: 27.1547 - MinusLogProbMetric: 27.1547 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 7.8125e-06 - 33s/epoch - 166ms/step
Epoch 954/1000
2023-10-27 12:34:17.923 
Epoch 954/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1631, val_MinusLogProbMetric: 28.1631

Epoch 954: val_loss did not improve from 28.16147
196/196 - 36s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1631 - val_MinusLogProbMetric: 28.1631 - lr: 7.8125e-06 - 36s/epoch - 186ms/step
Epoch 955/1000
2023-10-27 12:34:54.515 
Epoch 955/1000 
	 loss: 27.1550, MinusLogProbMetric: 27.1550, val_loss: 28.1643, val_MinusLogProbMetric: 28.1643

Epoch 955: val_loss did not improve from 28.16147
196/196 - 37s - loss: 27.1550 - MinusLogProbMetric: 27.1550 - val_loss: 28.1643 - val_MinusLogProbMetric: 28.1643 - lr: 7.8125e-06 - 37s/epoch - 187ms/step
Epoch 956/1000
2023-10-27 12:35:27.469 
Epoch 956/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1612, val_MinusLogProbMetric: 28.1612

Epoch 956: val_loss improved from 28.16147 to 28.16124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_396/weights/best_weights.h5
196/196 - 34s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1612 - val_MinusLogProbMetric: 28.1612 - lr: 7.8125e-06 - 34s/epoch - 172ms/step
Epoch 957/1000
2023-10-27 12:36:03.917 
Epoch 957/1000 
	 loss: 27.1551, MinusLogProbMetric: 27.1551, val_loss: 28.1619, val_MinusLogProbMetric: 28.1619

Epoch 957: val_loss did not improve from 28.16124
196/196 - 36s - loss: 27.1551 - MinusLogProbMetric: 27.1551 - val_loss: 28.1619 - val_MinusLogProbMetric: 28.1619 - lr: 7.8125e-06 - 36s/epoch - 182ms/step
Epoch 958/1000
2023-10-27 12:36:36.548 
Epoch 958/1000 
	 loss: 27.1548, MinusLogProbMetric: 27.1548, val_loss: 28.1628, val_MinusLogProbMetric: 28.1628

Epoch 958: val_loss did not improve from 28.16124
196/196 - 33s - loss: 27.1548 - MinusLogProbMetric: 27.1548 - val_loss: 28.1628 - val_MinusLogProbMetric: 28.1628 - lr: 7.8125e-06 - 33s/epoch - 166ms/step
Epoch 959/1000
2023-10-27 12:37:15.963 
Epoch 959/1000 
	 loss: 27.1547, MinusLogProbMetric: 27.1547, val_loss: 28.1630, val_MinusLogProbMetric: 28.1630

Epoch 959: val_loss did not improve from 28.16124
196/196 - 39s - loss: 27.1547 - MinusLogProbMetric: 27.1547 - val_loss: 28.1630 - val_MinusLogProbMetric: 28.1630 - lr: 7.8125e-06 - 39s/epoch - 201ms/step
Epoch 960/1000
2023-10-27 12:37:49.660 
Epoch 960/1000 
	 loss: 27.1550, MinusLogProbMetric: 27.1550, val_loss: 28.1625, val_MinusLogProbMetric: 28.1625

Epoch 960: val_loss did not improve from 28.16124
196/196 - 34s - loss: 27.1550 - MinusLogProbMetric: 27.1550 - val_loss: 28.1625 - val_MinusLogProbMetric: 28.1625 - lr: 7.8125e-06 - 34s/epoch - 172ms/step
Epoch 961/1000
2023-10-27 12:38:28.766 
Epoch 961/1000 
	 loss: 27.1548, MinusLogProbMetric: 27.1548, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 961: val_loss did not improve from 28.16124
196/196 - 39s - loss: 27.1548 - MinusLogProbMetric: 27.1548 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 7.8125e-06 - 39s/epoch - 199ms/step
Epoch 962/1000
2023-10-27 12:39:01.151 
Epoch 962/1000 
	 loss: 27.1547, MinusLogProbMetric: 27.1547, val_loss: 28.1622, val_MinusLogProbMetric: 28.1622

Epoch 962: val_loss did not improve from 28.16124
196/196 - 32s - loss: 27.1547 - MinusLogProbMetric: 27.1547 - val_loss: 28.1622 - val_MinusLogProbMetric: 28.1622 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 963/1000
2023-10-27 12:39:35.680 
Epoch 963/1000 
	 loss: 27.1548, MinusLogProbMetric: 27.1548, val_loss: 28.1645, val_MinusLogProbMetric: 28.1645

Epoch 963: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1548 - MinusLogProbMetric: 27.1548 - val_loss: 28.1645 - val_MinusLogProbMetric: 28.1645 - lr: 7.8125e-06 - 35s/epoch - 176ms/step
Epoch 964/1000
2023-10-27 12:40:08.087 
Epoch 964/1000 
	 loss: 27.1548, MinusLogProbMetric: 27.1548, val_loss: 28.1651, val_MinusLogProbMetric: 28.1651

Epoch 964: val_loss did not improve from 28.16124
196/196 - 32s - loss: 27.1548 - MinusLogProbMetric: 27.1548 - val_loss: 28.1651 - val_MinusLogProbMetric: 28.1651 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 965/1000
2023-10-27 12:40:44.759 
Epoch 965/1000 
	 loss: 27.1549, MinusLogProbMetric: 27.1549, val_loss: 28.1673, val_MinusLogProbMetric: 28.1673

Epoch 965: val_loss did not improve from 28.16124
196/196 - 37s - loss: 27.1549 - MinusLogProbMetric: 27.1549 - val_loss: 28.1673 - val_MinusLogProbMetric: 28.1673 - lr: 7.8125e-06 - 37s/epoch - 187ms/step
Epoch 966/1000
2023-10-27 12:41:18.445 
Epoch 966/1000 
	 loss: 27.1545, MinusLogProbMetric: 27.1545, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 966: val_loss did not improve from 28.16124
196/196 - 34s - loss: 27.1545 - MinusLogProbMetric: 27.1545 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 7.8125e-06 - 34s/epoch - 172ms/step
Epoch 967/1000
2023-10-27 12:41:52.221 
Epoch 967/1000 
	 loss: 27.1547, MinusLogProbMetric: 27.1547, val_loss: 28.1615, val_MinusLogProbMetric: 28.1615

Epoch 967: val_loss did not improve from 28.16124
196/196 - 34s - loss: 27.1547 - MinusLogProbMetric: 27.1547 - val_loss: 28.1615 - val_MinusLogProbMetric: 28.1615 - lr: 7.8125e-06 - 34s/epoch - 172ms/step
Epoch 968/1000
2023-10-27 12:42:24.444 
Epoch 968/1000 
	 loss: 27.1547, MinusLogProbMetric: 27.1547, val_loss: 28.1634, val_MinusLogProbMetric: 28.1634

Epoch 968: val_loss did not improve from 28.16124
196/196 - 32s - loss: 27.1547 - MinusLogProbMetric: 27.1547 - val_loss: 28.1634 - val_MinusLogProbMetric: 28.1634 - lr: 7.8125e-06 - 32s/epoch - 164ms/step
Epoch 969/1000
2023-10-27 12:42:59.196 
Epoch 969/1000 
	 loss: 27.1545, MinusLogProbMetric: 27.1545, val_loss: 28.1622, val_MinusLogProbMetric: 28.1622

Epoch 969: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1545 - MinusLogProbMetric: 27.1545 - val_loss: 28.1622 - val_MinusLogProbMetric: 28.1622 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 970/1000
2023-10-27 12:43:31.468 
Epoch 970/1000 
	 loss: 27.1542, MinusLogProbMetric: 27.1542, val_loss: 28.1640, val_MinusLogProbMetric: 28.1640

Epoch 970: val_loss did not improve from 28.16124
196/196 - 32s - loss: 27.1542 - MinusLogProbMetric: 27.1542 - val_loss: 28.1640 - val_MinusLogProbMetric: 28.1640 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 971/1000
2023-10-27 12:44:08.952 
Epoch 971/1000 
	 loss: 27.1545, MinusLogProbMetric: 27.1545, val_loss: 28.1632, val_MinusLogProbMetric: 28.1632

Epoch 971: val_loss did not improve from 28.16124
196/196 - 37s - loss: 27.1545 - MinusLogProbMetric: 27.1545 - val_loss: 28.1632 - val_MinusLogProbMetric: 28.1632 - lr: 7.8125e-06 - 37s/epoch - 191ms/step
Epoch 972/1000
2023-10-27 12:44:41.855 
Epoch 972/1000 
	 loss: 27.1546, MinusLogProbMetric: 27.1546, val_loss: 28.1679, val_MinusLogProbMetric: 28.1679

Epoch 972: val_loss did not improve from 28.16124
196/196 - 33s - loss: 27.1546 - MinusLogProbMetric: 27.1546 - val_loss: 28.1679 - val_MinusLogProbMetric: 28.1679 - lr: 7.8125e-06 - 33s/epoch - 168ms/step
Epoch 973/1000
2023-10-27 12:45:17.349 
Epoch 973/1000 
	 loss: 27.1549, MinusLogProbMetric: 27.1549, val_loss: 28.1625, val_MinusLogProbMetric: 28.1625

Epoch 973: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1549 - MinusLogProbMetric: 27.1549 - val_loss: 28.1625 - val_MinusLogProbMetric: 28.1625 - lr: 7.8125e-06 - 35s/epoch - 181ms/step
Epoch 974/1000
2023-10-27 12:45:51.138 
Epoch 974/1000 
	 loss: 27.1549, MinusLogProbMetric: 27.1549, val_loss: 28.1628, val_MinusLogProbMetric: 28.1628

Epoch 974: val_loss did not improve from 28.16124
196/196 - 34s - loss: 27.1549 - MinusLogProbMetric: 27.1549 - val_loss: 28.1628 - val_MinusLogProbMetric: 28.1628 - lr: 7.8125e-06 - 34s/epoch - 172ms/step
Epoch 975/1000
2023-10-27 12:46:23.793 
Epoch 975/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1638, val_MinusLogProbMetric: 28.1638

Epoch 975: val_loss did not improve from 28.16124
196/196 - 33s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1638 - val_MinusLogProbMetric: 28.1638 - lr: 7.8125e-06 - 33s/epoch - 167ms/step
Epoch 976/1000
2023-10-27 12:46:56.825 
Epoch 976/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1643, val_MinusLogProbMetric: 28.1643

Epoch 976: val_loss did not improve from 28.16124
196/196 - 33s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1643 - val_MinusLogProbMetric: 28.1643 - lr: 7.8125e-06 - 33s/epoch - 169ms/step
Epoch 977/1000
2023-10-27 12:47:34.880 
Epoch 977/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1636, val_MinusLogProbMetric: 28.1636

Epoch 977: val_loss did not improve from 28.16124
196/196 - 38s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1636 - val_MinusLogProbMetric: 28.1636 - lr: 7.8125e-06 - 38s/epoch - 194ms/step
Epoch 978/1000
2023-10-27 12:48:12.339 
Epoch 978/1000 
	 loss: 27.1548, MinusLogProbMetric: 27.1548, val_loss: 28.1623, val_MinusLogProbMetric: 28.1623

Epoch 978: val_loss did not improve from 28.16124
196/196 - 37s - loss: 27.1548 - MinusLogProbMetric: 27.1548 - val_loss: 28.1623 - val_MinusLogProbMetric: 28.1623 - lr: 7.8125e-06 - 37s/epoch - 191ms/step
Epoch 979/1000
2023-10-27 12:48:51.746 
Epoch 979/1000 
	 loss: 27.1549, MinusLogProbMetric: 27.1549, val_loss: 28.1625, val_MinusLogProbMetric: 28.1625

Epoch 979: val_loss did not improve from 28.16124
196/196 - 39s - loss: 27.1549 - MinusLogProbMetric: 27.1549 - val_loss: 28.1625 - val_MinusLogProbMetric: 28.1625 - lr: 7.8125e-06 - 39s/epoch - 201ms/step
Epoch 980/1000
2023-10-27 12:49:25.999 
Epoch 980/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1638, val_MinusLogProbMetric: 28.1638

Epoch 980: val_loss did not improve from 28.16124
196/196 - 34s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1638 - val_MinusLogProbMetric: 28.1638 - lr: 7.8125e-06 - 34s/epoch - 175ms/step
Epoch 981/1000
2023-10-27 12:50:06.384 
Epoch 981/1000 
	 loss: 27.1547, MinusLogProbMetric: 27.1547, val_loss: 28.1624, val_MinusLogProbMetric: 28.1624

Epoch 981: val_loss did not improve from 28.16124
196/196 - 40s - loss: 27.1547 - MinusLogProbMetric: 27.1547 - val_loss: 28.1624 - val_MinusLogProbMetric: 28.1624 - lr: 7.8125e-06 - 40s/epoch - 206ms/step
Epoch 982/1000
2023-10-27 12:50:47.894 
Epoch 982/1000 
	 loss: 27.1541, MinusLogProbMetric: 27.1541, val_loss: 28.1651, val_MinusLogProbMetric: 28.1651

Epoch 982: val_loss did not improve from 28.16124
196/196 - 42s - loss: 27.1541 - MinusLogProbMetric: 27.1541 - val_loss: 28.1651 - val_MinusLogProbMetric: 28.1651 - lr: 7.8125e-06 - 42s/epoch - 212ms/step
Epoch 983/1000
2023-10-27 12:51:30.172 
Epoch 983/1000 
	 loss: 27.1542, MinusLogProbMetric: 27.1542, val_loss: 28.1635, val_MinusLogProbMetric: 28.1635

Epoch 983: val_loss did not improve from 28.16124
196/196 - 42s - loss: 27.1542 - MinusLogProbMetric: 27.1542 - val_loss: 28.1635 - val_MinusLogProbMetric: 28.1635 - lr: 7.8125e-06 - 42s/epoch - 216ms/step
Epoch 984/1000
2023-10-27 12:52:12.078 
Epoch 984/1000 
	 loss: 27.1541, MinusLogProbMetric: 27.1541, val_loss: 28.1642, val_MinusLogProbMetric: 28.1642

Epoch 984: val_loss did not improve from 28.16124
196/196 - 42s - loss: 27.1541 - MinusLogProbMetric: 27.1541 - val_loss: 28.1642 - val_MinusLogProbMetric: 28.1642 - lr: 7.8125e-06 - 42s/epoch - 214ms/step
Epoch 985/1000
2023-10-27 12:52:53.525 
Epoch 985/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1639, val_MinusLogProbMetric: 28.1639

Epoch 985: val_loss did not improve from 28.16124
196/196 - 41s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1639 - val_MinusLogProbMetric: 28.1639 - lr: 7.8125e-06 - 41s/epoch - 212ms/step
Epoch 986/1000
2023-10-27 12:53:36.837 
Epoch 986/1000 
	 loss: 27.1543, MinusLogProbMetric: 27.1543, val_loss: 28.1638, val_MinusLogProbMetric: 28.1638

Epoch 986: val_loss did not improve from 28.16124
196/196 - 43s - loss: 27.1543 - MinusLogProbMetric: 27.1543 - val_loss: 28.1638 - val_MinusLogProbMetric: 28.1638 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 987/1000
2023-10-27 12:54:20.573 
Epoch 987/1000 
	 loss: 27.1543, MinusLogProbMetric: 27.1543, val_loss: 28.1643, val_MinusLogProbMetric: 28.1643

Epoch 987: val_loss did not improve from 28.16124
196/196 - 44s - loss: 27.1543 - MinusLogProbMetric: 27.1543 - val_loss: 28.1643 - val_MinusLogProbMetric: 28.1643 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 988/1000
2023-10-27 12:55:03.115 
Epoch 988/1000 
	 loss: 27.1539, MinusLogProbMetric: 27.1539, val_loss: 28.1615, val_MinusLogProbMetric: 28.1615

Epoch 988: val_loss did not improve from 28.16124
196/196 - 43s - loss: 27.1539 - MinusLogProbMetric: 27.1539 - val_loss: 28.1615 - val_MinusLogProbMetric: 28.1615 - lr: 7.8125e-06 - 43s/epoch - 217ms/step
Epoch 989/1000
2023-10-27 12:55:42.784 
Epoch 989/1000 
	 loss: 27.1545, MinusLogProbMetric: 27.1545, val_loss: 28.1647, val_MinusLogProbMetric: 28.1647

Epoch 989: val_loss did not improve from 28.16124
196/196 - 40s - loss: 27.1545 - MinusLogProbMetric: 27.1545 - val_loss: 28.1647 - val_MinusLogProbMetric: 28.1647 - lr: 7.8125e-06 - 40s/epoch - 202ms/step
Epoch 990/1000
2023-10-27 12:56:21.306 
Epoch 990/1000 
	 loss: 27.1546, MinusLogProbMetric: 27.1546, val_loss: 28.1645, val_MinusLogProbMetric: 28.1645

Epoch 990: val_loss did not improve from 28.16124
196/196 - 39s - loss: 27.1546 - MinusLogProbMetric: 27.1546 - val_loss: 28.1645 - val_MinusLogProbMetric: 28.1645 - lr: 7.8125e-06 - 39s/epoch - 197ms/step
Epoch 991/1000
2023-10-27 12:56:59.499 
Epoch 991/1000 
	 loss: 27.1543, MinusLogProbMetric: 27.1543, val_loss: 28.1647, val_MinusLogProbMetric: 28.1647

Epoch 991: val_loss did not improve from 28.16124
196/196 - 38s - loss: 27.1543 - MinusLogProbMetric: 27.1543 - val_loss: 28.1647 - val_MinusLogProbMetric: 28.1647 - lr: 7.8125e-06 - 38s/epoch - 195ms/step
Epoch 992/1000
2023-10-27 12:57:35.219 
Epoch 992/1000 
	 loss: 27.1540, MinusLogProbMetric: 27.1540, val_loss: 28.1635, val_MinusLogProbMetric: 28.1635

Epoch 992: val_loss did not improve from 28.16124
196/196 - 36s - loss: 27.1540 - MinusLogProbMetric: 27.1540 - val_loss: 28.1635 - val_MinusLogProbMetric: 28.1635 - lr: 7.8125e-06 - 36s/epoch - 182ms/step
Epoch 993/1000
2023-10-27 12:58:09.242 
Epoch 993/1000 
	 loss: 27.1538, MinusLogProbMetric: 27.1538, val_loss: 28.1642, val_MinusLogProbMetric: 28.1642

Epoch 993: val_loss did not improve from 28.16124
196/196 - 34s - loss: 27.1538 - MinusLogProbMetric: 27.1538 - val_loss: 28.1642 - val_MinusLogProbMetric: 28.1642 - lr: 7.8125e-06 - 34s/epoch - 174ms/step
Epoch 994/1000
2023-10-27 12:58:43.079 
Epoch 994/1000 
	 loss: 27.1542, MinusLogProbMetric: 27.1542, val_loss: 28.1655, val_MinusLogProbMetric: 28.1655

Epoch 994: val_loss did not improve from 28.16124
196/196 - 34s - loss: 27.1542 - MinusLogProbMetric: 27.1542 - val_loss: 28.1655 - val_MinusLogProbMetric: 28.1655 - lr: 7.8125e-06 - 34s/epoch - 173ms/step
Epoch 995/1000
2023-10-27 12:59:20.550 
Epoch 995/1000 
	 loss: 27.1539, MinusLogProbMetric: 27.1539, val_loss: 28.1632, val_MinusLogProbMetric: 28.1632

Epoch 995: val_loss did not improve from 28.16124
196/196 - 37s - loss: 27.1539 - MinusLogProbMetric: 27.1539 - val_loss: 28.1632 - val_MinusLogProbMetric: 28.1632 - lr: 7.8125e-06 - 37s/epoch - 191ms/step
Epoch 996/1000
2023-10-27 13:00:00.128 
Epoch 996/1000 
	 loss: 27.1540, MinusLogProbMetric: 27.1540, val_loss: 28.1660, val_MinusLogProbMetric: 28.1660

Epoch 996: val_loss did not improve from 28.16124
196/196 - 40s - loss: 27.1540 - MinusLogProbMetric: 27.1540 - val_loss: 28.1660 - val_MinusLogProbMetric: 28.1660 - lr: 7.8125e-06 - 40s/epoch - 202ms/step
Epoch 997/1000
2023-10-27 13:00:33.380 
Epoch 997/1000 
	 loss: 27.1540, MinusLogProbMetric: 27.1540, val_loss: 28.1640, val_MinusLogProbMetric: 28.1640

Epoch 997: val_loss did not improve from 28.16124
196/196 - 33s - loss: 27.1540 - MinusLogProbMetric: 27.1540 - val_loss: 28.1640 - val_MinusLogProbMetric: 28.1640 - lr: 7.8125e-06 - 33s/epoch - 170ms/step
Epoch 998/1000
2023-10-27 13:01:07.940 
Epoch 998/1000 
	 loss: 27.1542, MinusLogProbMetric: 27.1542, val_loss: 28.1638, val_MinusLogProbMetric: 28.1638

Epoch 998: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1542 - MinusLogProbMetric: 27.1542 - val_loss: 28.1638 - val_MinusLogProbMetric: 28.1638 - lr: 7.8125e-06 - 35s/epoch - 176ms/step
Epoch 999/1000
2023-10-27 13:01:46.470 
Epoch 999/1000 
	 loss: 27.1538, MinusLogProbMetric: 27.1538, val_loss: 28.1632, val_MinusLogProbMetric: 28.1632

Epoch 999: val_loss did not improve from 28.16124
196/196 - 39s - loss: 27.1538 - MinusLogProbMetric: 27.1538 - val_loss: 28.1632 - val_MinusLogProbMetric: 28.1632 - lr: 7.8125e-06 - 39s/epoch - 197ms/step
Epoch 1000/1000
2023-10-27 13:02:21.734 
Epoch 1000/1000 
	 loss: 27.1544, MinusLogProbMetric: 27.1544, val_loss: 28.1651, val_MinusLogProbMetric: 28.1651

Epoch 1000: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1544 - MinusLogProbMetric: 27.1544 - val_loss: 28.1651 - val_MinusLogProbMetric: 28.1651 - lr: 7.8125e-06 - 35s/epoch - 180ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 933.
Model trained in 39549.46 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.69 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.89 s.
===========
Run 396/720 done in 39557.08 s.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_399/ already exists.
Skipping it.
===========
Run 399/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_400/ already exists.
Skipping it.
===========
Run 400/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_401/ already exists.
Skipping it.
===========
Run 401/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_402/ already exists.
Skipping it.
===========
Run 402/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_403/ already exists.
Skipping it.
===========
Run 403/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_404/ already exists.
Skipping it.
===========
Run 404/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_407/ already exists.
Skipping it.
===========
Run 407/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_408/ already exists.
Skipping it.
===========
Run 408/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_409/ already exists.
Skipping it.
===========
Run 409/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_410/ already exists.
Skipping it.
===========
Run 410/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_411/ already exists.
Skipping it.
===========
Run 411/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_412/ already exists.
Skipping it.
===========
Run 412/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_413/ already exists.
Skipping it.
===========
Run 413/720 already exists. Skipping it.
===========

===========
Generating train data for run 414.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_360"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_361 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7ff0ab5e12d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef85786980>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef85786980>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0ab539cc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feee5c12980>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feee5c12ef0>, <keras.callbacks.ModelCheckpoint object at 0x7feee5c12fb0>, <keras.callbacks.EarlyStopping object at 0x7feee5c13220>, <keras.callbacks.ReduceLROnPlateau object at 0x7feee5c13250>, <keras.callbacks.TerminateOnNaN object at 0x7feee5c12e90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:02:29.489700
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:04:49.827 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11056.3760, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 140s - loss: nan - MinusLogProbMetric: 11056.3760 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 140s/epoch - 716ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 414.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_371"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_372 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7ff0c03eb970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fefe7052ce0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fefe7052ce0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef7ee202b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff79cd05bd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff79cd06140>, <keras.callbacks.ModelCheckpoint object at 0x7ff79cd06200>, <keras.callbacks.EarlyStopping object at 0x7ff79cd06470>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff79cd064a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff79cd060e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:04:58.551514
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:07:15.235 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10882.5322, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 10882.5322 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 136s/epoch - 696ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 414.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_382"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_383 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7ff2721a0040>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff3081345b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff3081345b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff066aee050>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feeed17d6f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feeed17dfc0>, <keras.callbacks.ModelCheckpoint object at 0x7feeed17cdc0>, <keras.callbacks.EarlyStopping object at 0x7feeed17d510>, <keras.callbacks.ReduceLROnPlateau object at 0x7feeed17d900>, <keras.callbacks.TerminateOnNaN object at 0x7ff03489b460>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:07:24.245482
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:09:32.405 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10977.7812, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 10977.7812 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 128s/epoch - 653ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 414.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_393"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_394 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7fefa5465de0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fee30432950>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fee30432950>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef57e894e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fefddda4640>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fefddda4bb0>, <keras.callbacks.ModelCheckpoint object at 0x7fefddda4c70>, <keras.callbacks.EarlyStopping object at 0x7fefddda4ee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fefddda4f10>, <keras.callbacks.TerminateOnNaN object at 0x7fefddda4b50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:09:40.792092
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:12:03.778 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11015.6104, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 11015.6104 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 143s/epoch - 729ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 414.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_404"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_405 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7ff0bb1c3f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff1281068c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff1281068c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff1ec733c10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1ec74e380>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff1ec74e8f0>, <keras.callbacks.ModelCheckpoint object at 0x7ff1ec74e9b0>, <keras.callbacks.EarlyStopping object at 0x7ff1ec74ec20>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff1ec74ec50>, <keras.callbacks.TerminateOnNaN object at 0x7ff1ec74e890>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:12:11.511382
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:14:21.567 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11030.7109, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 11030.7109 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 130s/epoch - 663ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 414.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_415"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_416 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7ff0c3717d90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feee4e79b10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feee4e79b10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0bb3bf8b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0bb375ff0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0bb376560>, <keras.callbacks.ModelCheckpoint object at 0x7ff0bb376620>, <keras.callbacks.EarlyStopping object at 0x7ff0bb376890>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0bb3768c0>, <keras.callbacks.TerminateOnNaN object at 0x7ff0bb376500>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:14:30.253323
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:16:50.525 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11035.6309, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 140s - loss: nan - MinusLogProbMetric: 11035.6309 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 140s/epoch - 715ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 414.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_426"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_427 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7fefde12a410>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef00c57700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef00c57700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feef4e9de70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fefddfadea0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fefddfae410>, <keras.callbacks.ModelCheckpoint object at 0x7fefddfae4d0>, <keras.callbacks.EarlyStopping object at 0x7fefddfae740>, <keras.callbacks.ReduceLROnPlateau object at 0x7fefddfae770>, <keras.callbacks.TerminateOnNaN object at 0x7fefddfae3b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:16:57.313702
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:19:25.031 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11043.9355, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 148s - loss: nan - MinusLogProbMetric: 11043.9355 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 148s/epoch - 753ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 414.
===========
Train data generated in 0.48 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_437"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_438 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7ff0673c0c40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0e862d480>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0e862d480>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef57e93010>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0c0bb99c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0c0bbbe50>, <keras.callbacks.ModelCheckpoint object at 0x7ff0c0bbb520>, <keras.callbacks.EarlyStopping object at 0x7ff0c0bba4a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0c0bbbdc0>, <keras.callbacks.TerminateOnNaN object at 0x7ff0c0bba530>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:19:35.381173
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:21:46.816 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11033.8535, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 11033.8535 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 131s/epoch - 669ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 414.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_448"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_449 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7fef1cecfdc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feee5f44250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feee5f44250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef0106e290>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee282e2cb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee282e3220>, <keras.callbacks.ModelCheckpoint object at 0x7fee282e32e0>, <keras.callbacks.EarlyStopping object at 0x7fee282e3550>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee282e3580>, <keras.callbacks.TerminateOnNaN object at 0x7fee282e31c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:21:55.255108
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:24:36.250 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11043.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 11043.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 161s/epoch - 821ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 414.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_459"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_460 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7ff71dd4fc10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feed593a110>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feed593a110>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0aac2e950>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fefddb8c730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fefddb8cca0>, <keras.callbacks.ModelCheckpoint object at 0x7fefddb8cd60>, <keras.callbacks.EarlyStopping object at 0x7fefddb8cfd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fefddb8d000>, <keras.callbacks.TerminateOnNaN object at 0x7fefddb8cc40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:24:44.101852
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:27:07.930 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11039.0693, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 11039.0693 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 144s/epoch - 733ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 414.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_470"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_471 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7ff3287291e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef7eab97e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef7eab97e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff32872bc40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff76a82b730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feef4d6cca0>, <keras.callbacks.ModelCheckpoint object at 0x7feef4d6c700>, <keras.callbacks.EarlyStopping object at 0x7feef4d6e650>, <keras.callbacks.ReduceLROnPlateau object at 0x7feef4d6e9e0>, <keras.callbacks.TerminateOnNaN object at 0x7feef4d6c340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-27 13:27:22.189915
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 13:29:48.906 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11041.0166, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 11041.0166 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 147s/epoch - 748ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 414/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_415/ already exists.
Skipping it.
===========
Run 415/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_416/ already exists.
Skipping it.
===========
Run 416/720 already exists. Skipping it.
===========

===========
Generating train data for run 417.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_417/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_417
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_476"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_477 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7fefe58b5870>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef7ff9a5c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef7ff9a5c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fefe68e74c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee2071c940>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_417/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee2071ceb0>, <keras.callbacks.ModelCheckpoint object at 0x7fee2071cf70>, <keras.callbacks.EarlyStopping object at 0x7fee2071d1e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee2071d210>, <keras.callbacks.TerminateOnNaN object at 0x7fee2071ce50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_417/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 417/720 with hyperparameters:
timestamp = 2023-10-27 13:29:53.196481
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 13:30:35.596809: F tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:504] ptxas returned an error during compilation of ptx to sass: 'INTERNAL: Failed to launch ptxas'  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.
