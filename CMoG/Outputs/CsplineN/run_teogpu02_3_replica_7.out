2023-10-30 14:00:40.393348: Importing os...
2023-10-30 14:00:40.393409: Importing sys...
2023-10-30 14:00:40.393423: Importing and initializing argparse...
Visible devices: [3]
2023-10-30 14:00:40.409785: Importing timer from timeit...
2023-10-30 14:00:40.410403: Setting env variables for tf import (only device [3] will be available)...
2023-10-30 14:00:40.410449: Importing numpy...
2023-10-30 14:00:40.600831: Importing pandas...
2023-10-30 14:00:40.793515: Importing shutil...
2023-10-30 14:00:40.793540: Importing subprocess...
2023-10-30 14:00:40.793548: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-30 14:00:42.876522: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-30 14:00:43.207773: Importing textwrap...
2023-10-30 14:00:43.207796: Importing timeit...
2023-10-30 14:00:43.207805: Importing traceback...
2023-10-30 14:00:43.207812: Importing typing...
2023-10-30 14:00:43.207821: Setting tf configs...
2023-10-30 14:00:43.338589: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-30 14:00:44.350909: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

===========
Generating train data for run 333.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1321920   
 r)                                                              
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f43140deb90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4304173fd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4304173fd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f428478dea0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f42847e3f40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f42845244f0>, <keras.callbacks.ModelCheckpoint object at 0x7f4284524640>, <keras.callbacks.EarlyStopping object at 0x7f4284524850>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4284524880>, <keras.callbacks.TerminateOnNaN object at 0x7f42845245b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:00:50.605101
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:02:41.827 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 111s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 111s/epoch - 567ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 333.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f46b782ca00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46b7357400>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46b7357400>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd84474f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46b6e06830>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46b6e06da0>, <keras.callbacks.ModelCheckpoint object at 0x7f46b6e06e60>, <keras.callbacks.EarlyStopping object at 0x7f46b6e070d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46b6e07100>, <keras.callbacks.TerminateOnNaN object at 0x7f46b6e06d40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:02:48.546410
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:04:36.497 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 108s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 108s/epoch - 551ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 333.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f422477bfa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f41805b4940>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f41805b4940>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4244404460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46b8f40be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46b8f41150>, <keras.callbacks.ModelCheckpoint object at 0x7f46b8f41210>, <keras.callbacks.EarlyStopping object at 0x7f46b8f41480>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46b8f414b0>, <keras.callbacks.TerminateOnNaN object at 0x7f46b8f410f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:04:43.708554
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:06:36.078 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 112s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 112s/epoch - 572ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 333.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f3fdebe0ee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4028225510>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4028225510>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f42047ad210>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46b869a050>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46b869a5c0>, <keras.callbacks.ModelCheckpoint object at 0x7f46b869a680>, <keras.callbacks.EarlyStopping object at 0x7f46b869a8f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46b869a920>, <keras.callbacks.TerminateOnNaN object at 0x7f46b869a560>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:06:42.588579
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:08:36.092 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 113s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 113s/epoch - 579ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 333.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f3fdefe5930>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46b806b880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46b806b880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f42647de050>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46b80a1fc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46b80a2530>, <keras.callbacks.ModelCheckpoint object at 0x7f46b80a25f0>, <keras.callbacks.EarlyStopping object at 0x7f46b80a2860>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46b80a2890>, <keras.callbacks.TerminateOnNaN object at 0x7f46b80a24d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:08:43.063174
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7f40a8466dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:10:40.545 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 117s/epoch - 599ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 333.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f3fdec8bc10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4264779810>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4264779810>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f408871ee00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fdec76da0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fdec77310>, <keras.callbacks.ModelCheckpoint object at 0x7f3fdec773d0>, <keras.callbacks.EarlyStopping object at 0x7f3fdec77640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fdec77670>, <keras.callbacks.TerminateOnNaN object at 0x7f3fdec772b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:10:47.994802
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f40a831ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:12:47.836 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 120s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 120s/epoch - 610ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 333.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f41c45d2620>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f412c1996f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f412c1996f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f42641481f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f426411c430>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f426411c9a0>, <keras.callbacks.ModelCheckpoint object at 0x7f426411ca60>, <keras.callbacks.EarlyStopping object at 0x7f426411ccd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f426411cd00>, <keras.callbacks.TerminateOnNaN object at 0x7f426411c940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:12:55.015036
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:14:50.174 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 115s/epoch - 586ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 333.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f3fd3f73970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd08b8220>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd08b8220>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd2172a70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fd0452470>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fd04529e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fd0452aa0>, <keras.callbacks.EarlyStopping object at 0x7f3fd0452d10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fd0452d40>, <keras.callbacks.TerminateOnNaN object at 0x7f3fd0452980>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:14:57.202249
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:16:57.954 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 121s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 121s/epoch - 615ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 333.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f3fd3afb580>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f406031abf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f406031abf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd90194b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fd3d5b820>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fd3d593c0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fd3d594e0>, <keras.callbacks.EarlyStopping object at 0x7f3fd3d5a8f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fd3d5a530>, <keras.callbacks.TerminateOnNaN object at 0x7f3fd3d59120>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:17:05.836027
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:19:10.767 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 125s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 125s/epoch - 636ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 333.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f3fcba87520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4686432b00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4686432b00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fcba87ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4686100730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4686100ca0>, <keras.callbacks.ModelCheckpoint object at 0x7f4686100d60>, <keras.callbacks.EarlyStopping object at 0x7f4686100fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4686101000>, <keras.callbacks.TerminateOnNaN object at 0x7f4686100c40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:19:18.338673
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:21:21.369 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 123s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 123s/epoch - 626ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 333.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f40a87a7850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f42046dd720>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f42046dd720>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fdf016110>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f40a83ae830>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f40a83af1c0>, <keras.callbacks.ModelCheckpoint object at 0x7f40a83aecb0>, <keras.callbacks.EarlyStopping object at 0x7f40a83ae590>, <keras.callbacks.ReduceLROnPlateau object at 0x7f40a83ac610>, <keras.callbacks.TerminateOnNaN object at 0x7f40a83aea70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-30 14:21:27.491417
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:23:19.785 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 112s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 112s/epoch - 573ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 333/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

===========
Generating train data for run 353.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f3fc9893b50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd3ef6020>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd3ef6020>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4684036a70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fc9ea8670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fc9ea8be0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fc9ea8ca0>, <keras.callbacks.EarlyStopping object at 0x7f3fc9ea8f10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fc9ea8f40>, <keras.callbacks.TerminateOnNaN object at 0x7f3fc9ea8b80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:23:23.823307
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:24:19.944 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 56s/epoch - 286ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 353.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_132"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_133 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f4685c3fe80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb0542b60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb0542b60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fbb5458a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f468575e4d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f468575ea40>, <keras.callbacks.ModelCheckpoint object at 0x7f468575eb00>, <keras.callbacks.EarlyStopping object at 0x7f468575ed70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f468575eda0>, <keras.callbacks.TerminateOnNaN object at 0x7f468575e9e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:24:23.825026
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:25:21.954 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 58s/epoch - 296ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 353.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_138"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_139 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f3fc83a7010>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb93f9030>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb93f9030>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fba43f580>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fc8ea4c10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fc8ea5180>, <keras.callbacks.ModelCheckpoint object at 0x7f3fc8ea5240>, <keras.callbacks.EarlyStopping object at 0x7f3fc8ea54b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fc8ea54e0>, <keras.callbacks.TerminateOnNaN object at 0x7f3fc8ea5120>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:25:26.015749
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:26:28.588 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 63s/epoch - 319ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 353.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_144"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_145 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f3fb04959f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb0495a80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb0495a80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fb0584c10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fc9911ae0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fc9912050>, <keras.callbacks.ModelCheckpoint object at 0x7f3fc9912110>, <keras.callbacks.EarlyStopping object at 0x7f3fc9912380>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fc99123b0>, <keras.callbacks.TerminateOnNaN object at 0x7f3fc9911ff0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:26:32.530719
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:27:28.160 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 56s/epoch - 283ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 353.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_150"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_151 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f468526f280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f46852bbeb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f46852bbeb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46850b9b70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4684ee5930>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4684ee5ea0>, <keras.callbacks.ModelCheckpoint object at 0x7f4684ee5f60>, <keras.callbacks.EarlyStopping object at 0x7f4684ee61d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4684ee6200>, <keras.callbacks.TerminateOnNaN object at 0x7f4684ee5e40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:27:31.712708
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:28:33.153 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 61s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 61s/epoch - 313ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 353.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_156"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_157 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f3fb0aa66e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee06a3760>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee06a3760>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fb295ffa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fc1fa73d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fc1fa7940>, <keras.callbacks.ModelCheckpoint object at 0x7f3fc1fa7a00>, <keras.callbacks.EarlyStopping object at 0x7f3fc1fa7c70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fc1fa7ca0>, <keras.callbacks.TerminateOnNaN object at 0x7f3fc1fa78e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:28:36.093862
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:29:31.822 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 56s/epoch - 284ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 353.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_162"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_163 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f3fc07a8e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc07a9bd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc07a9bd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f4684d43fa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f468613bca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f468613ac20>, <keras.callbacks.ModelCheckpoint object at 0x7f468613b040>, <keras.callbacks.EarlyStopping object at 0x7f468613a740>, <keras.callbacks.ReduceLROnPlateau object at 0x7f468613aad0>, <keras.callbacks.TerminateOnNaN object at 0x7f468613af20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:29:37.650995
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:30:35.794 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 58s/epoch - 296ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 353.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_168"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_169 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f3ee0c9e5c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3edbf2fc40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3edbf2fc40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fc8ea7a30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fb3b48cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fb3b49240>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb3b49300>, <keras.callbacks.EarlyStopping object at 0x7f3fb3b49570>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb3b495a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3fb3b491e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:30:39.661122
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:31:39.844 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 60s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 60s/epoch - 306ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 353.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_174"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_175 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f3fb0696e60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fbbe29120>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fbbe29120>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fc90b7d30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fb2435060>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fb24355d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb2435690>, <keras.callbacks.EarlyStopping object at 0x7f3fb2435900>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb2435930>, <keras.callbacks.TerminateOnNaN object at 0x7f3fb2435570>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:31:43.627570
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:32:39.467 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 56s/epoch - 285ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 353.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_180"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_181 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f3fb89cbe80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb2434c70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb2434c70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fba666800>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fd1393c40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fb2b101f0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb2b102b0>, <keras.callbacks.EarlyStopping object at 0x7f3fb2b10520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb2b10550>, <keras.callbacks.TerminateOnNaN object at 0x7f3fb2b10190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:32:43.074677
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:33:42.242 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 59s/epoch - 301ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 353.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_186"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_187 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f3fb339ae00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc0bf7e20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc0bf7e20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fb14bce50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fb0330220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fb0331300>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb0330f40>, <keras.callbacks.EarlyStopping object at 0x7f3fb0331990>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb0331b10>, <keras.callbacks.TerminateOnNaN object at 0x7f3fb0331090>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-30 14:33:46.229554
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:34:42.963 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 57s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 57s/epoch - 289ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 353/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

===========
Generating train data for run 365.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_198 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f3ee216f640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4008761e40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4008761e40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f46b6e38040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46b6e38340>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46b6e3b790>, <keras.callbacks.ModelCheckpoint object at 0x7f46b6e39ed0>, <keras.callbacks.EarlyStopping object at 0x7f46b6e3bc40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46b6e386a0>, <keras.callbacks.TerminateOnNaN object at 0x7f46b6e38640>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:34:55.252116
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:36:59.123 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 124s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 124s/epoch - 631ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 365.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_208"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_209 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f40cc2d76a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fcaf4a7a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fcaf4a7a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f426456f130>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3f0ba9e8c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3f0ba9ee30>, <keras.callbacks.ModelCheckpoint object at 0x7f3f0ba9eef0>, <keras.callbacks.EarlyStopping object at 0x7f3f0ba9f160>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3f0ba9f190>, <keras.callbacks.TerminateOnNaN object at 0x7f3f0ba9edd0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:37:06.455855
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:39:12.831 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 126s/epoch - 644ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0001111111111111111.
===========
Generating train data for run 365.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_219"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_220 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f3ee17c8040>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee14c51b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee14c51b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fb0f9b970>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f41802c18d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f41802c3010>, <keras.callbacks.ModelCheckpoint object at 0x7f41802c1600>, <keras.callbacks.EarlyStopping object at 0x7f41802c17e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f41802c16c0>, <keras.callbacks.TerminateOnNaN object at 0x7f41802c2fe0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:39:20.667414
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:41:27.582 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 127s/epoch - 646ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 3.703703703703703e-05.
===========
Generating train data for run 365.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_230"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_231 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f41e479bfd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f42642befb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f42642befb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fbb3de530>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f41e47c3040>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f41e47c35b0>, <keras.callbacks.ModelCheckpoint object at 0x7f41e47c3670>, <keras.callbacks.EarlyStopping object at 0x7f41e47c38e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f41e47c3910>, <keras.callbacks.TerminateOnNaN object at 0x7f41e47c3550>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:41:35.185983
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:43:45.467 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 130s/epoch - 664ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.2345679012345677e-05.
===========
Generating train data for run 365.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_241"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_242 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f3fb9b34f70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fca615d20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fca615d20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3f0a7d7a00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f406020e6b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f406020e7a0>, <keras.callbacks.ModelCheckpoint object at 0x7f406020e350>, <keras.callbacks.EarlyStopping object at 0x7f406020ebc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f406020e500>, <keras.callbacks.TerminateOnNaN object at 0x7f406020f6a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:43:53.457993
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:45:56.734 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 123s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 123s/epoch - 628ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.115226337448558e-06.
===========
Generating train data for run 365.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_252"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_253 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f3f0aa5fe20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc1253f10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc1253f10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd018dfc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ee3c55000>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3ee3c55570>, <keras.callbacks.ModelCheckpoint object at 0x7f3ee3c55630>, <keras.callbacks.EarlyStopping object at 0x7f3ee3c558a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3ee3c558d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3ee3c55510>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:46:04.273298
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:48:13.904 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 129s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 129s/epoch - 660ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.3717421124828526e-06.
===========
Generating train data for run 365.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_263"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_264 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f4224383f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f41802a9fc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f41802a9fc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd20cb6a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fb9d43310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fb9d405e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb9d42080>, <keras.callbacks.EarlyStopping object at 0x7f3fb9d438e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb9d43550>, <keras.callbacks.TerminateOnNaN object at 0x7f3fb9d40790>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:48:21.311187
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:50:26.754 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 125s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 125s/epoch - 639ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.572473708276175e-07.
===========
Generating train data for run 365.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_274"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_275 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f3f0948b520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb0580340>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb0580340>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fcb98e860>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fb9f421a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fb9f42710>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb9f427d0>, <keras.callbacks.EarlyStopping object at 0x7f3fb9f42a40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb9f42a70>, <keras.callbacks.TerminateOnNaN object at 0x7f3fb9f426b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:50:33.726413
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:52:42.561 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 129s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 129s/epoch - 656ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.524157902758725e-07.
===========
Generating train data for run 365.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_285"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_286 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f3fc86a7940>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd941d000>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd941d000>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f402815bbb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fd88c39a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fd88c1ab0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fd88c1e10>, <keras.callbacks.EarlyStopping object at 0x7f3fd88c3ee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fd88c0f10>, <keras.callbacks.TerminateOnNaN object at 0x7f3fd88c1c00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:52:49.993251
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:54:59.141 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 129s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 129s/epoch - 658ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 5.0805263425290834e-08.
===========
Generating train data for run 365.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_296"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_297 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f3ee0e03940>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd3dd2590>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd3dd2590>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd948ff40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fb9576950>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fb9576ec0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb9576f80>, <keras.callbacks.EarlyStopping object at 0x7f3fb95771f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb9577220>, <keras.callbacks.TerminateOnNaN object at 0x7f3fb9576e60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:55:06.717381
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:57:13.657 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 127s/epoch - 646ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.6935087808430278e-08.
===========
Generating train data for run 365.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_307"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_308 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f3fb2009a80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc094c5e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc094c5e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f418acefd00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fc07bbd60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46b9678310>, <keras.callbacks.ModelCheckpoint object at 0x7f46b96783d0>, <keras.callbacks.EarlyStopping object at 0x7f46b9678640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46b9678670>, <keras.callbacks.TerminateOnNaN object at 0x7f46b96782b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-30 14:57:20.351710
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:59:28.119 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 128s/epoch - 651ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 5.645029269476759e-09.
===========
Run 365/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

===========
Generating train data for run 379.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_313"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_314 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f3ee3af9a80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc1cf8880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc1cf8880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ed9710a30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3f09766320>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3f09766890>, <keras.callbacks.ModelCheckpoint object at 0x7f3f09766950>, <keras.callbacks.EarlyStopping object at 0x7f3f09766bc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3f09766bf0>, <keras.callbacks.TerminateOnNaN object at 0x7f3f09766830>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 14:59:33.396170
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:00:44.830 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 71s/epoch - 364ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 379.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_319"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_320 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f3fb13d4100>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fbacdf550>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fbacdf550>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3eda13f1c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fb2668a90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fb2669000>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb26690c0>, <keras.callbacks.EarlyStopping object at 0x7f3fb2669330>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb2669360>, <keras.callbacks.TerminateOnNaN object at 0x7f3fb2668fa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:00:49.733698
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:01:59.143 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 69s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 69s/epoch - 353ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 379.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_325"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_326 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f467b887be0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f467367ec50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f467367ec50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e69f37940>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46732febc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46732ff130>, <keras.callbacks.ModelCheckpoint object at 0x7f46732ff1f0>, <keras.callbacks.EarlyStopping object at 0x7f46732ff460>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46732ff490>, <keras.callbacks.TerminateOnNaN object at 0x7f46732ff0d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:02:04.435344
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:03:18.734 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 74s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 74s/epoch - 378ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 379.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_331"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_332 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f3e737c3f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e73a60400>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e73a60400>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e9b944ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e73783490>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e73783a00>, <keras.callbacks.ModelCheckpoint object at 0x7f3e73783ac0>, <keras.callbacks.EarlyStopping object at 0x7f3e73783d30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e73783d60>, <keras.callbacks.TerminateOnNaN object at 0x7f3e737839a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:03:23.618911
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:04:38.548 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 75s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 75s/epoch - 381ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 379.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_337"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_338 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f3ee2006620>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc882c6a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fc882c6a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fb1842050>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ee0b002e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3ee0b02e60>, <keras.callbacks.ModelCheckpoint object at 0x7f3ee0b016c0>, <keras.callbacks.EarlyStopping object at 0x7f3ee0b01f00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3ee0b01e10>, <keras.callbacks.TerminateOnNaN object at 0x7f3ee0b01060>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:04:43.571307
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:05:53.764 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 70s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 70s/epoch - 357ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 379.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_343"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_344 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f464abf7af0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f464a675e70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f464a675e70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e402f3040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f464a201030>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f464a2015a0>, <keras.callbacks.ModelCheckpoint object at 0x7f464a201660>, <keras.callbacks.EarlyStopping object at 0x7f464a2018d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f464a201900>, <keras.callbacks.TerminateOnNaN object at 0x7f464a201540>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:05:58.204757
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:07:09.220 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 71s/epoch - 362ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 379.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_349"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_350 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f3fc0f95ab0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee1f16200>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee1f16200>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ee1f8fa00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4684bba470>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4684bba9e0>, <keras.callbacks.ModelCheckpoint object at 0x7f4684bbaaa0>, <keras.callbacks.EarlyStopping object at 0x7f4684bbad10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4684bbad40>, <keras.callbacks.TerminateOnNaN object at 0x7f4684bba980>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:07:14.141356
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:08:22.229 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 68s/epoch - 346ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 379.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_355"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_356 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f3e98ecbdc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e6d25ffd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e6d25ffd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ed8933d30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e581043d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e58104940>, <keras.callbacks.ModelCheckpoint object at 0x7f3e58104a00>, <keras.callbacks.EarlyStopping object at 0x7f3e58104c70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e58104ca0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e581048e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:08:26.015524
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:09:43.603 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 78s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 78s/epoch - 395ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 379.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_361"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_362 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f3f0b4e1060>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ed8b100a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ed8b100a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f464ac635e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ee216e8c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fc1cf9e40>, <keras.callbacks.ModelCheckpoint object at 0x7f3fc1cfbee0>, <keras.callbacks.EarlyStopping object at 0x7f3fc1cf96f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fc1cfa830>, <keras.callbacks.TerminateOnNaN object at 0x7f3fc1cf94b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:09:48.602709
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:11:02.992 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 74s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 74s/epoch - 379ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 379.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_367"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_368 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f3e6d6e0310>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e18f233d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e18f233d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ea1e6b160>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e730a3940>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e730a3eb0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e730a3f70>, <keras.callbacks.EarlyStopping object at 0x7f3e730a3e80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e730a3e50>, <keras.callbacks.TerminateOnNaN object at 0x7f3e7309c220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:11:07.797770
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:12:17.884 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 70s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 70s/epoch - 356ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 379.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_379/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_379
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_373"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_374 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f3e195e1960>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e9b8a7f40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e9b8a7f40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f464a228b50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e19402bf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_379/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e19402fb0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e19401690>, <keras.callbacks.EarlyStopping object at 0x7f3e19400790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e194012d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e19401cc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_379/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 379/720 with hyperparameters:
timestamp = 2023-10-30 15:12:26.874967
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:13:54.524 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6785.3379, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 87s - loss: nan - MinusLogProbMetric: 6785.3379 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 87s/epoch - 446ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 379/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

===========
Generating train data for run 385.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_385/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_385/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_385/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_385
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_379"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_380 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f3e73a03190>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee1d74c10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee1d74c10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ed9c93160>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e68259e70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e6825a3e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e6825a4a0>, <keras.callbacks.EarlyStopping object at 0x7f3e6825a710>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e6825a740>, <keras.callbacks.TerminateOnNaN object at 0x7f3e6825a380>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_385/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 385/720 with hyperparameters:
timestamp = 2023-10-30 15:13:58.742534
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
2023-10-30 15:15:29.174 
Epoch 1/1000 
	 loss: 846.8345, MinusLogProbMetric: 846.8345, val_loss: 175.7587, val_MinusLogProbMetric: 175.7587

Epoch 1: val_loss improved from inf to 175.75871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 91s - loss: 846.8345 - MinusLogProbMetric: 846.8345 - val_loss: 175.7587 - val_MinusLogProbMetric: 175.7587 - lr: 0.0010 - 91s/epoch - 463ms/step
Epoch 2/1000
2023-10-30 15:15:59.572 
Epoch 2/1000 
	 loss: 142.1421, MinusLogProbMetric: 142.1421, val_loss: 118.9100, val_MinusLogProbMetric: 118.9100

Epoch 2: val_loss improved from 175.75871 to 118.91004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 142.1421 - MinusLogProbMetric: 142.1421 - val_loss: 118.9100 - val_MinusLogProbMetric: 118.9100 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 3/1000
2023-10-30 15:16:32.392 
Epoch 3/1000 
	 loss: 105.8382, MinusLogProbMetric: 105.8382, val_loss: 93.8077, val_MinusLogProbMetric: 93.8077

Epoch 3: val_loss improved from 118.91004 to 93.80768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 105.8382 - MinusLogProbMetric: 105.8382 - val_loss: 93.8077 - val_MinusLogProbMetric: 93.8077 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 4/1000
2023-10-30 15:17:01.088 
Epoch 4/1000 
	 loss: 86.2453, MinusLogProbMetric: 86.2453, val_loss: 79.0356, val_MinusLogProbMetric: 79.0356

Epoch 4: val_loss improved from 93.80768 to 79.03555, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 86.2453 - MinusLogProbMetric: 86.2453 - val_loss: 79.0356 - val_MinusLogProbMetric: 79.0356 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 5/1000
2023-10-30 15:17:29.489 
Epoch 5/1000 
	 loss: 72.7845, MinusLogProbMetric: 72.7845, val_loss: 68.3217, val_MinusLogProbMetric: 68.3217

Epoch 5: val_loss improved from 79.03555 to 68.32166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 72.7845 - MinusLogProbMetric: 72.7845 - val_loss: 68.3217 - val_MinusLogProbMetric: 68.3217 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 6/1000
2023-10-30 15:17:57.789 
Epoch 6/1000 
	 loss: 64.1975, MinusLogProbMetric: 64.1975, val_loss: 60.0053, val_MinusLogProbMetric: 60.0053

Epoch 6: val_loss improved from 68.32166 to 60.00534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 64.1975 - MinusLogProbMetric: 64.1975 - val_loss: 60.0053 - val_MinusLogProbMetric: 60.0053 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 7/1000
2023-10-30 15:18:26.688 
Epoch 7/1000 
	 loss: 57.9975, MinusLogProbMetric: 57.9975, val_loss: 58.6398, val_MinusLogProbMetric: 58.6398

Epoch 7: val_loss improved from 60.00534 to 58.63977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 57.9975 - MinusLogProbMetric: 57.9975 - val_loss: 58.6398 - val_MinusLogProbMetric: 58.6398 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 8/1000
2023-10-30 15:18:56.941 
Epoch 8/1000 
	 loss: 54.0633, MinusLogProbMetric: 54.0633, val_loss: 51.8550, val_MinusLogProbMetric: 51.8550

Epoch 8: val_loss improved from 58.63977 to 51.85503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 54.0633 - MinusLogProbMetric: 54.0633 - val_loss: 51.8550 - val_MinusLogProbMetric: 51.8550 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 9/1000
2023-10-30 15:19:30.343 
Epoch 9/1000 
	 loss: 55.9042, MinusLogProbMetric: 55.9042, val_loss: 50.3396, val_MinusLogProbMetric: 50.3396

Epoch 9: val_loss improved from 51.85503 to 50.33961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 55.9042 - MinusLogProbMetric: 55.9042 - val_loss: 50.3396 - val_MinusLogProbMetric: 50.3396 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 10/1000
2023-10-30 15:19:58.261 
Epoch 10/1000 
	 loss: 53.4329, MinusLogProbMetric: 53.4329, val_loss: 52.2807, val_MinusLogProbMetric: 52.2807

Epoch 10: val_loss did not improve from 50.33961
196/196 - 27s - loss: 53.4329 - MinusLogProbMetric: 53.4329 - val_loss: 52.2807 - val_MinusLogProbMetric: 52.2807 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 11/1000
2023-10-30 15:20:26.465 
Epoch 11/1000 
	 loss: 46.7867, MinusLogProbMetric: 46.7867, val_loss: 47.4009, val_MinusLogProbMetric: 47.4009

Epoch 11: val_loss improved from 50.33961 to 47.40091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 46.7867 - MinusLogProbMetric: 46.7867 - val_loss: 47.4009 - val_MinusLogProbMetric: 47.4009 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 12/1000
2023-10-30 15:20:55.058 
Epoch 12/1000 
	 loss: 45.3315, MinusLogProbMetric: 45.3315, val_loss: 47.7978, val_MinusLogProbMetric: 47.7978

Epoch 12: val_loss did not improve from 47.40091
196/196 - 28s - loss: 45.3315 - MinusLogProbMetric: 45.3315 - val_loss: 47.7978 - val_MinusLogProbMetric: 47.7978 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 13/1000
2023-10-30 15:21:23.566 
Epoch 13/1000 
	 loss: 48.7809, MinusLogProbMetric: 48.7809, val_loss: 43.9004, val_MinusLogProbMetric: 43.9004

Epoch 13: val_loss improved from 47.40091 to 43.90043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 48.7809 - MinusLogProbMetric: 48.7809 - val_loss: 43.9004 - val_MinusLogProbMetric: 43.9004 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 14/1000
2023-10-30 15:21:57.514 
Epoch 14/1000 
	 loss: 43.0348, MinusLogProbMetric: 43.0348, val_loss: 42.9918, val_MinusLogProbMetric: 42.9918

Epoch 14: val_loss improved from 43.90043 to 42.99184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 43.0348 - MinusLogProbMetric: 43.0348 - val_loss: 42.9918 - val_MinusLogProbMetric: 42.9918 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 15/1000
2023-10-30 15:22:27.807 
Epoch 15/1000 
	 loss: 43.0442, MinusLogProbMetric: 43.0442, val_loss: 43.5331, val_MinusLogProbMetric: 43.5331

Epoch 15: val_loss did not improve from 42.99184
196/196 - 30s - loss: 43.0442 - MinusLogProbMetric: 43.0442 - val_loss: 43.5331 - val_MinusLogProbMetric: 43.5331 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 16/1000
2023-10-30 15:22:54.889 
Epoch 16/1000 
	 loss: 41.9230, MinusLogProbMetric: 41.9230, val_loss: 41.1941, val_MinusLogProbMetric: 41.1941

Epoch 16: val_loss improved from 42.99184 to 41.19407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 27s - loss: 41.9230 - MinusLogProbMetric: 41.9230 - val_loss: 41.1941 - val_MinusLogProbMetric: 41.1941 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 17/1000
2023-10-30 15:23:22.386 
Epoch 17/1000 
	 loss: 41.1782, MinusLogProbMetric: 41.1782, val_loss: 39.8085, val_MinusLogProbMetric: 39.8085

Epoch 17: val_loss improved from 41.19407 to 39.80854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 27s - loss: 41.1782 - MinusLogProbMetric: 41.1782 - val_loss: 39.8085 - val_MinusLogProbMetric: 39.8085 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 18/1000
2023-10-30 15:23:50.019 
Epoch 18/1000 
	 loss: 40.6891, MinusLogProbMetric: 40.6891, val_loss: 43.5723, val_MinusLogProbMetric: 43.5723

Epoch 18: val_loss did not improve from 39.80854
196/196 - 27s - loss: 40.6891 - MinusLogProbMetric: 40.6891 - val_loss: 43.5723 - val_MinusLogProbMetric: 43.5723 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 19/1000
2023-10-30 15:24:17.344 
Epoch 19/1000 
	 loss: 40.3224, MinusLogProbMetric: 40.3224, val_loss: 39.1679, val_MinusLogProbMetric: 39.1679

Epoch 19: val_loss improved from 39.80854 to 39.16790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 40.3224 - MinusLogProbMetric: 40.3224 - val_loss: 39.1679 - val_MinusLogProbMetric: 39.1679 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 20/1000
2023-10-30 15:24:47.426 
Epoch 20/1000 
	 loss: 39.9149, MinusLogProbMetric: 39.9149, val_loss: 39.5501, val_MinusLogProbMetric: 39.5501

Epoch 20: val_loss did not improve from 39.16790
196/196 - 30s - loss: 39.9149 - MinusLogProbMetric: 39.9149 - val_loss: 39.5501 - val_MinusLogProbMetric: 39.5501 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 21/1000
2023-10-30 15:25:17.932 
Epoch 21/1000 
	 loss: 39.9827, MinusLogProbMetric: 39.9827, val_loss: 44.5976, val_MinusLogProbMetric: 44.5976

Epoch 21: val_loss did not improve from 39.16790
196/196 - 31s - loss: 39.9827 - MinusLogProbMetric: 39.9827 - val_loss: 44.5976 - val_MinusLogProbMetric: 44.5976 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 22/1000
2023-10-30 15:25:46.079 
Epoch 22/1000 
	 loss: 38.4956, MinusLogProbMetric: 38.4956, val_loss: 38.2353, val_MinusLogProbMetric: 38.2353

Epoch 22: val_loss improved from 39.16790 to 38.23533, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 38.4956 - MinusLogProbMetric: 38.4956 - val_loss: 38.2353 - val_MinusLogProbMetric: 38.2353 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 23/1000
2023-10-30 15:26:14.776 
Epoch 23/1000 
	 loss: 41.4223, MinusLogProbMetric: 41.4223, val_loss: 38.0105, val_MinusLogProbMetric: 38.0105

Epoch 23: val_loss improved from 38.23533 to 38.01047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 41.4223 - MinusLogProbMetric: 41.4223 - val_loss: 38.0105 - val_MinusLogProbMetric: 38.0105 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 24/1000
2023-10-30 15:26:43.469 
Epoch 24/1000 
	 loss: 38.2826, MinusLogProbMetric: 38.2826, val_loss: 38.6141, val_MinusLogProbMetric: 38.6141

Epoch 24: val_loss did not improve from 38.01047
196/196 - 28s - loss: 38.2826 - MinusLogProbMetric: 38.2826 - val_loss: 38.6141 - val_MinusLogProbMetric: 38.6141 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 25/1000
2023-10-30 15:27:11.209 
Epoch 25/1000 
	 loss: 37.4647, MinusLogProbMetric: 37.4647, val_loss: 37.2600, val_MinusLogProbMetric: 37.2600

Epoch 25: val_loss improved from 38.01047 to 37.26001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 37.4647 - MinusLogProbMetric: 37.4647 - val_loss: 37.2600 - val_MinusLogProbMetric: 37.2600 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 26/1000
2023-10-30 15:27:44.031 
Epoch 26/1000 
	 loss: 37.0980, MinusLogProbMetric: 37.0980, val_loss: 36.7012, val_MinusLogProbMetric: 36.7012

Epoch 26: val_loss improved from 37.26001 to 36.70118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 37.0980 - MinusLogProbMetric: 37.0980 - val_loss: 36.7012 - val_MinusLogProbMetric: 36.7012 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 27/1000
2023-10-30 15:28:14.923 
Epoch 27/1000 
	 loss: 37.3916, MinusLogProbMetric: 37.3916, val_loss: 38.3389, val_MinusLogProbMetric: 38.3389

Epoch 27: val_loss did not improve from 36.70118
196/196 - 30s - loss: 37.3916 - MinusLogProbMetric: 37.3916 - val_loss: 38.3389 - val_MinusLogProbMetric: 38.3389 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 28/1000
2023-10-30 15:28:43.557 
Epoch 28/1000 
	 loss: 37.4283, MinusLogProbMetric: 37.4283, val_loss: 38.1239, val_MinusLogProbMetric: 38.1239

Epoch 28: val_loss did not improve from 36.70118
196/196 - 29s - loss: 37.4283 - MinusLogProbMetric: 37.4283 - val_loss: 38.1239 - val_MinusLogProbMetric: 38.1239 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 29/1000
2023-10-30 15:29:12.170 
Epoch 29/1000 
	 loss: 36.8253, MinusLogProbMetric: 36.8253, val_loss: 36.0637, val_MinusLogProbMetric: 36.0637

Epoch 29: val_loss improved from 36.70118 to 36.06371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 36.8253 - MinusLogProbMetric: 36.8253 - val_loss: 36.0637 - val_MinusLogProbMetric: 36.0637 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 30/1000
2023-10-30 15:29:40.955 
Epoch 30/1000 
	 loss: 36.8087, MinusLogProbMetric: 36.8087, val_loss: 38.2123, val_MinusLogProbMetric: 38.2123

Epoch 30: val_loss did not improve from 36.06371
196/196 - 28s - loss: 36.8087 - MinusLogProbMetric: 36.8087 - val_loss: 38.2123 - val_MinusLogProbMetric: 38.2123 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 31/1000
2023-10-30 15:30:09.530 
Epoch 31/1000 
	 loss: 36.7257, MinusLogProbMetric: 36.7257, val_loss: 37.1749, val_MinusLogProbMetric: 37.1749

Epoch 31: val_loss did not improve from 36.06371
196/196 - 29s - loss: 36.7257 - MinusLogProbMetric: 36.7257 - val_loss: 37.1749 - val_MinusLogProbMetric: 37.1749 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 32/1000
2023-10-30 15:30:41.261 
Epoch 32/1000 
	 loss: 36.3094, MinusLogProbMetric: 36.3094, val_loss: 36.1835, val_MinusLogProbMetric: 36.1835

Epoch 32: val_loss did not improve from 36.06371
196/196 - 32s - loss: 36.3094 - MinusLogProbMetric: 36.3094 - val_loss: 36.1835 - val_MinusLogProbMetric: 36.1835 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 33/1000
2023-10-30 15:31:11.854 
Epoch 33/1000 
	 loss: 35.9090, MinusLogProbMetric: 35.9090, val_loss: 35.7725, val_MinusLogProbMetric: 35.7725

Epoch 33: val_loss improved from 36.06371 to 35.77246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 35.9090 - MinusLogProbMetric: 35.9090 - val_loss: 35.7725 - val_MinusLogProbMetric: 35.7725 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 34/1000
2023-10-30 15:31:40.960 
Epoch 34/1000 
	 loss: 35.5692, MinusLogProbMetric: 35.5692, val_loss: 39.3751, val_MinusLogProbMetric: 39.3751

Epoch 34: val_loss did not improve from 35.77246
196/196 - 29s - loss: 35.5692 - MinusLogProbMetric: 35.5692 - val_loss: 39.3751 - val_MinusLogProbMetric: 39.3751 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 35/1000
2023-10-30 15:32:09.476 
Epoch 35/1000 
	 loss: 35.7168, MinusLogProbMetric: 35.7168, val_loss: 35.5291, val_MinusLogProbMetric: 35.5291

Epoch 35: val_loss improved from 35.77246 to 35.52906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 35.7168 - MinusLogProbMetric: 35.7168 - val_loss: 35.5291 - val_MinusLogProbMetric: 35.5291 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 36/1000
2023-10-30 15:32:38.166 
Epoch 36/1000 
	 loss: 34.9522, MinusLogProbMetric: 34.9522, val_loss: 35.1429, val_MinusLogProbMetric: 35.1429

Epoch 36: val_loss improved from 35.52906 to 35.14291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 34.9522 - MinusLogProbMetric: 34.9522 - val_loss: 35.1429 - val_MinusLogProbMetric: 35.1429 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 37/1000
2023-10-30 15:33:06.877 
Epoch 37/1000 
	 loss: 35.0320, MinusLogProbMetric: 35.0320, val_loss: 36.4509, val_MinusLogProbMetric: 36.4509

Epoch 37: val_loss did not improve from 35.14291
196/196 - 28s - loss: 35.0320 - MinusLogProbMetric: 35.0320 - val_loss: 36.4509 - val_MinusLogProbMetric: 36.4509 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 38/1000
2023-10-30 15:33:36.608 
Epoch 38/1000 
	 loss: 35.9355, MinusLogProbMetric: 35.9355, val_loss: 34.7234, val_MinusLogProbMetric: 34.7234

Epoch 38: val_loss improved from 35.14291 to 34.72341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 35.9355 - MinusLogProbMetric: 35.9355 - val_loss: 34.7234 - val_MinusLogProbMetric: 34.7234 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 39/1000
2023-10-30 15:34:07.216 
Epoch 39/1000 
	 loss: 34.8192, MinusLogProbMetric: 34.8192, val_loss: 35.6157, val_MinusLogProbMetric: 35.6157

Epoch 39: val_loss did not improve from 34.72341
196/196 - 30s - loss: 34.8192 - MinusLogProbMetric: 34.8192 - val_loss: 35.6157 - val_MinusLogProbMetric: 35.6157 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 40/1000
2023-10-30 15:34:36.167 
Epoch 40/1000 
	 loss: 34.5276, MinusLogProbMetric: 34.5276, val_loss: 34.9159, val_MinusLogProbMetric: 34.9159

Epoch 40: val_loss did not improve from 34.72341
196/196 - 29s - loss: 34.5276 - MinusLogProbMetric: 34.5276 - val_loss: 34.9159 - val_MinusLogProbMetric: 34.9159 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 41/1000
2023-10-30 15:35:04.317 
Epoch 41/1000 
	 loss: 34.3648, MinusLogProbMetric: 34.3648, val_loss: 35.6334, val_MinusLogProbMetric: 35.6334

Epoch 41: val_loss did not improve from 34.72341
196/196 - 28s - loss: 34.3648 - MinusLogProbMetric: 34.3648 - val_loss: 35.6334 - val_MinusLogProbMetric: 35.6334 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 42/1000
2023-10-30 15:35:32.609 
Epoch 42/1000 
	 loss: 34.5530, MinusLogProbMetric: 34.5530, val_loss: 34.0981, val_MinusLogProbMetric: 34.0981

Epoch 42: val_loss improved from 34.72341 to 34.09808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 34.5530 - MinusLogProbMetric: 34.5530 - val_loss: 34.0981 - val_MinusLogProbMetric: 34.0981 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 43/1000
2023-10-30 15:36:01.197 
Epoch 43/1000 
	 loss: 34.4895, MinusLogProbMetric: 34.4895, val_loss: 35.8540, val_MinusLogProbMetric: 35.8540

Epoch 43: val_loss did not improve from 34.09808
196/196 - 28s - loss: 34.4895 - MinusLogProbMetric: 34.4895 - val_loss: 35.8540 - val_MinusLogProbMetric: 35.8540 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 44/1000
2023-10-30 15:36:29.979 
Epoch 44/1000 
	 loss: 34.0392, MinusLogProbMetric: 34.0392, val_loss: 35.7591, val_MinusLogProbMetric: 35.7591

Epoch 44: val_loss did not improve from 34.09808
196/196 - 29s - loss: 34.0392 - MinusLogProbMetric: 34.0392 - val_loss: 35.7591 - val_MinusLogProbMetric: 35.7591 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 45/1000
2023-10-30 15:36:57.262 
Epoch 45/1000 
	 loss: 34.1523, MinusLogProbMetric: 34.1523, val_loss: 34.8653, val_MinusLogProbMetric: 34.8653

Epoch 45: val_loss did not improve from 34.09808
196/196 - 27s - loss: 34.1523 - MinusLogProbMetric: 34.1523 - val_loss: 34.8653 - val_MinusLogProbMetric: 34.8653 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 46/1000
2023-10-30 15:37:24.191 
Epoch 46/1000 
	 loss: 33.8794, MinusLogProbMetric: 33.8794, val_loss: 34.7335, val_MinusLogProbMetric: 34.7335

Epoch 46: val_loss did not improve from 34.09808
196/196 - 27s - loss: 33.8794 - MinusLogProbMetric: 33.8794 - val_loss: 34.7335 - val_MinusLogProbMetric: 34.7335 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 47/1000
2023-10-30 15:37:51.227 
Epoch 47/1000 
	 loss: 33.6110, MinusLogProbMetric: 33.6110, val_loss: 33.7435, val_MinusLogProbMetric: 33.7435

Epoch 47: val_loss improved from 34.09808 to 33.74355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 27s - loss: 33.6110 - MinusLogProbMetric: 33.6110 - val_loss: 33.7435 - val_MinusLogProbMetric: 33.7435 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 48/1000
2023-10-30 15:38:19.698 
Epoch 48/1000 
	 loss: 33.7965, MinusLogProbMetric: 33.7965, val_loss: 33.5481, val_MinusLogProbMetric: 33.5481

Epoch 48: val_loss improved from 33.74355 to 33.54811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 33.7965 - MinusLogProbMetric: 33.7965 - val_loss: 33.5481 - val_MinusLogProbMetric: 33.5481 - lr: 0.0010 - 29s/epoch - 145ms/step
Epoch 49/1000
2023-10-30 15:38:48.006 
Epoch 49/1000 
	 loss: 33.3648, MinusLogProbMetric: 33.3648, val_loss: 33.2724, val_MinusLogProbMetric: 33.2724

Epoch 49: val_loss improved from 33.54811 to 33.27245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 33.3648 - MinusLogProbMetric: 33.3648 - val_loss: 33.2724 - val_MinusLogProbMetric: 33.2724 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 50/1000
2023-10-30 15:39:18.361 
Epoch 50/1000 
	 loss: 33.6195, MinusLogProbMetric: 33.6195, val_loss: 34.2500, val_MinusLogProbMetric: 34.2500

Epoch 50: val_loss did not improve from 33.27245
196/196 - 30s - loss: 33.6195 - MinusLogProbMetric: 33.6195 - val_loss: 34.2500 - val_MinusLogProbMetric: 34.2500 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 51/1000
2023-10-30 15:39:48.702 
Epoch 51/1000 
	 loss: 33.3211, MinusLogProbMetric: 33.3211, val_loss: 33.5053, val_MinusLogProbMetric: 33.5053

Epoch 51: val_loss did not improve from 33.27245
196/196 - 30s - loss: 33.3211 - MinusLogProbMetric: 33.3211 - val_loss: 33.5053 - val_MinusLogProbMetric: 33.5053 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 52/1000
2023-10-30 15:40:17.094 
Epoch 52/1000 
	 loss: 33.3245, MinusLogProbMetric: 33.3245, val_loss: 34.2689, val_MinusLogProbMetric: 34.2689

Epoch 52: val_loss did not improve from 33.27245
196/196 - 28s - loss: 33.3245 - MinusLogProbMetric: 33.3245 - val_loss: 34.2689 - val_MinusLogProbMetric: 34.2689 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 53/1000
2023-10-30 15:40:45.091 
Epoch 53/1000 
	 loss: 33.2404, MinusLogProbMetric: 33.2404, val_loss: 33.2261, val_MinusLogProbMetric: 33.2261

Epoch 53: val_loss improved from 33.27245 to 33.22606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 33.2404 - MinusLogProbMetric: 33.2404 - val_loss: 33.2261 - val_MinusLogProbMetric: 33.2261 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 54/1000
2023-10-30 15:41:14.210 
Epoch 54/1000 
	 loss: 33.2756, MinusLogProbMetric: 33.2756, val_loss: 33.0982, val_MinusLogProbMetric: 33.0982

Epoch 54: val_loss improved from 33.22606 to 33.09815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 33.2756 - MinusLogProbMetric: 33.2756 - val_loss: 33.0982 - val_MinusLogProbMetric: 33.0982 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 55/1000
2023-10-30 15:41:42.942 
Epoch 55/1000 
	 loss: 33.1702, MinusLogProbMetric: 33.1702, val_loss: 33.2333, val_MinusLogProbMetric: 33.2333

Epoch 55: val_loss did not improve from 33.09815
196/196 - 28s - loss: 33.1702 - MinusLogProbMetric: 33.1702 - val_loss: 33.2333 - val_MinusLogProbMetric: 33.2333 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 56/1000
2023-10-30 15:42:13.043 
Epoch 56/1000 
	 loss: 32.8873, MinusLogProbMetric: 32.8873, val_loss: 33.3337, val_MinusLogProbMetric: 33.3337

Epoch 56: val_loss did not improve from 33.09815
196/196 - 30s - loss: 32.8873 - MinusLogProbMetric: 32.8873 - val_loss: 33.3337 - val_MinusLogProbMetric: 33.3337 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 57/1000
2023-10-30 15:42:44.459 
Epoch 57/1000 
	 loss: 32.7121, MinusLogProbMetric: 32.7121, val_loss: 32.6636, val_MinusLogProbMetric: 32.6636

Epoch 57: val_loss improved from 33.09815 to 32.66359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 32.7121 - MinusLogProbMetric: 32.7121 - val_loss: 32.6636 - val_MinusLogProbMetric: 32.6636 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 58/1000
2023-10-30 15:43:13.083 
Epoch 58/1000 
	 loss: 33.7676, MinusLogProbMetric: 33.7676, val_loss: 32.2688, val_MinusLogProbMetric: 32.2688

Epoch 58: val_loss improved from 32.66359 to 32.26884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 33.7676 - MinusLogProbMetric: 33.7676 - val_loss: 32.2688 - val_MinusLogProbMetric: 32.2688 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 59/1000
2023-10-30 15:43:42.280 
Epoch 59/1000 
	 loss: 32.6830, MinusLogProbMetric: 32.6830, val_loss: 32.5152, val_MinusLogProbMetric: 32.5152

Epoch 59: val_loss did not improve from 32.26884
196/196 - 29s - loss: 32.6830 - MinusLogProbMetric: 32.6830 - val_loss: 32.5152 - val_MinusLogProbMetric: 32.5152 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 60/1000
2023-10-30 15:44:11.319 
Epoch 60/1000 
	 loss: 32.7016, MinusLogProbMetric: 32.7016, val_loss: 32.7728, val_MinusLogProbMetric: 32.7728

Epoch 60: val_loss did not improve from 32.26884
196/196 - 29s - loss: 32.7016 - MinusLogProbMetric: 32.7016 - val_loss: 32.7728 - val_MinusLogProbMetric: 32.7728 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 61/1000
2023-10-30 15:44:42.339 
Epoch 61/1000 
	 loss: 32.4466, MinusLogProbMetric: 32.4466, val_loss: 32.5812, val_MinusLogProbMetric: 32.5812

Epoch 61: val_loss did not improve from 32.26884
196/196 - 31s - loss: 32.4466 - MinusLogProbMetric: 32.4466 - val_loss: 32.5812 - val_MinusLogProbMetric: 32.5812 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 62/1000
2023-10-30 15:45:10.882 
Epoch 62/1000 
	 loss: 32.2965, MinusLogProbMetric: 32.2965, val_loss: 33.6208, val_MinusLogProbMetric: 33.6208

Epoch 62: val_loss did not improve from 32.26884
196/196 - 29s - loss: 32.2965 - MinusLogProbMetric: 32.2965 - val_loss: 33.6208 - val_MinusLogProbMetric: 33.6208 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 63/1000
2023-10-30 15:45:39.196 
Epoch 63/1000 
	 loss: 32.3936, MinusLogProbMetric: 32.3936, val_loss: 33.1230, val_MinusLogProbMetric: 33.1230

Epoch 63: val_loss did not improve from 32.26884
196/196 - 28s - loss: 32.3936 - MinusLogProbMetric: 32.3936 - val_loss: 33.1230 - val_MinusLogProbMetric: 33.1230 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 64/1000
2023-10-30 15:46:07.492 
Epoch 64/1000 
	 loss: 32.2082, MinusLogProbMetric: 32.2082, val_loss: 32.4154, val_MinusLogProbMetric: 32.4154

Epoch 64: val_loss did not improve from 32.26884
196/196 - 28s - loss: 32.2082 - MinusLogProbMetric: 32.2082 - val_loss: 32.4154 - val_MinusLogProbMetric: 32.4154 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 65/1000
2023-10-30 15:46:35.487 
Epoch 65/1000 
	 loss: 32.2370, MinusLogProbMetric: 32.2370, val_loss: 32.8334, val_MinusLogProbMetric: 32.8334

Epoch 65: val_loss did not improve from 32.26884
196/196 - 28s - loss: 32.2370 - MinusLogProbMetric: 32.2370 - val_loss: 32.8334 - val_MinusLogProbMetric: 32.8334 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 66/1000
2023-10-30 15:47:02.632 
Epoch 66/1000 
	 loss: 32.1807, MinusLogProbMetric: 32.1807, val_loss: 31.5777, val_MinusLogProbMetric: 31.5777

Epoch 66: val_loss improved from 32.26884 to 31.57766, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 32.1807 - MinusLogProbMetric: 32.1807 - val_loss: 31.5777 - val_MinusLogProbMetric: 31.5777 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 67/1000
2023-10-30 15:47:29.256 
Epoch 67/1000 
	 loss: 32.1071, MinusLogProbMetric: 32.1071, val_loss: 31.8382, val_MinusLogProbMetric: 31.8382

Epoch 67: val_loss did not improve from 31.57766
196/196 - 26s - loss: 32.1071 - MinusLogProbMetric: 32.1071 - val_loss: 31.8382 - val_MinusLogProbMetric: 31.8382 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 68/1000
2023-10-30 15:47:55.960 
Epoch 68/1000 
	 loss: 32.0815, MinusLogProbMetric: 32.0815, val_loss: 31.7549, val_MinusLogProbMetric: 31.7549

Epoch 68: val_loss did not improve from 31.57766
196/196 - 27s - loss: 32.0815 - MinusLogProbMetric: 32.0815 - val_loss: 31.7549 - val_MinusLogProbMetric: 31.7549 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 69/1000
2023-10-30 15:48:23.185 
Epoch 69/1000 
	 loss: 32.0498, MinusLogProbMetric: 32.0498, val_loss: 31.9326, val_MinusLogProbMetric: 31.9326

Epoch 69: val_loss did not improve from 31.57766
196/196 - 27s - loss: 32.0498 - MinusLogProbMetric: 32.0498 - val_loss: 31.9326 - val_MinusLogProbMetric: 31.9326 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 70/1000
2023-10-30 15:48:51.778 
Epoch 70/1000 
	 loss: 32.0719, MinusLogProbMetric: 32.0719, val_loss: 31.8856, val_MinusLogProbMetric: 31.8856

Epoch 70: val_loss did not improve from 31.57766
196/196 - 29s - loss: 32.0719 - MinusLogProbMetric: 32.0719 - val_loss: 31.8856 - val_MinusLogProbMetric: 31.8856 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 71/1000
2023-10-30 15:49:19.874 
Epoch 71/1000 
	 loss: 31.8716, MinusLogProbMetric: 31.8716, val_loss: 31.7486, val_MinusLogProbMetric: 31.7486

Epoch 71: val_loss did not improve from 31.57766
196/196 - 28s - loss: 31.8716 - MinusLogProbMetric: 31.8716 - val_loss: 31.7486 - val_MinusLogProbMetric: 31.7486 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 72/1000
2023-10-30 15:49:47.989 
Epoch 72/1000 
	 loss: 31.9908, MinusLogProbMetric: 31.9908, val_loss: 33.9000, val_MinusLogProbMetric: 33.9000

Epoch 72: val_loss did not improve from 31.57766
196/196 - 28s - loss: 31.9908 - MinusLogProbMetric: 31.9908 - val_loss: 33.9000 - val_MinusLogProbMetric: 33.9000 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 73/1000
2023-10-30 15:50:15.194 
Epoch 73/1000 
	 loss: 32.0350, MinusLogProbMetric: 32.0350, val_loss: 31.6681, val_MinusLogProbMetric: 31.6681

Epoch 73: val_loss did not improve from 31.57766
196/196 - 27s - loss: 32.0350 - MinusLogProbMetric: 32.0350 - val_loss: 31.6681 - val_MinusLogProbMetric: 31.6681 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 74/1000
2023-10-30 15:50:43.957 
Epoch 74/1000 
	 loss: 31.9357, MinusLogProbMetric: 31.9357, val_loss: 33.5344, val_MinusLogProbMetric: 33.5344

Epoch 74: val_loss did not improve from 31.57766
196/196 - 29s - loss: 31.9357 - MinusLogProbMetric: 31.9357 - val_loss: 33.5344 - val_MinusLogProbMetric: 33.5344 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 75/1000
2023-10-30 15:51:13.746 
Epoch 75/1000 
	 loss: 31.8024, MinusLogProbMetric: 31.8024, val_loss: 31.6441, val_MinusLogProbMetric: 31.6441

Epoch 75: val_loss did not improve from 31.57766
196/196 - 30s - loss: 31.8024 - MinusLogProbMetric: 31.8024 - val_loss: 31.6441 - val_MinusLogProbMetric: 31.6441 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 76/1000
2023-10-30 15:51:43.072 
Epoch 76/1000 
	 loss: 31.7072, MinusLogProbMetric: 31.7072, val_loss: 31.7066, val_MinusLogProbMetric: 31.7066

Epoch 76: val_loss did not improve from 31.57766
196/196 - 29s - loss: 31.7072 - MinusLogProbMetric: 31.7072 - val_loss: 31.7066 - val_MinusLogProbMetric: 31.7066 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 77/1000
2023-10-30 15:52:09.813 
Epoch 77/1000 
	 loss: 31.7651, MinusLogProbMetric: 31.7651, val_loss: 32.0653, val_MinusLogProbMetric: 32.0653

Epoch 77: val_loss did not improve from 31.57766
196/196 - 27s - loss: 31.7651 - MinusLogProbMetric: 31.7651 - val_loss: 32.0653 - val_MinusLogProbMetric: 32.0653 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 78/1000
2023-10-30 15:52:36.712 
Epoch 78/1000 
	 loss: 31.6085, MinusLogProbMetric: 31.6085, val_loss: 32.1067, val_MinusLogProbMetric: 32.1067

Epoch 78: val_loss did not improve from 31.57766
196/196 - 27s - loss: 31.6085 - MinusLogProbMetric: 31.6085 - val_loss: 32.1067 - val_MinusLogProbMetric: 32.1067 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 79/1000
2023-10-30 15:53:03.431 
Epoch 79/1000 
	 loss: 31.6546, MinusLogProbMetric: 31.6546, val_loss: 32.0267, val_MinusLogProbMetric: 32.0267

Epoch 79: val_loss did not improve from 31.57766
196/196 - 27s - loss: 31.6546 - MinusLogProbMetric: 31.6546 - val_loss: 32.0267 - val_MinusLogProbMetric: 32.0267 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 80/1000
2023-10-30 15:53:30.477 
Epoch 80/1000 
	 loss: 31.6408, MinusLogProbMetric: 31.6408, val_loss: 32.4213, val_MinusLogProbMetric: 32.4213

Epoch 80: val_loss did not improve from 31.57766
196/196 - 27s - loss: 31.6408 - MinusLogProbMetric: 31.6408 - val_loss: 32.4213 - val_MinusLogProbMetric: 32.4213 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 81/1000
2023-10-30 15:54:02.600 
Epoch 81/1000 
	 loss: 31.4708, MinusLogProbMetric: 31.4708, val_loss: 32.7963, val_MinusLogProbMetric: 32.7963

Epoch 81: val_loss did not improve from 31.57766
196/196 - 32s - loss: 31.4708 - MinusLogProbMetric: 31.4708 - val_loss: 32.7963 - val_MinusLogProbMetric: 32.7963 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 82/1000
2023-10-30 15:54:30.898 
Epoch 82/1000 
	 loss: 31.6208, MinusLogProbMetric: 31.6208, val_loss: 32.1621, val_MinusLogProbMetric: 32.1621

Epoch 82: val_loss did not improve from 31.57766
196/196 - 28s - loss: 31.6208 - MinusLogProbMetric: 31.6208 - val_loss: 32.1621 - val_MinusLogProbMetric: 32.1621 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 83/1000
2023-10-30 15:54:59.220 
Epoch 83/1000 
	 loss: 31.4223, MinusLogProbMetric: 31.4223, val_loss: 31.7343, val_MinusLogProbMetric: 31.7343

Epoch 83: val_loss did not improve from 31.57766
196/196 - 28s - loss: 31.4223 - MinusLogProbMetric: 31.4223 - val_loss: 31.7343 - val_MinusLogProbMetric: 31.7343 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 84/1000
2023-10-30 15:55:26.714 
Epoch 84/1000 
	 loss: 31.5174, MinusLogProbMetric: 31.5174, val_loss: 31.2075, val_MinusLogProbMetric: 31.2075

Epoch 84: val_loss improved from 31.57766 to 31.20750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 31.5174 - MinusLogProbMetric: 31.5174 - val_loss: 31.2075 - val_MinusLogProbMetric: 31.2075 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 85/1000
2023-10-30 15:55:54.272 
Epoch 85/1000 
	 loss: 31.4018, MinusLogProbMetric: 31.4018, val_loss: 31.5977, val_MinusLogProbMetric: 31.5977

Epoch 85: val_loss did not improve from 31.20750
196/196 - 27s - loss: 31.4018 - MinusLogProbMetric: 31.4018 - val_loss: 31.5977 - val_MinusLogProbMetric: 31.5977 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 86/1000
2023-10-30 15:56:20.736 
Epoch 86/1000 
	 loss: 31.5062, MinusLogProbMetric: 31.5062, val_loss: 31.2625, val_MinusLogProbMetric: 31.2625

Epoch 86: val_loss did not improve from 31.20750
196/196 - 26s - loss: 31.5062 - MinusLogProbMetric: 31.5062 - val_loss: 31.2625 - val_MinusLogProbMetric: 31.2625 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 87/1000
2023-10-30 15:56:46.837 
Epoch 87/1000 
	 loss: 31.3458, MinusLogProbMetric: 31.3458, val_loss: 32.5406, val_MinusLogProbMetric: 32.5406

Epoch 87: val_loss did not improve from 31.20750
196/196 - 26s - loss: 31.3458 - MinusLogProbMetric: 31.3458 - val_loss: 32.5406 - val_MinusLogProbMetric: 32.5406 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 88/1000
2023-10-30 15:57:12.498 
Epoch 88/1000 
	 loss: 31.4177, MinusLogProbMetric: 31.4177, val_loss: 31.1358, val_MinusLogProbMetric: 31.1358

Epoch 88: val_loss improved from 31.20750 to 31.13578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 26s - loss: 31.4177 - MinusLogProbMetric: 31.4177 - val_loss: 31.1358 - val_MinusLogProbMetric: 31.1358 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 89/1000
2023-10-30 15:57:38.218 
Epoch 89/1000 
	 loss: 31.5316, MinusLogProbMetric: 31.5316, val_loss: 32.0104, val_MinusLogProbMetric: 32.0104

Epoch 89: val_loss did not improve from 31.13578
196/196 - 25s - loss: 31.5316 - MinusLogProbMetric: 31.5316 - val_loss: 32.0104 - val_MinusLogProbMetric: 32.0104 - lr: 0.0010 - 25s/epoch - 129ms/step
Epoch 90/1000
2023-10-30 15:58:04.497 
Epoch 90/1000 
	 loss: 31.2897, MinusLogProbMetric: 31.2897, val_loss: 32.8180, val_MinusLogProbMetric: 32.8180

Epoch 90: val_loss did not improve from 31.13578
196/196 - 26s - loss: 31.2897 - MinusLogProbMetric: 31.2897 - val_loss: 32.8180 - val_MinusLogProbMetric: 32.8180 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 91/1000
2023-10-30 15:58:31.620 
Epoch 91/1000 
	 loss: 31.2144, MinusLogProbMetric: 31.2144, val_loss: 32.0862, val_MinusLogProbMetric: 32.0862

Epoch 91: val_loss did not improve from 31.13578
196/196 - 27s - loss: 31.2144 - MinusLogProbMetric: 31.2144 - val_loss: 32.0862 - val_MinusLogProbMetric: 32.0862 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 92/1000
2023-10-30 15:58:58.317 
Epoch 92/1000 
	 loss: 31.2126, MinusLogProbMetric: 31.2126, val_loss: 31.2793, val_MinusLogProbMetric: 31.2793

Epoch 92: val_loss did not improve from 31.13578
196/196 - 27s - loss: 31.2126 - MinusLogProbMetric: 31.2126 - val_loss: 31.2793 - val_MinusLogProbMetric: 31.2793 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 93/1000
2023-10-30 15:59:24.734 
Epoch 93/1000 
	 loss: 31.3687, MinusLogProbMetric: 31.3687, val_loss: 31.7306, val_MinusLogProbMetric: 31.7306

Epoch 93: val_loss did not improve from 31.13578
196/196 - 26s - loss: 31.3687 - MinusLogProbMetric: 31.3687 - val_loss: 31.7306 - val_MinusLogProbMetric: 31.7306 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 94/1000
2023-10-30 15:59:50.719 
Epoch 94/1000 
	 loss: 31.1939, MinusLogProbMetric: 31.1939, val_loss: 31.2255, val_MinusLogProbMetric: 31.2255

Epoch 94: val_loss did not improve from 31.13578
196/196 - 26s - loss: 31.1939 - MinusLogProbMetric: 31.1939 - val_loss: 31.2255 - val_MinusLogProbMetric: 31.2255 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 95/1000
2023-10-30 16:00:17.217 
Epoch 95/1000 
	 loss: 31.1556, MinusLogProbMetric: 31.1556, val_loss: 31.0649, val_MinusLogProbMetric: 31.0649

Epoch 95: val_loss improved from 31.13578 to 31.06486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 27s - loss: 31.1556 - MinusLogProbMetric: 31.1556 - val_loss: 31.0649 - val_MinusLogProbMetric: 31.0649 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 96/1000
2023-10-30 16:00:45.359 
Epoch 96/1000 
	 loss: 30.8798, MinusLogProbMetric: 30.8798, val_loss: 31.0719, val_MinusLogProbMetric: 31.0719

Epoch 96: val_loss did not improve from 31.06486
196/196 - 28s - loss: 30.8798 - MinusLogProbMetric: 30.8798 - val_loss: 31.0719 - val_MinusLogProbMetric: 31.0719 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 97/1000
2023-10-30 16:01:13.108 
Epoch 97/1000 
	 loss: 31.3336, MinusLogProbMetric: 31.3336, val_loss: 31.4336, val_MinusLogProbMetric: 31.4336

Epoch 97: val_loss did not improve from 31.06486
196/196 - 28s - loss: 31.3336 - MinusLogProbMetric: 31.3336 - val_loss: 31.4336 - val_MinusLogProbMetric: 31.4336 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 98/1000
2023-10-30 16:01:39.515 
Epoch 98/1000 
	 loss: 31.1613, MinusLogProbMetric: 31.1613, val_loss: 31.6736, val_MinusLogProbMetric: 31.6736

Epoch 98: val_loss did not improve from 31.06486
196/196 - 26s - loss: 31.1613 - MinusLogProbMetric: 31.1613 - val_loss: 31.6736 - val_MinusLogProbMetric: 31.6736 - lr: 0.0010 - 26s/epoch - 135ms/step
Epoch 99/1000
2023-10-30 16:02:06.156 
Epoch 99/1000 
	 loss: 30.9456, MinusLogProbMetric: 30.9456, val_loss: 31.1033, val_MinusLogProbMetric: 31.1033

Epoch 99: val_loss did not improve from 31.06486
196/196 - 27s - loss: 30.9456 - MinusLogProbMetric: 30.9456 - val_loss: 31.1033 - val_MinusLogProbMetric: 31.1033 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 100/1000
2023-10-30 16:02:32.942 
Epoch 100/1000 
	 loss: 31.0907, MinusLogProbMetric: 31.0907, val_loss: 31.3302, val_MinusLogProbMetric: 31.3302

Epoch 100: val_loss did not improve from 31.06486
196/196 - 27s - loss: 31.0907 - MinusLogProbMetric: 31.0907 - val_loss: 31.3302 - val_MinusLogProbMetric: 31.3302 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 101/1000
2023-10-30 16:02:59.598 
Epoch 101/1000 
	 loss: 30.9541, MinusLogProbMetric: 30.9541, val_loss: 31.8310, val_MinusLogProbMetric: 31.8310

Epoch 101: val_loss did not improve from 31.06486
196/196 - 27s - loss: 30.9541 - MinusLogProbMetric: 30.9541 - val_loss: 31.8310 - val_MinusLogProbMetric: 31.8310 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 102/1000
2023-10-30 16:03:26.771 
Epoch 102/1000 
	 loss: 30.8774, MinusLogProbMetric: 30.8774, val_loss: 31.8763, val_MinusLogProbMetric: 31.8763

Epoch 102: val_loss did not improve from 31.06486
196/196 - 27s - loss: 30.8774 - MinusLogProbMetric: 30.8774 - val_loss: 31.8763 - val_MinusLogProbMetric: 31.8763 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 103/1000
2023-10-30 16:03:55.582 
Epoch 103/1000 
	 loss: 30.8762, MinusLogProbMetric: 30.8762, val_loss: 31.0041, val_MinusLogProbMetric: 31.0041

Epoch 103: val_loss improved from 31.06486 to 31.00409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 30.8762 - MinusLogProbMetric: 30.8762 - val_loss: 31.0041 - val_MinusLogProbMetric: 31.0041 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 104/1000
2023-10-30 16:04:22.918 
Epoch 104/1000 
	 loss: 30.9567, MinusLogProbMetric: 30.9567, val_loss: 31.4456, val_MinusLogProbMetric: 31.4456

Epoch 104: val_loss did not improve from 31.00409
196/196 - 27s - loss: 30.9567 - MinusLogProbMetric: 30.9567 - val_loss: 31.4456 - val_MinusLogProbMetric: 31.4456 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 105/1000
2023-10-30 16:04:49.897 
Epoch 105/1000 
	 loss: 30.8368, MinusLogProbMetric: 30.8368, val_loss: 30.8832, val_MinusLogProbMetric: 30.8832

Epoch 105: val_loss improved from 31.00409 to 30.88319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 27s - loss: 30.8368 - MinusLogProbMetric: 30.8368 - val_loss: 30.8832 - val_MinusLogProbMetric: 30.8832 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 106/1000
2023-10-30 16:05:17.353 
Epoch 106/1000 
	 loss: 30.7144, MinusLogProbMetric: 30.7144, val_loss: 32.4819, val_MinusLogProbMetric: 32.4819

Epoch 106: val_loss did not improve from 30.88319
196/196 - 27s - loss: 30.7144 - MinusLogProbMetric: 30.7144 - val_loss: 32.4819 - val_MinusLogProbMetric: 32.4819 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 107/1000
2023-10-30 16:05:43.990 
Epoch 107/1000 
	 loss: 30.7787, MinusLogProbMetric: 30.7787, val_loss: 30.4046, val_MinusLogProbMetric: 30.4046

Epoch 107: val_loss improved from 30.88319 to 30.40461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 27s - loss: 30.7787 - MinusLogProbMetric: 30.7787 - val_loss: 30.4046 - val_MinusLogProbMetric: 30.4046 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 108/1000
2023-10-30 16:06:11.022 
Epoch 108/1000 
	 loss: 30.9546, MinusLogProbMetric: 30.9546, val_loss: 30.6998, val_MinusLogProbMetric: 30.6998

Epoch 108: val_loss did not improve from 30.40461
196/196 - 27s - loss: 30.9546 - MinusLogProbMetric: 30.9546 - val_loss: 30.6998 - val_MinusLogProbMetric: 30.6998 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 109/1000
2023-10-30 16:06:36.427 
Epoch 109/1000 
	 loss: 30.8196, MinusLogProbMetric: 30.8196, val_loss: 30.7782, val_MinusLogProbMetric: 30.7782

Epoch 109: val_loss did not improve from 30.40461
196/196 - 25s - loss: 30.8196 - MinusLogProbMetric: 30.8196 - val_loss: 30.7782 - val_MinusLogProbMetric: 30.7782 - lr: 0.0010 - 25s/epoch - 130ms/step
Epoch 110/1000
2023-10-30 16:07:02.511 
Epoch 110/1000 
	 loss: 30.7122, MinusLogProbMetric: 30.7122, val_loss: 32.5154, val_MinusLogProbMetric: 32.5154

Epoch 110: val_loss did not improve from 30.40461
196/196 - 26s - loss: 30.7122 - MinusLogProbMetric: 30.7122 - val_loss: 32.5154 - val_MinusLogProbMetric: 32.5154 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 111/1000
2023-10-30 16:07:28.700 
Epoch 111/1000 
	 loss: 30.6675, MinusLogProbMetric: 30.6675, val_loss: 31.0633, val_MinusLogProbMetric: 31.0633

Epoch 111: val_loss did not improve from 30.40461
196/196 - 26s - loss: 30.6675 - MinusLogProbMetric: 30.6675 - val_loss: 31.0633 - val_MinusLogProbMetric: 31.0633 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 112/1000
2023-10-30 16:07:54.298 
Epoch 112/1000 
	 loss: 30.7478, MinusLogProbMetric: 30.7478, val_loss: 32.3545, val_MinusLogProbMetric: 32.3545

Epoch 112: val_loss did not improve from 30.40461
196/196 - 26s - loss: 30.7478 - MinusLogProbMetric: 30.7478 - val_loss: 32.3545 - val_MinusLogProbMetric: 32.3545 - lr: 0.0010 - 26s/epoch - 131ms/step
Epoch 113/1000
2023-10-30 16:08:19.082 
Epoch 113/1000 
	 loss: 30.5346, MinusLogProbMetric: 30.5346, val_loss: 30.3579, val_MinusLogProbMetric: 30.3579

Epoch 113: val_loss improved from 30.40461 to 30.35787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 25s - loss: 30.5346 - MinusLogProbMetric: 30.5346 - val_loss: 30.3579 - val_MinusLogProbMetric: 30.3579 - lr: 0.0010 - 25s/epoch - 128ms/step
Epoch 114/1000
2023-10-30 16:08:45.047 
Epoch 114/1000 
	 loss: 30.6880, MinusLogProbMetric: 30.6880, val_loss: 31.1453, val_MinusLogProbMetric: 31.1453

Epoch 114: val_loss did not improve from 30.35787
196/196 - 26s - loss: 30.6880 - MinusLogProbMetric: 30.6880 - val_loss: 31.1453 - val_MinusLogProbMetric: 31.1453 - lr: 0.0010 - 26s/epoch - 130ms/step
Epoch 115/1000
2023-10-30 16:09:11.313 
Epoch 115/1000 
	 loss: 30.5865, MinusLogProbMetric: 30.5865, val_loss: 31.1127, val_MinusLogProbMetric: 31.1127

Epoch 115: val_loss did not improve from 30.35787
196/196 - 26s - loss: 30.5865 - MinusLogProbMetric: 30.5865 - val_loss: 31.1127 - val_MinusLogProbMetric: 31.1127 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 116/1000
2023-10-30 16:09:39.287 
Epoch 116/1000 
	 loss: 30.6995, MinusLogProbMetric: 30.6995, val_loss: 32.1540, val_MinusLogProbMetric: 32.1540

Epoch 116: val_loss did not improve from 30.35787
196/196 - 28s - loss: 30.6995 - MinusLogProbMetric: 30.6995 - val_loss: 32.1540 - val_MinusLogProbMetric: 32.1540 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 117/1000
2023-10-30 16:10:08.275 
Epoch 117/1000 
	 loss: 30.5238, MinusLogProbMetric: 30.5238, val_loss: 31.3061, val_MinusLogProbMetric: 31.3061

Epoch 117: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.5238 - MinusLogProbMetric: 30.5238 - val_loss: 31.3061 - val_MinusLogProbMetric: 31.3061 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 118/1000
2023-10-30 16:10:37.143 
Epoch 118/1000 
	 loss: 30.5403, MinusLogProbMetric: 30.5403, val_loss: 31.1221, val_MinusLogProbMetric: 31.1221

Epoch 118: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.5403 - MinusLogProbMetric: 30.5403 - val_loss: 31.1221 - val_MinusLogProbMetric: 31.1221 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 119/1000
2023-10-30 16:11:04.763 
Epoch 119/1000 
	 loss: 30.3950, MinusLogProbMetric: 30.3950, val_loss: 31.0228, val_MinusLogProbMetric: 31.0228

Epoch 119: val_loss did not improve from 30.35787
196/196 - 28s - loss: 30.3950 - MinusLogProbMetric: 30.3950 - val_loss: 31.0228 - val_MinusLogProbMetric: 31.0228 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 120/1000
2023-10-30 16:11:33.345 
Epoch 120/1000 
	 loss: 30.7150, MinusLogProbMetric: 30.7150, val_loss: 30.7360, val_MinusLogProbMetric: 30.7360

Epoch 120: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.7150 - MinusLogProbMetric: 30.7150 - val_loss: 30.7360 - val_MinusLogProbMetric: 30.7360 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 121/1000
2023-10-30 16:12:01.773 
Epoch 121/1000 
	 loss: 30.5453, MinusLogProbMetric: 30.5453, val_loss: 30.4945, val_MinusLogProbMetric: 30.4945

Epoch 121: val_loss did not improve from 30.35787
196/196 - 28s - loss: 30.5453 - MinusLogProbMetric: 30.5453 - val_loss: 30.4945 - val_MinusLogProbMetric: 30.4945 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 122/1000
2023-10-30 16:12:30.192 
Epoch 122/1000 
	 loss: 30.5017, MinusLogProbMetric: 30.5017, val_loss: 30.8098, val_MinusLogProbMetric: 30.8098

Epoch 122: val_loss did not improve from 30.35787
196/196 - 28s - loss: 30.5017 - MinusLogProbMetric: 30.5017 - val_loss: 30.8098 - val_MinusLogProbMetric: 30.8098 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 123/1000
2023-10-30 16:12:58.725 
Epoch 123/1000 
	 loss: 30.4441, MinusLogProbMetric: 30.4441, val_loss: 31.1862, val_MinusLogProbMetric: 31.1862

Epoch 123: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.4441 - MinusLogProbMetric: 30.4441 - val_loss: 31.1862 - val_MinusLogProbMetric: 31.1862 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 124/1000
2023-10-30 16:13:27.628 
Epoch 124/1000 
	 loss: 30.4757, MinusLogProbMetric: 30.4757, val_loss: 30.8022, val_MinusLogProbMetric: 30.8022

Epoch 124: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.4757 - MinusLogProbMetric: 30.4757 - val_loss: 30.8022 - val_MinusLogProbMetric: 30.8022 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 125/1000
2023-10-30 16:13:56.669 
Epoch 125/1000 
	 loss: 30.3896, MinusLogProbMetric: 30.3896, val_loss: 31.8691, val_MinusLogProbMetric: 31.8691

Epoch 125: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.3896 - MinusLogProbMetric: 30.3896 - val_loss: 31.8691 - val_MinusLogProbMetric: 31.8691 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 126/1000
2023-10-30 16:14:25.671 
Epoch 126/1000 
	 loss: 30.3423, MinusLogProbMetric: 30.3423, val_loss: 31.5606, val_MinusLogProbMetric: 31.5606

Epoch 126: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.3423 - MinusLogProbMetric: 30.3423 - val_loss: 31.5606 - val_MinusLogProbMetric: 31.5606 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 127/1000
2023-10-30 16:14:54.174 
Epoch 127/1000 
	 loss: 30.4097, MinusLogProbMetric: 30.4097, val_loss: 30.4343, val_MinusLogProbMetric: 30.4343

Epoch 127: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.4097 - MinusLogProbMetric: 30.4097 - val_loss: 30.4343 - val_MinusLogProbMetric: 30.4343 - lr: 0.0010 - 29s/epoch - 145ms/step
Epoch 128/1000
2023-10-30 16:15:22.588 
Epoch 128/1000 
	 loss: 30.4334, MinusLogProbMetric: 30.4334, val_loss: 31.2445, val_MinusLogProbMetric: 31.2445

Epoch 128: val_loss did not improve from 30.35787
196/196 - 28s - loss: 30.4334 - MinusLogProbMetric: 30.4334 - val_loss: 31.2445 - val_MinusLogProbMetric: 31.2445 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 129/1000
2023-10-30 16:15:51.282 
Epoch 129/1000 
	 loss: 30.2922, MinusLogProbMetric: 30.2922, val_loss: 30.5395, val_MinusLogProbMetric: 30.5395

Epoch 129: val_loss did not improve from 30.35787
196/196 - 29s - loss: 30.2922 - MinusLogProbMetric: 30.2922 - val_loss: 30.5395 - val_MinusLogProbMetric: 30.5395 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 130/1000
2023-10-30 16:16:18.479 
Epoch 130/1000 
	 loss: 30.3222, MinusLogProbMetric: 30.3222, val_loss: 30.5188, val_MinusLogProbMetric: 30.5188

Epoch 130: val_loss did not improve from 30.35787
196/196 - 27s - loss: 30.3222 - MinusLogProbMetric: 30.3222 - val_loss: 30.5188 - val_MinusLogProbMetric: 30.5188 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 131/1000
2023-10-30 16:16:44.799 
Epoch 131/1000 
	 loss: 30.4238, MinusLogProbMetric: 30.4238, val_loss: 31.0728, val_MinusLogProbMetric: 31.0728

Epoch 131: val_loss did not improve from 30.35787
196/196 - 26s - loss: 30.4238 - MinusLogProbMetric: 30.4238 - val_loss: 31.0728 - val_MinusLogProbMetric: 31.0728 - lr: 0.0010 - 26s/epoch - 134ms/step
Epoch 132/1000
2023-10-30 16:17:10.831 
Epoch 132/1000 
	 loss: 30.3642, MinusLogProbMetric: 30.3642, val_loss: 30.5720, val_MinusLogProbMetric: 30.5720

Epoch 132: val_loss did not improve from 30.35787
196/196 - 26s - loss: 30.3642 - MinusLogProbMetric: 30.3642 - val_loss: 30.5720 - val_MinusLogProbMetric: 30.5720 - lr: 0.0010 - 26s/epoch - 133ms/step
Epoch 133/1000
2023-10-30 16:17:37.555 
Epoch 133/1000 
	 loss: 30.2119, MinusLogProbMetric: 30.2119, val_loss: 30.6279, val_MinusLogProbMetric: 30.6279

Epoch 133: val_loss did not improve from 30.35787
196/196 - 27s - loss: 30.2119 - MinusLogProbMetric: 30.2119 - val_loss: 30.6279 - val_MinusLogProbMetric: 30.6279 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 134/1000
2023-10-30 16:18:04.057 
Epoch 134/1000 
	 loss: 30.2867, MinusLogProbMetric: 30.2867, val_loss: 30.8951, val_MinusLogProbMetric: 30.8951

Epoch 134: val_loss did not improve from 30.35787
196/196 - 27s - loss: 30.2867 - MinusLogProbMetric: 30.2867 - val_loss: 30.8951 - val_MinusLogProbMetric: 30.8951 - lr: 0.0010 - 27s/epoch - 135ms/step
Epoch 135/1000
2023-10-30 16:18:33.102 
Epoch 135/1000 
	 loss: 30.2975, MinusLogProbMetric: 30.2975, val_loss: 30.1573, val_MinusLogProbMetric: 30.1573

Epoch 135: val_loss improved from 30.35787 to 30.15726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 30.2975 - MinusLogProbMetric: 30.2975 - val_loss: 30.1573 - val_MinusLogProbMetric: 30.1573 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 136/1000
2023-10-30 16:19:07.937 
Epoch 136/1000 
	 loss: 30.1221, MinusLogProbMetric: 30.1221, val_loss: 30.4069, val_MinusLogProbMetric: 30.4069

Epoch 136: val_loss did not improve from 30.15726
196/196 - 34s - loss: 30.1221 - MinusLogProbMetric: 30.1221 - val_loss: 30.4069 - val_MinusLogProbMetric: 30.4069 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 137/1000
2023-10-30 16:19:37.679 
Epoch 137/1000 
	 loss: 30.1026, MinusLogProbMetric: 30.1026, val_loss: 30.0376, val_MinusLogProbMetric: 30.0376

Epoch 137: val_loss improved from 30.15726 to 30.03761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 30.1026 - MinusLogProbMetric: 30.1026 - val_loss: 30.0376 - val_MinusLogProbMetric: 30.0376 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 138/1000
2023-10-30 16:20:06.078 
Epoch 138/1000 
	 loss: 30.0394, MinusLogProbMetric: 30.0394, val_loss: 30.2668, val_MinusLogProbMetric: 30.2668

Epoch 138: val_loss did not improve from 30.03761
196/196 - 28s - loss: 30.0394 - MinusLogProbMetric: 30.0394 - val_loss: 30.2668 - val_MinusLogProbMetric: 30.2668 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 139/1000
2023-10-30 16:20:34.237 
Epoch 139/1000 
	 loss: 30.1739, MinusLogProbMetric: 30.1739, val_loss: 30.2609, val_MinusLogProbMetric: 30.2609

Epoch 139: val_loss did not improve from 30.03761
196/196 - 28s - loss: 30.1739 - MinusLogProbMetric: 30.1739 - val_loss: 30.2609 - val_MinusLogProbMetric: 30.2609 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 140/1000
2023-10-30 16:21:09.015 
Epoch 140/1000 
	 loss: 30.1928, MinusLogProbMetric: 30.1928, val_loss: 29.7906, val_MinusLogProbMetric: 29.7906

Epoch 140: val_loss improved from 30.03761 to 29.79062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 30.1928 - MinusLogProbMetric: 30.1928 - val_loss: 29.7906 - val_MinusLogProbMetric: 29.7906 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 141/1000
2023-10-30 16:21:40.924 
Epoch 141/1000 
	 loss: 30.0071, MinusLogProbMetric: 30.0071, val_loss: 30.2079, val_MinusLogProbMetric: 30.2079

Epoch 141: val_loss did not improve from 29.79062
196/196 - 31s - loss: 30.0071 - MinusLogProbMetric: 30.0071 - val_loss: 30.2079 - val_MinusLogProbMetric: 30.2079 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 142/1000
2023-10-30 16:22:08.605 
Epoch 142/1000 
	 loss: 30.2703, MinusLogProbMetric: 30.2703, val_loss: 30.2852, val_MinusLogProbMetric: 30.2852

Epoch 142: val_loss did not improve from 29.79062
196/196 - 28s - loss: 30.2703 - MinusLogProbMetric: 30.2703 - val_loss: 30.2852 - val_MinusLogProbMetric: 30.2852 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 143/1000
2023-10-30 16:22:36.470 
Epoch 143/1000 
	 loss: 30.1803, MinusLogProbMetric: 30.1803, val_loss: 30.5008, val_MinusLogProbMetric: 30.5008

Epoch 143: val_loss did not improve from 29.79062
196/196 - 28s - loss: 30.1803 - MinusLogProbMetric: 30.1803 - val_loss: 30.5008 - val_MinusLogProbMetric: 30.5008 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 144/1000
2023-10-30 16:23:03.437 
Epoch 144/1000 
	 loss: 30.0600, MinusLogProbMetric: 30.0600, val_loss: 31.6589, val_MinusLogProbMetric: 31.6589

Epoch 144: val_loss did not improve from 29.79062
196/196 - 27s - loss: 30.0600 - MinusLogProbMetric: 30.0600 - val_loss: 31.6589 - val_MinusLogProbMetric: 31.6589 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 145/1000
2023-10-30 16:23:31.092 
Epoch 145/1000 
	 loss: 30.1786, MinusLogProbMetric: 30.1786, val_loss: 31.1173, val_MinusLogProbMetric: 31.1173

Epoch 145: val_loss did not improve from 29.79062
196/196 - 28s - loss: 30.1786 - MinusLogProbMetric: 30.1786 - val_loss: 31.1173 - val_MinusLogProbMetric: 31.1173 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 146/1000
2023-10-30 16:24:03.647 
Epoch 146/1000 
	 loss: 30.0458, MinusLogProbMetric: 30.0458, val_loss: 30.6405, val_MinusLogProbMetric: 30.6405

Epoch 146: val_loss did not improve from 29.79062
196/196 - 33s - loss: 30.0458 - MinusLogProbMetric: 30.0458 - val_loss: 30.6405 - val_MinusLogProbMetric: 30.6405 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 147/1000
2023-10-30 16:24:36.785 
Epoch 147/1000 
	 loss: 30.0086, MinusLogProbMetric: 30.0086, val_loss: 30.3403, val_MinusLogProbMetric: 30.3403

Epoch 147: val_loss did not improve from 29.79062
196/196 - 33s - loss: 30.0086 - MinusLogProbMetric: 30.0086 - val_loss: 30.3403 - val_MinusLogProbMetric: 30.3403 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 148/1000
2023-10-30 16:25:11.016 
Epoch 148/1000 
	 loss: 30.1583, MinusLogProbMetric: 30.1583, val_loss: 32.5792, val_MinusLogProbMetric: 32.5792

Epoch 148: val_loss did not improve from 29.79062
196/196 - 34s - loss: 30.1583 - MinusLogProbMetric: 30.1583 - val_loss: 32.5792 - val_MinusLogProbMetric: 32.5792 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 149/1000
2023-10-30 16:25:45.278 
Epoch 149/1000 
	 loss: 30.2311, MinusLogProbMetric: 30.2311, val_loss: 30.6078, val_MinusLogProbMetric: 30.6078

Epoch 149: val_loss did not improve from 29.79062
196/196 - 34s - loss: 30.2311 - MinusLogProbMetric: 30.2311 - val_loss: 30.6078 - val_MinusLogProbMetric: 30.6078 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 150/1000
2023-10-30 16:26:20.410 
Epoch 150/1000 
	 loss: 30.0272, MinusLogProbMetric: 30.0272, val_loss: 30.2694, val_MinusLogProbMetric: 30.2694

Epoch 150: val_loss did not improve from 29.79062
196/196 - 35s - loss: 30.0272 - MinusLogProbMetric: 30.0272 - val_loss: 30.2694 - val_MinusLogProbMetric: 30.2694 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 151/1000
2023-10-30 16:26:50.754 
Epoch 151/1000 
	 loss: 29.8548, MinusLogProbMetric: 29.8548, val_loss: 29.8961, val_MinusLogProbMetric: 29.8961

Epoch 151: val_loss did not improve from 29.79062
196/196 - 30s - loss: 29.8548 - MinusLogProbMetric: 29.8548 - val_loss: 29.8961 - val_MinusLogProbMetric: 29.8961 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 152/1000
2023-10-30 16:27:20.591 
Epoch 152/1000 
	 loss: 30.0106, MinusLogProbMetric: 30.0106, val_loss: 30.1246, val_MinusLogProbMetric: 30.1246

Epoch 152: val_loss did not improve from 29.79062
196/196 - 30s - loss: 30.0106 - MinusLogProbMetric: 30.0106 - val_loss: 30.1246 - val_MinusLogProbMetric: 30.1246 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 153/1000
2023-10-30 16:27:54.729 
Epoch 153/1000 
	 loss: 30.0340, MinusLogProbMetric: 30.0340, val_loss: 30.3612, val_MinusLogProbMetric: 30.3612

Epoch 153: val_loss did not improve from 29.79062
196/196 - 34s - loss: 30.0340 - MinusLogProbMetric: 30.0340 - val_loss: 30.3612 - val_MinusLogProbMetric: 30.3612 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 154/1000
2023-10-30 16:28:29.271 
Epoch 154/1000 
	 loss: 29.9648, MinusLogProbMetric: 29.9648, val_loss: 29.8009, val_MinusLogProbMetric: 29.8009

Epoch 154: val_loss did not improve from 29.79062
196/196 - 35s - loss: 29.9648 - MinusLogProbMetric: 29.9648 - val_loss: 29.8009 - val_MinusLogProbMetric: 29.8009 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 155/1000
2023-10-30 16:29:02.767 
Epoch 155/1000 
	 loss: 29.9784, MinusLogProbMetric: 29.9784, val_loss: 30.1292, val_MinusLogProbMetric: 30.1292

Epoch 155: val_loss did not improve from 29.79062
196/196 - 33s - loss: 29.9784 - MinusLogProbMetric: 29.9784 - val_loss: 30.1292 - val_MinusLogProbMetric: 30.1292 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 156/1000
2023-10-30 16:29:37.223 
Epoch 156/1000 
	 loss: 29.7930, MinusLogProbMetric: 29.7930, val_loss: 29.9960, val_MinusLogProbMetric: 29.9960

Epoch 156: val_loss did not improve from 29.79062
196/196 - 34s - loss: 29.7930 - MinusLogProbMetric: 29.7930 - val_loss: 29.9960 - val_MinusLogProbMetric: 29.9960 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 157/1000
2023-10-30 16:30:10.897 
Epoch 157/1000 
	 loss: 29.8474, MinusLogProbMetric: 29.8474, val_loss: 29.9479, val_MinusLogProbMetric: 29.9479

Epoch 157: val_loss did not improve from 29.79062
196/196 - 34s - loss: 29.8474 - MinusLogProbMetric: 29.8474 - val_loss: 29.9479 - val_MinusLogProbMetric: 29.9479 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 158/1000
2023-10-30 16:30:44.378 
Epoch 158/1000 
	 loss: 29.9352, MinusLogProbMetric: 29.9352, val_loss: 29.9386, val_MinusLogProbMetric: 29.9386

Epoch 158: val_loss did not improve from 29.79062
196/196 - 33s - loss: 29.9352 - MinusLogProbMetric: 29.9352 - val_loss: 29.9386 - val_MinusLogProbMetric: 29.9386 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 159/1000
2023-10-30 16:31:18.148 
Epoch 159/1000 
	 loss: 29.8538, MinusLogProbMetric: 29.8538, val_loss: 30.1814, val_MinusLogProbMetric: 30.1814

Epoch 159: val_loss did not improve from 29.79062
196/196 - 34s - loss: 29.8538 - MinusLogProbMetric: 29.8538 - val_loss: 30.1814 - val_MinusLogProbMetric: 30.1814 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 160/1000
2023-10-30 16:31:51.554 
Epoch 160/1000 
	 loss: 29.9293, MinusLogProbMetric: 29.9293, val_loss: 30.0538, val_MinusLogProbMetric: 30.0538

Epoch 160: val_loss did not improve from 29.79062
196/196 - 33s - loss: 29.9293 - MinusLogProbMetric: 29.9293 - val_loss: 30.0538 - val_MinusLogProbMetric: 30.0538 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 161/1000
2023-10-30 16:32:24.496 
Epoch 161/1000 
	 loss: 29.9323, MinusLogProbMetric: 29.9323, val_loss: 30.5643, val_MinusLogProbMetric: 30.5643

Epoch 161: val_loss did not improve from 29.79062
196/196 - 33s - loss: 29.9323 - MinusLogProbMetric: 29.9323 - val_loss: 30.5643 - val_MinusLogProbMetric: 30.5643 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 162/1000
2023-10-30 16:32:59.122 
Epoch 162/1000 
	 loss: 30.0126, MinusLogProbMetric: 30.0126, val_loss: 31.7002, val_MinusLogProbMetric: 31.7002

Epoch 162: val_loss did not improve from 29.79062
196/196 - 35s - loss: 30.0126 - MinusLogProbMetric: 30.0126 - val_loss: 31.7002 - val_MinusLogProbMetric: 31.7002 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 163/1000
2023-10-30 16:33:32.387 
Epoch 163/1000 
	 loss: 29.8807, MinusLogProbMetric: 29.8807, val_loss: 30.0775, val_MinusLogProbMetric: 30.0775

Epoch 163: val_loss did not improve from 29.79062
196/196 - 33s - loss: 29.8807 - MinusLogProbMetric: 29.8807 - val_loss: 30.0775 - val_MinusLogProbMetric: 30.0775 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 164/1000
2023-10-30 16:34:06.412 
Epoch 164/1000 
	 loss: 29.8201, MinusLogProbMetric: 29.8201, val_loss: 29.9713, val_MinusLogProbMetric: 29.9713

Epoch 164: val_loss did not improve from 29.79062
196/196 - 34s - loss: 29.8201 - MinusLogProbMetric: 29.8201 - val_loss: 29.9713 - val_MinusLogProbMetric: 29.9713 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 165/1000
2023-10-30 16:34:37.391 
Epoch 165/1000 
	 loss: 29.8397, MinusLogProbMetric: 29.8397, val_loss: 30.1483, val_MinusLogProbMetric: 30.1483

Epoch 165: val_loss did not improve from 29.79062
196/196 - 31s - loss: 29.8397 - MinusLogProbMetric: 29.8397 - val_loss: 30.1483 - val_MinusLogProbMetric: 30.1483 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 166/1000
2023-10-30 16:35:05.399 
Epoch 166/1000 
	 loss: 29.8879, MinusLogProbMetric: 29.8879, val_loss: 29.8557, val_MinusLogProbMetric: 29.8557

Epoch 166: val_loss did not improve from 29.79062
196/196 - 28s - loss: 29.8879 - MinusLogProbMetric: 29.8879 - val_loss: 29.8557 - val_MinusLogProbMetric: 29.8557 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 167/1000
2023-10-30 16:35:34.032 
Epoch 167/1000 
	 loss: 29.8267, MinusLogProbMetric: 29.8267, val_loss: 29.8360, val_MinusLogProbMetric: 29.8360

Epoch 167: val_loss did not improve from 29.79062
196/196 - 29s - loss: 29.8267 - MinusLogProbMetric: 29.8267 - val_loss: 29.8360 - val_MinusLogProbMetric: 29.8360 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 168/1000
2023-10-30 16:36:03.452 
Epoch 168/1000 
	 loss: 29.7657, MinusLogProbMetric: 29.7657, val_loss: 30.2721, val_MinusLogProbMetric: 30.2721

Epoch 168: val_loss did not improve from 29.79062
196/196 - 29s - loss: 29.7657 - MinusLogProbMetric: 29.7657 - val_loss: 30.2721 - val_MinusLogProbMetric: 30.2721 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 169/1000
2023-10-30 16:36:31.411 
Epoch 169/1000 
	 loss: 29.6941, MinusLogProbMetric: 29.6941, val_loss: 29.9139, val_MinusLogProbMetric: 29.9139

Epoch 169: val_loss did not improve from 29.79062
196/196 - 28s - loss: 29.6941 - MinusLogProbMetric: 29.6941 - val_loss: 29.9139 - val_MinusLogProbMetric: 29.9139 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 170/1000
2023-10-30 16:36:59.129 
Epoch 170/1000 
	 loss: 29.7872, MinusLogProbMetric: 29.7872, val_loss: 29.9636, val_MinusLogProbMetric: 29.9636

Epoch 170: val_loss did not improve from 29.79062
196/196 - 28s - loss: 29.7872 - MinusLogProbMetric: 29.7872 - val_loss: 29.9636 - val_MinusLogProbMetric: 29.9636 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 171/1000
2023-10-30 16:37:27.514 
Epoch 171/1000 
	 loss: 29.7413, MinusLogProbMetric: 29.7413, val_loss: 30.1615, val_MinusLogProbMetric: 30.1615

Epoch 171: val_loss did not improve from 29.79062
196/196 - 28s - loss: 29.7413 - MinusLogProbMetric: 29.7413 - val_loss: 30.1615 - val_MinusLogProbMetric: 30.1615 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 172/1000
2023-10-30 16:37:55.995 
Epoch 172/1000 
	 loss: 29.7435, MinusLogProbMetric: 29.7435, val_loss: 30.8047, val_MinusLogProbMetric: 30.8047

Epoch 172: val_loss did not improve from 29.79062
196/196 - 28s - loss: 29.7435 - MinusLogProbMetric: 29.7435 - val_loss: 30.8047 - val_MinusLogProbMetric: 30.8047 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 173/1000
2023-10-30 16:38:24.050 
Epoch 173/1000 
	 loss: 29.7849, MinusLogProbMetric: 29.7849, val_loss: 30.5692, val_MinusLogProbMetric: 30.5692

Epoch 173: val_loss did not improve from 29.79062
196/196 - 28s - loss: 29.7849 - MinusLogProbMetric: 29.7849 - val_loss: 30.5692 - val_MinusLogProbMetric: 30.5692 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 174/1000
2023-10-30 16:38:53.363 
Epoch 174/1000 
	 loss: 29.8779, MinusLogProbMetric: 29.8779, val_loss: 30.2863, val_MinusLogProbMetric: 30.2863

Epoch 174: val_loss did not improve from 29.79062
196/196 - 29s - loss: 29.8779 - MinusLogProbMetric: 29.8779 - val_loss: 30.2863 - val_MinusLogProbMetric: 30.2863 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 175/1000
2023-10-30 16:39:23.975 
Epoch 175/1000 
	 loss: 29.6928, MinusLogProbMetric: 29.6928, val_loss: 30.2298, val_MinusLogProbMetric: 30.2298

Epoch 175: val_loss did not improve from 29.79062
196/196 - 31s - loss: 29.6928 - MinusLogProbMetric: 29.6928 - val_loss: 30.2298 - val_MinusLogProbMetric: 30.2298 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 176/1000
2023-10-30 16:39:55.056 
Epoch 176/1000 
	 loss: 29.6811, MinusLogProbMetric: 29.6811, val_loss: 30.0135, val_MinusLogProbMetric: 30.0135

Epoch 176: val_loss did not improve from 29.79062
196/196 - 31s - loss: 29.6811 - MinusLogProbMetric: 29.6811 - val_loss: 30.0135 - val_MinusLogProbMetric: 30.0135 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 177/1000
2023-10-30 16:40:24.326 
Epoch 177/1000 
	 loss: 29.5633, MinusLogProbMetric: 29.5633, val_loss: 29.7031, val_MinusLogProbMetric: 29.7031

Epoch 177: val_loss improved from 29.79062 to 29.70314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 29.5633 - MinusLogProbMetric: 29.5633 - val_loss: 29.7031 - val_MinusLogProbMetric: 29.7031 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 178/1000
2023-10-30 16:40:52.847 
Epoch 178/1000 
	 loss: 29.7031, MinusLogProbMetric: 29.7031, val_loss: 29.8588, val_MinusLogProbMetric: 29.8588

Epoch 178: val_loss did not improve from 29.70314
196/196 - 28s - loss: 29.7031 - MinusLogProbMetric: 29.7031 - val_loss: 29.8588 - val_MinusLogProbMetric: 29.8588 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 179/1000
2023-10-30 16:41:25.148 
Epoch 179/1000 
	 loss: 29.7787, MinusLogProbMetric: 29.7787, val_loss: 29.8466, val_MinusLogProbMetric: 29.8466

Epoch 179: val_loss did not improve from 29.70314
196/196 - 32s - loss: 29.7787 - MinusLogProbMetric: 29.7787 - val_loss: 29.8466 - val_MinusLogProbMetric: 29.8466 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 180/1000
2023-10-30 16:41:57.814 
Epoch 180/1000 
	 loss: 29.7283, MinusLogProbMetric: 29.7283, val_loss: 30.0228, val_MinusLogProbMetric: 30.0228

Epoch 180: val_loss did not improve from 29.70314
196/196 - 33s - loss: 29.7283 - MinusLogProbMetric: 29.7283 - val_loss: 30.0228 - val_MinusLogProbMetric: 30.0228 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 181/1000
2023-10-30 16:42:33.056 
Epoch 181/1000 
	 loss: 29.6264, MinusLogProbMetric: 29.6264, val_loss: 29.6108, val_MinusLogProbMetric: 29.6108

Epoch 181: val_loss improved from 29.70314 to 29.61084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 29.6264 - MinusLogProbMetric: 29.6264 - val_loss: 29.6108 - val_MinusLogProbMetric: 29.6108 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 182/1000
2023-10-30 16:43:07.228 
Epoch 182/1000 
	 loss: 29.7838, MinusLogProbMetric: 29.7838, val_loss: 29.8241, val_MinusLogProbMetric: 29.8241

Epoch 182: val_loss did not improve from 29.61084
196/196 - 34s - loss: 29.7838 - MinusLogProbMetric: 29.7838 - val_loss: 29.8241 - val_MinusLogProbMetric: 29.8241 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 183/1000
2023-10-30 16:43:41.104 
Epoch 183/1000 
	 loss: 29.6422, MinusLogProbMetric: 29.6422, val_loss: 30.3794, val_MinusLogProbMetric: 30.3794

Epoch 183: val_loss did not improve from 29.61084
196/196 - 34s - loss: 29.6422 - MinusLogProbMetric: 29.6422 - val_loss: 30.3794 - val_MinusLogProbMetric: 30.3794 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 184/1000
2023-10-30 16:44:13.543 
Epoch 184/1000 
	 loss: 29.5178, MinusLogProbMetric: 29.5178, val_loss: 29.6067, val_MinusLogProbMetric: 29.6067

Epoch 184: val_loss improved from 29.61084 to 29.60665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 29.5178 - MinusLogProbMetric: 29.5178 - val_loss: 29.6067 - val_MinusLogProbMetric: 29.6067 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 185/1000
2023-10-30 16:44:46.505 
Epoch 185/1000 
	 loss: 29.7147, MinusLogProbMetric: 29.7147, val_loss: 30.3679, val_MinusLogProbMetric: 30.3679

Epoch 185: val_loss did not improve from 29.60665
196/196 - 32s - loss: 29.7147 - MinusLogProbMetric: 29.7147 - val_loss: 30.3679 - val_MinusLogProbMetric: 30.3679 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 186/1000
2023-10-30 16:45:15.887 
Epoch 186/1000 
	 loss: 29.7479, MinusLogProbMetric: 29.7479, val_loss: 30.8026, val_MinusLogProbMetric: 30.8026

Epoch 186: val_loss did not improve from 29.60665
196/196 - 29s - loss: 29.7479 - MinusLogProbMetric: 29.7479 - val_loss: 30.8026 - val_MinusLogProbMetric: 30.8026 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 187/1000
2023-10-30 16:45:43.759 
Epoch 187/1000 
	 loss: 29.5796, MinusLogProbMetric: 29.5796, val_loss: 29.6842, val_MinusLogProbMetric: 29.6842

Epoch 187: val_loss did not improve from 29.60665
196/196 - 28s - loss: 29.5796 - MinusLogProbMetric: 29.5796 - val_loss: 29.6842 - val_MinusLogProbMetric: 29.6842 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 188/1000
2023-10-30 16:46:12.467 
Epoch 188/1000 
	 loss: 29.6110, MinusLogProbMetric: 29.6110, val_loss: 29.8185, val_MinusLogProbMetric: 29.8185

Epoch 188: val_loss did not improve from 29.60665
196/196 - 29s - loss: 29.6110 - MinusLogProbMetric: 29.6110 - val_loss: 29.8185 - val_MinusLogProbMetric: 29.8185 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 189/1000
2023-10-30 16:46:44.520 
Epoch 189/1000 
	 loss: 29.5362, MinusLogProbMetric: 29.5362, val_loss: 29.7362, val_MinusLogProbMetric: 29.7362

Epoch 189: val_loss did not improve from 29.60665
196/196 - 32s - loss: 29.5362 - MinusLogProbMetric: 29.5362 - val_loss: 29.7362 - val_MinusLogProbMetric: 29.7362 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 190/1000
2023-10-30 16:47:18.463 
Epoch 190/1000 
	 loss: 29.5179, MinusLogProbMetric: 29.5179, val_loss: 30.6308, val_MinusLogProbMetric: 30.6308

Epoch 190: val_loss did not improve from 29.60665
196/196 - 34s - loss: 29.5179 - MinusLogProbMetric: 29.5179 - val_loss: 30.6308 - val_MinusLogProbMetric: 30.6308 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 191/1000
2023-10-30 16:47:52.966 
Epoch 191/1000 
	 loss: 29.6496, MinusLogProbMetric: 29.6496, val_loss: 30.1812, val_MinusLogProbMetric: 30.1812

Epoch 191: val_loss did not improve from 29.60665
196/196 - 34s - loss: 29.6496 - MinusLogProbMetric: 29.6496 - val_loss: 30.1812 - val_MinusLogProbMetric: 30.1812 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 192/1000
2023-10-30 16:48:27.079 
Epoch 192/1000 
	 loss: 29.5477, MinusLogProbMetric: 29.5477, val_loss: 29.8263, val_MinusLogProbMetric: 29.8263

Epoch 192: val_loss did not improve from 29.60665
196/196 - 34s - loss: 29.5477 - MinusLogProbMetric: 29.5477 - val_loss: 29.8263 - val_MinusLogProbMetric: 29.8263 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 193/1000
2023-10-30 16:49:01.510 
Epoch 193/1000 
	 loss: 29.6047, MinusLogProbMetric: 29.6047, val_loss: 30.0355, val_MinusLogProbMetric: 30.0355

Epoch 193: val_loss did not improve from 29.60665
196/196 - 34s - loss: 29.6047 - MinusLogProbMetric: 29.6047 - val_loss: 30.0355 - val_MinusLogProbMetric: 30.0355 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 194/1000
2023-10-30 16:49:35.682 
Epoch 194/1000 
	 loss: 29.6070, MinusLogProbMetric: 29.6070, val_loss: 30.2711, val_MinusLogProbMetric: 30.2711

Epoch 194: val_loss did not improve from 29.60665
196/196 - 34s - loss: 29.6070 - MinusLogProbMetric: 29.6070 - val_loss: 30.2711 - val_MinusLogProbMetric: 30.2711 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 195/1000
2023-10-30 16:50:09.480 
Epoch 195/1000 
	 loss: 29.6917, MinusLogProbMetric: 29.6917, val_loss: 31.4804, val_MinusLogProbMetric: 31.4804

Epoch 195: val_loss did not improve from 29.60665
196/196 - 34s - loss: 29.6917 - MinusLogProbMetric: 29.6917 - val_loss: 31.4804 - val_MinusLogProbMetric: 31.4804 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 196/1000
2023-10-30 16:50:42.163 
Epoch 196/1000 
	 loss: 29.6041, MinusLogProbMetric: 29.6041, val_loss: 30.5221, val_MinusLogProbMetric: 30.5221

Epoch 196: val_loss did not improve from 29.60665
196/196 - 33s - loss: 29.6041 - MinusLogProbMetric: 29.6041 - val_loss: 30.5221 - val_MinusLogProbMetric: 30.5221 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 197/1000
2023-10-30 16:51:15.288 
Epoch 197/1000 
	 loss: 29.5645, MinusLogProbMetric: 29.5645, val_loss: 30.1171, val_MinusLogProbMetric: 30.1171

Epoch 197: val_loss did not improve from 29.60665
196/196 - 33s - loss: 29.5645 - MinusLogProbMetric: 29.5645 - val_loss: 30.1171 - val_MinusLogProbMetric: 30.1171 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 198/1000
2023-10-30 16:51:48.473 
Epoch 198/1000 
	 loss: 29.5310, MinusLogProbMetric: 29.5310, val_loss: 30.4732, val_MinusLogProbMetric: 30.4732

Epoch 198: val_loss did not improve from 29.60665
196/196 - 33s - loss: 29.5310 - MinusLogProbMetric: 29.5310 - val_loss: 30.4732 - val_MinusLogProbMetric: 30.4732 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 199/1000
2023-10-30 16:52:21.162 
Epoch 199/1000 
	 loss: 29.4920, MinusLogProbMetric: 29.4920, val_loss: 29.8551, val_MinusLogProbMetric: 29.8551

Epoch 199: val_loss did not improve from 29.60665
196/196 - 33s - loss: 29.4920 - MinusLogProbMetric: 29.4920 - val_loss: 29.8551 - val_MinusLogProbMetric: 29.8551 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 200/1000
2023-10-30 16:52:49.427 
Epoch 200/1000 
	 loss: 29.5570, MinusLogProbMetric: 29.5570, val_loss: 30.1007, val_MinusLogProbMetric: 30.1007

Epoch 200: val_loss did not improve from 29.60665
196/196 - 28s - loss: 29.5570 - MinusLogProbMetric: 29.5570 - val_loss: 30.1007 - val_MinusLogProbMetric: 30.1007 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 201/1000
2023-10-30 16:53:17.430 
Epoch 201/1000 
	 loss: 29.5073, MinusLogProbMetric: 29.5073, val_loss: 30.1070, val_MinusLogProbMetric: 30.1070

Epoch 201: val_loss did not improve from 29.60665
196/196 - 28s - loss: 29.5073 - MinusLogProbMetric: 29.5073 - val_loss: 30.1070 - val_MinusLogProbMetric: 30.1070 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 202/1000
2023-10-30 16:53:46.118 
Epoch 202/1000 
	 loss: 29.5132, MinusLogProbMetric: 29.5132, val_loss: 29.7077, val_MinusLogProbMetric: 29.7077

Epoch 202: val_loss did not improve from 29.60665
196/196 - 29s - loss: 29.5132 - MinusLogProbMetric: 29.5132 - val_loss: 29.7077 - val_MinusLogProbMetric: 29.7077 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 203/1000
2023-10-30 16:54:14.660 
Epoch 203/1000 
	 loss: 29.4389, MinusLogProbMetric: 29.4389, val_loss: 29.7078, val_MinusLogProbMetric: 29.7078

Epoch 203: val_loss did not improve from 29.60665
196/196 - 29s - loss: 29.4389 - MinusLogProbMetric: 29.4389 - val_loss: 29.7078 - val_MinusLogProbMetric: 29.7078 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 204/1000
2023-10-30 16:54:47.843 
Epoch 204/1000 
	 loss: 29.4097, MinusLogProbMetric: 29.4097, val_loss: 29.3417, val_MinusLogProbMetric: 29.3417

Epoch 204: val_loss improved from 29.60665 to 29.34169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 29.4097 - MinusLogProbMetric: 29.4097 - val_loss: 29.3417 - val_MinusLogProbMetric: 29.3417 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 205/1000
2023-10-30 16:55:22.251 
Epoch 205/1000 
	 loss: 29.5599, MinusLogProbMetric: 29.5599, val_loss: 29.9472, val_MinusLogProbMetric: 29.9472

Epoch 205: val_loss did not improve from 29.34169
196/196 - 34s - loss: 29.5599 - MinusLogProbMetric: 29.5599 - val_loss: 29.9472 - val_MinusLogProbMetric: 29.9472 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 206/1000
2023-10-30 16:55:54.474 
Epoch 206/1000 
	 loss: 29.4167, MinusLogProbMetric: 29.4167, val_loss: 30.6102, val_MinusLogProbMetric: 30.6102

Epoch 206: val_loss did not improve from 29.34169
196/196 - 32s - loss: 29.4167 - MinusLogProbMetric: 29.4167 - val_loss: 30.6102 - val_MinusLogProbMetric: 30.6102 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 207/1000
2023-10-30 16:56:28.623 
Epoch 207/1000 
	 loss: 29.5088, MinusLogProbMetric: 29.5088, val_loss: 30.0103, val_MinusLogProbMetric: 30.0103

Epoch 207: val_loss did not improve from 29.34169
196/196 - 34s - loss: 29.5088 - MinusLogProbMetric: 29.5088 - val_loss: 30.0103 - val_MinusLogProbMetric: 30.0103 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 208/1000
2023-10-30 16:57:02.793 
Epoch 208/1000 
	 loss: 29.3897, MinusLogProbMetric: 29.3897, val_loss: 29.3247, val_MinusLogProbMetric: 29.3247

Epoch 208: val_loss improved from 29.34169 to 29.32467, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 29.3897 - MinusLogProbMetric: 29.3897 - val_loss: 29.3247 - val_MinusLogProbMetric: 29.3247 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 209/1000
2023-10-30 16:57:35.328 
Epoch 209/1000 
	 loss: 29.4166, MinusLogProbMetric: 29.4166, val_loss: 29.7466, val_MinusLogProbMetric: 29.7466

Epoch 209: val_loss did not improve from 29.32467
196/196 - 32s - loss: 29.4166 - MinusLogProbMetric: 29.4166 - val_loss: 29.7466 - val_MinusLogProbMetric: 29.7466 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 210/1000
2023-10-30 16:58:08.877 
Epoch 210/1000 
	 loss: 29.4169, MinusLogProbMetric: 29.4169, val_loss: 29.7329, val_MinusLogProbMetric: 29.7329

Epoch 210: val_loss did not improve from 29.32467
196/196 - 34s - loss: 29.4169 - MinusLogProbMetric: 29.4169 - val_loss: 29.7329 - val_MinusLogProbMetric: 29.7329 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 211/1000
2023-10-30 16:58:42.382 
Epoch 211/1000 
	 loss: 29.4971, MinusLogProbMetric: 29.4971, val_loss: 31.3302, val_MinusLogProbMetric: 31.3302

Epoch 211: val_loss did not improve from 29.32467
196/196 - 34s - loss: 29.4971 - MinusLogProbMetric: 29.4971 - val_loss: 31.3302 - val_MinusLogProbMetric: 31.3302 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 212/1000
2023-10-30 16:59:16.295 
Epoch 212/1000 
	 loss: 29.5124, MinusLogProbMetric: 29.5124, val_loss: 29.5004, val_MinusLogProbMetric: 29.5004

Epoch 212: val_loss did not improve from 29.32467
196/196 - 34s - loss: 29.5124 - MinusLogProbMetric: 29.5124 - val_loss: 29.5004 - val_MinusLogProbMetric: 29.5004 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 213/1000
2023-10-30 16:59:49.555 
Epoch 213/1000 
	 loss: 29.3264, MinusLogProbMetric: 29.3264, val_loss: 30.5128, val_MinusLogProbMetric: 30.5128

Epoch 213: val_loss did not improve from 29.32467
196/196 - 33s - loss: 29.3264 - MinusLogProbMetric: 29.3264 - val_loss: 30.5128 - val_MinusLogProbMetric: 30.5128 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 214/1000
2023-10-30 17:00:23.467 
Epoch 214/1000 
	 loss: 29.3244, MinusLogProbMetric: 29.3244, val_loss: 30.1527, val_MinusLogProbMetric: 30.1527

Epoch 214: val_loss did not improve from 29.32467
196/196 - 34s - loss: 29.3244 - MinusLogProbMetric: 29.3244 - val_loss: 30.1527 - val_MinusLogProbMetric: 30.1527 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 215/1000
2023-10-30 17:00:56.240 
Epoch 215/1000 
	 loss: 29.4838, MinusLogProbMetric: 29.4838, val_loss: 29.5152, val_MinusLogProbMetric: 29.5152

Epoch 215: val_loss did not improve from 29.32467
196/196 - 33s - loss: 29.4838 - MinusLogProbMetric: 29.4838 - val_loss: 29.5152 - val_MinusLogProbMetric: 29.5152 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 216/1000
2023-10-30 17:01:30.216 
Epoch 216/1000 
	 loss: 29.3130, MinusLogProbMetric: 29.3130, val_loss: 29.5854, val_MinusLogProbMetric: 29.5854

Epoch 216: val_loss did not improve from 29.32467
196/196 - 34s - loss: 29.3130 - MinusLogProbMetric: 29.3130 - val_loss: 29.5854 - val_MinusLogProbMetric: 29.5854 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 217/1000
2023-10-30 17:02:03.096 
Epoch 217/1000 
	 loss: 29.3773, MinusLogProbMetric: 29.3773, val_loss: 30.4032, val_MinusLogProbMetric: 30.4032

Epoch 217: val_loss did not improve from 29.32467
196/196 - 33s - loss: 29.3773 - MinusLogProbMetric: 29.3773 - val_loss: 30.4032 - val_MinusLogProbMetric: 30.4032 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 218/1000
2023-10-30 17:02:36.586 
Epoch 218/1000 
	 loss: 29.3672, MinusLogProbMetric: 29.3672, val_loss: 29.3052, val_MinusLogProbMetric: 29.3052

Epoch 218: val_loss improved from 29.32467 to 29.30518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 29.3672 - MinusLogProbMetric: 29.3672 - val_loss: 29.3052 - val_MinusLogProbMetric: 29.3052 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 219/1000
2023-10-30 17:03:11.341 
Epoch 219/1000 
	 loss: 29.3076, MinusLogProbMetric: 29.3076, val_loss: 29.7047, val_MinusLogProbMetric: 29.7047

Epoch 219: val_loss did not improve from 29.30518
196/196 - 34s - loss: 29.3076 - MinusLogProbMetric: 29.3076 - val_loss: 29.7047 - val_MinusLogProbMetric: 29.7047 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 220/1000
2023-10-30 17:03:44.366 
Epoch 220/1000 
	 loss: 29.3898, MinusLogProbMetric: 29.3898, val_loss: 29.3065, val_MinusLogProbMetric: 29.3065

Epoch 220: val_loss did not improve from 29.30518
196/196 - 33s - loss: 29.3898 - MinusLogProbMetric: 29.3898 - val_loss: 29.3065 - val_MinusLogProbMetric: 29.3065 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 221/1000
2023-10-30 17:04:18.410 
Epoch 221/1000 
	 loss: 29.3813, MinusLogProbMetric: 29.3813, val_loss: 30.2788, val_MinusLogProbMetric: 30.2788

Epoch 221: val_loss did not improve from 29.30518
196/196 - 34s - loss: 29.3813 - MinusLogProbMetric: 29.3813 - val_loss: 30.2788 - val_MinusLogProbMetric: 30.2788 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 222/1000
2023-10-30 17:04:51.241 
Epoch 222/1000 
	 loss: 29.3252, MinusLogProbMetric: 29.3252, val_loss: 29.8038, val_MinusLogProbMetric: 29.8038

Epoch 222: val_loss did not improve from 29.30518
196/196 - 33s - loss: 29.3252 - MinusLogProbMetric: 29.3252 - val_loss: 29.8038 - val_MinusLogProbMetric: 29.8038 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 223/1000
2023-10-30 17:05:25.090 
Epoch 223/1000 
	 loss: 29.3013, MinusLogProbMetric: 29.3013, val_loss: 29.8479, val_MinusLogProbMetric: 29.8479

Epoch 223: val_loss did not improve from 29.30518
196/196 - 34s - loss: 29.3013 - MinusLogProbMetric: 29.3013 - val_loss: 29.8479 - val_MinusLogProbMetric: 29.8479 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 224/1000
2023-10-30 17:05:58.975 
Epoch 224/1000 
	 loss: 29.3502, MinusLogProbMetric: 29.3502, val_loss: 29.5988, val_MinusLogProbMetric: 29.5988

Epoch 224: val_loss did not improve from 29.30518
196/196 - 34s - loss: 29.3502 - MinusLogProbMetric: 29.3502 - val_loss: 29.5988 - val_MinusLogProbMetric: 29.5988 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 225/1000
2023-10-30 17:06:31.675 
Epoch 225/1000 
	 loss: 29.3188, MinusLogProbMetric: 29.3188, val_loss: 29.3620, val_MinusLogProbMetric: 29.3620

Epoch 225: val_loss did not improve from 29.30518
196/196 - 33s - loss: 29.3188 - MinusLogProbMetric: 29.3188 - val_loss: 29.3620 - val_MinusLogProbMetric: 29.3620 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 226/1000
2023-10-30 17:07:05.691 
Epoch 226/1000 
	 loss: 29.3636, MinusLogProbMetric: 29.3636, val_loss: 30.2359, val_MinusLogProbMetric: 30.2359

Epoch 226: val_loss did not improve from 29.30518
196/196 - 34s - loss: 29.3636 - MinusLogProbMetric: 29.3636 - val_loss: 30.2359 - val_MinusLogProbMetric: 30.2359 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 227/1000
2023-10-30 17:07:38.847 
Epoch 227/1000 
	 loss: 29.2473, MinusLogProbMetric: 29.2473, val_loss: 30.8807, val_MinusLogProbMetric: 30.8807

Epoch 227: val_loss did not improve from 29.30518
196/196 - 33s - loss: 29.2473 - MinusLogProbMetric: 29.2473 - val_loss: 30.8807 - val_MinusLogProbMetric: 30.8807 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 228/1000
2023-10-30 17:08:13.035 
Epoch 228/1000 
	 loss: 29.4703, MinusLogProbMetric: 29.4703, val_loss: 29.3057, val_MinusLogProbMetric: 29.3057

Epoch 228: val_loss did not improve from 29.30518
196/196 - 34s - loss: 29.4703 - MinusLogProbMetric: 29.4703 - val_loss: 29.3057 - val_MinusLogProbMetric: 29.3057 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 229/1000
2023-10-30 17:08:46.283 
Epoch 229/1000 
	 loss: 29.2221, MinusLogProbMetric: 29.2221, val_loss: 30.0647, val_MinusLogProbMetric: 30.0647

Epoch 229: val_loss did not improve from 29.30518
196/196 - 33s - loss: 29.2221 - MinusLogProbMetric: 29.2221 - val_loss: 30.0647 - val_MinusLogProbMetric: 30.0647 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 230/1000
2023-10-30 17:09:19.737 
Epoch 230/1000 
	 loss: 29.2717, MinusLogProbMetric: 29.2717, val_loss: 30.1125, val_MinusLogProbMetric: 30.1125

Epoch 230: val_loss did not improve from 29.30518
196/196 - 33s - loss: 29.2717 - MinusLogProbMetric: 29.2717 - val_loss: 30.1125 - val_MinusLogProbMetric: 30.1125 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 231/1000
2023-10-30 17:09:53.098 
Epoch 231/1000 
	 loss: 29.4566, MinusLogProbMetric: 29.4566, val_loss: 29.2093, val_MinusLogProbMetric: 29.2093

Epoch 231: val_loss improved from 29.30518 to 29.20927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 29.4566 - MinusLogProbMetric: 29.4566 - val_loss: 29.2093 - val_MinusLogProbMetric: 29.2093 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 232/1000
2023-10-30 17:10:27.182 
Epoch 232/1000 
	 loss: 29.2084, MinusLogProbMetric: 29.2084, val_loss: 30.0227, val_MinusLogProbMetric: 30.0227

Epoch 232: val_loss did not improve from 29.20927
196/196 - 34s - loss: 29.2084 - MinusLogProbMetric: 29.2084 - val_loss: 30.0227 - val_MinusLogProbMetric: 30.0227 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 233/1000
2023-10-30 17:11:01.477 
Epoch 233/1000 
	 loss: 29.2766, MinusLogProbMetric: 29.2766, val_loss: 29.9058, val_MinusLogProbMetric: 29.9058

Epoch 233: val_loss did not improve from 29.20927
196/196 - 34s - loss: 29.2766 - MinusLogProbMetric: 29.2766 - val_loss: 29.9058 - val_MinusLogProbMetric: 29.9058 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 234/1000
2023-10-30 17:11:33.338 
Epoch 234/1000 
	 loss: 29.2715, MinusLogProbMetric: 29.2715, val_loss: 29.7688, val_MinusLogProbMetric: 29.7688

Epoch 234: val_loss did not improve from 29.20927
196/196 - 32s - loss: 29.2715 - MinusLogProbMetric: 29.2715 - val_loss: 29.7688 - val_MinusLogProbMetric: 29.7688 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 235/1000
2023-10-30 17:12:01.987 
Epoch 235/1000 
	 loss: 29.2778, MinusLogProbMetric: 29.2778, val_loss: 29.5253, val_MinusLogProbMetric: 29.5253

Epoch 235: val_loss did not improve from 29.20927
196/196 - 29s - loss: 29.2778 - MinusLogProbMetric: 29.2778 - val_loss: 29.5253 - val_MinusLogProbMetric: 29.5253 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 236/1000
2023-10-30 17:12:31.863 
Epoch 236/1000 
	 loss: 29.4375, MinusLogProbMetric: 29.4375, val_loss: 31.9136, val_MinusLogProbMetric: 31.9136

Epoch 236: val_loss did not improve from 29.20927
196/196 - 30s - loss: 29.4375 - MinusLogProbMetric: 29.4375 - val_loss: 31.9136 - val_MinusLogProbMetric: 31.9136 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 237/1000
2023-10-30 17:13:03.157 
Epoch 237/1000 
	 loss: 29.3694, MinusLogProbMetric: 29.3694, val_loss: 29.6347, val_MinusLogProbMetric: 29.6347

Epoch 237: val_loss did not improve from 29.20927
196/196 - 31s - loss: 29.3694 - MinusLogProbMetric: 29.3694 - val_loss: 29.6347 - val_MinusLogProbMetric: 29.6347 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 238/1000
2023-10-30 17:13:36.662 
Epoch 238/1000 
	 loss: 29.2583, MinusLogProbMetric: 29.2583, val_loss: 29.4856, val_MinusLogProbMetric: 29.4856

Epoch 238: val_loss did not improve from 29.20927
196/196 - 34s - loss: 29.2583 - MinusLogProbMetric: 29.2583 - val_loss: 29.4856 - val_MinusLogProbMetric: 29.4856 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 239/1000
2023-10-30 17:14:10.841 
Epoch 239/1000 
	 loss: 29.4125, MinusLogProbMetric: 29.4125, val_loss: 29.7082, val_MinusLogProbMetric: 29.7082

Epoch 239: val_loss did not improve from 29.20927
196/196 - 34s - loss: 29.4125 - MinusLogProbMetric: 29.4125 - val_loss: 29.7082 - val_MinusLogProbMetric: 29.7082 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 240/1000
2023-10-30 17:14:44.167 
Epoch 240/1000 
	 loss: 29.3594, MinusLogProbMetric: 29.3594, val_loss: 30.1171, val_MinusLogProbMetric: 30.1171

Epoch 240: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.3594 - MinusLogProbMetric: 29.3594 - val_loss: 30.1171 - val_MinusLogProbMetric: 30.1171 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 241/1000
2023-10-30 17:15:17.569 
Epoch 241/1000 
	 loss: 29.2485, MinusLogProbMetric: 29.2485, val_loss: 29.3475, val_MinusLogProbMetric: 29.3475

Epoch 241: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.2485 - MinusLogProbMetric: 29.2485 - val_loss: 29.3475 - val_MinusLogProbMetric: 29.3475 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 242/1000
2023-10-30 17:15:50.798 
Epoch 242/1000 
	 loss: 29.2269, MinusLogProbMetric: 29.2269, val_loss: 29.3373, val_MinusLogProbMetric: 29.3373

Epoch 242: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.2269 - MinusLogProbMetric: 29.2269 - val_loss: 29.3373 - val_MinusLogProbMetric: 29.3373 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 243/1000
2023-10-30 17:16:24.338 
Epoch 243/1000 
	 loss: 29.2864, MinusLogProbMetric: 29.2864, val_loss: 29.7652, val_MinusLogProbMetric: 29.7652

Epoch 243: val_loss did not improve from 29.20927
196/196 - 34s - loss: 29.2864 - MinusLogProbMetric: 29.2864 - val_loss: 29.7652 - val_MinusLogProbMetric: 29.7652 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 244/1000
2023-10-30 17:16:57.092 
Epoch 244/1000 
	 loss: 29.3186, MinusLogProbMetric: 29.3186, val_loss: 29.2325, val_MinusLogProbMetric: 29.2325

Epoch 244: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.3186 - MinusLogProbMetric: 29.3186 - val_loss: 29.2325 - val_MinusLogProbMetric: 29.2325 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 245/1000
2023-10-30 17:17:30.918 
Epoch 245/1000 
	 loss: 29.1333, MinusLogProbMetric: 29.1333, val_loss: 29.4172, val_MinusLogProbMetric: 29.4172

Epoch 245: val_loss did not improve from 29.20927
196/196 - 34s - loss: 29.1333 - MinusLogProbMetric: 29.1333 - val_loss: 29.4172 - val_MinusLogProbMetric: 29.4172 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 246/1000
2023-10-30 17:18:04.145 
Epoch 246/1000 
	 loss: 29.2388, MinusLogProbMetric: 29.2388, val_loss: 29.8469, val_MinusLogProbMetric: 29.8469

Epoch 246: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.2388 - MinusLogProbMetric: 29.2388 - val_loss: 29.8469 - val_MinusLogProbMetric: 29.8469 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 247/1000
2023-10-30 17:18:37.480 
Epoch 247/1000 
	 loss: 29.1409, MinusLogProbMetric: 29.1409, val_loss: 30.1087, val_MinusLogProbMetric: 30.1087

Epoch 247: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.1409 - MinusLogProbMetric: 29.1409 - val_loss: 30.1087 - val_MinusLogProbMetric: 30.1087 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 248/1000
2023-10-30 17:19:11.508 
Epoch 248/1000 
	 loss: 29.1870, MinusLogProbMetric: 29.1870, val_loss: 29.3099, val_MinusLogProbMetric: 29.3099

Epoch 248: val_loss did not improve from 29.20927
196/196 - 34s - loss: 29.1870 - MinusLogProbMetric: 29.1870 - val_loss: 29.3099 - val_MinusLogProbMetric: 29.3099 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 249/1000
2023-10-30 17:19:45.528 
Epoch 249/1000 
	 loss: 29.2925, MinusLogProbMetric: 29.2925, val_loss: 29.5678, val_MinusLogProbMetric: 29.5678

Epoch 249: val_loss did not improve from 29.20927
196/196 - 34s - loss: 29.2925 - MinusLogProbMetric: 29.2925 - val_loss: 29.5678 - val_MinusLogProbMetric: 29.5678 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 250/1000
2023-10-30 17:20:18.890 
Epoch 250/1000 
	 loss: 29.2176, MinusLogProbMetric: 29.2176, val_loss: 30.8461, val_MinusLogProbMetric: 30.8461

Epoch 250: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.2176 - MinusLogProbMetric: 29.2176 - val_loss: 30.8461 - val_MinusLogProbMetric: 30.8461 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 251/1000
2023-10-30 17:20:52.050 
Epoch 251/1000 
	 loss: 29.1192, MinusLogProbMetric: 29.1192, val_loss: 29.9844, val_MinusLogProbMetric: 29.9844

Epoch 251: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.1192 - MinusLogProbMetric: 29.1192 - val_loss: 29.9844 - val_MinusLogProbMetric: 29.9844 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 252/1000
2023-10-30 17:21:25.541 
Epoch 252/1000 
	 loss: 29.3087, MinusLogProbMetric: 29.3087, val_loss: 29.5913, val_MinusLogProbMetric: 29.5913

Epoch 252: val_loss did not improve from 29.20927
196/196 - 33s - loss: 29.3087 - MinusLogProbMetric: 29.3087 - val_loss: 29.5913 - val_MinusLogProbMetric: 29.5913 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 253/1000
2023-10-30 17:21:56.524 
Epoch 253/1000 
	 loss: 29.1804, MinusLogProbMetric: 29.1804, val_loss: 29.4133, val_MinusLogProbMetric: 29.4133

Epoch 253: val_loss did not improve from 29.20927
196/196 - 31s - loss: 29.1804 - MinusLogProbMetric: 29.1804 - val_loss: 29.4133 - val_MinusLogProbMetric: 29.4133 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 254/1000
2023-10-30 17:22:24.655 
Epoch 254/1000 
	 loss: 29.1776, MinusLogProbMetric: 29.1776, val_loss: 29.1950, val_MinusLogProbMetric: 29.1950

Epoch 254: val_loss improved from 29.20927 to 29.19498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 29.1776 - MinusLogProbMetric: 29.1776 - val_loss: 29.1950 - val_MinusLogProbMetric: 29.1950 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 255/1000
2023-10-30 17:22:53.334 
Epoch 255/1000 
	 loss: 29.1829, MinusLogProbMetric: 29.1829, val_loss: 29.4734, val_MinusLogProbMetric: 29.4734

Epoch 255: val_loss did not improve from 29.19498
196/196 - 28s - loss: 29.1829 - MinusLogProbMetric: 29.1829 - val_loss: 29.4734 - val_MinusLogProbMetric: 29.4734 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 256/1000
2023-10-30 17:23:21.409 
Epoch 256/1000 
	 loss: 29.2020, MinusLogProbMetric: 29.2020, val_loss: 29.3680, val_MinusLogProbMetric: 29.3680

Epoch 256: val_loss did not improve from 29.19498
196/196 - 28s - loss: 29.2020 - MinusLogProbMetric: 29.2020 - val_loss: 29.3680 - val_MinusLogProbMetric: 29.3680 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 257/1000
2023-10-30 17:23:51.016 
Epoch 257/1000 
	 loss: 29.2686, MinusLogProbMetric: 29.2686, val_loss: 29.5438, val_MinusLogProbMetric: 29.5438

Epoch 257: val_loss did not improve from 29.19498
196/196 - 30s - loss: 29.2686 - MinusLogProbMetric: 29.2686 - val_loss: 29.5438 - val_MinusLogProbMetric: 29.5438 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 258/1000
2023-10-30 17:24:22.728 
Epoch 258/1000 
	 loss: 29.0766, MinusLogProbMetric: 29.0766, val_loss: 30.2471, val_MinusLogProbMetric: 30.2471

Epoch 258: val_loss did not improve from 29.19498
196/196 - 32s - loss: 29.0766 - MinusLogProbMetric: 29.0766 - val_loss: 30.2471 - val_MinusLogProbMetric: 30.2471 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 259/1000
2023-10-30 17:24:56.554 
Epoch 259/1000 
	 loss: 29.1472, MinusLogProbMetric: 29.1472, val_loss: 29.4599, val_MinusLogProbMetric: 29.4599

Epoch 259: val_loss did not improve from 29.19498
196/196 - 34s - loss: 29.1472 - MinusLogProbMetric: 29.1472 - val_loss: 29.4599 - val_MinusLogProbMetric: 29.4599 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 260/1000
2023-10-30 17:25:29.930 
Epoch 260/1000 
	 loss: 29.1790, MinusLogProbMetric: 29.1790, val_loss: 30.0979, val_MinusLogProbMetric: 30.0979

Epoch 260: val_loss did not improve from 29.19498
196/196 - 33s - loss: 29.1790 - MinusLogProbMetric: 29.1790 - val_loss: 30.0979 - val_MinusLogProbMetric: 30.0979 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 261/1000
2023-10-30 17:26:03.982 
Epoch 261/1000 
	 loss: 29.0498, MinusLogProbMetric: 29.0498, val_loss: 29.3156, val_MinusLogProbMetric: 29.3156

Epoch 261: val_loss did not improve from 29.19498
196/196 - 34s - loss: 29.0498 - MinusLogProbMetric: 29.0498 - val_loss: 29.3156 - val_MinusLogProbMetric: 29.3156 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 262/1000
2023-10-30 17:26:37.756 
Epoch 262/1000 
	 loss: 29.1275, MinusLogProbMetric: 29.1275, val_loss: 29.3224, val_MinusLogProbMetric: 29.3224

Epoch 262: val_loss did not improve from 29.19498
196/196 - 34s - loss: 29.1275 - MinusLogProbMetric: 29.1275 - val_loss: 29.3224 - val_MinusLogProbMetric: 29.3224 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 263/1000
2023-10-30 17:27:10.748 
Epoch 263/1000 
	 loss: 29.0552, MinusLogProbMetric: 29.0552, val_loss: 29.4194, val_MinusLogProbMetric: 29.4194

Epoch 263: val_loss did not improve from 29.19498
196/196 - 33s - loss: 29.0552 - MinusLogProbMetric: 29.0552 - val_loss: 29.4194 - val_MinusLogProbMetric: 29.4194 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 264/1000
2023-10-30 17:27:44.349 
Epoch 264/1000 
	 loss: 29.0633, MinusLogProbMetric: 29.0633, val_loss: 29.2948, val_MinusLogProbMetric: 29.2948

Epoch 264: val_loss did not improve from 29.19498
196/196 - 34s - loss: 29.0633 - MinusLogProbMetric: 29.0633 - val_loss: 29.2948 - val_MinusLogProbMetric: 29.2948 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 265/1000
2023-10-30 17:28:17.208 
Epoch 265/1000 
	 loss: 29.0923, MinusLogProbMetric: 29.0923, val_loss: 29.6299, val_MinusLogProbMetric: 29.6299

Epoch 265: val_loss did not improve from 29.19498
196/196 - 33s - loss: 29.0923 - MinusLogProbMetric: 29.0923 - val_loss: 29.6299 - val_MinusLogProbMetric: 29.6299 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 266/1000
2023-10-30 17:28:50.479 
Epoch 266/1000 
	 loss: 29.1174, MinusLogProbMetric: 29.1174, val_loss: 29.0748, val_MinusLogProbMetric: 29.0748

Epoch 266: val_loss improved from 29.19498 to 29.07475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 29.1174 - MinusLogProbMetric: 29.1174 - val_loss: 29.0748 - val_MinusLogProbMetric: 29.0748 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 267/1000
2023-10-30 17:29:24.970 
Epoch 267/1000 
	 loss: 29.1573, MinusLogProbMetric: 29.1573, val_loss: 30.3385, val_MinusLogProbMetric: 30.3385

Epoch 267: val_loss did not improve from 29.07475
196/196 - 34s - loss: 29.1573 - MinusLogProbMetric: 29.1573 - val_loss: 30.3385 - val_MinusLogProbMetric: 30.3385 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 268/1000
2023-10-30 17:29:58.976 
Epoch 268/1000 
	 loss: 29.1276, MinusLogProbMetric: 29.1276, val_loss: 30.0518, val_MinusLogProbMetric: 30.0518

Epoch 268: val_loss did not improve from 29.07475
196/196 - 34s - loss: 29.1276 - MinusLogProbMetric: 29.1276 - val_loss: 30.0518 - val_MinusLogProbMetric: 30.0518 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 269/1000
2023-10-30 17:30:31.215 
Epoch 269/1000 
	 loss: 29.1189, MinusLogProbMetric: 29.1189, val_loss: 29.4328, val_MinusLogProbMetric: 29.4328

Epoch 269: val_loss did not improve from 29.07475
196/196 - 32s - loss: 29.1189 - MinusLogProbMetric: 29.1189 - val_loss: 29.4328 - val_MinusLogProbMetric: 29.4328 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 270/1000
2023-10-30 17:31:05.524 
Epoch 270/1000 
	 loss: 29.1274, MinusLogProbMetric: 29.1274, val_loss: 30.7473, val_MinusLogProbMetric: 30.7473

Epoch 270: val_loss did not improve from 29.07475
196/196 - 34s - loss: 29.1274 - MinusLogProbMetric: 29.1274 - val_loss: 30.7473 - val_MinusLogProbMetric: 30.7473 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 271/1000
2023-10-30 17:31:39.251 
Epoch 271/1000 
	 loss: 29.0299, MinusLogProbMetric: 29.0299, val_loss: 29.8080, val_MinusLogProbMetric: 29.8080

Epoch 271: val_loss did not improve from 29.07475
196/196 - 34s - loss: 29.0299 - MinusLogProbMetric: 29.0299 - val_loss: 29.8080 - val_MinusLogProbMetric: 29.8080 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 272/1000
2023-10-30 17:32:12.826 
Epoch 272/1000 
	 loss: 29.2590, MinusLogProbMetric: 29.2590, val_loss: 29.2574, val_MinusLogProbMetric: 29.2574

Epoch 272: val_loss did not improve from 29.07475
196/196 - 34s - loss: 29.2590 - MinusLogProbMetric: 29.2590 - val_loss: 29.2574 - val_MinusLogProbMetric: 29.2574 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 273/1000
2023-10-30 17:32:45.602 
Epoch 273/1000 
	 loss: 28.9788, MinusLogProbMetric: 28.9788, val_loss: 29.4188, val_MinusLogProbMetric: 29.4188

Epoch 273: val_loss did not improve from 29.07475
196/196 - 33s - loss: 28.9788 - MinusLogProbMetric: 28.9788 - val_loss: 29.4188 - val_MinusLogProbMetric: 29.4188 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 274/1000
2023-10-30 17:33:18.513 
Epoch 274/1000 
	 loss: 29.0606, MinusLogProbMetric: 29.0606, val_loss: 29.3661, val_MinusLogProbMetric: 29.3661

Epoch 274: val_loss did not improve from 29.07475
196/196 - 33s - loss: 29.0606 - MinusLogProbMetric: 29.0606 - val_loss: 29.3661 - val_MinusLogProbMetric: 29.3661 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 275/1000
2023-10-30 17:33:50.654 
Epoch 275/1000 
	 loss: 29.0411, MinusLogProbMetric: 29.0411, val_loss: 29.5799, val_MinusLogProbMetric: 29.5799

Epoch 275: val_loss did not improve from 29.07475
196/196 - 32s - loss: 29.0411 - MinusLogProbMetric: 29.0411 - val_loss: 29.5799 - val_MinusLogProbMetric: 29.5799 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 276/1000
2023-10-30 17:34:18.054 
Epoch 276/1000 
	 loss: 28.9912, MinusLogProbMetric: 28.9912, val_loss: 29.8413, val_MinusLogProbMetric: 29.8413

Epoch 276: val_loss did not improve from 29.07475
196/196 - 27s - loss: 28.9912 - MinusLogProbMetric: 28.9912 - val_loss: 29.8413 - val_MinusLogProbMetric: 29.8413 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 277/1000
2023-10-30 17:34:48.360 
Epoch 277/1000 
	 loss: 29.1190, MinusLogProbMetric: 29.1190, val_loss: 29.5247, val_MinusLogProbMetric: 29.5247

Epoch 277: val_loss did not improve from 29.07475
196/196 - 30s - loss: 29.1190 - MinusLogProbMetric: 29.1190 - val_loss: 29.5247 - val_MinusLogProbMetric: 29.5247 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 278/1000
2023-10-30 17:35:18.842 
Epoch 278/1000 
	 loss: 29.3520, MinusLogProbMetric: 29.3520, val_loss: 29.9074, val_MinusLogProbMetric: 29.9074

Epoch 278: val_loss did not improve from 29.07475
196/196 - 30s - loss: 29.3520 - MinusLogProbMetric: 29.3520 - val_loss: 29.9074 - val_MinusLogProbMetric: 29.9074 - lr: 0.0010 - 30s/epoch - 156ms/step
Epoch 279/1000
2023-10-30 17:35:48.091 
Epoch 279/1000 
	 loss: 29.0221, MinusLogProbMetric: 29.0221, val_loss: 29.1255, val_MinusLogProbMetric: 29.1255

Epoch 279: val_loss did not improve from 29.07475
196/196 - 29s - loss: 29.0221 - MinusLogProbMetric: 29.0221 - val_loss: 29.1255 - val_MinusLogProbMetric: 29.1255 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 280/1000
2023-10-30 17:36:22.295 
Epoch 280/1000 
	 loss: 28.9666, MinusLogProbMetric: 28.9666, val_loss: 30.7300, val_MinusLogProbMetric: 30.7300

Epoch 280: val_loss did not improve from 29.07475
196/196 - 34s - loss: 28.9666 - MinusLogProbMetric: 28.9666 - val_loss: 30.7300 - val_MinusLogProbMetric: 30.7300 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 281/1000
2023-10-30 17:36:54.867 
Epoch 281/1000 
	 loss: 29.1325, MinusLogProbMetric: 29.1325, val_loss: 30.4620, val_MinusLogProbMetric: 30.4620

Epoch 281: val_loss did not improve from 29.07475
196/196 - 33s - loss: 29.1325 - MinusLogProbMetric: 29.1325 - val_loss: 30.4620 - val_MinusLogProbMetric: 30.4620 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 282/1000
2023-10-30 17:37:28.108 
Epoch 282/1000 
	 loss: 29.0360, MinusLogProbMetric: 29.0360, val_loss: 28.9909, val_MinusLogProbMetric: 28.9909

Epoch 282: val_loss improved from 29.07475 to 28.99094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 29.0360 - MinusLogProbMetric: 29.0360 - val_loss: 28.9909 - val_MinusLogProbMetric: 28.9909 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 283/1000
2023-10-30 17:38:03.223 
Epoch 283/1000 
	 loss: 28.9783, MinusLogProbMetric: 28.9783, val_loss: 30.3874, val_MinusLogProbMetric: 30.3874

Epoch 283: val_loss did not improve from 28.99094
196/196 - 35s - loss: 28.9783 - MinusLogProbMetric: 28.9783 - val_loss: 30.3874 - val_MinusLogProbMetric: 30.3874 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 284/1000
2023-10-30 17:38:37.881 
Epoch 284/1000 
	 loss: 29.0205, MinusLogProbMetric: 29.0205, val_loss: 29.0096, val_MinusLogProbMetric: 29.0096

Epoch 284: val_loss did not improve from 28.99094
196/196 - 35s - loss: 29.0205 - MinusLogProbMetric: 29.0205 - val_loss: 29.0096 - val_MinusLogProbMetric: 29.0096 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 285/1000
2023-10-30 17:39:12.003 
Epoch 285/1000 
	 loss: 29.0506, MinusLogProbMetric: 29.0506, val_loss: 29.7971, val_MinusLogProbMetric: 29.7971

Epoch 285: val_loss did not improve from 28.99094
196/196 - 34s - loss: 29.0506 - MinusLogProbMetric: 29.0506 - val_loss: 29.7971 - val_MinusLogProbMetric: 29.7971 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 286/1000
2023-10-30 17:39:45.827 
Epoch 286/1000 
	 loss: 28.9628, MinusLogProbMetric: 28.9628, val_loss: 30.3575, val_MinusLogProbMetric: 30.3575

Epoch 286: val_loss did not improve from 28.99094
196/196 - 34s - loss: 28.9628 - MinusLogProbMetric: 28.9628 - val_loss: 30.3575 - val_MinusLogProbMetric: 30.3575 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 287/1000
2023-10-30 17:40:19.939 
Epoch 287/1000 
	 loss: 29.0612, MinusLogProbMetric: 29.0612, val_loss: 31.0077, val_MinusLogProbMetric: 31.0077

Epoch 287: val_loss did not improve from 28.99094
196/196 - 34s - loss: 29.0612 - MinusLogProbMetric: 29.0612 - val_loss: 31.0077 - val_MinusLogProbMetric: 31.0077 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 288/1000
2023-10-30 17:40:53.944 
Epoch 288/1000 
	 loss: 29.1069, MinusLogProbMetric: 29.1069, val_loss: 29.5799, val_MinusLogProbMetric: 29.5799

Epoch 288: val_loss did not improve from 28.99094
196/196 - 34s - loss: 29.1069 - MinusLogProbMetric: 29.1069 - val_loss: 29.5799 - val_MinusLogProbMetric: 29.5799 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 289/1000
2023-10-30 17:41:27.714 
Epoch 289/1000 
	 loss: 28.9400, MinusLogProbMetric: 28.9400, val_loss: 29.0962, val_MinusLogProbMetric: 29.0962

Epoch 289: val_loss did not improve from 28.99094
196/196 - 34s - loss: 28.9400 - MinusLogProbMetric: 28.9400 - val_loss: 29.0962 - val_MinusLogProbMetric: 29.0962 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 290/1000
2023-10-30 17:42:02.374 
Epoch 290/1000 
	 loss: 28.9854, MinusLogProbMetric: 28.9854, val_loss: 29.4773, val_MinusLogProbMetric: 29.4773

Epoch 290: val_loss did not improve from 28.99094
196/196 - 35s - loss: 28.9854 - MinusLogProbMetric: 28.9854 - val_loss: 29.4773 - val_MinusLogProbMetric: 29.4773 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 291/1000
2023-10-30 17:42:36.631 
Epoch 291/1000 
	 loss: 28.9331, MinusLogProbMetric: 28.9331, val_loss: 29.2071, val_MinusLogProbMetric: 29.2071

Epoch 291: val_loss did not improve from 28.99094
196/196 - 34s - loss: 28.9331 - MinusLogProbMetric: 28.9331 - val_loss: 29.2071 - val_MinusLogProbMetric: 29.2071 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 292/1000
2023-10-30 17:43:09.862 
Epoch 292/1000 
	 loss: 29.0201, MinusLogProbMetric: 29.0201, val_loss: 29.5860, val_MinusLogProbMetric: 29.5860

Epoch 292: val_loss did not improve from 28.99094
196/196 - 33s - loss: 29.0201 - MinusLogProbMetric: 29.0201 - val_loss: 29.5860 - val_MinusLogProbMetric: 29.5860 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 293/1000
2023-10-30 17:43:39.492 
Epoch 293/1000 
	 loss: 29.0390, MinusLogProbMetric: 29.0390, val_loss: 29.1621, val_MinusLogProbMetric: 29.1621

Epoch 293: val_loss did not improve from 28.99094
196/196 - 30s - loss: 29.0390 - MinusLogProbMetric: 29.0390 - val_loss: 29.1621 - val_MinusLogProbMetric: 29.1621 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 294/1000
2023-10-30 17:44:11.836 
Epoch 294/1000 
	 loss: 28.8878, MinusLogProbMetric: 28.8878, val_loss: 29.4401, val_MinusLogProbMetric: 29.4401

Epoch 294: val_loss did not improve from 28.99094
196/196 - 32s - loss: 28.8878 - MinusLogProbMetric: 28.8878 - val_loss: 29.4401 - val_MinusLogProbMetric: 29.4401 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 295/1000
2023-10-30 17:44:45.878 
Epoch 295/1000 
	 loss: 28.9338, MinusLogProbMetric: 28.9338, val_loss: 28.8944, val_MinusLogProbMetric: 28.8944

Epoch 295: val_loss improved from 28.99094 to 28.89444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 28.9338 - MinusLogProbMetric: 28.9338 - val_loss: 28.8944 - val_MinusLogProbMetric: 28.8944 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 296/1000
2023-10-30 17:45:20.231 
Epoch 296/1000 
	 loss: 29.1036, MinusLogProbMetric: 29.1036, val_loss: 29.2917, val_MinusLogProbMetric: 29.2917

Epoch 296: val_loss did not improve from 28.89444
196/196 - 34s - loss: 29.1036 - MinusLogProbMetric: 29.1036 - val_loss: 29.2917 - val_MinusLogProbMetric: 29.2917 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 297/1000
2023-10-30 17:45:52.704 
Epoch 297/1000 
	 loss: 28.9316, MinusLogProbMetric: 28.9316, val_loss: 29.3750, val_MinusLogProbMetric: 29.3750

Epoch 297: val_loss did not improve from 28.89444
196/196 - 32s - loss: 28.9316 - MinusLogProbMetric: 28.9316 - val_loss: 29.3750 - val_MinusLogProbMetric: 29.3750 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 298/1000
2023-10-30 17:46:24.862 
Epoch 298/1000 
	 loss: 28.9172, MinusLogProbMetric: 28.9172, val_loss: 30.2512, val_MinusLogProbMetric: 30.2512

Epoch 298: val_loss did not improve from 28.89444
196/196 - 32s - loss: 28.9172 - MinusLogProbMetric: 28.9172 - val_loss: 30.2512 - val_MinusLogProbMetric: 30.2512 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 299/1000
2023-10-30 17:46:58.487 
Epoch 299/1000 
	 loss: 28.9644, MinusLogProbMetric: 28.9644, val_loss: 29.4046, val_MinusLogProbMetric: 29.4046

Epoch 299: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.9644 - MinusLogProbMetric: 28.9644 - val_loss: 29.4046 - val_MinusLogProbMetric: 29.4046 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 300/1000
2023-10-30 17:47:32.462 
Epoch 300/1000 
	 loss: 28.9098, MinusLogProbMetric: 28.9098, val_loss: 29.3878, val_MinusLogProbMetric: 29.3878

Epoch 300: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.9098 - MinusLogProbMetric: 28.9098 - val_loss: 29.3878 - val_MinusLogProbMetric: 29.3878 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 301/1000
2023-10-30 17:48:06.073 
Epoch 301/1000 
	 loss: 28.9820, MinusLogProbMetric: 28.9820, val_loss: 29.2075, val_MinusLogProbMetric: 29.2075

Epoch 301: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.9820 - MinusLogProbMetric: 28.9820 - val_loss: 29.2075 - val_MinusLogProbMetric: 29.2075 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 302/1000
2023-10-30 17:48:38.697 
Epoch 302/1000 
	 loss: 28.9249, MinusLogProbMetric: 28.9249, val_loss: 29.0371, val_MinusLogProbMetric: 29.0371

Epoch 302: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.9249 - MinusLogProbMetric: 28.9249 - val_loss: 29.0371 - val_MinusLogProbMetric: 29.0371 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 303/1000
2023-10-30 17:49:11.286 
Epoch 303/1000 
	 loss: 28.9885, MinusLogProbMetric: 28.9885, val_loss: 29.4120, val_MinusLogProbMetric: 29.4120

Epoch 303: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.9885 - MinusLogProbMetric: 28.9885 - val_loss: 29.4120 - val_MinusLogProbMetric: 29.4120 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 304/1000
2023-10-30 17:49:45.099 
Epoch 304/1000 
	 loss: 28.9365, MinusLogProbMetric: 28.9365, val_loss: 29.5612, val_MinusLogProbMetric: 29.5612

Epoch 304: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.9365 - MinusLogProbMetric: 28.9365 - val_loss: 29.5612 - val_MinusLogProbMetric: 29.5612 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 305/1000
2023-10-30 17:50:18.592 
Epoch 305/1000 
	 loss: 28.8861, MinusLogProbMetric: 28.8861, val_loss: 29.4686, val_MinusLogProbMetric: 29.4686

Epoch 305: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.8861 - MinusLogProbMetric: 28.8861 - val_loss: 29.4686 - val_MinusLogProbMetric: 29.4686 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 306/1000
2023-10-30 17:50:47.884 
Epoch 306/1000 
	 loss: 28.8545, MinusLogProbMetric: 28.8545, val_loss: 28.9676, val_MinusLogProbMetric: 28.9676

Epoch 306: val_loss did not improve from 28.89444
196/196 - 29s - loss: 28.8545 - MinusLogProbMetric: 28.8545 - val_loss: 28.9676 - val_MinusLogProbMetric: 28.9676 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 307/1000
2023-10-30 17:51:19.603 
Epoch 307/1000 
	 loss: 28.8473, MinusLogProbMetric: 28.8473, val_loss: 29.4802, val_MinusLogProbMetric: 29.4802

Epoch 307: val_loss did not improve from 28.89444
196/196 - 32s - loss: 28.8473 - MinusLogProbMetric: 28.8473 - val_loss: 29.4802 - val_MinusLogProbMetric: 29.4802 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 308/1000
2023-10-30 17:51:52.862 
Epoch 308/1000 
	 loss: 28.8827, MinusLogProbMetric: 28.8827, val_loss: 29.3805, val_MinusLogProbMetric: 29.3805

Epoch 308: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.8827 - MinusLogProbMetric: 28.8827 - val_loss: 29.3805 - val_MinusLogProbMetric: 29.3805 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 309/1000
2023-10-30 17:52:26.500 
Epoch 309/1000 
	 loss: 28.9991, MinusLogProbMetric: 28.9991, val_loss: 29.1137, val_MinusLogProbMetric: 29.1137

Epoch 309: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.9991 - MinusLogProbMetric: 28.9991 - val_loss: 29.1137 - val_MinusLogProbMetric: 29.1137 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 310/1000
2023-10-30 17:53:00.542 
Epoch 310/1000 
	 loss: 28.7956, MinusLogProbMetric: 28.7956, val_loss: 30.4993, val_MinusLogProbMetric: 30.4993

Epoch 310: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.7956 - MinusLogProbMetric: 28.7956 - val_loss: 30.4993 - val_MinusLogProbMetric: 30.4993 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 311/1000
2023-10-30 17:53:33.946 
Epoch 311/1000 
	 loss: 29.0228, MinusLogProbMetric: 29.0228, val_loss: 29.4591, val_MinusLogProbMetric: 29.4591

Epoch 311: val_loss did not improve from 28.89444
196/196 - 33s - loss: 29.0228 - MinusLogProbMetric: 29.0228 - val_loss: 29.4591 - val_MinusLogProbMetric: 29.4591 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 312/1000
2023-10-30 17:54:06.073 
Epoch 312/1000 
	 loss: 29.0206, MinusLogProbMetric: 29.0206, val_loss: 29.0324, val_MinusLogProbMetric: 29.0324

Epoch 312: val_loss did not improve from 28.89444
196/196 - 32s - loss: 29.0206 - MinusLogProbMetric: 29.0206 - val_loss: 29.0324 - val_MinusLogProbMetric: 29.0324 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 313/1000
2023-10-30 17:54:39.996 
Epoch 313/1000 
	 loss: 28.8472, MinusLogProbMetric: 28.8472, val_loss: 30.0708, val_MinusLogProbMetric: 30.0708

Epoch 313: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.8472 - MinusLogProbMetric: 28.8472 - val_loss: 30.0708 - val_MinusLogProbMetric: 30.0708 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 314/1000
2023-10-30 17:55:13.557 
Epoch 314/1000 
	 loss: 28.8391, MinusLogProbMetric: 28.8391, val_loss: 29.1095, val_MinusLogProbMetric: 29.1095

Epoch 314: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.8391 - MinusLogProbMetric: 28.8391 - val_loss: 29.1095 - val_MinusLogProbMetric: 29.1095 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 315/1000
2023-10-30 17:55:47.325 
Epoch 315/1000 
	 loss: 28.8668, MinusLogProbMetric: 28.8668, val_loss: 29.4106, val_MinusLogProbMetric: 29.4106

Epoch 315: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.8668 - MinusLogProbMetric: 28.8668 - val_loss: 29.4106 - val_MinusLogProbMetric: 29.4106 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 316/1000
2023-10-30 17:56:20.435 
Epoch 316/1000 
	 loss: 28.8295, MinusLogProbMetric: 28.8295, val_loss: 29.0259, val_MinusLogProbMetric: 29.0259

Epoch 316: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.8295 - MinusLogProbMetric: 28.8295 - val_loss: 29.0259 - val_MinusLogProbMetric: 29.0259 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 317/1000
2023-10-30 17:56:54.321 
Epoch 317/1000 
	 loss: 28.8514, MinusLogProbMetric: 28.8514, val_loss: 29.2074, val_MinusLogProbMetric: 29.2074

Epoch 317: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.8514 - MinusLogProbMetric: 28.8514 - val_loss: 29.2074 - val_MinusLogProbMetric: 29.2074 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 318/1000
2023-10-30 17:57:27.871 
Epoch 318/1000 
	 loss: 29.0243, MinusLogProbMetric: 29.0243, val_loss: 29.0872, val_MinusLogProbMetric: 29.0872

Epoch 318: val_loss did not improve from 28.89444
196/196 - 34s - loss: 29.0243 - MinusLogProbMetric: 29.0243 - val_loss: 29.0872 - val_MinusLogProbMetric: 29.0872 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 319/1000
2023-10-30 17:58:00.489 
Epoch 319/1000 
	 loss: 28.8551, MinusLogProbMetric: 28.8551, val_loss: 29.2520, val_MinusLogProbMetric: 29.2520

Epoch 319: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.8551 - MinusLogProbMetric: 28.8551 - val_loss: 29.2520 - val_MinusLogProbMetric: 29.2520 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 320/1000
2023-10-30 17:58:33.833 
Epoch 320/1000 
	 loss: 28.8513, MinusLogProbMetric: 28.8513, val_loss: 28.9963, val_MinusLogProbMetric: 28.9963

Epoch 320: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.8513 - MinusLogProbMetric: 28.8513 - val_loss: 28.9963 - val_MinusLogProbMetric: 28.9963 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 321/1000
2023-10-30 17:59:07.329 
Epoch 321/1000 
	 loss: 28.7788, MinusLogProbMetric: 28.7788, val_loss: 29.2772, val_MinusLogProbMetric: 29.2772

Epoch 321: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.7788 - MinusLogProbMetric: 28.7788 - val_loss: 29.2772 - val_MinusLogProbMetric: 29.2772 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 322/1000
2023-10-30 17:59:39.926 
Epoch 322/1000 
	 loss: 28.8751, MinusLogProbMetric: 28.8751, val_loss: 29.4348, val_MinusLogProbMetric: 29.4348

Epoch 322: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.8751 - MinusLogProbMetric: 28.8751 - val_loss: 29.4348 - val_MinusLogProbMetric: 29.4348 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 323/1000
2023-10-30 18:00:13.600 
Epoch 323/1000 
	 loss: 28.8438, MinusLogProbMetric: 28.8438, val_loss: 29.2107, val_MinusLogProbMetric: 29.2107

Epoch 323: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.8438 - MinusLogProbMetric: 28.8438 - val_loss: 29.2107 - val_MinusLogProbMetric: 29.2107 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 324/1000
2023-10-30 18:00:46.987 
Epoch 324/1000 
	 loss: 29.0199, MinusLogProbMetric: 29.0199, val_loss: 29.5145, val_MinusLogProbMetric: 29.5145

Epoch 324: val_loss did not improve from 28.89444
196/196 - 33s - loss: 29.0199 - MinusLogProbMetric: 29.0199 - val_loss: 29.5145 - val_MinusLogProbMetric: 29.5145 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 325/1000
2023-10-30 18:01:20.039 
Epoch 325/1000 
	 loss: 28.7798, MinusLogProbMetric: 28.7798, val_loss: 29.2621, val_MinusLogProbMetric: 29.2621

Epoch 325: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.7798 - MinusLogProbMetric: 28.7798 - val_loss: 29.2621 - val_MinusLogProbMetric: 29.2621 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 326/1000
2023-10-30 18:01:51.921 
Epoch 326/1000 
	 loss: 28.9394, MinusLogProbMetric: 28.9394, val_loss: 29.1446, val_MinusLogProbMetric: 29.1446

Epoch 326: val_loss did not improve from 28.89444
196/196 - 32s - loss: 28.9394 - MinusLogProbMetric: 28.9394 - val_loss: 29.1446 - val_MinusLogProbMetric: 29.1446 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 327/1000
2023-10-30 18:02:25.165 
Epoch 327/1000 
	 loss: 29.0272, MinusLogProbMetric: 29.0272, val_loss: 29.3337, val_MinusLogProbMetric: 29.3337

Epoch 327: val_loss did not improve from 28.89444
196/196 - 33s - loss: 29.0272 - MinusLogProbMetric: 29.0272 - val_loss: 29.3337 - val_MinusLogProbMetric: 29.3337 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 328/1000
2023-10-30 18:02:58.885 
Epoch 328/1000 
	 loss: 28.7143, MinusLogProbMetric: 28.7143, val_loss: 29.3571, val_MinusLogProbMetric: 29.3571

Epoch 328: val_loss did not improve from 28.89444
196/196 - 34s - loss: 28.7143 - MinusLogProbMetric: 28.7143 - val_loss: 29.3571 - val_MinusLogProbMetric: 29.3571 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 329/1000
2023-10-30 18:03:31.920 
Epoch 329/1000 
	 loss: 28.8555, MinusLogProbMetric: 28.8555, val_loss: 29.5648, val_MinusLogProbMetric: 29.5648

Epoch 329: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.8555 - MinusLogProbMetric: 28.8555 - val_loss: 29.5648 - val_MinusLogProbMetric: 29.5648 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 330/1000
2023-10-30 18:04:04.127 
Epoch 330/1000 
	 loss: 28.8111, MinusLogProbMetric: 28.8111, val_loss: 28.9965, val_MinusLogProbMetric: 28.9965

Epoch 330: val_loss did not improve from 28.89444
196/196 - 32s - loss: 28.8111 - MinusLogProbMetric: 28.8111 - val_loss: 28.9965 - val_MinusLogProbMetric: 28.9965 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 331/1000
2023-10-30 18:04:37.317 
Epoch 331/1000 
	 loss: 28.9454, MinusLogProbMetric: 28.9454, val_loss: 29.7539, val_MinusLogProbMetric: 29.7539

Epoch 331: val_loss did not improve from 28.89444
196/196 - 33s - loss: 28.9454 - MinusLogProbMetric: 28.9454 - val_loss: 29.7539 - val_MinusLogProbMetric: 29.7539 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 332/1000
2023-10-30 18:05:10.600 
Epoch 332/1000 
	 loss: 28.8555, MinusLogProbMetric: 28.8555, val_loss: 28.8810, val_MinusLogProbMetric: 28.8810

Epoch 332: val_loss improved from 28.89444 to 28.88102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 28.8555 - MinusLogProbMetric: 28.8555 - val_loss: 28.8810 - val_MinusLogProbMetric: 28.8810 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 333/1000
2023-10-30 18:05:45.221 
Epoch 333/1000 
	 loss: 28.8779, MinusLogProbMetric: 28.8779, val_loss: 28.9902, val_MinusLogProbMetric: 28.9902

Epoch 333: val_loss did not improve from 28.88102
196/196 - 34s - loss: 28.8779 - MinusLogProbMetric: 28.8779 - val_loss: 28.9902 - val_MinusLogProbMetric: 28.9902 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 334/1000
2023-10-30 18:06:18.902 
Epoch 334/1000 
	 loss: 28.8151, MinusLogProbMetric: 28.8151, val_loss: 29.7781, val_MinusLogProbMetric: 29.7781

Epoch 334: val_loss did not improve from 28.88102
196/196 - 34s - loss: 28.8151 - MinusLogProbMetric: 28.8151 - val_loss: 29.7781 - val_MinusLogProbMetric: 29.7781 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 335/1000
2023-10-30 18:06:51.723 
Epoch 335/1000 
	 loss: 28.9006, MinusLogProbMetric: 28.9006, val_loss: 29.7947, val_MinusLogProbMetric: 29.7947

Epoch 335: val_loss did not improve from 28.88102
196/196 - 33s - loss: 28.9006 - MinusLogProbMetric: 28.9006 - val_loss: 29.7947 - val_MinusLogProbMetric: 29.7947 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 336/1000
2023-10-30 18:07:25.696 
Epoch 336/1000 
	 loss: 28.7812, MinusLogProbMetric: 28.7812, val_loss: 29.4462, val_MinusLogProbMetric: 29.4462

Epoch 336: val_loss did not improve from 28.88102
196/196 - 34s - loss: 28.7812 - MinusLogProbMetric: 28.7812 - val_loss: 29.4462 - val_MinusLogProbMetric: 29.4462 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 337/1000
2023-10-30 18:08:00.337 
Epoch 337/1000 
	 loss: 28.7723, MinusLogProbMetric: 28.7723, val_loss: 28.8039, val_MinusLogProbMetric: 28.8039

Epoch 337: val_loss improved from 28.88102 to 28.80392, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 28.7723 - MinusLogProbMetric: 28.7723 - val_loss: 28.8039 - val_MinusLogProbMetric: 28.8039 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 338/1000
2023-10-30 18:08:33.845 
Epoch 338/1000 
	 loss: 28.7218, MinusLogProbMetric: 28.7218, val_loss: 29.4949, val_MinusLogProbMetric: 29.4949

Epoch 338: val_loss did not improve from 28.80392
196/196 - 33s - loss: 28.7218 - MinusLogProbMetric: 28.7218 - val_loss: 29.4949 - val_MinusLogProbMetric: 29.4949 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 339/1000
2023-10-30 18:09:06.449 
Epoch 339/1000 
	 loss: 28.7428, MinusLogProbMetric: 28.7428, val_loss: 29.2411, val_MinusLogProbMetric: 29.2411

Epoch 339: val_loss did not improve from 28.80392
196/196 - 33s - loss: 28.7428 - MinusLogProbMetric: 28.7428 - val_loss: 29.2411 - val_MinusLogProbMetric: 29.2411 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 340/1000
2023-10-30 18:09:39.458 
Epoch 340/1000 
	 loss: 28.8310, MinusLogProbMetric: 28.8310, val_loss: 29.0638, val_MinusLogProbMetric: 29.0638

Epoch 340: val_loss did not improve from 28.80392
196/196 - 33s - loss: 28.8310 - MinusLogProbMetric: 28.8310 - val_loss: 29.0638 - val_MinusLogProbMetric: 29.0638 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 341/1000
2023-10-30 18:10:12.663 
Epoch 341/1000 
	 loss: 28.7091, MinusLogProbMetric: 28.7091, val_loss: 29.1934, val_MinusLogProbMetric: 29.1934

Epoch 341: val_loss did not improve from 28.80392
196/196 - 33s - loss: 28.7091 - MinusLogProbMetric: 28.7091 - val_loss: 29.1934 - val_MinusLogProbMetric: 29.1934 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 342/1000
2023-10-30 18:10:45.987 
Epoch 342/1000 
	 loss: 28.8295, MinusLogProbMetric: 28.8295, val_loss: 29.2230, val_MinusLogProbMetric: 29.2230

Epoch 342: val_loss did not improve from 28.80392
196/196 - 33s - loss: 28.8295 - MinusLogProbMetric: 28.8295 - val_loss: 29.2230 - val_MinusLogProbMetric: 29.2230 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 343/1000
2023-10-30 18:11:20.308 
Epoch 343/1000 
	 loss: 28.7702, MinusLogProbMetric: 28.7702, val_loss: 29.9481, val_MinusLogProbMetric: 29.9481

Epoch 343: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7702 - MinusLogProbMetric: 28.7702 - val_loss: 29.9481 - val_MinusLogProbMetric: 29.9481 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 344/1000
2023-10-30 18:11:53.567 
Epoch 344/1000 
	 loss: 28.8352, MinusLogProbMetric: 28.8352, val_loss: 29.0850, val_MinusLogProbMetric: 29.0850

Epoch 344: val_loss did not improve from 28.80392
196/196 - 33s - loss: 28.8352 - MinusLogProbMetric: 28.8352 - val_loss: 29.0850 - val_MinusLogProbMetric: 29.0850 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 345/1000
2023-10-30 18:12:27.538 
Epoch 345/1000 
	 loss: 28.7130, MinusLogProbMetric: 28.7130, val_loss: 29.2656, val_MinusLogProbMetric: 29.2656

Epoch 345: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7130 - MinusLogProbMetric: 28.7130 - val_loss: 29.2656 - val_MinusLogProbMetric: 29.2656 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 346/1000
2023-10-30 18:13:01.427 
Epoch 346/1000 
	 loss: 28.7797, MinusLogProbMetric: 28.7797, val_loss: 28.9953, val_MinusLogProbMetric: 28.9953

Epoch 346: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7797 - MinusLogProbMetric: 28.7797 - val_loss: 28.9953 - val_MinusLogProbMetric: 28.9953 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 347/1000
2023-10-30 18:13:34.902 
Epoch 347/1000 
	 loss: 28.8316, MinusLogProbMetric: 28.8316, val_loss: 29.8078, val_MinusLogProbMetric: 29.8078

Epoch 347: val_loss did not improve from 28.80392
196/196 - 33s - loss: 28.8316 - MinusLogProbMetric: 28.8316 - val_loss: 29.8078 - val_MinusLogProbMetric: 29.8078 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 348/1000
2023-10-30 18:14:09.318 
Epoch 348/1000 
	 loss: 28.7456, MinusLogProbMetric: 28.7456, val_loss: 28.8925, val_MinusLogProbMetric: 28.8925

Epoch 348: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7456 - MinusLogProbMetric: 28.7456 - val_loss: 28.8925 - val_MinusLogProbMetric: 28.8925 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 349/1000
2023-10-30 18:14:43.576 
Epoch 349/1000 
	 loss: 28.7098, MinusLogProbMetric: 28.7098, val_loss: 29.2496, val_MinusLogProbMetric: 29.2496

Epoch 349: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7098 - MinusLogProbMetric: 28.7098 - val_loss: 29.2496 - val_MinusLogProbMetric: 29.2496 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 350/1000
2023-10-30 18:15:17.121 
Epoch 350/1000 
	 loss: 28.7548, MinusLogProbMetric: 28.7548, val_loss: 29.0995, val_MinusLogProbMetric: 29.0995

Epoch 350: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7548 - MinusLogProbMetric: 28.7548 - val_loss: 29.0995 - val_MinusLogProbMetric: 29.0995 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 351/1000
2023-10-30 18:15:50.748 
Epoch 351/1000 
	 loss: 28.7580, MinusLogProbMetric: 28.7580, val_loss: 29.1223, val_MinusLogProbMetric: 29.1223

Epoch 351: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7580 - MinusLogProbMetric: 28.7580 - val_loss: 29.1223 - val_MinusLogProbMetric: 29.1223 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 352/1000
2023-10-30 18:16:25.044 
Epoch 352/1000 
	 loss: 28.7823, MinusLogProbMetric: 28.7823, val_loss: 29.2281, val_MinusLogProbMetric: 29.2281

Epoch 352: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7823 - MinusLogProbMetric: 28.7823 - val_loss: 29.2281 - val_MinusLogProbMetric: 29.2281 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 353/1000
2023-10-30 18:16:57.899 
Epoch 353/1000 
	 loss: 28.7603, MinusLogProbMetric: 28.7603, val_loss: 29.0070, val_MinusLogProbMetric: 29.0070

Epoch 353: val_loss did not improve from 28.80392
196/196 - 33s - loss: 28.7603 - MinusLogProbMetric: 28.7603 - val_loss: 29.0070 - val_MinusLogProbMetric: 29.0070 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 354/1000
2023-10-30 18:17:32.763 
Epoch 354/1000 
	 loss: 28.7566, MinusLogProbMetric: 28.7566, val_loss: 29.2031, val_MinusLogProbMetric: 29.2031

Epoch 354: val_loss did not improve from 28.80392
196/196 - 35s - loss: 28.7566 - MinusLogProbMetric: 28.7566 - val_loss: 29.2031 - val_MinusLogProbMetric: 29.2031 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 355/1000
2023-10-30 18:18:06.708 
Epoch 355/1000 
	 loss: 28.7532, MinusLogProbMetric: 28.7532, val_loss: 29.2193, val_MinusLogProbMetric: 29.2193

Epoch 355: val_loss did not improve from 28.80392
196/196 - 34s - loss: 28.7532 - MinusLogProbMetric: 28.7532 - val_loss: 29.2193 - val_MinusLogProbMetric: 29.2193 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 356/1000
2023-10-30 18:18:40.652 
Epoch 356/1000 
	 loss: 28.7336, MinusLogProbMetric: 28.7336, val_loss: 28.6981, val_MinusLogProbMetric: 28.6981

Epoch 356: val_loss improved from 28.80392 to 28.69813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 28.7336 - MinusLogProbMetric: 28.7336 - val_loss: 28.6981 - val_MinusLogProbMetric: 28.6981 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 357/1000
2023-10-30 18:19:14.950 
Epoch 357/1000 
	 loss: 28.7320, MinusLogProbMetric: 28.7320, val_loss: 29.7109, val_MinusLogProbMetric: 29.7109

Epoch 357: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.7320 - MinusLogProbMetric: 28.7320 - val_loss: 29.7109 - val_MinusLogProbMetric: 29.7109 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 358/1000
2023-10-30 18:19:48.768 
Epoch 358/1000 
	 loss: 28.7418, MinusLogProbMetric: 28.7418, val_loss: 29.4295, val_MinusLogProbMetric: 29.4295

Epoch 358: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.7418 - MinusLogProbMetric: 28.7418 - val_loss: 29.4295 - val_MinusLogProbMetric: 29.4295 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 359/1000
2023-10-30 18:20:21.649 
Epoch 359/1000 
	 loss: 28.7298, MinusLogProbMetric: 28.7298, val_loss: 29.1729, val_MinusLogProbMetric: 29.1729

Epoch 359: val_loss did not improve from 28.69813
196/196 - 33s - loss: 28.7298 - MinusLogProbMetric: 28.7298 - val_loss: 29.1729 - val_MinusLogProbMetric: 29.1729 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 360/1000
2023-10-30 18:20:54.973 
Epoch 360/1000 
	 loss: 28.7158, MinusLogProbMetric: 28.7158, val_loss: 28.9492, val_MinusLogProbMetric: 28.9492

Epoch 360: val_loss did not improve from 28.69813
196/196 - 33s - loss: 28.7158 - MinusLogProbMetric: 28.7158 - val_loss: 28.9492 - val_MinusLogProbMetric: 28.9492 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 361/1000
2023-10-30 18:21:28.199 
Epoch 361/1000 
	 loss: 28.7815, MinusLogProbMetric: 28.7815, val_loss: 29.2528, val_MinusLogProbMetric: 29.2528

Epoch 361: val_loss did not improve from 28.69813
196/196 - 33s - loss: 28.7815 - MinusLogProbMetric: 28.7815 - val_loss: 29.2528 - val_MinusLogProbMetric: 29.2528 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 362/1000
2023-10-30 18:22:00.977 
Epoch 362/1000 
	 loss: 28.6253, MinusLogProbMetric: 28.6253, val_loss: 30.0891, val_MinusLogProbMetric: 30.0891

Epoch 362: val_loss did not improve from 28.69813
196/196 - 33s - loss: 28.6253 - MinusLogProbMetric: 28.6253 - val_loss: 30.0891 - val_MinusLogProbMetric: 30.0891 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 363/1000
2023-10-30 18:22:34.516 
Epoch 363/1000 
	 loss: 28.7089, MinusLogProbMetric: 28.7089, val_loss: 29.0231, val_MinusLogProbMetric: 29.0231

Epoch 363: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.7089 - MinusLogProbMetric: 28.7089 - val_loss: 29.0231 - val_MinusLogProbMetric: 29.0231 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 364/1000
2023-10-30 18:23:07.341 
Epoch 364/1000 
	 loss: 28.6850, MinusLogProbMetric: 28.6850, val_loss: 29.0271, val_MinusLogProbMetric: 29.0271

Epoch 364: val_loss did not improve from 28.69813
196/196 - 33s - loss: 28.6850 - MinusLogProbMetric: 28.6850 - val_loss: 29.0271 - val_MinusLogProbMetric: 29.0271 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 365/1000
2023-10-30 18:23:41.160 
Epoch 365/1000 
	 loss: 28.7059, MinusLogProbMetric: 28.7059, val_loss: 29.1551, val_MinusLogProbMetric: 29.1551

Epoch 365: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.7059 - MinusLogProbMetric: 28.7059 - val_loss: 29.1551 - val_MinusLogProbMetric: 29.1551 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 366/1000
2023-10-30 18:24:14.577 
Epoch 366/1000 
	 loss: 28.6421, MinusLogProbMetric: 28.6421, val_loss: 29.0359, val_MinusLogProbMetric: 29.0359

Epoch 366: val_loss did not improve from 28.69813
196/196 - 33s - loss: 28.6421 - MinusLogProbMetric: 28.6421 - val_loss: 29.0359 - val_MinusLogProbMetric: 29.0359 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 367/1000
2023-10-30 18:24:48.462 
Epoch 367/1000 
	 loss: 28.7752, MinusLogProbMetric: 28.7752, val_loss: 28.7204, val_MinusLogProbMetric: 28.7204

Epoch 367: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.7752 - MinusLogProbMetric: 28.7752 - val_loss: 28.7204 - val_MinusLogProbMetric: 28.7204 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 368/1000
2023-10-30 18:25:22.349 
Epoch 368/1000 
	 loss: 28.7035, MinusLogProbMetric: 28.7035, val_loss: 29.0655, val_MinusLogProbMetric: 29.0655

Epoch 368: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.7035 - MinusLogProbMetric: 28.7035 - val_loss: 29.0655 - val_MinusLogProbMetric: 29.0655 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 369/1000
2023-10-30 18:25:56.463 
Epoch 369/1000 
	 loss: 28.6545, MinusLogProbMetric: 28.6545, val_loss: 29.5670, val_MinusLogProbMetric: 29.5670

Epoch 369: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.6545 - MinusLogProbMetric: 28.6545 - val_loss: 29.5670 - val_MinusLogProbMetric: 29.5670 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 370/1000
2023-10-30 18:26:28.933 
Epoch 370/1000 
	 loss: 28.6339, MinusLogProbMetric: 28.6339, val_loss: 29.1779, val_MinusLogProbMetric: 29.1779

Epoch 370: val_loss did not improve from 28.69813
196/196 - 32s - loss: 28.6339 - MinusLogProbMetric: 28.6339 - val_loss: 29.1779 - val_MinusLogProbMetric: 29.1779 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 371/1000
2023-10-30 18:27:02.304 
Epoch 371/1000 
	 loss: 28.8376, MinusLogProbMetric: 28.8376, val_loss: 28.9765, val_MinusLogProbMetric: 28.9765

Epoch 371: val_loss did not improve from 28.69813
196/196 - 33s - loss: 28.8376 - MinusLogProbMetric: 28.8376 - val_loss: 28.9765 - val_MinusLogProbMetric: 28.9765 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 372/1000
2023-10-30 18:27:36.747 
Epoch 372/1000 
	 loss: 28.6179, MinusLogProbMetric: 28.6179, val_loss: 29.6289, val_MinusLogProbMetric: 29.6289

Epoch 372: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.6179 - MinusLogProbMetric: 28.6179 - val_loss: 29.6289 - val_MinusLogProbMetric: 29.6289 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 373/1000
2023-10-30 18:28:11.074 
Epoch 373/1000 
	 loss: 28.6900, MinusLogProbMetric: 28.6900, val_loss: 29.0943, val_MinusLogProbMetric: 29.0943

Epoch 373: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.6900 - MinusLogProbMetric: 28.6900 - val_loss: 29.0943 - val_MinusLogProbMetric: 29.0943 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 374/1000
2023-10-30 18:28:45.082 
Epoch 374/1000 
	 loss: 28.6560, MinusLogProbMetric: 28.6560, val_loss: 29.3288, val_MinusLogProbMetric: 29.3288

Epoch 374: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.6560 - MinusLogProbMetric: 28.6560 - val_loss: 29.3288 - val_MinusLogProbMetric: 29.3288 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 375/1000
2023-10-30 18:29:19.145 
Epoch 375/1000 
	 loss: 28.6229, MinusLogProbMetric: 28.6229, val_loss: 29.0561, val_MinusLogProbMetric: 29.0561

Epoch 375: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.6229 - MinusLogProbMetric: 28.6229 - val_loss: 29.0561 - val_MinusLogProbMetric: 29.0561 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 376/1000
2023-10-30 18:29:51.666 
Epoch 376/1000 
	 loss: 28.6228, MinusLogProbMetric: 28.6228, val_loss: 29.5388, val_MinusLogProbMetric: 29.5388

Epoch 376: val_loss did not improve from 28.69813
196/196 - 33s - loss: 28.6228 - MinusLogProbMetric: 28.6228 - val_loss: 29.5388 - val_MinusLogProbMetric: 29.5388 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 377/1000
2023-10-30 18:30:23.853 
Epoch 377/1000 
	 loss: 28.7784, MinusLogProbMetric: 28.7784, val_loss: 29.4745, val_MinusLogProbMetric: 29.4745

Epoch 377: val_loss did not improve from 28.69813
196/196 - 32s - loss: 28.7784 - MinusLogProbMetric: 28.7784 - val_loss: 29.4745 - val_MinusLogProbMetric: 29.4745 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 378/1000
2023-10-30 18:30:57.692 
Epoch 378/1000 
	 loss: 28.6121, MinusLogProbMetric: 28.6121, val_loss: 30.9152, val_MinusLogProbMetric: 30.9152

Epoch 378: val_loss did not improve from 28.69813
196/196 - 34s - loss: 28.6121 - MinusLogProbMetric: 28.6121 - val_loss: 30.9152 - val_MinusLogProbMetric: 30.9152 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 379/1000
2023-10-30 18:31:30.073 
Epoch 379/1000 
	 loss: 29.1587, MinusLogProbMetric: 29.1587, val_loss: 29.4767, val_MinusLogProbMetric: 29.4767

Epoch 379: val_loss did not improve from 28.69813
196/196 - 32s - loss: 29.1587 - MinusLogProbMetric: 29.1587 - val_loss: 29.4767 - val_MinusLogProbMetric: 29.4767 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 380/1000
2023-10-30 18:32:00.100 
Epoch 380/1000 
	 loss: 28.6062, MinusLogProbMetric: 28.6062, val_loss: 28.9247, val_MinusLogProbMetric: 28.9247

Epoch 380: val_loss did not improve from 28.69813
196/196 - 30s - loss: 28.6062 - MinusLogProbMetric: 28.6062 - val_loss: 28.9247 - val_MinusLogProbMetric: 28.9247 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 381/1000
2023-10-30 18:32:29.450 
Epoch 381/1000 
	 loss: 28.7240, MinusLogProbMetric: 28.7240, val_loss: 29.0184, val_MinusLogProbMetric: 29.0184

Epoch 381: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.7240 - MinusLogProbMetric: 28.7240 - val_loss: 29.0184 - val_MinusLogProbMetric: 29.0184 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 382/1000
2023-10-30 18:32:57.920 
Epoch 382/1000 
	 loss: 28.6292, MinusLogProbMetric: 28.6292, val_loss: 28.8765, val_MinusLogProbMetric: 28.8765

Epoch 382: val_loss did not improve from 28.69813
196/196 - 28s - loss: 28.6292 - MinusLogProbMetric: 28.6292 - val_loss: 28.8765 - val_MinusLogProbMetric: 28.8765 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 383/1000
2023-10-30 18:33:26.819 
Epoch 383/1000 
	 loss: 28.6222, MinusLogProbMetric: 28.6222, val_loss: 29.1526, val_MinusLogProbMetric: 29.1526

Epoch 383: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.6222 - MinusLogProbMetric: 28.6222 - val_loss: 29.1526 - val_MinusLogProbMetric: 29.1526 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 384/1000
2023-10-30 18:33:55.587 
Epoch 384/1000 
	 loss: 28.5897, MinusLogProbMetric: 28.5897, val_loss: 28.7703, val_MinusLogProbMetric: 28.7703

Epoch 384: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.5897 - MinusLogProbMetric: 28.5897 - val_loss: 28.7703 - val_MinusLogProbMetric: 28.7703 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 385/1000
2023-10-30 18:34:24.485 
Epoch 385/1000 
	 loss: 28.6096, MinusLogProbMetric: 28.6096, val_loss: 29.4312, val_MinusLogProbMetric: 29.4312

Epoch 385: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.6096 - MinusLogProbMetric: 28.6096 - val_loss: 29.4312 - val_MinusLogProbMetric: 29.4312 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 386/1000
2023-10-30 18:34:54.065 
Epoch 386/1000 
	 loss: 28.7149, MinusLogProbMetric: 28.7149, val_loss: 29.1765, val_MinusLogProbMetric: 29.1765

Epoch 386: val_loss did not improve from 28.69813
196/196 - 30s - loss: 28.7149 - MinusLogProbMetric: 28.7149 - val_loss: 29.1765 - val_MinusLogProbMetric: 29.1765 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 387/1000
2023-10-30 18:35:26.411 
Epoch 387/1000 
	 loss: 28.5347, MinusLogProbMetric: 28.5347, val_loss: 29.2179, val_MinusLogProbMetric: 29.2179

Epoch 387: val_loss did not improve from 28.69813
196/196 - 32s - loss: 28.5347 - MinusLogProbMetric: 28.5347 - val_loss: 29.2179 - val_MinusLogProbMetric: 29.2179 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 388/1000
2023-10-30 18:35:55.718 
Epoch 388/1000 
	 loss: 28.6039, MinusLogProbMetric: 28.6039, val_loss: 28.8699, val_MinusLogProbMetric: 28.8699

Epoch 388: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.6039 - MinusLogProbMetric: 28.6039 - val_loss: 28.8699 - val_MinusLogProbMetric: 28.8699 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 389/1000
2023-10-30 18:36:24.792 
Epoch 389/1000 
	 loss: 28.6510, MinusLogProbMetric: 28.6510, val_loss: 29.2128, val_MinusLogProbMetric: 29.2128

Epoch 389: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.6510 - MinusLogProbMetric: 28.6510 - val_loss: 29.2128 - val_MinusLogProbMetric: 29.2128 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 390/1000
2023-10-30 18:36:55.286 
Epoch 390/1000 
	 loss: 28.6355, MinusLogProbMetric: 28.6355, val_loss: 28.8896, val_MinusLogProbMetric: 28.8896

Epoch 390: val_loss did not improve from 28.69813
196/196 - 30s - loss: 28.6355 - MinusLogProbMetric: 28.6355 - val_loss: 28.8896 - val_MinusLogProbMetric: 28.8896 - lr: 0.0010 - 30s/epoch - 156ms/step
Epoch 391/1000
2023-10-30 18:37:24.137 
Epoch 391/1000 
	 loss: 28.6543, MinusLogProbMetric: 28.6543, val_loss: 29.0383, val_MinusLogProbMetric: 29.0383

Epoch 391: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.6543 - MinusLogProbMetric: 28.6543 - val_loss: 29.0383 - val_MinusLogProbMetric: 29.0383 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 392/1000
2023-10-30 18:37:52.935 
Epoch 392/1000 
	 loss: 28.6040, MinusLogProbMetric: 28.6040, val_loss: 29.0065, val_MinusLogProbMetric: 29.0065

Epoch 392: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.6040 - MinusLogProbMetric: 28.6040 - val_loss: 29.0065 - val_MinusLogProbMetric: 29.0065 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 393/1000
2023-10-30 18:38:21.737 
Epoch 393/1000 
	 loss: 28.7596, MinusLogProbMetric: 28.7596, val_loss: 29.4838, val_MinusLogProbMetric: 29.4838

Epoch 393: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.7596 - MinusLogProbMetric: 28.7596 - val_loss: 29.4838 - val_MinusLogProbMetric: 29.4838 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 394/1000
2023-10-30 18:38:50.964 
Epoch 394/1000 
	 loss: 28.5466, MinusLogProbMetric: 28.5466, val_loss: 28.7865, val_MinusLogProbMetric: 28.7865

Epoch 394: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.5466 - MinusLogProbMetric: 28.5466 - val_loss: 28.7865 - val_MinusLogProbMetric: 28.7865 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 395/1000
2023-10-30 18:39:22.295 
Epoch 395/1000 
	 loss: 28.8331, MinusLogProbMetric: 28.8331, val_loss: 29.3349, val_MinusLogProbMetric: 29.3349

Epoch 395: val_loss did not improve from 28.69813
196/196 - 31s - loss: 28.8331 - MinusLogProbMetric: 28.8331 - val_loss: 29.3349 - val_MinusLogProbMetric: 29.3349 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 396/1000
2023-10-30 18:39:51.264 
Epoch 396/1000 
	 loss: 28.5483, MinusLogProbMetric: 28.5483, val_loss: 29.1346, val_MinusLogProbMetric: 29.1346

Epoch 396: val_loss did not improve from 28.69813
196/196 - 29s - loss: 28.5483 - MinusLogProbMetric: 28.5483 - val_loss: 29.1346 - val_MinusLogProbMetric: 29.1346 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 397/1000
2023-10-30 18:40:21.132 
Epoch 397/1000 
	 loss: 28.6559, MinusLogProbMetric: 28.6559, val_loss: 29.1912, val_MinusLogProbMetric: 29.1912

Epoch 397: val_loss did not improve from 28.69813
196/196 - 30s - loss: 28.6559 - MinusLogProbMetric: 28.6559 - val_loss: 29.1912 - val_MinusLogProbMetric: 29.1912 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 398/1000
2023-10-30 18:40:50.818 
Epoch 398/1000 
	 loss: 28.5395, MinusLogProbMetric: 28.5395, val_loss: 28.8294, val_MinusLogProbMetric: 28.8294

Epoch 398: val_loss did not improve from 28.69813
196/196 - 30s - loss: 28.5395 - MinusLogProbMetric: 28.5395 - val_loss: 28.8294 - val_MinusLogProbMetric: 28.8294 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 399/1000
2023-10-30 18:41:20.648 
Epoch 399/1000 
	 loss: 28.5868, MinusLogProbMetric: 28.5868, val_loss: 28.6601, val_MinusLogProbMetric: 28.6601

Epoch 399: val_loss improved from 28.69813 to 28.66014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 28.5868 - MinusLogProbMetric: 28.5868 - val_loss: 28.6601 - val_MinusLogProbMetric: 28.6601 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 400/1000
2023-10-30 18:41:51.970 
Epoch 400/1000 
	 loss: 28.7190, MinusLogProbMetric: 28.7190, val_loss: 29.2220, val_MinusLogProbMetric: 29.2220

Epoch 400: val_loss did not improve from 28.66014
196/196 - 31s - loss: 28.7190 - MinusLogProbMetric: 28.7190 - val_loss: 29.2220 - val_MinusLogProbMetric: 29.2220 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 401/1000
2023-10-30 18:42:23.002 
Epoch 401/1000 
	 loss: 28.5919, MinusLogProbMetric: 28.5919, val_loss: 29.1676, val_MinusLogProbMetric: 29.1676

Epoch 401: val_loss did not improve from 28.66014
196/196 - 31s - loss: 28.5919 - MinusLogProbMetric: 28.5919 - val_loss: 29.1676 - val_MinusLogProbMetric: 29.1676 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 402/1000
2023-10-30 18:42:54.143 
Epoch 402/1000 
	 loss: 28.7046, MinusLogProbMetric: 28.7046, val_loss: 29.1713, val_MinusLogProbMetric: 29.1713

Epoch 402: val_loss did not improve from 28.66014
196/196 - 31s - loss: 28.7046 - MinusLogProbMetric: 28.7046 - val_loss: 29.1713 - val_MinusLogProbMetric: 29.1713 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 403/1000
2023-10-30 18:43:23.231 
Epoch 403/1000 
	 loss: 28.5777, MinusLogProbMetric: 28.5777, val_loss: 28.9728, val_MinusLogProbMetric: 28.9728

Epoch 403: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5777 - MinusLogProbMetric: 28.5777 - val_loss: 28.9728 - val_MinusLogProbMetric: 28.9728 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 404/1000
2023-10-30 18:43:52.765 
Epoch 404/1000 
	 loss: 28.5546, MinusLogProbMetric: 28.5546, val_loss: 28.8602, val_MinusLogProbMetric: 28.8602

Epoch 404: val_loss did not improve from 28.66014
196/196 - 30s - loss: 28.5546 - MinusLogProbMetric: 28.5546 - val_loss: 28.8602 - val_MinusLogProbMetric: 28.8602 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 405/1000
2023-10-30 18:44:21.592 
Epoch 405/1000 
	 loss: 28.5438, MinusLogProbMetric: 28.5438, val_loss: 28.7647, val_MinusLogProbMetric: 28.7647

Epoch 405: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5438 - MinusLogProbMetric: 28.5438 - val_loss: 28.7647 - val_MinusLogProbMetric: 28.7647 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 406/1000
2023-10-30 18:44:50.285 
Epoch 406/1000 
	 loss: 28.5538, MinusLogProbMetric: 28.5538, val_loss: 29.2156, val_MinusLogProbMetric: 29.2156

Epoch 406: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5538 - MinusLogProbMetric: 28.5538 - val_loss: 29.2156 - val_MinusLogProbMetric: 29.2156 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 407/1000
2023-10-30 18:45:19.210 
Epoch 407/1000 
	 loss: 28.5301, MinusLogProbMetric: 28.5301, val_loss: 28.9125, val_MinusLogProbMetric: 28.9125

Epoch 407: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5301 - MinusLogProbMetric: 28.5301 - val_loss: 28.9125 - val_MinusLogProbMetric: 28.9125 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 408/1000
2023-10-30 18:45:47.953 
Epoch 408/1000 
	 loss: 28.6585, MinusLogProbMetric: 28.6585, val_loss: 29.2418, val_MinusLogProbMetric: 29.2418

Epoch 408: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.6585 - MinusLogProbMetric: 28.6585 - val_loss: 29.2418 - val_MinusLogProbMetric: 29.2418 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 409/1000
2023-10-30 18:46:16.826 
Epoch 409/1000 
	 loss: 28.5727, MinusLogProbMetric: 28.5727, val_loss: 29.0435, val_MinusLogProbMetric: 29.0435

Epoch 409: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5727 - MinusLogProbMetric: 28.5727 - val_loss: 29.0435 - val_MinusLogProbMetric: 29.0435 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 410/1000
2023-10-30 18:46:47.576 
Epoch 410/1000 
	 loss: 28.5970, MinusLogProbMetric: 28.5970, val_loss: 29.0222, val_MinusLogProbMetric: 29.0222

Epoch 410: val_loss did not improve from 28.66014
196/196 - 31s - loss: 28.5970 - MinusLogProbMetric: 28.5970 - val_loss: 29.0222 - val_MinusLogProbMetric: 29.0222 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 411/1000
2023-10-30 18:47:17.617 
Epoch 411/1000 
	 loss: 28.6186, MinusLogProbMetric: 28.6186, val_loss: 29.3890, val_MinusLogProbMetric: 29.3890

Epoch 411: val_loss did not improve from 28.66014
196/196 - 30s - loss: 28.6186 - MinusLogProbMetric: 28.6186 - val_loss: 29.3890 - val_MinusLogProbMetric: 29.3890 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 412/1000
2023-10-30 18:47:46.903 
Epoch 412/1000 
	 loss: 28.6890, MinusLogProbMetric: 28.6890, val_loss: 29.5307, val_MinusLogProbMetric: 29.5307

Epoch 412: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.6890 - MinusLogProbMetric: 28.6890 - val_loss: 29.5307 - val_MinusLogProbMetric: 29.5307 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 413/1000
2023-10-30 18:48:16.865 
Epoch 413/1000 
	 loss: 28.6234, MinusLogProbMetric: 28.6234, val_loss: 29.0301, val_MinusLogProbMetric: 29.0301

Epoch 413: val_loss did not improve from 28.66014
196/196 - 30s - loss: 28.6234 - MinusLogProbMetric: 28.6234 - val_loss: 29.0301 - val_MinusLogProbMetric: 29.0301 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 414/1000
2023-10-30 18:48:46.606 
Epoch 414/1000 
	 loss: 28.6711, MinusLogProbMetric: 28.6711, val_loss: 28.9275, val_MinusLogProbMetric: 28.9275

Epoch 414: val_loss did not improve from 28.66014
196/196 - 30s - loss: 28.6711 - MinusLogProbMetric: 28.6711 - val_loss: 28.9275 - val_MinusLogProbMetric: 28.9275 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 415/1000
2023-10-30 18:49:17.195 
Epoch 415/1000 
	 loss: 28.6713, MinusLogProbMetric: 28.6713, val_loss: 28.8730, val_MinusLogProbMetric: 28.8730

Epoch 415: val_loss did not improve from 28.66014
196/196 - 31s - loss: 28.6713 - MinusLogProbMetric: 28.6713 - val_loss: 28.8730 - val_MinusLogProbMetric: 28.8730 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 416/1000
2023-10-30 18:49:47.009 
Epoch 416/1000 
	 loss: 28.5649, MinusLogProbMetric: 28.5649, val_loss: 29.3256, val_MinusLogProbMetric: 29.3256

Epoch 416: val_loss did not improve from 28.66014
196/196 - 30s - loss: 28.5649 - MinusLogProbMetric: 28.5649 - val_loss: 29.3256 - val_MinusLogProbMetric: 29.3256 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 417/1000
2023-10-30 18:50:20.866 
Epoch 417/1000 
	 loss: 28.4748, MinusLogProbMetric: 28.4748, val_loss: 28.7715, val_MinusLogProbMetric: 28.7715

Epoch 417: val_loss did not improve from 28.66014
196/196 - 34s - loss: 28.4748 - MinusLogProbMetric: 28.4748 - val_loss: 28.7715 - val_MinusLogProbMetric: 28.7715 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 418/1000
2023-10-30 18:50:50.516 
Epoch 418/1000 
	 loss: 28.5334, MinusLogProbMetric: 28.5334, val_loss: 29.0336, val_MinusLogProbMetric: 29.0336

Epoch 418: val_loss did not improve from 28.66014
196/196 - 30s - loss: 28.5334 - MinusLogProbMetric: 28.5334 - val_loss: 29.0336 - val_MinusLogProbMetric: 29.0336 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 419/1000
2023-10-30 18:51:24.334 
Epoch 419/1000 
	 loss: 28.7714, MinusLogProbMetric: 28.7714, val_loss: 29.2340, val_MinusLogProbMetric: 29.2340

Epoch 419: val_loss did not improve from 28.66014
196/196 - 34s - loss: 28.7714 - MinusLogProbMetric: 28.7714 - val_loss: 29.2340 - val_MinusLogProbMetric: 29.2340 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 420/1000
2023-10-30 18:51:57.099 
Epoch 420/1000 
	 loss: 28.4936, MinusLogProbMetric: 28.4936, val_loss: 28.8459, val_MinusLogProbMetric: 28.8459

Epoch 420: val_loss did not improve from 28.66014
196/196 - 33s - loss: 28.4936 - MinusLogProbMetric: 28.4936 - val_loss: 28.8459 - val_MinusLogProbMetric: 28.8459 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 421/1000
2023-10-30 18:52:28.394 
Epoch 421/1000 
	 loss: 28.4900, MinusLogProbMetric: 28.4900, val_loss: 29.3691, val_MinusLogProbMetric: 29.3691

Epoch 421: val_loss did not improve from 28.66014
196/196 - 31s - loss: 28.4900 - MinusLogProbMetric: 28.4900 - val_loss: 29.3691 - val_MinusLogProbMetric: 29.3691 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 422/1000
2023-10-30 18:52:57.456 
Epoch 422/1000 
	 loss: 28.6319, MinusLogProbMetric: 28.6319, val_loss: 29.0978, val_MinusLogProbMetric: 29.0978

Epoch 422: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.6319 - MinusLogProbMetric: 28.6319 - val_loss: 29.0978 - val_MinusLogProbMetric: 29.0978 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 423/1000
2023-10-30 18:53:26.337 
Epoch 423/1000 
	 loss: 28.5501, MinusLogProbMetric: 28.5501, val_loss: 30.3195, val_MinusLogProbMetric: 30.3195

Epoch 423: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5501 - MinusLogProbMetric: 28.5501 - val_loss: 30.3195 - val_MinusLogProbMetric: 30.3195 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 424/1000
2023-10-30 18:53:55.279 
Epoch 424/1000 
	 loss: 28.5129, MinusLogProbMetric: 28.5129, val_loss: 28.7168, val_MinusLogProbMetric: 28.7168

Epoch 424: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5129 - MinusLogProbMetric: 28.5129 - val_loss: 28.7168 - val_MinusLogProbMetric: 28.7168 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 425/1000
2023-10-30 18:54:24.554 
Epoch 425/1000 
	 loss: 28.6144, MinusLogProbMetric: 28.6144, val_loss: 29.1782, val_MinusLogProbMetric: 29.1782

Epoch 425: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.6144 - MinusLogProbMetric: 28.6144 - val_loss: 29.1782 - val_MinusLogProbMetric: 29.1782 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 426/1000
2023-10-30 18:54:53.509 
Epoch 426/1000 
	 loss: 28.5052, MinusLogProbMetric: 28.5052, val_loss: 28.7117, val_MinusLogProbMetric: 28.7117

Epoch 426: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5052 - MinusLogProbMetric: 28.5052 - val_loss: 28.7117 - val_MinusLogProbMetric: 28.7117 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 427/1000
2023-10-30 18:55:23.135 
Epoch 427/1000 
	 loss: 28.4722, MinusLogProbMetric: 28.4722, val_loss: 29.3779, val_MinusLogProbMetric: 29.3779

Epoch 427: val_loss did not improve from 28.66014
196/196 - 30s - loss: 28.4722 - MinusLogProbMetric: 28.4722 - val_loss: 29.3779 - val_MinusLogProbMetric: 29.3779 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 428/1000
2023-10-30 18:55:52.052 
Epoch 428/1000 
	 loss: 28.5255, MinusLogProbMetric: 28.5255, val_loss: 28.9600, val_MinusLogProbMetric: 28.9600

Epoch 428: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5255 - MinusLogProbMetric: 28.5255 - val_loss: 28.9600 - val_MinusLogProbMetric: 28.9600 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 429/1000
2023-10-30 18:56:20.893 
Epoch 429/1000 
	 loss: 28.5153, MinusLogProbMetric: 28.5153, val_loss: 28.8302, val_MinusLogProbMetric: 28.8302

Epoch 429: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5153 - MinusLogProbMetric: 28.5153 - val_loss: 28.8302 - val_MinusLogProbMetric: 28.8302 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 430/1000
2023-10-30 18:56:50.185 
Epoch 430/1000 
	 loss: 28.4501, MinusLogProbMetric: 28.4501, val_loss: 29.2337, val_MinusLogProbMetric: 29.2337

Epoch 430: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.4501 - MinusLogProbMetric: 28.4501 - val_loss: 29.2337 - val_MinusLogProbMetric: 29.2337 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 431/1000
2023-10-30 18:57:18.831 
Epoch 431/1000 
	 loss: 28.4930, MinusLogProbMetric: 28.4930, val_loss: 29.1247, val_MinusLogProbMetric: 29.1247

Epoch 431: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.4930 - MinusLogProbMetric: 28.4930 - val_loss: 29.1247 - val_MinusLogProbMetric: 29.1247 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 432/1000
2023-10-30 18:57:47.383 
Epoch 432/1000 
	 loss: 28.4728, MinusLogProbMetric: 28.4728, val_loss: 29.0806, val_MinusLogProbMetric: 29.0806

Epoch 432: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.4728 - MinusLogProbMetric: 28.4728 - val_loss: 29.0806 - val_MinusLogProbMetric: 29.0806 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 433/1000
2023-10-30 18:58:17.330 
Epoch 433/1000 
	 loss: 28.5428, MinusLogProbMetric: 28.5428, val_loss: 29.2717, val_MinusLogProbMetric: 29.2717

Epoch 433: val_loss did not improve from 28.66014
196/196 - 30s - loss: 28.5428 - MinusLogProbMetric: 28.5428 - val_loss: 29.2717 - val_MinusLogProbMetric: 29.2717 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 434/1000
2023-10-30 18:58:46.725 
Epoch 434/1000 
	 loss: 28.5241, MinusLogProbMetric: 28.5241, val_loss: 28.7037, val_MinusLogProbMetric: 28.7037

Epoch 434: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.5241 - MinusLogProbMetric: 28.5241 - val_loss: 28.7037 - val_MinusLogProbMetric: 28.7037 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 435/1000
2023-10-30 18:59:16.170 
Epoch 435/1000 
	 loss: 28.4804, MinusLogProbMetric: 28.4804, val_loss: 28.7714, val_MinusLogProbMetric: 28.7714

Epoch 435: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.4804 - MinusLogProbMetric: 28.4804 - val_loss: 28.7714 - val_MinusLogProbMetric: 28.7714 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 436/1000
2023-10-30 18:59:44.946 
Epoch 436/1000 
	 loss: 28.4567, MinusLogProbMetric: 28.4567, val_loss: 28.8692, val_MinusLogProbMetric: 28.8692

Epoch 436: val_loss did not improve from 28.66014
196/196 - 29s - loss: 28.4567 - MinusLogProbMetric: 28.4567 - val_loss: 28.8692 - val_MinusLogProbMetric: 28.8692 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 437/1000
2023-10-30 19:00:16.466 
Epoch 437/1000 
	 loss: 28.5099, MinusLogProbMetric: 28.5099, val_loss: 28.9874, val_MinusLogProbMetric: 28.9874

Epoch 437: val_loss did not improve from 28.66014
196/196 - 32s - loss: 28.5099 - MinusLogProbMetric: 28.5099 - val_loss: 28.9874 - val_MinusLogProbMetric: 28.9874 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 438/1000
2023-10-30 19:00:47.084 
Epoch 438/1000 
	 loss: 28.5728, MinusLogProbMetric: 28.5728, val_loss: 28.9015, val_MinusLogProbMetric: 28.9015

Epoch 438: val_loss did not improve from 28.66014
196/196 - 31s - loss: 28.5728 - MinusLogProbMetric: 28.5728 - val_loss: 28.9015 - val_MinusLogProbMetric: 28.9015 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 439/1000
2023-10-30 19:01:15.484 
Epoch 439/1000 
	 loss: 28.5049, MinusLogProbMetric: 28.5049, val_loss: 28.8468, val_MinusLogProbMetric: 28.8468

Epoch 439: val_loss did not improve from 28.66014
196/196 - 28s - loss: 28.5049 - MinusLogProbMetric: 28.5049 - val_loss: 28.8468 - val_MinusLogProbMetric: 28.8468 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 440/1000
2023-10-30 19:01:46.565 
Epoch 440/1000 
	 loss: 28.5319, MinusLogProbMetric: 28.5319, val_loss: 28.7918, val_MinusLogProbMetric: 28.7918

Epoch 440: val_loss did not improve from 28.66014
196/196 - 31s - loss: 28.5319 - MinusLogProbMetric: 28.5319 - val_loss: 28.7918 - val_MinusLogProbMetric: 28.7918 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 441/1000
2023-10-30 19:02:21.008 
Epoch 441/1000 
	 loss: 28.5132, MinusLogProbMetric: 28.5132, val_loss: 28.6089, val_MinusLogProbMetric: 28.6089

Epoch 441: val_loss improved from 28.66014 to 28.60888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 28.5132 - MinusLogProbMetric: 28.5132 - val_loss: 28.6089 - val_MinusLogProbMetric: 28.6089 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 442/1000
2023-10-30 19:02:51.476 
Epoch 442/1000 
	 loss: 28.4595, MinusLogProbMetric: 28.4595, val_loss: 29.6111, val_MinusLogProbMetric: 29.6111

Epoch 442: val_loss did not improve from 28.60888
196/196 - 30s - loss: 28.4595 - MinusLogProbMetric: 28.4595 - val_loss: 29.6111 - val_MinusLogProbMetric: 29.6111 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 443/1000
2023-10-30 19:03:20.676 
Epoch 443/1000 
	 loss: 28.4525, MinusLogProbMetric: 28.4525, val_loss: 29.3753, val_MinusLogProbMetric: 29.3753

Epoch 443: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4525 - MinusLogProbMetric: 28.4525 - val_loss: 29.3753 - val_MinusLogProbMetric: 29.3753 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 444/1000
2023-10-30 19:03:49.356 
Epoch 444/1000 
	 loss: 28.4407, MinusLogProbMetric: 28.4407, val_loss: 28.8989, val_MinusLogProbMetric: 28.8989

Epoch 444: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4407 - MinusLogProbMetric: 28.4407 - val_loss: 28.8989 - val_MinusLogProbMetric: 28.8989 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 445/1000
2023-10-30 19:04:18.397 
Epoch 445/1000 
	 loss: 28.4421, MinusLogProbMetric: 28.4421, val_loss: 28.7764, val_MinusLogProbMetric: 28.7764

Epoch 445: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4421 - MinusLogProbMetric: 28.4421 - val_loss: 28.7764 - val_MinusLogProbMetric: 28.7764 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 446/1000
2023-10-30 19:04:47.480 
Epoch 446/1000 
	 loss: 28.4295, MinusLogProbMetric: 28.4295, val_loss: 29.1463, val_MinusLogProbMetric: 29.1463

Epoch 446: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4295 - MinusLogProbMetric: 28.4295 - val_loss: 29.1463 - val_MinusLogProbMetric: 29.1463 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 447/1000
2023-10-30 19:05:16.226 
Epoch 447/1000 
	 loss: 28.4644, MinusLogProbMetric: 28.4644, val_loss: 29.9184, val_MinusLogProbMetric: 29.9184

Epoch 447: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4644 - MinusLogProbMetric: 28.4644 - val_loss: 29.9184 - val_MinusLogProbMetric: 29.9184 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 448/1000
2023-10-30 19:05:44.977 
Epoch 448/1000 
	 loss: 28.5280, MinusLogProbMetric: 28.5280, val_loss: 29.0202, val_MinusLogProbMetric: 29.0202

Epoch 448: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.5280 - MinusLogProbMetric: 28.5280 - val_loss: 29.0202 - val_MinusLogProbMetric: 29.0202 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 449/1000
2023-10-30 19:06:13.922 
Epoch 449/1000 
	 loss: 28.4837, MinusLogProbMetric: 28.4837, val_loss: 28.8673, val_MinusLogProbMetric: 28.8673

Epoch 449: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4837 - MinusLogProbMetric: 28.4837 - val_loss: 28.8673 - val_MinusLogProbMetric: 28.8673 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 450/1000
2023-10-30 19:06:42.449 
Epoch 450/1000 
	 loss: 28.4343, MinusLogProbMetric: 28.4343, val_loss: 29.1312, val_MinusLogProbMetric: 29.1312

Epoch 450: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4343 - MinusLogProbMetric: 28.4343 - val_loss: 29.1312 - val_MinusLogProbMetric: 29.1312 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 451/1000
2023-10-30 19:07:11.219 
Epoch 451/1000 
	 loss: 28.4677, MinusLogProbMetric: 28.4677, val_loss: 28.6492, val_MinusLogProbMetric: 28.6492

Epoch 451: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4677 - MinusLogProbMetric: 28.4677 - val_loss: 28.6492 - val_MinusLogProbMetric: 28.6492 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 452/1000
2023-10-30 19:07:39.901 
Epoch 452/1000 
	 loss: 28.4243, MinusLogProbMetric: 28.4243, val_loss: 28.8878, val_MinusLogProbMetric: 28.8878

Epoch 452: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4243 - MinusLogProbMetric: 28.4243 - val_loss: 28.8878 - val_MinusLogProbMetric: 28.8878 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 453/1000
2023-10-30 19:08:08.354 
Epoch 453/1000 
	 loss: 28.4341, MinusLogProbMetric: 28.4341, val_loss: 28.6752, val_MinusLogProbMetric: 28.6752

Epoch 453: val_loss did not improve from 28.60888
196/196 - 28s - loss: 28.4341 - MinusLogProbMetric: 28.4341 - val_loss: 28.6752 - val_MinusLogProbMetric: 28.6752 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 454/1000
2023-10-30 19:08:37.157 
Epoch 454/1000 
	 loss: 28.6030, MinusLogProbMetric: 28.6030, val_loss: 28.7402, val_MinusLogProbMetric: 28.7402

Epoch 454: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.6030 - MinusLogProbMetric: 28.6030 - val_loss: 28.7402 - val_MinusLogProbMetric: 28.7402 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 455/1000
2023-10-30 19:09:06.033 
Epoch 455/1000 
	 loss: 28.4189, MinusLogProbMetric: 28.4189, val_loss: 28.9687, val_MinusLogProbMetric: 28.9687

Epoch 455: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4189 - MinusLogProbMetric: 28.4189 - val_loss: 28.9687 - val_MinusLogProbMetric: 28.9687 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 456/1000
2023-10-30 19:09:34.787 
Epoch 456/1000 
	 loss: 28.4080, MinusLogProbMetric: 28.4080, val_loss: 29.4869, val_MinusLogProbMetric: 29.4869

Epoch 456: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4080 - MinusLogProbMetric: 28.4080 - val_loss: 29.4869 - val_MinusLogProbMetric: 29.4869 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 457/1000
2023-10-30 19:10:03.370 
Epoch 457/1000 
	 loss: 28.4162, MinusLogProbMetric: 28.4162, val_loss: 28.6505, val_MinusLogProbMetric: 28.6505

Epoch 457: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4162 - MinusLogProbMetric: 28.4162 - val_loss: 28.6505 - val_MinusLogProbMetric: 28.6505 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 458/1000
2023-10-30 19:10:32.165 
Epoch 458/1000 
	 loss: 28.4333, MinusLogProbMetric: 28.4333, val_loss: 29.0739, val_MinusLogProbMetric: 29.0739

Epoch 458: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4333 - MinusLogProbMetric: 28.4333 - val_loss: 29.0739 - val_MinusLogProbMetric: 29.0739 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 459/1000
2023-10-30 19:11:00.802 
Epoch 459/1000 
	 loss: 28.4462, MinusLogProbMetric: 28.4462, val_loss: 28.8083, val_MinusLogProbMetric: 28.8083

Epoch 459: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4462 - MinusLogProbMetric: 28.4462 - val_loss: 28.8083 - val_MinusLogProbMetric: 28.8083 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 460/1000
2023-10-30 19:11:32.282 
Epoch 460/1000 
	 loss: 28.5573, MinusLogProbMetric: 28.5573, val_loss: 28.7221, val_MinusLogProbMetric: 28.7221

Epoch 460: val_loss did not improve from 28.60888
196/196 - 31s - loss: 28.5573 - MinusLogProbMetric: 28.5573 - val_loss: 28.7221 - val_MinusLogProbMetric: 28.7221 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 461/1000
2023-10-30 19:12:02.309 
Epoch 461/1000 
	 loss: 28.4453, MinusLogProbMetric: 28.4453, val_loss: 28.8852, val_MinusLogProbMetric: 28.8852

Epoch 461: val_loss did not improve from 28.60888
196/196 - 30s - loss: 28.4453 - MinusLogProbMetric: 28.4453 - val_loss: 28.8852 - val_MinusLogProbMetric: 28.8852 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 462/1000
2023-10-30 19:12:31.976 
Epoch 462/1000 
	 loss: 28.5027, MinusLogProbMetric: 28.5027, val_loss: 28.8726, val_MinusLogProbMetric: 28.8726

Epoch 462: val_loss did not improve from 28.60888
196/196 - 30s - loss: 28.5027 - MinusLogProbMetric: 28.5027 - val_loss: 28.8726 - val_MinusLogProbMetric: 28.8726 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 463/1000
2023-10-30 19:13:01.123 
Epoch 463/1000 
	 loss: 28.4349, MinusLogProbMetric: 28.4349, val_loss: 29.0422, val_MinusLogProbMetric: 29.0422

Epoch 463: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4349 - MinusLogProbMetric: 28.4349 - val_loss: 29.0422 - val_MinusLogProbMetric: 29.0422 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 464/1000
2023-10-30 19:13:30.162 
Epoch 464/1000 
	 loss: 28.4041, MinusLogProbMetric: 28.4041, val_loss: 29.1848, val_MinusLogProbMetric: 29.1848

Epoch 464: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4041 - MinusLogProbMetric: 28.4041 - val_loss: 29.1848 - val_MinusLogProbMetric: 29.1848 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 465/1000
2023-10-30 19:13:59.120 
Epoch 465/1000 
	 loss: 28.3840, MinusLogProbMetric: 28.3840, val_loss: 28.8078, val_MinusLogProbMetric: 28.8078

Epoch 465: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.3840 - MinusLogProbMetric: 28.3840 - val_loss: 28.8078 - val_MinusLogProbMetric: 28.8078 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 466/1000
2023-10-30 19:14:27.925 
Epoch 466/1000 
	 loss: 28.4957, MinusLogProbMetric: 28.4957, val_loss: 29.6962, val_MinusLogProbMetric: 29.6962

Epoch 466: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4957 - MinusLogProbMetric: 28.4957 - val_loss: 29.6962 - val_MinusLogProbMetric: 29.6962 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 467/1000
2023-10-30 19:14:57.939 
Epoch 467/1000 
	 loss: 28.4611, MinusLogProbMetric: 28.4611, val_loss: 28.7742, val_MinusLogProbMetric: 28.7742

Epoch 467: val_loss did not improve from 28.60888
196/196 - 30s - loss: 28.4611 - MinusLogProbMetric: 28.4611 - val_loss: 28.7742 - val_MinusLogProbMetric: 28.7742 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 468/1000
2023-10-30 19:15:27.222 
Epoch 468/1000 
	 loss: 28.4547, MinusLogProbMetric: 28.4547, val_loss: 28.9246, val_MinusLogProbMetric: 28.9246

Epoch 468: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4547 - MinusLogProbMetric: 28.4547 - val_loss: 28.9246 - val_MinusLogProbMetric: 28.9246 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 469/1000
2023-10-30 19:15:56.010 
Epoch 469/1000 
	 loss: 28.4365, MinusLogProbMetric: 28.4365, val_loss: 28.7487, val_MinusLogProbMetric: 28.7487

Epoch 469: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4365 - MinusLogProbMetric: 28.4365 - val_loss: 28.7487 - val_MinusLogProbMetric: 28.7487 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 470/1000
2023-10-30 19:16:24.844 
Epoch 470/1000 
	 loss: 28.3750, MinusLogProbMetric: 28.3750, val_loss: 29.1091, val_MinusLogProbMetric: 29.1091

Epoch 470: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.3750 - MinusLogProbMetric: 28.3750 - val_loss: 29.1091 - val_MinusLogProbMetric: 29.1091 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 471/1000
2023-10-30 19:16:53.505 
Epoch 471/1000 
	 loss: 28.4211, MinusLogProbMetric: 28.4211, val_loss: 28.8368, val_MinusLogProbMetric: 28.8368

Epoch 471: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4211 - MinusLogProbMetric: 28.4211 - val_loss: 28.8368 - val_MinusLogProbMetric: 28.8368 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 472/1000
2023-10-30 19:17:22.315 
Epoch 472/1000 
	 loss: 28.4304, MinusLogProbMetric: 28.4304, val_loss: 29.0817, val_MinusLogProbMetric: 29.0817

Epoch 472: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4304 - MinusLogProbMetric: 28.4304 - val_loss: 29.0817 - val_MinusLogProbMetric: 29.0817 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 473/1000
2023-10-30 19:17:52.434 
Epoch 473/1000 
	 loss: 28.4778, MinusLogProbMetric: 28.4778, val_loss: 28.9783, val_MinusLogProbMetric: 28.9783

Epoch 473: val_loss did not improve from 28.60888
196/196 - 30s - loss: 28.4778 - MinusLogProbMetric: 28.4778 - val_loss: 28.9783 - val_MinusLogProbMetric: 28.9783 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 474/1000
2023-10-30 19:18:20.675 
Epoch 474/1000 
	 loss: 28.4039, MinusLogProbMetric: 28.4039, val_loss: 28.7007, val_MinusLogProbMetric: 28.7007

Epoch 474: val_loss did not improve from 28.60888
196/196 - 28s - loss: 28.4039 - MinusLogProbMetric: 28.4039 - val_loss: 28.7007 - val_MinusLogProbMetric: 28.7007 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 475/1000
2023-10-30 19:18:49.633 
Epoch 475/1000 
	 loss: 28.4290, MinusLogProbMetric: 28.4290, val_loss: 28.7026, val_MinusLogProbMetric: 28.7026

Epoch 475: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.4290 - MinusLogProbMetric: 28.4290 - val_loss: 28.7026 - val_MinusLogProbMetric: 28.7026 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 476/1000
2023-10-30 19:19:17.206 
Epoch 476/1000 
	 loss: 28.4074, MinusLogProbMetric: 28.4074, val_loss: 28.8163, val_MinusLogProbMetric: 28.8163

Epoch 476: val_loss did not improve from 28.60888
196/196 - 28s - loss: 28.4074 - MinusLogProbMetric: 28.4074 - val_loss: 28.8163 - val_MinusLogProbMetric: 28.8163 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 477/1000
2023-10-30 19:19:46.238 
Epoch 477/1000 
	 loss: 28.5092, MinusLogProbMetric: 28.5092, val_loss: 29.2423, val_MinusLogProbMetric: 29.2423

Epoch 477: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.5092 - MinusLogProbMetric: 28.5092 - val_loss: 29.2423 - val_MinusLogProbMetric: 29.2423 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 478/1000
2023-10-30 19:20:16.505 
Epoch 478/1000 
	 loss: 28.3753, MinusLogProbMetric: 28.3753, val_loss: 28.8296, val_MinusLogProbMetric: 28.8296

Epoch 478: val_loss did not improve from 28.60888
196/196 - 30s - loss: 28.3753 - MinusLogProbMetric: 28.3753 - val_loss: 28.8296 - val_MinusLogProbMetric: 28.8296 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 479/1000
2023-10-30 19:20:45.211 
Epoch 479/1000 
	 loss: 28.3786, MinusLogProbMetric: 28.3786, val_loss: 28.9568, val_MinusLogProbMetric: 28.9568

Epoch 479: val_loss did not improve from 28.60888
196/196 - 29s - loss: 28.3786 - MinusLogProbMetric: 28.3786 - val_loss: 28.9568 - val_MinusLogProbMetric: 28.9568 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 480/1000
2023-10-30 19:21:13.212 
Epoch 480/1000 
	 loss: 28.3677, MinusLogProbMetric: 28.3677, val_loss: 28.6240, val_MinusLogProbMetric: 28.6240

Epoch 480: val_loss did not improve from 28.60888
196/196 - 28s - loss: 28.3677 - MinusLogProbMetric: 28.3677 - val_loss: 28.6240 - val_MinusLogProbMetric: 28.6240 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 481/1000
2023-10-30 19:21:44.765 
Epoch 481/1000 
	 loss: 28.3795, MinusLogProbMetric: 28.3795, val_loss: 28.5267, val_MinusLogProbMetric: 28.5267

Epoch 481: val_loss improved from 28.60888 to 28.52673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 28.3795 - MinusLogProbMetric: 28.3795 - val_loss: 28.5267 - val_MinusLogProbMetric: 28.5267 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 482/1000
2023-10-30 19:22:17.038 
Epoch 482/1000 
	 loss: 28.3575, MinusLogProbMetric: 28.3575, val_loss: 29.0512, val_MinusLogProbMetric: 29.0512

Epoch 482: val_loss did not improve from 28.52673
196/196 - 32s - loss: 28.3575 - MinusLogProbMetric: 28.3575 - val_loss: 29.0512 - val_MinusLogProbMetric: 29.0512 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 483/1000
2023-10-30 19:22:44.704 
Epoch 483/1000 
	 loss: 28.3572, MinusLogProbMetric: 28.3572, val_loss: 28.9953, val_MinusLogProbMetric: 28.9953

Epoch 483: val_loss did not improve from 28.52673
196/196 - 28s - loss: 28.3572 - MinusLogProbMetric: 28.3572 - val_loss: 28.9953 - val_MinusLogProbMetric: 28.9953 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 484/1000
2023-10-30 19:23:13.648 
Epoch 484/1000 
	 loss: 28.3899, MinusLogProbMetric: 28.3899, val_loss: 28.6470, val_MinusLogProbMetric: 28.6470

Epoch 484: val_loss did not improve from 28.52673
196/196 - 29s - loss: 28.3899 - MinusLogProbMetric: 28.3899 - val_loss: 28.6470 - val_MinusLogProbMetric: 28.6470 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 485/1000
2023-10-30 19:23:42.825 
Epoch 485/1000 
	 loss: 28.3791, MinusLogProbMetric: 28.3791, val_loss: 28.7983, val_MinusLogProbMetric: 28.7983

Epoch 485: val_loss did not improve from 28.52673
196/196 - 29s - loss: 28.3791 - MinusLogProbMetric: 28.3791 - val_loss: 28.7983 - val_MinusLogProbMetric: 28.7983 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 486/1000
2023-10-30 19:24:13.531 
Epoch 486/1000 
	 loss: 28.3369, MinusLogProbMetric: 28.3369, val_loss: 29.0657, val_MinusLogProbMetric: 29.0657

Epoch 486: val_loss did not improve from 28.52673
196/196 - 31s - loss: 28.3369 - MinusLogProbMetric: 28.3369 - val_loss: 29.0657 - val_MinusLogProbMetric: 29.0657 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 487/1000
2023-10-30 19:24:45.233 
Epoch 487/1000 
	 loss: 28.4391, MinusLogProbMetric: 28.4391, val_loss: 28.7530, val_MinusLogProbMetric: 28.7530

Epoch 487: val_loss did not improve from 28.52673
196/196 - 32s - loss: 28.4391 - MinusLogProbMetric: 28.4391 - val_loss: 28.7530 - val_MinusLogProbMetric: 28.7530 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 488/1000
2023-10-30 19:25:16.479 
Epoch 488/1000 
	 loss: 28.3028, MinusLogProbMetric: 28.3028, val_loss: 28.7678, val_MinusLogProbMetric: 28.7678

Epoch 488: val_loss did not improve from 28.52673
196/196 - 31s - loss: 28.3028 - MinusLogProbMetric: 28.3028 - val_loss: 28.7678 - val_MinusLogProbMetric: 28.7678 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 489/1000
2023-10-30 19:25:48.449 
Epoch 489/1000 
	 loss: 28.4290, MinusLogProbMetric: 28.4290, val_loss: 28.9426, val_MinusLogProbMetric: 28.9426

Epoch 489: val_loss did not improve from 28.52673
196/196 - 32s - loss: 28.4290 - MinusLogProbMetric: 28.4290 - val_loss: 28.9426 - val_MinusLogProbMetric: 28.9426 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 490/1000
2023-10-30 19:26:17.622 
Epoch 490/1000 
	 loss: 28.4089, MinusLogProbMetric: 28.4089, val_loss: 28.6374, val_MinusLogProbMetric: 28.6374

Epoch 490: val_loss did not improve from 28.52673
196/196 - 29s - loss: 28.4089 - MinusLogProbMetric: 28.4089 - val_loss: 28.6374 - val_MinusLogProbMetric: 28.6374 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 491/1000
2023-10-30 19:26:47.070 
Epoch 491/1000 
	 loss: 28.3823, MinusLogProbMetric: 28.3823, val_loss: 28.6295, val_MinusLogProbMetric: 28.6295

Epoch 491: val_loss did not improve from 28.52673
196/196 - 29s - loss: 28.3823 - MinusLogProbMetric: 28.3823 - val_loss: 28.6295 - val_MinusLogProbMetric: 28.6295 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 492/1000
2023-10-30 19:27:17.374 
Epoch 492/1000 
	 loss: 28.3569, MinusLogProbMetric: 28.3569, val_loss: 28.9326, val_MinusLogProbMetric: 28.9326

Epoch 492: val_loss did not improve from 28.52673
196/196 - 30s - loss: 28.3569 - MinusLogProbMetric: 28.3569 - val_loss: 28.9326 - val_MinusLogProbMetric: 28.9326 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 493/1000
2023-10-30 19:27:47.130 
Epoch 493/1000 
	 loss: 28.3325, MinusLogProbMetric: 28.3325, val_loss: 29.0669, val_MinusLogProbMetric: 29.0669

Epoch 493: val_loss did not improve from 28.52673
196/196 - 30s - loss: 28.3325 - MinusLogProbMetric: 28.3325 - val_loss: 29.0669 - val_MinusLogProbMetric: 29.0669 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 494/1000
2023-10-30 19:28:16.181 
Epoch 494/1000 
	 loss: 28.3892, MinusLogProbMetric: 28.3892, val_loss: 28.8973, val_MinusLogProbMetric: 28.8973

Epoch 494: val_loss did not improve from 28.52673
196/196 - 29s - loss: 28.3892 - MinusLogProbMetric: 28.3892 - val_loss: 28.8973 - val_MinusLogProbMetric: 28.8973 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 495/1000
2023-10-30 19:28:44.111 
Epoch 495/1000 
	 loss: 28.3205, MinusLogProbMetric: 28.3205, val_loss: 29.1350, val_MinusLogProbMetric: 29.1350

Epoch 495: val_loss did not improve from 28.52673
196/196 - 28s - loss: 28.3205 - MinusLogProbMetric: 28.3205 - val_loss: 29.1350 - val_MinusLogProbMetric: 29.1350 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 496/1000
2023-10-30 19:29:13.029 
Epoch 496/1000 
	 loss: 28.3172, MinusLogProbMetric: 28.3172, val_loss: 28.6479, val_MinusLogProbMetric: 28.6479

Epoch 496: val_loss did not improve from 28.52673
196/196 - 29s - loss: 28.3172 - MinusLogProbMetric: 28.3172 - val_loss: 28.6479 - val_MinusLogProbMetric: 28.6479 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 497/1000
2023-10-30 19:29:44.309 
Epoch 497/1000 
	 loss: 28.3861, MinusLogProbMetric: 28.3861, val_loss: 28.8129, val_MinusLogProbMetric: 28.8129

Epoch 497: val_loss did not improve from 28.52673
196/196 - 31s - loss: 28.3861 - MinusLogProbMetric: 28.3861 - val_loss: 28.8129 - val_MinusLogProbMetric: 28.8129 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 498/1000
2023-10-30 19:30:17.012 
Epoch 498/1000 
	 loss: 28.3529, MinusLogProbMetric: 28.3529, val_loss: 28.6357, val_MinusLogProbMetric: 28.6357

Epoch 498: val_loss did not improve from 28.52673
196/196 - 33s - loss: 28.3529 - MinusLogProbMetric: 28.3529 - val_loss: 28.6357 - val_MinusLogProbMetric: 28.6357 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 499/1000
2023-10-30 19:30:50.998 
Epoch 499/1000 
	 loss: 28.3637, MinusLogProbMetric: 28.3637, val_loss: 28.7149, val_MinusLogProbMetric: 28.7149

Epoch 499: val_loss did not improve from 28.52673
196/196 - 34s - loss: 28.3637 - MinusLogProbMetric: 28.3637 - val_loss: 28.7149 - val_MinusLogProbMetric: 28.7149 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 500/1000
2023-10-30 19:31:25.323 
Epoch 500/1000 
	 loss: 28.3685, MinusLogProbMetric: 28.3685, val_loss: 28.9714, val_MinusLogProbMetric: 28.9714

Epoch 500: val_loss did not improve from 28.52673
196/196 - 34s - loss: 28.3685 - MinusLogProbMetric: 28.3685 - val_loss: 28.9714 - val_MinusLogProbMetric: 28.9714 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 501/1000
2023-10-30 19:31:59.580 
Epoch 501/1000 
	 loss: 28.3001, MinusLogProbMetric: 28.3001, val_loss: 28.7894, val_MinusLogProbMetric: 28.7894

Epoch 501: val_loss did not improve from 28.52673
196/196 - 34s - loss: 28.3001 - MinusLogProbMetric: 28.3001 - val_loss: 28.7894 - val_MinusLogProbMetric: 28.7894 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 502/1000
2023-10-30 19:32:32.472 
Epoch 502/1000 
	 loss: 28.3722, MinusLogProbMetric: 28.3722, val_loss: 28.6503, val_MinusLogProbMetric: 28.6503

Epoch 502: val_loss did not improve from 28.52673
196/196 - 33s - loss: 28.3722 - MinusLogProbMetric: 28.3722 - val_loss: 28.6503 - val_MinusLogProbMetric: 28.6503 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 503/1000
2023-10-30 19:33:06.990 
Epoch 503/1000 
	 loss: 28.2973, MinusLogProbMetric: 28.2973, val_loss: 30.0672, val_MinusLogProbMetric: 30.0672

Epoch 503: val_loss did not improve from 28.52673
196/196 - 35s - loss: 28.2973 - MinusLogProbMetric: 28.2973 - val_loss: 30.0672 - val_MinusLogProbMetric: 30.0672 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 504/1000
2023-10-30 19:33:38.950 
Epoch 504/1000 
	 loss: 28.4750, MinusLogProbMetric: 28.4750, val_loss: 28.6294, val_MinusLogProbMetric: 28.6294

Epoch 504: val_loss did not improve from 28.52673
196/196 - 32s - loss: 28.4750 - MinusLogProbMetric: 28.4750 - val_loss: 28.6294 - val_MinusLogProbMetric: 28.6294 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 505/1000
2023-10-30 19:34:11.587 
Epoch 505/1000 
	 loss: 28.3494, MinusLogProbMetric: 28.3494, val_loss: 29.0262, val_MinusLogProbMetric: 29.0262

Epoch 505: val_loss did not improve from 28.52673
196/196 - 33s - loss: 28.3494 - MinusLogProbMetric: 28.3494 - val_loss: 29.0262 - val_MinusLogProbMetric: 29.0262 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 506/1000
2023-10-30 19:34:45.045 
Epoch 506/1000 
	 loss: 28.3199, MinusLogProbMetric: 28.3199, val_loss: 28.8609, val_MinusLogProbMetric: 28.8609

Epoch 506: val_loss did not improve from 28.52673
196/196 - 33s - loss: 28.3199 - MinusLogProbMetric: 28.3199 - val_loss: 28.8609 - val_MinusLogProbMetric: 28.8609 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 507/1000
2023-10-30 19:35:14.810 
Epoch 507/1000 
	 loss: 28.3470, MinusLogProbMetric: 28.3470, val_loss: 28.7497, val_MinusLogProbMetric: 28.7497

Epoch 507: val_loss did not improve from 28.52673
196/196 - 30s - loss: 28.3470 - MinusLogProbMetric: 28.3470 - val_loss: 28.7497 - val_MinusLogProbMetric: 28.7497 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 508/1000
2023-10-30 19:35:47.047 
Epoch 508/1000 
	 loss: 28.3521, MinusLogProbMetric: 28.3521, val_loss: 28.7463, val_MinusLogProbMetric: 28.7463

Epoch 508: val_loss did not improve from 28.52673
196/196 - 32s - loss: 28.3521 - MinusLogProbMetric: 28.3521 - val_loss: 28.7463 - val_MinusLogProbMetric: 28.7463 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 509/1000
2023-10-30 19:36:20.267 
Epoch 509/1000 
	 loss: 28.2698, MinusLogProbMetric: 28.2698, val_loss: 28.7016, val_MinusLogProbMetric: 28.7016

Epoch 509: val_loss did not improve from 28.52673
196/196 - 33s - loss: 28.2698 - MinusLogProbMetric: 28.2698 - val_loss: 28.7016 - val_MinusLogProbMetric: 28.7016 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 510/1000
2023-10-30 19:36:54.149 
Epoch 510/1000 
	 loss: 28.3211, MinusLogProbMetric: 28.3211, val_loss: 28.9437, val_MinusLogProbMetric: 28.9437

Epoch 510: val_loss did not improve from 28.52673
196/196 - 34s - loss: 28.3211 - MinusLogProbMetric: 28.3211 - val_loss: 28.9437 - val_MinusLogProbMetric: 28.9437 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 511/1000
2023-10-30 19:37:29.026 
Epoch 511/1000 
	 loss: 28.3813, MinusLogProbMetric: 28.3813, val_loss: 28.9322, val_MinusLogProbMetric: 28.9322

Epoch 511: val_loss did not improve from 28.52673
196/196 - 35s - loss: 28.3813 - MinusLogProbMetric: 28.3813 - val_loss: 28.9322 - val_MinusLogProbMetric: 28.9322 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 512/1000
2023-10-30 19:38:03.245 
Epoch 512/1000 
	 loss: 28.3412, MinusLogProbMetric: 28.3412, val_loss: 28.5567, val_MinusLogProbMetric: 28.5567

Epoch 512: val_loss did not improve from 28.52673
196/196 - 34s - loss: 28.3412 - MinusLogProbMetric: 28.3412 - val_loss: 28.5567 - val_MinusLogProbMetric: 28.5567 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 513/1000
2023-10-30 19:38:36.655 
Epoch 513/1000 
	 loss: 28.3425, MinusLogProbMetric: 28.3425, val_loss: 28.5104, val_MinusLogProbMetric: 28.5104

Epoch 513: val_loss improved from 28.52673 to 28.51040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 28.3425 - MinusLogProbMetric: 28.3425 - val_loss: 28.5104 - val_MinusLogProbMetric: 28.5104 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 514/1000
2023-10-30 19:39:10.812 
Epoch 514/1000 
	 loss: 28.3123, MinusLogProbMetric: 28.3123, val_loss: 28.7627, val_MinusLogProbMetric: 28.7627

Epoch 514: val_loss did not improve from 28.51040
196/196 - 34s - loss: 28.3123 - MinusLogProbMetric: 28.3123 - val_loss: 28.7627 - val_MinusLogProbMetric: 28.7627 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 515/1000
2023-10-30 19:39:44.781 
Epoch 515/1000 
	 loss: 28.3235, MinusLogProbMetric: 28.3235, val_loss: 29.3024, val_MinusLogProbMetric: 29.3024

Epoch 515: val_loss did not improve from 28.51040
196/196 - 34s - loss: 28.3235 - MinusLogProbMetric: 28.3235 - val_loss: 29.3024 - val_MinusLogProbMetric: 29.3024 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 516/1000
2023-10-30 19:40:18.479 
Epoch 516/1000 
	 loss: 28.3544, MinusLogProbMetric: 28.3544, val_loss: 28.6573, val_MinusLogProbMetric: 28.6573

Epoch 516: val_loss did not improve from 28.51040
196/196 - 34s - loss: 28.3544 - MinusLogProbMetric: 28.3544 - val_loss: 28.6573 - val_MinusLogProbMetric: 28.6573 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 517/1000
2023-10-30 19:40:51.476 
Epoch 517/1000 
	 loss: 28.4059, MinusLogProbMetric: 28.4059, val_loss: 28.5715, val_MinusLogProbMetric: 28.5715

Epoch 517: val_loss did not improve from 28.51040
196/196 - 33s - loss: 28.4059 - MinusLogProbMetric: 28.4059 - val_loss: 28.5715 - val_MinusLogProbMetric: 28.5715 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 518/1000
2023-10-30 19:41:25.779 
Epoch 518/1000 
	 loss: 28.2819, MinusLogProbMetric: 28.2819, val_loss: 28.8332, val_MinusLogProbMetric: 28.8332

Epoch 518: val_loss did not improve from 28.51040
196/196 - 34s - loss: 28.2819 - MinusLogProbMetric: 28.2819 - val_loss: 28.8332 - val_MinusLogProbMetric: 28.8332 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 519/1000
2023-10-30 19:41:57.929 
Epoch 519/1000 
	 loss: 28.3010, MinusLogProbMetric: 28.3010, val_loss: 28.4420, val_MinusLogProbMetric: 28.4420

Epoch 519: val_loss improved from 28.51040 to 28.44200, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 28.3010 - MinusLogProbMetric: 28.3010 - val_loss: 28.4420 - val_MinusLogProbMetric: 28.4420 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 520/1000
2023-10-30 19:42:31.808 
Epoch 520/1000 
	 loss: 28.3390, MinusLogProbMetric: 28.3390, val_loss: 28.9114, val_MinusLogProbMetric: 28.9114

Epoch 520: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.3390 - MinusLogProbMetric: 28.3390 - val_loss: 28.9114 - val_MinusLogProbMetric: 28.9114 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 521/1000
2023-10-30 19:43:05.772 
Epoch 521/1000 
	 loss: 28.3823, MinusLogProbMetric: 28.3823, val_loss: 29.5531, val_MinusLogProbMetric: 29.5531

Epoch 521: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3823 - MinusLogProbMetric: 28.3823 - val_loss: 29.5531 - val_MinusLogProbMetric: 29.5531 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 522/1000
2023-10-30 19:43:37.802 
Epoch 522/1000 
	 loss: 28.3614, MinusLogProbMetric: 28.3614, val_loss: 29.1263, val_MinusLogProbMetric: 29.1263

Epoch 522: val_loss did not improve from 28.44200
196/196 - 32s - loss: 28.3614 - MinusLogProbMetric: 28.3614 - val_loss: 29.1263 - val_MinusLogProbMetric: 29.1263 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 523/1000
2023-10-30 19:44:12.080 
Epoch 523/1000 
	 loss: 28.3601, MinusLogProbMetric: 28.3601, val_loss: 28.5453, val_MinusLogProbMetric: 28.5453

Epoch 523: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3601 - MinusLogProbMetric: 28.3601 - val_loss: 28.5453 - val_MinusLogProbMetric: 28.5453 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 524/1000
2023-10-30 19:44:45.684 
Epoch 524/1000 
	 loss: 28.4002, MinusLogProbMetric: 28.4002, val_loss: 28.8436, val_MinusLogProbMetric: 28.8436

Epoch 524: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.4002 - MinusLogProbMetric: 28.4002 - val_loss: 28.8436 - val_MinusLogProbMetric: 28.8436 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 525/1000
2023-10-30 19:45:19.772 
Epoch 525/1000 
	 loss: 28.3567, MinusLogProbMetric: 28.3567, val_loss: 28.9127, val_MinusLogProbMetric: 28.9127

Epoch 525: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3567 - MinusLogProbMetric: 28.3567 - val_loss: 28.9127 - val_MinusLogProbMetric: 28.9127 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 526/1000
2023-10-30 19:45:53.560 
Epoch 526/1000 
	 loss: 28.3106, MinusLogProbMetric: 28.3106, val_loss: 28.7021, val_MinusLogProbMetric: 28.7021

Epoch 526: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3106 - MinusLogProbMetric: 28.3106 - val_loss: 28.7021 - val_MinusLogProbMetric: 28.7021 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 527/1000
2023-10-30 19:46:26.168 
Epoch 527/1000 
	 loss: 28.2892, MinusLogProbMetric: 28.2892, val_loss: 28.7263, val_MinusLogProbMetric: 28.7263

Epoch 527: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.2892 - MinusLogProbMetric: 28.2892 - val_loss: 28.7263 - val_MinusLogProbMetric: 28.7263 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 528/1000
2023-10-30 19:46:59.963 
Epoch 528/1000 
	 loss: 28.2815, MinusLogProbMetric: 28.2815, val_loss: 29.0427, val_MinusLogProbMetric: 29.0427

Epoch 528: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2815 - MinusLogProbMetric: 28.2815 - val_loss: 29.0427 - val_MinusLogProbMetric: 29.0427 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 529/1000
2023-10-30 19:47:34.267 
Epoch 529/1000 
	 loss: 28.3423, MinusLogProbMetric: 28.3423, val_loss: 28.8591, val_MinusLogProbMetric: 28.8591

Epoch 529: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3423 - MinusLogProbMetric: 28.3423 - val_loss: 28.8591 - val_MinusLogProbMetric: 28.8591 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 530/1000
2023-10-30 19:48:08.399 
Epoch 530/1000 
	 loss: 28.2945, MinusLogProbMetric: 28.2945, val_loss: 28.6588, val_MinusLogProbMetric: 28.6588

Epoch 530: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2945 - MinusLogProbMetric: 28.2945 - val_loss: 28.6588 - val_MinusLogProbMetric: 28.6588 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 531/1000
2023-10-30 19:48:42.356 
Epoch 531/1000 
	 loss: 28.2700, MinusLogProbMetric: 28.2700, val_loss: 28.5170, val_MinusLogProbMetric: 28.5170

Epoch 531: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2700 - MinusLogProbMetric: 28.2700 - val_loss: 28.5170 - val_MinusLogProbMetric: 28.5170 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 532/1000
2023-10-30 19:49:16.262 
Epoch 532/1000 
	 loss: 28.2663, MinusLogProbMetric: 28.2663, val_loss: 29.0493, val_MinusLogProbMetric: 29.0493

Epoch 532: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2663 - MinusLogProbMetric: 28.2663 - val_loss: 29.0493 - val_MinusLogProbMetric: 29.0493 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 533/1000
2023-10-30 19:49:50.269 
Epoch 533/1000 
	 loss: 28.2764, MinusLogProbMetric: 28.2764, val_loss: 29.0825, val_MinusLogProbMetric: 29.0825

Epoch 533: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2764 - MinusLogProbMetric: 28.2764 - val_loss: 29.0825 - val_MinusLogProbMetric: 29.0825 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 534/1000
2023-10-30 19:50:24.030 
Epoch 534/1000 
	 loss: 28.2557, MinusLogProbMetric: 28.2557, val_loss: 28.8359, val_MinusLogProbMetric: 28.8359

Epoch 534: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2557 - MinusLogProbMetric: 28.2557 - val_loss: 28.8359 - val_MinusLogProbMetric: 28.8359 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 535/1000
2023-10-30 19:50:57.993 
Epoch 535/1000 
	 loss: 28.2512, MinusLogProbMetric: 28.2512, val_loss: 28.6966, val_MinusLogProbMetric: 28.6966

Epoch 535: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2512 - MinusLogProbMetric: 28.2512 - val_loss: 28.6966 - val_MinusLogProbMetric: 28.6966 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 536/1000
2023-10-30 19:51:32.261 
Epoch 536/1000 
	 loss: 28.2410, MinusLogProbMetric: 28.2410, val_loss: 28.9216, val_MinusLogProbMetric: 28.9216

Epoch 536: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2410 - MinusLogProbMetric: 28.2410 - val_loss: 28.9216 - val_MinusLogProbMetric: 28.9216 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 537/1000
2023-10-30 19:52:06.337 
Epoch 537/1000 
	 loss: 28.3005, MinusLogProbMetric: 28.3005, val_loss: 28.6603, val_MinusLogProbMetric: 28.6603

Epoch 537: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3005 - MinusLogProbMetric: 28.3005 - val_loss: 28.6603 - val_MinusLogProbMetric: 28.6603 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 538/1000
2023-10-30 19:52:40.460 
Epoch 538/1000 
	 loss: 28.3004, MinusLogProbMetric: 28.3004, val_loss: 28.8521, val_MinusLogProbMetric: 28.8521

Epoch 538: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3004 - MinusLogProbMetric: 28.3004 - val_loss: 28.8521 - val_MinusLogProbMetric: 28.8521 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 539/1000
2023-10-30 19:53:13.870 
Epoch 539/1000 
	 loss: 28.2659, MinusLogProbMetric: 28.2659, val_loss: 28.7752, val_MinusLogProbMetric: 28.7752

Epoch 539: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.2659 - MinusLogProbMetric: 28.2659 - val_loss: 28.7752 - val_MinusLogProbMetric: 28.7752 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 540/1000
2023-10-30 19:53:47.535 
Epoch 540/1000 
	 loss: 28.3376, MinusLogProbMetric: 28.3376, val_loss: 29.1857, val_MinusLogProbMetric: 29.1857

Epoch 540: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3376 - MinusLogProbMetric: 28.3376 - val_loss: 29.1857 - val_MinusLogProbMetric: 29.1857 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 541/1000
2023-10-30 19:54:21.259 
Epoch 541/1000 
	 loss: 28.2917, MinusLogProbMetric: 28.2917, val_loss: 28.8265, val_MinusLogProbMetric: 28.8265

Epoch 541: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2917 - MinusLogProbMetric: 28.2917 - val_loss: 28.8265 - val_MinusLogProbMetric: 28.8265 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 542/1000
2023-10-30 19:54:54.491 
Epoch 542/1000 
	 loss: 28.2834, MinusLogProbMetric: 28.2834, val_loss: 28.5649, val_MinusLogProbMetric: 28.5649

Epoch 542: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.2834 - MinusLogProbMetric: 28.2834 - val_loss: 28.5649 - val_MinusLogProbMetric: 28.5649 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 543/1000
2023-10-30 19:55:27.726 
Epoch 543/1000 
	 loss: 28.2822, MinusLogProbMetric: 28.2822, val_loss: 28.9827, val_MinusLogProbMetric: 28.9827

Epoch 543: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.2822 - MinusLogProbMetric: 28.2822 - val_loss: 28.9827 - val_MinusLogProbMetric: 28.9827 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 544/1000
2023-10-30 19:56:00.359 
Epoch 544/1000 
	 loss: 28.2767, MinusLogProbMetric: 28.2767, val_loss: 28.7710, val_MinusLogProbMetric: 28.7710

Epoch 544: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.2767 - MinusLogProbMetric: 28.2767 - val_loss: 28.7710 - val_MinusLogProbMetric: 28.7710 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 545/1000
2023-10-30 19:56:34.661 
Epoch 545/1000 
	 loss: 28.3046, MinusLogProbMetric: 28.3046, val_loss: 28.6772, val_MinusLogProbMetric: 28.6772

Epoch 545: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.3046 - MinusLogProbMetric: 28.3046 - val_loss: 28.6772 - val_MinusLogProbMetric: 28.6772 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 546/1000
2023-10-30 19:57:07.405 
Epoch 546/1000 
	 loss: 28.3072, MinusLogProbMetric: 28.3072, val_loss: 28.8764, val_MinusLogProbMetric: 28.8764

Epoch 546: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.3072 - MinusLogProbMetric: 28.3072 - val_loss: 28.8764 - val_MinusLogProbMetric: 28.8764 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 547/1000
2023-10-30 19:57:40.479 
Epoch 547/1000 
	 loss: 28.3478, MinusLogProbMetric: 28.3478, val_loss: 28.7134, val_MinusLogProbMetric: 28.7134

Epoch 547: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.3478 - MinusLogProbMetric: 28.3478 - val_loss: 28.7134 - val_MinusLogProbMetric: 28.7134 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 548/1000
2023-10-30 19:58:14.917 
Epoch 548/1000 
	 loss: 28.2479, MinusLogProbMetric: 28.2479, val_loss: 28.9102, val_MinusLogProbMetric: 28.9102

Epoch 548: val_loss did not improve from 28.44200
196/196 - 34s - loss: 28.2479 - MinusLogProbMetric: 28.2479 - val_loss: 28.9102 - val_MinusLogProbMetric: 28.9102 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 549/1000
2023-10-30 19:58:48.279 
Epoch 549/1000 
	 loss: 28.2985, MinusLogProbMetric: 28.2985, val_loss: 28.8049, val_MinusLogProbMetric: 28.8049

Epoch 549: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.2985 - MinusLogProbMetric: 28.2985 - val_loss: 28.8049 - val_MinusLogProbMetric: 28.8049 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 550/1000
2023-10-30 19:59:18.282 
Epoch 550/1000 
	 loss: 28.2648, MinusLogProbMetric: 28.2648, val_loss: 28.6678, val_MinusLogProbMetric: 28.6678

Epoch 550: val_loss did not improve from 28.44200
196/196 - 30s - loss: 28.2648 - MinusLogProbMetric: 28.2648 - val_loss: 28.6678 - val_MinusLogProbMetric: 28.6678 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 551/1000
2023-10-30 19:59:47.153 
Epoch 551/1000 
	 loss: 28.2920, MinusLogProbMetric: 28.2920, val_loss: 28.7400, val_MinusLogProbMetric: 28.7400

Epoch 551: val_loss did not improve from 28.44200
196/196 - 29s - loss: 28.2920 - MinusLogProbMetric: 28.2920 - val_loss: 28.7400 - val_MinusLogProbMetric: 28.7400 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 552/1000
2023-10-30 20:00:15.733 
Epoch 552/1000 
	 loss: 28.3664, MinusLogProbMetric: 28.3664, val_loss: 29.0947, val_MinusLogProbMetric: 29.0947

Epoch 552: val_loss did not improve from 28.44200
196/196 - 29s - loss: 28.3664 - MinusLogProbMetric: 28.3664 - val_loss: 29.0947 - val_MinusLogProbMetric: 29.0947 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 553/1000
2023-10-30 20:00:45.057 
Epoch 553/1000 
	 loss: 28.2613, MinusLogProbMetric: 28.2613, val_loss: 28.6372, val_MinusLogProbMetric: 28.6372

Epoch 553: val_loss did not improve from 28.44200
196/196 - 29s - loss: 28.2613 - MinusLogProbMetric: 28.2613 - val_loss: 28.6372 - val_MinusLogProbMetric: 28.6372 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 554/1000
2023-10-30 20:01:17.794 
Epoch 554/1000 
	 loss: 28.2629, MinusLogProbMetric: 28.2629, val_loss: 28.5606, val_MinusLogProbMetric: 28.5606

Epoch 554: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.2629 - MinusLogProbMetric: 28.2629 - val_loss: 28.5606 - val_MinusLogProbMetric: 28.5606 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 555/1000
2023-10-30 20:01:47.958 
Epoch 555/1000 
	 loss: 28.1906, MinusLogProbMetric: 28.1906, val_loss: 28.8054, val_MinusLogProbMetric: 28.8054

Epoch 555: val_loss did not improve from 28.44200
196/196 - 30s - loss: 28.1906 - MinusLogProbMetric: 28.1906 - val_loss: 28.8054 - val_MinusLogProbMetric: 28.8054 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 556/1000
2023-10-30 20:02:16.321 
Epoch 556/1000 
	 loss: 28.2516, MinusLogProbMetric: 28.2516, val_loss: 28.7711, val_MinusLogProbMetric: 28.7711

Epoch 556: val_loss did not improve from 28.44200
196/196 - 28s - loss: 28.2516 - MinusLogProbMetric: 28.2516 - val_loss: 28.7711 - val_MinusLogProbMetric: 28.7711 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 557/1000
2023-10-30 20:02:45.097 
Epoch 557/1000 
	 loss: 28.3103, MinusLogProbMetric: 28.3103, val_loss: 29.0390, val_MinusLogProbMetric: 29.0390

Epoch 557: val_loss did not improve from 28.44200
196/196 - 29s - loss: 28.3103 - MinusLogProbMetric: 28.3103 - val_loss: 29.0390 - val_MinusLogProbMetric: 29.0390 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 558/1000
2023-10-30 20:03:13.808 
Epoch 558/1000 
	 loss: 28.1828, MinusLogProbMetric: 28.1828, val_loss: 28.4631, val_MinusLogProbMetric: 28.4631

Epoch 558: val_loss did not improve from 28.44200
196/196 - 29s - loss: 28.1828 - MinusLogProbMetric: 28.1828 - val_loss: 28.4631 - val_MinusLogProbMetric: 28.4631 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 559/1000
2023-10-30 20:03:44.960 
Epoch 559/1000 
	 loss: 28.2388, MinusLogProbMetric: 28.2388, val_loss: 28.6832, val_MinusLogProbMetric: 28.6832

Epoch 559: val_loss did not improve from 28.44200
196/196 - 31s - loss: 28.2388 - MinusLogProbMetric: 28.2388 - val_loss: 28.6832 - val_MinusLogProbMetric: 28.6832 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 560/1000
2023-10-30 20:04:15.601 
Epoch 560/1000 
	 loss: 28.1885, MinusLogProbMetric: 28.1885, val_loss: 28.9976, val_MinusLogProbMetric: 28.9976

Epoch 560: val_loss did not improve from 28.44200
196/196 - 31s - loss: 28.1885 - MinusLogProbMetric: 28.1885 - val_loss: 28.9976 - val_MinusLogProbMetric: 28.9976 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 561/1000
2023-10-30 20:04:43.632 
Epoch 561/1000 
	 loss: 28.2523, MinusLogProbMetric: 28.2523, val_loss: 28.8313, val_MinusLogProbMetric: 28.8313

Epoch 561: val_loss did not improve from 28.44200
196/196 - 28s - loss: 28.2523 - MinusLogProbMetric: 28.2523 - val_loss: 28.8313 - val_MinusLogProbMetric: 28.8313 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 562/1000
2023-10-30 20:05:12.036 
Epoch 562/1000 
	 loss: 28.2850, MinusLogProbMetric: 28.2850, val_loss: 28.6987, val_MinusLogProbMetric: 28.6987

Epoch 562: val_loss did not improve from 28.44200
196/196 - 28s - loss: 28.2850 - MinusLogProbMetric: 28.2850 - val_loss: 28.6987 - val_MinusLogProbMetric: 28.6987 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 563/1000
2023-10-30 20:05:40.920 
Epoch 563/1000 
	 loss: 28.2285, MinusLogProbMetric: 28.2285, val_loss: 28.7038, val_MinusLogProbMetric: 28.7038

Epoch 563: val_loss did not improve from 28.44200
196/196 - 29s - loss: 28.2285 - MinusLogProbMetric: 28.2285 - val_loss: 28.7038 - val_MinusLogProbMetric: 28.7038 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 564/1000
2023-10-30 20:06:09.246 
Epoch 564/1000 
	 loss: 28.2436, MinusLogProbMetric: 28.2436, val_loss: 28.7169, val_MinusLogProbMetric: 28.7169

Epoch 564: val_loss did not improve from 28.44200
196/196 - 28s - loss: 28.2436 - MinusLogProbMetric: 28.2436 - val_loss: 28.7169 - val_MinusLogProbMetric: 28.7169 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 565/1000
2023-10-30 20:06:42.179 
Epoch 565/1000 
	 loss: 28.2513, MinusLogProbMetric: 28.2513, val_loss: 28.5872, val_MinusLogProbMetric: 28.5872

Epoch 565: val_loss did not improve from 28.44200
196/196 - 33s - loss: 28.2513 - MinusLogProbMetric: 28.2513 - val_loss: 28.5872 - val_MinusLogProbMetric: 28.5872 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 566/1000
2023-10-30 20:07:13.275 
Epoch 566/1000 
	 loss: 28.2391, MinusLogProbMetric: 28.2391, val_loss: 28.9022, val_MinusLogProbMetric: 28.9022

Epoch 566: val_loss did not improve from 28.44200
196/196 - 31s - loss: 28.2391 - MinusLogProbMetric: 28.2391 - val_loss: 28.9022 - val_MinusLogProbMetric: 28.9022 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 567/1000
2023-10-30 20:07:42.089 
Epoch 567/1000 
	 loss: 28.2673, MinusLogProbMetric: 28.2673, val_loss: 28.7017, val_MinusLogProbMetric: 28.7017

Epoch 567: val_loss did not improve from 28.44200
196/196 - 29s - loss: 28.2673 - MinusLogProbMetric: 28.2673 - val_loss: 28.7017 - val_MinusLogProbMetric: 28.7017 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 568/1000
2023-10-30 20:08:09.901 
Epoch 568/1000 
	 loss: 28.2207, MinusLogProbMetric: 28.2207, val_loss: 28.4932, val_MinusLogProbMetric: 28.4932

Epoch 568: val_loss did not improve from 28.44200
196/196 - 28s - loss: 28.2207 - MinusLogProbMetric: 28.2207 - val_loss: 28.4932 - val_MinusLogProbMetric: 28.4932 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 569/1000
2023-10-30 20:08:38.346 
Epoch 569/1000 
	 loss: 28.2069, MinusLogProbMetric: 28.2069, val_loss: 28.5670, val_MinusLogProbMetric: 28.5670

Epoch 569: val_loss did not improve from 28.44200
196/196 - 28s - loss: 28.2069 - MinusLogProbMetric: 28.2069 - val_loss: 28.5670 - val_MinusLogProbMetric: 28.5670 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 570/1000
2023-10-30 20:09:09.273 
Epoch 570/1000 
	 loss: 27.8159, MinusLogProbMetric: 27.8159, val_loss: 28.2355, val_MinusLogProbMetric: 28.2355

Epoch 570: val_loss improved from 28.44200 to 28.23551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 27.8159 - MinusLogProbMetric: 27.8159 - val_loss: 28.2355 - val_MinusLogProbMetric: 28.2355 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 571/1000
2023-10-30 20:09:43.176 
Epoch 571/1000 
	 loss: 27.7941, MinusLogProbMetric: 27.7941, val_loss: 28.4414, val_MinusLogProbMetric: 28.4414

Epoch 571: val_loss did not improve from 28.23551
196/196 - 33s - loss: 27.7941 - MinusLogProbMetric: 27.7941 - val_loss: 28.4414 - val_MinusLogProbMetric: 28.4414 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 572/1000
2023-10-30 20:10:10.985 
Epoch 572/1000 
	 loss: 27.7878, MinusLogProbMetric: 27.7878, val_loss: 28.3719, val_MinusLogProbMetric: 28.3719

Epoch 572: val_loss did not improve from 28.23551
196/196 - 28s - loss: 27.7878 - MinusLogProbMetric: 27.7878 - val_loss: 28.3719 - val_MinusLogProbMetric: 28.3719 - lr: 5.0000e-04 - 28s/epoch - 142ms/step
Epoch 573/1000
2023-10-30 20:10:39.436 
Epoch 573/1000 
	 loss: 27.8022, MinusLogProbMetric: 27.8022, val_loss: 28.4610, val_MinusLogProbMetric: 28.4610

Epoch 573: val_loss did not improve from 28.23551
196/196 - 28s - loss: 27.8022 - MinusLogProbMetric: 27.8022 - val_loss: 28.4610 - val_MinusLogProbMetric: 28.4610 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 574/1000
2023-10-30 20:11:08.185 
Epoch 574/1000 
	 loss: 27.7933, MinusLogProbMetric: 27.7933, val_loss: 28.2492, val_MinusLogProbMetric: 28.2492

Epoch 574: val_loss did not improve from 28.23551
196/196 - 29s - loss: 27.7933 - MinusLogProbMetric: 27.7933 - val_loss: 28.2492 - val_MinusLogProbMetric: 28.2492 - lr: 5.0000e-04 - 29s/epoch - 147ms/step
Epoch 575/1000
2023-10-30 20:11:37.331 
Epoch 575/1000 
	 loss: 27.8168, MinusLogProbMetric: 27.8168, val_loss: 28.3410, val_MinusLogProbMetric: 28.3410

Epoch 575: val_loss did not improve from 28.23551
196/196 - 29s - loss: 27.8168 - MinusLogProbMetric: 27.8168 - val_loss: 28.3410 - val_MinusLogProbMetric: 28.3410 - lr: 5.0000e-04 - 29s/epoch - 149ms/step
Epoch 576/1000
2023-10-30 20:12:08.569 
Epoch 576/1000 
	 loss: 27.8119, MinusLogProbMetric: 27.8119, val_loss: 28.2838, val_MinusLogProbMetric: 28.2838

Epoch 576: val_loss did not improve from 28.23551
196/196 - 31s - loss: 27.8119 - MinusLogProbMetric: 27.8119 - val_loss: 28.2838 - val_MinusLogProbMetric: 28.2838 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 577/1000
2023-10-30 20:12:40.913 
Epoch 577/1000 
	 loss: 27.8089, MinusLogProbMetric: 27.8089, val_loss: 28.2714, val_MinusLogProbMetric: 28.2714

Epoch 577: val_loss did not improve from 28.23551
196/196 - 32s - loss: 27.8089 - MinusLogProbMetric: 27.8089 - val_loss: 28.2714 - val_MinusLogProbMetric: 28.2714 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 578/1000
2023-10-30 20:13:10.351 
Epoch 578/1000 
	 loss: 27.8109, MinusLogProbMetric: 27.8109, val_loss: 28.3917, val_MinusLogProbMetric: 28.3917

Epoch 578: val_loss did not improve from 28.23551
196/196 - 29s - loss: 27.8109 - MinusLogProbMetric: 27.8109 - val_loss: 28.3917 - val_MinusLogProbMetric: 28.3917 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 579/1000
2023-10-30 20:13:39.306 
Epoch 579/1000 
	 loss: 27.7868, MinusLogProbMetric: 27.7868, val_loss: 28.2456, val_MinusLogProbMetric: 28.2456

Epoch 579: val_loss did not improve from 28.23551
196/196 - 29s - loss: 27.7868 - MinusLogProbMetric: 27.7868 - val_loss: 28.2456 - val_MinusLogProbMetric: 28.2456 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 580/1000
2023-10-30 20:14:07.929 
Epoch 580/1000 
	 loss: 27.8187, MinusLogProbMetric: 27.8187, val_loss: 28.2850, val_MinusLogProbMetric: 28.2850

Epoch 580: val_loss did not improve from 28.23551
196/196 - 29s - loss: 27.8187 - MinusLogProbMetric: 27.8187 - val_loss: 28.2850 - val_MinusLogProbMetric: 28.2850 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 581/1000
2023-10-30 20:14:38.871 
Epoch 581/1000 
	 loss: 27.8139, MinusLogProbMetric: 27.8139, val_loss: 28.1860, val_MinusLogProbMetric: 28.1860

Epoch 581: val_loss improved from 28.23551 to 28.18600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 27.8139 - MinusLogProbMetric: 27.8139 - val_loss: 28.1860 - val_MinusLogProbMetric: 28.1860 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 582/1000
2023-10-30 20:15:10.488 
Epoch 582/1000 
	 loss: 27.8084, MinusLogProbMetric: 27.8084, val_loss: 28.5791, val_MinusLogProbMetric: 28.5791

Epoch 582: val_loss did not improve from 28.18600
196/196 - 31s - loss: 27.8084 - MinusLogProbMetric: 27.8084 - val_loss: 28.5791 - val_MinusLogProbMetric: 28.5791 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 583/1000
2023-10-30 20:15:41.211 
Epoch 583/1000 
	 loss: 27.8049, MinusLogProbMetric: 27.8049, val_loss: 28.3239, val_MinusLogProbMetric: 28.3239

Epoch 583: val_loss did not improve from 28.18600
196/196 - 31s - loss: 27.8049 - MinusLogProbMetric: 27.8049 - val_loss: 28.3239 - val_MinusLogProbMetric: 28.3239 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 584/1000
2023-10-30 20:16:09.294 
Epoch 584/1000 
	 loss: 27.8055, MinusLogProbMetric: 27.8055, val_loss: 28.2414, val_MinusLogProbMetric: 28.2414

Epoch 584: val_loss did not improve from 28.18600
196/196 - 28s - loss: 27.8055 - MinusLogProbMetric: 27.8055 - val_loss: 28.2414 - val_MinusLogProbMetric: 28.2414 - lr: 5.0000e-04 - 28s/epoch - 143ms/step
Epoch 585/1000
2023-10-30 20:16:37.890 
Epoch 585/1000 
	 loss: 27.7907, MinusLogProbMetric: 27.7907, val_loss: 28.2023, val_MinusLogProbMetric: 28.2023

Epoch 585: val_loss did not improve from 28.18600
196/196 - 29s - loss: 27.7907 - MinusLogProbMetric: 27.7907 - val_loss: 28.2023 - val_MinusLogProbMetric: 28.2023 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 586/1000
2023-10-30 20:17:05.924 
Epoch 586/1000 
	 loss: 27.7791, MinusLogProbMetric: 27.7791, val_loss: 28.1904, val_MinusLogProbMetric: 28.1904

Epoch 586: val_loss did not improve from 28.18600
196/196 - 28s - loss: 27.7791 - MinusLogProbMetric: 27.7791 - val_loss: 28.1904 - val_MinusLogProbMetric: 28.1904 - lr: 5.0000e-04 - 28s/epoch - 143ms/step
Epoch 587/1000
2023-10-30 20:17:36.426 
Epoch 587/1000 
	 loss: 27.7744, MinusLogProbMetric: 27.7744, val_loss: 28.2248, val_MinusLogProbMetric: 28.2248

Epoch 587: val_loss did not improve from 28.18600
196/196 - 30s - loss: 27.7744 - MinusLogProbMetric: 27.7744 - val_loss: 28.2248 - val_MinusLogProbMetric: 28.2248 - lr: 5.0000e-04 - 30s/epoch - 156ms/step
Epoch 588/1000
2023-10-30 20:18:06.827 
Epoch 588/1000 
	 loss: 27.8324, MinusLogProbMetric: 27.8324, val_loss: 28.2587, val_MinusLogProbMetric: 28.2587

Epoch 588: val_loss did not improve from 28.18600
196/196 - 30s - loss: 27.8324 - MinusLogProbMetric: 27.8324 - val_loss: 28.2587 - val_MinusLogProbMetric: 28.2587 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 589/1000
2023-10-30 20:18:34.854 
Epoch 589/1000 
	 loss: 27.7764, MinusLogProbMetric: 27.7764, val_loss: 28.2779, val_MinusLogProbMetric: 28.2779

Epoch 589: val_loss did not improve from 28.18600
196/196 - 28s - loss: 27.7764 - MinusLogProbMetric: 27.7764 - val_loss: 28.2779 - val_MinusLogProbMetric: 28.2779 - lr: 5.0000e-04 - 28s/epoch - 143ms/step
Epoch 590/1000
2023-10-30 20:19:03.292 
Epoch 590/1000 
	 loss: 27.7853, MinusLogProbMetric: 27.7853, val_loss: 28.2046, val_MinusLogProbMetric: 28.2046

Epoch 590: val_loss did not improve from 28.18600
196/196 - 28s - loss: 27.7853 - MinusLogProbMetric: 27.7853 - val_loss: 28.2046 - val_MinusLogProbMetric: 28.2046 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 591/1000
2023-10-30 20:19:30.417 
Epoch 591/1000 
	 loss: 27.8313, MinusLogProbMetric: 27.8313, val_loss: 28.3361, val_MinusLogProbMetric: 28.3361

Epoch 591: val_loss did not improve from 28.18600
196/196 - 27s - loss: 27.8313 - MinusLogProbMetric: 27.8313 - val_loss: 28.3361 - val_MinusLogProbMetric: 28.3361 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 592/1000
2023-10-30 20:19:58.913 
Epoch 592/1000 
	 loss: 27.7809, MinusLogProbMetric: 27.7809, val_loss: 28.3308, val_MinusLogProbMetric: 28.3308

Epoch 592: val_loss did not improve from 28.18600
196/196 - 28s - loss: 27.7809 - MinusLogProbMetric: 27.7809 - val_loss: 28.3308 - val_MinusLogProbMetric: 28.3308 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 593/1000
2023-10-30 20:20:31.436 
Epoch 593/1000 
	 loss: 27.7922, MinusLogProbMetric: 27.7922, val_loss: 28.6189, val_MinusLogProbMetric: 28.6189

Epoch 593: val_loss did not improve from 28.18600
196/196 - 33s - loss: 27.7922 - MinusLogProbMetric: 27.7922 - val_loss: 28.6189 - val_MinusLogProbMetric: 28.6189 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 594/1000
2023-10-30 20:21:04.538 
Epoch 594/1000 
	 loss: 27.8250, MinusLogProbMetric: 27.8250, val_loss: 28.2808, val_MinusLogProbMetric: 28.2808

Epoch 594: val_loss did not improve from 28.18600
196/196 - 33s - loss: 27.8250 - MinusLogProbMetric: 27.8250 - val_loss: 28.2808 - val_MinusLogProbMetric: 28.2808 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 595/1000
2023-10-30 20:21:33.629 
Epoch 595/1000 
	 loss: 27.7849, MinusLogProbMetric: 27.7849, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 595: val_loss did not improve from 28.18600
196/196 - 29s - loss: 27.7849 - MinusLogProbMetric: 27.7849 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 596/1000
2023-10-30 20:22:02.454 
Epoch 596/1000 
	 loss: 27.8123, MinusLogProbMetric: 27.8123, val_loss: 28.3477, val_MinusLogProbMetric: 28.3477

Epoch 596: val_loss did not improve from 28.18600
196/196 - 29s - loss: 27.8123 - MinusLogProbMetric: 27.8123 - val_loss: 28.3477 - val_MinusLogProbMetric: 28.3477 - lr: 5.0000e-04 - 29s/epoch - 147ms/step
Epoch 597/1000
2023-10-30 20:22:30.936 
Epoch 597/1000 
	 loss: 27.8054, MinusLogProbMetric: 27.8054, val_loss: 28.2861, val_MinusLogProbMetric: 28.2861

Epoch 597: val_loss did not improve from 28.18600
196/196 - 28s - loss: 27.8054 - MinusLogProbMetric: 27.8054 - val_loss: 28.2861 - val_MinusLogProbMetric: 28.2861 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 598/1000
2023-10-30 20:23:00.896 
Epoch 598/1000 
	 loss: 27.7800, MinusLogProbMetric: 27.7800, val_loss: 28.6118, val_MinusLogProbMetric: 28.6118

Epoch 598: val_loss did not improve from 28.18600
196/196 - 30s - loss: 27.7800 - MinusLogProbMetric: 27.7800 - val_loss: 28.6118 - val_MinusLogProbMetric: 28.6118 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 599/1000
2023-10-30 20:23:34.366 
Epoch 599/1000 
	 loss: 27.7974, MinusLogProbMetric: 27.7974, val_loss: 28.2283, val_MinusLogProbMetric: 28.2283

Epoch 599: val_loss did not improve from 28.18600
196/196 - 33s - loss: 27.7974 - MinusLogProbMetric: 27.7974 - val_loss: 28.2283 - val_MinusLogProbMetric: 28.2283 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 600/1000
2023-10-30 20:24:07.450 
Epoch 600/1000 
	 loss: 27.7793, MinusLogProbMetric: 27.7793, val_loss: 28.2271, val_MinusLogProbMetric: 28.2271

Epoch 600: val_loss did not improve from 28.18600
196/196 - 33s - loss: 27.7793 - MinusLogProbMetric: 27.7793 - val_loss: 28.2271 - val_MinusLogProbMetric: 28.2271 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 601/1000
2023-10-30 20:24:35.916 
Epoch 601/1000 
	 loss: 27.7761, MinusLogProbMetric: 27.7761, val_loss: 28.3153, val_MinusLogProbMetric: 28.3153

Epoch 601: val_loss did not improve from 28.18600
196/196 - 28s - loss: 27.7761 - MinusLogProbMetric: 27.7761 - val_loss: 28.3153 - val_MinusLogProbMetric: 28.3153 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 602/1000
2023-10-30 20:25:04.585 
Epoch 602/1000 
	 loss: 27.7599, MinusLogProbMetric: 27.7599, val_loss: 28.3469, val_MinusLogProbMetric: 28.3469

Epoch 602: val_loss did not improve from 28.18600
196/196 - 29s - loss: 27.7599 - MinusLogProbMetric: 27.7599 - val_loss: 28.3469 - val_MinusLogProbMetric: 28.3469 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 603/1000
2023-10-30 20:25:33.286 
Epoch 603/1000 
	 loss: 27.7793, MinusLogProbMetric: 27.7793, val_loss: 28.4924, val_MinusLogProbMetric: 28.4924

Epoch 603: val_loss did not improve from 28.18600
196/196 - 29s - loss: 27.7793 - MinusLogProbMetric: 27.7793 - val_loss: 28.4924 - val_MinusLogProbMetric: 28.4924 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 604/1000
2023-10-30 20:26:03.285 
Epoch 604/1000 
	 loss: 27.8063, MinusLogProbMetric: 27.8063, val_loss: 28.2919, val_MinusLogProbMetric: 28.2919

Epoch 604: val_loss did not improve from 28.18600
196/196 - 30s - loss: 27.8063 - MinusLogProbMetric: 27.8063 - val_loss: 28.2919 - val_MinusLogProbMetric: 28.2919 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 605/1000
2023-10-30 20:26:36.764 
Epoch 605/1000 
	 loss: 27.7743, MinusLogProbMetric: 27.7743, val_loss: 28.2749, val_MinusLogProbMetric: 28.2749

Epoch 605: val_loss did not improve from 28.18600
196/196 - 33s - loss: 27.7743 - MinusLogProbMetric: 27.7743 - val_loss: 28.2749 - val_MinusLogProbMetric: 28.2749 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 606/1000
2023-10-30 20:27:07.056 
Epoch 606/1000 
	 loss: 27.7830, MinusLogProbMetric: 27.7830, val_loss: 28.4489, val_MinusLogProbMetric: 28.4489

Epoch 606: val_loss did not improve from 28.18600
196/196 - 30s - loss: 27.7830 - MinusLogProbMetric: 27.7830 - val_loss: 28.4489 - val_MinusLogProbMetric: 28.4489 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 607/1000
2023-10-30 20:27:35.978 
Epoch 607/1000 
	 loss: 27.7749, MinusLogProbMetric: 27.7749, val_loss: 28.3814, val_MinusLogProbMetric: 28.3814

Epoch 607: val_loss did not improve from 28.18600
196/196 - 29s - loss: 27.7749 - MinusLogProbMetric: 27.7749 - val_loss: 28.3814 - val_MinusLogProbMetric: 28.3814 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 608/1000
2023-10-30 20:28:05.369 
Epoch 608/1000 
	 loss: 27.7915, MinusLogProbMetric: 27.7915, val_loss: 28.4329, val_MinusLogProbMetric: 28.4329

Epoch 608: val_loss did not improve from 28.18600
196/196 - 29s - loss: 27.7915 - MinusLogProbMetric: 27.7915 - val_loss: 28.4329 - val_MinusLogProbMetric: 28.4329 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 609/1000
2023-10-30 20:28:34.080 
Epoch 609/1000 
	 loss: 27.7982, MinusLogProbMetric: 27.7982, val_loss: 28.3134, val_MinusLogProbMetric: 28.3134

Epoch 609: val_loss did not improve from 28.18600
196/196 - 29s - loss: 27.7982 - MinusLogProbMetric: 27.7982 - val_loss: 28.3134 - val_MinusLogProbMetric: 28.3134 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 610/1000
2023-10-30 20:29:06.937 
Epoch 610/1000 
	 loss: 27.7646, MinusLogProbMetric: 27.7646, val_loss: 28.3092, val_MinusLogProbMetric: 28.3092

Epoch 610: val_loss did not improve from 28.18600
196/196 - 33s - loss: 27.7646 - MinusLogProbMetric: 27.7646 - val_loss: 28.3092 - val_MinusLogProbMetric: 28.3092 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 611/1000
2023-10-30 20:29:39.304 
Epoch 611/1000 
	 loss: 27.8001, MinusLogProbMetric: 27.8001, val_loss: 28.1678, val_MinusLogProbMetric: 28.1678

Epoch 611: val_loss improved from 28.18600 to 28.16776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 27.8001 - MinusLogProbMetric: 27.8001 - val_loss: 28.1678 - val_MinusLogProbMetric: 28.1678 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 612/1000
2023-10-30 20:30:08.088 
Epoch 612/1000 
	 loss: 27.7873, MinusLogProbMetric: 27.7873, val_loss: 28.1674, val_MinusLogProbMetric: 28.1674

Epoch 612: val_loss improved from 28.16776 to 28.16745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 27.7873 - MinusLogProbMetric: 27.7873 - val_loss: 28.1674 - val_MinusLogProbMetric: 28.1674 - lr: 5.0000e-04 - 29s/epoch - 147ms/step
Epoch 613/1000
2023-10-30 20:30:39.416 
Epoch 613/1000 
	 loss: 27.8031, MinusLogProbMetric: 27.8031, val_loss: 28.3245, val_MinusLogProbMetric: 28.3245

Epoch 613: val_loss did not improve from 28.16745
196/196 - 31s - loss: 27.8031 - MinusLogProbMetric: 27.8031 - val_loss: 28.3245 - val_MinusLogProbMetric: 28.3245 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 614/1000
2023-10-30 20:31:11.432 
Epoch 614/1000 
	 loss: 27.8128, MinusLogProbMetric: 27.8128, val_loss: 28.2100, val_MinusLogProbMetric: 28.2100

Epoch 614: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.8128 - MinusLogProbMetric: 27.8128 - val_loss: 28.2100 - val_MinusLogProbMetric: 28.2100 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 615/1000
2023-10-30 20:31:40.493 
Epoch 615/1000 
	 loss: 27.7643, MinusLogProbMetric: 27.7643, val_loss: 28.2651, val_MinusLogProbMetric: 28.2651

Epoch 615: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7643 - MinusLogProbMetric: 27.7643 - val_loss: 28.2651 - val_MinusLogProbMetric: 28.2651 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 616/1000
2023-10-30 20:32:10.347 
Epoch 616/1000 
	 loss: 27.7643, MinusLogProbMetric: 27.7643, val_loss: 28.4898, val_MinusLogProbMetric: 28.4898

Epoch 616: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7643 - MinusLogProbMetric: 27.7643 - val_loss: 28.4898 - val_MinusLogProbMetric: 28.4898 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 617/1000
2023-10-30 20:32:39.066 
Epoch 617/1000 
	 loss: 27.7702, MinusLogProbMetric: 27.7702, val_loss: 28.3542, val_MinusLogProbMetric: 28.3542

Epoch 617: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7702 - MinusLogProbMetric: 27.7702 - val_loss: 28.3542 - val_MinusLogProbMetric: 28.3542 - lr: 5.0000e-04 - 29s/epoch - 147ms/step
Epoch 618/1000
2023-10-30 20:33:12.337 
Epoch 618/1000 
	 loss: 27.8020, MinusLogProbMetric: 27.8020, val_loss: 28.2971, val_MinusLogProbMetric: 28.2971

Epoch 618: val_loss did not improve from 28.16745
196/196 - 33s - loss: 27.8020 - MinusLogProbMetric: 27.8020 - val_loss: 28.2971 - val_MinusLogProbMetric: 28.2971 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 619/1000
2023-10-30 20:33:41.386 
Epoch 619/1000 
	 loss: 27.7692, MinusLogProbMetric: 27.7692, val_loss: 28.3516, val_MinusLogProbMetric: 28.3516

Epoch 619: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7692 - MinusLogProbMetric: 27.7692 - val_loss: 28.3516 - val_MinusLogProbMetric: 28.3516 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 620/1000
2023-10-30 20:34:13.666 
Epoch 620/1000 
	 loss: 27.7799, MinusLogProbMetric: 27.7799, val_loss: 28.2237, val_MinusLogProbMetric: 28.2237

Epoch 620: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.7799 - MinusLogProbMetric: 27.7799 - val_loss: 28.2237 - val_MinusLogProbMetric: 28.2237 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 621/1000
2023-10-30 20:34:42.540 
Epoch 621/1000 
	 loss: 27.7689, MinusLogProbMetric: 27.7689, val_loss: 28.2661, val_MinusLogProbMetric: 28.2661

Epoch 621: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7689 - MinusLogProbMetric: 27.7689 - val_loss: 28.2661 - val_MinusLogProbMetric: 28.2661 - lr: 5.0000e-04 - 29s/epoch - 147ms/step
Epoch 622/1000
2023-10-30 20:35:16.422 
Epoch 622/1000 
	 loss: 27.8206, MinusLogProbMetric: 27.8206, val_loss: 28.4025, val_MinusLogProbMetric: 28.4025

Epoch 622: val_loss did not improve from 28.16745
196/196 - 34s - loss: 27.8206 - MinusLogProbMetric: 27.8206 - val_loss: 28.4025 - val_MinusLogProbMetric: 28.4025 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 623/1000
2023-10-30 20:35:45.201 
Epoch 623/1000 
	 loss: 27.7737, MinusLogProbMetric: 27.7737, val_loss: 28.2355, val_MinusLogProbMetric: 28.2355

Epoch 623: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7737 - MinusLogProbMetric: 27.7737 - val_loss: 28.2355 - val_MinusLogProbMetric: 28.2355 - lr: 5.0000e-04 - 29s/epoch - 147ms/step
Epoch 624/1000
2023-10-30 20:36:19.205 
Epoch 624/1000 
	 loss: 27.8006, MinusLogProbMetric: 27.8006, val_loss: 28.4550, val_MinusLogProbMetric: 28.4550

Epoch 624: val_loss did not improve from 28.16745
196/196 - 34s - loss: 27.8006 - MinusLogProbMetric: 27.8006 - val_loss: 28.4550 - val_MinusLogProbMetric: 28.4550 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 625/1000
2023-10-30 20:36:47.752 
Epoch 625/1000 
	 loss: 27.7984, MinusLogProbMetric: 27.7984, val_loss: 28.5066, val_MinusLogProbMetric: 28.5066

Epoch 625: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7984 - MinusLogProbMetric: 27.7984 - val_loss: 28.5066 - val_MinusLogProbMetric: 28.5066 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 626/1000
2023-10-30 20:37:18.665 
Epoch 626/1000 
	 loss: 27.7723, MinusLogProbMetric: 27.7723, val_loss: 28.4514, val_MinusLogProbMetric: 28.4514

Epoch 626: val_loss did not improve from 28.16745
196/196 - 31s - loss: 27.7723 - MinusLogProbMetric: 27.7723 - val_loss: 28.4514 - val_MinusLogProbMetric: 28.4514 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 627/1000
2023-10-30 20:37:48.091 
Epoch 627/1000 
	 loss: 27.7816, MinusLogProbMetric: 27.7816, val_loss: 28.2489, val_MinusLogProbMetric: 28.2489

Epoch 627: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7816 - MinusLogProbMetric: 27.7816 - val_loss: 28.2489 - val_MinusLogProbMetric: 28.2489 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 628/1000
2023-10-30 20:38:18.577 
Epoch 628/1000 
	 loss: 27.7923, MinusLogProbMetric: 27.7923, val_loss: 28.2255, val_MinusLogProbMetric: 28.2255

Epoch 628: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7923 - MinusLogProbMetric: 27.7923 - val_loss: 28.2255 - val_MinusLogProbMetric: 28.2255 - lr: 5.0000e-04 - 30s/epoch - 156ms/step
Epoch 629/1000
2023-10-30 20:38:47.937 
Epoch 629/1000 
	 loss: 27.7595, MinusLogProbMetric: 27.7595, val_loss: 28.3051, val_MinusLogProbMetric: 28.3051

Epoch 629: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7595 - MinusLogProbMetric: 27.7595 - val_loss: 28.3051 - val_MinusLogProbMetric: 28.3051 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 630/1000
2023-10-30 20:39:18.003 
Epoch 630/1000 
	 loss: 27.7930, MinusLogProbMetric: 27.7930, val_loss: 28.4078, val_MinusLogProbMetric: 28.4078

Epoch 630: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7930 - MinusLogProbMetric: 27.7930 - val_loss: 28.4078 - val_MinusLogProbMetric: 28.4078 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 631/1000
2023-10-30 20:39:46.446 
Epoch 631/1000 
	 loss: 27.7722, MinusLogProbMetric: 27.7722, val_loss: 28.4419, val_MinusLogProbMetric: 28.4419

Epoch 631: val_loss did not improve from 28.16745
196/196 - 28s - loss: 27.7722 - MinusLogProbMetric: 27.7722 - val_loss: 28.4419 - val_MinusLogProbMetric: 28.4419 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 632/1000
2023-10-30 20:40:16.821 
Epoch 632/1000 
	 loss: 27.7815, MinusLogProbMetric: 27.7815, val_loss: 28.1963, val_MinusLogProbMetric: 28.1963

Epoch 632: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7815 - MinusLogProbMetric: 27.7815 - val_loss: 28.1963 - val_MinusLogProbMetric: 28.1963 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 633/1000
2023-10-30 20:40:48.561 
Epoch 633/1000 
	 loss: 27.7786, MinusLogProbMetric: 27.7786, val_loss: 28.4525, val_MinusLogProbMetric: 28.4525

Epoch 633: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.7786 - MinusLogProbMetric: 27.7786 - val_loss: 28.4525 - val_MinusLogProbMetric: 28.4525 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 634/1000
2023-10-30 20:41:17.937 
Epoch 634/1000 
	 loss: 27.7765, MinusLogProbMetric: 27.7765, val_loss: 28.2627, val_MinusLogProbMetric: 28.2627

Epoch 634: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7765 - MinusLogProbMetric: 27.7765 - val_loss: 28.2627 - val_MinusLogProbMetric: 28.2627 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 635/1000
2023-10-30 20:41:49.153 
Epoch 635/1000 
	 loss: 27.7951, MinusLogProbMetric: 27.7951, val_loss: 28.4534, val_MinusLogProbMetric: 28.4534

Epoch 635: val_loss did not improve from 28.16745
196/196 - 31s - loss: 27.7951 - MinusLogProbMetric: 27.7951 - val_loss: 28.4534 - val_MinusLogProbMetric: 28.4534 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 636/1000
2023-10-30 20:42:17.615 
Epoch 636/1000 
	 loss: 27.8050, MinusLogProbMetric: 27.8050, val_loss: 28.2950, val_MinusLogProbMetric: 28.2950

Epoch 636: val_loss did not improve from 28.16745
196/196 - 28s - loss: 27.8050 - MinusLogProbMetric: 27.8050 - val_loss: 28.2950 - val_MinusLogProbMetric: 28.2950 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 637/1000
2023-10-30 20:42:50.060 
Epoch 637/1000 
	 loss: 27.7739, MinusLogProbMetric: 27.7739, val_loss: 28.4604, val_MinusLogProbMetric: 28.4604

Epoch 637: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.7739 - MinusLogProbMetric: 27.7739 - val_loss: 28.4604 - val_MinusLogProbMetric: 28.4604 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 638/1000
2023-10-30 20:43:19.360 
Epoch 638/1000 
	 loss: 27.7760, MinusLogProbMetric: 27.7760, val_loss: 28.2448, val_MinusLogProbMetric: 28.2448

Epoch 638: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7760 - MinusLogProbMetric: 27.7760 - val_loss: 28.2448 - val_MinusLogProbMetric: 28.2448 - lr: 5.0000e-04 - 29s/epoch - 149ms/step
Epoch 639/1000
2023-10-30 20:43:49.150 
Epoch 639/1000 
	 loss: 27.7583, MinusLogProbMetric: 27.7583, val_loss: 28.2696, val_MinusLogProbMetric: 28.2696

Epoch 639: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7583 - MinusLogProbMetric: 27.7583 - val_loss: 28.2696 - val_MinusLogProbMetric: 28.2696 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 640/1000
2023-10-30 20:44:21.754 
Epoch 640/1000 
	 loss: 27.7754, MinusLogProbMetric: 27.7754, val_loss: 28.2817, val_MinusLogProbMetric: 28.2817

Epoch 640: val_loss did not improve from 28.16745
196/196 - 33s - loss: 27.7754 - MinusLogProbMetric: 27.7754 - val_loss: 28.2817 - val_MinusLogProbMetric: 28.2817 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 641/1000
2023-10-30 20:44:50.104 
Epoch 641/1000 
	 loss: 27.8163, MinusLogProbMetric: 27.8163, val_loss: 28.2642, val_MinusLogProbMetric: 28.2642

Epoch 641: val_loss did not improve from 28.16745
196/196 - 28s - loss: 27.8163 - MinusLogProbMetric: 27.8163 - val_loss: 28.2642 - val_MinusLogProbMetric: 28.2642 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 642/1000
2023-10-30 20:45:19.273 
Epoch 642/1000 
	 loss: 27.7365, MinusLogProbMetric: 27.7365, val_loss: 28.2838, val_MinusLogProbMetric: 28.2838

Epoch 642: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7365 - MinusLogProbMetric: 27.7365 - val_loss: 28.2838 - val_MinusLogProbMetric: 28.2838 - lr: 5.0000e-04 - 29s/epoch - 149ms/step
Epoch 643/1000
2023-10-30 20:45:49.916 
Epoch 643/1000 
	 loss: 27.7828, MinusLogProbMetric: 27.7828, val_loss: 28.2054, val_MinusLogProbMetric: 28.2054

Epoch 643: val_loss did not improve from 28.16745
196/196 - 31s - loss: 27.7828 - MinusLogProbMetric: 27.7828 - val_loss: 28.2054 - val_MinusLogProbMetric: 28.2054 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 644/1000
2023-10-30 20:46:18.845 
Epoch 644/1000 
	 loss: 27.7817, MinusLogProbMetric: 27.7817, val_loss: 28.4491, val_MinusLogProbMetric: 28.4491

Epoch 644: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7817 - MinusLogProbMetric: 27.7817 - val_loss: 28.4491 - val_MinusLogProbMetric: 28.4491 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 645/1000
2023-10-30 20:46:48.558 
Epoch 645/1000 
	 loss: 27.7707, MinusLogProbMetric: 27.7707, val_loss: 28.2870, val_MinusLogProbMetric: 28.2870

Epoch 645: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7707 - MinusLogProbMetric: 27.7707 - val_loss: 28.2870 - val_MinusLogProbMetric: 28.2870 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 646/1000
2023-10-30 20:47:18.976 
Epoch 646/1000 
	 loss: 27.7958, MinusLogProbMetric: 27.7958, val_loss: 28.4702, val_MinusLogProbMetric: 28.4702

Epoch 646: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7958 - MinusLogProbMetric: 27.7958 - val_loss: 28.4702 - val_MinusLogProbMetric: 28.4702 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 647/1000
2023-10-30 20:47:49.002 
Epoch 647/1000 
	 loss: 27.7843, MinusLogProbMetric: 27.7843, val_loss: 28.2444, val_MinusLogProbMetric: 28.2444

Epoch 647: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7843 - MinusLogProbMetric: 27.7843 - val_loss: 28.2444 - val_MinusLogProbMetric: 28.2444 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 648/1000
2023-10-30 20:48:21.198 
Epoch 648/1000 
	 loss: 27.7829, MinusLogProbMetric: 27.7829, val_loss: 28.3715, val_MinusLogProbMetric: 28.3715

Epoch 648: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.7829 - MinusLogProbMetric: 27.7829 - val_loss: 28.3715 - val_MinusLogProbMetric: 28.3715 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 649/1000
2023-10-30 20:48:49.560 
Epoch 649/1000 
	 loss: 27.7734, MinusLogProbMetric: 27.7734, val_loss: 28.3585, val_MinusLogProbMetric: 28.3585

Epoch 649: val_loss did not improve from 28.16745
196/196 - 28s - loss: 27.7734 - MinusLogProbMetric: 27.7734 - val_loss: 28.3585 - val_MinusLogProbMetric: 28.3585 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 650/1000
2023-10-30 20:49:19.084 
Epoch 650/1000 
	 loss: 27.7638, MinusLogProbMetric: 27.7638, val_loss: 28.3197, val_MinusLogProbMetric: 28.3197

Epoch 650: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7638 - MinusLogProbMetric: 27.7638 - val_loss: 28.3197 - val_MinusLogProbMetric: 28.3197 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 651/1000
2023-10-30 20:49:50.844 
Epoch 651/1000 
	 loss: 27.7806, MinusLogProbMetric: 27.7806, val_loss: 28.2605, val_MinusLogProbMetric: 28.2605

Epoch 651: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.7806 - MinusLogProbMetric: 27.7806 - val_loss: 28.2605 - val_MinusLogProbMetric: 28.2605 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 652/1000
2023-10-30 20:50:19.154 
Epoch 652/1000 
	 loss: 27.7550, MinusLogProbMetric: 27.7550, val_loss: 28.2312, val_MinusLogProbMetric: 28.2312

Epoch 652: val_loss did not improve from 28.16745
196/196 - 28s - loss: 27.7550 - MinusLogProbMetric: 27.7550 - val_loss: 28.2312 - val_MinusLogProbMetric: 28.2312 - lr: 5.0000e-04 - 28s/epoch - 144ms/step
Epoch 653/1000
2023-10-30 20:50:50.965 
Epoch 653/1000 
	 loss: 27.8014, MinusLogProbMetric: 27.8014, val_loss: 28.2955, val_MinusLogProbMetric: 28.2955

Epoch 653: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.8014 - MinusLogProbMetric: 27.8014 - val_loss: 28.2955 - val_MinusLogProbMetric: 28.2955 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 654/1000
2023-10-30 20:51:22.541 
Epoch 654/1000 
	 loss: 27.7759, MinusLogProbMetric: 27.7759, val_loss: 28.2789, val_MinusLogProbMetric: 28.2789

Epoch 654: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.7759 - MinusLogProbMetric: 27.7759 - val_loss: 28.2789 - val_MinusLogProbMetric: 28.2789 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 655/1000
2023-10-30 20:51:52.020 
Epoch 655/1000 
	 loss: 27.7751, MinusLogProbMetric: 27.7751, val_loss: 28.3717, val_MinusLogProbMetric: 28.3717

Epoch 655: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7751 - MinusLogProbMetric: 27.7751 - val_loss: 28.3717 - val_MinusLogProbMetric: 28.3717 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 656/1000
2023-10-30 20:52:25.454 
Epoch 656/1000 
	 loss: 27.7722, MinusLogProbMetric: 27.7722, val_loss: 28.1944, val_MinusLogProbMetric: 28.1944

Epoch 656: val_loss did not improve from 28.16745
196/196 - 33s - loss: 27.7722 - MinusLogProbMetric: 27.7722 - val_loss: 28.1944 - val_MinusLogProbMetric: 28.1944 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 657/1000
2023-10-30 20:52:54.443 
Epoch 657/1000 
	 loss: 27.7931, MinusLogProbMetric: 27.7931, val_loss: 28.2246, val_MinusLogProbMetric: 28.2246

Epoch 657: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7931 - MinusLogProbMetric: 27.7931 - val_loss: 28.2246 - val_MinusLogProbMetric: 28.2246 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 658/1000
2023-10-30 20:53:23.789 
Epoch 658/1000 
	 loss: 27.7970, MinusLogProbMetric: 27.7970, val_loss: 28.2184, val_MinusLogProbMetric: 28.2184

Epoch 658: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7970 - MinusLogProbMetric: 27.7970 - val_loss: 28.2184 - val_MinusLogProbMetric: 28.2184 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 659/1000
2023-10-30 20:53:55.768 
Epoch 659/1000 
	 loss: 27.7725, MinusLogProbMetric: 27.7725, val_loss: 28.5384, val_MinusLogProbMetric: 28.5384

Epoch 659: val_loss did not improve from 28.16745
196/196 - 32s - loss: 27.7725 - MinusLogProbMetric: 27.7725 - val_loss: 28.5384 - val_MinusLogProbMetric: 28.5384 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 660/1000
2023-10-30 20:54:25.263 
Epoch 660/1000 
	 loss: 27.7805, MinusLogProbMetric: 27.7805, val_loss: 28.2525, val_MinusLogProbMetric: 28.2525

Epoch 660: val_loss did not improve from 28.16745
196/196 - 29s - loss: 27.7805 - MinusLogProbMetric: 27.7805 - val_loss: 28.2525 - val_MinusLogProbMetric: 28.2525 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 661/1000
2023-10-30 20:54:55.308 
Epoch 661/1000 
	 loss: 27.7515, MinusLogProbMetric: 27.7515, val_loss: 28.4910, val_MinusLogProbMetric: 28.4910

Epoch 661: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7515 - MinusLogProbMetric: 27.7515 - val_loss: 28.4910 - val_MinusLogProbMetric: 28.4910 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 662/1000
2023-10-30 20:55:25.568 
Epoch 662/1000 
	 loss: 27.7813, MinusLogProbMetric: 27.7813, val_loss: 28.2130, val_MinusLogProbMetric: 28.2130

Epoch 662: val_loss did not improve from 28.16745
196/196 - 30s - loss: 27.7813 - MinusLogProbMetric: 27.7813 - val_loss: 28.2130 - val_MinusLogProbMetric: 28.2130 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 663/1000
2023-10-30 20:55:54.069 
Epoch 663/1000 
	 loss: 27.6265, MinusLogProbMetric: 27.6265, val_loss: 28.1534, val_MinusLogProbMetric: 28.1534

Epoch 663: val_loss improved from 28.16745 to 28.15339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 27.6265 - MinusLogProbMetric: 27.6265 - val_loss: 28.1534 - val_MinusLogProbMetric: 28.1534 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 664/1000
2023-10-30 20:56:26.092 
Epoch 664/1000 
	 loss: 27.6092, MinusLogProbMetric: 27.6092, val_loss: 28.1391, val_MinusLogProbMetric: 28.1391

Epoch 664: val_loss improved from 28.15339 to 28.13914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 27.6092 - MinusLogProbMetric: 27.6092 - val_loss: 28.1391 - val_MinusLogProbMetric: 28.1391 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 665/1000
2023-10-30 20:56:57.373 
Epoch 665/1000 
	 loss: 27.6128, MinusLogProbMetric: 27.6128, val_loss: 28.1003, val_MinusLogProbMetric: 28.1003

Epoch 665: val_loss improved from 28.13914 to 28.10025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 27.6128 - MinusLogProbMetric: 27.6128 - val_loss: 28.1003 - val_MinusLogProbMetric: 28.1003 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 666/1000
2023-10-30 20:57:27.238 
Epoch 666/1000 
	 loss: 27.6114, MinusLogProbMetric: 27.6114, val_loss: 28.1279, val_MinusLogProbMetric: 28.1279

Epoch 666: val_loss did not improve from 28.10025
196/196 - 29s - loss: 27.6114 - MinusLogProbMetric: 27.6114 - val_loss: 28.1279 - val_MinusLogProbMetric: 28.1279 - lr: 2.5000e-04 - 29s/epoch - 150ms/step
Epoch 667/1000
2023-10-30 20:57:57.059 
Epoch 667/1000 
	 loss: 27.6098, MinusLogProbMetric: 27.6098, val_loss: 28.1193, val_MinusLogProbMetric: 28.1193

Epoch 667: val_loss did not improve from 28.10025
196/196 - 30s - loss: 27.6098 - MinusLogProbMetric: 27.6098 - val_loss: 28.1193 - val_MinusLogProbMetric: 28.1193 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 668/1000
2023-10-30 20:58:28.138 
Epoch 668/1000 
	 loss: 27.6082, MinusLogProbMetric: 27.6082, val_loss: 28.1096, val_MinusLogProbMetric: 28.1096

Epoch 668: val_loss did not improve from 28.10025
196/196 - 31s - loss: 27.6082 - MinusLogProbMetric: 27.6082 - val_loss: 28.1096 - val_MinusLogProbMetric: 28.1096 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 669/1000
2023-10-30 20:59:01.981 
Epoch 669/1000 
	 loss: 27.6044, MinusLogProbMetric: 27.6044, val_loss: 28.0877, val_MinusLogProbMetric: 28.0877

Epoch 669: val_loss improved from 28.10025 to 28.08770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 27.6044 - MinusLogProbMetric: 27.6044 - val_loss: 28.0877 - val_MinusLogProbMetric: 28.0877 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 670/1000
2023-10-30 20:59:32.755 
Epoch 670/1000 
	 loss: 27.5991, MinusLogProbMetric: 27.5991, val_loss: 28.1210, val_MinusLogProbMetric: 28.1210

Epoch 670: val_loss did not improve from 28.08770
196/196 - 30s - loss: 27.5991 - MinusLogProbMetric: 27.5991 - val_loss: 28.1210 - val_MinusLogProbMetric: 28.1210 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 671/1000
2023-10-30 21:00:01.408 
Epoch 671/1000 
	 loss: 27.6075, MinusLogProbMetric: 27.6075, val_loss: 28.1620, val_MinusLogProbMetric: 28.1620

Epoch 671: val_loss did not improve from 28.08770
196/196 - 29s - loss: 27.6075 - MinusLogProbMetric: 27.6075 - val_loss: 28.1620 - val_MinusLogProbMetric: 28.1620 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 672/1000
2023-10-30 21:00:29.876 
Epoch 672/1000 
	 loss: 27.5982, MinusLogProbMetric: 27.5982, val_loss: 28.1497, val_MinusLogProbMetric: 28.1497

Epoch 672: val_loss did not improve from 28.08770
196/196 - 28s - loss: 27.5982 - MinusLogProbMetric: 27.5982 - val_loss: 28.1497 - val_MinusLogProbMetric: 28.1497 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 673/1000
2023-10-30 21:00:59.485 
Epoch 673/1000 
	 loss: 27.6030, MinusLogProbMetric: 27.6030, val_loss: 28.0875, val_MinusLogProbMetric: 28.0875

Epoch 673: val_loss improved from 28.08770 to 28.08749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 27.6030 - MinusLogProbMetric: 27.6030 - val_loss: 28.0875 - val_MinusLogProbMetric: 28.0875 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 674/1000
2023-10-30 21:01:31.696 
Epoch 674/1000 
	 loss: 27.5999, MinusLogProbMetric: 27.5999, val_loss: 28.0726, val_MinusLogProbMetric: 28.0726

Epoch 674: val_loss improved from 28.08749 to 28.07263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 27.5999 - MinusLogProbMetric: 27.5999 - val_loss: 28.0726 - val_MinusLogProbMetric: 28.0726 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 675/1000
2023-10-30 21:02:01.301 
Epoch 675/1000 
	 loss: 27.5967, MinusLogProbMetric: 27.5967, val_loss: 28.1323, val_MinusLogProbMetric: 28.1323

Epoch 675: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.5967 - MinusLogProbMetric: 27.5967 - val_loss: 28.1323 - val_MinusLogProbMetric: 28.1323 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 676/1000
2023-10-30 21:02:29.988 
Epoch 676/1000 
	 loss: 27.6062, MinusLogProbMetric: 27.6062, val_loss: 28.0844, val_MinusLogProbMetric: 28.0844

Epoch 676: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.6062 - MinusLogProbMetric: 27.6062 - val_loss: 28.0844 - val_MinusLogProbMetric: 28.0844 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 677/1000
2023-10-30 21:02:58.381 
Epoch 677/1000 
	 loss: 27.6017, MinusLogProbMetric: 27.6017, val_loss: 28.1991, val_MinusLogProbMetric: 28.1991

Epoch 677: val_loss did not improve from 28.07263
196/196 - 28s - loss: 27.6017 - MinusLogProbMetric: 27.6017 - val_loss: 28.1991 - val_MinusLogProbMetric: 28.1991 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 678/1000
2023-10-30 21:03:29.509 
Epoch 678/1000 
	 loss: 27.6092, MinusLogProbMetric: 27.6092, val_loss: 28.1526, val_MinusLogProbMetric: 28.1526

Epoch 678: val_loss did not improve from 28.07263
196/196 - 31s - loss: 27.6092 - MinusLogProbMetric: 27.6092 - val_loss: 28.1526 - val_MinusLogProbMetric: 28.1526 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 679/1000
2023-10-30 21:03:58.413 
Epoch 679/1000 
	 loss: 27.5989, MinusLogProbMetric: 27.5989, val_loss: 28.2827, val_MinusLogProbMetric: 28.2827

Epoch 679: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.5989 - MinusLogProbMetric: 27.5989 - val_loss: 28.2827 - val_MinusLogProbMetric: 28.2827 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 680/1000
2023-10-30 21:04:27.124 
Epoch 680/1000 
	 loss: 27.6023, MinusLogProbMetric: 27.6023, val_loss: 28.2227, val_MinusLogProbMetric: 28.2227

Epoch 680: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.6023 - MinusLogProbMetric: 27.6023 - val_loss: 28.2227 - val_MinusLogProbMetric: 28.2227 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 681/1000
2023-10-30 21:04:55.437 
Epoch 681/1000 
	 loss: 27.6001, MinusLogProbMetric: 27.6001, val_loss: 28.1052, val_MinusLogProbMetric: 28.1052

Epoch 681: val_loss did not improve from 28.07263
196/196 - 28s - loss: 27.6001 - MinusLogProbMetric: 27.6001 - val_loss: 28.1052 - val_MinusLogProbMetric: 28.1052 - lr: 2.5000e-04 - 28s/epoch - 144ms/step
Epoch 682/1000
2023-10-30 21:05:24.192 
Epoch 682/1000 
	 loss: 27.6005, MinusLogProbMetric: 27.6005, val_loss: 28.1256, val_MinusLogProbMetric: 28.1256

Epoch 682: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.6005 - MinusLogProbMetric: 27.6005 - val_loss: 28.1256 - val_MinusLogProbMetric: 28.1256 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 683/1000
2023-10-30 21:05:58.123 
Epoch 683/1000 
	 loss: 27.6061, MinusLogProbMetric: 27.6061, val_loss: 28.1091, val_MinusLogProbMetric: 28.1091

Epoch 683: val_loss did not improve from 28.07263
196/196 - 34s - loss: 27.6061 - MinusLogProbMetric: 27.6061 - val_loss: 28.1091 - val_MinusLogProbMetric: 28.1091 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 684/1000
2023-10-30 21:06:28.835 
Epoch 684/1000 
	 loss: 27.5939, MinusLogProbMetric: 27.5939, val_loss: 28.1253, val_MinusLogProbMetric: 28.1253

Epoch 684: val_loss did not improve from 28.07263
196/196 - 31s - loss: 27.5939 - MinusLogProbMetric: 27.5939 - val_loss: 28.1253 - val_MinusLogProbMetric: 28.1253 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 685/1000
2023-10-30 21:06:57.617 
Epoch 685/1000 
	 loss: 27.5989, MinusLogProbMetric: 27.5989, val_loss: 28.1161, val_MinusLogProbMetric: 28.1161

Epoch 685: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.5989 - MinusLogProbMetric: 27.5989 - val_loss: 28.1161 - val_MinusLogProbMetric: 28.1161 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 686/1000
2023-10-30 21:07:26.629 
Epoch 686/1000 
	 loss: 27.6114, MinusLogProbMetric: 27.6114, val_loss: 28.1065, val_MinusLogProbMetric: 28.1065

Epoch 686: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.6114 - MinusLogProbMetric: 27.6114 - val_loss: 28.1065 - val_MinusLogProbMetric: 28.1065 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 687/1000
2023-10-30 21:07:57.948 
Epoch 687/1000 
	 loss: 27.6052, MinusLogProbMetric: 27.6052, val_loss: 28.0799, val_MinusLogProbMetric: 28.0799

Epoch 687: val_loss did not improve from 28.07263
196/196 - 31s - loss: 27.6052 - MinusLogProbMetric: 27.6052 - val_loss: 28.0799 - val_MinusLogProbMetric: 28.0799 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 688/1000
2023-10-30 21:08:29.163 
Epoch 688/1000 
	 loss: 27.5897, MinusLogProbMetric: 27.5897, val_loss: 28.1146, val_MinusLogProbMetric: 28.1146

Epoch 688: val_loss did not improve from 28.07263
196/196 - 31s - loss: 27.5897 - MinusLogProbMetric: 27.5897 - val_loss: 28.1146 - val_MinusLogProbMetric: 28.1146 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 689/1000
2023-10-30 21:08:57.599 
Epoch 689/1000 
	 loss: 27.6087, MinusLogProbMetric: 27.6087, val_loss: 28.1738, val_MinusLogProbMetric: 28.1738

Epoch 689: val_loss did not improve from 28.07263
196/196 - 28s - loss: 27.6087 - MinusLogProbMetric: 27.6087 - val_loss: 28.1738 - val_MinusLogProbMetric: 28.1738 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 690/1000
2023-10-30 21:09:26.059 
Epoch 690/1000 
	 loss: 27.5969, MinusLogProbMetric: 27.5969, val_loss: 28.0824, val_MinusLogProbMetric: 28.0824

Epoch 690: val_loss did not improve from 28.07263
196/196 - 28s - loss: 27.5969 - MinusLogProbMetric: 27.5969 - val_loss: 28.0824 - val_MinusLogProbMetric: 28.0824 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 691/1000
2023-10-30 21:09:54.414 
Epoch 691/1000 
	 loss: 27.5949, MinusLogProbMetric: 27.5949, val_loss: 28.2111, val_MinusLogProbMetric: 28.2111

Epoch 691: val_loss did not improve from 28.07263
196/196 - 28s - loss: 27.5949 - MinusLogProbMetric: 27.5949 - val_loss: 28.2111 - val_MinusLogProbMetric: 28.2111 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 692/1000
2023-10-30 21:10:24.782 
Epoch 692/1000 
	 loss: 27.6216, MinusLogProbMetric: 27.6216, val_loss: 28.1523, val_MinusLogProbMetric: 28.1523

Epoch 692: val_loss did not improve from 28.07263
196/196 - 30s - loss: 27.6216 - MinusLogProbMetric: 27.6216 - val_loss: 28.1523 - val_MinusLogProbMetric: 28.1523 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 693/1000
2023-10-30 21:10:57.270 
Epoch 693/1000 
	 loss: 27.5979, MinusLogProbMetric: 27.5979, val_loss: 28.1341, val_MinusLogProbMetric: 28.1341

Epoch 693: val_loss did not improve from 28.07263
196/196 - 32s - loss: 27.5979 - MinusLogProbMetric: 27.5979 - val_loss: 28.1341 - val_MinusLogProbMetric: 28.1341 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 694/1000
2023-10-30 21:11:26.517 
Epoch 694/1000 
	 loss: 27.6137, MinusLogProbMetric: 27.6137, val_loss: 28.1304, val_MinusLogProbMetric: 28.1304

Epoch 694: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.6137 - MinusLogProbMetric: 27.6137 - val_loss: 28.1304 - val_MinusLogProbMetric: 28.1304 - lr: 2.5000e-04 - 29s/epoch - 149ms/step
Epoch 695/1000
2023-10-30 21:11:55.148 
Epoch 695/1000 
	 loss: 27.6063, MinusLogProbMetric: 27.6063, val_loss: 28.1864, val_MinusLogProbMetric: 28.1864

Epoch 695: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.6063 - MinusLogProbMetric: 27.6063 - val_loss: 28.1864 - val_MinusLogProbMetric: 28.1864 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 696/1000
2023-10-30 21:12:23.882 
Epoch 696/1000 
	 loss: 27.5952, MinusLogProbMetric: 27.5952, val_loss: 28.0852, val_MinusLogProbMetric: 28.0852

Epoch 696: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.5952 - MinusLogProbMetric: 27.5952 - val_loss: 28.0852 - val_MinusLogProbMetric: 28.0852 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 697/1000
2023-10-30 21:12:56.520 
Epoch 697/1000 
	 loss: 27.6119, MinusLogProbMetric: 27.6119, val_loss: 28.1193, val_MinusLogProbMetric: 28.1193

Epoch 697: val_loss did not improve from 28.07263
196/196 - 33s - loss: 27.6119 - MinusLogProbMetric: 27.6119 - val_loss: 28.1193 - val_MinusLogProbMetric: 28.1193 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 698/1000
2023-10-30 21:13:25.022 
Epoch 698/1000 
	 loss: 27.6001, MinusLogProbMetric: 27.6001, val_loss: 28.1201, val_MinusLogProbMetric: 28.1201

Epoch 698: val_loss did not improve from 28.07263
196/196 - 28s - loss: 27.6001 - MinusLogProbMetric: 27.6001 - val_loss: 28.1201 - val_MinusLogProbMetric: 28.1201 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 699/1000
2023-10-30 21:13:53.567 
Epoch 699/1000 
	 loss: 27.6134, MinusLogProbMetric: 27.6134, val_loss: 28.0820, val_MinusLogProbMetric: 28.0820

Epoch 699: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.6134 - MinusLogProbMetric: 27.6134 - val_loss: 28.0820 - val_MinusLogProbMetric: 28.0820 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 700/1000
2023-10-30 21:14:22.291 
Epoch 700/1000 
	 loss: 27.6032, MinusLogProbMetric: 27.6032, val_loss: 28.1444, val_MinusLogProbMetric: 28.1444

Epoch 700: val_loss did not improve from 28.07263
196/196 - 29s - loss: 27.6032 - MinusLogProbMetric: 27.6032 - val_loss: 28.1444 - val_MinusLogProbMetric: 28.1444 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 701/1000
2023-10-30 21:14:54.176 
Epoch 701/1000 
	 loss: 27.6136, MinusLogProbMetric: 27.6136, val_loss: 28.1890, val_MinusLogProbMetric: 28.1890

Epoch 701: val_loss did not improve from 28.07263
196/196 - 32s - loss: 27.6136 - MinusLogProbMetric: 27.6136 - val_loss: 28.1890 - val_MinusLogProbMetric: 28.1890 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 702/1000
2023-10-30 21:15:25.079 
Epoch 702/1000 
	 loss: 27.6133, MinusLogProbMetric: 27.6133, val_loss: 28.1138, val_MinusLogProbMetric: 28.1138

Epoch 702: val_loss did not improve from 28.07263
196/196 - 31s - loss: 27.6133 - MinusLogProbMetric: 27.6133 - val_loss: 28.1138 - val_MinusLogProbMetric: 28.1138 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 703/1000
2023-10-30 21:15:53.656 
Epoch 703/1000 
	 loss: 27.6106, MinusLogProbMetric: 27.6106, val_loss: 28.0714, val_MinusLogProbMetric: 28.0714

Epoch 703: val_loss improved from 28.07263 to 28.07144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 27.6106 - MinusLogProbMetric: 27.6106 - val_loss: 28.0714 - val_MinusLogProbMetric: 28.0714 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 704/1000
2023-10-30 21:16:22.465 
Epoch 704/1000 
	 loss: 27.5996, MinusLogProbMetric: 27.5996, val_loss: 28.1348, val_MinusLogProbMetric: 28.1348

Epoch 704: val_loss did not improve from 28.07144
196/196 - 28s - loss: 27.5996 - MinusLogProbMetric: 27.5996 - val_loss: 28.1348 - val_MinusLogProbMetric: 28.1348 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 705/1000
2023-10-30 21:16:51.214 
Epoch 705/1000 
	 loss: 27.5944, MinusLogProbMetric: 27.5944, val_loss: 28.0909, val_MinusLogProbMetric: 28.0909

Epoch 705: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.5944 - MinusLogProbMetric: 27.5944 - val_loss: 28.0909 - val_MinusLogProbMetric: 28.0909 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 706/1000
2023-10-30 21:17:20.819 
Epoch 706/1000 
	 loss: 27.6098, MinusLogProbMetric: 27.6098, val_loss: 28.1438, val_MinusLogProbMetric: 28.1438

Epoch 706: val_loss did not improve from 28.07144
196/196 - 30s - loss: 27.6098 - MinusLogProbMetric: 27.6098 - val_loss: 28.1438 - val_MinusLogProbMetric: 28.1438 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 707/1000
2023-10-30 21:17:52.623 
Epoch 707/1000 
	 loss: 27.5991, MinusLogProbMetric: 27.5991, val_loss: 28.2055, val_MinusLogProbMetric: 28.2055

Epoch 707: val_loss did not improve from 28.07144
196/196 - 32s - loss: 27.5991 - MinusLogProbMetric: 27.5991 - val_loss: 28.2055 - val_MinusLogProbMetric: 28.2055 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 708/1000
2023-10-30 21:18:21.369 
Epoch 708/1000 
	 loss: 27.6081, MinusLogProbMetric: 27.6081, val_loss: 28.1212, val_MinusLogProbMetric: 28.1212

Epoch 708: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.6081 - MinusLogProbMetric: 27.6081 - val_loss: 28.1212 - val_MinusLogProbMetric: 28.1212 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 709/1000
2023-10-30 21:18:50.518 
Epoch 709/1000 
	 loss: 27.6121, MinusLogProbMetric: 27.6121, val_loss: 28.1630, val_MinusLogProbMetric: 28.1630

Epoch 709: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.6121 - MinusLogProbMetric: 27.6121 - val_loss: 28.1630 - val_MinusLogProbMetric: 28.1630 - lr: 2.5000e-04 - 29s/epoch - 149ms/step
Epoch 710/1000
2023-10-30 21:19:22.161 
Epoch 710/1000 
	 loss: 27.5876, MinusLogProbMetric: 27.5876, val_loss: 28.1175, val_MinusLogProbMetric: 28.1175

Epoch 710: val_loss did not improve from 28.07144
196/196 - 32s - loss: 27.5876 - MinusLogProbMetric: 27.5876 - val_loss: 28.1175 - val_MinusLogProbMetric: 28.1175 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 711/1000
2023-10-30 21:19:51.859 
Epoch 711/1000 
	 loss: 27.6099, MinusLogProbMetric: 27.6099, val_loss: 28.1202, val_MinusLogProbMetric: 28.1202

Epoch 711: val_loss did not improve from 28.07144
196/196 - 30s - loss: 27.6099 - MinusLogProbMetric: 27.6099 - val_loss: 28.1202 - val_MinusLogProbMetric: 28.1202 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 712/1000
2023-10-30 21:20:20.435 
Epoch 712/1000 
	 loss: 27.6056, MinusLogProbMetric: 27.6056, val_loss: 28.0877, val_MinusLogProbMetric: 28.0877

Epoch 712: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.6056 - MinusLogProbMetric: 27.6056 - val_loss: 28.0877 - val_MinusLogProbMetric: 28.0877 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 713/1000
2023-10-30 21:20:48.882 
Epoch 713/1000 
	 loss: 27.5975, MinusLogProbMetric: 27.5975, val_loss: 28.1351, val_MinusLogProbMetric: 28.1351

Epoch 713: val_loss did not improve from 28.07144
196/196 - 28s - loss: 27.5975 - MinusLogProbMetric: 27.5975 - val_loss: 28.1351 - val_MinusLogProbMetric: 28.1351 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 714/1000
2023-10-30 21:21:18.009 
Epoch 714/1000 
	 loss: 27.6003, MinusLogProbMetric: 27.6003, val_loss: 28.1515, val_MinusLogProbMetric: 28.1515

Epoch 714: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.6003 - MinusLogProbMetric: 27.6003 - val_loss: 28.1515 - val_MinusLogProbMetric: 28.1515 - lr: 2.5000e-04 - 29s/epoch - 149ms/step
Epoch 715/1000
2023-10-30 21:21:50.400 
Epoch 715/1000 
	 loss: 27.6105, MinusLogProbMetric: 27.6105, val_loss: 28.1097, val_MinusLogProbMetric: 28.1097

Epoch 715: val_loss did not improve from 28.07144
196/196 - 32s - loss: 27.6105 - MinusLogProbMetric: 27.6105 - val_loss: 28.1097 - val_MinusLogProbMetric: 28.1097 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 716/1000
2023-10-30 21:22:21.194 
Epoch 716/1000 
	 loss: 27.6097, MinusLogProbMetric: 27.6097, val_loss: 28.1932, val_MinusLogProbMetric: 28.1932

Epoch 716: val_loss did not improve from 28.07144
196/196 - 31s - loss: 27.6097 - MinusLogProbMetric: 27.6097 - val_loss: 28.1932 - val_MinusLogProbMetric: 28.1932 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 717/1000
2023-10-30 21:22:50.414 
Epoch 717/1000 
	 loss: 27.5934, MinusLogProbMetric: 27.5934, val_loss: 28.1732, val_MinusLogProbMetric: 28.1732

Epoch 717: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.5934 - MinusLogProbMetric: 27.5934 - val_loss: 28.1732 - val_MinusLogProbMetric: 28.1732 - lr: 2.5000e-04 - 29s/epoch - 149ms/step
Epoch 718/1000
2023-10-30 21:23:19.151 
Epoch 718/1000 
	 loss: 27.5972, MinusLogProbMetric: 27.5972, val_loss: 28.1172, val_MinusLogProbMetric: 28.1172

Epoch 718: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.5972 - MinusLogProbMetric: 27.5972 - val_loss: 28.1172 - val_MinusLogProbMetric: 28.1172 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 719/1000
2023-10-30 21:23:48.350 
Epoch 719/1000 
	 loss: 27.5890, MinusLogProbMetric: 27.5890, val_loss: 28.1258, val_MinusLogProbMetric: 28.1258

Epoch 719: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.5890 - MinusLogProbMetric: 27.5890 - val_loss: 28.1258 - val_MinusLogProbMetric: 28.1258 - lr: 2.5000e-04 - 29s/epoch - 149ms/step
Epoch 720/1000
2023-10-30 21:24:19.774 
Epoch 720/1000 
	 loss: 27.6086, MinusLogProbMetric: 27.6086, val_loss: 28.1088, val_MinusLogProbMetric: 28.1088

Epoch 720: val_loss did not improve from 28.07144
196/196 - 31s - loss: 27.6086 - MinusLogProbMetric: 27.6086 - val_loss: 28.1088 - val_MinusLogProbMetric: 28.1088 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 721/1000
2023-10-30 21:24:51.706 
Epoch 721/1000 
	 loss: 27.5944, MinusLogProbMetric: 27.5944, val_loss: 28.2058, val_MinusLogProbMetric: 28.2058

Epoch 721: val_loss did not improve from 28.07144
196/196 - 32s - loss: 27.5944 - MinusLogProbMetric: 27.5944 - val_loss: 28.2058 - val_MinusLogProbMetric: 28.2058 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 722/1000
2023-10-30 21:25:20.637 
Epoch 722/1000 
	 loss: 27.5927, MinusLogProbMetric: 27.5927, val_loss: 28.1081, val_MinusLogProbMetric: 28.1081

Epoch 722: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.5927 - MinusLogProbMetric: 27.5927 - val_loss: 28.1081 - val_MinusLogProbMetric: 28.1081 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 723/1000
2023-10-30 21:25:49.512 
Epoch 723/1000 
	 loss: 27.5989, MinusLogProbMetric: 27.5989, val_loss: 28.1100, val_MinusLogProbMetric: 28.1100

Epoch 723: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.5989 - MinusLogProbMetric: 27.5989 - val_loss: 28.1100 - val_MinusLogProbMetric: 28.1100 - lr: 2.5000e-04 - 29s/epoch - 147ms/step
Epoch 724/1000
2023-10-30 21:26:18.650 
Epoch 724/1000 
	 loss: 27.6004, MinusLogProbMetric: 27.6004, val_loss: 28.1284, val_MinusLogProbMetric: 28.1284

Epoch 724: val_loss did not improve from 28.07144
196/196 - 29s - loss: 27.6004 - MinusLogProbMetric: 27.6004 - val_loss: 28.1284 - val_MinusLogProbMetric: 28.1284 - lr: 2.5000e-04 - 29s/epoch - 149ms/step
Epoch 725/1000
2023-10-30 21:26:48.202 
Epoch 725/1000 
	 loss: 27.6105, MinusLogProbMetric: 27.6105, val_loss: 28.1628, val_MinusLogProbMetric: 28.1628

Epoch 725: val_loss did not improve from 28.07144
196/196 - 30s - loss: 27.6105 - MinusLogProbMetric: 27.6105 - val_loss: 28.1628 - val_MinusLogProbMetric: 28.1628 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 726/1000
2023-10-30 21:27:19.288 
Epoch 726/1000 
	 loss: 27.6131, MinusLogProbMetric: 27.6131, val_loss: 28.1230, val_MinusLogProbMetric: 28.1230

Epoch 726: val_loss did not improve from 28.07144
196/196 - 31s - loss: 27.6131 - MinusLogProbMetric: 27.6131 - val_loss: 28.1230 - val_MinusLogProbMetric: 28.1230 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 727/1000
2023-10-30 21:27:51.654 
Epoch 727/1000 
	 loss: 27.6152, MinusLogProbMetric: 27.6152, val_loss: 28.0693, val_MinusLogProbMetric: 28.0693

Epoch 727: val_loss improved from 28.07144 to 28.06932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 27.6152 - MinusLogProbMetric: 27.6152 - val_loss: 28.0693 - val_MinusLogProbMetric: 28.0693 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 728/1000
2023-10-30 21:28:20.276 
Epoch 728/1000 
	 loss: 27.6037, MinusLogProbMetric: 27.6037, val_loss: 28.0851, val_MinusLogProbMetric: 28.0851

Epoch 728: val_loss did not improve from 28.06932
196/196 - 28s - loss: 27.6037 - MinusLogProbMetric: 27.6037 - val_loss: 28.0851 - val_MinusLogProbMetric: 28.0851 - lr: 2.5000e-04 - 28s/epoch - 144ms/step
Epoch 729/1000
2023-10-30 21:28:48.636 
Epoch 729/1000 
	 loss: 27.5969, MinusLogProbMetric: 27.5969, val_loss: 28.1805, val_MinusLogProbMetric: 28.1805

Epoch 729: val_loss did not improve from 28.06932
196/196 - 28s - loss: 27.5969 - MinusLogProbMetric: 27.5969 - val_loss: 28.1805 - val_MinusLogProbMetric: 28.1805 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 730/1000
2023-10-30 21:29:16.870 
Epoch 730/1000 
	 loss: 27.6021, MinusLogProbMetric: 27.6021, val_loss: 28.1438, val_MinusLogProbMetric: 28.1438

Epoch 730: val_loss did not improve from 28.06932
196/196 - 28s - loss: 27.6021 - MinusLogProbMetric: 27.6021 - val_loss: 28.1438 - val_MinusLogProbMetric: 28.1438 - lr: 2.5000e-04 - 28s/epoch - 144ms/step
Epoch 731/1000
2023-10-30 21:29:44.201 
Epoch 731/1000 
	 loss: 27.6069, MinusLogProbMetric: 27.6069, val_loss: 28.1374, val_MinusLogProbMetric: 28.1374

Epoch 731: val_loss did not improve from 28.06932
196/196 - 27s - loss: 27.6069 - MinusLogProbMetric: 27.6069 - val_loss: 28.1374 - val_MinusLogProbMetric: 28.1374 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 732/1000
2023-10-30 21:30:16.725 
Epoch 732/1000 
	 loss: 27.5924, MinusLogProbMetric: 27.5924, val_loss: 28.1241, val_MinusLogProbMetric: 28.1241

Epoch 732: val_loss did not improve from 28.06932
196/196 - 33s - loss: 27.5924 - MinusLogProbMetric: 27.5924 - val_loss: 28.1241 - val_MinusLogProbMetric: 28.1241 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 733/1000
2023-10-30 21:30:50.244 
Epoch 733/1000 
	 loss: 27.6155, MinusLogProbMetric: 27.6155, val_loss: 28.1216, val_MinusLogProbMetric: 28.1216

Epoch 733: val_loss did not improve from 28.06932
196/196 - 34s - loss: 27.6155 - MinusLogProbMetric: 27.6155 - val_loss: 28.1216 - val_MinusLogProbMetric: 28.1216 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 734/1000
2023-10-30 21:31:24.052 
Epoch 734/1000 
	 loss: 27.5934, MinusLogProbMetric: 27.5934, val_loss: 28.1650, val_MinusLogProbMetric: 28.1650

Epoch 734: val_loss did not improve from 28.06932
196/196 - 34s - loss: 27.5934 - MinusLogProbMetric: 27.5934 - val_loss: 28.1650 - val_MinusLogProbMetric: 28.1650 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 735/1000
2023-10-30 21:31:57.759 
Epoch 735/1000 
	 loss: 27.5882, MinusLogProbMetric: 27.5882, val_loss: 28.1904, val_MinusLogProbMetric: 28.1904

Epoch 735: val_loss did not improve from 28.06932
196/196 - 34s - loss: 27.5882 - MinusLogProbMetric: 27.5882 - val_loss: 28.1904 - val_MinusLogProbMetric: 28.1904 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 736/1000
2023-10-30 21:32:31.773 
Epoch 736/1000 
	 loss: 27.5940, MinusLogProbMetric: 27.5940, val_loss: 28.0532, val_MinusLogProbMetric: 28.0532

Epoch 736: val_loss improved from 28.06932 to 28.05323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 27.5940 - MinusLogProbMetric: 27.5940 - val_loss: 28.0532 - val_MinusLogProbMetric: 28.0532 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 737/1000
2023-10-30 21:33:06.558 
Epoch 737/1000 
	 loss: 27.5958, MinusLogProbMetric: 27.5958, val_loss: 28.0875, val_MinusLogProbMetric: 28.0875

Epoch 737: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.5958 - MinusLogProbMetric: 27.5958 - val_loss: 28.0875 - val_MinusLogProbMetric: 28.0875 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 738/1000
2023-10-30 21:33:39.528 
Epoch 738/1000 
	 loss: 27.5864, MinusLogProbMetric: 27.5864, val_loss: 28.1805, val_MinusLogProbMetric: 28.1805

Epoch 738: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5864 - MinusLogProbMetric: 27.5864 - val_loss: 28.1805 - val_MinusLogProbMetric: 28.1805 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 739/1000
2023-10-30 21:34:11.995 
Epoch 739/1000 
	 loss: 27.6048, MinusLogProbMetric: 27.6048, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 739: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.6048 - MinusLogProbMetric: 27.6048 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 740/1000
2023-10-30 21:34:41.950 
Epoch 740/1000 
	 loss: 27.6020, MinusLogProbMetric: 27.6020, val_loss: 28.1566, val_MinusLogProbMetric: 28.1566

Epoch 740: val_loss did not improve from 28.05323
196/196 - 30s - loss: 27.6020 - MinusLogProbMetric: 27.6020 - val_loss: 28.1566 - val_MinusLogProbMetric: 28.1566 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 741/1000
2023-10-30 21:35:14.938 
Epoch 741/1000 
	 loss: 27.6033, MinusLogProbMetric: 27.6033, val_loss: 28.1006, val_MinusLogProbMetric: 28.1006

Epoch 741: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.6033 - MinusLogProbMetric: 27.6033 - val_loss: 28.1006 - val_MinusLogProbMetric: 28.1006 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 742/1000
2023-10-30 21:35:49.153 
Epoch 742/1000 
	 loss: 27.6006, MinusLogProbMetric: 27.6006, val_loss: 28.1449, val_MinusLogProbMetric: 28.1449

Epoch 742: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.6006 - MinusLogProbMetric: 27.6006 - val_loss: 28.1449 - val_MinusLogProbMetric: 28.1449 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 743/1000
2023-10-30 21:36:22.740 
Epoch 743/1000 
	 loss: 27.6108, MinusLogProbMetric: 27.6108, val_loss: 28.0907, val_MinusLogProbMetric: 28.0907

Epoch 743: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.6108 - MinusLogProbMetric: 27.6108 - val_loss: 28.0907 - val_MinusLogProbMetric: 28.0907 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 744/1000
2023-10-30 21:36:57.419 
Epoch 744/1000 
	 loss: 27.5944, MinusLogProbMetric: 27.5944, val_loss: 28.1009, val_MinusLogProbMetric: 28.1009

Epoch 744: val_loss did not improve from 28.05323
196/196 - 35s - loss: 27.5944 - MinusLogProbMetric: 27.5944 - val_loss: 28.1009 - val_MinusLogProbMetric: 28.1009 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 745/1000
2023-10-30 21:37:30.786 
Epoch 745/1000 
	 loss: 27.5959, MinusLogProbMetric: 27.5959, val_loss: 28.0787, val_MinusLogProbMetric: 28.0787

Epoch 745: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5959 - MinusLogProbMetric: 27.5959 - val_loss: 28.0787 - val_MinusLogProbMetric: 28.0787 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 746/1000
2023-10-30 21:38:05.174 
Epoch 746/1000 
	 loss: 27.5941, MinusLogProbMetric: 27.5941, val_loss: 28.1056, val_MinusLogProbMetric: 28.1056

Epoch 746: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.5941 - MinusLogProbMetric: 27.5941 - val_loss: 28.1056 - val_MinusLogProbMetric: 28.1056 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 747/1000
2023-10-30 21:38:38.283 
Epoch 747/1000 
	 loss: 27.6017, MinusLogProbMetric: 27.6017, val_loss: 28.1053, val_MinusLogProbMetric: 28.1053

Epoch 747: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.6017 - MinusLogProbMetric: 27.6017 - val_loss: 28.1053 - val_MinusLogProbMetric: 28.1053 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 748/1000
2023-10-30 21:39:11.457 
Epoch 748/1000 
	 loss: 27.6036, MinusLogProbMetric: 27.6036, val_loss: 28.0942, val_MinusLogProbMetric: 28.0942

Epoch 748: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.6036 - MinusLogProbMetric: 27.6036 - val_loss: 28.0942 - val_MinusLogProbMetric: 28.0942 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 749/1000
2023-10-30 21:39:44.557 
Epoch 749/1000 
	 loss: 27.5897, MinusLogProbMetric: 27.5897, val_loss: 28.1668, val_MinusLogProbMetric: 28.1668

Epoch 749: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5897 - MinusLogProbMetric: 27.5897 - val_loss: 28.1668 - val_MinusLogProbMetric: 28.1668 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 750/1000
2023-10-30 21:40:17.538 
Epoch 750/1000 
	 loss: 27.5993, MinusLogProbMetric: 27.5993, val_loss: 28.1029, val_MinusLogProbMetric: 28.1029

Epoch 750: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5993 - MinusLogProbMetric: 27.5993 - val_loss: 28.1029 - val_MinusLogProbMetric: 28.1029 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 751/1000
2023-10-30 21:40:49.607 
Epoch 751/1000 
	 loss: 27.5969, MinusLogProbMetric: 27.5969, val_loss: 28.1024, val_MinusLogProbMetric: 28.1024

Epoch 751: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.5969 - MinusLogProbMetric: 27.5969 - val_loss: 28.1024 - val_MinusLogProbMetric: 28.1024 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 752/1000
2023-10-30 21:41:24.139 
Epoch 752/1000 
	 loss: 27.5991, MinusLogProbMetric: 27.5991, val_loss: 28.1658, val_MinusLogProbMetric: 28.1658

Epoch 752: val_loss did not improve from 28.05323
196/196 - 35s - loss: 27.5991 - MinusLogProbMetric: 27.5991 - val_loss: 28.1658 - val_MinusLogProbMetric: 28.1658 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 753/1000
2023-10-30 21:41:57.436 
Epoch 753/1000 
	 loss: 27.6025, MinusLogProbMetric: 27.6025, val_loss: 28.0767, val_MinusLogProbMetric: 28.0767

Epoch 753: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.6025 - MinusLogProbMetric: 27.6025 - val_loss: 28.0767 - val_MinusLogProbMetric: 28.0767 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 754/1000
2023-10-30 21:42:28.851 
Epoch 754/1000 
	 loss: 27.5983, MinusLogProbMetric: 27.5983, val_loss: 28.1882, val_MinusLogProbMetric: 28.1882

Epoch 754: val_loss did not improve from 28.05323
196/196 - 31s - loss: 27.5983 - MinusLogProbMetric: 27.5983 - val_loss: 28.1882 - val_MinusLogProbMetric: 28.1882 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 755/1000
2023-10-30 21:43:01.355 
Epoch 755/1000 
	 loss: 27.5887, MinusLogProbMetric: 27.5887, val_loss: 28.1017, val_MinusLogProbMetric: 28.1017

Epoch 755: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.5887 - MinusLogProbMetric: 27.5887 - val_loss: 28.1017 - val_MinusLogProbMetric: 28.1017 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 756/1000
2023-10-30 21:43:33.527 
Epoch 756/1000 
	 loss: 27.5871, MinusLogProbMetric: 27.5871, val_loss: 28.0665, val_MinusLogProbMetric: 28.0665

Epoch 756: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.5871 - MinusLogProbMetric: 27.5871 - val_loss: 28.0665 - val_MinusLogProbMetric: 28.0665 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 757/1000
2023-10-30 21:44:06.821 
Epoch 757/1000 
	 loss: 27.5937, MinusLogProbMetric: 27.5937, val_loss: 28.0927, val_MinusLogProbMetric: 28.0927

Epoch 757: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5937 - MinusLogProbMetric: 27.5937 - val_loss: 28.0927 - val_MinusLogProbMetric: 28.0927 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 758/1000
2023-10-30 21:44:40.139 
Epoch 758/1000 
	 loss: 27.5958, MinusLogProbMetric: 27.5958, val_loss: 28.1157, val_MinusLogProbMetric: 28.1157

Epoch 758: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5958 - MinusLogProbMetric: 27.5958 - val_loss: 28.1157 - val_MinusLogProbMetric: 28.1157 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 759/1000
2023-10-30 21:45:12.187 
Epoch 759/1000 
	 loss: 27.6036, MinusLogProbMetric: 27.6036, val_loss: 28.1433, val_MinusLogProbMetric: 28.1433

Epoch 759: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.6036 - MinusLogProbMetric: 27.6036 - val_loss: 28.1433 - val_MinusLogProbMetric: 28.1433 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 760/1000
2023-10-30 21:45:45.338 
Epoch 760/1000 
	 loss: 27.6035, MinusLogProbMetric: 27.6035, val_loss: 28.1136, val_MinusLogProbMetric: 28.1136

Epoch 760: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.6035 - MinusLogProbMetric: 27.6035 - val_loss: 28.1136 - val_MinusLogProbMetric: 28.1136 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 761/1000
2023-10-30 21:46:18.729 
Epoch 761/1000 
	 loss: 27.5872, MinusLogProbMetric: 27.5872, val_loss: 28.0943, val_MinusLogProbMetric: 28.0943

Epoch 761: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5872 - MinusLogProbMetric: 27.5872 - val_loss: 28.0943 - val_MinusLogProbMetric: 28.0943 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 762/1000
2023-10-30 21:46:51.475 
Epoch 762/1000 
	 loss: 27.5896, MinusLogProbMetric: 27.5896, val_loss: 28.0847, val_MinusLogProbMetric: 28.0847

Epoch 762: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5896 - MinusLogProbMetric: 27.5896 - val_loss: 28.0847 - val_MinusLogProbMetric: 28.0847 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 763/1000
2023-10-30 21:47:25.566 
Epoch 763/1000 
	 loss: 27.5984, MinusLogProbMetric: 27.5984, val_loss: 28.1448, val_MinusLogProbMetric: 28.1448

Epoch 763: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.5984 - MinusLogProbMetric: 27.5984 - val_loss: 28.1448 - val_MinusLogProbMetric: 28.1448 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 764/1000
2023-10-30 21:47:58.358 
Epoch 764/1000 
	 loss: 27.6065, MinusLogProbMetric: 27.6065, val_loss: 28.1658, val_MinusLogProbMetric: 28.1658

Epoch 764: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.6065 - MinusLogProbMetric: 27.6065 - val_loss: 28.1658 - val_MinusLogProbMetric: 28.1658 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 765/1000
2023-10-30 21:48:31.235 
Epoch 765/1000 
	 loss: 27.5896, MinusLogProbMetric: 27.5896, val_loss: 28.1138, val_MinusLogProbMetric: 28.1138

Epoch 765: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5896 - MinusLogProbMetric: 27.5896 - val_loss: 28.1138 - val_MinusLogProbMetric: 28.1138 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 766/1000
2023-10-30 21:49:05.216 
Epoch 766/1000 
	 loss: 27.5880, MinusLogProbMetric: 27.5880, val_loss: 28.1098, val_MinusLogProbMetric: 28.1098

Epoch 766: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.5880 - MinusLogProbMetric: 27.5880 - val_loss: 28.1098 - val_MinusLogProbMetric: 28.1098 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 767/1000
2023-10-30 21:49:36.784 
Epoch 767/1000 
	 loss: 27.6051, MinusLogProbMetric: 27.6051, val_loss: 28.1857, val_MinusLogProbMetric: 28.1857

Epoch 767: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.6051 - MinusLogProbMetric: 27.6051 - val_loss: 28.1857 - val_MinusLogProbMetric: 28.1857 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 768/1000
2023-10-30 21:50:08.992 
Epoch 768/1000 
	 loss: 27.6007, MinusLogProbMetric: 27.6007, val_loss: 28.0589, val_MinusLogProbMetric: 28.0589

Epoch 768: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.6007 - MinusLogProbMetric: 27.6007 - val_loss: 28.0589 - val_MinusLogProbMetric: 28.0589 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 769/1000
2023-10-30 21:50:42.001 
Epoch 769/1000 
	 loss: 27.5887, MinusLogProbMetric: 27.5887, val_loss: 28.0894, val_MinusLogProbMetric: 28.0894

Epoch 769: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5887 - MinusLogProbMetric: 27.5887 - val_loss: 28.0894 - val_MinusLogProbMetric: 28.0894 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 770/1000
2023-10-30 21:51:15.627 
Epoch 770/1000 
	 loss: 27.5830, MinusLogProbMetric: 27.5830, val_loss: 28.0613, val_MinusLogProbMetric: 28.0613

Epoch 770: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.5830 - MinusLogProbMetric: 27.5830 - val_loss: 28.0613 - val_MinusLogProbMetric: 28.0613 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 771/1000
2023-10-30 21:51:49.282 
Epoch 771/1000 
	 loss: 27.5822, MinusLogProbMetric: 27.5822, val_loss: 28.1042, val_MinusLogProbMetric: 28.1042

Epoch 771: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.5822 - MinusLogProbMetric: 27.5822 - val_loss: 28.1042 - val_MinusLogProbMetric: 28.1042 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 772/1000
2023-10-30 21:52:22.947 
Epoch 772/1000 
	 loss: 27.5929, MinusLogProbMetric: 27.5929, val_loss: 28.1404, val_MinusLogProbMetric: 28.1404

Epoch 772: val_loss did not improve from 28.05323
196/196 - 34s - loss: 27.5929 - MinusLogProbMetric: 27.5929 - val_loss: 28.1404 - val_MinusLogProbMetric: 28.1404 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 773/1000
2023-10-30 21:52:55.751 
Epoch 773/1000 
	 loss: 27.5851, MinusLogProbMetric: 27.5851, val_loss: 28.0578, val_MinusLogProbMetric: 28.0578

Epoch 773: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5851 - MinusLogProbMetric: 27.5851 - val_loss: 28.0578 - val_MinusLogProbMetric: 28.0578 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 774/1000
2023-10-30 21:53:28.197 
Epoch 774/1000 
	 loss: 27.5710, MinusLogProbMetric: 27.5710, val_loss: 28.1283, val_MinusLogProbMetric: 28.1283

Epoch 774: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.5710 - MinusLogProbMetric: 27.5710 - val_loss: 28.1283 - val_MinusLogProbMetric: 28.1283 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 775/1000
2023-10-30 21:53:59.441 
Epoch 775/1000 
	 loss: 27.5882, MinusLogProbMetric: 27.5882, val_loss: 28.1085, val_MinusLogProbMetric: 28.1085

Epoch 775: val_loss did not improve from 28.05323
196/196 - 31s - loss: 27.5882 - MinusLogProbMetric: 27.5882 - val_loss: 28.1085 - val_MinusLogProbMetric: 28.1085 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 776/1000
2023-10-30 21:54:31.391 
Epoch 776/1000 
	 loss: 27.5888, MinusLogProbMetric: 27.5888, val_loss: 28.1932, val_MinusLogProbMetric: 28.1932

Epoch 776: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.5888 - MinusLogProbMetric: 27.5888 - val_loss: 28.1932 - val_MinusLogProbMetric: 28.1932 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 777/1000
2023-10-30 21:55:04.150 
Epoch 777/1000 
	 loss: 27.5841, MinusLogProbMetric: 27.5841, val_loss: 28.1096, val_MinusLogProbMetric: 28.1096

Epoch 777: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5841 - MinusLogProbMetric: 27.5841 - val_loss: 28.1096 - val_MinusLogProbMetric: 28.1096 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 778/1000
2023-10-30 21:55:37.452 
Epoch 778/1000 
	 loss: 27.5820, MinusLogProbMetric: 27.5820, val_loss: 28.0977, val_MinusLogProbMetric: 28.0977

Epoch 778: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5820 - MinusLogProbMetric: 27.5820 - val_loss: 28.0977 - val_MinusLogProbMetric: 28.0977 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 779/1000
2023-10-30 21:56:12.102 
Epoch 779/1000 
	 loss: 27.5873, MinusLogProbMetric: 27.5873, val_loss: 28.1025, val_MinusLogProbMetric: 28.1025

Epoch 779: val_loss did not improve from 28.05323
196/196 - 35s - loss: 27.5873 - MinusLogProbMetric: 27.5873 - val_loss: 28.1025 - val_MinusLogProbMetric: 28.1025 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 780/1000
2023-10-30 21:56:45.307 
Epoch 780/1000 
	 loss: 27.5791, MinusLogProbMetric: 27.5791, val_loss: 28.0649, val_MinusLogProbMetric: 28.0649

Epoch 780: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5791 - MinusLogProbMetric: 27.5791 - val_loss: 28.0649 - val_MinusLogProbMetric: 28.0649 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 781/1000
2023-10-30 21:57:18.265 
Epoch 781/1000 
	 loss: 27.5773, MinusLogProbMetric: 27.5773, val_loss: 28.1441, val_MinusLogProbMetric: 28.1441

Epoch 781: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5773 - MinusLogProbMetric: 27.5773 - val_loss: 28.1441 - val_MinusLogProbMetric: 28.1441 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 782/1000
2023-10-30 21:57:51.379 
Epoch 782/1000 
	 loss: 27.5879, MinusLogProbMetric: 27.5879, val_loss: 28.0638, val_MinusLogProbMetric: 28.0638

Epoch 782: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5879 - MinusLogProbMetric: 27.5879 - val_loss: 28.0638 - val_MinusLogProbMetric: 28.0638 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 783/1000
2023-10-30 21:58:23.375 
Epoch 783/1000 
	 loss: 27.5838, MinusLogProbMetric: 27.5838, val_loss: 28.0813, val_MinusLogProbMetric: 28.0813

Epoch 783: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.5838 - MinusLogProbMetric: 27.5838 - val_loss: 28.0813 - val_MinusLogProbMetric: 28.0813 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 784/1000
2023-10-30 21:58:55.877 
Epoch 784/1000 
	 loss: 27.5795, MinusLogProbMetric: 27.5795, val_loss: 28.0735, val_MinusLogProbMetric: 28.0735

Epoch 784: val_loss did not improve from 28.05323
196/196 - 32s - loss: 27.5795 - MinusLogProbMetric: 27.5795 - val_loss: 28.0735 - val_MinusLogProbMetric: 28.0735 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 785/1000
2023-10-30 21:59:29.038 
Epoch 785/1000 
	 loss: 27.5908, MinusLogProbMetric: 27.5908, val_loss: 28.0862, val_MinusLogProbMetric: 28.0862

Epoch 785: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5908 - MinusLogProbMetric: 27.5908 - val_loss: 28.0862 - val_MinusLogProbMetric: 28.0862 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 786/1000
2023-10-30 22:00:02.373 
Epoch 786/1000 
	 loss: 27.5769, MinusLogProbMetric: 27.5769, val_loss: 28.1065, val_MinusLogProbMetric: 28.1065

Epoch 786: val_loss did not improve from 28.05323
196/196 - 33s - loss: 27.5769 - MinusLogProbMetric: 27.5769 - val_loss: 28.1065 - val_MinusLogProbMetric: 28.1065 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 787/1000
2023-10-30 22:00:35.300 
Epoch 787/1000 
	 loss: 27.5197, MinusLogProbMetric: 27.5197, val_loss: 28.0467, val_MinusLogProbMetric: 28.0467

Epoch 787: val_loss improved from 28.05323 to 28.04670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 27.5197 - MinusLogProbMetric: 27.5197 - val_loss: 28.0467 - val_MinusLogProbMetric: 28.0467 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 788/1000
2023-10-30 22:01:08.735 
Epoch 788/1000 
	 loss: 27.5191, MinusLogProbMetric: 27.5191, val_loss: 28.0525, val_MinusLogProbMetric: 28.0525

Epoch 788: val_loss did not improve from 28.04670
196/196 - 33s - loss: 27.5191 - MinusLogProbMetric: 27.5191 - val_loss: 28.0525 - val_MinusLogProbMetric: 28.0525 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 789/1000
2023-10-30 22:01:42.633 
Epoch 789/1000 
	 loss: 27.5218, MinusLogProbMetric: 27.5218, val_loss: 28.0187, val_MinusLogProbMetric: 28.0187

Epoch 789: val_loss improved from 28.04670 to 28.01867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 27.5218 - MinusLogProbMetric: 27.5218 - val_loss: 28.0187 - val_MinusLogProbMetric: 28.0187 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 790/1000
2023-10-30 22:02:17.212 
Epoch 790/1000 
	 loss: 27.5217, MinusLogProbMetric: 27.5217, val_loss: 28.0335, val_MinusLogProbMetric: 28.0335

Epoch 790: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5217 - MinusLogProbMetric: 27.5217 - val_loss: 28.0335 - val_MinusLogProbMetric: 28.0335 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 791/1000
2023-10-30 22:02:50.401 
Epoch 791/1000 
	 loss: 27.5171, MinusLogProbMetric: 27.5171, val_loss: 28.0662, val_MinusLogProbMetric: 28.0662

Epoch 791: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5171 - MinusLogProbMetric: 27.5171 - val_loss: 28.0662 - val_MinusLogProbMetric: 28.0662 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 792/1000
2023-10-30 22:03:24.377 
Epoch 792/1000 
	 loss: 27.5169, MinusLogProbMetric: 27.5169, val_loss: 28.0526, val_MinusLogProbMetric: 28.0526

Epoch 792: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5169 - MinusLogProbMetric: 27.5169 - val_loss: 28.0526 - val_MinusLogProbMetric: 28.0526 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 793/1000
2023-10-30 22:03:58.187 
Epoch 793/1000 
	 loss: 27.5181, MinusLogProbMetric: 27.5181, val_loss: 28.0465, val_MinusLogProbMetric: 28.0465

Epoch 793: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5181 - MinusLogProbMetric: 27.5181 - val_loss: 28.0465 - val_MinusLogProbMetric: 28.0465 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 794/1000
2023-10-30 22:04:30.021 
Epoch 794/1000 
	 loss: 27.5158, MinusLogProbMetric: 27.5158, val_loss: 28.0264, val_MinusLogProbMetric: 28.0264

Epoch 794: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5158 - MinusLogProbMetric: 27.5158 - val_loss: 28.0264 - val_MinusLogProbMetric: 28.0264 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 795/1000
2023-10-30 22:05:02.997 
Epoch 795/1000 
	 loss: 27.5178, MinusLogProbMetric: 27.5178, val_loss: 28.0382, val_MinusLogProbMetric: 28.0382

Epoch 795: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5178 - MinusLogProbMetric: 27.5178 - val_loss: 28.0382 - val_MinusLogProbMetric: 28.0382 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 796/1000
2023-10-30 22:05:35.936 
Epoch 796/1000 
	 loss: 27.5206, MinusLogProbMetric: 27.5206, val_loss: 28.0497, val_MinusLogProbMetric: 28.0497

Epoch 796: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5206 - MinusLogProbMetric: 27.5206 - val_loss: 28.0497 - val_MinusLogProbMetric: 28.0497 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 797/1000
2023-10-30 22:06:08.704 
Epoch 797/1000 
	 loss: 27.5201, MinusLogProbMetric: 27.5201, val_loss: 28.0759, val_MinusLogProbMetric: 28.0759

Epoch 797: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5201 - MinusLogProbMetric: 27.5201 - val_loss: 28.0759 - val_MinusLogProbMetric: 28.0759 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 798/1000
2023-10-30 22:06:41.085 
Epoch 798/1000 
	 loss: 27.5210, MinusLogProbMetric: 27.5210, val_loss: 28.0659, val_MinusLogProbMetric: 28.0659

Epoch 798: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5210 - MinusLogProbMetric: 27.5210 - val_loss: 28.0659 - val_MinusLogProbMetric: 28.0659 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 799/1000
2023-10-30 22:07:14.128 
Epoch 799/1000 
	 loss: 27.5148, MinusLogProbMetric: 27.5148, val_loss: 28.0300, val_MinusLogProbMetric: 28.0300

Epoch 799: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5148 - MinusLogProbMetric: 27.5148 - val_loss: 28.0300 - val_MinusLogProbMetric: 28.0300 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 800/1000
2023-10-30 22:07:46.888 
Epoch 800/1000 
	 loss: 27.5200, MinusLogProbMetric: 27.5200, val_loss: 28.0346, val_MinusLogProbMetric: 28.0346

Epoch 800: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5200 - MinusLogProbMetric: 27.5200 - val_loss: 28.0346 - val_MinusLogProbMetric: 28.0346 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 801/1000
2023-10-30 22:08:19.771 
Epoch 801/1000 
	 loss: 27.5193, MinusLogProbMetric: 27.5193, val_loss: 28.0337, val_MinusLogProbMetric: 28.0337

Epoch 801: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5193 - MinusLogProbMetric: 27.5193 - val_loss: 28.0337 - val_MinusLogProbMetric: 28.0337 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 802/1000
2023-10-30 22:08:52.912 
Epoch 802/1000 
	 loss: 27.5156, MinusLogProbMetric: 27.5156, val_loss: 28.0291, val_MinusLogProbMetric: 28.0291

Epoch 802: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5156 - MinusLogProbMetric: 27.5156 - val_loss: 28.0291 - val_MinusLogProbMetric: 28.0291 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 803/1000
2023-10-30 22:09:24.608 
Epoch 803/1000 
	 loss: 27.5177, MinusLogProbMetric: 27.5177, val_loss: 28.0652, val_MinusLogProbMetric: 28.0652

Epoch 803: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5177 - MinusLogProbMetric: 27.5177 - val_loss: 28.0652 - val_MinusLogProbMetric: 28.0652 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 804/1000
2023-10-30 22:09:56.285 
Epoch 804/1000 
	 loss: 27.5181, MinusLogProbMetric: 27.5181, val_loss: 28.0599, val_MinusLogProbMetric: 28.0599

Epoch 804: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5181 - MinusLogProbMetric: 27.5181 - val_loss: 28.0599 - val_MinusLogProbMetric: 28.0599 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 805/1000
2023-10-30 22:10:29.396 
Epoch 805/1000 
	 loss: 27.5215, MinusLogProbMetric: 27.5215, val_loss: 28.0415, val_MinusLogProbMetric: 28.0415

Epoch 805: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5215 - MinusLogProbMetric: 27.5215 - val_loss: 28.0415 - val_MinusLogProbMetric: 28.0415 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 806/1000
2023-10-30 22:11:03.851 
Epoch 806/1000 
	 loss: 27.5174, MinusLogProbMetric: 27.5174, val_loss: 28.0425, val_MinusLogProbMetric: 28.0425

Epoch 806: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5174 - MinusLogProbMetric: 27.5174 - val_loss: 28.0425 - val_MinusLogProbMetric: 28.0425 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 807/1000
2023-10-30 22:11:37.296 
Epoch 807/1000 
	 loss: 27.5164, MinusLogProbMetric: 27.5164, val_loss: 28.0316, val_MinusLogProbMetric: 28.0316

Epoch 807: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5164 - MinusLogProbMetric: 27.5164 - val_loss: 28.0316 - val_MinusLogProbMetric: 28.0316 - lr: 1.2500e-04 - 33s/epoch - 171ms/step
Epoch 808/1000
2023-10-30 22:12:10.459 
Epoch 808/1000 
	 loss: 27.5158, MinusLogProbMetric: 27.5158, val_loss: 28.0532, val_MinusLogProbMetric: 28.0532

Epoch 808: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5158 - MinusLogProbMetric: 27.5158 - val_loss: 28.0532 - val_MinusLogProbMetric: 28.0532 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 809/1000
2023-10-30 22:12:44.061 
Epoch 809/1000 
	 loss: 27.5138, MinusLogProbMetric: 27.5138, val_loss: 28.0244, val_MinusLogProbMetric: 28.0244

Epoch 809: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5138 - MinusLogProbMetric: 27.5138 - val_loss: 28.0244 - val_MinusLogProbMetric: 28.0244 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 810/1000
2023-10-30 22:13:18.217 
Epoch 810/1000 
	 loss: 27.5201, MinusLogProbMetric: 27.5201, val_loss: 28.0529, val_MinusLogProbMetric: 28.0529

Epoch 810: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5201 - MinusLogProbMetric: 27.5201 - val_loss: 28.0529 - val_MinusLogProbMetric: 28.0529 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 811/1000
2023-10-30 22:13:51.936 
Epoch 811/1000 
	 loss: 27.5193, MinusLogProbMetric: 27.5193, val_loss: 28.0379, val_MinusLogProbMetric: 28.0379

Epoch 811: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5193 - MinusLogProbMetric: 27.5193 - val_loss: 28.0379 - val_MinusLogProbMetric: 28.0379 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 812/1000
2023-10-30 22:14:24.781 
Epoch 812/1000 
	 loss: 27.5143, MinusLogProbMetric: 27.5143, val_loss: 28.0294, val_MinusLogProbMetric: 28.0294

Epoch 812: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5143 - MinusLogProbMetric: 27.5143 - val_loss: 28.0294 - val_MinusLogProbMetric: 28.0294 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 813/1000
2023-10-30 22:14:56.939 
Epoch 813/1000 
	 loss: 27.5168, MinusLogProbMetric: 27.5168, val_loss: 28.0394, val_MinusLogProbMetric: 28.0394

Epoch 813: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5168 - MinusLogProbMetric: 27.5168 - val_loss: 28.0394 - val_MinusLogProbMetric: 28.0394 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 814/1000
2023-10-30 22:15:29.867 
Epoch 814/1000 
	 loss: 27.5179, MinusLogProbMetric: 27.5179, val_loss: 28.0274, val_MinusLogProbMetric: 28.0274

Epoch 814: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5179 - MinusLogProbMetric: 27.5179 - val_loss: 28.0274 - val_MinusLogProbMetric: 28.0274 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 815/1000
2023-10-30 22:16:02.394 
Epoch 815/1000 
	 loss: 27.5111, MinusLogProbMetric: 27.5111, val_loss: 28.0483, val_MinusLogProbMetric: 28.0483

Epoch 815: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5111 - MinusLogProbMetric: 27.5111 - val_loss: 28.0483 - val_MinusLogProbMetric: 28.0483 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 816/1000
2023-10-30 22:16:35.665 
Epoch 816/1000 
	 loss: 27.5163, MinusLogProbMetric: 27.5163, val_loss: 28.0493, val_MinusLogProbMetric: 28.0493

Epoch 816: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5163 - MinusLogProbMetric: 27.5163 - val_loss: 28.0493 - val_MinusLogProbMetric: 28.0493 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 817/1000
2023-10-30 22:17:08.114 
Epoch 817/1000 
	 loss: 27.5115, MinusLogProbMetric: 27.5115, val_loss: 28.0207, val_MinusLogProbMetric: 28.0207

Epoch 817: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5115 - MinusLogProbMetric: 27.5115 - val_loss: 28.0207 - val_MinusLogProbMetric: 28.0207 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 818/1000
2023-10-30 22:17:39.247 
Epoch 818/1000 
	 loss: 27.5152, MinusLogProbMetric: 27.5152, val_loss: 28.0277, val_MinusLogProbMetric: 28.0277

Epoch 818: val_loss did not improve from 28.01867
196/196 - 31s - loss: 27.5152 - MinusLogProbMetric: 27.5152 - val_loss: 28.0277 - val_MinusLogProbMetric: 28.0277 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 819/1000
2023-10-30 22:18:12.093 
Epoch 819/1000 
	 loss: 27.5159, MinusLogProbMetric: 27.5159, val_loss: 28.0483, val_MinusLogProbMetric: 28.0483

Epoch 819: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5159 - MinusLogProbMetric: 27.5159 - val_loss: 28.0483 - val_MinusLogProbMetric: 28.0483 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 820/1000
2023-10-30 22:18:45.934 
Epoch 820/1000 
	 loss: 27.5144, MinusLogProbMetric: 27.5144, val_loss: 28.0361, val_MinusLogProbMetric: 28.0361

Epoch 820: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5144 - MinusLogProbMetric: 27.5144 - val_loss: 28.0361 - val_MinusLogProbMetric: 28.0361 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 821/1000
2023-10-30 22:19:19.583 
Epoch 821/1000 
	 loss: 27.5166, MinusLogProbMetric: 27.5166, val_loss: 28.0692, val_MinusLogProbMetric: 28.0692

Epoch 821: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5166 - MinusLogProbMetric: 27.5166 - val_loss: 28.0692 - val_MinusLogProbMetric: 28.0692 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 822/1000
2023-10-30 22:19:52.495 
Epoch 822/1000 
	 loss: 27.5176, MinusLogProbMetric: 27.5176, val_loss: 28.0286, val_MinusLogProbMetric: 28.0286

Epoch 822: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5176 - MinusLogProbMetric: 27.5176 - val_loss: 28.0286 - val_MinusLogProbMetric: 28.0286 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 823/1000
2023-10-30 22:20:25.441 
Epoch 823/1000 
	 loss: 27.5123, MinusLogProbMetric: 27.5123, val_loss: 28.0334, val_MinusLogProbMetric: 28.0334

Epoch 823: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5123 - MinusLogProbMetric: 27.5123 - val_loss: 28.0334 - val_MinusLogProbMetric: 28.0334 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 824/1000
2023-10-30 22:20:58.821 
Epoch 824/1000 
	 loss: 27.5172, MinusLogProbMetric: 27.5172, val_loss: 28.0381, val_MinusLogProbMetric: 28.0381

Epoch 824: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5172 - MinusLogProbMetric: 27.5172 - val_loss: 28.0381 - val_MinusLogProbMetric: 28.0381 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 825/1000
2023-10-30 22:21:32.380 
Epoch 825/1000 
	 loss: 27.5139, MinusLogProbMetric: 27.5139, val_loss: 28.0403, val_MinusLogProbMetric: 28.0403

Epoch 825: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5139 - MinusLogProbMetric: 27.5139 - val_loss: 28.0403 - val_MinusLogProbMetric: 28.0403 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 826/1000
2023-10-30 22:22:06.831 
Epoch 826/1000 
	 loss: 27.5137, MinusLogProbMetric: 27.5137, val_loss: 28.0415, val_MinusLogProbMetric: 28.0415

Epoch 826: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5137 - MinusLogProbMetric: 27.5137 - val_loss: 28.0415 - val_MinusLogProbMetric: 28.0415 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 827/1000
2023-10-30 22:22:38.238 
Epoch 827/1000 
	 loss: 27.5164, MinusLogProbMetric: 27.5164, val_loss: 28.0570, val_MinusLogProbMetric: 28.0570

Epoch 827: val_loss did not improve from 28.01867
196/196 - 31s - loss: 27.5164 - MinusLogProbMetric: 27.5164 - val_loss: 28.0570 - val_MinusLogProbMetric: 28.0570 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 828/1000
2023-10-30 22:23:10.033 
Epoch 828/1000 
	 loss: 27.5143, MinusLogProbMetric: 27.5143, val_loss: 28.0429, val_MinusLogProbMetric: 28.0429

Epoch 828: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5143 - MinusLogProbMetric: 27.5143 - val_loss: 28.0429 - val_MinusLogProbMetric: 28.0429 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 829/1000
2023-10-30 22:23:43.601 
Epoch 829/1000 
	 loss: 27.5131, MinusLogProbMetric: 27.5131, val_loss: 28.0556, val_MinusLogProbMetric: 28.0556

Epoch 829: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5131 - MinusLogProbMetric: 27.5131 - val_loss: 28.0556 - val_MinusLogProbMetric: 28.0556 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 830/1000
2023-10-30 22:24:17.629 
Epoch 830/1000 
	 loss: 27.5136, MinusLogProbMetric: 27.5136, val_loss: 28.0263, val_MinusLogProbMetric: 28.0263

Epoch 830: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5136 - MinusLogProbMetric: 27.5136 - val_loss: 28.0263 - val_MinusLogProbMetric: 28.0263 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 831/1000
2023-10-30 22:24:50.818 
Epoch 831/1000 
	 loss: 27.5178, MinusLogProbMetric: 27.5178, val_loss: 28.0244, val_MinusLogProbMetric: 28.0244

Epoch 831: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5178 - MinusLogProbMetric: 27.5178 - val_loss: 28.0244 - val_MinusLogProbMetric: 28.0244 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 832/1000
2023-10-30 22:25:24.503 
Epoch 832/1000 
	 loss: 27.5163, MinusLogProbMetric: 27.5163, val_loss: 28.0288, val_MinusLogProbMetric: 28.0288

Epoch 832: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5163 - MinusLogProbMetric: 27.5163 - val_loss: 28.0288 - val_MinusLogProbMetric: 28.0288 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 833/1000
2023-10-30 22:25:57.405 
Epoch 833/1000 
	 loss: 27.5148, MinusLogProbMetric: 27.5148, val_loss: 28.0321, val_MinusLogProbMetric: 28.0321

Epoch 833: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5148 - MinusLogProbMetric: 27.5148 - val_loss: 28.0321 - val_MinusLogProbMetric: 28.0321 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 834/1000
2023-10-30 22:26:29.695 
Epoch 834/1000 
	 loss: 27.5129, MinusLogProbMetric: 27.5129, val_loss: 28.0708, val_MinusLogProbMetric: 28.0708

Epoch 834: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5129 - MinusLogProbMetric: 27.5129 - val_loss: 28.0708 - val_MinusLogProbMetric: 28.0708 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 835/1000
2023-10-30 22:27:03.542 
Epoch 835/1000 
	 loss: 27.5113, MinusLogProbMetric: 27.5113, val_loss: 28.0417, val_MinusLogProbMetric: 28.0417

Epoch 835: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5113 - MinusLogProbMetric: 27.5113 - val_loss: 28.0417 - val_MinusLogProbMetric: 28.0417 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 836/1000
2023-10-30 22:27:36.663 
Epoch 836/1000 
	 loss: 27.5104, MinusLogProbMetric: 27.5104, val_loss: 28.0253, val_MinusLogProbMetric: 28.0253

Epoch 836: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5104 - MinusLogProbMetric: 27.5104 - val_loss: 28.0253 - val_MinusLogProbMetric: 28.0253 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 837/1000
2023-10-30 22:28:10.352 
Epoch 837/1000 
	 loss: 27.5096, MinusLogProbMetric: 27.5096, val_loss: 28.0587, val_MinusLogProbMetric: 28.0587

Epoch 837: val_loss did not improve from 28.01867
196/196 - 34s - loss: 27.5096 - MinusLogProbMetric: 27.5096 - val_loss: 28.0587 - val_MinusLogProbMetric: 28.0587 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 838/1000
2023-10-30 22:28:42.446 
Epoch 838/1000 
	 loss: 27.5111, MinusLogProbMetric: 27.5111, val_loss: 28.0427, val_MinusLogProbMetric: 28.0427

Epoch 838: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.5111 - MinusLogProbMetric: 27.5111 - val_loss: 28.0427 - val_MinusLogProbMetric: 28.0427 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 839/1000
2023-10-30 22:29:15.550 
Epoch 839/1000 
	 loss: 27.5120, MinusLogProbMetric: 27.5120, val_loss: 28.0233, val_MinusLogProbMetric: 28.0233

Epoch 839: val_loss did not improve from 28.01867
196/196 - 33s - loss: 27.5120 - MinusLogProbMetric: 27.5120 - val_loss: 28.0233 - val_MinusLogProbMetric: 28.0233 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 840/1000
2023-10-30 22:29:47.820 
Epoch 840/1000 
	 loss: 27.4816, MinusLogProbMetric: 27.4816, val_loss: 28.0193, val_MinusLogProbMetric: 28.0193

Epoch 840: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.4816 - MinusLogProbMetric: 27.4816 - val_loss: 28.0193 - val_MinusLogProbMetric: 28.0193 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 841/1000
2023-10-30 22:30:19.441 
Epoch 841/1000 
	 loss: 27.4832, MinusLogProbMetric: 27.4832, val_loss: 28.0230, val_MinusLogProbMetric: 28.0230

Epoch 841: val_loss did not improve from 28.01867
196/196 - 32s - loss: 27.4832 - MinusLogProbMetric: 27.4832 - val_loss: 28.0230 - val_MinusLogProbMetric: 28.0230 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 842/1000
2023-10-30 22:30:51.053 
Epoch 842/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.0059, val_MinusLogProbMetric: 28.0059

Epoch 842: val_loss improved from 28.01867 to 28.00589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.0059 - val_MinusLogProbMetric: 28.0059 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 843/1000
2023-10-30 22:31:24.049 
Epoch 843/1000 
	 loss: 27.4819, MinusLogProbMetric: 27.4819, val_loss: 28.0078, val_MinusLogProbMetric: 28.0078

Epoch 843: val_loss did not improve from 28.00589
196/196 - 33s - loss: 27.4819 - MinusLogProbMetric: 27.4819 - val_loss: 28.0078 - val_MinusLogProbMetric: 28.0078 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 844/1000
2023-10-30 22:31:57.676 
Epoch 844/1000 
	 loss: 27.4845, MinusLogProbMetric: 27.4845, val_loss: 28.0087, val_MinusLogProbMetric: 28.0087

Epoch 844: val_loss did not improve from 28.00589
196/196 - 34s - loss: 27.4845 - MinusLogProbMetric: 27.4845 - val_loss: 28.0087 - val_MinusLogProbMetric: 28.0087 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 845/1000
2023-10-30 22:32:31.684 
Epoch 845/1000 
	 loss: 27.4822, MinusLogProbMetric: 27.4822, val_loss: 28.0180, val_MinusLogProbMetric: 28.0180

Epoch 845: val_loss did not improve from 28.00589
196/196 - 34s - loss: 27.4822 - MinusLogProbMetric: 27.4822 - val_loss: 28.0180 - val_MinusLogProbMetric: 28.0180 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 846/1000
2023-10-30 22:33:04.108 
Epoch 846/1000 
	 loss: 27.4852, MinusLogProbMetric: 27.4852, val_loss: 28.0161, val_MinusLogProbMetric: 28.0161

Epoch 846: val_loss did not improve from 28.00589
196/196 - 32s - loss: 27.4852 - MinusLogProbMetric: 27.4852 - val_loss: 28.0161 - val_MinusLogProbMetric: 28.0161 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 847/1000
2023-10-30 22:33:37.271 
Epoch 847/1000 
	 loss: 27.4830, MinusLogProbMetric: 27.4830, val_loss: 28.0156, val_MinusLogProbMetric: 28.0156

Epoch 847: val_loss did not improve from 28.00589
196/196 - 33s - loss: 27.4830 - MinusLogProbMetric: 27.4830 - val_loss: 28.0156 - val_MinusLogProbMetric: 28.0156 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 848/1000
2023-10-30 22:34:11.101 
Epoch 848/1000 
	 loss: 27.4835, MinusLogProbMetric: 27.4835, val_loss: 28.0093, val_MinusLogProbMetric: 28.0093

Epoch 848: val_loss did not improve from 28.00589
196/196 - 34s - loss: 27.4835 - MinusLogProbMetric: 27.4835 - val_loss: 28.0093 - val_MinusLogProbMetric: 28.0093 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 849/1000
2023-10-30 22:34:44.578 
Epoch 849/1000 
	 loss: 27.4842, MinusLogProbMetric: 27.4842, val_loss: 28.0128, val_MinusLogProbMetric: 28.0128

Epoch 849: val_loss did not improve from 28.00589
196/196 - 33s - loss: 27.4842 - MinusLogProbMetric: 27.4842 - val_loss: 28.0128 - val_MinusLogProbMetric: 28.0128 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 850/1000
2023-10-30 22:35:18.379 
Epoch 850/1000 
	 loss: 27.4835, MinusLogProbMetric: 27.4835, val_loss: 28.0061, val_MinusLogProbMetric: 28.0061

Epoch 850: val_loss did not improve from 28.00589
196/196 - 34s - loss: 27.4835 - MinusLogProbMetric: 27.4835 - val_loss: 28.0061 - val_MinusLogProbMetric: 28.0061 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 851/1000
2023-10-30 22:35:52.880 
Epoch 851/1000 
	 loss: 27.4841, MinusLogProbMetric: 27.4841, val_loss: 28.0165, val_MinusLogProbMetric: 28.0165

Epoch 851: val_loss did not improve from 28.00589
196/196 - 34s - loss: 27.4841 - MinusLogProbMetric: 27.4841 - val_loss: 28.0165 - val_MinusLogProbMetric: 28.0165 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 852/1000
2023-10-30 22:36:25.651 
Epoch 852/1000 
	 loss: 27.4842, MinusLogProbMetric: 27.4842, val_loss: 28.0101, val_MinusLogProbMetric: 28.0101

Epoch 852: val_loss did not improve from 28.00589
196/196 - 33s - loss: 27.4842 - MinusLogProbMetric: 27.4842 - val_loss: 28.0101 - val_MinusLogProbMetric: 28.0101 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 853/1000
2023-10-30 22:36:59.315 
Epoch 853/1000 
	 loss: 27.4818, MinusLogProbMetric: 27.4818, val_loss: 28.0175, val_MinusLogProbMetric: 28.0175

Epoch 853: val_loss did not improve from 28.00589
196/196 - 34s - loss: 27.4818 - MinusLogProbMetric: 27.4818 - val_loss: 28.0175 - val_MinusLogProbMetric: 28.0175 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 854/1000
2023-10-30 22:37:31.876 
Epoch 854/1000 
	 loss: 27.4826, MinusLogProbMetric: 27.4826, val_loss: 28.0158, val_MinusLogProbMetric: 28.0158

Epoch 854: val_loss did not improve from 28.00589
196/196 - 33s - loss: 27.4826 - MinusLogProbMetric: 27.4826 - val_loss: 28.0158 - val_MinusLogProbMetric: 28.0158 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 855/1000
2023-10-30 22:38:05.595 
Epoch 855/1000 
	 loss: 27.4831, MinusLogProbMetric: 27.4831, val_loss: 28.0052, val_MinusLogProbMetric: 28.0052

Epoch 855: val_loss improved from 28.00589 to 28.00524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 27.4831 - MinusLogProbMetric: 27.4831 - val_loss: 28.0052 - val_MinusLogProbMetric: 28.0052 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 856/1000
2023-10-30 22:38:39.228 
Epoch 856/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 28.0102, val_MinusLogProbMetric: 28.0102

Epoch 856: val_loss did not improve from 28.00524
196/196 - 33s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 28.0102 - val_MinusLogProbMetric: 28.0102 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 857/1000
2023-10-30 22:39:12.725 
Epoch 857/1000 
	 loss: 27.4810, MinusLogProbMetric: 27.4810, val_loss: 28.0178, val_MinusLogProbMetric: 28.0178

Epoch 857: val_loss did not improve from 28.00524
196/196 - 33s - loss: 27.4810 - MinusLogProbMetric: 27.4810 - val_loss: 28.0178 - val_MinusLogProbMetric: 28.0178 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 858/1000
2023-10-30 22:39:46.005 
Epoch 858/1000 
	 loss: 27.4846, MinusLogProbMetric: 27.4846, val_loss: 28.0191, val_MinusLogProbMetric: 28.0191

Epoch 858: val_loss did not improve from 28.00524
196/196 - 33s - loss: 27.4846 - MinusLogProbMetric: 27.4846 - val_loss: 28.0191 - val_MinusLogProbMetric: 28.0191 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 859/1000
2023-10-30 22:40:19.111 
Epoch 859/1000 
	 loss: 27.4840, MinusLogProbMetric: 27.4840, val_loss: 28.0152, val_MinusLogProbMetric: 28.0152

Epoch 859: val_loss did not improve from 28.00524
196/196 - 33s - loss: 27.4840 - MinusLogProbMetric: 27.4840 - val_loss: 28.0152 - val_MinusLogProbMetric: 28.0152 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 860/1000
2023-10-30 22:40:53.425 
Epoch 860/1000 
	 loss: 27.4837, MinusLogProbMetric: 27.4837, val_loss: 28.0132, val_MinusLogProbMetric: 28.0132

Epoch 860: val_loss did not improve from 28.00524
196/196 - 34s - loss: 27.4837 - MinusLogProbMetric: 27.4837 - val_loss: 28.0132 - val_MinusLogProbMetric: 28.0132 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 861/1000
2023-10-30 22:41:25.963 
Epoch 861/1000 
	 loss: 27.4839, MinusLogProbMetric: 27.4839, val_loss: 28.0019, val_MinusLogProbMetric: 28.0019

Epoch 861: val_loss improved from 28.00524 to 28.00187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 27.4839 - MinusLogProbMetric: 27.4839 - val_loss: 28.0019 - val_MinusLogProbMetric: 28.0019 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 862/1000
2023-10-30 22:41:58.957 
Epoch 862/1000 
	 loss: 27.4817, MinusLogProbMetric: 27.4817, val_loss: 28.0135, val_MinusLogProbMetric: 28.0135

Epoch 862: val_loss did not improve from 28.00187
196/196 - 33s - loss: 27.4817 - MinusLogProbMetric: 27.4817 - val_loss: 28.0135 - val_MinusLogProbMetric: 28.0135 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 863/1000
2023-10-30 22:42:31.791 
Epoch 863/1000 
	 loss: 27.4827, MinusLogProbMetric: 27.4827, val_loss: 28.0242, val_MinusLogProbMetric: 28.0242

Epoch 863: val_loss did not improve from 28.00187
196/196 - 33s - loss: 27.4827 - MinusLogProbMetric: 27.4827 - val_loss: 28.0242 - val_MinusLogProbMetric: 28.0242 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 864/1000
2023-10-30 22:43:03.953 
Epoch 864/1000 
	 loss: 27.4849, MinusLogProbMetric: 27.4849, val_loss: 28.0366, val_MinusLogProbMetric: 28.0366

Epoch 864: val_loss did not improve from 28.00187
196/196 - 32s - loss: 27.4849 - MinusLogProbMetric: 27.4849 - val_loss: 28.0366 - val_MinusLogProbMetric: 28.0366 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 865/1000
2023-10-30 22:43:36.607 
Epoch 865/1000 
	 loss: 27.4823, MinusLogProbMetric: 27.4823, val_loss: 28.0270, val_MinusLogProbMetric: 28.0270

Epoch 865: val_loss did not improve from 28.00187
196/196 - 33s - loss: 27.4823 - MinusLogProbMetric: 27.4823 - val_loss: 28.0270 - val_MinusLogProbMetric: 28.0270 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 866/1000
2023-10-30 22:44:09.891 
Epoch 866/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 28.0005, val_MinusLogProbMetric: 28.0005

Epoch 866: val_loss improved from 28.00187 to 28.00051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 28.0005 - val_MinusLogProbMetric: 28.0005 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 867/1000
2023-10-30 22:44:42.492 
Epoch 867/1000 
	 loss: 27.4818, MinusLogProbMetric: 27.4818, val_loss: 28.0255, val_MinusLogProbMetric: 28.0255

Epoch 867: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4818 - MinusLogProbMetric: 27.4818 - val_loss: 28.0255 - val_MinusLogProbMetric: 28.0255 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 868/1000
2023-10-30 22:45:16.095 
Epoch 868/1000 
	 loss: 27.4830, MinusLogProbMetric: 27.4830, val_loss: 28.0268, val_MinusLogProbMetric: 28.0268

Epoch 868: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4830 - MinusLogProbMetric: 27.4830 - val_loss: 28.0268 - val_MinusLogProbMetric: 28.0268 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 869/1000
2023-10-30 22:45:49.773 
Epoch 869/1000 
	 loss: 27.4849, MinusLogProbMetric: 27.4849, val_loss: 28.0094, val_MinusLogProbMetric: 28.0094

Epoch 869: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4849 - MinusLogProbMetric: 27.4849 - val_loss: 28.0094 - val_MinusLogProbMetric: 28.0094 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 870/1000
2023-10-30 22:46:22.491 
Epoch 870/1000 
	 loss: 27.4832, MinusLogProbMetric: 27.4832, val_loss: 28.0202, val_MinusLogProbMetric: 28.0202

Epoch 870: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4832 - MinusLogProbMetric: 27.4832 - val_loss: 28.0202 - val_MinusLogProbMetric: 28.0202 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 871/1000
2023-10-30 22:46:54.151 
Epoch 871/1000 
	 loss: 27.4821, MinusLogProbMetric: 27.4821, val_loss: 28.0101, val_MinusLogProbMetric: 28.0101

Epoch 871: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4821 - MinusLogProbMetric: 27.4821 - val_loss: 28.0101 - val_MinusLogProbMetric: 28.0101 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 872/1000
2023-10-30 22:47:28.048 
Epoch 872/1000 
	 loss: 27.4803, MinusLogProbMetric: 27.4803, val_loss: 28.0162, val_MinusLogProbMetric: 28.0162

Epoch 872: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4803 - MinusLogProbMetric: 27.4803 - val_loss: 28.0162 - val_MinusLogProbMetric: 28.0162 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 873/1000
2023-10-30 22:48:01.657 
Epoch 873/1000 
	 loss: 27.4856, MinusLogProbMetric: 27.4856, val_loss: 28.0096, val_MinusLogProbMetric: 28.0096

Epoch 873: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4856 - MinusLogProbMetric: 27.4856 - val_loss: 28.0096 - val_MinusLogProbMetric: 28.0096 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 874/1000
2023-10-30 22:48:34.511 
Epoch 874/1000 
	 loss: 27.4818, MinusLogProbMetric: 27.4818, val_loss: 28.0163, val_MinusLogProbMetric: 28.0163

Epoch 874: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4818 - MinusLogProbMetric: 27.4818 - val_loss: 28.0163 - val_MinusLogProbMetric: 28.0163 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 875/1000
2023-10-30 22:49:06.915 
Epoch 875/1000 
	 loss: 27.4818, MinusLogProbMetric: 27.4818, val_loss: 28.0112, val_MinusLogProbMetric: 28.0112

Epoch 875: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4818 - MinusLogProbMetric: 27.4818 - val_loss: 28.0112 - val_MinusLogProbMetric: 28.0112 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 876/1000
2023-10-30 22:49:40.787 
Epoch 876/1000 
	 loss: 27.4819, MinusLogProbMetric: 27.4819, val_loss: 28.0155, val_MinusLogProbMetric: 28.0155

Epoch 876: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4819 - MinusLogProbMetric: 27.4819 - val_loss: 28.0155 - val_MinusLogProbMetric: 28.0155 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 877/1000
2023-10-30 22:50:13.945 
Epoch 877/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 28.0078, val_MinusLogProbMetric: 28.0078

Epoch 877: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 28.0078 - val_MinusLogProbMetric: 28.0078 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 878/1000
2023-10-30 22:50:47.208 
Epoch 878/1000 
	 loss: 27.4835, MinusLogProbMetric: 27.4835, val_loss: 28.0140, val_MinusLogProbMetric: 28.0140

Epoch 878: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4835 - MinusLogProbMetric: 27.4835 - val_loss: 28.0140 - val_MinusLogProbMetric: 28.0140 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 879/1000
2023-10-30 22:51:20.034 
Epoch 879/1000 
	 loss: 27.4808, MinusLogProbMetric: 27.4808, val_loss: 28.0007, val_MinusLogProbMetric: 28.0007

Epoch 879: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4808 - MinusLogProbMetric: 27.4808 - val_loss: 28.0007 - val_MinusLogProbMetric: 28.0007 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 880/1000
2023-10-30 22:51:54.411 
Epoch 880/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 28.0197, val_MinusLogProbMetric: 28.0197

Epoch 880: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 28.0197 - val_MinusLogProbMetric: 28.0197 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 881/1000
2023-10-30 22:52:29.016 
Epoch 881/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 28.0043, val_MinusLogProbMetric: 28.0043

Epoch 881: val_loss did not improve from 28.00051
196/196 - 35s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 28.0043 - val_MinusLogProbMetric: 28.0043 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 882/1000
2023-10-30 22:53:02.625 
Epoch 882/1000 
	 loss: 27.4811, MinusLogProbMetric: 27.4811, val_loss: 28.0260, val_MinusLogProbMetric: 28.0260

Epoch 882: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4811 - MinusLogProbMetric: 27.4811 - val_loss: 28.0260 - val_MinusLogProbMetric: 28.0260 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 883/1000
2023-10-30 22:53:36.497 
Epoch 883/1000 
	 loss: 27.4837, MinusLogProbMetric: 27.4837, val_loss: 28.0029, val_MinusLogProbMetric: 28.0029

Epoch 883: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4837 - MinusLogProbMetric: 27.4837 - val_loss: 28.0029 - val_MinusLogProbMetric: 28.0029 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 884/1000
2023-10-30 22:54:09.383 
Epoch 884/1000 
	 loss: 27.4807, MinusLogProbMetric: 27.4807, val_loss: 28.0096, val_MinusLogProbMetric: 28.0096

Epoch 884: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4807 - MinusLogProbMetric: 27.4807 - val_loss: 28.0096 - val_MinusLogProbMetric: 28.0096 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 885/1000
2023-10-30 22:54:42.082 
Epoch 885/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 28.0052, val_MinusLogProbMetric: 28.0052

Epoch 885: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 28.0052 - val_MinusLogProbMetric: 28.0052 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 886/1000
2023-10-30 22:55:16.364 
Epoch 886/1000 
	 loss: 27.4805, MinusLogProbMetric: 27.4805, val_loss: 28.0093, val_MinusLogProbMetric: 28.0093

Epoch 886: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4805 - MinusLogProbMetric: 27.4805 - val_loss: 28.0093 - val_MinusLogProbMetric: 28.0093 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 887/1000
2023-10-30 22:55:48.666 
Epoch 887/1000 
	 loss: 27.4813, MinusLogProbMetric: 27.4813, val_loss: 28.0016, val_MinusLogProbMetric: 28.0016

Epoch 887: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4813 - MinusLogProbMetric: 27.4813 - val_loss: 28.0016 - val_MinusLogProbMetric: 28.0016 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 888/1000
2023-10-30 22:56:20.448 
Epoch 888/1000 
	 loss: 27.4840, MinusLogProbMetric: 27.4840, val_loss: 28.0218, val_MinusLogProbMetric: 28.0218

Epoch 888: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4840 - MinusLogProbMetric: 27.4840 - val_loss: 28.0218 - val_MinusLogProbMetric: 28.0218 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 889/1000
2023-10-30 22:56:53.353 
Epoch 889/1000 
	 loss: 27.4814, MinusLogProbMetric: 27.4814, val_loss: 28.0183, val_MinusLogProbMetric: 28.0183

Epoch 889: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4814 - MinusLogProbMetric: 27.4814 - val_loss: 28.0183 - val_MinusLogProbMetric: 28.0183 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 890/1000
2023-10-30 22:57:25.478 
Epoch 890/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.0182, val_MinusLogProbMetric: 28.0182

Epoch 890: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.0182 - val_MinusLogProbMetric: 28.0182 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 891/1000
2023-10-30 22:57:57.686 
Epoch 891/1000 
	 loss: 27.4818, MinusLogProbMetric: 27.4818, val_loss: 28.0113, val_MinusLogProbMetric: 28.0113

Epoch 891: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4818 - MinusLogProbMetric: 27.4818 - val_loss: 28.0113 - val_MinusLogProbMetric: 28.0113 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 892/1000
2023-10-30 22:58:30.485 
Epoch 892/1000 
	 loss: 27.4789, MinusLogProbMetric: 27.4789, val_loss: 28.0118, val_MinusLogProbMetric: 28.0118

Epoch 892: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4789 - MinusLogProbMetric: 27.4789 - val_loss: 28.0118 - val_MinusLogProbMetric: 28.0118 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 893/1000
2023-10-30 22:59:02.939 
Epoch 893/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 28.0112, val_MinusLogProbMetric: 28.0112

Epoch 893: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 28.0112 - val_MinusLogProbMetric: 28.0112 - lr: 6.2500e-05 - 32s/epoch - 166ms/step
Epoch 894/1000
2023-10-30 22:59:36.064 
Epoch 894/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 28.0151, val_MinusLogProbMetric: 28.0151

Epoch 894: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 28.0151 - val_MinusLogProbMetric: 28.0151 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 895/1000
2023-10-30 23:00:10.212 
Epoch 895/1000 
	 loss: 27.4816, MinusLogProbMetric: 27.4816, val_loss: 28.0183, val_MinusLogProbMetric: 28.0183

Epoch 895: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4816 - MinusLogProbMetric: 27.4816 - val_loss: 28.0183 - val_MinusLogProbMetric: 28.0183 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 896/1000
2023-10-30 23:00:43.610 
Epoch 896/1000 
	 loss: 27.4800, MinusLogProbMetric: 27.4800, val_loss: 28.0244, val_MinusLogProbMetric: 28.0244

Epoch 896: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4800 - MinusLogProbMetric: 27.4800 - val_loss: 28.0244 - val_MinusLogProbMetric: 28.0244 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 897/1000
2023-10-30 23:01:16.906 
Epoch 897/1000 
	 loss: 27.4828, MinusLogProbMetric: 27.4828, val_loss: 28.0167, val_MinusLogProbMetric: 28.0167

Epoch 897: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4828 - MinusLogProbMetric: 27.4828 - val_loss: 28.0167 - val_MinusLogProbMetric: 28.0167 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 898/1000
2023-10-30 23:01:50.224 
Epoch 898/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.0182, val_MinusLogProbMetric: 28.0182

Epoch 898: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.0182 - val_MinusLogProbMetric: 28.0182 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 899/1000
2023-10-30 23:02:24.352 
Epoch 899/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 28.0182, val_MinusLogProbMetric: 28.0182

Epoch 899: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 28.0182 - val_MinusLogProbMetric: 28.0182 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 900/1000
2023-10-30 23:02:56.210 
Epoch 900/1000 
	 loss: 27.4782, MinusLogProbMetric: 27.4782, val_loss: 28.0117, val_MinusLogProbMetric: 28.0117

Epoch 900: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4782 - MinusLogProbMetric: 27.4782 - val_loss: 28.0117 - val_MinusLogProbMetric: 28.0117 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 901/1000
2023-10-30 23:03:30.065 
Epoch 901/1000 
	 loss: 27.4789, MinusLogProbMetric: 27.4789, val_loss: 28.0090, val_MinusLogProbMetric: 28.0090

Epoch 901: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4789 - MinusLogProbMetric: 27.4789 - val_loss: 28.0090 - val_MinusLogProbMetric: 28.0090 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 902/1000
2023-10-30 23:04:03.723 
Epoch 902/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 28.0109, val_MinusLogProbMetric: 28.0109

Epoch 902: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 28.0109 - val_MinusLogProbMetric: 28.0109 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 903/1000
2023-10-30 23:04:37.336 
Epoch 903/1000 
	 loss: 27.4801, MinusLogProbMetric: 27.4801, val_loss: 28.0178, val_MinusLogProbMetric: 28.0178

Epoch 903: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4801 - MinusLogProbMetric: 27.4801 - val_loss: 28.0178 - val_MinusLogProbMetric: 28.0178 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 904/1000
2023-10-30 23:05:10.324 
Epoch 904/1000 
	 loss: 27.4792, MinusLogProbMetric: 27.4792, val_loss: 28.0047, val_MinusLogProbMetric: 28.0047

Epoch 904: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4792 - MinusLogProbMetric: 27.4792 - val_loss: 28.0047 - val_MinusLogProbMetric: 28.0047 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 905/1000
2023-10-30 23:05:45.291 
Epoch 905/1000 
	 loss: 27.4783, MinusLogProbMetric: 27.4783, val_loss: 28.0180, val_MinusLogProbMetric: 28.0180

Epoch 905: val_loss did not improve from 28.00051
196/196 - 35s - loss: 27.4783 - MinusLogProbMetric: 27.4783 - val_loss: 28.0180 - val_MinusLogProbMetric: 28.0180 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 906/1000
2023-10-30 23:06:19.322 
Epoch 906/1000 
	 loss: 27.4826, MinusLogProbMetric: 27.4826, val_loss: 28.0113, val_MinusLogProbMetric: 28.0113

Epoch 906: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4826 - MinusLogProbMetric: 27.4826 - val_loss: 28.0113 - val_MinusLogProbMetric: 28.0113 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 907/1000
2023-10-30 23:06:53.459 
Epoch 907/1000 
	 loss: 27.4812, MinusLogProbMetric: 27.4812, val_loss: 28.0141, val_MinusLogProbMetric: 28.0141

Epoch 907: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4812 - MinusLogProbMetric: 27.4812 - val_loss: 28.0141 - val_MinusLogProbMetric: 28.0141 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 908/1000
2023-10-30 23:07:26.951 
Epoch 908/1000 
	 loss: 27.4803, MinusLogProbMetric: 27.4803, val_loss: 28.0127, val_MinusLogProbMetric: 28.0127

Epoch 908: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4803 - MinusLogProbMetric: 27.4803 - val_loss: 28.0127 - val_MinusLogProbMetric: 28.0127 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 909/1000
2023-10-30 23:08:00.575 
Epoch 909/1000 
	 loss: 27.4811, MinusLogProbMetric: 27.4811, val_loss: 28.0187, val_MinusLogProbMetric: 28.0187

Epoch 909: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4811 - MinusLogProbMetric: 27.4811 - val_loss: 28.0187 - val_MinusLogProbMetric: 28.0187 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 910/1000
2023-10-30 23:08:34.466 
Epoch 910/1000 
	 loss: 27.4792, MinusLogProbMetric: 27.4792, val_loss: 28.0019, val_MinusLogProbMetric: 28.0019

Epoch 910: val_loss did not improve from 28.00051
196/196 - 34s - loss: 27.4792 - MinusLogProbMetric: 27.4792 - val_loss: 28.0019 - val_MinusLogProbMetric: 28.0019 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 911/1000
2023-10-30 23:09:07.790 
Epoch 911/1000 
	 loss: 27.4800, MinusLogProbMetric: 27.4800, val_loss: 28.0036, val_MinusLogProbMetric: 28.0036

Epoch 911: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4800 - MinusLogProbMetric: 27.4800 - val_loss: 28.0036 - val_MinusLogProbMetric: 28.0036 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 912/1000
2023-10-30 23:09:41.163 
Epoch 912/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 28.0127, val_MinusLogProbMetric: 28.0127

Epoch 912: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 28.0127 - val_MinusLogProbMetric: 28.0127 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 913/1000
2023-10-30 23:10:13.145 
Epoch 913/1000 
	 loss: 27.4816, MinusLogProbMetric: 27.4816, val_loss: 28.0268, val_MinusLogProbMetric: 28.0268

Epoch 913: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4816 - MinusLogProbMetric: 27.4816 - val_loss: 28.0268 - val_MinusLogProbMetric: 28.0268 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 914/1000
2023-10-30 23:10:46.259 
Epoch 914/1000 
	 loss: 27.4801, MinusLogProbMetric: 27.4801, val_loss: 28.0115, val_MinusLogProbMetric: 28.0115

Epoch 914: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4801 - MinusLogProbMetric: 27.4801 - val_loss: 28.0115 - val_MinusLogProbMetric: 28.0115 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 915/1000
2023-10-30 23:11:21.162 
Epoch 915/1000 
	 loss: 27.4827, MinusLogProbMetric: 27.4827, val_loss: 28.0148, val_MinusLogProbMetric: 28.0148

Epoch 915: val_loss did not improve from 28.00051
196/196 - 35s - loss: 27.4827 - MinusLogProbMetric: 27.4827 - val_loss: 28.0148 - val_MinusLogProbMetric: 28.0148 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 916/1000
2023-10-30 23:11:54.097 
Epoch 916/1000 
	 loss: 27.4800, MinusLogProbMetric: 27.4800, val_loss: 28.0159, val_MinusLogProbMetric: 28.0159

Epoch 916: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4800 - MinusLogProbMetric: 27.4800 - val_loss: 28.0159 - val_MinusLogProbMetric: 28.0159 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 917/1000
2023-10-30 23:12:25.929 
Epoch 917/1000 
	 loss: 27.4662, MinusLogProbMetric: 27.4662, val_loss: 28.0061, val_MinusLogProbMetric: 28.0061

Epoch 917: val_loss did not improve from 28.00051
196/196 - 32s - loss: 27.4662 - MinusLogProbMetric: 27.4662 - val_loss: 28.0061 - val_MinusLogProbMetric: 28.0061 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 918/1000
2023-10-30 23:12:59.253 
Epoch 918/1000 
	 loss: 27.4655, MinusLogProbMetric: 27.4655, val_loss: 28.0055, val_MinusLogProbMetric: 28.0055

Epoch 918: val_loss did not improve from 28.00051
196/196 - 33s - loss: 27.4655 - MinusLogProbMetric: 27.4655 - val_loss: 28.0055 - val_MinusLogProbMetric: 28.0055 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 919/1000
2023-10-30 23:13:32.102 
Epoch 919/1000 
	 loss: 27.4652, MinusLogProbMetric: 27.4652, val_loss: 27.9948, val_MinusLogProbMetric: 27.9948

Epoch 919: val_loss improved from 28.00051 to 27.99476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 27.4652 - MinusLogProbMetric: 27.4652 - val_loss: 27.9948 - val_MinusLogProbMetric: 27.9948 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 920/1000
2023-10-30 23:14:05.886 
Epoch 920/1000 
	 loss: 27.4652, MinusLogProbMetric: 27.4652, val_loss: 28.0045, val_MinusLogProbMetric: 28.0045

Epoch 920: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4652 - MinusLogProbMetric: 27.4652 - val_loss: 28.0045 - val_MinusLogProbMetric: 28.0045 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 921/1000
2023-10-30 23:14:39.597 
Epoch 921/1000 
	 loss: 27.4659, MinusLogProbMetric: 27.4659, val_loss: 28.0086, val_MinusLogProbMetric: 28.0086

Epoch 921: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4659 - MinusLogProbMetric: 27.4659 - val_loss: 28.0086 - val_MinusLogProbMetric: 28.0086 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 922/1000
2023-10-30 23:15:13.299 
Epoch 922/1000 
	 loss: 27.4658, MinusLogProbMetric: 27.4658, val_loss: 28.0021, val_MinusLogProbMetric: 28.0021

Epoch 922: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4658 - MinusLogProbMetric: 27.4658 - val_loss: 28.0021 - val_MinusLogProbMetric: 28.0021 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 923/1000
2023-10-30 23:15:45.847 
Epoch 923/1000 
	 loss: 27.4651, MinusLogProbMetric: 27.4651, val_loss: 28.0023, val_MinusLogProbMetric: 28.0023

Epoch 923: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4651 - MinusLogProbMetric: 27.4651 - val_loss: 28.0023 - val_MinusLogProbMetric: 28.0023 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 924/1000
2023-10-30 23:16:19.484 
Epoch 924/1000 
	 loss: 27.4647, MinusLogProbMetric: 27.4647, val_loss: 28.0000, val_MinusLogProbMetric: 28.0000

Epoch 924: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4647 - MinusLogProbMetric: 27.4647 - val_loss: 28.0000 - val_MinusLogProbMetric: 28.0000 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 925/1000
2023-10-30 23:16:53.671 
Epoch 925/1000 
	 loss: 27.4639, MinusLogProbMetric: 27.4639, val_loss: 28.0016, val_MinusLogProbMetric: 28.0016

Epoch 925: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4639 - MinusLogProbMetric: 27.4639 - val_loss: 28.0016 - val_MinusLogProbMetric: 28.0016 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 926/1000
2023-10-30 23:17:27.695 
Epoch 926/1000 
	 loss: 27.4654, MinusLogProbMetric: 27.4654, val_loss: 28.0014, val_MinusLogProbMetric: 28.0014

Epoch 926: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4654 - MinusLogProbMetric: 27.4654 - val_loss: 28.0014 - val_MinusLogProbMetric: 28.0014 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 927/1000
2023-10-30 23:18:00.311 
Epoch 927/1000 
	 loss: 27.4653, MinusLogProbMetric: 27.4653, val_loss: 28.0033, val_MinusLogProbMetric: 28.0033

Epoch 927: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4653 - MinusLogProbMetric: 27.4653 - val_loss: 28.0033 - val_MinusLogProbMetric: 28.0033 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 928/1000
2023-10-30 23:18:34.743 
Epoch 928/1000 
	 loss: 27.4645, MinusLogProbMetric: 27.4645, val_loss: 28.0042, val_MinusLogProbMetric: 28.0042

Epoch 928: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4645 - MinusLogProbMetric: 27.4645 - val_loss: 28.0042 - val_MinusLogProbMetric: 28.0042 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 929/1000
2023-10-30 23:19:07.786 
Epoch 929/1000 
	 loss: 27.4650, MinusLogProbMetric: 27.4650, val_loss: 27.9972, val_MinusLogProbMetric: 27.9972

Epoch 929: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4650 - MinusLogProbMetric: 27.4650 - val_loss: 27.9972 - val_MinusLogProbMetric: 27.9972 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 930/1000
2023-10-30 23:19:39.488 
Epoch 930/1000 
	 loss: 27.4652, MinusLogProbMetric: 27.4652, val_loss: 28.0046, val_MinusLogProbMetric: 28.0046

Epoch 930: val_loss did not improve from 27.99476
196/196 - 32s - loss: 27.4652 - MinusLogProbMetric: 27.4652 - val_loss: 28.0046 - val_MinusLogProbMetric: 28.0046 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 931/1000
2023-10-30 23:20:12.265 
Epoch 931/1000 
	 loss: 27.4657, MinusLogProbMetric: 27.4657, val_loss: 28.0051, val_MinusLogProbMetric: 28.0051

Epoch 931: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4657 - MinusLogProbMetric: 27.4657 - val_loss: 28.0051 - val_MinusLogProbMetric: 28.0051 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 932/1000
2023-10-30 23:20:45.284 
Epoch 932/1000 
	 loss: 27.4652, MinusLogProbMetric: 27.4652, val_loss: 28.0061, val_MinusLogProbMetric: 28.0061

Epoch 932: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4652 - MinusLogProbMetric: 27.4652 - val_loss: 28.0061 - val_MinusLogProbMetric: 28.0061 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 933/1000
2023-10-30 23:21:17.815 
Epoch 933/1000 
	 loss: 27.4661, MinusLogProbMetric: 27.4661, val_loss: 28.0022, val_MinusLogProbMetric: 28.0022

Epoch 933: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4661 - MinusLogProbMetric: 27.4661 - val_loss: 28.0022 - val_MinusLogProbMetric: 28.0022 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 934/1000
2023-10-30 23:21:50.773 
Epoch 934/1000 
	 loss: 27.4659, MinusLogProbMetric: 27.4659, val_loss: 28.0008, val_MinusLogProbMetric: 28.0008

Epoch 934: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4659 - MinusLogProbMetric: 27.4659 - val_loss: 28.0008 - val_MinusLogProbMetric: 28.0008 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 935/1000
2023-10-30 23:22:22.670 
Epoch 935/1000 
	 loss: 27.4651, MinusLogProbMetric: 27.4651, val_loss: 27.9959, val_MinusLogProbMetric: 27.9959

Epoch 935: val_loss did not improve from 27.99476
196/196 - 32s - loss: 27.4651 - MinusLogProbMetric: 27.4651 - val_loss: 27.9959 - val_MinusLogProbMetric: 27.9959 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 936/1000
2023-10-30 23:22:55.831 
Epoch 936/1000 
	 loss: 27.4651, MinusLogProbMetric: 27.4651, val_loss: 27.9990, val_MinusLogProbMetric: 27.9990

Epoch 936: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4651 - MinusLogProbMetric: 27.4651 - val_loss: 27.9990 - val_MinusLogProbMetric: 27.9990 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 937/1000
2023-10-30 23:23:28.623 
Epoch 937/1000 
	 loss: 27.4652, MinusLogProbMetric: 27.4652, val_loss: 27.9977, val_MinusLogProbMetric: 27.9977

Epoch 937: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4652 - MinusLogProbMetric: 27.4652 - val_loss: 27.9977 - val_MinusLogProbMetric: 27.9977 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 938/1000
2023-10-30 23:24:02.001 
Epoch 938/1000 
	 loss: 27.4659, MinusLogProbMetric: 27.4659, val_loss: 28.0047, val_MinusLogProbMetric: 28.0047

Epoch 938: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4659 - MinusLogProbMetric: 27.4659 - val_loss: 28.0047 - val_MinusLogProbMetric: 28.0047 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 939/1000
2023-10-30 23:24:34.228 
Epoch 939/1000 
	 loss: 27.4642, MinusLogProbMetric: 27.4642, val_loss: 27.9976, val_MinusLogProbMetric: 27.9976

Epoch 939: val_loss did not improve from 27.99476
196/196 - 32s - loss: 27.4642 - MinusLogProbMetric: 27.4642 - val_loss: 27.9976 - val_MinusLogProbMetric: 27.9976 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 940/1000
2023-10-30 23:25:07.311 
Epoch 940/1000 
	 loss: 27.4644, MinusLogProbMetric: 27.4644, val_loss: 28.0009, val_MinusLogProbMetric: 28.0009

Epoch 940: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4644 - MinusLogProbMetric: 27.4644 - val_loss: 28.0009 - val_MinusLogProbMetric: 28.0009 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 941/1000
2023-10-30 23:25:41.365 
Epoch 941/1000 
	 loss: 27.4643, MinusLogProbMetric: 27.4643, val_loss: 27.9960, val_MinusLogProbMetric: 27.9960

Epoch 941: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4643 - MinusLogProbMetric: 27.4643 - val_loss: 27.9960 - val_MinusLogProbMetric: 27.9960 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 942/1000
2023-10-30 23:26:15.654 
Epoch 942/1000 
	 loss: 27.4647, MinusLogProbMetric: 27.4647, val_loss: 28.0107, val_MinusLogProbMetric: 28.0107

Epoch 942: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4647 - MinusLogProbMetric: 27.4647 - val_loss: 28.0107 - val_MinusLogProbMetric: 28.0107 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 943/1000
2023-10-30 23:26:49.882 
Epoch 943/1000 
	 loss: 27.4646, MinusLogProbMetric: 27.4646, val_loss: 27.9977, val_MinusLogProbMetric: 27.9977

Epoch 943: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4646 - MinusLogProbMetric: 27.4646 - val_loss: 27.9977 - val_MinusLogProbMetric: 27.9977 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 944/1000
2023-10-30 23:27:23.529 
Epoch 944/1000 
	 loss: 27.4651, MinusLogProbMetric: 27.4651, val_loss: 28.0023, val_MinusLogProbMetric: 28.0023

Epoch 944: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4651 - MinusLogProbMetric: 27.4651 - val_loss: 28.0023 - val_MinusLogProbMetric: 28.0023 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 945/1000
2023-10-30 23:27:57.927 
Epoch 945/1000 
	 loss: 27.4633, MinusLogProbMetric: 27.4633, val_loss: 27.9983, val_MinusLogProbMetric: 27.9983

Epoch 945: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4633 - MinusLogProbMetric: 27.4633 - val_loss: 27.9983 - val_MinusLogProbMetric: 27.9983 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 946/1000
2023-10-30 23:28:31.626 
Epoch 946/1000 
	 loss: 27.4642, MinusLogProbMetric: 27.4642, val_loss: 28.0077, val_MinusLogProbMetric: 28.0077

Epoch 946: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4642 - MinusLogProbMetric: 27.4642 - val_loss: 28.0077 - val_MinusLogProbMetric: 28.0077 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 947/1000
2023-10-30 23:29:04.604 
Epoch 947/1000 
	 loss: 27.4645, MinusLogProbMetric: 27.4645, val_loss: 27.9958, val_MinusLogProbMetric: 27.9958

Epoch 947: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4645 - MinusLogProbMetric: 27.4645 - val_loss: 27.9958 - val_MinusLogProbMetric: 27.9958 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 948/1000
2023-10-30 23:29:37.257 
Epoch 948/1000 
	 loss: 27.4657, MinusLogProbMetric: 27.4657, val_loss: 27.9980, val_MinusLogProbMetric: 27.9980

Epoch 948: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4657 - MinusLogProbMetric: 27.4657 - val_loss: 27.9980 - val_MinusLogProbMetric: 27.9980 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 949/1000
2023-10-30 23:30:09.609 
Epoch 949/1000 
	 loss: 27.4637, MinusLogProbMetric: 27.4637, val_loss: 27.9994, val_MinusLogProbMetric: 27.9994

Epoch 949: val_loss did not improve from 27.99476
196/196 - 32s - loss: 27.4637 - MinusLogProbMetric: 27.4637 - val_loss: 27.9994 - val_MinusLogProbMetric: 27.9994 - lr: 3.1250e-05 - 32s/epoch - 165ms/step
Epoch 950/1000
2023-10-30 23:30:43.207 
Epoch 950/1000 
	 loss: 27.4647, MinusLogProbMetric: 27.4647, val_loss: 27.9977, val_MinusLogProbMetric: 27.9977

Epoch 950: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4647 - MinusLogProbMetric: 27.4647 - val_loss: 27.9977 - val_MinusLogProbMetric: 27.9977 - lr: 3.1250e-05 - 34s/epoch - 171ms/step
Epoch 951/1000
2023-10-30 23:31:16.901 
Epoch 951/1000 
	 loss: 27.4640, MinusLogProbMetric: 27.4640, val_loss: 28.0045, val_MinusLogProbMetric: 28.0045

Epoch 951: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4640 - MinusLogProbMetric: 27.4640 - val_loss: 28.0045 - val_MinusLogProbMetric: 28.0045 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 952/1000
2023-10-30 23:31:50.825 
Epoch 952/1000 
	 loss: 27.4641, MinusLogProbMetric: 27.4641, val_loss: 27.9992, val_MinusLogProbMetric: 27.9992

Epoch 952: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4641 - MinusLogProbMetric: 27.4641 - val_loss: 27.9992 - val_MinusLogProbMetric: 27.9992 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 953/1000
2023-10-30 23:32:24.714 
Epoch 953/1000 
	 loss: 27.4648, MinusLogProbMetric: 27.4648, val_loss: 28.0024, val_MinusLogProbMetric: 28.0024

Epoch 953: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4648 - MinusLogProbMetric: 27.4648 - val_loss: 28.0024 - val_MinusLogProbMetric: 28.0024 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 954/1000
2023-10-30 23:32:58.486 
Epoch 954/1000 
	 loss: 27.4656, MinusLogProbMetric: 27.4656, val_loss: 28.0147, val_MinusLogProbMetric: 28.0147

Epoch 954: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4656 - MinusLogProbMetric: 27.4656 - val_loss: 28.0147 - val_MinusLogProbMetric: 28.0147 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 955/1000
2023-10-30 23:33:32.141 
Epoch 955/1000 
	 loss: 27.4671, MinusLogProbMetric: 27.4671, val_loss: 27.9993, val_MinusLogProbMetric: 27.9993

Epoch 955: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4671 - MinusLogProbMetric: 27.4671 - val_loss: 27.9993 - val_MinusLogProbMetric: 27.9993 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 956/1000
2023-10-30 23:34:05.736 
Epoch 956/1000 
	 loss: 27.4657, MinusLogProbMetric: 27.4657, val_loss: 28.0049, val_MinusLogProbMetric: 28.0049

Epoch 956: val_loss did not improve from 27.99476
196/196 - 34s - loss: 27.4657 - MinusLogProbMetric: 27.4657 - val_loss: 28.0049 - val_MinusLogProbMetric: 28.0049 - lr: 3.1250e-05 - 34s/epoch - 171ms/step
Epoch 957/1000
2023-10-30 23:34:38.336 
Epoch 957/1000 
	 loss: 27.4644, MinusLogProbMetric: 27.4644, val_loss: 28.0083, val_MinusLogProbMetric: 28.0083

Epoch 957: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4644 - MinusLogProbMetric: 27.4644 - val_loss: 28.0083 - val_MinusLogProbMetric: 28.0083 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 958/1000
2023-10-30 23:35:11.116 
Epoch 958/1000 
	 loss: 27.4652, MinusLogProbMetric: 27.4652, val_loss: 28.0085, val_MinusLogProbMetric: 28.0085

Epoch 958: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4652 - MinusLogProbMetric: 27.4652 - val_loss: 28.0085 - val_MinusLogProbMetric: 28.0085 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 959/1000
2023-10-30 23:35:43.908 
Epoch 959/1000 
	 loss: 27.4651, MinusLogProbMetric: 27.4651, val_loss: 28.0042, val_MinusLogProbMetric: 28.0042

Epoch 959: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4651 - MinusLogProbMetric: 27.4651 - val_loss: 28.0042 - val_MinusLogProbMetric: 28.0042 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 960/1000
2023-10-30 23:36:16.927 
Epoch 960/1000 
	 loss: 27.4648, MinusLogProbMetric: 27.4648, val_loss: 27.9982, val_MinusLogProbMetric: 27.9982

Epoch 960: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4648 - MinusLogProbMetric: 27.4648 - val_loss: 27.9982 - val_MinusLogProbMetric: 27.9982 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 961/1000
2023-10-30 23:36:50.091 
Epoch 961/1000 
	 loss: 27.4650, MinusLogProbMetric: 27.4650, val_loss: 28.0061, val_MinusLogProbMetric: 28.0061

Epoch 961: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4650 - MinusLogProbMetric: 27.4650 - val_loss: 28.0061 - val_MinusLogProbMetric: 28.0061 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 962/1000
2023-10-30 23:37:21.840 
Epoch 962/1000 
	 loss: 27.4641, MinusLogProbMetric: 27.4641, val_loss: 27.9951, val_MinusLogProbMetric: 27.9951

Epoch 962: val_loss did not improve from 27.99476
196/196 - 32s - loss: 27.4641 - MinusLogProbMetric: 27.4641 - val_loss: 27.9951 - val_MinusLogProbMetric: 27.9951 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 963/1000
2023-10-30 23:37:54.366 
Epoch 963/1000 
	 loss: 27.4639, MinusLogProbMetric: 27.4639, val_loss: 27.9972, val_MinusLogProbMetric: 27.9972

Epoch 963: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4639 - MinusLogProbMetric: 27.4639 - val_loss: 27.9972 - val_MinusLogProbMetric: 27.9972 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 964/1000
2023-10-30 23:38:27.530 
Epoch 964/1000 
	 loss: 27.4644, MinusLogProbMetric: 27.4644, val_loss: 28.0004, val_MinusLogProbMetric: 28.0004

Epoch 964: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4644 - MinusLogProbMetric: 27.4644 - val_loss: 28.0004 - val_MinusLogProbMetric: 28.0004 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 965/1000
2023-10-30 23:38:59.732 
Epoch 965/1000 
	 loss: 27.4637, MinusLogProbMetric: 27.4637, val_loss: 27.9980, val_MinusLogProbMetric: 27.9980

Epoch 965: val_loss did not improve from 27.99476
196/196 - 32s - loss: 27.4637 - MinusLogProbMetric: 27.4637 - val_loss: 27.9980 - val_MinusLogProbMetric: 27.9980 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 966/1000
2023-10-30 23:39:31.232 
Epoch 966/1000 
	 loss: 27.4644, MinusLogProbMetric: 27.4644, val_loss: 28.0014, val_MinusLogProbMetric: 28.0014

Epoch 966: val_loss did not improve from 27.99476
196/196 - 31s - loss: 27.4644 - MinusLogProbMetric: 27.4644 - val_loss: 28.0014 - val_MinusLogProbMetric: 28.0014 - lr: 3.1250e-05 - 31s/epoch - 161ms/step
Epoch 967/1000
2023-10-30 23:40:04.307 
Epoch 967/1000 
	 loss: 27.4640, MinusLogProbMetric: 27.4640, val_loss: 27.9994, val_MinusLogProbMetric: 27.9994

Epoch 967: val_loss did not improve from 27.99476
196/196 - 33s - loss: 27.4640 - MinusLogProbMetric: 27.4640 - val_loss: 27.9994 - val_MinusLogProbMetric: 27.9994 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 968/1000
2023-10-30 23:40:37.471 
Epoch 968/1000 
	 loss: 27.4643, MinusLogProbMetric: 27.4643, val_loss: 27.9913, val_MinusLogProbMetric: 27.9913

Epoch 968: val_loss improved from 27.99476 to 27.99128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 27.4643 - MinusLogProbMetric: 27.4643 - val_loss: 27.9913 - val_MinusLogProbMetric: 27.9913 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 969/1000
2023-10-30 23:41:10.751 
Epoch 969/1000 
	 loss: 27.4646, MinusLogProbMetric: 27.4646, val_loss: 28.0032, val_MinusLogProbMetric: 28.0032

Epoch 969: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4646 - MinusLogProbMetric: 27.4646 - val_loss: 28.0032 - val_MinusLogProbMetric: 28.0032 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 970/1000
2023-10-30 23:41:42.943 
Epoch 970/1000 
	 loss: 27.4652, MinusLogProbMetric: 27.4652, val_loss: 28.0031, val_MinusLogProbMetric: 28.0031

Epoch 970: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4652 - MinusLogProbMetric: 27.4652 - val_loss: 28.0031 - val_MinusLogProbMetric: 28.0031 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 971/1000
2023-10-30 23:42:15.929 
Epoch 971/1000 
	 loss: 27.4646, MinusLogProbMetric: 27.4646, val_loss: 28.0005, val_MinusLogProbMetric: 28.0005

Epoch 971: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4646 - MinusLogProbMetric: 27.4646 - val_loss: 28.0005 - val_MinusLogProbMetric: 28.0005 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 972/1000
2023-10-30 23:42:48.152 
Epoch 972/1000 
	 loss: 27.4630, MinusLogProbMetric: 27.4630, val_loss: 27.9947, val_MinusLogProbMetric: 27.9947

Epoch 972: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4630 - MinusLogProbMetric: 27.4630 - val_loss: 27.9947 - val_MinusLogProbMetric: 27.9947 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 973/1000
2023-10-30 23:43:19.662 
Epoch 973/1000 
	 loss: 27.4655, MinusLogProbMetric: 27.4655, val_loss: 28.0095, val_MinusLogProbMetric: 28.0095

Epoch 973: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4655 - MinusLogProbMetric: 27.4655 - val_loss: 28.0095 - val_MinusLogProbMetric: 28.0095 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 974/1000
2023-10-30 23:43:51.859 
Epoch 974/1000 
	 loss: 27.4637, MinusLogProbMetric: 27.4637, val_loss: 27.9996, val_MinusLogProbMetric: 27.9996

Epoch 974: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4637 - MinusLogProbMetric: 27.4637 - val_loss: 27.9996 - val_MinusLogProbMetric: 27.9996 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 975/1000
2023-10-30 23:44:25.792 
Epoch 975/1000 
	 loss: 27.4640, MinusLogProbMetric: 27.4640, val_loss: 27.9942, val_MinusLogProbMetric: 27.9942

Epoch 975: val_loss did not improve from 27.99128
196/196 - 34s - loss: 27.4640 - MinusLogProbMetric: 27.4640 - val_loss: 27.9942 - val_MinusLogProbMetric: 27.9942 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 976/1000
2023-10-30 23:44:59.780 
Epoch 976/1000 
	 loss: 27.4644, MinusLogProbMetric: 27.4644, val_loss: 28.0060, val_MinusLogProbMetric: 28.0060

Epoch 976: val_loss did not improve from 27.99128
196/196 - 34s - loss: 27.4644 - MinusLogProbMetric: 27.4644 - val_loss: 28.0060 - val_MinusLogProbMetric: 28.0060 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 977/1000
2023-10-30 23:45:34.627 
Epoch 977/1000 
	 loss: 27.4633, MinusLogProbMetric: 27.4633, val_loss: 28.0026, val_MinusLogProbMetric: 28.0026

Epoch 977: val_loss did not improve from 27.99128
196/196 - 35s - loss: 27.4633 - MinusLogProbMetric: 27.4633 - val_loss: 28.0026 - val_MinusLogProbMetric: 28.0026 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 978/1000
2023-10-30 23:46:09.255 
Epoch 978/1000 
	 loss: 27.4646, MinusLogProbMetric: 27.4646, val_loss: 27.9992, val_MinusLogProbMetric: 27.9992

Epoch 978: val_loss did not improve from 27.99128
196/196 - 35s - loss: 27.4646 - MinusLogProbMetric: 27.4646 - val_loss: 27.9992 - val_MinusLogProbMetric: 27.9992 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 979/1000
2023-10-30 23:46:42.612 
Epoch 979/1000 
	 loss: 27.4629, MinusLogProbMetric: 27.4629, val_loss: 28.0000, val_MinusLogProbMetric: 28.0000

Epoch 979: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4629 - MinusLogProbMetric: 27.4629 - val_loss: 28.0000 - val_MinusLogProbMetric: 28.0000 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 980/1000
2023-10-30 23:47:16.060 
Epoch 980/1000 
	 loss: 27.4627, MinusLogProbMetric: 27.4627, val_loss: 28.0031, val_MinusLogProbMetric: 28.0031

Epoch 980: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4627 - MinusLogProbMetric: 27.4627 - val_loss: 28.0031 - val_MinusLogProbMetric: 28.0031 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 981/1000
2023-10-30 23:47:48.824 
Epoch 981/1000 
	 loss: 27.4632, MinusLogProbMetric: 27.4632, val_loss: 28.0086, val_MinusLogProbMetric: 28.0086

Epoch 981: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4632 - MinusLogProbMetric: 27.4632 - val_loss: 28.0086 - val_MinusLogProbMetric: 28.0086 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 982/1000
2023-10-30 23:48:21.660 
Epoch 982/1000 
	 loss: 27.4645, MinusLogProbMetric: 27.4645, val_loss: 28.0054, val_MinusLogProbMetric: 28.0054

Epoch 982: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4645 - MinusLogProbMetric: 27.4645 - val_loss: 28.0054 - val_MinusLogProbMetric: 28.0054 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 983/1000
2023-10-30 23:48:54.216 
Epoch 983/1000 
	 loss: 27.4668, MinusLogProbMetric: 27.4668, val_loss: 27.9986, val_MinusLogProbMetric: 27.9986

Epoch 983: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4668 - MinusLogProbMetric: 27.4668 - val_loss: 27.9986 - val_MinusLogProbMetric: 27.9986 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 984/1000
2023-10-30 23:49:27.635 
Epoch 984/1000 
	 loss: 27.4640, MinusLogProbMetric: 27.4640, val_loss: 27.9994, val_MinusLogProbMetric: 27.9994

Epoch 984: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4640 - MinusLogProbMetric: 27.4640 - val_loss: 27.9994 - val_MinusLogProbMetric: 27.9994 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 985/1000
2023-10-30 23:50:00.875 
Epoch 985/1000 
	 loss: 27.4631, MinusLogProbMetric: 27.4631, val_loss: 27.9977, val_MinusLogProbMetric: 27.9977

Epoch 985: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4631 - MinusLogProbMetric: 27.4631 - val_loss: 27.9977 - val_MinusLogProbMetric: 27.9977 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 986/1000
2023-10-30 23:50:33.239 
Epoch 986/1000 
	 loss: 27.4637, MinusLogProbMetric: 27.4637, val_loss: 28.0011, val_MinusLogProbMetric: 28.0011

Epoch 986: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4637 - MinusLogProbMetric: 27.4637 - val_loss: 28.0011 - val_MinusLogProbMetric: 28.0011 - lr: 3.1250e-05 - 32s/epoch - 165ms/step
Epoch 987/1000
2023-10-30 23:51:05.800 
Epoch 987/1000 
	 loss: 27.4638, MinusLogProbMetric: 27.4638, val_loss: 27.9957, val_MinusLogProbMetric: 27.9957

Epoch 987: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4638 - MinusLogProbMetric: 27.4638 - val_loss: 27.9957 - val_MinusLogProbMetric: 27.9957 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 988/1000
2023-10-30 23:51:39.272 
Epoch 988/1000 
	 loss: 27.4649, MinusLogProbMetric: 27.4649, val_loss: 28.0029, val_MinusLogProbMetric: 28.0029

Epoch 988: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4649 - MinusLogProbMetric: 27.4649 - val_loss: 28.0029 - val_MinusLogProbMetric: 28.0029 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 989/1000
2023-10-30 23:52:12.691 
Epoch 989/1000 
	 loss: 27.4649, MinusLogProbMetric: 27.4649, val_loss: 27.9979, val_MinusLogProbMetric: 27.9979

Epoch 989: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4649 - MinusLogProbMetric: 27.4649 - val_loss: 27.9979 - val_MinusLogProbMetric: 27.9979 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 990/1000
2023-10-30 23:52:44.886 
Epoch 990/1000 
	 loss: 27.4632, MinusLogProbMetric: 27.4632, val_loss: 28.0030, val_MinusLogProbMetric: 28.0030

Epoch 990: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4632 - MinusLogProbMetric: 27.4632 - val_loss: 28.0030 - val_MinusLogProbMetric: 28.0030 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 991/1000
2023-10-30 23:53:16.566 
Epoch 991/1000 
	 loss: 27.4637, MinusLogProbMetric: 27.4637, val_loss: 28.0053, val_MinusLogProbMetric: 28.0053

Epoch 991: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4637 - MinusLogProbMetric: 27.4637 - val_loss: 28.0053 - val_MinusLogProbMetric: 28.0053 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 992/1000
2023-10-30 23:53:51.160 
Epoch 992/1000 
	 loss: 27.4630, MinusLogProbMetric: 27.4630, val_loss: 28.0013, val_MinusLogProbMetric: 28.0013

Epoch 992: val_loss did not improve from 27.99128
196/196 - 35s - loss: 27.4630 - MinusLogProbMetric: 27.4630 - val_loss: 28.0013 - val_MinusLogProbMetric: 28.0013 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 993/1000
2023-10-30 23:54:24.827 
Epoch 993/1000 
	 loss: 27.4626, MinusLogProbMetric: 27.4626, val_loss: 27.9983, val_MinusLogProbMetric: 27.9983

Epoch 993: val_loss did not improve from 27.99128
196/196 - 34s - loss: 27.4626 - MinusLogProbMetric: 27.4626 - val_loss: 27.9983 - val_MinusLogProbMetric: 27.9983 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 994/1000
2023-10-30 23:54:58.263 
Epoch 994/1000 
	 loss: 27.4625, MinusLogProbMetric: 27.4625, val_loss: 28.0026, val_MinusLogProbMetric: 28.0026

Epoch 994: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4625 - MinusLogProbMetric: 27.4625 - val_loss: 28.0026 - val_MinusLogProbMetric: 28.0026 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 995/1000
2023-10-30 23:55:30.888 
Epoch 995/1000 
	 loss: 27.4634, MinusLogProbMetric: 27.4634, val_loss: 27.9994, val_MinusLogProbMetric: 27.9994

Epoch 995: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4634 - MinusLogProbMetric: 27.4634 - val_loss: 27.9994 - val_MinusLogProbMetric: 27.9994 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 996/1000
2023-10-30 23:56:04.387 
Epoch 996/1000 
	 loss: 27.4621, MinusLogProbMetric: 27.4621, val_loss: 28.0067, val_MinusLogProbMetric: 28.0067

Epoch 996: val_loss did not improve from 27.99128
196/196 - 33s - loss: 27.4621 - MinusLogProbMetric: 27.4621 - val_loss: 28.0067 - val_MinusLogProbMetric: 28.0067 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 997/1000
2023-10-30 23:56:36.843 
Epoch 997/1000 
	 loss: 27.4637, MinusLogProbMetric: 27.4637, val_loss: 28.0029, val_MinusLogProbMetric: 28.0029

Epoch 997: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4637 - MinusLogProbMetric: 27.4637 - val_loss: 28.0029 - val_MinusLogProbMetric: 28.0029 - lr: 3.1250e-05 - 32s/epoch - 166ms/step
Epoch 998/1000
2023-10-30 23:57:07.898 
Epoch 998/1000 
	 loss: 27.4647, MinusLogProbMetric: 27.4647, val_loss: 27.9976, val_MinusLogProbMetric: 27.9976

Epoch 998: val_loss did not improve from 27.99128
196/196 - 31s - loss: 27.4647 - MinusLogProbMetric: 27.4647 - val_loss: 27.9976 - val_MinusLogProbMetric: 27.9976 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 999/1000
2023-10-30 23:57:39.909 
Epoch 999/1000 
	 loss: 27.4640, MinusLogProbMetric: 27.4640, val_loss: 28.0024, val_MinusLogProbMetric: 28.0024

Epoch 999: val_loss did not improve from 27.99128
196/196 - 32s - loss: 27.4640 - MinusLogProbMetric: 27.4640 - val_loss: 28.0024 - val_MinusLogProbMetric: 28.0024 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 1000/1000
2023-10-30 23:58:10.722 
Epoch 1000/1000 
	 loss: 27.4632, MinusLogProbMetric: 27.4632, val_loss: 28.0010, val_MinusLogProbMetric: 28.0010

Epoch 1000: val_loss did not improve from 27.99128
196/196 - 31s - loss: 27.4632 - MinusLogProbMetric: 27.4632 - val_loss: 28.0010 - val_MinusLogProbMetric: 28.0010 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 926.
Model trained in 31452.08 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.69 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.11 s.
===========
Run 385/720 done in 31457.14 s.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_399/ already exists.
Skipping it.
===========
Run 399/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_400/ already exists.
Skipping it.
===========
Run 400/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_401/ already exists.
Skipping it.
===========
Run 401/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_402/ already exists.
Skipping it.
===========
Run 402/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_403/ already exists.
Skipping it.
===========
Run 403/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_404/ already exists.
Skipping it.
===========
Run 404/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_407/ already exists.
Skipping it.
===========
Run 407/720 already exists. Skipping it.
===========

===========
Generating train data for run 408.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_390"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_391 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f3e392501c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e397a4730>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e397a4730>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e3887ee90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e3160efe0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e3160f550>, <keras.callbacks.ModelCheckpoint object at 0x7f3e3160f610>, <keras.callbacks.EarlyStopping object at 0x7f3e3160f880>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e3160f8b0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e3160f4f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-30 23:58:19.723792
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:01:13.976 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 174s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 174s/epoch - 888ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 408.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_401"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_402 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f3e18457c10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e01282ce0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e01282ce0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e383e6530>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e1878ac50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e1878b1c0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e1878b280>, <keras.callbacks.EarlyStopping object at 0x7f3e1878b4f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e1878b520>, <keras.callbacks.TerminateOnNaN object at 0x7f3e1878b160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:01:24.159842
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:04:13.140 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 169s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 169s/epoch - 861ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 408.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_412"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_413 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f3e00f99ae0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e403af160>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e403af160>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e30a04880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e307e3c70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e30710220>, <keras.callbacks.ModelCheckpoint object at 0x7f3e307102e0>, <keras.callbacks.EarlyStopping object at 0x7f3e30710550>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e30710580>, <keras.callbacks.TerminateOnNaN object at 0x7f3e307101c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:04:24.161649
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:07:02.386 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 158s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 158s/epoch - 806ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 408.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_423"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_424 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f3fd0a6fca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f40686d64d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f40686d64d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd0a29ae0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4284273520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4284273a90>, <keras.callbacks.ModelCheckpoint object at 0x7f4284273b50>, <keras.callbacks.EarlyStopping object at 0x7f4284273dc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4284273df0>, <keras.callbacks.TerminateOnNaN object at 0x7f4284273a30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:07:11.698781
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:09:54.378 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 163s/epoch - 829ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 408.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_434"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_435 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f4684d47af0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f42047aebc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f42047aebc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f468612b040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46b8b905b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f412c7b67a0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fb3c5a6e0>, <keras.callbacks.EarlyStopping object at 0x7f412c7b6a10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fb3c5b880>, <keras.callbacks.TerminateOnNaN object at 0x7f412c7b43a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:10:05.134927
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:12:58.085 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 173s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 173s/epoch - 882ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 408.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_445"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_446 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f3e393f4820>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd9326a10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fd9326a10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd034c910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f40885b28c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f40885b2470>, <keras.callbacks.ModelCheckpoint object at 0x7f40885b00d0>, <keras.callbacks.EarlyStopping object at 0x7f40885b0d60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f40885b1360>, <keras.callbacks.TerminateOnNaN object at 0x7f40885b1ba0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:13:07.565772
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:15:50.175 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 163s/epoch - 830ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 408.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_456"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_457 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f46157cd120>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3da5618970>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3da5618970>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f42642bd8a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45e4cf3bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45e4d34160>, <keras.callbacks.ModelCheckpoint object at 0x7f45e4d34220>, <keras.callbacks.EarlyStopping object at 0x7f45e4d34490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45e4d344c0>, <keras.callbacks.TerminateOnNaN object at 0x7f45e4d34100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:16:01.344337
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:18:47.950 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 167s/epoch - 850ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 408.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_467"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_468 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f3e00c6b880>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3f08ea8100>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3f08ea8100>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3df9ddc160>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e000e2140>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e000e26b0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e000e2770>, <keras.callbacks.EarlyStopping object at 0x7f3e000e29e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e000e2a10>, <keras.callbacks.TerminateOnNaN object at 0x7f3e000e2650>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:18:57.116876
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:21:53.616 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 176s/epoch - 900ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 408.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_478"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_479 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f3e188511e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f41ac20d120>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f41ac20d120>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ee387acb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f46b6e3a7d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f46b6e39810>, <keras.callbacks.ModelCheckpoint object at 0x7f46b6e3bbb0>, <keras.callbacks.EarlyStopping object at 0x7f46b6e38eb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f46b6e3ba00>, <keras.callbacks.TerminateOnNaN object at 0x7f46b6e3b160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:22:02.409912
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:24:34.301 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 152s/epoch - 774ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 408.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_489"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_490 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f40085f7040>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f418ac25d20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f418ac25d20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f41c4210610>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fc09cab90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fc09c91e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fc09cae30>, <keras.callbacks.EarlyStopping object at 0x7f3fc09c9960>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fc09c9570>, <keras.callbacks.TerminateOnNaN object at 0x7f3fc09ca890>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:24:52.883779
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:27:30.639 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 158s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 158s/epoch - 805ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 408.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_500"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_501 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f45e4681210>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d8c894490>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d8c894490>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c0c7189d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45dae43250>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45dae437c0>, <keras.callbacks.ModelCheckpoint object at 0x7f45dae43880>, <keras.callbacks.EarlyStopping object at 0x7f45dae43af0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45dae43b20>, <keras.callbacks.TerminateOnNaN object at 0x7f45dae43760>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-31 00:27:41.043411
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:30:25.397 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 164s/epoch - 839ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 408/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 409.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_506"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_507 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7f3d9c7653f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3df9c43fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3df9c43fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd8947dc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e001c3fd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e001ac580>, <keras.callbacks.ModelCheckpoint object at 0x7f3e001ac640>, <keras.callbacks.EarlyStopping object at 0x7f3e001ac8b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e001ac8e0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e001ac520>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:30:30.054148
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:31:26.300 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 56s/epoch - 286ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 409.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_512"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_513 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7f3c44a18640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3da4713040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3da4713040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45e4b5a2f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c14a63e20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3df92603d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3df9260490>, <keras.callbacks.EarlyStopping object at 0x7f3df9260700>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3df9260730>, <keras.callbacks.TerminateOnNaN object at 0x7f3df9260370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:31:30.991759
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:32:27.969 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 57s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 57s/epoch - 290ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 409.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_518"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_519 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7f3d9d30c430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c1e958bb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c1e958bb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e19f57ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d8de2fb80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c1ec48130>, <keras.callbacks.ModelCheckpoint object at 0x7f3c1ec481f0>, <keras.callbacks.EarlyStopping object at 0x7f3c1ec48460>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c1ec48490>, <keras.callbacks.TerminateOnNaN object at 0x7f3c1ec480d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:32:32.565021
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:33:39.461 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 67s/epoch - 341ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 409.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_524"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_525 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7f3e999009d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3da52a90f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3da52a90f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ee0406050>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3cac7450f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3cac745660>, <keras.callbacks.ModelCheckpoint object at 0x7f3cac745720>, <keras.callbacks.EarlyStopping object at 0x7f3cac745990>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3cac7459c0>, <keras.callbacks.TerminateOnNaN object at 0x7f3cac745600>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:33:44.060378
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:34:42.885 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 59s/epoch - 299ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 409.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_530"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_531 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7f3c1f70c160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c1eba55a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c1eba55a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ee0b7d480>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e24620f10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e24621480>, <keras.callbacks.ModelCheckpoint object at 0x7f3e24621540>, <keras.callbacks.EarlyStopping object at 0x7f3e246217b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e246217e0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e24621420>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:34:46.602341
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:35:43.560 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 57s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 57s/epoch - 290ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 409.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_536"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_537 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7f3c14b7dd80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c47c9bd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c47c9bd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3cacac68f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e24dfcc40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e24dfd1b0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e24dfd270>, <keras.callbacks.EarlyStopping object at 0x7f3e24dfd4e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e24dfd510>, <keras.callbacks.TerminateOnNaN object at 0x7f3e24dfd150>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:35:47.920013
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:36:41.246 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 53s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 53s/epoch - 271ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 409.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_542"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_543 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_62 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_62/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_62'")
self.model: <keras.engine.functional.Functional object at 0x7f3e999a2470>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f468449fc10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f468449fc10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f468449e560>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f468449fb50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f468449e650>, <keras.callbacks.ModelCheckpoint object at 0x7f468449e140>, <keras.callbacks.EarlyStopping object at 0x7f468449c9a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f468449c910>, <keras.callbacks.TerminateOnNaN object at 0x7f468449e4a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:36:52.952265
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:37:49.502 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 56s/epoch - 288ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 409.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_548"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_549 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_63 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_63/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_63'")
self.model: <keras.engine.functional.Functional object at 0x7f3cad093970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb0edbe20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb0edbe20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c479ef4c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45e3ad1c60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45e3ad21d0>, <keras.callbacks.ModelCheckpoint object at 0x7f45e3ad2290>, <keras.callbacks.EarlyStopping object at 0x7f45e3ad2500>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45e3ad2530>, <keras.callbacks.TerminateOnNaN object at 0x7f45e3ad2170>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:37:53.917163
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:38:52.714 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 59s/epoch - 299ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 409.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_554"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_555 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_64 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_64/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_64'")
self.model: <keras.engine.functional.Functional object at 0x7f3df82c7ac0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3edb40b700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3edb40b700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45e39e7eb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3cad4ac700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3cad4acc70>, <keras.callbacks.ModelCheckpoint object at 0x7f3cad4acd30>, <keras.callbacks.EarlyStopping object at 0x7f3cad4acfa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3cad4acfd0>, <keras.callbacks.TerminateOnNaN object at 0x7f3cad4acc10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:38:57.178993
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:39:51.602 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 54s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 54s/epoch - 277ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 409.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_560"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_561 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_65 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_65/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_65'")
self.model: <keras.engine.functional.Functional object at 0x7f45b1d137c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45b1ec2f80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45b1ec2f80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f45a95fbf70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45a94e4430>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45a94e49a0>, <keras.callbacks.ModelCheckpoint object at 0x7f45a94e4a60>, <keras.callbacks.EarlyStopping object at 0x7f45a94e4cd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45a94e4d00>, <keras.callbacks.TerminateOnNaN object at 0x7f45a94e4940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:39:55.603743
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:41:02.047 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 66s/epoch - 338ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 409.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_566"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_567 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_66 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_66/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_66'")
self.model: <keras.engine.functional.Functional object at 0x7f3bf41f7fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb987f250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb987f250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3edbe5cd60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e6c5d3cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e6c5d2bf0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e6c5d2d70>, <keras.callbacks.EarlyStopping object at 0x7f3e6c5d1120>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e6c5d1510>, <keras.callbacks.TerminateOnNaN object at 0x7f3e6c5d1840>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-31 00:41:06.449043
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:42:04.575 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 58s/epoch - 296ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 409/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_410/ already exists.
Skipping it.
===========
Run 410/720 already exists. Skipping it.
===========

===========
Generating train data for run 411.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_572"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_573 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_67 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_67/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_67'")
self.model: <keras.engine.functional.Functional object at 0x7f3e683bf4f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d8d3159c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d8d3159c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3bfe1927a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e30312530>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e30312aa0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e30312b60>, <keras.callbacks.EarlyStopping object at 0x7f3e30312dd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e30312e00>, <keras.callbacks.TerminateOnNaN object at 0x7f3e30312a40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:42:09.563289
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:43:18.081 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 68s/epoch - 349ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 411.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_578"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_579 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_68 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_68/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_68'")
self.model: <keras.engine.functional.Functional object at 0x7f3cad6c8d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c247e9870>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c247e9870>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e3137d4b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c44b50670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c44b50be0>, <keras.callbacks.ModelCheckpoint object at 0x7f3c44b50ca0>, <keras.callbacks.EarlyStopping object at 0x7f3c44b50f10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c44b50f40>, <keras.callbacks.TerminateOnNaN object at 0x7f3c44b50b80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:43:23.505660
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:44:32.454 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 69s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 69s/epoch - 351ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 411.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_584"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_585 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_69 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_69/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_69'")
self.model: <keras.engine.functional.Functional object at 0x7f45a0343eb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45a06b3190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45a06b3190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3bffd931f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f45a039a650>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f45a039abc0>, <keras.callbacks.ModelCheckpoint object at 0x7f45a039ac80>, <keras.callbacks.EarlyStopping object at 0x7f45a039aef0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f45a039af20>, <keras.callbacks.TerminateOnNaN object at 0x7f45a039ab60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:44:37.378801
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:46:03.507 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 86s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 86s/epoch - 439ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 411.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_590"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_591 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_70 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_70/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_70'")
self.model: <keras.engine.functional.Functional object at 0x7f4180657010>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c17d2a9b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c17d2a9b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fb29ca2f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fd218e710>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fd218e1d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fd218de70>, <keras.callbacks.EarlyStopping object at 0x7f3fd218fbb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fd218d870>, <keras.callbacks.TerminateOnNaN object at 0x7f3fd218e8c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:46:09.752787
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:47:24.491 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 75s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 75s/epoch - 380ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 411.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_596"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_597 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_71 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_71/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_71'")
self.model: <keras.engine.functional.Functional object at 0x7f3bf6e67070>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e3136fd60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e3136fd60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3c17dbcbb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e18800a60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e18800fd0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e18801090>, <keras.callbacks.EarlyStopping object at 0x7f3e18801300>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e18801330>, <keras.callbacks.TerminateOnNaN object at 0x7f3e18800f70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:47:29.773000
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:48:39.990 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 70s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 70s/epoch - 357ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 411.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_602"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_603 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_72 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_72/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_72'")
self.model: <keras.engine.functional.Functional object at 0x7f3bfd493e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3bfc771c90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3bfc771c90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3bfdc4b3d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3bfe9fab60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3bfe9fb0d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3bfe9fb190>, <keras.callbacks.EarlyStopping object at 0x7f3bfe9fb400>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3bfe9fb430>, <keras.callbacks.TerminateOnNaN object at 0x7f3bfe9fb070>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:48:45.746738
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:50:17.877 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 92s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 92s/epoch - 469ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 411.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_608"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_609 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_73 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_73/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_73'")
self.model: <keras.engine.functional.Functional object at 0x7f41e479bfd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3da50afe80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3da50afe80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fd0f6dfc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3edb5302b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3edb533eb0>, <keras.callbacks.ModelCheckpoint object at 0x7f3edb530460>, <keras.callbacks.EarlyStopping object at 0x7f3edb530fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3edb531f90>, <keras.callbacks.TerminateOnNaN object at 0x7f3edb5305e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:50:24.334462
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:51:37.364 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 73s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 73s/epoch - 372ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 411.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_614"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_615 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_74 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_74/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_74'")
self.model: <keras.engine.functional.Functional object at 0x7f3c27ffff40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb8e8c190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3fb8e8c190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e309137c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e249fdc30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e249fe710>, <keras.callbacks.ModelCheckpoint object at 0x7f3e249fd630>, <keras.callbacks.EarlyStopping object at 0x7f3e249fcbb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e249fc850>, <keras.callbacks.TerminateOnNaN object at 0x7f3e249fe7a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:51:44.011611
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:52:55.112 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 71s/epoch - 361ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 411.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_620"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_621 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_75 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_75/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_75'")
self.model: <keras.engine.functional.Functional object at 0x7f3e249e4a30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f45da869660>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f45da869660>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3bfc4b9b10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c046ace50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c046ad3c0>, <keras.callbacks.ModelCheckpoint object at 0x7f3c046ad480>, <keras.callbacks.EarlyStopping object at 0x7f3c046ad6f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c046ad720>, <keras.callbacks.TerminateOnNaN object at 0x7f3c046ad360>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:53:00.381258
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:54:33.947 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 93s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 93s/epoch - 477ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 411.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_626"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_627 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_76 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_76/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_76'")
self.model: <keras.engine.functional.Functional object at 0x7f3ee0588eb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f4028351480>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4028351480>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ee225c4c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3fd0b20f40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3fd0b21cf0>, <keras.callbacks.ModelCheckpoint object at 0x7f3fd0b21240>, <keras.callbacks.EarlyStopping object at 0x7f3fd0b215d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3fd0b21ae0>, <keras.callbacks.TerminateOnNaN object at 0x7f3fd0b20a90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:54:39.691860
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:55:51.326 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 71s/epoch - 365ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 411.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_411/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_411
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_632"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_633 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_77 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_77/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_77'")
self.model: <keras.engine.functional.Functional object at 0x7f3d8cc9fb50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee225fa90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ee225fa90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3de44b4eb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3cacce6620>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_411/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3cacce7df0>, <keras.callbacks.ModelCheckpoint object at 0x7f3cacce7400>, <keras.callbacks.EarlyStopping object at 0x7f3cacce4a30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3cacce49d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3cacce7460>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_411/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 411/720 with hyperparameters:
timestamp = 2023-10-31 00:55:56.760397
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:57:05.064 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9859.8994, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 9859.8994 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 68s/epoch - 348ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 411/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_412/ already exists.
Skipping it.
===========
Run 412/720 already exists. Skipping it.
===========

===========
Generating train data for run 413.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_413/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_413/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_413/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_413
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_643"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_644 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_78 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_78/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_78'")
self.model: <keras.engine.functional.Functional object at 0x7f3be6e53ac0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c04a6fac0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c04a6fac0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3be6c6e110>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3be6e82ad0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_413/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3be6e83040>, <keras.callbacks.ModelCheckpoint object at 0x7f3be6e83100>, <keras.callbacks.EarlyStopping object at 0x7f3be6e83370>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3be6e833a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3be6e82fe0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_413/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 413/720 with hyperparameters:
timestamp = 2023-10-31 00:57:12.816052
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 00:59:28.079 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12296.1855, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 12296.1855 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 135s/epoch - 690ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 413.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_413/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_413/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_413/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_413
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_654"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_655 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_79 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_79/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_79'")
self.model: <keras.engine.functional.Functional object at 0x7f3c45313ee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3be6e9a830>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3be6e9a830>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3fb8d29180>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e19431540>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_413/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e194327a0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e19432410>, <keras.callbacks.EarlyStopping object at 0x7f3e19433790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e19431ea0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e19431600>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_413/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 413/720 with hyperparameters:
timestamp = 2023-10-31 00:59:35.148347
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
LLVM ERROR: pthread_create failed: Resource temporarily unavailable
LLVM ERROR: pthread_create failed: Resource temporarily unavailable
