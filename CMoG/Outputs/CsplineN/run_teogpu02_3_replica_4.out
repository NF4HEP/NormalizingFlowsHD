2023-10-26 15:58:42.447221: Importing os...
2023-10-26 15:58:42.447296: Importing sys...
2023-10-26 15:58:42.447313: Importing and initializing argparse...
Visible devices: [3]
2023-10-26 15:58:42.464596: Importing timer from timeit...
2023-10-26 15:58:42.465192: Setting env variables for tf import (only device [3] will be available)...
2023-10-26 15:58:42.465239: Importing numpy...
2023-10-26 15:58:42.639351: Importing pandas...
2023-10-26 15:58:42.831172: Importing shutil...
2023-10-26 15:58:42.831197: Importing subprocess...
2023-10-26 15:58:42.831205: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-26 15:58:45.037822: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-26 15:58:45.422487: Importing textwrap...
2023-10-26 15:58:45.422514: Importing timeit...
2023-10-26 15:58:45.422523: Importing traceback...
2023-10-26 15:58:45.422530: Importing typing...
2023-10-26 15:58:45.422540: Setting tf configs...
2023-10-26 15:58:45.585670: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-26 15:58:46.976379: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

===========
Generating train data for run 390.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_390
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  3291840   
 r)                                                              
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f3a5c542bf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3a046bff70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3a046bff70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a5c5af940>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3a0449bd30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3a043d42e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3a043d4430>, <keras.callbacks.EarlyStopping object at 0x7f3a043d4640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3a043d4670>, <keras.callbacks.TerminateOnNaN object at 0x7f3a043d43a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_390/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 390/720 with hyperparameters:
timestamp = 2023-10-26 15:58:57.591546
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 8: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 16:01:31.218 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6440.0078, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 153s - loss: nan - MinusLogProbMetric: 6440.0078 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 153s/epoch - 782ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 390.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_390
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f3e243d9cc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e2429f1c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e2429f1c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f37407f2dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e239f2560>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e239f2ad0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e239f2b90>, <keras.callbacks.EarlyStopping object at 0x7f3e239f2e00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e239f2e30>, <keras.callbacks.TerminateOnNaN object at 0x7f3e239f2a70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_390/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 390/720 with hyperparameters:
timestamp = 2023-10-26 16:01:38.723437
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 28: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 16:03:58.594 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5497.2686, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 140s - loss: nan - MinusLogProbMetric: 5497.2686 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 140s/epoch - 713ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0001111111111111111.
===========
Generating train data for run 390.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_390
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f3e02d72590>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e0af13ac0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e0af13ac0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3748340310>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e02da8cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e02da9240>, <keras.callbacks.ModelCheckpoint object at 0x7f3e02da9300>, <keras.callbacks.EarlyStopping object at 0x7f3e02da9570>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e02da95a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e02da91e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_390/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 390/720 with hyperparameters:
timestamp = 2023-10-26 16:04:09.122846
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
2023-10-26 16:07:28.826 
Epoch 1/1000 
	 loss: 3562.4502, MinusLogProbMetric: 3562.4502, val_loss: 1361.8987, val_MinusLogProbMetric: 1361.8987

Epoch 1: val_loss improved from inf to 1361.89868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 201s - loss: 3562.4502 - MinusLogProbMetric: 3562.4502 - val_loss: 1361.8987 - val_MinusLogProbMetric: 1361.8987 - lr: 1.1111e-04 - 201s/epoch - 1s/step
Epoch 2/1000
2023-10-26 16:08:34.671 
Epoch 2/1000 
	 loss: 1020.7606, MinusLogProbMetric: 1020.7606, val_loss: 778.3412, val_MinusLogProbMetric: 778.3412

Epoch 2: val_loss improved from 1361.89868 to 778.34125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 1020.7606 - MinusLogProbMetric: 1020.7606 - val_loss: 778.3412 - val_MinusLogProbMetric: 778.3412 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 3/1000
2023-10-26 16:09:42.660 
Epoch 3/1000 
	 loss: 859.1404, MinusLogProbMetric: 859.1404, val_loss: 1448.2214, val_MinusLogProbMetric: 1448.2214

Epoch 3: val_loss did not improve from 778.34125
196/196 - 67s - loss: 859.1404 - MinusLogProbMetric: 859.1404 - val_loss: 1448.2214 - val_MinusLogProbMetric: 1448.2214 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 4/1000
2023-10-26 16:10:47.683 
Epoch 4/1000 
	 loss: 856.2634, MinusLogProbMetric: 856.2634, val_loss: 706.0568, val_MinusLogProbMetric: 706.0568

Epoch 4: val_loss improved from 778.34125 to 706.05682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 856.2634 - MinusLogProbMetric: 856.2634 - val_loss: 706.0568 - val_MinusLogProbMetric: 706.0568 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 5/1000
2023-10-26 16:11:55.783 
Epoch 5/1000 
	 loss: 643.7581, MinusLogProbMetric: 643.7581, val_loss: 672.8295, val_MinusLogProbMetric: 672.8295

Epoch 5: val_loss improved from 706.05682 to 672.82953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 643.7581 - MinusLogProbMetric: 643.7581 - val_loss: 672.8295 - val_MinusLogProbMetric: 672.8295 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 6/1000
2023-10-26 16:13:03.314 
Epoch 6/1000 
	 loss: 596.3902, MinusLogProbMetric: 596.3902, val_loss: 543.1716, val_MinusLogProbMetric: 543.1716

Epoch 6: val_loss improved from 672.82953 to 543.17157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 596.3902 - MinusLogProbMetric: 596.3902 - val_loss: 543.1716 - val_MinusLogProbMetric: 543.1716 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 7/1000
2023-10-26 16:14:09.863 
Epoch 7/1000 
	 loss: 588.7147, MinusLogProbMetric: 588.7147, val_loss: 872.2192, val_MinusLogProbMetric: 872.2192

Epoch 7: val_loss did not improve from 543.17157
196/196 - 66s - loss: 588.7147 - MinusLogProbMetric: 588.7147 - val_loss: 872.2192 - val_MinusLogProbMetric: 872.2192 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 8/1000
2023-10-26 16:15:15.252 
Epoch 8/1000 
	 loss: 538.9626, MinusLogProbMetric: 538.9626, val_loss: 484.5827, val_MinusLogProbMetric: 484.5827

Epoch 8: val_loss improved from 543.17157 to 484.58270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 538.9626 - MinusLogProbMetric: 538.9626 - val_loss: 484.5827 - val_MinusLogProbMetric: 484.5827 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 9/1000
2023-10-26 16:16:23.328 
Epoch 9/1000 
	 loss: 462.1088, MinusLogProbMetric: 462.1088, val_loss: 420.1187, val_MinusLogProbMetric: 420.1187

Epoch 9: val_loss improved from 484.58270 to 420.11871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 462.1088 - MinusLogProbMetric: 462.1088 - val_loss: 420.1187 - val_MinusLogProbMetric: 420.1187 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 10/1000
2023-10-26 16:17:27.630 
Epoch 10/1000 
	 loss: 424.7005, MinusLogProbMetric: 424.7005, val_loss: 396.6442, val_MinusLogProbMetric: 396.6442

Epoch 10: val_loss improved from 420.11871 to 396.64420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 64s - loss: 424.7005 - MinusLogProbMetric: 424.7005 - val_loss: 396.6442 - val_MinusLogProbMetric: 396.6442 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 11/1000
2023-10-26 16:18:35.080 
Epoch 11/1000 
	 loss: 472.7529, MinusLogProbMetric: 472.7529, val_loss: 458.6800, val_MinusLogProbMetric: 458.6800

Epoch 11: val_loss did not improve from 396.64420
196/196 - 66s - loss: 472.7529 - MinusLogProbMetric: 472.7529 - val_loss: 458.6800 - val_MinusLogProbMetric: 458.6800 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 12/1000
2023-10-26 16:19:40.955 
Epoch 12/1000 
	 loss: 405.5430, MinusLogProbMetric: 405.5430, val_loss: 380.3742, val_MinusLogProbMetric: 380.3742

Epoch 12: val_loss improved from 396.64420 to 380.37418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 405.5430 - MinusLogProbMetric: 405.5430 - val_loss: 380.3742 - val_MinusLogProbMetric: 380.3742 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 13/1000
2023-10-26 16:20:47.279 
Epoch 13/1000 
	 loss: 371.9196, MinusLogProbMetric: 371.9196, val_loss: 385.5543, val_MinusLogProbMetric: 385.5543

Epoch 13: val_loss did not improve from 380.37418
196/196 - 65s - loss: 371.9196 - MinusLogProbMetric: 371.9196 - val_loss: 385.5543 - val_MinusLogProbMetric: 385.5543 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 14/1000
2023-10-26 16:21:54.202 
Epoch 14/1000 
	 loss: 360.4442, MinusLogProbMetric: 360.4442, val_loss: 373.3886, val_MinusLogProbMetric: 373.3886

Epoch 14: val_loss improved from 380.37418 to 373.38861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 360.4442 - MinusLogProbMetric: 360.4442 - val_loss: 373.3886 - val_MinusLogProbMetric: 373.3886 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 15/1000
2023-10-26 16:22:59.618 
Epoch 15/1000 
	 loss: 341.4815, MinusLogProbMetric: 341.4815, val_loss: 334.4696, val_MinusLogProbMetric: 334.4696

Epoch 15: val_loss improved from 373.38861 to 334.46960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 341.4815 - MinusLogProbMetric: 341.4815 - val_loss: 334.4696 - val_MinusLogProbMetric: 334.4696 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 16/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 172: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 16:23:56.719 
Epoch 16/1000 
	 loss: nan, MinusLogProbMetric: 327.9831, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 16: val_loss did not improve from 334.46960
196/196 - 56s - loss: nan - MinusLogProbMetric: 327.9831 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 56s/epoch - 286ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 3.703703703703703e-05.
===========
Generating train data for run 390.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_390
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f3874298700>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e0290b610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e0290b610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f39545c48b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e027d5150>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e027d56c0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e027d5780>, <keras.callbacks.EarlyStopping object at 0x7f3e027d59f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e027d5a20>, <keras.callbacks.TerminateOnNaN object at 0x7f3e027d5660>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 390/720 with hyperparameters:
timestamp = 2023-10-26 16:24:07.809861
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
2023-10-26 16:27:25.098 
Epoch 1/1000 
	 loss: 475.4715, MinusLogProbMetric: 475.4715, val_loss: 402.9450, val_MinusLogProbMetric: 402.9450

Epoch 1: val_loss improved from inf to 402.94504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 198s - loss: 475.4715 - MinusLogProbMetric: 475.4715 - val_loss: 402.9450 - val_MinusLogProbMetric: 402.9450 - lr: 3.7037e-05 - 198s/epoch - 1s/step
Epoch 2/1000
2023-10-26 16:28:32.917 
Epoch 2/1000 
	 loss: 392.1949, MinusLogProbMetric: 392.1949, val_loss: 335.8403, val_MinusLogProbMetric: 335.8403

Epoch 2: val_loss improved from 402.94504 to 335.84033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 392.1949 - MinusLogProbMetric: 392.1949 - val_loss: 335.8403 - val_MinusLogProbMetric: 335.8403 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 3/1000
2023-10-26 16:29:38.376 
Epoch 3/1000 
	 loss: 318.8833, MinusLogProbMetric: 318.8833, val_loss: 287.7763, val_MinusLogProbMetric: 287.7763

Epoch 3: val_loss improved from 335.84033 to 287.77631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 65s - loss: 318.8833 - MinusLogProbMetric: 318.8833 - val_loss: 287.7763 - val_MinusLogProbMetric: 287.7763 - lr: 3.7037e-05 - 65s/epoch - 334ms/step
Epoch 4/1000
2023-10-26 16:30:44.339 
Epoch 4/1000 
	 loss: 270.7849, MinusLogProbMetric: 270.7849, val_loss: 252.2635, val_MinusLogProbMetric: 252.2635

Epoch 4: val_loss improved from 287.77631 to 252.26350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 270.7849 - MinusLogProbMetric: 270.7849 - val_loss: 252.2635 - val_MinusLogProbMetric: 252.2635 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 5/1000
2023-10-26 16:31:50.685 
Epoch 5/1000 
	 loss: 241.1463, MinusLogProbMetric: 241.1463, val_loss: 227.2514, val_MinusLogProbMetric: 227.2514

Epoch 5: val_loss improved from 252.26350 to 227.25137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 241.1463 - MinusLogProbMetric: 241.1463 - val_loss: 227.2514 - val_MinusLogProbMetric: 227.2514 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 6/1000
2023-10-26 16:32:59.094 
Epoch 6/1000 
	 loss: 243.6607, MinusLogProbMetric: 243.6607, val_loss: 244.7138, val_MinusLogProbMetric: 244.7138

Epoch 6: val_loss did not improve from 227.25137
196/196 - 67s - loss: 243.6607 - MinusLogProbMetric: 243.6607 - val_loss: 244.7138 - val_MinusLogProbMetric: 244.7138 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 7/1000
2023-10-26 16:34:00.687 
Epoch 7/1000 
	 loss: 230.5122, MinusLogProbMetric: 230.5122, val_loss: 212.7352, val_MinusLogProbMetric: 212.7352

Epoch 7: val_loss improved from 227.25137 to 212.73517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 63s - loss: 230.5122 - MinusLogProbMetric: 230.5122 - val_loss: 212.7352 - val_MinusLogProbMetric: 212.7352 - lr: 3.7037e-05 - 63s/epoch - 319ms/step
Epoch 8/1000
2023-10-26 16:35:23.954 
Epoch 8/1000 
	 loss: 205.1806, MinusLogProbMetric: 205.1806, val_loss: 196.1332, val_MinusLogProbMetric: 196.1332

Epoch 8: val_loss improved from 212.73517 to 196.13321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 83s - loss: 205.1806 - MinusLogProbMetric: 205.1806 - val_loss: 196.1332 - val_MinusLogProbMetric: 196.1332 - lr: 3.7037e-05 - 83s/epoch - 425ms/step
Epoch 9/1000
2023-10-26 16:36:31.735 
Epoch 9/1000 
	 loss: 211.2097, MinusLogProbMetric: 211.2097, val_loss: 223.1168, val_MinusLogProbMetric: 223.1168

Epoch 9: val_loss did not improve from 196.13321
196/196 - 67s - loss: 211.2097 - MinusLogProbMetric: 211.2097 - val_loss: 223.1168 - val_MinusLogProbMetric: 223.1168 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 10/1000
2023-10-26 16:37:35.162 
Epoch 10/1000 
	 loss: 202.7809, MinusLogProbMetric: 202.7809, val_loss: 184.9203, val_MinusLogProbMetric: 184.9203

Epoch 10: val_loss improved from 196.13321 to 184.92032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 64s - loss: 202.7809 - MinusLogProbMetric: 202.7809 - val_loss: 184.9203 - val_MinusLogProbMetric: 184.9203 - lr: 3.7037e-05 - 64s/epoch - 328ms/step
Epoch 11/1000
2023-10-26 16:38:37.820 
Epoch 11/1000 
	 loss: 194.1122, MinusLogProbMetric: 194.1122, val_loss: 329.0156, val_MinusLogProbMetric: 329.0156

Epoch 11: val_loss did not improve from 184.92032
196/196 - 62s - loss: 194.1122 - MinusLogProbMetric: 194.1122 - val_loss: 329.0156 - val_MinusLogProbMetric: 329.0156 - lr: 3.7037e-05 - 62s/epoch - 316ms/step
Epoch 12/1000
2023-10-26 16:39:43.419 
Epoch 12/1000 
	 loss: 412.5402, MinusLogProbMetric: 412.5402, val_loss: 242.3758, val_MinusLogProbMetric: 242.3758

Epoch 12: val_loss did not improve from 184.92032
196/196 - 66s - loss: 412.5402 - MinusLogProbMetric: 412.5402 - val_loss: 242.3758 - val_MinusLogProbMetric: 242.3758 - lr: 3.7037e-05 - 66s/epoch - 335ms/step
Epoch 13/1000
2023-10-26 16:40:48.204 
Epoch 13/1000 
	 loss: 211.7601, MinusLogProbMetric: 211.7601, val_loss: 178.7644, val_MinusLogProbMetric: 178.7644

Epoch 13: val_loss improved from 184.92032 to 178.76443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 211.7601 - MinusLogProbMetric: 211.7601 - val_loss: 178.7644 - val_MinusLogProbMetric: 178.7644 - lr: 3.7037e-05 - 66s/epoch - 335ms/step
Epoch 14/1000
2023-10-26 16:41:52.778 
Epoch 14/1000 
	 loss: 165.5242, MinusLogProbMetric: 165.5242, val_loss: 153.6570, val_MinusLogProbMetric: 153.6570

Epoch 14: val_loss improved from 178.76443 to 153.65701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 65s - loss: 165.5242 - MinusLogProbMetric: 165.5242 - val_loss: 153.6570 - val_MinusLogProbMetric: 153.6570 - lr: 3.7037e-05 - 65s/epoch - 330ms/step
Epoch 15/1000
2023-10-26 16:42:59.753 
Epoch 15/1000 
	 loss: 183.5884, MinusLogProbMetric: 183.5884, val_loss: 147.3371, val_MinusLogProbMetric: 147.3371

Epoch 15: val_loss improved from 153.65701 to 147.33705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 183.5884 - MinusLogProbMetric: 183.5884 - val_loss: 147.3371 - val_MinusLogProbMetric: 147.3371 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 16/1000
2023-10-26 16:44:06.761 
Epoch 16/1000 
	 loss: 140.0846, MinusLogProbMetric: 140.0846, val_loss: 133.4416, val_MinusLogProbMetric: 133.4416

Epoch 16: val_loss improved from 147.33705 to 133.44162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 140.0846 - MinusLogProbMetric: 140.0846 - val_loss: 133.4416 - val_MinusLogProbMetric: 133.4416 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 17/1000
2023-10-26 16:45:11.860 
Epoch 17/1000 
	 loss: 130.5029, MinusLogProbMetric: 130.5029, val_loss: 124.1125, val_MinusLogProbMetric: 124.1125

Epoch 17: val_loss improved from 133.44162 to 124.11246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 65s - loss: 130.5029 - MinusLogProbMetric: 130.5029 - val_loss: 124.1125 - val_MinusLogProbMetric: 124.1125 - lr: 3.7037e-05 - 65s/epoch - 332ms/step
Epoch 18/1000
2023-10-26 16:46:17.238 
Epoch 18/1000 
	 loss: 179.6193, MinusLogProbMetric: 179.6193, val_loss: 352.0414, val_MinusLogProbMetric: 352.0414

Epoch 18: val_loss did not improve from 124.11246
196/196 - 64s - loss: 179.6193 - MinusLogProbMetric: 179.6193 - val_loss: 352.0414 - val_MinusLogProbMetric: 352.0414 - lr: 3.7037e-05 - 64s/epoch - 329ms/step
Epoch 19/1000
2023-10-26 16:47:19.885 
Epoch 19/1000 
	 loss: 211.4755, MinusLogProbMetric: 211.4755, val_loss: 172.6643, val_MinusLogProbMetric: 172.6643

Epoch 19: val_loss did not improve from 124.11246
196/196 - 63s - loss: 211.4755 - MinusLogProbMetric: 211.4755 - val_loss: 172.6643 - val_MinusLogProbMetric: 172.6643 - lr: 3.7037e-05 - 63s/epoch - 320ms/step
Epoch 20/1000
2023-10-26 16:48:26.474 
Epoch 20/1000 
	 loss: 159.8031, MinusLogProbMetric: 159.8031, val_loss: 150.7774, val_MinusLogProbMetric: 150.7774

Epoch 20: val_loss did not improve from 124.11246
196/196 - 67s - loss: 159.8031 - MinusLogProbMetric: 159.8031 - val_loss: 150.7774 - val_MinusLogProbMetric: 150.7774 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 21/1000
2023-10-26 16:49:29.803 
Epoch 21/1000 
	 loss: 140.7932, MinusLogProbMetric: 140.7932, val_loss: 133.4181, val_MinusLogProbMetric: 133.4181

Epoch 21: val_loss did not improve from 124.11246
196/196 - 63s - loss: 140.7932 - MinusLogProbMetric: 140.7932 - val_loss: 133.4181 - val_MinusLogProbMetric: 133.4181 - lr: 3.7037e-05 - 63s/epoch - 323ms/step
Epoch 22/1000
2023-10-26 16:50:31.186 
Epoch 22/1000 
	 loss: 128.9907, MinusLogProbMetric: 128.9907, val_loss: 124.1176, val_MinusLogProbMetric: 124.1176

Epoch 22: val_loss did not improve from 124.11246
196/196 - 61s - loss: 128.9907 - MinusLogProbMetric: 128.9907 - val_loss: 124.1176 - val_MinusLogProbMetric: 124.1176 - lr: 3.7037e-05 - 61s/epoch - 313ms/step
Epoch 23/1000
2023-10-26 16:51:39.645 
Epoch 23/1000 
	 loss: 120.0044, MinusLogProbMetric: 120.0044, val_loss: 116.8236, val_MinusLogProbMetric: 116.8236

Epoch 23: val_loss improved from 124.11246 to 116.82360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 70s - loss: 120.0044 - MinusLogProbMetric: 120.0044 - val_loss: 116.8236 - val_MinusLogProbMetric: 116.8236 - lr: 3.7037e-05 - 70s/epoch - 355ms/step
Epoch 24/1000
2023-10-26 16:52:46.253 
Epoch 24/1000 
	 loss: 113.4928, MinusLogProbMetric: 113.4928, val_loss: 112.9184, val_MinusLogProbMetric: 112.9184

Epoch 24: val_loss improved from 116.82360 to 112.91836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 113.4928 - MinusLogProbMetric: 113.4928 - val_loss: 112.9184 - val_MinusLogProbMetric: 112.9184 - lr: 3.7037e-05 - 67s/epoch - 339ms/step
Epoch 25/1000
2023-10-26 16:53:47.659 
Epoch 25/1000 
	 loss: 108.3654, MinusLogProbMetric: 108.3654, val_loss: 106.1781, val_MinusLogProbMetric: 106.1781

Epoch 25: val_loss improved from 112.91836 to 106.17812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 61s - loss: 108.3654 - MinusLogProbMetric: 108.3654 - val_loss: 106.1781 - val_MinusLogProbMetric: 106.1781 - lr: 3.7037e-05 - 61s/epoch - 312ms/step
Epoch 26/1000
2023-10-26 16:54:52.640 
Epoch 26/1000 
	 loss: 103.6344, MinusLogProbMetric: 103.6344, val_loss: 101.5322, val_MinusLogProbMetric: 101.5322

Epoch 26: val_loss improved from 106.17812 to 101.53215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 65s - loss: 103.6344 - MinusLogProbMetric: 103.6344 - val_loss: 101.5322 - val_MinusLogProbMetric: 101.5322 - lr: 3.7037e-05 - 65s/epoch - 332ms/step
Epoch 27/1000
2023-10-26 16:55:59.295 
Epoch 27/1000 
	 loss: 99.8035, MinusLogProbMetric: 99.8035, val_loss: 98.3956, val_MinusLogProbMetric: 98.3956

Epoch 27: val_loss improved from 101.53215 to 98.39558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 99.8035 - MinusLogProbMetric: 99.8035 - val_loss: 98.3956 - val_MinusLogProbMetric: 98.3956 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 28/1000
2023-10-26 16:57:04.846 
Epoch 28/1000 
	 loss: 98.3148, MinusLogProbMetric: 98.3148, val_loss: 95.7656, val_MinusLogProbMetric: 95.7656

Epoch 28: val_loss improved from 98.39558 to 95.76556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 98.3148 - MinusLogProbMetric: 98.3148 - val_loss: 95.7656 - val_MinusLogProbMetric: 95.7656 - lr: 3.7037e-05 - 66s/epoch - 335ms/step
Epoch 29/1000
2023-10-26 16:58:11.372 
Epoch 29/1000 
	 loss: 94.2418, MinusLogProbMetric: 94.2418, val_loss: 92.7530, val_MinusLogProbMetric: 92.7530

Epoch 29: val_loss improved from 95.76556 to 92.75301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 94.2418 - MinusLogProbMetric: 94.2418 - val_loss: 92.7530 - val_MinusLogProbMetric: 92.7530 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 30/1000
2023-10-26 16:59:18.570 
Epoch 30/1000 
	 loss: 91.2830, MinusLogProbMetric: 91.2830, val_loss: 90.6181, val_MinusLogProbMetric: 90.6181

Epoch 30: val_loss improved from 92.75301 to 90.61806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 91.2830 - MinusLogProbMetric: 91.2830 - val_loss: 90.6181 - val_MinusLogProbMetric: 90.6181 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 31/1000
2023-10-26 17:00:24.183 
Epoch 31/1000 
	 loss: 88.8144, MinusLogProbMetric: 88.8144, val_loss: 88.2426, val_MinusLogProbMetric: 88.2426

Epoch 31: val_loss improved from 90.61806 to 88.24259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 88.8144 - MinusLogProbMetric: 88.8144 - val_loss: 88.2426 - val_MinusLogProbMetric: 88.2426 - lr: 3.7037e-05 - 66s/epoch - 334ms/step
Epoch 32/1000
2023-10-26 17:01:30.898 
Epoch 32/1000 
	 loss: 86.9453, MinusLogProbMetric: 86.9453, val_loss: 85.7821, val_MinusLogProbMetric: 85.7821

Epoch 32: val_loss improved from 88.24259 to 85.78208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 86.9453 - MinusLogProbMetric: 86.9453 - val_loss: 85.7821 - val_MinusLogProbMetric: 85.7821 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 33/1000
2023-10-26 17:02:34.076 
Epoch 33/1000 
	 loss: 84.6270, MinusLogProbMetric: 84.6270, val_loss: 84.3248, val_MinusLogProbMetric: 84.3248

Epoch 33: val_loss improved from 85.78208 to 84.32478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 63s - loss: 84.6270 - MinusLogProbMetric: 84.6270 - val_loss: 84.3248 - val_MinusLogProbMetric: 84.3248 - lr: 3.7037e-05 - 63s/epoch - 323ms/step
Epoch 34/1000
2023-10-26 17:03:40.941 
Epoch 34/1000 
	 loss: 83.3078, MinusLogProbMetric: 83.3078, val_loss: 82.3827, val_MinusLogProbMetric: 82.3827

Epoch 34: val_loss improved from 84.32478 to 82.38274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 83.3078 - MinusLogProbMetric: 83.3078 - val_loss: 82.3827 - val_MinusLogProbMetric: 82.3827 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 35/1000
2023-10-26 17:04:41.426 
Epoch 35/1000 
	 loss: 81.0503, MinusLogProbMetric: 81.0503, val_loss: 80.2231, val_MinusLogProbMetric: 80.2231

Epoch 35: val_loss improved from 82.38274 to 80.22308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 61s - loss: 81.0503 - MinusLogProbMetric: 81.0503 - val_loss: 80.2231 - val_MinusLogProbMetric: 80.2231 - lr: 3.7037e-05 - 61s/epoch - 309ms/step
Epoch 36/1000
2023-10-26 17:05:46.995 
Epoch 36/1000 
	 loss: 79.4472, MinusLogProbMetric: 79.4472, val_loss: 78.7494, val_MinusLogProbMetric: 78.7494

Epoch 36: val_loss improved from 80.22308 to 78.74936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 79.4472 - MinusLogProbMetric: 79.4472 - val_loss: 78.7494 - val_MinusLogProbMetric: 78.7494 - lr: 3.7037e-05 - 66s/epoch - 335ms/step
Epoch 37/1000
2023-10-26 17:06:52.129 
Epoch 37/1000 
	 loss: 78.0498, MinusLogProbMetric: 78.0498, val_loss: 77.5385, val_MinusLogProbMetric: 77.5385

Epoch 37: val_loss improved from 78.74936 to 77.53848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 65s - loss: 78.0498 - MinusLogProbMetric: 78.0498 - val_loss: 77.5385 - val_MinusLogProbMetric: 77.5385 - lr: 3.7037e-05 - 65s/epoch - 332ms/step
Epoch 38/1000
2023-10-26 17:07:58.553 
Epoch 38/1000 
	 loss: 77.2488, MinusLogProbMetric: 77.2488, val_loss: 76.1603, val_MinusLogProbMetric: 76.1603

Epoch 38: val_loss improved from 77.53848 to 76.16032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 77.2488 - MinusLogProbMetric: 77.2488 - val_loss: 76.1603 - val_MinusLogProbMetric: 76.1603 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 39/1000
2023-10-26 17:09:04.034 
Epoch 39/1000 
	 loss: 75.6739, MinusLogProbMetric: 75.6739, val_loss: 75.2763, val_MinusLogProbMetric: 75.2763

Epoch 39: val_loss improved from 76.16032 to 75.27629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 65s - loss: 75.6739 - MinusLogProbMetric: 75.6739 - val_loss: 75.2763 - val_MinusLogProbMetric: 75.2763 - lr: 3.7037e-05 - 65s/epoch - 333ms/step
Epoch 40/1000
2023-10-26 17:10:08.788 
Epoch 40/1000 
	 loss: 74.3840, MinusLogProbMetric: 74.3840, val_loss: 73.9313, val_MinusLogProbMetric: 73.9313

Epoch 40: val_loss improved from 75.27629 to 73.93134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 65s - loss: 74.3840 - MinusLogProbMetric: 74.3840 - val_loss: 73.9313 - val_MinusLogProbMetric: 73.9313 - lr: 3.7037e-05 - 65s/epoch - 332ms/step
Epoch 41/1000
2023-10-26 17:11:16.637 
Epoch 41/1000 
	 loss: 73.1995, MinusLogProbMetric: 73.1995, val_loss: 73.3636, val_MinusLogProbMetric: 73.3636

Epoch 41: val_loss improved from 73.93134 to 73.36356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 73.1995 - MinusLogProbMetric: 73.1995 - val_loss: 73.3636 - val_MinusLogProbMetric: 73.3636 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 42/1000
2023-10-26 17:12:22.068 
Epoch 42/1000 
	 loss: 72.3258, MinusLogProbMetric: 72.3258, val_loss: 71.9504, val_MinusLogProbMetric: 71.9504

Epoch 42: val_loss improved from 73.36356 to 71.95042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 72.3258 - MinusLogProbMetric: 72.3258 - val_loss: 71.9504 - val_MinusLogProbMetric: 71.9504 - lr: 3.7037e-05 - 66s/epoch - 334ms/step
Epoch 43/1000
2023-10-26 17:13:26.352 
Epoch 43/1000 
	 loss: 108.5638, MinusLogProbMetric: 108.5638, val_loss: 88.7226, val_MinusLogProbMetric: 88.7226

Epoch 43: val_loss did not improve from 71.95042
196/196 - 63s - loss: 108.5638 - MinusLogProbMetric: 108.5638 - val_loss: 88.7226 - val_MinusLogProbMetric: 88.7226 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 44/1000
2023-10-26 17:14:32.248 
Epoch 44/1000 
	 loss: 79.7678, MinusLogProbMetric: 79.7678, val_loss: 75.1883, val_MinusLogProbMetric: 75.1883

Epoch 44: val_loss did not improve from 71.95042
196/196 - 66s - loss: 79.7678 - MinusLogProbMetric: 79.7678 - val_loss: 75.1883 - val_MinusLogProbMetric: 75.1883 - lr: 3.7037e-05 - 66s/epoch - 336ms/step
Epoch 45/1000
2023-10-26 17:15:35.277 
Epoch 45/1000 
	 loss: 73.0024, MinusLogProbMetric: 73.0024, val_loss: 71.9642, val_MinusLogProbMetric: 71.9642

Epoch 45: val_loss did not improve from 71.95042
196/196 - 63s - loss: 73.0024 - MinusLogProbMetric: 73.0024 - val_loss: 71.9642 - val_MinusLogProbMetric: 71.9642 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 46/1000
2023-10-26 17:16:42.326 
Epoch 46/1000 
	 loss: 71.4559, MinusLogProbMetric: 71.4559, val_loss: 70.4741, val_MinusLogProbMetric: 70.4741

Epoch 46: val_loss improved from 71.95042 to 70.47413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 71.4559 - MinusLogProbMetric: 71.4559 - val_loss: 70.4741 - val_MinusLogProbMetric: 70.4741 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 47/1000
2023-10-26 17:17:48.994 
Epoch 47/1000 
	 loss: 69.5816, MinusLogProbMetric: 69.5816, val_loss: 69.2491, val_MinusLogProbMetric: 69.2491

Epoch 47: val_loss improved from 70.47413 to 69.24908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 69.5816 - MinusLogProbMetric: 69.5816 - val_loss: 69.2491 - val_MinusLogProbMetric: 69.2491 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 48/1000
2023-10-26 17:18:50.668 
Epoch 48/1000 
	 loss: 68.4392, MinusLogProbMetric: 68.4392, val_loss: 68.8066, val_MinusLogProbMetric: 68.8066

Epoch 48: val_loss improved from 69.24908 to 68.80657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 62s - loss: 68.4392 - MinusLogProbMetric: 68.4392 - val_loss: 68.8066 - val_MinusLogProbMetric: 68.8066 - lr: 3.7037e-05 - 62s/epoch - 316ms/step
Epoch 49/1000
2023-10-26 17:19:53.063 
Epoch 49/1000 
	 loss: 67.6942, MinusLogProbMetric: 67.6942, val_loss: 67.3098, val_MinusLogProbMetric: 67.3098

Epoch 49: val_loss improved from 68.80657 to 67.30981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 62s - loss: 67.6942 - MinusLogProbMetric: 67.6942 - val_loss: 67.3098 - val_MinusLogProbMetric: 67.3098 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 50/1000
2023-10-26 17:20:59.902 
Epoch 50/1000 
	 loss: 67.1852, MinusLogProbMetric: 67.1852, val_loss: 66.5508, val_MinusLogProbMetric: 66.5508

Epoch 50: val_loss improved from 67.30981 to 66.55080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 67.1852 - MinusLogProbMetric: 67.1852 - val_loss: 66.5508 - val_MinusLogProbMetric: 66.5508 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 51/1000
2023-10-26 17:22:03.891 
Epoch 51/1000 
	 loss: 65.8709, MinusLogProbMetric: 65.8709, val_loss: 65.6453, val_MinusLogProbMetric: 65.6453

Epoch 51: val_loss improved from 66.55080 to 65.64532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 64s - loss: 65.8709 - MinusLogProbMetric: 65.8709 - val_loss: 65.6453 - val_MinusLogProbMetric: 65.6453 - lr: 3.7037e-05 - 64s/epoch - 327ms/step
Epoch 52/1000
2023-10-26 17:23:11.881 
Epoch 52/1000 
	 loss: 65.0851, MinusLogProbMetric: 65.0851, val_loss: 64.8992, val_MinusLogProbMetric: 64.8992

Epoch 52: val_loss improved from 65.64532 to 64.89922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 65.0851 - MinusLogProbMetric: 65.0851 - val_loss: 64.8992 - val_MinusLogProbMetric: 64.8992 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 53/1000
2023-10-26 17:24:15.212 
Epoch 53/1000 
	 loss: 65.2231, MinusLogProbMetric: 65.2231, val_loss: 65.2683, val_MinusLogProbMetric: 65.2683

Epoch 53: val_loss did not improve from 64.89922
196/196 - 62s - loss: 65.2231 - MinusLogProbMetric: 65.2231 - val_loss: 65.2683 - val_MinusLogProbMetric: 65.2683 - lr: 3.7037e-05 - 62s/epoch - 318ms/step
Epoch 54/1000
2023-10-26 17:25:21.325 
Epoch 54/1000 
	 loss: 63.8989, MinusLogProbMetric: 63.8989, val_loss: 64.1346, val_MinusLogProbMetric: 64.1346

Epoch 54: val_loss improved from 64.89922 to 64.13456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 63.8989 - MinusLogProbMetric: 63.8989 - val_loss: 64.1346 - val_MinusLogProbMetric: 64.1346 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 55/1000
2023-10-26 17:26:27.656 
Epoch 55/1000 
	 loss: 64.3984, MinusLogProbMetric: 64.3984, val_loss: 63.0669, val_MinusLogProbMetric: 63.0669

Epoch 55: val_loss improved from 64.13456 to 63.06692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 64.3984 - MinusLogProbMetric: 64.3984 - val_loss: 63.0669 - val_MinusLogProbMetric: 63.0669 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 56/1000
2023-10-26 17:27:30.665 
Epoch 56/1000 
	 loss: 62.6418, MinusLogProbMetric: 62.6418, val_loss: 62.3341, val_MinusLogProbMetric: 62.3341

Epoch 56: val_loss improved from 63.06692 to 62.33412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 63s - loss: 62.6418 - MinusLogProbMetric: 62.6418 - val_loss: 62.3341 - val_MinusLogProbMetric: 62.3341 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 57/1000
2023-10-26 17:28:37.100 
Epoch 57/1000 
	 loss: 63.1412, MinusLogProbMetric: 63.1412, val_loss: 62.4290, val_MinusLogProbMetric: 62.4290

Epoch 57: val_loss did not improve from 62.33412
196/196 - 66s - loss: 63.1412 - MinusLogProbMetric: 63.1412 - val_loss: 62.4290 - val_MinusLogProbMetric: 62.4290 - lr: 3.7037e-05 - 66s/epoch - 334ms/step
Epoch 58/1000
2023-10-26 17:29:42.808 
Epoch 58/1000 
	 loss: 62.2947, MinusLogProbMetric: 62.2947, val_loss: 61.4477, val_MinusLogProbMetric: 61.4477

Epoch 58: val_loss improved from 62.33412 to 61.44769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 62.2947 - MinusLogProbMetric: 62.2947 - val_loss: 61.4477 - val_MinusLogProbMetric: 61.4477 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 59/1000
2023-10-26 17:30:50.881 
Epoch 59/1000 
	 loss: 60.9057, MinusLogProbMetric: 60.9057, val_loss: 61.2733, val_MinusLogProbMetric: 61.2733

Epoch 59: val_loss improved from 61.44769 to 61.27335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 60.9057 - MinusLogProbMetric: 60.9057 - val_loss: 61.2733 - val_MinusLogProbMetric: 61.2733 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 60/1000
2023-10-26 17:31:57.863 
Epoch 60/1000 
	 loss: 60.4372, MinusLogProbMetric: 60.4372, val_loss: 60.6692, val_MinusLogProbMetric: 60.6692

Epoch 60: val_loss improved from 61.27335 to 60.66917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 60.4372 - MinusLogProbMetric: 60.4372 - val_loss: 60.6692 - val_MinusLogProbMetric: 60.6692 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 61/1000
2023-10-26 17:33:05.792 
Epoch 61/1000 
	 loss: 67.7330, MinusLogProbMetric: 67.7330, val_loss: 62.8690, val_MinusLogProbMetric: 62.8690

Epoch 61: val_loss did not improve from 60.66917
196/196 - 67s - loss: 67.7330 - MinusLogProbMetric: 67.7330 - val_loss: 62.8690 - val_MinusLogProbMetric: 62.8690 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 62/1000
2023-10-26 17:34:09.470 
Epoch 62/1000 
	 loss: 60.9975, MinusLogProbMetric: 60.9975, val_loss: 60.6141, val_MinusLogProbMetric: 60.6141

Epoch 62: val_loss improved from 60.66917 to 60.61410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 64s - loss: 60.9975 - MinusLogProbMetric: 60.9975 - val_loss: 60.6141 - val_MinusLogProbMetric: 60.6141 - lr: 3.7037e-05 - 64s/epoch - 329ms/step
Epoch 63/1000
2023-10-26 17:35:15.859 
Epoch 63/1000 
	 loss: 59.7982, MinusLogProbMetric: 59.7982, val_loss: 59.6425, val_MinusLogProbMetric: 59.6425

Epoch 63: val_loss improved from 60.61410 to 59.64248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 59.7982 - MinusLogProbMetric: 59.7982 - val_loss: 59.6425 - val_MinusLogProbMetric: 59.6425 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 64/1000
2023-10-26 17:36:22.856 
Epoch 64/1000 
	 loss: 59.5902, MinusLogProbMetric: 59.5902, val_loss: 58.9279, val_MinusLogProbMetric: 58.9279

Epoch 64: val_loss improved from 59.64248 to 58.92793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 59.5902 - MinusLogProbMetric: 59.5902 - val_loss: 58.9279 - val_MinusLogProbMetric: 58.9279 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 65/1000
2023-10-26 17:37:30.487 
Epoch 65/1000 
	 loss: 61.0605, MinusLogProbMetric: 61.0605, val_loss: 59.6998, val_MinusLogProbMetric: 59.6998

Epoch 65: val_loss did not improve from 58.92793
196/196 - 67s - loss: 61.0605 - MinusLogProbMetric: 61.0605 - val_loss: 59.6998 - val_MinusLogProbMetric: 59.6998 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 66/1000
2023-10-26 17:38:37.387 
Epoch 66/1000 
	 loss: 57.8051, MinusLogProbMetric: 57.8051, val_loss: 57.5032, val_MinusLogProbMetric: 57.5032

Epoch 66: val_loss improved from 58.92793 to 57.50318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 57.8051 - MinusLogProbMetric: 57.8051 - val_loss: 57.5032 - val_MinusLogProbMetric: 57.5032 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 67/1000
2023-10-26 17:39:44.553 
Epoch 67/1000 
	 loss: 57.0399, MinusLogProbMetric: 57.0399, val_loss: 56.9863, val_MinusLogProbMetric: 56.9863

Epoch 67: val_loss improved from 57.50318 to 56.98629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 57.0399 - MinusLogProbMetric: 57.0399 - val_loss: 56.9863 - val_MinusLogProbMetric: 56.9863 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 68/1000
2023-10-26 17:40:52.227 
Epoch 68/1000 
	 loss: 56.6006, MinusLogProbMetric: 56.6006, val_loss: 56.8123, val_MinusLogProbMetric: 56.8123

Epoch 68: val_loss improved from 56.98629 to 56.81226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 56.6006 - MinusLogProbMetric: 56.6006 - val_loss: 56.8123 - val_MinusLogProbMetric: 56.8123 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 69/1000
2023-10-26 17:41:59.918 
Epoch 69/1000 
	 loss: 56.8381, MinusLogProbMetric: 56.8381, val_loss: 73.9827, val_MinusLogProbMetric: 73.9827

Epoch 69: val_loss did not improve from 56.81226
196/196 - 67s - loss: 56.8381 - MinusLogProbMetric: 56.8381 - val_loss: 73.9827 - val_MinusLogProbMetric: 73.9827 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 70/1000
2023-10-26 17:43:07.285 
Epoch 70/1000 
	 loss: 73.8098, MinusLogProbMetric: 73.8098, val_loss: 58.0006, val_MinusLogProbMetric: 58.0006

Epoch 70: val_loss did not improve from 56.81226
196/196 - 67s - loss: 73.8098 - MinusLogProbMetric: 73.8098 - val_loss: 58.0006 - val_MinusLogProbMetric: 58.0006 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 71/1000
2023-10-26 17:44:06.822 
Epoch 71/1000 
	 loss: 56.6116, MinusLogProbMetric: 56.6116, val_loss: 56.4219, val_MinusLogProbMetric: 56.4219

Epoch 71: val_loss improved from 56.81226 to 56.42187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 60s - loss: 56.6116 - MinusLogProbMetric: 56.6116 - val_loss: 56.4219 - val_MinusLogProbMetric: 56.4219 - lr: 3.7037e-05 - 60s/epoch - 308ms/step
Epoch 72/1000
2023-10-26 17:45:07.559 
Epoch 72/1000 
	 loss: 55.7620, MinusLogProbMetric: 55.7620, val_loss: 56.3455, val_MinusLogProbMetric: 56.3455

Epoch 72: val_loss improved from 56.42187 to 56.34551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 61s - loss: 55.7620 - MinusLogProbMetric: 55.7620 - val_loss: 56.3455 - val_MinusLogProbMetric: 56.3455 - lr: 3.7037e-05 - 61s/epoch - 311ms/step
Epoch 73/1000
2023-10-26 17:46:15.823 
Epoch 73/1000 
	 loss: 159.0075, MinusLogProbMetric: 159.0075, val_loss: 116.2808, val_MinusLogProbMetric: 116.2808

Epoch 73: val_loss did not improve from 56.34551
196/196 - 67s - loss: 159.0075 - MinusLogProbMetric: 159.0075 - val_loss: 116.2808 - val_MinusLogProbMetric: 116.2808 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 74/1000
2023-10-26 17:47:22.824 
Epoch 74/1000 
	 loss: 101.0962, MinusLogProbMetric: 101.0962, val_loss: 89.3475, val_MinusLogProbMetric: 89.3475

Epoch 74: val_loss did not improve from 56.34551
196/196 - 67s - loss: 101.0962 - MinusLogProbMetric: 101.0962 - val_loss: 89.3475 - val_MinusLogProbMetric: 89.3475 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 75/1000
2023-10-26 17:48:29.022 
Epoch 75/1000 
	 loss: 98.7161, MinusLogProbMetric: 98.7161, val_loss: 145.9384, val_MinusLogProbMetric: 145.9384

Epoch 75: val_loss did not improve from 56.34551
196/196 - 66s - loss: 98.7161 - MinusLogProbMetric: 98.7161 - val_loss: 145.9384 - val_MinusLogProbMetric: 145.9384 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 76/1000
2023-10-26 17:49:35.569 
Epoch 76/1000 
	 loss: 100.1245, MinusLogProbMetric: 100.1245, val_loss: 85.0869, val_MinusLogProbMetric: 85.0869

Epoch 76: val_loss did not improve from 56.34551
196/196 - 67s - loss: 100.1245 - MinusLogProbMetric: 100.1245 - val_loss: 85.0869 - val_MinusLogProbMetric: 85.0869 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 77/1000
2023-10-26 17:50:41.704 
Epoch 77/1000 
	 loss: 78.2198, MinusLogProbMetric: 78.2198, val_loss: 73.7073, val_MinusLogProbMetric: 73.7073

Epoch 77: val_loss did not improve from 56.34551
196/196 - 66s - loss: 78.2198 - MinusLogProbMetric: 78.2198 - val_loss: 73.7073 - val_MinusLogProbMetric: 73.7073 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 78/1000
2023-10-26 17:51:48.745 
Epoch 78/1000 
	 loss: 71.3504, MinusLogProbMetric: 71.3504, val_loss: 69.8217, val_MinusLogProbMetric: 69.8217

Epoch 78: val_loss did not improve from 56.34551
196/196 - 67s - loss: 71.3504 - MinusLogProbMetric: 71.3504 - val_loss: 69.8217 - val_MinusLogProbMetric: 69.8217 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 79/1000
2023-10-26 17:52:55.274 
Epoch 79/1000 
	 loss: 68.1802, MinusLogProbMetric: 68.1802, val_loss: 67.4274, val_MinusLogProbMetric: 67.4274

Epoch 79: val_loss did not improve from 56.34551
196/196 - 67s - loss: 68.1802 - MinusLogProbMetric: 68.1802 - val_loss: 67.4274 - val_MinusLogProbMetric: 67.4274 - lr: 3.7037e-05 - 67s/epoch - 339ms/step
Epoch 80/1000
2023-10-26 17:54:01.852 
Epoch 80/1000 
	 loss: 66.0783, MinusLogProbMetric: 66.0783, val_loss: 65.2905, val_MinusLogProbMetric: 65.2905

Epoch 80: val_loss did not improve from 56.34551
196/196 - 67s - loss: 66.0783 - MinusLogProbMetric: 66.0783 - val_loss: 65.2905 - val_MinusLogProbMetric: 65.2905 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 81/1000
2023-10-26 17:55:08.923 
Epoch 81/1000 
	 loss: 64.3855, MinusLogProbMetric: 64.3855, val_loss: 64.0584, val_MinusLogProbMetric: 64.0584

Epoch 81: val_loss did not improve from 56.34551
196/196 - 67s - loss: 64.3855 - MinusLogProbMetric: 64.3855 - val_loss: 64.0584 - val_MinusLogProbMetric: 64.0584 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 82/1000
2023-10-26 17:56:15.629 
Epoch 82/1000 
	 loss: 63.0528, MinusLogProbMetric: 63.0528, val_loss: 62.5844, val_MinusLogProbMetric: 62.5844

Epoch 82: val_loss did not improve from 56.34551
196/196 - 67s - loss: 63.0528 - MinusLogProbMetric: 63.0528 - val_loss: 62.5844 - val_MinusLogProbMetric: 62.5844 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 83/1000
2023-10-26 17:57:22.397 
Epoch 83/1000 
	 loss: 61.9077, MinusLogProbMetric: 61.9077, val_loss: 61.7953, val_MinusLogProbMetric: 61.7953

Epoch 83: val_loss did not improve from 56.34551
196/196 - 67s - loss: 61.9077 - MinusLogProbMetric: 61.9077 - val_loss: 61.7953 - val_MinusLogProbMetric: 61.7953 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 84/1000
2023-10-26 17:58:29.509 
Epoch 84/1000 
	 loss: 60.9056, MinusLogProbMetric: 60.9056, val_loss: 60.7461, val_MinusLogProbMetric: 60.7461

Epoch 84: val_loss did not improve from 56.34551
196/196 - 67s - loss: 60.9056 - MinusLogProbMetric: 60.9056 - val_loss: 60.7461 - val_MinusLogProbMetric: 60.7461 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 85/1000
2023-10-26 17:59:36.819 
Epoch 85/1000 
	 loss: 60.0417, MinusLogProbMetric: 60.0417, val_loss: 59.9002, val_MinusLogProbMetric: 59.9002

Epoch 85: val_loss did not improve from 56.34551
196/196 - 67s - loss: 60.0417 - MinusLogProbMetric: 60.0417 - val_loss: 59.9002 - val_MinusLogProbMetric: 59.9002 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 86/1000
2023-10-26 18:00:44.287 
Epoch 86/1000 
	 loss: 59.2314, MinusLogProbMetric: 59.2314, val_loss: 58.9465, val_MinusLogProbMetric: 58.9465

Epoch 86: val_loss did not improve from 56.34551
196/196 - 67s - loss: 59.2314 - MinusLogProbMetric: 59.2314 - val_loss: 58.9465 - val_MinusLogProbMetric: 58.9465 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 87/1000
2023-10-26 18:01:51.186 
Epoch 87/1000 
	 loss: 58.2795, MinusLogProbMetric: 58.2795, val_loss: 64.3115, val_MinusLogProbMetric: 64.3115

Epoch 87: val_loss did not improve from 56.34551
196/196 - 67s - loss: 58.2795 - MinusLogProbMetric: 58.2795 - val_loss: 64.3115 - val_MinusLogProbMetric: 64.3115 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 88/1000
2023-10-26 18:02:58.189 
Epoch 88/1000 
	 loss: 58.6792, MinusLogProbMetric: 58.6792, val_loss: 59.3143, val_MinusLogProbMetric: 59.3143

Epoch 88: val_loss did not improve from 56.34551
196/196 - 67s - loss: 58.6792 - MinusLogProbMetric: 58.6792 - val_loss: 59.3143 - val_MinusLogProbMetric: 59.3143 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 89/1000
2023-10-26 18:04:05.396 
Epoch 89/1000 
	 loss: 60.6017, MinusLogProbMetric: 60.6017, val_loss: 56.7204, val_MinusLogProbMetric: 56.7204

Epoch 89: val_loss did not improve from 56.34551
196/196 - 67s - loss: 60.6017 - MinusLogProbMetric: 60.6017 - val_loss: 56.7204 - val_MinusLogProbMetric: 56.7204 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 90/1000
2023-10-26 18:05:11.821 
Epoch 90/1000 
	 loss: 56.3295, MinusLogProbMetric: 56.3295, val_loss: 55.9803, val_MinusLogProbMetric: 55.9803

Epoch 90: val_loss improved from 56.34551 to 55.98027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 56.3295 - MinusLogProbMetric: 56.3295 - val_loss: 55.9803 - val_MinusLogProbMetric: 55.9803 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 91/1000
2023-10-26 18:06:18.139 
Epoch 91/1000 
	 loss: 55.4689, MinusLogProbMetric: 55.4689, val_loss: 55.5734, val_MinusLogProbMetric: 55.5734

Epoch 91: val_loss improved from 55.98027 to 55.57338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 66s - loss: 55.4689 - MinusLogProbMetric: 55.4689 - val_loss: 55.5734 - val_MinusLogProbMetric: 55.5734 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 92/1000
2023-10-26 18:07:20.233 
Epoch 92/1000 
	 loss: 55.0301, MinusLogProbMetric: 55.0301, val_loss: 55.0403, val_MinusLogProbMetric: 55.0403

Epoch 92: val_loss improved from 55.57338 to 55.04026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 62s - loss: 55.0301 - MinusLogProbMetric: 55.0301 - val_loss: 55.0403 - val_MinusLogProbMetric: 55.0403 - lr: 3.7037e-05 - 62s/epoch - 317ms/step
Epoch 93/1000
2023-10-26 18:08:24.097 
Epoch 93/1000 
	 loss: 54.6325, MinusLogProbMetric: 54.6325, val_loss: 54.8122, val_MinusLogProbMetric: 54.8122

Epoch 93: val_loss improved from 55.04026 to 54.81217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 64s - loss: 54.6325 - MinusLogProbMetric: 54.6325 - val_loss: 54.8122 - val_MinusLogProbMetric: 54.8122 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 94/1000
2023-10-26 18:09:30.936 
Epoch 94/1000 
	 loss: 54.1858, MinusLogProbMetric: 54.1858, val_loss: 54.1838, val_MinusLogProbMetric: 54.1838

Epoch 94: val_loss improved from 54.81217 to 54.18379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 54.1858 - MinusLogProbMetric: 54.1858 - val_loss: 54.1838 - val_MinusLogProbMetric: 54.1838 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 95/1000
2023-10-26 18:10:38.265 
Epoch 95/1000 
	 loss: 53.6917, MinusLogProbMetric: 53.6917, val_loss: 53.8090, val_MinusLogProbMetric: 53.8090

Epoch 95: val_loss improved from 54.18379 to 53.80898, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 53.6917 - MinusLogProbMetric: 53.6917 - val_loss: 53.8090 - val_MinusLogProbMetric: 53.8090 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 96/1000
2023-10-26 18:11:45.342 
Epoch 96/1000 
	 loss: 53.3729, MinusLogProbMetric: 53.3729, val_loss: 53.5317, val_MinusLogProbMetric: 53.5317

Epoch 96: val_loss improved from 53.80898 to 53.53170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 53.3729 - MinusLogProbMetric: 53.3729 - val_loss: 53.5317 - val_MinusLogProbMetric: 53.5317 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 97/1000
2023-10-26 18:12:53.016 
Epoch 97/1000 
	 loss: 52.9503, MinusLogProbMetric: 52.9503, val_loss: 53.0849, val_MinusLogProbMetric: 53.0849

Epoch 97: val_loss improved from 53.53170 to 53.08487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 52.9503 - MinusLogProbMetric: 52.9503 - val_loss: 53.0849 - val_MinusLogProbMetric: 53.0849 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 98/1000
2023-10-26 18:14:01.609 
Epoch 98/1000 
	 loss: 52.6273, MinusLogProbMetric: 52.6273, val_loss: 52.7005, val_MinusLogProbMetric: 52.7005

Epoch 98: val_loss improved from 53.08487 to 52.70050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 52.6273 - MinusLogProbMetric: 52.6273 - val_loss: 52.7005 - val_MinusLogProbMetric: 52.7005 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 99/1000
2023-10-26 18:15:09.813 
Epoch 99/1000 
	 loss: 52.3868, MinusLogProbMetric: 52.3868, val_loss: 52.3728, val_MinusLogProbMetric: 52.3728

Epoch 99: val_loss improved from 52.70050 to 52.37276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 52.3868 - MinusLogProbMetric: 52.3868 - val_loss: 52.3728 - val_MinusLogProbMetric: 52.3728 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 100/1000
2023-10-26 18:16:18.185 
Epoch 100/1000 
	 loss: 52.1802, MinusLogProbMetric: 52.1802, val_loss: 52.1416, val_MinusLogProbMetric: 52.1416

Epoch 100: val_loss improved from 52.37276 to 52.14159, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 52.1802 - MinusLogProbMetric: 52.1802 - val_loss: 52.1416 - val_MinusLogProbMetric: 52.1416 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 101/1000
2023-10-26 18:17:20.024 
Epoch 101/1000 
	 loss: 52.0436, MinusLogProbMetric: 52.0436, val_loss: 62.0265, val_MinusLogProbMetric: 62.0265

Epoch 101: val_loss did not improve from 52.14159
196/196 - 61s - loss: 52.0436 - MinusLogProbMetric: 52.0436 - val_loss: 62.0265 - val_MinusLogProbMetric: 62.0265 - lr: 3.7037e-05 - 61s/epoch - 310ms/step
Epoch 102/1000
2023-10-26 18:18:22.449 
Epoch 102/1000 
	 loss: 55.0943, MinusLogProbMetric: 55.0943, val_loss: 52.0866, val_MinusLogProbMetric: 52.0866

Epoch 102: val_loss improved from 52.14159 to 52.08664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 63s - loss: 55.0943 - MinusLogProbMetric: 55.0943 - val_loss: 52.0866 - val_MinusLogProbMetric: 52.0866 - lr: 3.7037e-05 - 63s/epoch - 323ms/step
Epoch 103/1000
2023-10-26 18:19:19.486 
Epoch 103/1000 
	 loss: 51.6952, MinusLogProbMetric: 51.6952, val_loss: 51.6798, val_MinusLogProbMetric: 51.6798

Epoch 103: val_loss improved from 52.08664 to 51.67978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 57s - loss: 51.6952 - MinusLogProbMetric: 51.6952 - val_loss: 51.6798 - val_MinusLogProbMetric: 51.6798 - lr: 3.7037e-05 - 57s/epoch - 292ms/step
Epoch 104/1000
2023-10-26 18:20:27.671 
Epoch 104/1000 
	 loss: 51.2915, MinusLogProbMetric: 51.2915, val_loss: 51.2032, val_MinusLogProbMetric: 51.2032

Epoch 104: val_loss improved from 51.67978 to 51.20325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 51.2915 - MinusLogProbMetric: 51.2915 - val_loss: 51.2032 - val_MinusLogProbMetric: 51.2032 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 105/1000
2023-10-26 18:21:35.561 
Epoch 105/1000 
	 loss: 83.1603, MinusLogProbMetric: 83.1603, val_loss: 94.5528, val_MinusLogProbMetric: 94.5528

Epoch 105: val_loss did not improve from 51.20325
196/196 - 67s - loss: 83.1603 - MinusLogProbMetric: 83.1603 - val_loss: 94.5528 - val_MinusLogProbMetric: 94.5528 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 106/1000
2023-10-26 18:22:42.921 
Epoch 106/1000 
	 loss: 92.3560, MinusLogProbMetric: 92.3560, val_loss: 91.0968, val_MinusLogProbMetric: 91.0968

Epoch 106: val_loss did not improve from 51.20325
196/196 - 67s - loss: 92.3560 - MinusLogProbMetric: 92.3560 - val_loss: 91.0968 - val_MinusLogProbMetric: 91.0968 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 107/1000
2023-10-26 18:23:49.747 
Epoch 107/1000 
	 loss: 90.0240, MinusLogProbMetric: 90.0240, val_loss: 89.6269, val_MinusLogProbMetric: 89.6269

Epoch 107: val_loss did not improve from 51.20325
196/196 - 67s - loss: 90.0240 - MinusLogProbMetric: 90.0240 - val_loss: 89.6269 - val_MinusLogProbMetric: 89.6269 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 108/1000
2023-10-26 18:24:56.474 
Epoch 108/1000 
	 loss: 88.6068, MinusLogProbMetric: 88.6068, val_loss: 87.6725, val_MinusLogProbMetric: 87.6725

Epoch 108: val_loss did not improve from 51.20325
196/196 - 67s - loss: 88.6068 - MinusLogProbMetric: 88.6068 - val_loss: 87.6725 - val_MinusLogProbMetric: 87.6725 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 109/1000
2023-10-26 18:26:03.655 
Epoch 109/1000 
	 loss: 65.7427, MinusLogProbMetric: 65.7427, val_loss: 55.9022, val_MinusLogProbMetric: 55.9022

Epoch 109: val_loss did not improve from 51.20325
196/196 - 67s - loss: 65.7427 - MinusLogProbMetric: 65.7427 - val_loss: 55.9022 - val_MinusLogProbMetric: 55.9022 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 110/1000
2023-10-26 18:27:10.390 
Epoch 110/1000 
	 loss: 52.9145, MinusLogProbMetric: 52.9145, val_loss: 52.2554, val_MinusLogProbMetric: 52.2554

Epoch 110: val_loss did not improve from 51.20325
196/196 - 67s - loss: 52.9145 - MinusLogProbMetric: 52.9145 - val_loss: 52.2554 - val_MinusLogProbMetric: 52.2554 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 111/1000
2023-10-26 18:28:17.600 
Epoch 111/1000 
	 loss: 51.7173, MinusLogProbMetric: 51.7173, val_loss: 51.3867, val_MinusLogProbMetric: 51.3867

Epoch 111: val_loss did not improve from 51.20325
196/196 - 67s - loss: 51.7173 - MinusLogProbMetric: 51.7173 - val_loss: 51.3867 - val_MinusLogProbMetric: 51.3867 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 112/1000
2023-10-26 18:29:24.571 
Epoch 112/1000 
	 loss: 51.0968, MinusLogProbMetric: 51.0968, val_loss: 50.9391, val_MinusLogProbMetric: 50.9391

Epoch 112: val_loss improved from 51.20325 to 50.93909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 51.0968 - MinusLogProbMetric: 51.0968 - val_loss: 50.9391 - val_MinusLogProbMetric: 50.9391 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 113/1000
2023-10-26 18:30:32.689 
Epoch 113/1000 
	 loss: 50.4606, MinusLogProbMetric: 50.4606, val_loss: 50.6205, val_MinusLogProbMetric: 50.6205

Epoch 113: val_loss improved from 50.93909 to 50.62053, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 50.4606 - MinusLogProbMetric: 50.4606 - val_loss: 50.6205 - val_MinusLogProbMetric: 50.6205 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 114/1000
2023-10-26 18:31:40.526 
Epoch 114/1000 
	 loss: 50.0974, MinusLogProbMetric: 50.0974, val_loss: 50.1346, val_MinusLogProbMetric: 50.1346

Epoch 114: val_loss improved from 50.62053 to 50.13458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 50.0974 - MinusLogProbMetric: 50.0974 - val_loss: 50.1346 - val_MinusLogProbMetric: 50.1346 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 115/1000
2023-10-26 18:32:48.518 
Epoch 115/1000 
	 loss: 49.7551, MinusLogProbMetric: 49.7551, val_loss: 50.0862, val_MinusLogProbMetric: 50.0862

Epoch 115: val_loss improved from 50.13458 to 50.08620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 49.7551 - MinusLogProbMetric: 49.7551 - val_loss: 50.0862 - val_MinusLogProbMetric: 50.0862 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 116/1000
2023-10-26 18:33:56.884 
Epoch 116/1000 
	 loss: 51.0164, MinusLogProbMetric: 51.0164, val_loss: 90.0505, val_MinusLogProbMetric: 90.0505

Epoch 116: val_loss did not improve from 50.08620
196/196 - 67s - loss: 51.0164 - MinusLogProbMetric: 51.0164 - val_loss: 90.0505 - val_MinusLogProbMetric: 90.0505 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 117/1000
2023-10-26 18:35:04.152 
Epoch 117/1000 
	 loss: 63.4112, MinusLogProbMetric: 63.4112, val_loss: 60.4399, val_MinusLogProbMetric: 60.4399

Epoch 117: val_loss did not improve from 50.08620
196/196 - 67s - loss: 63.4112 - MinusLogProbMetric: 63.4112 - val_loss: 60.4399 - val_MinusLogProbMetric: 60.4399 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 118/1000
2023-10-26 18:36:11.174 
Epoch 118/1000 
	 loss: 59.2843, MinusLogProbMetric: 59.2843, val_loss: 59.1339, val_MinusLogProbMetric: 59.1339

Epoch 118: val_loss did not improve from 50.08620
196/196 - 67s - loss: 59.2843 - MinusLogProbMetric: 59.2843 - val_loss: 59.1339 - val_MinusLogProbMetric: 59.1339 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 119/1000
2023-10-26 18:37:18.586 
Epoch 119/1000 
	 loss: 50.9729, MinusLogProbMetric: 50.9729, val_loss: 65.8445, val_MinusLogProbMetric: 65.8445

Epoch 119: val_loss did not improve from 50.08620
196/196 - 67s - loss: 50.9729 - MinusLogProbMetric: 50.9729 - val_loss: 65.8445 - val_MinusLogProbMetric: 65.8445 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 120/1000
2023-10-26 18:38:25.689 
Epoch 120/1000 
	 loss: 57.9873, MinusLogProbMetric: 57.9873, val_loss: 53.3930, val_MinusLogProbMetric: 53.3930

Epoch 120: val_loss did not improve from 50.08620
196/196 - 67s - loss: 57.9873 - MinusLogProbMetric: 57.9873 - val_loss: 53.3930 - val_MinusLogProbMetric: 53.3930 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 121/1000
2023-10-26 18:39:32.652 
Epoch 121/1000 
	 loss: 55.3192, MinusLogProbMetric: 55.3192, val_loss: 57.4912, val_MinusLogProbMetric: 57.4912

Epoch 121: val_loss did not improve from 50.08620
196/196 - 67s - loss: 55.3192 - MinusLogProbMetric: 55.3192 - val_loss: 57.4912 - val_MinusLogProbMetric: 57.4912 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 122/1000
2023-10-26 18:40:39.394 
Epoch 122/1000 
	 loss: 58.3989, MinusLogProbMetric: 58.3989, val_loss: 58.3985, val_MinusLogProbMetric: 58.3985

Epoch 122: val_loss did not improve from 50.08620
196/196 - 67s - loss: 58.3989 - MinusLogProbMetric: 58.3989 - val_loss: 58.3985 - val_MinusLogProbMetric: 58.3985 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 123/1000
2023-10-26 18:41:46.170 
Epoch 123/1000 
	 loss: 58.7653, MinusLogProbMetric: 58.7653, val_loss: 100.7740, val_MinusLogProbMetric: 100.7740

Epoch 123: val_loss did not improve from 50.08620
196/196 - 67s - loss: 58.7653 - MinusLogProbMetric: 58.7653 - val_loss: 100.7740 - val_MinusLogProbMetric: 100.7740 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 124/1000
2023-10-26 18:42:53.255 
Epoch 124/1000 
	 loss: 62.0194, MinusLogProbMetric: 62.0194, val_loss: 58.4071, val_MinusLogProbMetric: 58.4071

Epoch 124: val_loss did not improve from 50.08620
196/196 - 67s - loss: 62.0194 - MinusLogProbMetric: 62.0194 - val_loss: 58.4071 - val_MinusLogProbMetric: 58.4071 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 125/1000
2023-10-26 18:44:00.205 
Epoch 125/1000 
	 loss: 57.5707, MinusLogProbMetric: 57.5707, val_loss: 57.4920, val_MinusLogProbMetric: 57.4920

Epoch 125: val_loss did not improve from 50.08620
196/196 - 67s - loss: 57.5707 - MinusLogProbMetric: 57.5707 - val_loss: 57.4920 - val_MinusLogProbMetric: 57.4920 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 126/1000
2023-10-26 18:45:07.417 
Epoch 126/1000 
	 loss: 57.9660, MinusLogProbMetric: 57.9660, val_loss: 57.9606, val_MinusLogProbMetric: 57.9606

Epoch 126: val_loss did not improve from 50.08620
196/196 - 67s - loss: 57.9660 - MinusLogProbMetric: 57.9660 - val_loss: 57.9606 - val_MinusLogProbMetric: 57.9606 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 127/1000
2023-10-26 18:46:14.256 
Epoch 127/1000 
	 loss: 211.7610, MinusLogProbMetric: 211.7610, val_loss: 170.8269, val_MinusLogProbMetric: 170.8269

Epoch 127: val_loss did not improve from 50.08620
196/196 - 67s - loss: 211.7610 - MinusLogProbMetric: 211.7610 - val_loss: 170.8269 - val_MinusLogProbMetric: 170.8269 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 128/1000
2023-10-26 18:47:20.910 
Epoch 128/1000 
	 loss: 135.8641, MinusLogProbMetric: 135.8641, val_loss: 116.0928, val_MinusLogProbMetric: 116.0928

Epoch 128: val_loss did not improve from 50.08620
196/196 - 67s - loss: 135.8641 - MinusLogProbMetric: 135.8641 - val_loss: 116.0928 - val_MinusLogProbMetric: 116.0928 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 129/1000
2023-10-26 18:48:27.949 
Epoch 129/1000 
	 loss: 107.6889, MinusLogProbMetric: 107.6889, val_loss: 96.8048, val_MinusLogProbMetric: 96.8048

Epoch 129: val_loss did not improve from 50.08620
196/196 - 67s - loss: 107.6889 - MinusLogProbMetric: 107.6889 - val_loss: 96.8048 - val_MinusLogProbMetric: 96.8048 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 130/1000
2023-10-26 18:49:34.251 
Epoch 130/1000 
	 loss: 91.9274, MinusLogProbMetric: 91.9274, val_loss: 87.3895, val_MinusLogProbMetric: 87.3895

Epoch 130: val_loss did not improve from 50.08620
196/196 - 66s - loss: 91.9274 - MinusLogProbMetric: 91.9274 - val_loss: 87.3895 - val_MinusLogProbMetric: 87.3895 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 131/1000
2023-10-26 18:50:41.419 
Epoch 131/1000 
	 loss: 84.7503, MinusLogProbMetric: 84.7503, val_loss: 81.6720, val_MinusLogProbMetric: 81.6720

Epoch 131: val_loss did not improve from 50.08620
196/196 - 67s - loss: 84.7503 - MinusLogProbMetric: 84.7503 - val_loss: 81.6720 - val_MinusLogProbMetric: 81.6720 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 132/1000
2023-10-26 18:51:48.548 
Epoch 132/1000 
	 loss: 79.8286, MinusLogProbMetric: 79.8286, val_loss: 77.0701, val_MinusLogProbMetric: 77.0701

Epoch 132: val_loss did not improve from 50.08620
196/196 - 67s - loss: 79.8286 - MinusLogProbMetric: 79.8286 - val_loss: 77.0701 - val_MinusLogProbMetric: 77.0701 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 133/1000
2023-10-26 18:52:55.393 
Epoch 133/1000 
	 loss: 75.3253, MinusLogProbMetric: 75.3253, val_loss: 74.0891, val_MinusLogProbMetric: 74.0891

Epoch 133: val_loss did not improve from 50.08620
196/196 - 67s - loss: 75.3253 - MinusLogProbMetric: 75.3253 - val_loss: 74.0891 - val_MinusLogProbMetric: 74.0891 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 134/1000
2023-10-26 18:54:01.540 
Epoch 134/1000 
	 loss: 72.1937, MinusLogProbMetric: 72.1937, val_loss: 71.1353, val_MinusLogProbMetric: 71.1353

Epoch 134: val_loss did not improve from 50.08620
196/196 - 66s - loss: 72.1937 - MinusLogProbMetric: 72.1937 - val_loss: 71.1353 - val_MinusLogProbMetric: 71.1353 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 135/1000
2023-10-26 18:55:08.401 
Epoch 135/1000 
	 loss: 69.7876, MinusLogProbMetric: 69.7876, val_loss: 69.0970, val_MinusLogProbMetric: 69.0970

Epoch 135: val_loss did not improve from 50.08620
196/196 - 67s - loss: 69.7876 - MinusLogProbMetric: 69.7876 - val_loss: 69.0970 - val_MinusLogProbMetric: 69.0970 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 136/1000
2023-10-26 18:56:15.289 
Epoch 136/1000 
	 loss: 67.6618, MinusLogProbMetric: 67.6618, val_loss: 66.7786, val_MinusLogProbMetric: 66.7786

Epoch 136: val_loss did not improve from 50.08620
196/196 - 67s - loss: 67.6618 - MinusLogProbMetric: 67.6618 - val_loss: 66.7786 - val_MinusLogProbMetric: 66.7786 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 137/1000
2023-10-26 18:57:22.079 
Epoch 137/1000 
	 loss: 65.9726, MinusLogProbMetric: 65.9726, val_loss: 65.1946, val_MinusLogProbMetric: 65.1946

Epoch 137: val_loss did not improve from 50.08620
196/196 - 67s - loss: 65.9726 - MinusLogProbMetric: 65.9726 - val_loss: 65.1946 - val_MinusLogProbMetric: 65.1946 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 138/1000
2023-10-26 18:58:29.277 
Epoch 138/1000 
	 loss: 64.3917, MinusLogProbMetric: 64.3917, val_loss: 64.1482, val_MinusLogProbMetric: 64.1482

Epoch 138: val_loss did not improve from 50.08620
196/196 - 67s - loss: 64.3917 - MinusLogProbMetric: 64.3917 - val_loss: 64.1482 - val_MinusLogProbMetric: 64.1482 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 139/1000
2023-10-26 18:59:35.963 
Epoch 139/1000 
	 loss: 62.9051, MinusLogProbMetric: 62.9051, val_loss: 62.2939, val_MinusLogProbMetric: 62.2939

Epoch 139: val_loss did not improve from 50.08620
196/196 - 67s - loss: 62.9051 - MinusLogProbMetric: 62.9051 - val_loss: 62.2939 - val_MinusLogProbMetric: 62.2939 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 140/1000
2023-10-26 19:00:42.757 
Epoch 140/1000 
	 loss: 62.0921, MinusLogProbMetric: 62.0921, val_loss: 61.4748, val_MinusLogProbMetric: 61.4748

Epoch 140: val_loss did not improve from 50.08620
196/196 - 67s - loss: 62.0921 - MinusLogProbMetric: 62.0921 - val_loss: 61.4748 - val_MinusLogProbMetric: 61.4748 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 141/1000
2023-10-26 19:01:49.211 
Epoch 141/1000 
	 loss: 60.8214, MinusLogProbMetric: 60.8214, val_loss: 61.2753, val_MinusLogProbMetric: 61.2753

Epoch 141: val_loss did not improve from 50.08620
196/196 - 66s - loss: 60.8214 - MinusLogProbMetric: 60.8214 - val_loss: 61.2753 - val_MinusLogProbMetric: 61.2753 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 142/1000
2023-10-26 19:02:56.111 
Epoch 142/1000 
	 loss: 59.7097, MinusLogProbMetric: 59.7097, val_loss: 59.4368, val_MinusLogProbMetric: 59.4368

Epoch 142: val_loss did not improve from 50.08620
196/196 - 67s - loss: 59.7097 - MinusLogProbMetric: 59.7097 - val_loss: 59.4368 - val_MinusLogProbMetric: 59.4368 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 143/1000
2023-10-26 19:04:02.709 
Epoch 143/1000 
	 loss: 58.8975, MinusLogProbMetric: 58.8975, val_loss: 58.7721, val_MinusLogProbMetric: 58.7721

Epoch 143: val_loss did not improve from 50.08620
196/196 - 67s - loss: 58.8975 - MinusLogProbMetric: 58.8975 - val_loss: 58.7721 - val_MinusLogProbMetric: 58.7721 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 144/1000
2023-10-26 19:05:09.740 
Epoch 144/1000 
	 loss: 57.9902, MinusLogProbMetric: 57.9902, val_loss: 58.3765, val_MinusLogProbMetric: 58.3765

Epoch 144: val_loss did not improve from 50.08620
196/196 - 67s - loss: 57.9902 - MinusLogProbMetric: 57.9902 - val_loss: 58.3765 - val_MinusLogProbMetric: 58.3765 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 145/1000
2023-10-26 19:06:16.483 
Epoch 145/1000 
	 loss: 57.1278, MinusLogProbMetric: 57.1278, val_loss: 56.9105, val_MinusLogProbMetric: 56.9105

Epoch 145: val_loss did not improve from 50.08620
196/196 - 67s - loss: 57.1278 - MinusLogProbMetric: 57.1278 - val_loss: 56.9105 - val_MinusLogProbMetric: 56.9105 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 146/1000
2023-10-26 19:07:20.097 
Epoch 146/1000 
	 loss: 56.3066, MinusLogProbMetric: 56.3066, val_loss: 56.0475, val_MinusLogProbMetric: 56.0475

Epoch 146: val_loss did not improve from 50.08620
196/196 - 64s - loss: 56.3066 - MinusLogProbMetric: 56.3066 - val_loss: 56.0475 - val_MinusLogProbMetric: 56.0475 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 147/1000
2023-10-26 19:08:21.494 
Epoch 147/1000 
	 loss: 55.4361, MinusLogProbMetric: 55.4361, val_loss: 55.3336, val_MinusLogProbMetric: 55.3336

Epoch 147: val_loss did not improve from 50.08620
196/196 - 61s - loss: 55.4361 - MinusLogProbMetric: 55.4361 - val_loss: 55.3336 - val_MinusLogProbMetric: 55.3336 - lr: 3.7037e-05 - 61s/epoch - 313ms/step
Epoch 148/1000
2023-10-26 19:09:24.125 
Epoch 148/1000 
	 loss: 54.7191, MinusLogProbMetric: 54.7191, val_loss: 54.7778, val_MinusLogProbMetric: 54.7778

Epoch 148: val_loss did not improve from 50.08620
196/196 - 63s - loss: 54.7191 - MinusLogProbMetric: 54.7191 - val_loss: 54.7778 - val_MinusLogProbMetric: 54.7778 - lr: 3.7037e-05 - 63s/epoch - 320ms/step
Epoch 149/1000
2023-10-26 19:10:31.145 
Epoch 149/1000 
	 loss: 54.1557, MinusLogProbMetric: 54.1557, val_loss: 54.1750, val_MinusLogProbMetric: 54.1750

Epoch 149: val_loss did not improve from 50.08620
196/196 - 67s - loss: 54.1557 - MinusLogProbMetric: 54.1557 - val_loss: 54.1750 - val_MinusLogProbMetric: 54.1750 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 150/1000
2023-10-26 19:11:36.279 
Epoch 150/1000 
	 loss: 53.6744, MinusLogProbMetric: 53.6744, val_loss: 53.6018, val_MinusLogProbMetric: 53.6018

Epoch 150: val_loss did not improve from 50.08620
196/196 - 65s - loss: 53.6744 - MinusLogProbMetric: 53.6744 - val_loss: 53.6018 - val_MinusLogProbMetric: 53.6018 - lr: 3.7037e-05 - 65s/epoch - 332ms/step
Epoch 151/1000
2023-10-26 19:12:43.428 
Epoch 151/1000 
	 loss: 53.1396, MinusLogProbMetric: 53.1396, val_loss: 53.0494, val_MinusLogProbMetric: 53.0494

Epoch 151: val_loss did not improve from 50.08620
196/196 - 67s - loss: 53.1396 - MinusLogProbMetric: 53.1396 - val_loss: 53.0494 - val_MinusLogProbMetric: 53.0494 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 152/1000
2023-10-26 19:13:50.968 
Epoch 152/1000 
	 loss: 52.6636, MinusLogProbMetric: 52.6636, val_loss: 52.7933, val_MinusLogProbMetric: 52.7933

Epoch 152: val_loss did not improve from 50.08620
196/196 - 68s - loss: 52.6636 - MinusLogProbMetric: 52.6636 - val_loss: 52.7933 - val_MinusLogProbMetric: 52.7933 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 153/1000
2023-10-26 19:14:58.457 
Epoch 153/1000 
	 loss: 52.2991, MinusLogProbMetric: 52.2991, val_loss: 52.4137, val_MinusLogProbMetric: 52.4137

Epoch 153: val_loss did not improve from 50.08620
196/196 - 67s - loss: 52.2991 - MinusLogProbMetric: 52.2991 - val_loss: 52.4137 - val_MinusLogProbMetric: 52.4137 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 154/1000
2023-10-26 19:16:06.083 
Epoch 154/1000 
	 loss: 51.8763, MinusLogProbMetric: 51.8763, val_loss: 51.9394, val_MinusLogProbMetric: 51.9394

Epoch 154: val_loss did not improve from 50.08620
196/196 - 68s - loss: 51.8763 - MinusLogProbMetric: 51.8763 - val_loss: 51.9394 - val_MinusLogProbMetric: 51.9394 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 155/1000
2023-10-26 19:17:13.993 
Epoch 155/1000 
	 loss: 51.4911, MinusLogProbMetric: 51.4911, val_loss: 51.6377, val_MinusLogProbMetric: 51.6377

Epoch 155: val_loss did not improve from 50.08620
196/196 - 68s - loss: 51.4911 - MinusLogProbMetric: 51.4911 - val_loss: 51.6377 - val_MinusLogProbMetric: 51.6377 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 156/1000
2023-10-26 19:18:21.355 
Epoch 156/1000 
	 loss: 51.1088, MinusLogProbMetric: 51.1088, val_loss: 51.1753, val_MinusLogProbMetric: 51.1753

Epoch 156: val_loss did not improve from 50.08620
196/196 - 67s - loss: 51.1088 - MinusLogProbMetric: 51.1088 - val_loss: 51.1753 - val_MinusLogProbMetric: 51.1753 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 157/1000
2023-10-26 19:19:29.139 
Epoch 157/1000 
	 loss: 50.7384, MinusLogProbMetric: 50.7384, val_loss: 50.9018, val_MinusLogProbMetric: 50.9018

Epoch 157: val_loss did not improve from 50.08620
196/196 - 68s - loss: 50.7384 - MinusLogProbMetric: 50.7384 - val_loss: 50.9018 - val_MinusLogProbMetric: 50.9018 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 158/1000
2023-10-26 19:20:36.266 
Epoch 158/1000 
	 loss: 50.3947, MinusLogProbMetric: 50.3947, val_loss: 50.4947, val_MinusLogProbMetric: 50.4947

Epoch 158: val_loss did not improve from 50.08620
196/196 - 67s - loss: 50.3947 - MinusLogProbMetric: 50.3947 - val_loss: 50.4947 - val_MinusLogProbMetric: 50.4947 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 159/1000
2023-10-26 19:21:43.544 
Epoch 159/1000 
	 loss: 50.0805, MinusLogProbMetric: 50.0805, val_loss: 50.1360, val_MinusLogProbMetric: 50.1360

Epoch 159: val_loss did not improve from 50.08620
196/196 - 67s - loss: 50.0805 - MinusLogProbMetric: 50.0805 - val_loss: 50.1360 - val_MinusLogProbMetric: 50.1360 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 160/1000
2023-10-26 19:22:51.770 
Epoch 160/1000 
	 loss: 49.7127, MinusLogProbMetric: 49.7127, val_loss: 49.8123, val_MinusLogProbMetric: 49.8123

Epoch 160: val_loss improved from 50.08620 to 49.81231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 49.7127 - MinusLogProbMetric: 49.7127 - val_loss: 49.8123 - val_MinusLogProbMetric: 49.8123 - lr: 3.7037e-05 - 69s/epoch - 353ms/step
Epoch 161/1000
2023-10-26 19:24:00.709 
Epoch 161/1000 
	 loss: 49.3582, MinusLogProbMetric: 49.3582, val_loss: 49.4621, val_MinusLogProbMetric: 49.4621

Epoch 161: val_loss improved from 49.81231 to 49.46212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 49.3582 - MinusLogProbMetric: 49.3582 - val_loss: 49.4621 - val_MinusLogProbMetric: 49.4621 - lr: 3.7037e-05 - 69s/epoch - 351ms/step
Epoch 162/1000
2023-10-26 19:25:08.392 
Epoch 162/1000 
	 loss: 49.0821, MinusLogProbMetric: 49.0821, val_loss: 49.1086, val_MinusLogProbMetric: 49.1086

Epoch 162: val_loss improved from 49.46212 to 49.10860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 49.0821 - MinusLogProbMetric: 49.0821 - val_loss: 49.1086 - val_MinusLogProbMetric: 49.1086 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 163/1000
2023-10-26 19:26:16.899 
Epoch 163/1000 
	 loss: 48.7287, MinusLogProbMetric: 48.7287, val_loss: 48.8418, val_MinusLogProbMetric: 48.8418

Epoch 163: val_loss improved from 49.10860 to 48.84177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 48.7287 - MinusLogProbMetric: 48.7287 - val_loss: 48.8418 - val_MinusLogProbMetric: 48.8418 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 164/1000
2023-10-26 19:27:26.174 
Epoch 164/1000 
	 loss: 48.6921, MinusLogProbMetric: 48.6921, val_loss: 50.9078, val_MinusLogProbMetric: 50.9078

Epoch 164: val_loss did not improve from 48.84177
196/196 - 68s - loss: 48.6921 - MinusLogProbMetric: 48.6921 - val_loss: 50.9078 - val_MinusLogProbMetric: 50.9078 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 165/1000
2023-10-26 19:28:33.824 
Epoch 165/1000 
	 loss: 48.5394, MinusLogProbMetric: 48.5394, val_loss: 48.6825, val_MinusLogProbMetric: 48.6825

Epoch 165: val_loss improved from 48.84177 to 48.68252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 48.5394 - MinusLogProbMetric: 48.5394 - val_loss: 48.6825 - val_MinusLogProbMetric: 48.6825 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 166/1000
2023-10-26 19:29:42.270 
Epoch 166/1000 
	 loss: 50.4473, MinusLogProbMetric: 50.4473, val_loss: 48.8964, val_MinusLogProbMetric: 48.8964

Epoch 166: val_loss did not improve from 48.68252
196/196 - 68s - loss: 50.4473 - MinusLogProbMetric: 50.4473 - val_loss: 48.8964 - val_MinusLogProbMetric: 48.8964 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 167/1000
2023-10-26 19:30:37.552 
Epoch 167/1000 
	 loss: 48.2493, MinusLogProbMetric: 48.2493, val_loss: 48.2628, val_MinusLogProbMetric: 48.2628

Epoch 167: val_loss improved from 48.68252 to 48.26281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 56s - loss: 48.2493 - MinusLogProbMetric: 48.2493 - val_loss: 48.2628 - val_MinusLogProbMetric: 48.2628 - lr: 3.7037e-05 - 56s/epoch - 286ms/step
Epoch 168/1000
2023-10-26 19:31:42.368 
Epoch 168/1000 
	 loss: 47.7842, MinusLogProbMetric: 47.7842, val_loss: 47.9134, val_MinusLogProbMetric: 47.9134

Epoch 168: val_loss improved from 48.26281 to 47.91341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 65s - loss: 47.7842 - MinusLogProbMetric: 47.7842 - val_loss: 47.9134 - val_MinusLogProbMetric: 47.9134 - lr: 3.7037e-05 - 65s/epoch - 331ms/step
Epoch 169/1000
2023-10-26 19:32:50.585 
Epoch 169/1000 
	 loss: 47.4706, MinusLogProbMetric: 47.4706, val_loss: 47.6039, val_MinusLogProbMetric: 47.6039

Epoch 169: val_loss improved from 47.91341 to 47.60390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 47.4706 - MinusLogProbMetric: 47.4706 - val_loss: 47.6039 - val_MinusLogProbMetric: 47.6039 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 170/1000
2023-10-26 19:33:59.093 
Epoch 170/1000 
	 loss: 47.2161, MinusLogProbMetric: 47.2161, val_loss: 47.2957, val_MinusLogProbMetric: 47.2957

Epoch 170: val_loss improved from 47.60390 to 47.29572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 47.2161 - MinusLogProbMetric: 47.2161 - val_loss: 47.2957 - val_MinusLogProbMetric: 47.2957 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 171/1000
2023-10-26 19:35:07.710 
Epoch 171/1000 
	 loss: 47.0241, MinusLogProbMetric: 47.0241, val_loss: 47.1706, val_MinusLogProbMetric: 47.1706

Epoch 171: val_loss improved from 47.29572 to 47.17064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 47.0241 - MinusLogProbMetric: 47.0241 - val_loss: 47.1706 - val_MinusLogProbMetric: 47.1706 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 172/1000
2023-10-26 19:36:15.719 
Epoch 172/1000 
	 loss: 46.6957, MinusLogProbMetric: 46.6957, val_loss: 46.9412, val_MinusLogProbMetric: 46.9412

Epoch 172: val_loss improved from 47.17064 to 46.94122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 46.6957 - MinusLogProbMetric: 46.6957 - val_loss: 46.9412 - val_MinusLogProbMetric: 46.9412 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 173/1000
2023-10-26 19:37:23.948 
Epoch 173/1000 
	 loss: 46.4962, MinusLogProbMetric: 46.4962, val_loss: 47.0541, val_MinusLogProbMetric: 47.0541

Epoch 173: val_loss did not improve from 46.94122
196/196 - 67s - loss: 46.4962 - MinusLogProbMetric: 46.4962 - val_loss: 47.0541 - val_MinusLogProbMetric: 47.0541 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 174/1000
2023-10-26 19:38:31.129 
Epoch 174/1000 
	 loss: 46.3667, MinusLogProbMetric: 46.3667, val_loss: 46.4674, val_MinusLogProbMetric: 46.4674

Epoch 174: val_loss improved from 46.94122 to 46.46736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 46.3667 - MinusLogProbMetric: 46.3667 - val_loss: 46.4674 - val_MinusLogProbMetric: 46.4674 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 175/1000
2023-10-26 19:39:39.774 
Epoch 175/1000 
	 loss: 46.0694, MinusLogProbMetric: 46.0694, val_loss: 46.3265, val_MinusLogProbMetric: 46.3265

Epoch 175: val_loss improved from 46.46736 to 46.32649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 46.0694 - MinusLogProbMetric: 46.0694 - val_loss: 46.3265 - val_MinusLogProbMetric: 46.3265 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 176/1000
2023-10-26 19:40:47.800 
Epoch 176/1000 
	 loss: 45.8472, MinusLogProbMetric: 45.8472, val_loss: 46.0827, val_MinusLogProbMetric: 46.0827

Epoch 176: val_loss improved from 46.32649 to 46.08274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 45.8472 - MinusLogProbMetric: 45.8472 - val_loss: 46.0827 - val_MinusLogProbMetric: 46.0827 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 177/1000
2023-10-26 19:41:56.150 
Epoch 177/1000 
	 loss: 45.7729, MinusLogProbMetric: 45.7729, val_loss: 46.0373, val_MinusLogProbMetric: 46.0373

Epoch 177: val_loss improved from 46.08274 to 46.03726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 45.7729 - MinusLogProbMetric: 45.7729 - val_loss: 46.0373 - val_MinusLogProbMetric: 46.0373 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 178/1000
2023-10-26 19:43:03.662 
Epoch 178/1000 
	 loss: 45.5274, MinusLogProbMetric: 45.5274, val_loss: 45.7368, val_MinusLogProbMetric: 45.7368

Epoch 178: val_loss improved from 46.03726 to 45.73682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 45.5274 - MinusLogProbMetric: 45.5274 - val_loss: 45.7368 - val_MinusLogProbMetric: 45.7368 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 179/1000
2023-10-26 19:44:11.660 
Epoch 179/1000 
	 loss: 45.3043, MinusLogProbMetric: 45.3043, val_loss: 45.5702, val_MinusLogProbMetric: 45.5702

Epoch 179: val_loss improved from 45.73682 to 45.57016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 45.3043 - MinusLogProbMetric: 45.3043 - val_loss: 45.5702 - val_MinusLogProbMetric: 45.5702 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 180/1000
2023-10-26 19:45:19.253 
Epoch 180/1000 
	 loss: 45.1483, MinusLogProbMetric: 45.1483, val_loss: 45.4667, val_MinusLogProbMetric: 45.4667

Epoch 180: val_loss improved from 45.57016 to 45.46670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 45.1483 - MinusLogProbMetric: 45.1483 - val_loss: 45.4667 - val_MinusLogProbMetric: 45.4667 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 181/1000
2023-10-26 19:46:27.787 
Epoch 181/1000 
	 loss: 44.9640, MinusLogProbMetric: 44.9640, val_loss: 45.4981, val_MinusLogProbMetric: 45.4981

Epoch 181: val_loss did not improve from 45.46670
196/196 - 67s - loss: 44.9640 - MinusLogProbMetric: 44.9640 - val_loss: 45.4981 - val_MinusLogProbMetric: 45.4981 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 182/1000
2023-10-26 19:47:35.123 
Epoch 182/1000 
	 loss: 126.0579, MinusLogProbMetric: 126.0579, val_loss: 72.6502, val_MinusLogProbMetric: 72.6502

Epoch 182: val_loss did not improve from 45.46670
196/196 - 67s - loss: 126.0579 - MinusLogProbMetric: 126.0579 - val_loss: 72.6502 - val_MinusLogProbMetric: 72.6502 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 183/1000
2023-10-26 19:48:42.795 
Epoch 183/1000 
	 loss: 64.7886, MinusLogProbMetric: 64.7886, val_loss: 59.5021, val_MinusLogProbMetric: 59.5021

Epoch 183: val_loss did not improve from 45.46670
196/196 - 68s - loss: 64.7886 - MinusLogProbMetric: 64.7886 - val_loss: 59.5021 - val_MinusLogProbMetric: 59.5021 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 184/1000
2023-10-26 19:49:50.156 
Epoch 184/1000 
	 loss: 55.7081, MinusLogProbMetric: 55.7081, val_loss: 53.5091, val_MinusLogProbMetric: 53.5091

Epoch 184: val_loss did not improve from 45.46670
196/196 - 67s - loss: 55.7081 - MinusLogProbMetric: 55.7081 - val_loss: 53.5091 - val_MinusLogProbMetric: 53.5091 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 185/1000
2023-10-26 19:50:57.356 
Epoch 185/1000 
	 loss: 51.5927, MinusLogProbMetric: 51.5927, val_loss: 50.7228, val_MinusLogProbMetric: 50.7228

Epoch 185: val_loss did not improve from 45.46670
196/196 - 67s - loss: 51.5927 - MinusLogProbMetric: 51.5927 - val_loss: 50.7228 - val_MinusLogProbMetric: 50.7228 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 186/1000
2023-10-26 19:52:04.297 
Epoch 186/1000 
	 loss: 49.6720, MinusLogProbMetric: 49.6720, val_loss: 49.3521, val_MinusLogProbMetric: 49.3521

Epoch 186: val_loss did not improve from 45.46670
196/196 - 67s - loss: 49.6720 - MinusLogProbMetric: 49.6720 - val_loss: 49.3521 - val_MinusLogProbMetric: 49.3521 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 187/1000
2023-10-26 19:53:11.286 
Epoch 187/1000 
	 loss: 53.4498, MinusLogProbMetric: 53.4498, val_loss: 49.1870, val_MinusLogProbMetric: 49.1870

Epoch 187: val_loss did not improve from 45.46670
196/196 - 67s - loss: 53.4498 - MinusLogProbMetric: 53.4498 - val_loss: 49.1870 - val_MinusLogProbMetric: 49.1870 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 188/1000
2023-10-26 19:54:18.443 
Epoch 188/1000 
	 loss: 48.4368, MinusLogProbMetric: 48.4368, val_loss: 48.1853, val_MinusLogProbMetric: 48.1853

Epoch 188: val_loss did not improve from 45.46670
196/196 - 67s - loss: 48.4368 - MinusLogProbMetric: 48.4368 - val_loss: 48.1853 - val_MinusLogProbMetric: 48.1853 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 189/1000
2023-10-26 19:55:25.653 
Epoch 189/1000 
	 loss: 47.7265, MinusLogProbMetric: 47.7265, val_loss: 48.5550, val_MinusLogProbMetric: 48.5550

Epoch 189: val_loss did not improve from 45.46670
196/196 - 67s - loss: 47.7265 - MinusLogProbMetric: 47.7265 - val_loss: 48.5550 - val_MinusLogProbMetric: 48.5550 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 190/1000
2023-10-26 19:56:32.188 
Epoch 190/1000 
	 loss: 46.9959, MinusLogProbMetric: 46.9959, val_loss: 46.9697, val_MinusLogProbMetric: 46.9697

Epoch 190: val_loss did not improve from 45.46670
196/196 - 67s - loss: 46.9959 - MinusLogProbMetric: 46.9959 - val_loss: 46.9697 - val_MinusLogProbMetric: 46.9697 - lr: 3.7037e-05 - 67s/epoch - 339ms/step
Epoch 191/1000
2023-10-26 19:57:39.454 
Epoch 191/1000 
	 loss: 46.5168, MinusLogProbMetric: 46.5168, val_loss: 46.5212, val_MinusLogProbMetric: 46.5212

Epoch 191: val_loss did not improve from 45.46670
196/196 - 67s - loss: 46.5168 - MinusLogProbMetric: 46.5168 - val_loss: 46.5212 - val_MinusLogProbMetric: 46.5212 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 192/1000
2023-10-26 19:58:46.738 
Epoch 192/1000 
	 loss: 46.1802, MinusLogProbMetric: 46.1802, val_loss: 46.2473, val_MinusLogProbMetric: 46.2473

Epoch 192: val_loss did not improve from 45.46670
196/196 - 67s - loss: 46.1802 - MinusLogProbMetric: 46.1802 - val_loss: 46.2473 - val_MinusLogProbMetric: 46.2473 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 193/1000
2023-10-26 19:59:53.240 
Epoch 193/1000 
	 loss: 45.8708, MinusLogProbMetric: 45.8708, val_loss: 47.0398, val_MinusLogProbMetric: 47.0398

Epoch 193: val_loss did not improve from 45.46670
196/196 - 67s - loss: 45.8708 - MinusLogProbMetric: 45.8708 - val_loss: 47.0398 - val_MinusLogProbMetric: 47.0398 - lr: 3.7037e-05 - 67s/epoch - 339ms/step
Epoch 194/1000
2023-10-26 20:00:59.633 
Epoch 194/1000 
	 loss: 45.5795, MinusLogProbMetric: 45.5795, val_loss: 46.4608, val_MinusLogProbMetric: 46.4608

Epoch 194: val_loss did not improve from 45.46670
196/196 - 66s - loss: 45.5795 - MinusLogProbMetric: 45.5795 - val_loss: 46.4608 - val_MinusLogProbMetric: 46.4608 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 195/1000
2023-10-26 20:02:06.480 
Epoch 195/1000 
	 loss: 45.3191, MinusLogProbMetric: 45.3191, val_loss: 45.2244, val_MinusLogProbMetric: 45.2244

Epoch 195: val_loss improved from 45.46670 to 45.22445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 45.3191 - MinusLogProbMetric: 45.3191 - val_loss: 45.2244 - val_MinusLogProbMetric: 45.2244 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 196/1000
2023-10-26 20:03:15.237 
Epoch 196/1000 
	 loss: 45.0284, MinusLogProbMetric: 45.0284, val_loss: 45.9512, val_MinusLogProbMetric: 45.9512

Epoch 196: val_loss did not improve from 45.22445
196/196 - 68s - loss: 45.0284 - MinusLogProbMetric: 45.0284 - val_loss: 45.9512 - val_MinusLogProbMetric: 45.9512 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 197/1000
2023-10-26 20:04:22.089 
Epoch 197/1000 
	 loss: 44.8157, MinusLogProbMetric: 44.8157, val_loss: 45.1034, val_MinusLogProbMetric: 45.1034

Epoch 197: val_loss improved from 45.22445 to 45.10343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 44.8157 - MinusLogProbMetric: 44.8157 - val_loss: 45.1034 - val_MinusLogProbMetric: 45.1034 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 198/1000
2023-10-26 20:05:29.650 
Epoch 198/1000 
	 loss: 44.8078, MinusLogProbMetric: 44.8078, val_loss: 44.9749, val_MinusLogProbMetric: 44.9749

Epoch 198: val_loss improved from 45.10343 to 44.97488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 44.8078 - MinusLogProbMetric: 44.8078 - val_loss: 44.9749 - val_MinusLogProbMetric: 44.9749 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 199/1000
2023-10-26 20:06:38.095 
Epoch 199/1000 
	 loss: 44.5725, MinusLogProbMetric: 44.5725, val_loss: 44.8714, val_MinusLogProbMetric: 44.8714

Epoch 199: val_loss improved from 44.97488 to 44.87136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 44.5725 - MinusLogProbMetric: 44.5725 - val_loss: 44.8714 - val_MinusLogProbMetric: 44.8714 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 200/1000
2023-10-26 20:07:46.479 
Epoch 200/1000 
	 loss: 44.2675, MinusLogProbMetric: 44.2675, val_loss: 44.6083, val_MinusLogProbMetric: 44.6083

Epoch 200: val_loss improved from 44.87136 to 44.60831, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 44.2675 - MinusLogProbMetric: 44.2675 - val_loss: 44.6083 - val_MinusLogProbMetric: 44.6083 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 201/1000
2023-10-26 20:08:54.738 
Epoch 201/1000 
	 loss: 43.9657, MinusLogProbMetric: 43.9657, val_loss: 44.0877, val_MinusLogProbMetric: 44.0877

Epoch 201: val_loss improved from 44.60831 to 44.08773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 43.9657 - MinusLogProbMetric: 43.9657 - val_loss: 44.0877 - val_MinusLogProbMetric: 44.0877 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 202/1000
2023-10-26 20:10:02.466 
Epoch 202/1000 
	 loss: 53.7295, MinusLogProbMetric: 53.7295, val_loss: 63.8856, val_MinusLogProbMetric: 63.8856

Epoch 202: val_loss did not improve from 44.08773
196/196 - 67s - loss: 53.7295 - MinusLogProbMetric: 53.7295 - val_loss: 63.8856 - val_MinusLogProbMetric: 63.8856 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 203/1000
2023-10-26 20:11:08.496 
Epoch 203/1000 
	 loss: 48.9510, MinusLogProbMetric: 48.9510, val_loss: 45.9663, val_MinusLogProbMetric: 45.9663

Epoch 203: val_loss did not improve from 44.08773
196/196 - 66s - loss: 48.9510 - MinusLogProbMetric: 48.9510 - val_loss: 45.9663 - val_MinusLogProbMetric: 45.9663 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 204/1000
2023-10-26 20:12:15.173 
Epoch 204/1000 
	 loss: 45.1943, MinusLogProbMetric: 45.1943, val_loss: 44.8433, val_MinusLogProbMetric: 44.8433

Epoch 204: val_loss did not improve from 44.08773
196/196 - 67s - loss: 45.1943 - MinusLogProbMetric: 45.1943 - val_loss: 44.8433 - val_MinusLogProbMetric: 44.8433 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 205/1000
2023-10-26 20:13:22.376 
Epoch 205/1000 
	 loss: 44.5275, MinusLogProbMetric: 44.5275, val_loss: 44.4817, val_MinusLogProbMetric: 44.4817

Epoch 205: val_loss did not improve from 44.08773
196/196 - 67s - loss: 44.5275 - MinusLogProbMetric: 44.5275 - val_loss: 44.4817 - val_MinusLogProbMetric: 44.4817 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 206/1000
2023-10-26 20:14:30.041 
Epoch 206/1000 
	 loss: 43.9776, MinusLogProbMetric: 43.9776, val_loss: 43.8110, val_MinusLogProbMetric: 43.8110

Epoch 206: val_loss improved from 44.08773 to 43.81101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 43.9776 - MinusLogProbMetric: 43.9776 - val_loss: 43.8110 - val_MinusLogProbMetric: 43.8110 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 207/1000
2023-10-26 20:15:38.839 
Epoch 207/1000 
	 loss: 43.4675, MinusLogProbMetric: 43.4675, val_loss: 43.3856, val_MinusLogProbMetric: 43.3856

Epoch 207: val_loss improved from 43.81101 to 43.38561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 43.4675 - MinusLogProbMetric: 43.4675 - val_loss: 43.3856 - val_MinusLogProbMetric: 43.3856 - lr: 3.7037e-05 - 69s/epoch - 351ms/step
Epoch 208/1000
2023-10-26 20:16:47.130 
Epoch 208/1000 
	 loss: 43.0784, MinusLogProbMetric: 43.0784, val_loss: 43.6178, val_MinusLogProbMetric: 43.6178

Epoch 208: val_loss did not improve from 43.38561
196/196 - 67s - loss: 43.0784 - MinusLogProbMetric: 43.0784 - val_loss: 43.6178 - val_MinusLogProbMetric: 43.6178 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 209/1000
2023-10-26 20:17:53.968 
Epoch 209/1000 
	 loss: 42.8912, MinusLogProbMetric: 42.8912, val_loss: 43.4461, val_MinusLogProbMetric: 43.4461

Epoch 209: val_loss did not improve from 43.38561
196/196 - 67s - loss: 42.8912 - MinusLogProbMetric: 42.8912 - val_loss: 43.4461 - val_MinusLogProbMetric: 43.4461 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 210/1000
2023-10-26 20:19:00.526 
Epoch 210/1000 
	 loss: 59.0030, MinusLogProbMetric: 59.0030, val_loss: 46.5009, val_MinusLogProbMetric: 46.5009

Epoch 210: val_loss did not improve from 43.38561
196/196 - 67s - loss: 59.0030 - MinusLogProbMetric: 59.0030 - val_loss: 46.5009 - val_MinusLogProbMetric: 46.5009 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 211/1000
2023-10-26 20:20:07.106 
Epoch 211/1000 
	 loss: 45.0110, MinusLogProbMetric: 45.0110, val_loss: 44.8379, val_MinusLogProbMetric: 44.8379

Epoch 211: val_loss did not improve from 43.38561
196/196 - 67s - loss: 45.0110 - MinusLogProbMetric: 45.0110 - val_loss: 44.8379 - val_MinusLogProbMetric: 44.8379 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 212/1000
2023-10-26 20:21:14.307 
Epoch 212/1000 
	 loss: 43.9938, MinusLogProbMetric: 43.9938, val_loss: 44.0073, val_MinusLogProbMetric: 44.0073

Epoch 212: val_loss did not improve from 43.38561
196/196 - 67s - loss: 43.9938 - MinusLogProbMetric: 43.9938 - val_loss: 44.0073 - val_MinusLogProbMetric: 44.0073 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 213/1000
2023-10-26 20:22:20.642 
Epoch 213/1000 
	 loss: 43.4230, MinusLogProbMetric: 43.4230, val_loss: 43.4790, val_MinusLogProbMetric: 43.4790

Epoch 213: val_loss did not improve from 43.38561
196/196 - 66s - loss: 43.4230 - MinusLogProbMetric: 43.4230 - val_loss: 43.4790 - val_MinusLogProbMetric: 43.4790 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 214/1000
2023-10-26 20:23:27.917 
Epoch 214/1000 
	 loss: 43.0999, MinusLogProbMetric: 43.0999, val_loss: 43.2535, val_MinusLogProbMetric: 43.2535

Epoch 214: val_loss improved from 43.38561 to 43.25347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 43.0999 - MinusLogProbMetric: 43.0999 - val_loss: 43.2535 - val_MinusLogProbMetric: 43.2535 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 215/1000
2023-10-26 20:24:35.755 
Epoch 215/1000 
	 loss: 42.7786, MinusLogProbMetric: 42.7786, val_loss: 42.9193, val_MinusLogProbMetric: 42.9193

Epoch 215: val_loss improved from 43.25347 to 42.91935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 42.7786 - MinusLogProbMetric: 42.7786 - val_loss: 42.9193 - val_MinusLogProbMetric: 42.9193 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 216/1000
2023-10-26 20:25:44.001 
Epoch 216/1000 
	 loss: 42.5976, MinusLogProbMetric: 42.5976, val_loss: 42.8862, val_MinusLogProbMetric: 42.8862

Epoch 216: val_loss improved from 42.91935 to 42.88625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 42.5976 - MinusLogProbMetric: 42.5976 - val_loss: 42.8862 - val_MinusLogProbMetric: 42.8862 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 217/1000
2023-10-26 20:26:52.063 
Epoch 217/1000 
	 loss: 42.4143, MinusLogProbMetric: 42.4143, val_loss: 42.5107, val_MinusLogProbMetric: 42.5107

Epoch 217: val_loss improved from 42.88625 to 42.51074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 42.4143 - MinusLogProbMetric: 42.4143 - val_loss: 42.5107 - val_MinusLogProbMetric: 42.5107 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 218/1000
2023-10-26 20:27:59.741 
Epoch 218/1000 
	 loss: 61.6428, MinusLogProbMetric: 61.6428, val_loss: 103.9429, val_MinusLogProbMetric: 103.9429

Epoch 218: val_loss did not improve from 42.51074
196/196 - 67s - loss: 61.6428 - MinusLogProbMetric: 61.6428 - val_loss: 103.9429 - val_MinusLogProbMetric: 103.9429 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 219/1000
2023-10-26 20:29:06.151 
Epoch 219/1000 
	 loss: 65.5207, MinusLogProbMetric: 65.5207, val_loss: 54.8285, val_MinusLogProbMetric: 54.8285

Epoch 219: val_loss did not improve from 42.51074
196/196 - 66s - loss: 65.5207 - MinusLogProbMetric: 65.5207 - val_loss: 54.8285 - val_MinusLogProbMetric: 54.8285 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 220/1000
2023-10-26 20:30:13.499 
Epoch 220/1000 
	 loss: 51.0567, MinusLogProbMetric: 51.0567, val_loss: 48.6459, val_MinusLogProbMetric: 48.6459

Epoch 220: val_loss did not improve from 42.51074
196/196 - 67s - loss: 51.0567 - MinusLogProbMetric: 51.0567 - val_loss: 48.6459 - val_MinusLogProbMetric: 48.6459 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 221/1000
2023-10-26 20:31:20.463 
Epoch 221/1000 
	 loss: 47.2700, MinusLogProbMetric: 47.2700, val_loss: 46.7754, val_MinusLogProbMetric: 46.7754

Epoch 221: val_loss did not improve from 42.51074
196/196 - 67s - loss: 47.2700 - MinusLogProbMetric: 47.2700 - val_loss: 46.7754 - val_MinusLogProbMetric: 46.7754 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 222/1000
2023-10-26 20:32:27.414 
Epoch 222/1000 
	 loss: 45.9938, MinusLogProbMetric: 45.9938, val_loss: 45.9048, val_MinusLogProbMetric: 45.9048

Epoch 222: val_loss did not improve from 42.51074
196/196 - 67s - loss: 45.9938 - MinusLogProbMetric: 45.9938 - val_loss: 45.9048 - val_MinusLogProbMetric: 45.9048 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 223/1000
2023-10-26 20:33:34.329 
Epoch 223/1000 
	 loss: 45.3143, MinusLogProbMetric: 45.3143, val_loss: 45.3371, val_MinusLogProbMetric: 45.3371

Epoch 223: val_loss did not improve from 42.51074
196/196 - 67s - loss: 45.3143 - MinusLogProbMetric: 45.3143 - val_loss: 45.3371 - val_MinusLogProbMetric: 45.3371 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 224/1000
2023-10-26 20:34:41.030 
Epoch 224/1000 
	 loss: 44.8340, MinusLogProbMetric: 44.8340, val_loss: 45.1556, val_MinusLogProbMetric: 45.1556

Epoch 224: val_loss did not improve from 42.51074
196/196 - 67s - loss: 44.8340 - MinusLogProbMetric: 44.8340 - val_loss: 45.1556 - val_MinusLogProbMetric: 45.1556 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 225/1000
2023-10-26 20:35:47.805 
Epoch 225/1000 
	 loss: 44.5074, MinusLogProbMetric: 44.5074, val_loss: 44.5223, val_MinusLogProbMetric: 44.5223

Epoch 225: val_loss did not improve from 42.51074
196/196 - 67s - loss: 44.5074 - MinusLogProbMetric: 44.5074 - val_loss: 44.5223 - val_MinusLogProbMetric: 44.5223 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 226/1000
2023-10-26 20:36:54.835 
Epoch 226/1000 
	 loss: 44.1217, MinusLogProbMetric: 44.1217, val_loss: 44.1164, val_MinusLogProbMetric: 44.1164

Epoch 226: val_loss did not improve from 42.51074
196/196 - 67s - loss: 44.1217 - MinusLogProbMetric: 44.1217 - val_loss: 44.1164 - val_MinusLogProbMetric: 44.1164 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 227/1000
2023-10-26 20:38:02.010 
Epoch 227/1000 
	 loss: 43.8778, MinusLogProbMetric: 43.8778, val_loss: 44.0878, val_MinusLogProbMetric: 44.0878

Epoch 227: val_loss did not improve from 42.51074
196/196 - 67s - loss: 43.8778 - MinusLogProbMetric: 43.8778 - val_loss: 44.0878 - val_MinusLogProbMetric: 44.0878 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 228/1000
2023-10-26 20:39:09.120 
Epoch 228/1000 
	 loss: 43.5921, MinusLogProbMetric: 43.5921, val_loss: 43.7626, val_MinusLogProbMetric: 43.7626

Epoch 228: val_loss did not improve from 42.51074
196/196 - 67s - loss: 43.5921 - MinusLogProbMetric: 43.5921 - val_loss: 43.7626 - val_MinusLogProbMetric: 43.7626 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 229/1000
2023-10-26 20:40:15.828 
Epoch 229/1000 
	 loss: 43.2879, MinusLogProbMetric: 43.2879, val_loss: 43.6443, val_MinusLogProbMetric: 43.6443

Epoch 229: val_loss did not improve from 42.51074
196/196 - 67s - loss: 43.2879 - MinusLogProbMetric: 43.2879 - val_loss: 43.6443 - val_MinusLogProbMetric: 43.6443 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 230/1000
2023-10-26 20:41:22.694 
Epoch 230/1000 
	 loss: 43.3020, MinusLogProbMetric: 43.3020, val_loss: 43.5659, val_MinusLogProbMetric: 43.5659

Epoch 230: val_loss did not improve from 42.51074
196/196 - 67s - loss: 43.3020 - MinusLogProbMetric: 43.3020 - val_loss: 43.5659 - val_MinusLogProbMetric: 43.5659 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 231/1000
2023-10-26 20:42:29.244 
Epoch 231/1000 
	 loss: 43.0630, MinusLogProbMetric: 43.0630, val_loss: 43.2452, val_MinusLogProbMetric: 43.2452

Epoch 231: val_loss did not improve from 42.51074
196/196 - 67s - loss: 43.0630 - MinusLogProbMetric: 43.0630 - val_loss: 43.2452 - val_MinusLogProbMetric: 43.2452 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 232/1000
2023-10-26 20:43:35.724 
Epoch 232/1000 
	 loss: 42.7985, MinusLogProbMetric: 42.7985, val_loss: 43.1435, val_MinusLogProbMetric: 43.1435

Epoch 232: val_loss did not improve from 42.51074
196/196 - 66s - loss: 42.7985 - MinusLogProbMetric: 42.7985 - val_loss: 43.1435 - val_MinusLogProbMetric: 43.1435 - lr: 3.7037e-05 - 66s/epoch - 339ms/step
Epoch 233/1000
2023-10-26 20:44:42.448 
Epoch 233/1000 
	 loss: 42.6679, MinusLogProbMetric: 42.6679, val_loss: 42.9700, val_MinusLogProbMetric: 42.9700

Epoch 233: val_loss did not improve from 42.51074
196/196 - 67s - loss: 42.6679 - MinusLogProbMetric: 42.6679 - val_loss: 42.9700 - val_MinusLogProbMetric: 42.9700 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 234/1000
2023-10-26 20:45:49.222 
Epoch 234/1000 
	 loss: 42.5166, MinusLogProbMetric: 42.5166, val_loss: 42.9857, val_MinusLogProbMetric: 42.9857

Epoch 234: val_loss did not improve from 42.51074
196/196 - 67s - loss: 42.5166 - MinusLogProbMetric: 42.5166 - val_loss: 42.9857 - val_MinusLogProbMetric: 42.9857 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 235/1000
2023-10-26 20:46:56.138 
Epoch 235/1000 
	 loss: 42.3430, MinusLogProbMetric: 42.3430, val_loss: 42.4852, val_MinusLogProbMetric: 42.4852

Epoch 235: val_loss improved from 42.51074 to 42.48520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 42.3430 - MinusLogProbMetric: 42.3430 - val_loss: 42.4852 - val_MinusLogProbMetric: 42.4852 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 236/1000
2023-10-26 20:48:04.066 
Epoch 236/1000 
	 loss: 44.0315, MinusLogProbMetric: 44.0315, val_loss: 42.8545, val_MinusLogProbMetric: 42.8545

Epoch 236: val_loss did not improve from 42.48520
196/196 - 67s - loss: 44.0315 - MinusLogProbMetric: 44.0315 - val_loss: 42.8545 - val_MinusLogProbMetric: 42.8545 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 237/1000
2023-10-26 20:49:11.132 
Epoch 237/1000 
	 loss: 42.3915, MinusLogProbMetric: 42.3915, val_loss: 42.4614, val_MinusLogProbMetric: 42.4614

Epoch 237: val_loss improved from 42.48520 to 42.46145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 42.3915 - MinusLogProbMetric: 42.3915 - val_loss: 42.4614 - val_MinusLogProbMetric: 42.4614 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 238/1000
2023-10-26 20:50:19.238 
Epoch 238/1000 
	 loss: 42.1553, MinusLogProbMetric: 42.1553, val_loss: 42.1883, val_MinusLogProbMetric: 42.1883

Epoch 238: val_loss improved from 42.46145 to 42.18829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 42.1553 - MinusLogProbMetric: 42.1553 - val_loss: 42.1883 - val_MinusLogProbMetric: 42.1883 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 239/1000
2023-10-26 20:51:27.675 
Epoch 239/1000 
	 loss: 42.0011, MinusLogProbMetric: 42.0011, val_loss: 42.3327, val_MinusLogProbMetric: 42.3327

Epoch 239: val_loss did not improve from 42.18829
196/196 - 67s - loss: 42.0011 - MinusLogProbMetric: 42.0011 - val_loss: 42.3327 - val_MinusLogProbMetric: 42.3327 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 240/1000
2023-10-26 20:52:35.199 
Epoch 240/1000 
	 loss: 41.8632, MinusLogProbMetric: 41.8632, val_loss: 42.1498, val_MinusLogProbMetric: 42.1498

Epoch 240: val_loss improved from 42.18829 to 42.14983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 41.8632 - MinusLogProbMetric: 41.8632 - val_loss: 42.1498 - val_MinusLogProbMetric: 42.1498 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 241/1000
2023-10-26 20:53:43.828 
Epoch 241/1000 
	 loss: 41.8980, MinusLogProbMetric: 41.8980, val_loss: 42.1921, val_MinusLogProbMetric: 42.1921

Epoch 241: val_loss did not improve from 42.14983
196/196 - 68s - loss: 41.8980 - MinusLogProbMetric: 41.8980 - val_loss: 42.1921 - val_MinusLogProbMetric: 42.1921 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 242/1000
2023-10-26 20:54:50.826 
Epoch 242/1000 
	 loss: 41.8114, MinusLogProbMetric: 41.8114, val_loss: 42.0239, val_MinusLogProbMetric: 42.0239

Epoch 242: val_loss improved from 42.14983 to 42.02392, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 41.8114 - MinusLogProbMetric: 41.8114 - val_loss: 42.0239 - val_MinusLogProbMetric: 42.0239 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 243/1000
2023-10-26 20:55:59.073 
Epoch 243/1000 
	 loss: 41.6148, MinusLogProbMetric: 41.6148, val_loss: 41.8019, val_MinusLogProbMetric: 41.8019

Epoch 243: val_loss improved from 42.02392 to 41.80186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 41.6148 - MinusLogProbMetric: 41.6148 - val_loss: 41.8019 - val_MinusLogProbMetric: 41.8019 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 244/1000
2023-10-26 20:57:07.383 
Epoch 244/1000 
	 loss: 41.4703, MinusLogProbMetric: 41.4703, val_loss: 41.7046, val_MinusLogProbMetric: 41.7046

Epoch 244: val_loss improved from 41.80186 to 41.70456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 41.4703 - MinusLogProbMetric: 41.4703 - val_loss: 41.7046 - val_MinusLogProbMetric: 41.7046 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 245/1000
2023-10-26 20:58:15.912 
Epoch 245/1000 
	 loss: 41.3545, MinusLogProbMetric: 41.3545, val_loss: 41.8319, val_MinusLogProbMetric: 41.8319

Epoch 245: val_loss did not improve from 41.70456
196/196 - 67s - loss: 41.3545 - MinusLogProbMetric: 41.3545 - val_loss: 41.8319 - val_MinusLogProbMetric: 41.8319 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 246/1000
2023-10-26 20:59:22.756 
Epoch 246/1000 
	 loss: 41.3195, MinusLogProbMetric: 41.3195, val_loss: 41.5561, val_MinusLogProbMetric: 41.5561

Epoch 246: val_loss improved from 41.70456 to 41.55607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 41.3195 - MinusLogProbMetric: 41.3195 - val_loss: 41.5561 - val_MinusLogProbMetric: 41.5561 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 247/1000
2023-10-26 21:00:30.572 
Epoch 247/1000 
	 loss: 41.1825, MinusLogProbMetric: 41.1825, val_loss: 41.3419, val_MinusLogProbMetric: 41.3419

Epoch 247: val_loss improved from 41.55607 to 41.34192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 41.1825 - MinusLogProbMetric: 41.1825 - val_loss: 41.3419 - val_MinusLogProbMetric: 41.3419 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 248/1000
2023-10-26 21:01:37.919 
Epoch 248/1000 
	 loss: 41.0354, MinusLogProbMetric: 41.0354, val_loss: 41.3035, val_MinusLogProbMetric: 41.3035

Epoch 248: val_loss improved from 41.34192 to 41.30354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 41.0354 - MinusLogProbMetric: 41.0354 - val_loss: 41.3035 - val_MinusLogProbMetric: 41.3035 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 249/1000
2023-10-26 21:02:45.466 
Epoch 249/1000 
	 loss: 40.9262, MinusLogProbMetric: 40.9262, val_loss: 41.2625, val_MinusLogProbMetric: 41.2625

Epoch 249: val_loss improved from 41.30354 to 41.26246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 40.9262 - MinusLogProbMetric: 40.9262 - val_loss: 41.2625 - val_MinusLogProbMetric: 41.2625 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 250/1000
2023-10-26 21:03:53.031 
Epoch 250/1000 
	 loss: 40.8823, MinusLogProbMetric: 40.8823, val_loss: 41.0037, val_MinusLogProbMetric: 41.0037

Epoch 250: val_loss improved from 41.26246 to 41.00372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 40.8823 - MinusLogProbMetric: 40.8823 - val_loss: 41.0037 - val_MinusLogProbMetric: 41.0037 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 251/1000
2023-10-26 21:05:00.882 
Epoch 251/1000 
	 loss: 40.7824, MinusLogProbMetric: 40.7824, val_loss: 41.1422, val_MinusLogProbMetric: 41.1422

Epoch 251: val_loss did not improve from 41.00372
196/196 - 67s - loss: 40.7824 - MinusLogProbMetric: 40.7824 - val_loss: 41.1422 - val_MinusLogProbMetric: 41.1422 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 252/1000
2023-10-26 21:06:08.188 
Epoch 252/1000 
	 loss: 40.7514, MinusLogProbMetric: 40.7514, val_loss: 40.8975, val_MinusLogProbMetric: 40.8975

Epoch 252: val_loss improved from 41.00372 to 40.89754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 40.7514 - MinusLogProbMetric: 40.7514 - val_loss: 40.8975 - val_MinusLogProbMetric: 40.8975 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 253/1000
2023-10-26 21:07:16.467 
Epoch 253/1000 
	 loss: 41.0063, MinusLogProbMetric: 41.0063, val_loss: 41.0622, val_MinusLogProbMetric: 41.0622

Epoch 253: val_loss did not improve from 40.89754
196/196 - 67s - loss: 41.0063 - MinusLogProbMetric: 41.0063 - val_loss: 41.0622 - val_MinusLogProbMetric: 41.0622 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 254/1000
2023-10-26 21:08:23.617 
Epoch 254/1000 
	 loss: 40.5239, MinusLogProbMetric: 40.5239, val_loss: 40.6780, val_MinusLogProbMetric: 40.6780

Epoch 254: val_loss improved from 40.89754 to 40.67802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 40.5239 - MinusLogProbMetric: 40.5239 - val_loss: 40.6780 - val_MinusLogProbMetric: 40.6780 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 255/1000
2023-10-26 21:09:31.515 
Epoch 255/1000 
	 loss: 40.5667, MinusLogProbMetric: 40.5667, val_loss: 40.7563, val_MinusLogProbMetric: 40.7563

Epoch 255: val_loss did not improve from 40.67802
196/196 - 67s - loss: 40.5667 - MinusLogProbMetric: 40.5667 - val_loss: 40.7563 - val_MinusLogProbMetric: 40.7563 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 256/1000
2023-10-26 21:10:38.628 
Epoch 256/1000 
	 loss: 40.3758, MinusLogProbMetric: 40.3758, val_loss: 40.7808, val_MinusLogProbMetric: 40.7808

Epoch 256: val_loss did not improve from 40.67802
196/196 - 67s - loss: 40.3758 - MinusLogProbMetric: 40.3758 - val_loss: 40.7808 - val_MinusLogProbMetric: 40.7808 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 257/1000
2023-10-26 21:11:45.564 
Epoch 257/1000 
	 loss: 40.2727, MinusLogProbMetric: 40.2727, val_loss: 40.7935, val_MinusLogProbMetric: 40.7935

Epoch 257: val_loss did not improve from 40.67802
196/196 - 67s - loss: 40.2727 - MinusLogProbMetric: 40.2727 - val_loss: 40.7935 - val_MinusLogProbMetric: 40.7935 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 258/1000
2023-10-26 21:12:52.379 
Epoch 258/1000 
	 loss: 40.3443, MinusLogProbMetric: 40.3443, val_loss: 40.9374, val_MinusLogProbMetric: 40.9374

Epoch 258: val_loss did not improve from 40.67802
196/196 - 67s - loss: 40.3443 - MinusLogProbMetric: 40.3443 - val_loss: 40.9374 - val_MinusLogProbMetric: 40.9374 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 259/1000
2023-10-26 21:13:58.650 
Epoch 259/1000 
	 loss: 40.1992, MinusLogProbMetric: 40.1992, val_loss: 40.5191, val_MinusLogProbMetric: 40.5191

Epoch 259: val_loss improved from 40.67802 to 40.51913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 40.1992 - MinusLogProbMetric: 40.1992 - val_loss: 40.5191 - val_MinusLogProbMetric: 40.5191 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 260/1000
2023-10-26 21:15:06.414 
Epoch 260/1000 
	 loss: 40.0482, MinusLogProbMetric: 40.0482, val_loss: 40.8710, val_MinusLogProbMetric: 40.8710

Epoch 260: val_loss did not improve from 40.51913
196/196 - 67s - loss: 40.0482 - MinusLogProbMetric: 40.0482 - val_loss: 40.8710 - val_MinusLogProbMetric: 40.8710 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 261/1000
2023-10-26 21:16:13.071 
Epoch 261/1000 
	 loss: 40.0752, MinusLogProbMetric: 40.0752, val_loss: 40.4959, val_MinusLogProbMetric: 40.4959

Epoch 261: val_loss improved from 40.51913 to 40.49590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 40.0752 - MinusLogProbMetric: 40.0752 - val_loss: 40.4959 - val_MinusLogProbMetric: 40.4959 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 262/1000
2023-10-26 21:17:20.779 
Epoch 262/1000 
	 loss: 68.9854, MinusLogProbMetric: 68.9854, val_loss: 46.8675, val_MinusLogProbMetric: 46.8675

Epoch 262: val_loss did not improve from 40.49590
196/196 - 67s - loss: 68.9854 - MinusLogProbMetric: 68.9854 - val_loss: 46.8675 - val_MinusLogProbMetric: 46.8675 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 263/1000
2023-10-26 21:18:27.809 
Epoch 263/1000 
	 loss: 44.8167, MinusLogProbMetric: 44.8167, val_loss: 43.8262, val_MinusLogProbMetric: 43.8262

Epoch 263: val_loss did not improve from 40.49590
196/196 - 67s - loss: 44.8167 - MinusLogProbMetric: 44.8167 - val_loss: 43.8262 - val_MinusLogProbMetric: 43.8262 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 264/1000
2023-10-26 21:19:35.052 
Epoch 264/1000 
	 loss: 42.8729, MinusLogProbMetric: 42.8729, val_loss: 41.4900, val_MinusLogProbMetric: 41.4900

Epoch 264: val_loss did not improve from 40.49590
196/196 - 67s - loss: 42.8729 - MinusLogProbMetric: 42.8729 - val_loss: 41.4900 - val_MinusLogProbMetric: 41.4900 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 265/1000
2023-10-26 21:20:42.147 
Epoch 265/1000 
	 loss: 40.7119, MinusLogProbMetric: 40.7119, val_loss: 40.8578, val_MinusLogProbMetric: 40.8578

Epoch 265: val_loss did not improve from 40.49590
196/196 - 67s - loss: 40.7119 - MinusLogProbMetric: 40.7119 - val_loss: 40.8578 - val_MinusLogProbMetric: 40.8578 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 266/1000
2023-10-26 21:21:48.938 
Epoch 266/1000 
	 loss: 40.4811, MinusLogProbMetric: 40.4811, val_loss: 40.5900, val_MinusLogProbMetric: 40.5900

Epoch 266: val_loss did not improve from 40.49590
196/196 - 67s - loss: 40.4811 - MinusLogProbMetric: 40.4811 - val_loss: 40.5900 - val_MinusLogProbMetric: 40.5900 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 267/1000
2023-10-26 21:22:56.444 
Epoch 267/1000 
	 loss: 40.1764, MinusLogProbMetric: 40.1764, val_loss: 40.5388, val_MinusLogProbMetric: 40.5388

Epoch 267: val_loss did not improve from 40.49590
196/196 - 68s - loss: 40.1764 - MinusLogProbMetric: 40.1764 - val_loss: 40.5388 - val_MinusLogProbMetric: 40.5388 - lr: 3.7037e-05 - 68s/epoch - 344ms/step
Epoch 268/1000
2023-10-26 21:24:03.189 
Epoch 268/1000 
	 loss: 40.1870, MinusLogProbMetric: 40.1870, val_loss: 40.4126, val_MinusLogProbMetric: 40.4126

Epoch 268: val_loss improved from 40.49590 to 40.41262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 40.1870 - MinusLogProbMetric: 40.1870 - val_loss: 40.4126 - val_MinusLogProbMetric: 40.4126 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 269/1000
2023-10-26 21:25:11.452 
Epoch 269/1000 
	 loss: 39.9812, MinusLogProbMetric: 39.9812, val_loss: 40.1596, val_MinusLogProbMetric: 40.1596

Epoch 269: val_loss improved from 40.41262 to 40.15958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 39.9812 - MinusLogProbMetric: 39.9812 - val_loss: 40.1596 - val_MinusLogProbMetric: 40.1596 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 270/1000
2023-10-26 21:26:19.771 
Epoch 270/1000 
	 loss: 79.5604, MinusLogProbMetric: 79.5604, val_loss: 51.0154, val_MinusLogProbMetric: 51.0154

Epoch 270: val_loss did not improve from 40.15958
196/196 - 67s - loss: 79.5604 - MinusLogProbMetric: 79.5604 - val_loss: 51.0154 - val_MinusLogProbMetric: 51.0154 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 271/1000
2023-10-26 21:27:27.323 
Epoch 271/1000 
	 loss: 46.6485, MinusLogProbMetric: 46.6485, val_loss: 44.4447, val_MinusLogProbMetric: 44.4447

Epoch 271: val_loss did not improve from 40.15958
196/196 - 68s - loss: 46.6485 - MinusLogProbMetric: 46.6485 - val_loss: 44.4447 - val_MinusLogProbMetric: 44.4447 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 272/1000
2023-10-26 21:28:34.082 
Epoch 272/1000 
	 loss: 43.0891, MinusLogProbMetric: 43.0891, val_loss: 42.7360, val_MinusLogProbMetric: 42.7360

Epoch 272: val_loss did not improve from 40.15958
196/196 - 67s - loss: 43.0891 - MinusLogProbMetric: 43.0891 - val_loss: 42.7360 - val_MinusLogProbMetric: 42.7360 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 273/1000
2023-10-26 21:29:41.419 
Epoch 273/1000 
	 loss: 42.1743, MinusLogProbMetric: 42.1743, val_loss: 42.3927, val_MinusLogProbMetric: 42.3927

Epoch 273: val_loss did not improve from 40.15958
196/196 - 67s - loss: 42.1743 - MinusLogProbMetric: 42.1743 - val_loss: 42.3927 - val_MinusLogProbMetric: 42.3927 - lr: 3.7037e-05 - 67s/epoch - 344ms/step
Epoch 274/1000
2023-10-26 21:30:48.374 
Epoch 274/1000 
	 loss: 41.4900, MinusLogProbMetric: 41.4900, val_loss: 42.3174, val_MinusLogProbMetric: 42.3174

Epoch 274: val_loss did not improve from 40.15958
196/196 - 67s - loss: 41.4900 - MinusLogProbMetric: 41.4900 - val_loss: 42.3174 - val_MinusLogProbMetric: 42.3174 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 275/1000
2023-10-26 21:31:55.637 
Epoch 275/1000 
	 loss: 41.0302, MinusLogProbMetric: 41.0302, val_loss: 40.8940, val_MinusLogProbMetric: 40.8940

Epoch 275: val_loss did not improve from 40.15958
196/196 - 67s - loss: 41.0302 - MinusLogProbMetric: 41.0302 - val_loss: 40.8940 - val_MinusLogProbMetric: 40.8940 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 276/1000
2023-10-26 21:33:02.791 
Epoch 276/1000 
	 loss: 40.6503, MinusLogProbMetric: 40.6503, val_loss: 41.0949, val_MinusLogProbMetric: 41.0949

Epoch 276: val_loss did not improve from 40.15958
196/196 - 67s - loss: 40.6503 - MinusLogProbMetric: 40.6503 - val_loss: 41.0949 - val_MinusLogProbMetric: 41.0949 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 277/1000
2023-10-26 21:34:09.743 
Epoch 277/1000 
	 loss: 40.3578, MinusLogProbMetric: 40.3578, val_loss: 40.2674, val_MinusLogProbMetric: 40.2674

Epoch 277: val_loss did not improve from 40.15958
196/196 - 67s - loss: 40.3578 - MinusLogProbMetric: 40.3578 - val_loss: 40.2674 - val_MinusLogProbMetric: 40.2674 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 278/1000
2023-10-26 21:35:16.424 
Epoch 278/1000 
	 loss: 40.1455, MinusLogProbMetric: 40.1455, val_loss: 40.3739, val_MinusLogProbMetric: 40.3739

Epoch 278: val_loss did not improve from 40.15958
196/196 - 67s - loss: 40.1455 - MinusLogProbMetric: 40.1455 - val_loss: 40.3739 - val_MinusLogProbMetric: 40.3739 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 279/1000
2023-10-26 21:36:22.739 
Epoch 279/1000 
	 loss: 39.9185, MinusLogProbMetric: 39.9185, val_loss: 40.3906, val_MinusLogProbMetric: 40.3906

Epoch 279: val_loss did not improve from 40.15958
196/196 - 66s - loss: 39.9185 - MinusLogProbMetric: 39.9185 - val_loss: 40.3906 - val_MinusLogProbMetric: 40.3906 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 280/1000
2023-10-26 21:37:29.108 
Epoch 280/1000 
	 loss: 39.7213, MinusLogProbMetric: 39.7213, val_loss: 40.0394, val_MinusLogProbMetric: 40.0394

Epoch 280: val_loss improved from 40.15958 to 40.03938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 67s - loss: 39.7213 - MinusLogProbMetric: 39.7213 - val_loss: 40.0394 - val_MinusLogProbMetric: 40.0394 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 281/1000
2023-10-26 21:38:37.293 
Epoch 281/1000 
	 loss: 42.6498, MinusLogProbMetric: 42.6498, val_loss: 128.3697, val_MinusLogProbMetric: 128.3697

Epoch 281: val_loss did not improve from 40.03938
196/196 - 67s - loss: 42.6498 - MinusLogProbMetric: 42.6498 - val_loss: 128.3697 - val_MinusLogProbMetric: 128.3697 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 282/1000
2023-10-26 21:39:44.824 
Epoch 282/1000 
	 loss: 53.1000, MinusLogProbMetric: 53.1000, val_loss: 45.6255, val_MinusLogProbMetric: 45.6255

Epoch 282: val_loss did not improve from 40.03938
196/196 - 68s - loss: 53.1000 - MinusLogProbMetric: 53.1000 - val_loss: 45.6255 - val_MinusLogProbMetric: 45.6255 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 283/1000
2023-10-26 21:40:51.631 
Epoch 283/1000 
	 loss: 44.2410, MinusLogProbMetric: 44.2410, val_loss: 43.5534, val_MinusLogProbMetric: 43.5534

Epoch 283: val_loss did not improve from 40.03938
196/196 - 67s - loss: 44.2410 - MinusLogProbMetric: 44.2410 - val_loss: 43.5534 - val_MinusLogProbMetric: 43.5534 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 284/1000
2023-10-26 21:41:58.357 
Epoch 284/1000 
	 loss: 42.1277, MinusLogProbMetric: 42.1277, val_loss: 41.9128, val_MinusLogProbMetric: 41.9128

Epoch 284: val_loss did not improve from 40.03938
196/196 - 67s - loss: 42.1277 - MinusLogProbMetric: 42.1277 - val_loss: 41.9128 - val_MinusLogProbMetric: 41.9128 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 285/1000
2023-10-26 21:43:05.190 
Epoch 285/1000 
	 loss: 41.1340, MinusLogProbMetric: 41.1340, val_loss: 40.9774, val_MinusLogProbMetric: 40.9774

Epoch 285: val_loss did not improve from 40.03938
196/196 - 67s - loss: 41.1340 - MinusLogProbMetric: 41.1340 - val_loss: 40.9774 - val_MinusLogProbMetric: 40.9774 - lr: 3.7037e-05 - 67s/epoch - 341ms/step
Epoch 286/1000
2023-10-26 21:44:12.462 
Epoch 286/1000 
	 loss: 43.4186, MinusLogProbMetric: 43.4186, val_loss: 41.0052, val_MinusLogProbMetric: 41.0052

Epoch 286: val_loss did not improve from 40.03938
196/196 - 67s - loss: 43.4186 - MinusLogProbMetric: 43.4186 - val_loss: 41.0052 - val_MinusLogProbMetric: 41.0052 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 287/1000
2023-10-26 21:45:19.093 
Epoch 287/1000 
	 loss: 40.0139, MinusLogProbMetric: 40.0139, val_loss: 40.0648, val_MinusLogProbMetric: 40.0648

Epoch 287: val_loss did not improve from 40.03938
196/196 - 67s - loss: 40.0139 - MinusLogProbMetric: 40.0139 - val_loss: 40.0648 - val_MinusLogProbMetric: 40.0648 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 288/1000
2023-10-26 21:46:26.154 
Epoch 288/1000 
	 loss: 39.5506, MinusLogProbMetric: 39.5506, val_loss: 39.6537, val_MinusLogProbMetric: 39.6537

Epoch 288: val_loss improved from 40.03938 to 39.65372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 39.5506 - MinusLogProbMetric: 39.5506 - val_loss: 39.6537 - val_MinusLogProbMetric: 39.6537 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 289/1000
2023-10-26 21:47:34.148 
Epoch 289/1000 
	 loss: 39.3622, MinusLogProbMetric: 39.3622, val_loss: 40.0970, val_MinusLogProbMetric: 40.0970

Epoch 289: val_loss did not improve from 39.65372
196/196 - 67s - loss: 39.3622 - MinusLogProbMetric: 39.3622 - val_loss: 40.0970 - val_MinusLogProbMetric: 40.0970 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 290/1000
2023-10-26 21:48:41.422 
Epoch 290/1000 
	 loss: 39.1922, MinusLogProbMetric: 39.1922, val_loss: 39.3430, val_MinusLogProbMetric: 39.3430

Epoch 290: val_loss improved from 39.65372 to 39.34298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 39.1922 - MinusLogProbMetric: 39.1922 - val_loss: 39.3430 - val_MinusLogProbMetric: 39.3430 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 291/1000
2023-10-26 21:49:49.321 
Epoch 291/1000 
	 loss: 39.1108, MinusLogProbMetric: 39.1108, val_loss: 39.2239, val_MinusLogProbMetric: 39.2239

Epoch 291: val_loss improved from 39.34298 to 39.22394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 39.1108 - MinusLogProbMetric: 39.1108 - val_loss: 39.2239 - val_MinusLogProbMetric: 39.2239 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 292/1000
2023-10-26 21:50:57.997 
Epoch 292/1000 
	 loss: 38.9828, MinusLogProbMetric: 38.9828, val_loss: 39.5283, val_MinusLogProbMetric: 39.5283

Epoch 292: val_loss did not improve from 39.22394
196/196 - 68s - loss: 38.9828 - MinusLogProbMetric: 38.9828 - val_loss: 39.5283 - val_MinusLogProbMetric: 39.5283 - lr: 3.7037e-05 - 68s/epoch - 345ms/step
Epoch 293/1000
2023-10-26 21:52:04.725 
Epoch 293/1000 
	 loss: 38.9701, MinusLogProbMetric: 38.9701, val_loss: 39.3924, val_MinusLogProbMetric: 39.3924

Epoch 293: val_loss did not improve from 39.22394
196/196 - 67s - loss: 38.9701 - MinusLogProbMetric: 38.9701 - val_loss: 39.3924 - val_MinusLogProbMetric: 39.3924 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 294/1000
2023-10-26 21:53:11.993 
Epoch 294/1000 
	 loss: 38.8265, MinusLogProbMetric: 38.8265, val_loss: 39.9815, val_MinusLogProbMetric: 39.9815

Epoch 294: val_loss did not improve from 39.22394
196/196 - 67s - loss: 38.8265 - MinusLogProbMetric: 38.8265 - val_loss: 39.9815 - val_MinusLogProbMetric: 39.9815 - lr: 3.7037e-05 - 67s/epoch - 343ms/step
Epoch 295/1000
2023-10-26 21:54:19.033 
Epoch 295/1000 
	 loss: 39.0293, MinusLogProbMetric: 39.0293, val_loss: 39.3822, val_MinusLogProbMetric: 39.3822

Epoch 295: val_loss did not improve from 39.22394
196/196 - 67s - loss: 39.0293 - MinusLogProbMetric: 39.0293 - val_loss: 39.3822 - val_MinusLogProbMetric: 39.3822 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 296/1000
2023-10-26 21:55:25.868 
Epoch 296/1000 
	 loss: 38.8508, MinusLogProbMetric: 38.8508, val_loss: 38.9501, val_MinusLogProbMetric: 38.9501

Epoch 296: val_loss improved from 39.22394 to 38.95011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 38.8508 - MinusLogProbMetric: 38.8508 - val_loss: 38.9501 - val_MinusLogProbMetric: 38.9501 - lr: 3.7037e-05 - 68s/epoch - 346ms/step
Epoch 297/1000
2023-10-26 21:56:34.038 
Epoch 297/1000 
	 loss: 38.6808, MinusLogProbMetric: 38.6808, val_loss: 38.7995, val_MinusLogProbMetric: 38.7995

Epoch 297: val_loss improved from 38.95011 to 38.79948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 38.6808 - MinusLogProbMetric: 38.6808 - val_loss: 38.7995 - val_MinusLogProbMetric: 38.7995 - lr: 3.7037e-05 - 68s/epoch - 348ms/step
Epoch 298/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 127: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 21:57:20.760 
Epoch 298/1000 
	 loss: nan, MinusLogProbMetric: 77.7476, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 298: val_loss did not improve from 38.79948
196/196 - 46s - loss: nan - MinusLogProbMetric: 77.7476 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 46s/epoch - 233ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.2345679012345677e-05.
===========
Generating train data for run 390.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_390/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_390
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f374ae9a170>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e022994b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e022994b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e022fbd00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e02053280>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e020537f0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e020538b0>, <keras.callbacks.EarlyStopping object at 0x7f3e02053b20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e02053b50>, <keras.callbacks.TerminateOnNaN object at 0x7f3e02053790>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 390/720 with hyperparameters:
timestamp = 2023-10-26 21:57:30.720237
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
2023-10-26 22:00:46.132 
Epoch 1/1000 
	 loss: 42.2283, MinusLogProbMetric: 42.2283, val_loss: 38.9683, val_MinusLogProbMetric: 38.9683

Epoch 1: val_loss improved from inf to 38.96825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 196s - loss: 42.2283 - MinusLogProbMetric: 42.2283 - val_loss: 38.9683 - val_MinusLogProbMetric: 38.9683 - lr: 1.2346e-05 - 196s/epoch - 1s/step
Epoch 2/1000
2023-10-26 22:01:54.925 
Epoch 2/1000 
	 loss: 38.7295, MinusLogProbMetric: 38.7295, val_loss: 38.7557, val_MinusLogProbMetric: 38.7557

Epoch 2: val_loss improved from 38.96825 to 38.75573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 38.7295 - MinusLogProbMetric: 38.7295 - val_loss: 38.7557 - val_MinusLogProbMetric: 38.7557 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 3/1000
2023-10-26 22:03:03.449 
Epoch 3/1000 
	 loss: 38.6610, MinusLogProbMetric: 38.6610, val_loss: 38.9196, val_MinusLogProbMetric: 38.9196

Epoch 3: val_loss did not improve from 38.75573
196/196 - 68s - loss: 38.6610 - MinusLogProbMetric: 38.6610 - val_loss: 38.9196 - val_MinusLogProbMetric: 38.9196 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 4/1000
2023-10-26 22:04:10.387 
Epoch 4/1000 
	 loss: 38.4287, MinusLogProbMetric: 38.4287, val_loss: 38.6699, val_MinusLogProbMetric: 38.6699

Epoch 4: val_loss improved from 38.75573 to 38.66993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 38.4287 - MinusLogProbMetric: 38.4287 - val_loss: 38.6699 - val_MinusLogProbMetric: 38.6699 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 5/1000
2023-10-26 22:05:19.027 
Epoch 5/1000 
	 loss: 38.2351, MinusLogProbMetric: 38.2351, val_loss: 38.7681, val_MinusLogProbMetric: 38.7681

Epoch 5: val_loss did not improve from 38.66993
196/196 - 68s - loss: 38.2351 - MinusLogProbMetric: 38.2351 - val_loss: 38.7681 - val_MinusLogProbMetric: 38.7681 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 6/1000
2023-10-26 22:06:26.466 
Epoch 6/1000 
	 loss: 38.2034, MinusLogProbMetric: 38.2034, val_loss: 38.9376, val_MinusLogProbMetric: 38.9376

Epoch 6: val_loss did not improve from 38.66993
196/196 - 67s - loss: 38.2034 - MinusLogProbMetric: 38.2034 - val_loss: 38.9376 - val_MinusLogProbMetric: 38.9376 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 7/1000
2023-10-26 22:07:34.101 
Epoch 7/1000 
	 loss: 37.9294, MinusLogProbMetric: 37.9294, val_loss: 38.5139, val_MinusLogProbMetric: 38.5139

Epoch 7: val_loss improved from 38.66993 to 38.51387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 37.9294 - MinusLogProbMetric: 37.9294 - val_loss: 38.5139 - val_MinusLogProbMetric: 38.5139 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 8/1000
2023-10-26 22:08:42.401 
Epoch 8/1000 
	 loss: 37.5001, MinusLogProbMetric: 37.5001, val_loss: 37.4602, val_MinusLogProbMetric: 37.4602

Epoch 8: val_loss improved from 38.51387 to 37.46017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 37.5001 - MinusLogProbMetric: 37.5001 - val_loss: 37.4602 - val_MinusLogProbMetric: 37.4602 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 9/1000
2023-10-26 22:09:50.430 
Epoch 9/1000 
	 loss: 37.3477, MinusLogProbMetric: 37.3477, val_loss: 38.2130, val_MinusLogProbMetric: 38.2130

Epoch 9: val_loss did not improve from 37.46017
196/196 - 67s - loss: 37.3477 - MinusLogProbMetric: 37.3477 - val_loss: 38.2130 - val_MinusLogProbMetric: 38.2130 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 10/1000
2023-10-26 22:10:57.008 
Epoch 10/1000 
	 loss: 36.9558, MinusLogProbMetric: 36.9558, val_loss: 37.5319, val_MinusLogProbMetric: 37.5319

Epoch 10: val_loss did not improve from 37.46017
196/196 - 67s - loss: 36.9558 - MinusLogProbMetric: 36.9558 - val_loss: 37.5319 - val_MinusLogProbMetric: 37.5319 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 11/1000
2023-10-26 22:12:04.036 
Epoch 11/1000 
	 loss: 41.2046, MinusLogProbMetric: 41.2046, val_loss: 37.1698, val_MinusLogProbMetric: 37.1698

Epoch 11: val_loss improved from 37.46017 to 37.16977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 41.2046 - MinusLogProbMetric: 41.2046 - val_loss: 37.1698 - val_MinusLogProbMetric: 37.1698 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 12/1000
2023-10-26 22:13:12.370 
Epoch 12/1000 
	 loss: 37.4714, MinusLogProbMetric: 37.4714, val_loss: 39.6564, val_MinusLogProbMetric: 39.6564

Epoch 12: val_loss did not improve from 37.16977
196/196 - 67s - loss: 37.4714 - MinusLogProbMetric: 37.4714 - val_loss: 39.6564 - val_MinusLogProbMetric: 39.6564 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 13/1000
2023-10-26 22:14:19.746 
Epoch 13/1000 
	 loss: 37.7280, MinusLogProbMetric: 37.7280, val_loss: 38.2442, val_MinusLogProbMetric: 38.2442

Epoch 13: val_loss did not improve from 37.16977
196/196 - 67s - loss: 37.7280 - MinusLogProbMetric: 37.7280 - val_loss: 38.2442 - val_MinusLogProbMetric: 38.2442 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 14/1000
2023-10-26 22:15:26.528 
Epoch 14/1000 
	 loss: 37.3151, MinusLogProbMetric: 37.3151, val_loss: 37.2285, val_MinusLogProbMetric: 37.2285

Epoch 14: val_loss did not improve from 37.16977
196/196 - 67s - loss: 37.3151 - MinusLogProbMetric: 37.3151 - val_loss: 37.2285 - val_MinusLogProbMetric: 37.2285 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 15/1000
2023-10-26 22:16:33.959 
Epoch 15/1000 
	 loss: 37.0657, MinusLogProbMetric: 37.0657, val_loss: 37.7925, val_MinusLogProbMetric: 37.7925

Epoch 15: val_loss did not improve from 37.16977
196/196 - 67s - loss: 37.0657 - MinusLogProbMetric: 37.0657 - val_loss: 37.7925 - val_MinusLogProbMetric: 37.7925 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 16/1000
2023-10-26 22:17:41.574 
Epoch 16/1000 
	 loss: 36.8175, MinusLogProbMetric: 36.8175, val_loss: 37.6158, val_MinusLogProbMetric: 37.6158

Epoch 16: val_loss did not improve from 37.16977
196/196 - 68s - loss: 36.8175 - MinusLogProbMetric: 36.8175 - val_loss: 37.6158 - val_MinusLogProbMetric: 37.6158 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 17/1000
2023-10-26 22:18:48.829 
Epoch 17/1000 
	 loss: 36.7977, MinusLogProbMetric: 36.7977, val_loss: 37.1689, val_MinusLogProbMetric: 37.1689

Epoch 17: val_loss improved from 37.16977 to 37.16895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 36.7977 - MinusLogProbMetric: 36.7977 - val_loss: 37.1689 - val_MinusLogProbMetric: 37.1689 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 18/1000
2023-10-26 22:19:57.585 
Epoch 18/1000 
	 loss: 36.9060, MinusLogProbMetric: 36.9060, val_loss: 37.1384, val_MinusLogProbMetric: 37.1384

Epoch 18: val_loss improved from 37.16895 to 37.13841, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 69s - loss: 36.9060 - MinusLogProbMetric: 36.9060 - val_loss: 37.1384 - val_MinusLogProbMetric: 37.1384 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 19/1000
2023-10-26 22:21:05.577 
Epoch 19/1000 
	 loss: 36.3323, MinusLogProbMetric: 36.3323, val_loss: 36.6413, val_MinusLogProbMetric: 36.6413

Epoch 19: val_loss improved from 37.13841 to 36.64129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 36.3323 - MinusLogProbMetric: 36.3323 - val_loss: 36.6413 - val_MinusLogProbMetric: 36.6413 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 20/1000
2023-10-26 22:22:13.993 
Epoch 20/1000 
	 loss: 36.9236, MinusLogProbMetric: 36.9236, val_loss: 36.0565, val_MinusLogProbMetric: 36.0565

Epoch 20: val_loss improved from 36.64129 to 36.05653, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_390/weights/best_weights.h5
196/196 - 68s - loss: 36.9236 - MinusLogProbMetric: 36.9236 - val_loss: 36.0565 - val_MinusLogProbMetric: 36.0565 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 21/1000
2023-10-26 22:23:21.837 
Epoch 21/1000 
	 loss: 36.3493, MinusLogProbMetric: 36.3493, val_loss: 36.6745, val_MinusLogProbMetric: 36.6745

Epoch 21: val_loss did not improve from 36.05653
196/196 - 67s - loss: 36.3493 - MinusLogProbMetric: 36.3493 - val_loss: 36.6745 - val_MinusLogProbMetric: 36.6745 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 22/1000
2023-10-26 22:24:28.826 
Epoch 22/1000 
	 loss: 37.0614, MinusLogProbMetric: 37.0614, val_loss: 36.2174, val_MinusLogProbMetric: 36.2174

Epoch 22: val_loss did not improve from 36.05653
196/196 - 67s - loss: 37.0614 - MinusLogProbMetric: 37.0614 - val_loss: 36.2174 - val_MinusLogProbMetric: 36.2174 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 23/1000
2023-10-26 22:25:35.744 
Epoch 23/1000 
	 loss: 48.9048, MinusLogProbMetric: 48.9048, val_loss: 47.6952, val_MinusLogProbMetric: 47.6952

Epoch 23: val_loss did not improve from 36.05653
196/196 - 67s - loss: 48.9048 - MinusLogProbMetric: 48.9048 - val_loss: 47.6952 - val_MinusLogProbMetric: 47.6952 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 24/1000
2023-10-26 22:26:43.161 
Epoch 24/1000 
	 loss: 146.1910, MinusLogProbMetric: 146.1910, val_loss: 111.8571, val_MinusLogProbMetric: 111.8571

Epoch 24: val_loss did not improve from 36.05653
196/196 - 67s - loss: 146.1910 - MinusLogProbMetric: 146.1910 - val_loss: 111.8571 - val_MinusLogProbMetric: 111.8571 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 25/1000
2023-10-26 22:27:50.240 
Epoch 25/1000 
	 loss: 105.2685, MinusLogProbMetric: 105.2685, val_loss: 102.5222, val_MinusLogProbMetric: 102.5222

Epoch 25: val_loss did not improve from 36.05653
196/196 - 67s - loss: 105.2685 - MinusLogProbMetric: 105.2685 - val_loss: 102.5222 - val_MinusLogProbMetric: 102.5222 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 26/1000
2023-10-26 22:28:56.686 
Epoch 26/1000 
	 loss: 95.7391, MinusLogProbMetric: 95.7391, val_loss: 91.8658, val_MinusLogProbMetric: 91.8658

Epoch 26: val_loss did not improve from 36.05653
196/196 - 66s - loss: 95.7391 - MinusLogProbMetric: 95.7391 - val_loss: 91.8658 - val_MinusLogProbMetric: 91.8658 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 27/1000
2023-10-26 22:30:04.041 
Epoch 27/1000 
	 loss: 91.5889, MinusLogProbMetric: 91.5889, val_loss: 86.9881, val_MinusLogProbMetric: 86.9881

Epoch 27: val_loss did not improve from 36.05653
196/196 - 67s - loss: 91.5889 - MinusLogProbMetric: 91.5889 - val_loss: 86.9881 - val_MinusLogProbMetric: 86.9881 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 28/1000
2023-10-26 22:31:11.581 
Epoch 28/1000 
	 loss: 84.5882, MinusLogProbMetric: 84.5882, val_loss: 82.9603, val_MinusLogProbMetric: 82.9603

Epoch 28: val_loss did not improve from 36.05653
196/196 - 68s - loss: 84.5882 - MinusLogProbMetric: 84.5882 - val_loss: 82.9603 - val_MinusLogProbMetric: 82.9603 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 29/1000
2023-10-26 22:32:19.517 
Epoch 29/1000 
	 loss: 81.5822, MinusLogProbMetric: 81.5822, val_loss: 80.8737, val_MinusLogProbMetric: 80.8737

Epoch 29: val_loss did not improve from 36.05653
196/196 - 68s - loss: 81.5822 - MinusLogProbMetric: 81.5822 - val_loss: 80.8737 - val_MinusLogProbMetric: 80.8737 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 30/1000
2023-10-26 22:33:27.020 
Epoch 30/1000 
	 loss: 79.7769, MinusLogProbMetric: 79.7769, val_loss: 79.3988, val_MinusLogProbMetric: 79.3988

Epoch 30: val_loss did not improve from 36.05653
196/196 - 67s - loss: 79.7769 - MinusLogProbMetric: 79.7769 - val_loss: 79.3988 - val_MinusLogProbMetric: 79.3988 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 31/1000
2023-10-26 22:34:34.077 
Epoch 31/1000 
	 loss: 78.5637, MinusLogProbMetric: 78.5637, val_loss: 78.2138, val_MinusLogProbMetric: 78.2138

Epoch 31: val_loss did not improve from 36.05653
196/196 - 67s - loss: 78.5637 - MinusLogProbMetric: 78.5637 - val_loss: 78.2138 - val_MinusLogProbMetric: 78.2138 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 32/1000
2023-10-26 22:35:41.120 
Epoch 32/1000 
	 loss: 77.6748, MinusLogProbMetric: 77.6748, val_loss: 77.4159, val_MinusLogProbMetric: 77.4159

Epoch 32: val_loss did not improve from 36.05653
196/196 - 67s - loss: 77.6748 - MinusLogProbMetric: 77.6748 - val_loss: 77.4159 - val_MinusLogProbMetric: 77.4159 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 33/1000
2023-10-26 22:36:48.167 
Epoch 33/1000 
	 loss: 76.9181, MinusLogProbMetric: 76.9181, val_loss: 76.8149, val_MinusLogProbMetric: 76.8149

Epoch 33: val_loss did not improve from 36.05653
196/196 - 67s - loss: 76.9181 - MinusLogProbMetric: 76.9181 - val_loss: 76.8149 - val_MinusLogProbMetric: 76.8149 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 34/1000
2023-10-26 22:37:55.104 
Epoch 34/1000 
	 loss: 79.7160, MinusLogProbMetric: 79.7160, val_loss: 76.6530, val_MinusLogProbMetric: 76.6530

Epoch 34: val_loss did not improve from 36.05653
196/196 - 67s - loss: 79.7160 - MinusLogProbMetric: 79.7160 - val_loss: 76.6530 - val_MinusLogProbMetric: 76.6530 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 35/1000
2023-10-26 22:39:01.690 
Epoch 35/1000 
	 loss: 75.9950, MinusLogProbMetric: 75.9950, val_loss: 75.8328, val_MinusLogProbMetric: 75.8328

Epoch 35: val_loss did not improve from 36.05653
196/196 - 67s - loss: 75.9950 - MinusLogProbMetric: 75.9950 - val_loss: 75.8328 - val_MinusLogProbMetric: 75.8328 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 36/1000
2023-10-26 22:40:09.018 
Epoch 36/1000 
	 loss: 75.4046, MinusLogProbMetric: 75.4046, val_loss: 75.2698, val_MinusLogProbMetric: 75.2698

Epoch 36: val_loss did not improve from 36.05653
196/196 - 67s - loss: 75.4046 - MinusLogProbMetric: 75.4046 - val_loss: 75.2698 - val_MinusLogProbMetric: 75.2698 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 37/1000
2023-10-26 22:41:16.185 
Epoch 37/1000 
	 loss: 74.9555, MinusLogProbMetric: 74.9555, val_loss: 74.9583, val_MinusLogProbMetric: 74.9583

Epoch 37: val_loss did not improve from 36.05653
196/196 - 67s - loss: 74.9555 - MinusLogProbMetric: 74.9555 - val_loss: 74.9583 - val_MinusLogProbMetric: 74.9583 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 38/1000
2023-10-26 22:42:23.664 
Epoch 38/1000 
	 loss: 74.6447, MinusLogProbMetric: 74.6447, val_loss: 74.5917, val_MinusLogProbMetric: 74.5917

Epoch 38: val_loss did not improve from 36.05653
196/196 - 67s - loss: 74.6447 - MinusLogProbMetric: 74.6447 - val_loss: 74.5917 - val_MinusLogProbMetric: 74.5917 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 39/1000
2023-10-26 22:43:31.224 
Epoch 39/1000 
	 loss: 74.3287, MinusLogProbMetric: 74.3287, val_loss: 74.3506, val_MinusLogProbMetric: 74.3506

Epoch 39: val_loss did not improve from 36.05653
196/196 - 68s - loss: 74.3287 - MinusLogProbMetric: 74.3287 - val_loss: 74.3506 - val_MinusLogProbMetric: 74.3506 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 40/1000
2023-10-26 22:44:38.728 
Epoch 40/1000 
	 loss: 74.0759, MinusLogProbMetric: 74.0759, val_loss: 74.0903, val_MinusLogProbMetric: 74.0903

Epoch 40: val_loss did not improve from 36.05653
196/196 - 68s - loss: 74.0759 - MinusLogProbMetric: 74.0759 - val_loss: 74.0903 - val_MinusLogProbMetric: 74.0903 - lr: 1.2346e-05 - 68s/epoch - 344ms/step
Epoch 41/1000
2023-10-26 22:45:45.543 
Epoch 41/1000 
	 loss: 73.8248, MinusLogProbMetric: 73.8248, val_loss: 73.8366, val_MinusLogProbMetric: 73.8366

Epoch 41: val_loss did not improve from 36.05653
196/196 - 67s - loss: 73.8248 - MinusLogProbMetric: 73.8248 - val_loss: 73.8366 - val_MinusLogProbMetric: 73.8366 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 42/1000
2023-10-26 22:46:52.578 
Epoch 42/1000 
	 loss: 73.6411, MinusLogProbMetric: 73.6411, val_loss: 73.6639, val_MinusLogProbMetric: 73.6639

Epoch 42: val_loss did not improve from 36.05653
196/196 - 67s - loss: 73.6411 - MinusLogProbMetric: 73.6411 - val_loss: 73.6639 - val_MinusLogProbMetric: 73.6639 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 43/1000
2023-10-26 22:47:59.701 
Epoch 43/1000 
	 loss: 73.3676, MinusLogProbMetric: 73.3676, val_loss: 73.5465, val_MinusLogProbMetric: 73.5465

Epoch 43: val_loss did not improve from 36.05653
196/196 - 67s - loss: 73.3676 - MinusLogProbMetric: 73.3676 - val_loss: 73.5465 - val_MinusLogProbMetric: 73.5465 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 44/1000
2023-10-26 22:49:06.972 
Epoch 44/1000 
	 loss: 73.1705, MinusLogProbMetric: 73.1705, val_loss: 73.2926, val_MinusLogProbMetric: 73.2926

Epoch 44: val_loss did not improve from 36.05653
196/196 - 67s - loss: 73.1705 - MinusLogProbMetric: 73.1705 - val_loss: 73.2926 - val_MinusLogProbMetric: 73.2926 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 45/1000
2023-10-26 22:50:14.667 
Epoch 45/1000 
	 loss: 72.9858, MinusLogProbMetric: 72.9858, val_loss: 73.0080, val_MinusLogProbMetric: 73.0080

Epoch 45: val_loss did not improve from 36.05653
196/196 - 68s - loss: 72.9858 - MinusLogProbMetric: 72.9858 - val_loss: 73.0080 - val_MinusLogProbMetric: 73.0080 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 46/1000
2023-10-26 22:51:22.414 
Epoch 46/1000 
	 loss: 72.7993, MinusLogProbMetric: 72.7993, val_loss: 72.8578, val_MinusLogProbMetric: 72.8578

Epoch 46: val_loss did not improve from 36.05653
196/196 - 68s - loss: 72.7993 - MinusLogProbMetric: 72.7993 - val_loss: 72.8578 - val_MinusLogProbMetric: 72.8578 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 47/1000
2023-10-26 22:52:30.694 
Epoch 47/1000 
	 loss: 72.6176, MinusLogProbMetric: 72.6176, val_loss: 72.7009, val_MinusLogProbMetric: 72.7009

Epoch 47: val_loss did not improve from 36.05653
196/196 - 68s - loss: 72.6176 - MinusLogProbMetric: 72.6176 - val_loss: 72.7009 - val_MinusLogProbMetric: 72.7009 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 48/1000
2023-10-26 22:53:38.117 
Epoch 48/1000 
	 loss: 72.4705, MinusLogProbMetric: 72.4705, val_loss: 72.6131, val_MinusLogProbMetric: 72.6131

Epoch 48: val_loss did not improve from 36.05653
196/196 - 67s - loss: 72.4705 - MinusLogProbMetric: 72.4705 - val_loss: 72.6131 - val_MinusLogProbMetric: 72.6131 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 49/1000
2023-10-26 22:54:44.776 
Epoch 49/1000 
	 loss: 72.3208, MinusLogProbMetric: 72.3208, val_loss: 72.4583, val_MinusLogProbMetric: 72.4583

Epoch 49: val_loss did not improve from 36.05653
196/196 - 67s - loss: 72.3208 - MinusLogProbMetric: 72.3208 - val_loss: 72.4583 - val_MinusLogProbMetric: 72.4583 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 50/1000
2023-10-26 22:55:52.193 
Epoch 50/1000 
	 loss: 72.1840, MinusLogProbMetric: 72.1840, val_loss: 72.3330, val_MinusLogProbMetric: 72.3330

Epoch 50: val_loss did not improve from 36.05653
196/196 - 67s - loss: 72.1840 - MinusLogProbMetric: 72.1840 - val_loss: 72.3330 - val_MinusLogProbMetric: 72.3330 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 51/1000
2023-10-26 22:56:58.907 
Epoch 51/1000 
	 loss: 72.0420, MinusLogProbMetric: 72.0420, val_loss: 72.0828, val_MinusLogProbMetric: 72.0828

Epoch 51: val_loss did not improve from 36.05653
196/196 - 67s - loss: 72.0420 - MinusLogProbMetric: 72.0420 - val_loss: 72.0828 - val_MinusLogProbMetric: 72.0828 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 52/1000
2023-10-26 22:58:06.742 
Epoch 52/1000 
	 loss: 71.9767, MinusLogProbMetric: 71.9767, val_loss: 72.0291, val_MinusLogProbMetric: 72.0291

Epoch 52: val_loss did not improve from 36.05653
196/196 - 68s - loss: 71.9767 - MinusLogProbMetric: 71.9767 - val_loss: 72.0291 - val_MinusLogProbMetric: 72.0291 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 53/1000
2023-10-26 22:59:13.961 
Epoch 53/1000 
	 loss: 71.7880, MinusLogProbMetric: 71.7880, val_loss: 71.7729, val_MinusLogProbMetric: 71.7729

Epoch 53: val_loss did not improve from 36.05653
196/196 - 67s - loss: 71.7880 - MinusLogProbMetric: 71.7880 - val_loss: 71.7729 - val_MinusLogProbMetric: 71.7729 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 54/1000
2023-10-26 23:00:21.086 
Epoch 54/1000 
	 loss: 71.6476, MinusLogProbMetric: 71.6476, val_loss: 71.7641, val_MinusLogProbMetric: 71.7641

Epoch 54: val_loss did not improve from 36.05653
196/196 - 67s - loss: 71.6476 - MinusLogProbMetric: 71.6476 - val_loss: 71.7641 - val_MinusLogProbMetric: 71.7641 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 55/1000
2023-10-26 23:01:28.723 
Epoch 55/1000 
	 loss: 71.5348, MinusLogProbMetric: 71.5348, val_loss: 71.7868, val_MinusLogProbMetric: 71.7868

Epoch 55: val_loss did not improve from 36.05653
196/196 - 68s - loss: 71.5348 - MinusLogProbMetric: 71.5348 - val_loss: 71.7868 - val_MinusLogProbMetric: 71.7868 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 56/1000
2023-10-26 23:02:35.945 
Epoch 56/1000 
	 loss: 71.4201, MinusLogProbMetric: 71.4201, val_loss: 71.6686, val_MinusLogProbMetric: 71.6686

Epoch 56: val_loss did not improve from 36.05653
196/196 - 67s - loss: 71.4201 - MinusLogProbMetric: 71.4201 - val_loss: 71.6686 - val_MinusLogProbMetric: 71.6686 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 57/1000
2023-10-26 23:03:42.284 
Epoch 57/1000 
	 loss: 71.3374, MinusLogProbMetric: 71.3374, val_loss: 71.5724, val_MinusLogProbMetric: 71.5724

Epoch 57: val_loss did not improve from 36.05653
196/196 - 66s - loss: 71.3374 - MinusLogProbMetric: 71.3374 - val_loss: 71.5724 - val_MinusLogProbMetric: 71.5724 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 58/1000
2023-10-26 23:04:48.916 
Epoch 58/1000 
	 loss: 71.2154, MinusLogProbMetric: 71.2154, val_loss: 71.2302, val_MinusLogProbMetric: 71.2302

Epoch 58: val_loss did not improve from 36.05653
196/196 - 67s - loss: 71.2154 - MinusLogProbMetric: 71.2154 - val_loss: 71.2302 - val_MinusLogProbMetric: 71.2302 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 59/1000
2023-10-26 23:05:56.377 
Epoch 59/1000 
	 loss: 71.1225, MinusLogProbMetric: 71.1225, val_loss: 71.2537, val_MinusLogProbMetric: 71.2537

Epoch 59: val_loss did not improve from 36.05653
196/196 - 67s - loss: 71.1225 - MinusLogProbMetric: 71.1225 - val_loss: 71.2537 - val_MinusLogProbMetric: 71.2537 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 60/1000
2023-10-26 23:07:03.818 
Epoch 60/1000 
	 loss: 89.8181, MinusLogProbMetric: 89.8181, val_loss: 147.4635, val_MinusLogProbMetric: 147.4635

Epoch 60: val_loss did not improve from 36.05653
196/196 - 67s - loss: 89.8181 - MinusLogProbMetric: 89.8181 - val_loss: 147.4635 - val_MinusLogProbMetric: 147.4635 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 61/1000
2023-10-26 23:08:11.455 
Epoch 61/1000 
	 loss: 88.9036, MinusLogProbMetric: 88.9036, val_loss: 77.2349, val_MinusLogProbMetric: 77.2349

Epoch 61: val_loss did not improve from 36.05653
196/196 - 68s - loss: 88.9036 - MinusLogProbMetric: 88.9036 - val_loss: 77.2349 - val_MinusLogProbMetric: 77.2349 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 62/1000
2023-10-26 23:09:18.049 
Epoch 62/1000 
	 loss: 75.4233, MinusLogProbMetric: 75.4233, val_loss: 74.3896, val_MinusLogProbMetric: 74.3896

Epoch 62: val_loss did not improve from 36.05653
196/196 - 67s - loss: 75.4233 - MinusLogProbMetric: 75.4233 - val_loss: 74.3896 - val_MinusLogProbMetric: 74.3896 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 63/1000
2023-10-26 23:10:25.293 
Epoch 63/1000 
	 loss: 73.1832, MinusLogProbMetric: 73.1832, val_loss: 72.8994, val_MinusLogProbMetric: 72.8994

Epoch 63: val_loss did not improve from 36.05653
196/196 - 67s - loss: 73.1832 - MinusLogProbMetric: 73.1832 - val_loss: 72.8994 - val_MinusLogProbMetric: 72.8994 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 64/1000
2023-10-26 23:11:32.083 
Epoch 64/1000 
	 loss: 72.2526, MinusLogProbMetric: 72.2526, val_loss: 72.3674, val_MinusLogProbMetric: 72.3674

Epoch 64: val_loss did not improve from 36.05653
196/196 - 67s - loss: 72.2526 - MinusLogProbMetric: 72.2526 - val_loss: 72.3674 - val_MinusLogProbMetric: 72.3674 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 65/1000
2023-10-26 23:12:39.269 
Epoch 65/1000 
	 loss: 71.7946, MinusLogProbMetric: 71.7946, val_loss: 71.7925, val_MinusLogProbMetric: 71.7925

Epoch 65: val_loss did not improve from 36.05653
196/196 - 67s - loss: 71.7946 - MinusLogProbMetric: 71.7946 - val_loss: 71.7925 - val_MinusLogProbMetric: 71.7925 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 66/1000
2023-10-26 23:13:46.824 
Epoch 66/1000 
	 loss: 71.5040, MinusLogProbMetric: 71.5040, val_loss: 71.5966, val_MinusLogProbMetric: 71.5966

Epoch 66: val_loss did not improve from 36.05653
196/196 - 68s - loss: 71.5040 - MinusLogProbMetric: 71.5040 - val_loss: 71.5966 - val_MinusLogProbMetric: 71.5966 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 67/1000
2023-10-26 23:14:54.251 
Epoch 67/1000 
	 loss: 71.2873, MinusLogProbMetric: 71.2873, val_loss: 71.3515, val_MinusLogProbMetric: 71.3515

Epoch 67: val_loss did not improve from 36.05653
196/196 - 67s - loss: 71.2873 - MinusLogProbMetric: 71.2873 - val_loss: 71.3515 - val_MinusLogProbMetric: 71.3515 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 68/1000
2023-10-26 23:16:01.871 
Epoch 68/1000 
	 loss: 71.1253, MinusLogProbMetric: 71.1253, val_loss: 71.1911, val_MinusLogProbMetric: 71.1911

Epoch 68: val_loss did not improve from 36.05653
196/196 - 68s - loss: 71.1253 - MinusLogProbMetric: 71.1253 - val_loss: 71.1911 - val_MinusLogProbMetric: 71.1911 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 69/1000
2023-10-26 23:17:08.997 
Epoch 69/1000 
	 loss: 70.9321, MinusLogProbMetric: 70.9321, val_loss: 71.0445, val_MinusLogProbMetric: 71.0445

Epoch 69: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.9321 - MinusLogProbMetric: 70.9321 - val_loss: 71.0445 - val_MinusLogProbMetric: 71.0445 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 70/1000
2023-10-26 23:18:16.362 
Epoch 70/1000 
	 loss: 70.8619, MinusLogProbMetric: 70.8619, val_loss: 70.9019, val_MinusLogProbMetric: 70.9019

Epoch 70: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.8619 - MinusLogProbMetric: 70.8619 - val_loss: 70.9019 - val_MinusLogProbMetric: 70.9019 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 71/1000
2023-10-26 23:19:23.535 
Epoch 71/1000 
	 loss: 70.6307, MinusLogProbMetric: 70.6307, val_loss: 70.8664, val_MinusLogProbMetric: 70.8664

Epoch 71: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.6307 - MinusLogProbMetric: 70.6307 - val_loss: 70.8664 - val_MinusLogProbMetric: 70.8664 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 72/1000
2023-10-26 23:20:31.049 
Epoch 72/1000 
	 loss: 70.5551, MinusLogProbMetric: 70.5551, val_loss: 70.6904, val_MinusLogProbMetric: 70.6904

Epoch 72: val_loss did not improve from 36.05653
196/196 - 68s - loss: 70.5551 - MinusLogProbMetric: 70.5551 - val_loss: 70.6904 - val_MinusLogProbMetric: 70.6904 - lr: 6.1728e-06 - 68s/epoch - 344ms/step
Epoch 73/1000
2023-10-26 23:21:38.006 
Epoch 73/1000 
	 loss: 70.4957, MinusLogProbMetric: 70.4957, val_loss: 70.7382, val_MinusLogProbMetric: 70.7382

Epoch 73: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.4957 - MinusLogProbMetric: 70.4957 - val_loss: 70.7382 - val_MinusLogProbMetric: 70.7382 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 74/1000
2023-10-26 23:22:44.527 
Epoch 74/1000 
	 loss: 70.4468, MinusLogProbMetric: 70.4468, val_loss: 70.6345, val_MinusLogProbMetric: 70.6345

Epoch 74: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.4468 - MinusLogProbMetric: 70.4468 - val_loss: 70.6345 - val_MinusLogProbMetric: 70.6345 - lr: 6.1728e-06 - 67s/epoch - 339ms/step
Epoch 75/1000
2023-10-26 23:23:51.283 
Epoch 75/1000 
	 loss: 70.3807, MinusLogProbMetric: 70.3807, val_loss: 70.5350, val_MinusLogProbMetric: 70.5350

Epoch 75: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.3807 - MinusLogProbMetric: 70.3807 - val_loss: 70.5350 - val_MinusLogProbMetric: 70.5350 - lr: 6.1728e-06 - 67s/epoch - 341ms/step
Epoch 76/1000
2023-10-26 23:24:58.362 
Epoch 76/1000 
	 loss: 70.3592, MinusLogProbMetric: 70.3592, val_loss: 70.5104, val_MinusLogProbMetric: 70.5104

Epoch 76: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.3592 - MinusLogProbMetric: 70.3592 - val_loss: 70.5104 - val_MinusLogProbMetric: 70.5104 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 77/1000
2023-10-26 23:26:05.961 
Epoch 77/1000 
	 loss: 70.3492, MinusLogProbMetric: 70.3492, val_loss: 70.4990, val_MinusLogProbMetric: 70.4990

Epoch 77: val_loss did not improve from 36.05653
196/196 - 68s - loss: 70.3492 - MinusLogProbMetric: 70.3492 - val_loss: 70.4990 - val_MinusLogProbMetric: 70.4990 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 78/1000
2023-10-26 23:27:13.051 
Epoch 78/1000 
	 loss: 70.2702, MinusLogProbMetric: 70.2702, val_loss: 70.4179, val_MinusLogProbMetric: 70.4179

Epoch 78: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.2702 - MinusLogProbMetric: 70.2702 - val_loss: 70.4179 - val_MinusLogProbMetric: 70.4179 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 79/1000
2023-10-26 23:28:20.220 
Epoch 79/1000 
	 loss: 70.2021, MinusLogProbMetric: 70.2021, val_loss: 70.3879, val_MinusLogProbMetric: 70.3879

Epoch 79: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.2021 - MinusLogProbMetric: 70.2021 - val_loss: 70.3879 - val_MinusLogProbMetric: 70.3879 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 80/1000
2023-10-26 23:29:26.592 
Epoch 80/1000 
	 loss: 70.1665, MinusLogProbMetric: 70.1665, val_loss: 70.4246, val_MinusLogProbMetric: 70.4246

Epoch 80: val_loss did not improve from 36.05653
196/196 - 66s - loss: 70.1665 - MinusLogProbMetric: 70.1665 - val_loss: 70.4246 - val_MinusLogProbMetric: 70.4246 - lr: 6.1728e-06 - 66s/epoch - 339ms/step
Epoch 81/1000
2023-10-26 23:30:33.271 
Epoch 81/1000 
	 loss: 70.1222, MinusLogProbMetric: 70.1222, val_loss: 70.3026, val_MinusLogProbMetric: 70.3026

Epoch 81: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.1222 - MinusLogProbMetric: 70.1222 - val_loss: 70.3026 - val_MinusLogProbMetric: 70.3026 - lr: 6.1728e-06 - 67s/epoch - 340ms/step
Epoch 82/1000
2023-10-26 23:31:40.465 
Epoch 82/1000 
	 loss: 70.0777, MinusLogProbMetric: 70.0777, val_loss: 70.2461, val_MinusLogProbMetric: 70.2461

Epoch 82: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.0777 - MinusLogProbMetric: 70.0777 - val_loss: 70.2461 - val_MinusLogProbMetric: 70.2461 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 83/1000
2023-10-26 23:32:47.581 
Epoch 83/1000 
	 loss: 70.0383, MinusLogProbMetric: 70.0383, val_loss: 70.3422, val_MinusLogProbMetric: 70.3422

Epoch 83: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.0383 - MinusLogProbMetric: 70.0383 - val_loss: 70.3422 - val_MinusLogProbMetric: 70.3422 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 84/1000
2023-10-26 23:33:54.845 
Epoch 84/1000 
	 loss: 70.0078, MinusLogProbMetric: 70.0078, val_loss: 70.1294, val_MinusLogProbMetric: 70.1294

Epoch 84: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.0078 - MinusLogProbMetric: 70.0078 - val_loss: 70.1294 - val_MinusLogProbMetric: 70.1294 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 85/1000
2023-10-26 23:35:02.223 
Epoch 85/1000 
	 loss: 69.9631, MinusLogProbMetric: 69.9631, val_loss: 70.1196, val_MinusLogProbMetric: 70.1196

Epoch 85: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.9631 - MinusLogProbMetric: 69.9631 - val_loss: 70.1196 - val_MinusLogProbMetric: 70.1196 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 86/1000
2023-10-26 23:36:09.760 
Epoch 86/1000 
	 loss: 69.9194, MinusLogProbMetric: 69.9194, val_loss: 70.0866, val_MinusLogProbMetric: 70.0866

Epoch 86: val_loss did not improve from 36.05653
196/196 - 68s - loss: 69.9194 - MinusLogProbMetric: 69.9194 - val_loss: 70.0866 - val_MinusLogProbMetric: 70.0866 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 87/1000
2023-10-26 23:37:17.139 
Epoch 87/1000 
	 loss: 69.8875, MinusLogProbMetric: 69.8875, val_loss: 70.1077, val_MinusLogProbMetric: 70.1077

Epoch 87: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.8875 - MinusLogProbMetric: 69.8875 - val_loss: 70.1077 - val_MinusLogProbMetric: 70.1077 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 88/1000
2023-10-26 23:38:24.415 
Epoch 88/1000 
	 loss: 69.8721, MinusLogProbMetric: 69.8721, val_loss: 70.1003, val_MinusLogProbMetric: 70.1003

Epoch 88: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.8721 - MinusLogProbMetric: 69.8721 - val_loss: 70.1003 - val_MinusLogProbMetric: 70.1003 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 89/1000
2023-10-26 23:39:31.956 
Epoch 89/1000 
	 loss: 69.8159, MinusLogProbMetric: 69.8159, val_loss: 69.9824, val_MinusLogProbMetric: 69.9824

Epoch 89: val_loss did not improve from 36.05653
196/196 - 68s - loss: 69.8159 - MinusLogProbMetric: 69.8159 - val_loss: 69.9824 - val_MinusLogProbMetric: 69.9824 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 90/1000
2023-10-26 23:40:39.570 
Epoch 90/1000 
	 loss: 69.7964, MinusLogProbMetric: 69.7964, val_loss: 69.9506, val_MinusLogProbMetric: 69.9506

Epoch 90: val_loss did not improve from 36.05653
196/196 - 68s - loss: 69.7964 - MinusLogProbMetric: 69.7964 - val_loss: 69.9506 - val_MinusLogProbMetric: 69.9506 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 91/1000
2023-10-26 23:41:46.814 
Epoch 91/1000 
	 loss: 69.7520, MinusLogProbMetric: 69.7520, val_loss: 69.9621, val_MinusLogProbMetric: 69.9621

Epoch 91: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.7520 - MinusLogProbMetric: 69.7520 - val_loss: 69.9621 - val_MinusLogProbMetric: 69.9621 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 92/1000
2023-10-26 23:42:53.741 
Epoch 92/1000 
	 loss: 69.7297, MinusLogProbMetric: 69.7297, val_loss: 69.9183, val_MinusLogProbMetric: 69.9183

Epoch 92: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.7297 - MinusLogProbMetric: 69.7297 - val_loss: 69.9183 - val_MinusLogProbMetric: 69.9183 - lr: 6.1728e-06 - 67s/epoch - 341ms/step
Epoch 93/1000
2023-10-26 23:44:00.214 
Epoch 93/1000 
	 loss: 69.7351, MinusLogProbMetric: 69.7351, val_loss: 69.8723, val_MinusLogProbMetric: 69.8723

Epoch 93: val_loss did not improve from 36.05653
196/196 - 66s - loss: 69.7351 - MinusLogProbMetric: 69.7351 - val_loss: 69.8723 - val_MinusLogProbMetric: 69.8723 - lr: 6.1728e-06 - 66s/epoch - 339ms/step
Epoch 94/1000
2023-10-26 23:45:07.125 
Epoch 94/1000 
	 loss: 69.6652, MinusLogProbMetric: 69.6652, val_loss: 69.8247, val_MinusLogProbMetric: 69.8247

Epoch 94: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.6652 - MinusLogProbMetric: 69.6652 - val_loss: 69.8247 - val_MinusLogProbMetric: 69.8247 - lr: 6.1728e-06 - 67s/epoch - 341ms/step
Epoch 95/1000
2023-10-26 23:46:14.083 
Epoch 95/1000 
	 loss: 69.6294, MinusLogProbMetric: 69.6294, val_loss: 69.8123, val_MinusLogProbMetric: 69.8123

Epoch 95: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.6294 - MinusLogProbMetric: 69.6294 - val_loss: 69.8123 - val_MinusLogProbMetric: 69.8123 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 96/1000
2023-10-26 23:47:21.131 
Epoch 96/1000 
	 loss: 69.5974, MinusLogProbMetric: 69.5974, val_loss: 69.8568, val_MinusLogProbMetric: 69.8568

Epoch 96: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.5974 - MinusLogProbMetric: 69.5974 - val_loss: 69.8568 - val_MinusLogProbMetric: 69.8568 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 97/1000
2023-10-26 23:48:28.428 
Epoch 97/1000 
	 loss: 69.5682, MinusLogProbMetric: 69.5682, val_loss: 69.7460, val_MinusLogProbMetric: 69.7460

Epoch 97: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.5682 - MinusLogProbMetric: 69.5682 - val_loss: 69.7460 - val_MinusLogProbMetric: 69.7460 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 98/1000
2023-10-26 23:49:35.307 
Epoch 98/1000 
	 loss: 69.5257, MinusLogProbMetric: 69.5257, val_loss: 69.7285, val_MinusLogProbMetric: 69.7285

Epoch 98: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.5257 - MinusLogProbMetric: 69.5257 - val_loss: 69.7285 - val_MinusLogProbMetric: 69.7285 - lr: 6.1728e-06 - 67s/epoch - 341ms/step
Epoch 99/1000
2023-10-26 23:50:42.518 
Epoch 99/1000 
	 loss: 69.5012, MinusLogProbMetric: 69.5012, val_loss: 69.6480, val_MinusLogProbMetric: 69.6480

Epoch 99: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.5012 - MinusLogProbMetric: 69.5012 - val_loss: 69.6480 - val_MinusLogProbMetric: 69.6480 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 100/1000
2023-10-26 23:51:49.198 
Epoch 100/1000 
	 loss: 69.4818, MinusLogProbMetric: 69.4818, val_loss: 69.6717, val_MinusLogProbMetric: 69.6717

Epoch 100: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.4818 - MinusLogProbMetric: 69.4818 - val_loss: 69.6717 - val_MinusLogProbMetric: 69.6717 - lr: 6.1728e-06 - 67s/epoch - 340ms/step
Epoch 101/1000
2023-10-26 23:52:56.725 
Epoch 101/1000 
	 loss: 69.4314, MinusLogProbMetric: 69.4314, val_loss: 69.6330, val_MinusLogProbMetric: 69.6330

Epoch 101: val_loss did not improve from 36.05653
196/196 - 68s - loss: 69.4314 - MinusLogProbMetric: 69.4314 - val_loss: 69.6330 - val_MinusLogProbMetric: 69.6330 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 102/1000
2023-10-26 23:54:03.915 
Epoch 102/1000 
	 loss: 69.4181, MinusLogProbMetric: 69.4181, val_loss: 69.5967, val_MinusLogProbMetric: 69.5967

Epoch 102: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.4181 - MinusLogProbMetric: 69.4181 - val_loss: 69.5967 - val_MinusLogProbMetric: 69.5967 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 103/1000
2023-10-26 23:55:11.247 
Epoch 103/1000 
	 loss: 69.4008, MinusLogProbMetric: 69.4008, val_loss: 69.7727, val_MinusLogProbMetric: 69.7727

Epoch 103: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.4008 - MinusLogProbMetric: 69.4008 - val_loss: 69.7727 - val_MinusLogProbMetric: 69.7727 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 104/1000
2023-10-26 23:56:18.487 
Epoch 104/1000 
	 loss: 69.3540, MinusLogProbMetric: 69.3540, val_loss: 69.5644, val_MinusLogProbMetric: 69.5644

Epoch 104: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.3540 - MinusLogProbMetric: 69.3540 - val_loss: 69.5644 - val_MinusLogProbMetric: 69.5644 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 105/1000
2023-10-26 23:57:26.022 
Epoch 105/1000 
	 loss: 69.3380, MinusLogProbMetric: 69.3380, val_loss: 69.5536, val_MinusLogProbMetric: 69.5536

Epoch 105: val_loss did not improve from 36.05653
196/196 - 68s - loss: 69.3380 - MinusLogProbMetric: 69.3380 - val_loss: 69.5536 - val_MinusLogProbMetric: 69.5536 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 106/1000
2023-10-26 23:58:33.380 
Epoch 106/1000 
	 loss: 69.3247, MinusLogProbMetric: 69.3247, val_loss: 69.5253, val_MinusLogProbMetric: 69.5253

Epoch 106: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.3247 - MinusLogProbMetric: 69.3247 - val_loss: 69.5253 - val_MinusLogProbMetric: 69.5253 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 107/1000
2023-10-26 23:59:40.506 
Epoch 107/1000 
	 loss: 69.2720, MinusLogProbMetric: 69.2720, val_loss: 69.5303, val_MinusLogProbMetric: 69.5303

Epoch 107: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.2720 - MinusLogProbMetric: 69.2720 - val_loss: 69.5303 - val_MinusLogProbMetric: 69.5303 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 108/1000
2023-10-27 00:00:47.311 
Epoch 108/1000 
	 loss: 69.2497, MinusLogProbMetric: 69.2497, val_loss: 69.5097, val_MinusLogProbMetric: 69.5097

Epoch 108: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.2497 - MinusLogProbMetric: 69.2497 - val_loss: 69.5097 - val_MinusLogProbMetric: 69.5097 - lr: 6.1728e-06 - 67s/epoch - 341ms/step
Epoch 109/1000
2023-10-27 00:01:55.000 
Epoch 109/1000 
	 loss: 69.2240, MinusLogProbMetric: 69.2240, val_loss: 69.4520, val_MinusLogProbMetric: 69.4520

Epoch 109: val_loss did not improve from 36.05653
196/196 - 68s - loss: 69.2240 - MinusLogProbMetric: 69.2240 - val_loss: 69.4520 - val_MinusLogProbMetric: 69.4520 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 110/1000
2023-10-27 00:03:01.994 
Epoch 110/1000 
	 loss: 69.1973, MinusLogProbMetric: 69.1973, val_loss: 69.4071, val_MinusLogProbMetric: 69.4071

Epoch 110: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.1973 - MinusLogProbMetric: 69.1973 - val_loss: 69.4071 - val_MinusLogProbMetric: 69.4071 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 111/1000
2023-10-27 00:04:08.486 
Epoch 111/1000 
	 loss: 69.1660, MinusLogProbMetric: 69.1660, val_loss: 69.3450, val_MinusLogProbMetric: 69.3450

Epoch 111: val_loss did not improve from 36.05653
196/196 - 66s - loss: 69.1660 - MinusLogProbMetric: 69.1660 - val_loss: 69.3450 - val_MinusLogProbMetric: 69.3450 - lr: 6.1728e-06 - 66s/epoch - 339ms/step
Epoch 112/1000
2023-10-27 00:05:15.254 
Epoch 112/1000 
	 loss: 69.1403, MinusLogProbMetric: 69.1403, val_loss: 69.3307, val_MinusLogProbMetric: 69.3307

Epoch 112: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.1403 - MinusLogProbMetric: 69.1403 - val_loss: 69.3307 - val_MinusLogProbMetric: 69.3307 - lr: 6.1728e-06 - 67s/epoch - 341ms/step
Epoch 113/1000
2023-10-27 00:06:21.971 
Epoch 113/1000 
	 loss: 69.1252, MinusLogProbMetric: 69.1252, val_loss: 69.3141, val_MinusLogProbMetric: 69.3141

Epoch 113: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.1252 - MinusLogProbMetric: 69.1252 - val_loss: 69.3141 - val_MinusLogProbMetric: 69.3141 - lr: 6.1728e-06 - 67s/epoch - 340ms/step
Epoch 114/1000
2023-10-27 00:07:29.556 
Epoch 114/1000 
	 loss: 69.0899, MinusLogProbMetric: 69.0899, val_loss: 69.2531, val_MinusLogProbMetric: 69.2531

Epoch 114: val_loss did not improve from 36.05653
196/196 - 68s - loss: 69.0899 - MinusLogProbMetric: 69.0899 - val_loss: 69.2531 - val_MinusLogProbMetric: 69.2531 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 115/1000
2023-10-27 00:08:36.588 
Epoch 115/1000 
	 loss: 70.6171, MinusLogProbMetric: 70.6171, val_loss: 69.7855, val_MinusLogProbMetric: 69.7855

Epoch 115: val_loss did not improve from 36.05653
196/196 - 67s - loss: 70.6171 - MinusLogProbMetric: 70.6171 - val_loss: 69.7855 - val_MinusLogProbMetric: 69.7855 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 116/1000
2023-10-27 00:09:44.018 
Epoch 116/1000 
	 loss: 69.2936, MinusLogProbMetric: 69.2936, val_loss: 69.4728, val_MinusLogProbMetric: 69.4728

Epoch 116: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.2936 - MinusLogProbMetric: 69.2936 - val_loss: 69.4728 - val_MinusLogProbMetric: 69.4728 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 117/1000
2023-10-27 00:10:51.080 
Epoch 117/1000 
	 loss: 69.1153, MinusLogProbMetric: 69.1153, val_loss: 69.2427, val_MinusLogProbMetric: 69.2427

Epoch 117: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.1153 - MinusLogProbMetric: 69.1153 - val_loss: 69.2427 - val_MinusLogProbMetric: 69.2427 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 118/1000
2023-10-27 00:11:58.292 
Epoch 118/1000 
	 loss: 69.0691, MinusLogProbMetric: 69.0691, val_loss: 69.3070, val_MinusLogProbMetric: 69.3070

Epoch 118: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.0691 - MinusLogProbMetric: 69.0691 - val_loss: 69.3070 - val_MinusLogProbMetric: 69.3070 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 119/1000
2023-10-27 00:13:05.518 
Epoch 119/1000 
	 loss: 69.0615, MinusLogProbMetric: 69.0615, val_loss: 69.5515, val_MinusLogProbMetric: 69.5515

Epoch 119: val_loss did not improve from 36.05653
196/196 - 67s - loss: 69.0615 - MinusLogProbMetric: 69.0615 - val_loss: 69.5515 - val_MinusLogProbMetric: 69.5515 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 120/1000
2023-10-27 00:14:13.577 
Epoch 120/1000 
	 loss: 68.9960, MinusLogProbMetric: 68.9960, val_loss: 69.2878, val_MinusLogProbMetric: 69.2878

Epoch 120: val_loss did not improve from 36.05653
Restoring model weights from the end of the best epoch: 20.
196/196 - 69s - loss: 68.9960 - MinusLogProbMetric: 68.9960 - val_loss: 69.2878 - val_MinusLogProbMetric: 69.2878 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 120: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 926.
Model trained in 8203.42 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.27 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 2.11 s.
===========
Run 390/720 done in 29727.11 s.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

===========
Generating train data for run 395.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_395/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_395/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_395/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_395
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  908640    
 yer)                                                            
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f37fc25f610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e01664130>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e01664130>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3bbe4b7970>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e01daec50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e01daf1c0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e01daf280>, <keras.callbacks.EarlyStopping object at 0x7f3e01daf4f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e01daf520>, <keras.callbacks.TerminateOnNaN object at 0x7f3e01daf160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_395/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 395/720 with hyperparameters:
timestamp = 2023-10-27 00:14:22.682499
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
2023-10-27 00:16:24.710 
Epoch 1/1000 
	 loss: 1176.4713, MinusLogProbMetric: 1176.4713, val_loss: 321.2710, val_MinusLogProbMetric: 321.2710

Epoch 1: val_loss improved from inf to 321.27103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 123s - loss: 1176.4713 - MinusLogProbMetric: 1176.4713 - val_loss: 321.2710 - val_MinusLogProbMetric: 321.2710 - lr: 0.0010 - 123s/epoch - 625ms/step
Epoch 2/1000
2023-10-27 00:17:07.755 
Epoch 2/1000 
	 loss: 248.7597, MinusLogProbMetric: 248.7597, val_loss: 195.8826, val_MinusLogProbMetric: 195.8826

Epoch 2: val_loss improved from 321.27103 to 195.88261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 248.7597 - MinusLogProbMetric: 248.7597 - val_loss: 195.8826 - val_MinusLogProbMetric: 195.8826 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 3/1000
2023-10-27 00:17:51.330 
Epoch 3/1000 
	 loss: 158.5473, MinusLogProbMetric: 158.5473, val_loss: 135.3668, val_MinusLogProbMetric: 135.3668

Epoch 3: val_loss improved from 195.88261 to 135.36682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 158.5473 - MinusLogProbMetric: 158.5473 - val_loss: 135.3668 - val_MinusLogProbMetric: 135.3668 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 4/1000
2023-10-27 00:18:35.000 
Epoch 4/1000 
	 loss: 122.1944, MinusLogProbMetric: 122.1944, val_loss: 109.0620, val_MinusLogProbMetric: 109.0620

Epoch 4: val_loss improved from 135.36682 to 109.06196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 122.1944 - MinusLogProbMetric: 122.1944 - val_loss: 109.0620 - val_MinusLogProbMetric: 109.0620 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 5/1000
2023-10-27 00:19:18.799 
Epoch 5/1000 
	 loss: 103.8320, MinusLogProbMetric: 103.8320, val_loss: 95.3435, val_MinusLogProbMetric: 95.3435

Epoch 5: val_loss improved from 109.06196 to 95.34350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 103.8320 - MinusLogProbMetric: 103.8320 - val_loss: 95.3435 - val_MinusLogProbMetric: 95.3435 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 6/1000
2023-10-27 00:20:01.912 
Epoch 6/1000 
	 loss: 87.8976, MinusLogProbMetric: 87.8976, val_loss: 81.8901, val_MinusLogProbMetric: 81.8901

Epoch 6: val_loss improved from 95.34350 to 81.89014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 87.8976 - MinusLogProbMetric: 87.8976 - val_loss: 81.8901 - val_MinusLogProbMetric: 81.8901 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 7/1000
2023-10-27 00:20:44.680 
Epoch 7/1000 
	 loss: 76.8919, MinusLogProbMetric: 76.8919, val_loss: 73.6473, val_MinusLogProbMetric: 73.6473

Epoch 7: val_loss improved from 81.89014 to 73.64727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 76.8919 - MinusLogProbMetric: 76.8919 - val_loss: 73.6473 - val_MinusLogProbMetric: 73.6473 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 8/1000
2023-10-27 00:21:27.738 
Epoch 8/1000 
	 loss: 69.1408, MinusLogProbMetric: 69.1408, val_loss: 66.0577, val_MinusLogProbMetric: 66.0577

Epoch 8: val_loss improved from 73.64727 to 66.05775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 69.1408 - MinusLogProbMetric: 69.1408 - val_loss: 66.0577 - val_MinusLogProbMetric: 66.0577 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 9/1000
2023-10-27 00:22:11.316 
Epoch 9/1000 
	 loss: 63.7048, MinusLogProbMetric: 63.7048, val_loss: 61.5959, val_MinusLogProbMetric: 61.5959

Epoch 9: val_loss improved from 66.05775 to 61.59586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 63.7048 - MinusLogProbMetric: 63.7048 - val_loss: 61.5959 - val_MinusLogProbMetric: 61.5959 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 10/1000
2023-10-27 00:22:53.989 
Epoch 10/1000 
	 loss: 59.4220, MinusLogProbMetric: 59.4220, val_loss: 57.3408, val_MinusLogProbMetric: 57.3408

Epoch 10: val_loss improved from 61.59586 to 57.34084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 59.4220 - MinusLogProbMetric: 59.4220 - val_loss: 57.3408 - val_MinusLogProbMetric: 57.3408 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 11/1000
2023-10-27 00:23:37.216 
Epoch 11/1000 
	 loss: 56.0656, MinusLogProbMetric: 56.0656, val_loss: 55.2553, val_MinusLogProbMetric: 55.2553

Epoch 11: val_loss improved from 57.34084 to 55.25533, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 56.0656 - MinusLogProbMetric: 56.0656 - val_loss: 55.2553 - val_MinusLogProbMetric: 55.2553 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 12/1000
2023-10-27 00:24:20.435 
Epoch 12/1000 
	 loss: 53.1708, MinusLogProbMetric: 53.1708, val_loss: 51.6737, val_MinusLogProbMetric: 51.6737

Epoch 12: val_loss improved from 55.25533 to 51.67369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 53.1708 - MinusLogProbMetric: 53.1708 - val_loss: 51.6737 - val_MinusLogProbMetric: 51.6737 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 13/1000
2023-10-27 00:25:03.421 
Epoch 13/1000 
	 loss: 52.0267, MinusLogProbMetric: 52.0267, val_loss: 51.2431, val_MinusLogProbMetric: 51.2431

Epoch 13: val_loss improved from 51.67369 to 51.24311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 52.0267 - MinusLogProbMetric: 52.0267 - val_loss: 51.2431 - val_MinusLogProbMetric: 51.2431 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 14/1000
2023-10-27 00:25:46.813 
Epoch 14/1000 
	 loss: 49.2140, MinusLogProbMetric: 49.2140, val_loss: 48.0270, val_MinusLogProbMetric: 48.0270

Epoch 14: val_loss improved from 51.24311 to 48.02700, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 49.2140 - MinusLogProbMetric: 49.2140 - val_loss: 48.0270 - val_MinusLogProbMetric: 48.0270 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 15/1000
2023-10-27 00:26:30.801 
Epoch 15/1000 
	 loss: 47.3999, MinusLogProbMetric: 47.3999, val_loss: 47.3210, val_MinusLogProbMetric: 47.3210

Epoch 15: val_loss improved from 48.02700 to 47.32101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 47.3999 - MinusLogProbMetric: 47.3999 - val_loss: 47.3210 - val_MinusLogProbMetric: 47.3210 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 16/1000
2023-10-27 00:27:14.014 
Epoch 16/1000 
	 loss: 46.2199, MinusLogProbMetric: 46.2199, val_loss: 46.3815, val_MinusLogProbMetric: 46.3815

Epoch 16: val_loss improved from 47.32101 to 46.38151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 46.2199 - MinusLogProbMetric: 46.2199 - val_loss: 46.3815 - val_MinusLogProbMetric: 46.3815 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 17/1000
2023-10-27 00:27:57.187 
Epoch 17/1000 
	 loss: 45.3314, MinusLogProbMetric: 45.3314, val_loss: 44.2728, val_MinusLogProbMetric: 44.2728

Epoch 17: val_loss improved from 46.38151 to 44.27276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 45.3314 - MinusLogProbMetric: 45.3314 - val_loss: 44.2728 - val_MinusLogProbMetric: 44.2728 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 18/1000
2023-10-27 00:28:40.360 
Epoch 18/1000 
	 loss: 44.2904, MinusLogProbMetric: 44.2904, val_loss: 44.8904, val_MinusLogProbMetric: 44.8904

Epoch 18: val_loss did not improve from 44.27276
196/196 - 43s - loss: 44.2904 - MinusLogProbMetric: 44.2904 - val_loss: 44.8904 - val_MinusLogProbMetric: 44.8904 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 19/1000
2023-10-27 00:29:23.064 
Epoch 19/1000 
	 loss: 43.6846, MinusLogProbMetric: 43.6846, val_loss: 43.4647, val_MinusLogProbMetric: 43.4647

Epoch 19: val_loss improved from 44.27276 to 43.46473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 43.6846 - MinusLogProbMetric: 43.6846 - val_loss: 43.4647 - val_MinusLogProbMetric: 43.4647 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 20/1000
2023-10-27 00:30:06.398 
Epoch 20/1000 
	 loss: 42.6129, MinusLogProbMetric: 42.6129, val_loss: 42.3701, val_MinusLogProbMetric: 42.3701

Epoch 20: val_loss improved from 43.46473 to 42.37012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 42.6129 - MinusLogProbMetric: 42.6129 - val_loss: 42.3701 - val_MinusLogProbMetric: 42.3701 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 21/1000
2023-10-27 00:30:49.673 
Epoch 21/1000 
	 loss: 41.9595, MinusLogProbMetric: 41.9595, val_loss: 42.4306, val_MinusLogProbMetric: 42.4306

Epoch 21: val_loss did not improve from 42.37012
196/196 - 42s - loss: 41.9595 - MinusLogProbMetric: 41.9595 - val_loss: 42.4306 - val_MinusLogProbMetric: 42.4306 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 22/1000
2023-10-27 00:31:32.447 
Epoch 22/1000 
	 loss: 41.3785, MinusLogProbMetric: 41.3785, val_loss: 42.9289, val_MinusLogProbMetric: 42.9289

Epoch 22: val_loss did not improve from 42.37012
196/196 - 43s - loss: 41.3785 - MinusLogProbMetric: 41.3785 - val_loss: 42.9289 - val_MinusLogProbMetric: 42.9289 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 23/1000
2023-10-27 00:32:15.077 
Epoch 23/1000 
	 loss: 40.8768, MinusLogProbMetric: 40.8768, val_loss: 40.4334, val_MinusLogProbMetric: 40.4334

Epoch 23: val_loss improved from 42.37012 to 40.43340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 40.8768 - MinusLogProbMetric: 40.8768 - val_loss: 40.4334 - val_MinusLogProbMetric: 40.4334 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 24/1000
2023-10-27 00:32:58.554 
Epoch 24/1000 
	 loss: 40.2054, MinusLogProbMetric: 40.2054, val_loss: 40.1950, val_MinusLogProbMetric: 40.1950

Epoch 24: val_loss improved from 40.43340 to 40.19498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 40.2054 - MinusLogProbMetric: 40.2054 - val_loss: 40.1950 - val_MinusLogProbMetric: 40.1950 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 25/1000
2023-10-27 00:33:42.839 
Epoch 25/1000 
	 loss: 40.0336, MinusLogProbMetric: 40.0336, val_loss: 40.3475, val_MinusLogProbMetric: 40.3475

Epoch 25: val_loss did not improve from 40.19498
196/196 - 43s - loss: 40.0336 - MinusLogProbMetric: 40.0336 - val_loss: 40.3475 - val_MinusLogProbMetric: 40.3475 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 26/1000
2023-10-27 00:34:25.958 
Epoch 26/1000 
	 loss: 39.7077, MinusLogProbMetric: 39.7077, val_loss: 39.2647, val_MinusLogProbMetric: 39.2647

Epoch 26: val_loss improved from 40.19498 to 39.26474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 39.7077 - MinusLogProbMetric: 39.7077 - val_loss: 39.2647 - val_MinusLogProbMetric: 39.2647 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 27/1000
2023-10-27 00:35:10.071 
Epoch 27/1000 
	 loss: 39.2112, MinusLogProbMetric: 39.2112, val_loss: 39.1388, val_MinusLogProbMetric: 39.1388

Epoch 27: val_loss improved from 39.26474 to 39.13881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 39.2112 - MinusLogProbMetric: 39.2112 - val_loss: 39.1388 - val_MinusLogProbMetric: 39.1388 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 28/1000
2023-10-27 00:35:53.253 
Epoch 28/1000 
	 loss: 38.9617, MinusLogProbMetric: 38.9617, val_loss: 39.9049, val_MinusLogProbMetric: 39.9049

Epoch 28: val_loss did not improve from 39.13881
196/196 - 42s - loss: 38.9617 - MinusLogProbMetric: 38.9617 - val_loss: 39.9049 - val_MinusLogProbMetric: 39.9049 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 29/1000
2023-10-27 00:36:35.284 
Epoch 29/1000 
	 loss: 38.5766, MinusLogProbMetric: 38.5766, val_loss: 38.8993, val_MinusLogProbMetric: 38.8993

Epoch 29: val_loss improved from 39.13881 to 38.89929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 38.5766 - MinusLogProbMetric: 38.5766 - val_loss: 38.8993 - val_MinusLogProbMetric: 38.8993 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 30/1000
2023-10-27 00:37:18.573 
Epoch 30/1000 
	 loss: 38.1807, MinusLogProbMetric: 38.1807, val_loss: 39.3089, val_MinusLogProbMetric: 39.3089

Epoch 30: val_loss did not improve from 38.89929
196/196 - 42s - loss: 38.1807 - MinusLogProbMetric: 38.1807 - val_loss: 39.3089 - val_MinusLogProbMetric: 39.3089 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 31/1000
2023-10-27 00:38:01.314 
Epoch 31/1000 
	 loss: 38.1124, MinusLogProbMetric: 38.1124, val_loss: 40.8029, val_MinusLogProbMetric: 40.8029

Epoch 31: val_loss did not improve from 38.89929
196/196 - 43s - loss: 38.1124 - MinusLogProbMetric: 38.1124 - val_loss: 40.8029 - val_MinusLogProbMetric: 40.8029 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 32/1000
2023-10-27 00:38:44.164 
Epoch 32/1000 
	 loss: 37.9043, MinusLogProbMetric: 37.9043, val_loss: 39.6456, val_MinusLogProbMetric: 39.6456

Epoch 32: val_loss did not improve from 38.89929
196/196 - 43s - loss: 37.9043 - MinusLogProbMetric: 37.9043 - val_loss: 39.6456 - val_MinusLogProbMetric: 39.6456 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 33/1000
2023-10-27 00:39:26.838 
Epoch 33/1000 
	 loss: 37.6340, MinusLogProbMetric: 37.6340, val_loss: 36.8568, val_MinusLogProbMetric: 36.8568

Epoch 33: val_loss improved from 38.89929 to 36.85675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 37.6340 - MinusLogProbMetric: 37.6340 - val_loss: 36.8568 - val_MinusLogProbMetric: 36.8568 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 34/1000
2023-10-27 00:40:10.343 
Epoch 34/1000 
	 loss: 39.8781, MinusLogProbMetric: 39.8781, val_loss: 38.6363, val_MinusLogProbMetric: 38.6363

Epoch 34: val_loss did not improve from 36.85675
196/196 - 43s - loss: 39.8781 - MinusLogProbMetric: 39.8781 - val_loss: 38.6363 - val_MinusLogProbMetric: 38.6363 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 35/1000
2023-10-27 00:40:53.264 
Epoch 35/1000 
	 loss: 37.9676, MinusLogProbMetric: 37.9676, val_loss: 37.7916, val_MinusLogProbMetric: 37.7916

Epoch 35: val_loss did not improve from 36.85675
196/196 - 43s - loss: 37.9676 - MinusLogProbMetric: 37.9676 - val_loss: 37.7916 - val_MinusLogProbMetric: 37.7916 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 36/1000
2023-10-27 00:41:35.776 
Epoch 36/1000 
	 loss: 37.4319, MinusLogProbMetric: 37.4319, val_loss: 36.8033, val_MinusLogProbMetric: 36.8033

Epoch 36: val_loss improved from 36.85675 to 36.80335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 37.4319 - MinusLogProbMetric: 37.4319 - val_loss: 36.8033 - val_MinusLogProbMetric: 36.8033 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 37/1000
2023-10-27 00:42:19.347 
Epoch 37/1000 
	 loss: 37.2214, MinusLogProbMetric: 37.2214, val_loss: 36.4563, val_MinusLogProbMetric: 36.4563

Epoch 37: val_loss improved from 36.80335 to 36.45626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 37.2214 - MinusLogProbMetric: 37.2214 - val_loss: 36.4563 - val_MinusLogProbMetric: 36.4563 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 38/1000
2023-10-27 00:43:02.946 
Epoch 38/1000 
	 loss: 37.0305, MinusLogProbMetric: 37.0305, val_loss: 37.4419, val_MinusLogProbMetric: 37.4419

Epoch 38: val_loss did not improve from 36.45626
196/196 - 43s - loss: 37.0305 - MinusLogProbMetric: 37.0305 - val_loss: 37.4419 - val_MinusLogProbMetric: 37.4419 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 39/1000
2023-10-27 00:43:45.401 
Epoch 39/1000 
	 loss: 36.7306, MinusLogProbMetric: 36.7306, val_loss: 36.6254, val_MinusLogProbMetric: 36.6254

Epoch 39: val_loss did not improve from 36.45626
196/196 - 42s - loss: 36.7306 - MinusLogProbMetric: 36.7306 - val_loss: 36.6254 - val_MinusLogProbMetric: 36.6254 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 40/1000
2023-10-27 00:44:28.411 
Epoch 40/1000 
	 loss: 36.6059, MinusLogProbMetric: 36.6059, val_loss: 36.2247, val_MinusLogProbMetric: 36.2247

Epoch 40: val_loss improved from 36.45626 to 36.22470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 36.6059 - MinusLogProbMetric: 36.6059 - val_loss: 36.2247 - val_MinusLogProbMetric: 36.2247 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 41/1000
2023-10-27 00:45:12.986 
Epoch 41/1000 
	 loss: 36.3696, MinusLogProbMetric: 36.3696, val_loss: 37.3646, val_MinusLogProbMetric: 37.3646

Epoch 41: val_loss did not improve from 36.22470
196/196 - 43s - loss: 36.3696 - MinusLogProbMetric: 36.3696 - val_loss: 37.3646 - val_MinusLogProbMetric: 37.3646 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 42/1000
2023-10-27 00:45:55.659 
Epoch 42/1000 
	 loss: 36.2228, MinusLogProbMetric: 36.2228, val_loss: 36.1951, val_MinusLogProbMetric: 36.1951

Epoch 42: val_loss improved from 36.22470 to 36.19505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 36.2228 - MinusLogProbMetric: 36.2228 - val_loss: 36.1951 - val_MinusLogProbMetric: 36.1951 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 43/1000
2023-10-27 00:46:39.174 
Epoch 43/1000 
	 loss: 36.0358, MinusLogProbMetric: 36.0358, val_loss: 36.5875, val_MinusLogProbMetric: 36.5875

Epoch 43: val_loss did not improve from 36.19505
196/196 - 43s - loss: 36.0358 - MinusLogProbMetric: 36.0358 - val_loss: 36.5875 - val_MinusLogProbMetric: 36.5875 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 44/1000
2023-10-27 00:47:22.420 
Epoch 44/1000 
	 loss: 35.9567, MinusLogProbMetric: 35.9567, val_loss: 35.8496, val_MinusLogProbMetric: 35.8496

Epoch 44: val_loss improved from 36.19505 to 35.84963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 35.9567 - MinusLogProbMetric: 35.9567 - val_loss: 35.8496 - val_MinusLogProbMetric: 35.8496 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 45/1000
2023-10-27 00:48:06.277 
Epoch 45/1000 
	 loss: 35.5402, MinusLogProbMetric: 35.5402, val_loss: 35.5465, val_MinusLogProbMetric: 35.5465

Epoch 45: val_loss improved from 35.84963 to 35.54649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 35.5402 - MinusLogProbMetric: 35.5402 - val_loss: 35.5465 - val_MinusLogProbMetric: 35.5465 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 46/1000
2023-10-27 00:48:49.803 
Epoch 46/1000 
	 loss: 35.4604, MinusLogProbMetric: 35.4604, val_loss: 36.2357, val_MinusLogProbMetric: 36.2357

Epoch 46: val_loss did not improve from 35.54649
196/196 - 43s - loss: 35.4604 - MinusLogProbMetric: 35.4604 - val_loss: 36.2357 - val_MinusLogProbMetric: 36.2357 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 47/1000
2023-10-27 00:49:32.774 
Epoch 47/1000 
	 loss: 35.5837, MinusLogProbMetric: 35.5837, val_loss: 35.6715, val_MinusLogProbMetric: 35.6715

Epoch 47: val_loss did not improve from 35.54649
196/196 - 43s - loss: 35.5837 - MinusLogProbMetric: 35.5837 - val_loss: 35.6715 - val_MinusLogProbMetric: 35.6715 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 48/1000
2023-10-27 00:50:15.323 
Epoch 48/1000 
	 loss: 35.3422, MinusLogProbMetric: 35.3422, val_loss: 35.4275, val_MinusLogProbMetric: 35.4275

Epoch 48: val_loss improved from 35.54649 to 35.42747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 35.3422 - MinusLogProbMetric: 35.3422 - val_loss: 35.4275 - val_MinusLogProbMetric: 35.4275 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 49/1000
2023-10-27 00:50:59.182 
Epoch 49/1000 
	 loss: 35.3047, MinusLogProbMetric: 35.3047, val_loss: 35.1195, val_MinusLogProbMetric: 35.1195

Epoch 49: val_loss improved from 35.42747 to 35.11949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 35.3047 - MinusLogProbMetric: 35.3047 - val_loss: 35.1195 - val_MinusLogProbMetric: 35.1195 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 50/1000
2023-10-27 00:51:42.966 
Epoch 50/1000 
	 loss: 35.2353, MinusLogProbMetric: 35.2353, val_loss: 34.9327, val_MinusLogProbMetric: 34.9327

Epoch 50: val_loss improved from 35.11949 to 34.93275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 35.2353 - MinusLogProbMetric: 35.2353 - val_loss: 34.9327 - val_MinusLogProbMetric: 34.9327 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 51/1000
2023-10-27 00:52:26.459 
Epoch 51/1000 
	 loss: 35.0821, MinusLogProbMetric: 35.0821, val_loss: 34.6903, val_MinusLogProbMetric: 34.6903

Epoch 51: val_loss improved from 34.93275 to 34.69032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 35.0821 - MinusLogProbMetric: 35.0821 - val_loss: 34.6903 - val_MinusLogProbMetric: 34.6903 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 52/1000
2023-10-27 00:53:10.083 
Epoch 52/1000 
	 loss: 34.8782, MinusLogProbMetric: 34.8782, val_loss: 34.8044, val_MinusLogProbMetric: 34.8044

Epoch 52: val_loss did not improve from 34.69032
196/196 - 43s - loss: 34.8782 - MinusLogProbMetric: 34.8782 - val_loss: 34.8044 - val_MinusLogProbMetric: 34.8044 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 53/1000
2023-10-27 00:53:52.998 
Epoch 53/1000 
	 loss: 34.8226, MinusLogProbMetric: 34.8226, val_loss: 34.9015, val_MinusLogProbMetric: 34.9015

Epoch 53: val_loss did not improve from 34.69032
196/196 - 43s - loss: 34.8226 - MinusLogProbMetric: 34.8226 - val_loss: 34.9015 - val_MinusLogProbMetric: 34.9015 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 54/1000
2023-10-27 00:54:35.920 
Epoch 54/1000 
	 loss: 34.7240, MinusLogProbMetric: 34.7240, val_loss: 35.1834, val_MinusLogProbMetric: 35.1834

Epoch 54: val_loss did not improve from 34.69032
196/196 - 43s - loss: 34.7240 - MinusLogProbMetric: 34.7240 - val_loss: 35.1834 - val_MinusLogProbMetric: 35.1834 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 55/1000
2023-10-27 00:55:18.887 
Epoch 55/1000 
	 loss: 34.5654, MinusLogProbMetric: 34.5654, val_loss: 35.4833, val_MinusLogProbMetric: 35.4833

Epoch 55: val_loss did not improve from 34.69032
196/196 - 43s - loss: 34.5654 - MinusLogProbMetric: 34.5654 - val_loss: 35.4833 - val_MinusLogProbMetric: 35.4833 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 56/1000
2023-10-27 00:56:01.865 
Epoch 56/1000 
	 loss: 34.6115, MinusLogProbMetric: 34.6115, val_loss: 34.1073, val_MinusLogProbMetric: 34.1073

Epoch 56: val_loss improved from 34.69032 to 34.10733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 34.6115 - MinusLogProbMetric: 34.6115 - val_loss: 34.1073 - val_MinusLogProbMetric: 34.1073 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 57/1000
2023-10-27 00:56:45.490 
Epoch 57/1000 
	 loss: 34.3742, MinusLogProbMetric: 34.3742, val_loss: 35.7996, val_MinusLogProbMetric: 35.7996

Epoch 57: val_loss did not improve from 34.10733
196/196 - 43s - loss: 34.3742 - MinusLogProbMetric: 34.3742 - val_loss: 35.7996 - val_MinusLogProbMetric: 35.7996 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 58/1000
2023-10-27 00:57:28.389 
Epoch 58/1000 
	 loss: 34.3364, MinusLogProbMetric: 34.3364, val_loss: 34.3401, val_MinusLogProbMetric: 34.3401

Epoch 58: val_loss did not improve from 34.10733
196/196 - 43s - loss: 34.3364 - MinusLogProbMetric: 34.3364 - val_loss: 34.3401 - val_MinusLogProbMetric: 34.3401 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 59/1000
2023-10-27 00:58:10.873 
Epoch 59/1000 
	 loss: 34.2539, MinusLogProbMetric: 34.2539, val_loss: 33.4035, val_MinusLogProbMetric: 33.4035

Epoch 59: val_loss improved from 34.10733 to 33.40349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 34.2539 - MinusLogProbMetric: 34.2539 - val_loss: 33.4035 - val_MinusLogProbMetric: 33.4035 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 60/1000
2023-10-27 00:58:54.806 
Epoch 60/1000 
	 loss: 33.9863, MinusLogProbMetric: 33.9863, val_loss: 36.3812, val_MinusLogProbMetric: 36.3812

Epoch 60: val_loss did not improve from 33.40349
196/196 - 43s - loss: 33.9863 - MinusLogProbMetric: 33.9863 - val_loss: 36.3812 - val_MinusLogProbMetric: 36.3812 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 61/1000
2023-10-27 00:59:37.297 
Epoch 61/1000 
	 loss: 34.2586, MinusLogProbMetric: 34.2586, val_loss: 35.1091, val_MinusLogProbMetric: 35.1091

Epoch 61: val_loss did not improve from 33.40349
196/196 - 42s - loss: 34.2586 - MinusLogProbMetric: 34.2586 - val_loss: 35.1091 - val_MinusLogProbMetric: 35.1091 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 62/1000
2023-10-27 01:00:19.858 
Epoch 62/1000 
	 loss: 34.3495, MinusLogProbMetric: 34.3495, val_loss: 34.1905, val_MinusLogProbMetric: 34.1905

Epoch 62: val_loss did not improve from 33.40349
196/196 - 43s - loss: 34.3495 - MinusLogProbMetric: 34.3495 - val_loss: 34.1905 - val_MinusLogProbMetric: 34.1905 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 63/1000
2023-10-27 01:01:02.226 
Epoch 63/1000 
	 loss: 33.8959, MinusLogProbMetric: 33.8959, val_loss: 36.2281, val_MinusLogProbMetric: 36.2281

Epoch 63: val_loss did not improve from 33.40349
196/196 - 42s - loss: 33.8959 - MinusLogProbMetric: 33.8959 - val_loss: 36.2281 - val_MinusLogProbMetric: 36.2281 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 64/1000
2023-10-27 01:01:45.104 
Epoch 64/1000 
	 loss: 34.0015, MinusLogProbMetric: 34.0015, val_loss: 34.1630, val_MinusLogProbMetric: 34.1630

Epoch 64: val_loss did not improve from 33.40349
196/196 - 43s - loss: 34.0015 - MinusLogProbMetric: 34.0015 - val_loss: 34.1630 - val_MinusLogProbMetric: 34.1630 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 65/1000
2023-10-27 01:02:28.202 
Epoch 65/1000 
	 loss: 34.0062, MinusLogProbMetric: 34.0062, val_loss: 34.1308, val_MinusLogProbMetric: 34.1308

Epoch 65: val_loss did not improve from 33.40349
196/196 - 43s - loss: 34.0062 - MinusLogProbMetric: 34.0062 - val_loss: 34.1308 - val_MinusLogProbMetric: 34.1308 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 66/1000
2023-10-27 01:03:11.121 
Epoch 66/1000 
	 loss: 33.9053, MinusLogProbMetric: 33.9053, val_loss: 34.2947, val_MinusLogProbMetric: 34.2947

Epoch 66: val_loss did not improve from 33.40349
196/196 - 43s - loss: 33.9053 - MinusLogProbMetric: 33.9053 - val_loss: 34.2947 - val_MinusLogProbMetric: 34.2947 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 67/1000
2023-10-27 01:03:54.057 
Epoch 67/1000 
	 loss: 34.0329, MinusLogProbMetric: 34.0329, val_loss: 33.6856, val_MinusLogProbMetric: 33.6856

Epoch 67: val_loss did not improve from 33.40349
196/196 - 43s - loss: 34.0329 - MinusLogProbMetric: 34.0329 - val_loss: 33.6856 - val_MinusLogProbMetric: 33.6856 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 68/1000
2023-10-27 01:04:36.947 
Epoch 68/1000 
	 loss: 33.7865, MinusLogProbMetric: 33.7865, val_loss: 34.2535, val_MinusLogProbMetric: 34.2535

Epoch 68: val_loss did not improve from 33.40349
196/196 - 43s - loss: 33.7865 - MinusLogProbMetric: 33.7865 - val_loss: 34.2535 - val_MinusLogProbMetric: 34.2535 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 69/1000
2023-10-27 01:05:19.849 
Epoch 69/1000 
	 loss: 33.5853, MinusLogProbMetric: 33.5853, val_loss: 33.8253, val_MinusLogProbMetric: 33.8253

Epoch 69: val_loss did not improve from 33.40349
196/196 - 43s - loss: 33.5853 - MinusLogProbMetric: 33.5853 - val_loss: 33.8253 - val_MinusLogProbMetric: 33.8253 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 70/1000
2023-10-27 01:06:02.923 
Epoch 70/1000 
	 loss: 33.8773, MinusLogProbMetric: 33.8773, val_loss: 33.4657, val_MinusLogProbMetric: 33.4657

Epoch 70: val_loss did not improve from 33.40349
196/196 - 43s - loss: 33.8773 - MinusLogProbMetric: 33.8773 - val_loss: 33.4657 - val_MinusLogProbMetric: 33.4657 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 71/1000
2023-10-27 01:06:45.613 
Epoch 71/1000 
	 loss: 33.6421, MinusLogProbMetric: 33.6421, val_loss: 33.1080, val_MinusLogProbMetric: 33.1080

Epoch 71: val_loss improved from 33.40349 to 33.10798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 33.6421 - MinusLogProbMetric: 33.6421 - val_loss: 33.1080 - val_MinusLogProbMetric: 33.1080 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 72/1000
2023-10-27 01:07:29.363 
Epoch 72/1000 
	 loss: 33.5464, MinusLogProbMetric: 33.5464, val_loss: 34.0204, val_MinusLogProbMetric: 34.0204

Epoch 72: val_loss did not improve from 33.10798
196/196 - 43s - loss: 33.5464 - MinusLogProbMetric: 33.5464 - val_loss: 34.0204 - val_MinusLogProbMetric: 34.0204 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 73/1000
2023-10-27 01:08:12.290 
Epoch 73/1000 
	 loss: 33.4308, MinusLogProbMetric: 33.4308, val_loss: 33.3710, val_MinusLogProbMetric: 33.3710

Epoch 73: val_loss did not improve from 33.10798
196/196 - 43s - loss: 33.4308 - MinusLogProbMetric: 33.4308 - val_loss: 33.3710 - val_MinusLogProbMetric: 33.3710 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 74/1000
2023-10-27 01:08:55.276 
Epoch 74/1000 
	 loss: 33.3560, MinusLogProbMetric: 33.3560, val_loss: 33.8727, val_MinusLogProbMetric: 33.8727

Epoch 74: val_loss did not improve from 33.10798
196/196 - 43s - loss: 33.3560 - MinusLogProbMetric: 33.3560 - val_loss: 33.8727 - val_MinusLogProbMetric: 33.8727 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 75/1000
2023-10-27 01:09:37.635 
Epoch 75/1000 
	 loss: 33.1942, MinusLogProbMetric: 33.1942, val_loss: 32.5523, val_MinusLogProbMetric: 32.5523

Epoch 75: val_loss improved from 33.10798 to 32.55232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 33.1942 - MinusLogProbMetric: 33.1942 - val_loss: 32.5523 - val_MinusLogProbMetric: 32.5523 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 76/1000
2023-10-27 01:10:21.098 
Epoch 76/1000 
	 loss: 33.1468, MinusLogProbMetric: 33.1468, val_loss: 33.3794, val_MinusLogProbMetric: 33.3794

Epoch 76: val_loss did not improve from 32.55232
196/196 - 43s - loss: 33.1468 - MinusLogProbMetric: 33.1468 - val_loss: 33.3794 - val_MinusLogProbMetric: 33.3794 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 77/1000
2023-10-27 01:11:03.834 
Epoch 77/1000 
	 loss: 33.4192, MinusLogProbMetric: 33.4192, val_loss: 32.9004, val_MinusLogProbMetric: 32.9004

Epoch 77: val_loss did not improve from 32.55232
196/196 - 43s - loss: 33.4192 - MinusLogProbMetric: 33.4192 - val_loss: 32.9004 - val_MinusLogProbMetric: 32.9004 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 78/1000
2023-10-27 01:11:46.502 
Epoch 78/1000 
	 loss: 33.3957, MinusLogProbMetric: 33.3957, val_loss: 33.3580, val_MinusLogProbMetric: 33.3580

Epoch 78: val_loss did not improve from 32.55232
196/196 - 43s - loss: 33.3957 - MinusLogProbMetric: 33.3957 - val_loss: 33.3580 - val_MinusLogProbMetric: 33.3580 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 79/1000
2023-10-27 01:12:28.878 
Epoch 79/1000 
	 loss: 33.1786, MinusLogProbMetric: 33.1786, val_loss: 33.2049, val_MinusLogProbMetric: 33.2049

Epoch 79: val_loss did not improve from 32.55232
196/196 - 42s - loss: 33.1786 - MinusLogProbMetric: 33.1786 - val_loss: 33.2049 - val_MinusLogProbMetric: 33.2049 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 80/1000
2023-10-27 01:13:11.590 
Epoch 80/1000 
	 loss: 32.8845, MinusLogProbMetric: 32.8845, val_loss: 32.5603, val_MinusLogProbMetric: 32.5603

Epoch 80: val_loss did not improve from 32.55232
196/196 - 43s - loss: 32.8845 - MinusLogProbMetric: 32.8845 - val_loss: 32.5603 - val_MinusLogProbMetric: 32.5603 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 81/1000
2023-10-27 01:13:54.067 
Epoch 81/1000 
	 loss: 33.1856, MinusLogProbMetric: 33.1856, val_loss: 33.1870, val_MinusLogProbMetric: 33.1870

Epoch 81: val_loss did not improve from 32.55232
196/196 - 42s - loss: 33.1856 - MinusLogProbMetric: 33.1856 - val_loss: 33.1870 - val_MinusLogProbMetric: 33.1870 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 82/1000
2023-10-27 01:14:36.376 
Epoch 82/1000 
	 loss: 32.9736, MinusLogProbMetric: 32.9736, val_loss: 33.2954, val_MinusLogProbMetric: 33.2954

Epoch 82: val_loss did not improve from 32.55232
196/196 - 42s - loss: 32.9736 - MinusLogProbMetric: 32.9736 - val_loss: 33.2954 - val_MinusLogProbMetric: 33.2954 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 83/1000
2023-10-27 01:15:19.022 
Epoch 83/1000 
	 loss: 33.0488, MinusLogProbMetric: 33.0488, val_loss: 34.3257, val_MinusLogProbMetric: 34.3257

Epoch 83: val_loss did not improve from 32.55232
196/196 - 43s - loss: 33.0488 - MinusLogProbMetric: 33.0488 - val_loss: 34.3257 - val_MinusLogProbMetric: 34.3257 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 84/1000
2023-10-27 01:16:01.206 
Epoch 84/1000 
	 loss: 33.1844, MinusLogProbMetric: 33.1844, val_loss: 33.3746, val_MinusLogProbMetric: 33.3746

Epoch 84: val_loss did not improve from 32.55232
196/196 - 42s - loss: 33.1844 - MinusLogProbMetric: 33.1844 - val_loss: 33.3746 - val_MinusLogProbMetric: 33.3746 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 85/1000
2023-10-27 01:16:43.991 
Epoch 85/1000 
	 loss: 32.7184, MinusLogProbMetric: 32.7184, val_loss: 33.5262, val_MinusLogProbMetric: 33.5262

Epoch 85: val_loss did not improve from 32.55232
196/196 - 43s - loss: 32.7184 - MinusLogProbMetric: 32.7184 - val_loss: 33.5262 - val_MinusLogProbMetric: 33.5262 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 86/1000
2023-10-27 01:17:27.093 
Epoch 86/1000 
	 loss: 33.0585, MinusLogProbMetric: 33.0585, val_loss: 33.4541, val_MinusLogProbMetric: 33.4541

Epoch 86: val_loss did not improve from 32.55232
196/196 - 43s - loss: 33.0585 - MinusLogProbMetric: 33.0585 - val_loss: 33.4541 - val_MinusLogProbMetric: 33.4541 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 87/1000
2023-10-27 01:18:09.952 
Epoch 87/1000 
	 loss: 32.8072, MinusLogProbMetric: 32.8072, val_loss: 33.1030, val_MinusLogProbMetric: 33.1030

Epoch 87: val_loss did not improve from 32.55232
196/196 - 43s - loss: 32.8072 - MinusLogProbMetric: 32.8072 - val_loss: 33.1030 - val_MinusLogProbMetric: 33.1030 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 88/1000
2023-10-27 01:18:52.731 
Epoch 88/1000 
	 loss: 32.6889, MinusLogProbMetric: 32.6889, val_loss: 33.1739, val_MinusLogProbMetric: 33.1739

Epoch 88: val_loss did not improve from 32.55232
196/196 - 43s - loss: 32.6889 - MinusLogProbMetric: 32.6889 - val_loss: 33.1739 - val_MinusLogProbMetric: 33.1739 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 89/1000
2023-10-27 01:19:34.962 
Epoch 89/1000 
	 loss: 32.6853, MinusLogProbMetric: 32.6853, val_loss: 32.4656, val_MinusLogProbMetric: 32.4656

Epoch 89: val_loss improved from 32.55232 to 32.46564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 32.6853 - MinusLogProbMetric: 32.6853 - val_loss: 32.4656 - val_MinusLogProbMetric: 32.4656 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 90/1000
2023-10-27 01:20:18.413 
Epoch 90/1000 
	 loss: 32.8315, MinusLogProbMetric: 32.8315, val_loss: 33.1759, val_MinusLogProbMetric: 33.1759

Epoch 90: val_loss did not improve from 32.46564
196/196 - 43s - loss: 32.8315 - MinusLogProbMetric: 32.8315 - val_loss: 33.1759 - val_MinusLogProbMetric: 33.1759 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 91/1000
2023-10-27 01:21:00.801 
Epoch 91/1000 
	 loss: 32.6191, MinusLogProbMetric: 32.6191, val_loss: 32.2402, val_MinusLogProbMetric: 32.2402

Epoch 91: val_loss improved from 32.46564 to 32.24023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 32.6191 - MinusLogProbMetric: 32.6191 - val_loss: 32.2402 - val_MinusLogProbMetric: 32.2402 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 92/1000
2023-10-27 01:21:44.076 
Epoch 92/1000 
	 loss: 32.9632, MinusLogProbMetric: 32.9632, val_loss: 32.4662, val_MinusLogProbMetric: 32.4662

Epoch 92: val_loss did not improve from 32.24023
196/196 - 42s - loss: 32.9632 - MinusLogProbMetric: 32.9632 - val_loss: 32.4662 - val_MinusLogProbMetric: 32.4662 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 93/1000
2023-10-27 01:22:26.295 
Epoch 93/1000 
	 loss: 32.6181, MinusLogProbMetric: 32.6181, val_loss: 32.5065, val_MinusLogProbMetric: 32.5065

Epoch 93: val_loss did not improve from 32.24023
196/196 - 42s - loss: 32.6181 - MinusLogProbMetric: 32.6181 - val_loss: 32.5065 - val_MinusLogProbMetric: 32.5065 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 94/1000
2023-10-27 01:23:09.135 
Epoch 94/1000 
	 loss: 32.4980, MinusLogProbMetric: 32.4980, val_loss: 32.6988, val_MinusLogProbMetric: 32.6988

Epoch 94: val_loss did not improve from 32.24023
196/196 - 43s - loss: 32.4980 - MinusLogProbMetric: 32.4980 - val_loss: 32.6988 - val_MinusLogProbMetric: 32.6988 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 95/1000
2023-10-27 01:23:51.595 
Epoch 95/1000 
	 loss: 32.6870, MinusLogProbMetric: 32.6870, val_loss: 32.5136, val_MinusLogProbMetric: 32.5136

Epoch 95: val_loss did not improve from 32.24023
196/196 - 42s - loss: 32.6870 - MinusLogProbMetric: 32.6870 - val_loss: 32.5136 - val_MinusLogProbMetric: 32.5136 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 96/1000
2023-10-27 01:24:34.358 
Epoch 96/1000 
	 loss: 32.5711, MinusLogProbMetric: 32.5711, val_loss: 32.4109, val_MinusLogProbMetric: 32.4109

Epoch 96: val_loss did not improve from 32.24023
196/196 - 43s - loss: 32.5711 - MinusLogProbMetric: 32.5711 - val_loss: 32.4109 - val_MinusLogProbMetric: 32.4109 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 97/1000
2023-10-27 01:25:17.165 
Epoch 97/1000 
	 loss: 32.4691, MinusLogProbMetric: 32.4691, val_loss: 32.5831, val_MinusLogProbMetric: 32.5831

Epoch 97: val_loss did not improve from 32.24023
196/196 - 43s - loss: 32.4691 - MinusLogProbMetric: 32.4691 - val_loss: 32.5831 - val_MinusLogProbMetric: 32.5831 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 98/1000
2023-10-27 01:25:59.836 
Epoch 98/1000 
	 loss: 32.4537, MinusLogProbMetric: 32.4537, val_loss: 33.8993, val_MinusLogProbMetric: 33.8993

Epoch 98: val_loss did not improve from 32.24023
196/196 - 43s - loss: 32.4537 - MinusLogProbMetric: 32.4537 - val_loss: 33.8993 - val_MinusLogProbMetric: 33.8993 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 99/1000
2023-10-27 01:26:42.488 
Epoch 99/1000 
	 loss: 32.2298, MinusLogProbMetric: 32.2298, val_loss: 31.9255, val_MinusLogProbMetric: 31.9255

Epoch 99: val_loss improved from 32.24023 to 31.92546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 32.2298 - MinusLogProbMetric: 32.2298 - val_loss: 31.9255 - val_MinusLogProbMetric: 31.9255 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 100/1000
2023-10-27 01:27:26.029 
Epoch 100/1000 
	 loss: 32.2480, MinusLogProbMetric: 32.2480, val_loss: 32.7527, val_MinusLogProbMetric: 32.7527

Epoch 100: val_loss did not improve from 31.92546
196/196 - 43s - loss: 32.2480 - MinusLogProbMetric: 32.2480 - val_loss: 32.7527 - val_MinusLogProbMetric: 32.7527 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 101/1000
2023-10-27 01:28:08.386 
Epoch 101/1000 
	 loss: 32.3848, MinusLogProbMetric: 32.3848, val_loss: 31.8388, val_MinusLogProbMetric: 31.8388

Epoch 101: val_loss improved from 31.92546 to 31.83878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 32.3848 - MinusLogProbMetric: 32.3848 - val_loss: 31.8388 - val_MinusLogProbMetric: 31.8388 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 102/1000
2023-10-27 01:28:51.708 
Epoch 102/1000 
	 loss: 32.3152, MinusLogProbMetric: 32.3152, val_loss: 33.8748, val_MinusLogProbMetric: 33.8748

Epoch 102: val_loss did not improve from 31.83878
196/196 - 43s - loss: 32.3152 - MinusLogProbMetric: 32.3152 - val_loss: 33.8748 - val_MinusLogProbMetric: 33.8748 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 103/1000
2023-10-27 01:29:34.596 
Epoch 103/1000 
	 loss: 32.2941, MinusLogProbMetric: 32.2941, val_loss: 32.4101, val_MinusLogProbMetric: 32.4101

Epoch 103: val_loss did not improve from 31.83878
196/196 - 43s - loss: 32.2941 - MinusLogProbMetric: 32.2941 - val_loss: 32.4101 - val_MinusLogProbMetric: 32.4101 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 104/1000
2023-10-27 01:30:17.220 
Epoch 104/1000 
	 loss: 32.1875, MinusLogProbMetric: 32.1875, val_loss: 31.9592, val_MinusLogProbMetric: 31.9592

Epoch 104: val_loss did not improve from 31.83878
196/196 - 43s - loss: 32.1875 - MinusLogProbMetric: 32.1875 - val_loss: 31.9592 - val_MinusLogProbMetric: 31.9592 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 105/1000
2023-10-27 01:31:00.032 
Epoch 105/1000 
	 loss: 32.2542, MinusLogProbMetric: 32.2542, val_loss: 32.1105, val_MinusLogProbMetric: 32.1105

Epoch 105: val_loss did not improve from 31.83878
196/196 - 43s - loss: 32.2542 - MinusLogProbMetric: 32.2542 - val_loss: 32.1105 - val_MinusLogProbMetric: 32.1105 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 106/1000
2023-10-27 01:31:42.740 
Epoch 106/1000 
	 loss: 32.5447, MinusLogProbMetric: 32.5447, val_loss: 33.0176, val_MinusLogProbMetric: 33.0176

Epoch 106: val_loss did not improve from 31.83878
196/196 - 43s - loss: 32.5447 - MinusLogProbMetric: 32.5447 - val_loss: 33.0176 - val_MinusLogProbMetric: 33.0176 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 107/1000
2023-10-27 01:32:25.727 
Epoch 107/1000 
	 loss: 32.0071, MinusLogProbMetric: 32.0071, val_loss: 32.5589, val_MinusLogProbMetric: 32.5589

Epoch 107: val_loss did not improve from 31.83878
196/196 - 43s - loss: 32.0071 - MinusLogProbMetric: 32.0071 - val_loss: 32.5589 - val_MinusLogProbMetric: 32.5589 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 108/1000
2023-10-27 01:33:08.458 
Epoch 108/1000 
	 loss: 32.0804, MinusLogProbMetric: 32.0804, val_loss: 31.6923, val_MinusLogProbMetric: 31.6923

Epoch 108: val_loss improved from 31.83878 to 31.69234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 32.0804 - MinusLogProbMetric: 32.0804 - val_loss: 31.6923 - val_MinusLogProbMetric: 31.6923 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 109/1000
2023-10-27 01:33:51.571 
Epoch 109/1000 
	 loss: 32.1129, MinusLogProbMetric: 32.1129, val_loss: 31.7435, val_MinusLogProbMetric: 31.7435

Epoch 109: val_loss did not improve from 31.69234
196/196 - 42s - loss: 32.1129 - MinusLogProbMetric: 32.1129 - val_loss: 31.7435 - val_MinusLogProbMetric: 31.7435 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 110/1000
2023-10-27 01:34:34.020 
Epoch 110/1000 
	 loss: 32.0153, MinusLogProbMetric: 32.0153, val_loss: 31.5181, val_MinusLogProbMetric: 31.5181

Epoch 110: val_loss improved from 31.69234 to 31.51810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 32.0153 - MinusLogProbMetric: 32.0153 - val_loss: 31.5181 - val_MinusLogProbMetric: 31.5181 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 111/1000
2023-10-27 01:35:17.868 
Epoch 111/1000 
	 loss: 32.1086, MinusLogProbMetric: 32.1086, val_loss: 31.4320, val_MinusLogProbMetric: 31.4320

Epoch 111: val_loss improved from 31.51810 to 31.43202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 32.1086 - MinusLogProbMetric: 32.1086 - val_loss: 31.4320 - val_MinusLogProbMetric: 31.4320 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 112/1000
2023-10-27 01:36:01.289 
Epoch 112/1000 
	 loss: 31.8954, MinusLogProbMetric: 31.8954, val_loss: 31.7231, val_MinusLogProbMetric: 31.7231

Epoch 112: val_loss did not improve from 31.43202
196/196 - 43s - loss: 31.8954 - MinusLogProbMetric: 31.8954 - val_loss: 31.7231 - val_MinusLogProbMetric: 31.7231 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 113/1000
2023-10-27 01:36:44.059 
Epoch 113/1000 
	 loss: 31.9322, MinusLogProbMetric: 31.9322, val_loss: 31.8810, val_MinusLogProbMetric: 31.8810

Epoch 113: val_loss did not improve from 31.43202
196/196 - 43s - loss: 31.9322 - MinusLogProbMetric: 31.9322 - val_loss: 31.8810 - val_MinusLogProbMetric: 31.8810 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 114/1000
2023-10-27 01:37:26.492 
Epoch 114/1000 
	 loss: 31.7527, MinusLogProbMetric: 31.7527, val_loss: 32.0312, val_MinusLogProbMetric: 32.0312

Epoch 114: val_loss did not improve from 31.43202
196/196 - 42s - loss: 31.7527 - MinusLogProbMetric: 31.7527 - val_loss: 32.0312 - val_MinusLogProbMetric: 32.0312 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 115/1000
2023-10-27 01:38:09.163 
Epoch 115/1000 
	 loss: 31.8494, MinusLogProbMetric: 31.8494, val_loss: 31.3646, val_MinusLogProbMetric: 31.3646

Epoch 115: val_loss improved from 31.43202 to 31.36460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 31.8494 - MinusLogProbMetric: 31.8494 - val_loss: 31.3646 - val_MinusLogProbMetric: 31.3646 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 116/1000
2023-10-27 01:38:52.396 
Epoch 116/1000 
	 loss: 31.8895, MinusLogProbMetric: 31.8895, val_loss: 31.5729, val_MinusLogProbMetric: 31.5729

Epoch 116: val_loss did not improve from 31.36460
196/196 - 42s - loss: 31.8895 - MinusLogProbMetric: 31.8895 - val_loss: 31.5729 - val_MinusLogProbMetric: 31.5729 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 117/1000
2023-10-27 01:39:35.092 
Epoch 117/1000 
	 loss: 31.8687, MinusLogProbMetric: 31.8687, val_loss: 31.8795, val_MinusLogProbMetric: 31.8795

Epoch 117: val_loss did not improve from 31.36460
196/196 - 43s - loss: 31.8687 - MinusLogProbMetric: 31.8687 - val_loss: 31.8795 - val_MinusLogProbMetric: 31.8795 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 118/1000
2023-10-27 01:40:17.468 
Epoch 118/1000 
	 loss: 31.7452, MinusLogProbMetric: 31.7452, val_loss: 31.8609, val_MinusLogProbMetric: 31.8609

Epoch 118: val_loss did not improve from 31.36460
196/196 - 42s - loss: 31.7452 - MinusLogProbMetric: 31.7452 - val_loss: 31.8609 - val_MinusLogProbMetric: 31.8609 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 119/1000
2023-10-27 01:40:59.395 
Epoch 119/1000 
	 loss: 31.8867, MinusLogProbMetric: 31.8867, val_loss: 33.2696, val_MinusLogProbMetric: 33.2696

Epoch 119: val_loss did not improve from 31.36460
196/196 - 42s - loss: 31.8867 - MinusLogProbMetric: 31.8867 - val_loss: 33.2696 - val_MinusLogProbMetric: 33.2696 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 120/1000
2023-10-27 01:41:41.980 
Epoch 120/1000 
	 loss: 31.8675, MinusLogProbMetric: 31.8675, val_loss: 31.5297, val_MinusLogProbMetric: 31.5297

Epoch 120: val_loss did not improve from 31.36460
196/196 - 43s - loss: 31.8675 - MinusLogProbMetric: 31.8675 - val_loss: 31.5297 - val_MinusLogProbMetric: 31.5297 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 121/1000
2023-10-27 01:42:24.719 
Epoch 121/1000 
	 loss: 31.8045, MinusLogProbMetric: 31.8045, val_loss: 31.4010, val_MinusLogProbMetric: 31.4010

Epoch 121: val_loss did not improve from 31.36460
196/196 - 43s - loss: 31.8045 - MinusLogProbMetric: 31.8045 - val_loss: 31.4010 - val_MinusLogProbMetric: 31.4010 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 122/1000
2023-10-27 01:43:07.297 
Epoch 122/1000 
	 loss: 31.8033, MinusLogProbMetric: 31.8033, val_loss: 31.5447, val_MinusLogProbMetric: 31.5447

Epoch 122: val_loss did not improve from 31.36460
196/196 - 43s - loss: 31.8033 - MinusLogProbMetric: 31.8033 - val_loss: 31.5447 - val_MinusLogProbMetric: 31.5447 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 123/1000
2023-10-27 01:43:49.658 
Epoch 123/1000 
	 loss: 31.8490, MinusLogProbMetric: 31.8490, val_loss: 31.6135, val_MinusLogProbMetric: 31.6135

Epoch 123: val_loss did not improve from 31.36460
196/196 - 42s - loss: 31.8490 - MinusLogProbMetric: 31.8490 - val_loss: 31.6135 - val_MinusLogProbMetric: 31.6135 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 124/1000
2023-10-27 01:44:32.126 
Epoch 124/1000 
	 loss: 31.6002, MinusLogProbMetric: 31.6002, val_loss: 33.7586, val_MinusLogProbMetric: 33.7586

Epoch 124: val_loss did not improve from 31.36460
196/196 - 42s - loss: 31.6002 - MinusLogProbMetric: 31.6002 - val_loss: 33.7586 - val_MinusLogProbMetric: 33.7586 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 125/1000
2023-10-27 01:45:14.487 
Epoch 125/1000 
	 loss: 31.7910, MinusLogProbMetric: 31.7910, val_loss: 32.3489, val_MinusLogProbMetric: 32.3489

Epoch 125: val_loss did not improve from 31.36460
196/196 - 42s - loss: 31.7910 - MinusLogProbMetric: 31.7910 - val_loss: 32.3489 - val_MinusLogProbMetric: 32.3489 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 126/1000
2023-10-27 01:45:55.993 
Epoch 126/1000 
	 loss: 31.7559, MinusLogProbMetric: 31.7559, val_loss: 31.6764, val_MinusLogProbMetric: 31.6764

Epoch 126: val_loss did not improve from 31.36460
196/196 - 41s - loss: 31.7559 - MinusLogProbMetric: 31.7559 - val_loss: 31.6764 - val_MinusLogProbMetric: 31.6764 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 127/1000
2023-10-27 01:46:38.279 
Epoch 127/1000 
	 loss: 31.7419, MinusLogProbMetric: 31.7419, val_loss: 31.4945, val_MinusLogProbMetric: 31.4945

Epoch 127: val_loss did not improve from 31.36460
196/196 - 42s - loss: 31.7419 - MinusLogProbMetric: 31.7419 - val_loss: 31.4945 - val_MinusLogProbMetric: 31.4945 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 128/1000
2023-10-27 01:47:18.742 
Epoch 128/1000 
	 loss: 31.5423, MinusLogProbMetric: 31.5423, val_loss: 31.5694, val_MinusLogProbMetric: 31.5694

Epoch 128: val_loss did not improve from 31.36460
196/196 - 40s - loss: 31.5423 - MinusLogProbMetric: 31.5423 - val_loss: 31.5694 - val_MinusLogProbMetric: 31.5694 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 129/1000
2023-10-27 01:48:01.146 
Epoch 129/1000 
	 loss: 31.4518, MinusLogProbMetric: 31.4518, val_loss: 31.2873, val_MinusLogProbMetric: 31.2873

Epoch 129: val_loss improved from 31.36460 to 31.28725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 31.4518 - MinusLogProbMetric: 31.4518 - val_loss: 31.2873 - val_MinusLogProbMetric: 31.2873 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 130/1000
2023-10-27 01:48:42.790 
Epoch 130/1000 
	 loss: 31.5970, MinusLogProbMetric: 31.5970, val_loss: 31.5323, val_MinusLogProbMetric: 31.5323

Epoch 130: val_loss did not improve from 31.28725
196/196 - 41s - loss: 31.5970 - MinusLogProbMetric: 31.5970 - val_loss: 31.5323 - val_MinusLogProbMetric: 31.5323 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 131/1000
2023-10-27 01:49:25.230 
Epoch 131/1000 
	 loss: 31.4913, MinusLogProbMetric: 31.4913, val_loss: 31.0097, val_MinusLogProbMetric: 31.0097

Epoch 131: val_loss improved from 31.28725 to 31.00973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 31.4913 - MinusLogProbMetric: 31.4913 - val_loss: 31.0097 - val_MinusLogProbMetric: 31.0097 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 132/1000
2023-10-27 01:50:08.286 
Epoch 132/1000 
	 loss: 31.7134, MinusLogProbMetric: 31.7134, val_loss: 32.2256, val_MinusLogProbMetric: 32.2256

Epoch 132: val_loss did not improve from 31.00973
196/196 - 42s - loss: 31.7134 - MinusLogProbMetric: 31.7134 - val_loss: 32.2256 - val_MinusLogProbMetric: 32.2256 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 133/1000
2023-10-27 01:50:50.672 
Epoch 133/1000 
	 loss: 31.7383, MinusLogProbMetric: 31.7383, val_loss: 32.7839, val_MinusLogProbMetric: 32.7839

Epoch 133: val_loss did not improve from 31.00973
196/196 - 42s - loss: 31.7383 - MinusLogProbMetric: 31.7383 - val_loss: 32.7839 - val_MinusLogProbMetric: 32.7839 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 134/1000
2023-10-27 01:51:32.787 
Epoch 134/1000 
	 loss: 31.4814, MinusLogProbMetric: 31.4814, val_loss: 31.7004, val_MinusLogProbMetric: 31.7004

Epoch 134: val_loss did not improve from 31.00973
196/196 - 42s - loss: 31.4814 - MinusLogProbMetric: 31.4814 - val_loss: 31.7004 - val_MinusLogProbMetric: 31.7004 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 135/1000
2023-10-27 01:52:14.930 
Epoch 135/1000 
	 loss: 31.5888, MinusLogProbMetric: 31.5888, val_loss: 31.3016, val_MinusLogProbMetric: 31.3016

Epoch 135: val_loss did not improve from 31.00973
196/196 - 42s - loss: 31.5888 - MinusLogProbMetric: 31.5888 - val_loss: 31.3016 - val_MinusLogProbMetric: 31.3016 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 136/1000
2023-10-27 01:52:57.181 
Epoch 136/1000 
	 loss: 31.5805, MinusLogProbMetric: 31.5805, val_loss: 32.1544, val_MinusLogProbMetric: 32.1544

Epoch 136: val_loss did not improve from 31.00973
196/196 - 42s - loss: 31.5805 - MinusLogProbMetric: 31.5805 - val_loss: 32.1544 - val_MinusLogProbMetric: 32.1544 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 137/1000
2023-10-27 01:53:39.695 
Epoch 137/1000 
	 loss: 31.3438, MinusLogProbMetric: 31.3438, val_loss: 31.0844, val_MinusLogProbMetric: 31.0844

Epoch 137: val_loss did not improve from 31.00973
196/196 - 43s - loss: 31.3438 - MinusLogProbMetric: 31.3438 - val_loss: 31.0844 - val_MinusLogProbMetric: 31.0844 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 138/1000
2023-10-27 01:54:22.381 
Epoch 138/1000 
	 loss: 31.3784, MinusLogProbMetric: 31.3784, val_loss: 33.8309, val_MinusLogProbMetric: 33.8309

Epoch 138: val_loss did not improve from 31.00973
196/196 - 43s - loss: 31.3784 - MinusLogProbMetric: 31.3784 - val_loss: 33.8309 - val_MinusLogProbMetric: 33.8309 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 139/1000
2023-10-27 01:55:05.208 
Epoch 139/1000 
	 loss: 31.3112, MinusLogProbMetric: 31.3112, val_loss: 31.6908, val_MinusLogProbMetric: 31.6908

Epoch 139: val_loss did not improve from 31.00973
196/196 - 43s - loss: 31.3112 - MinusLogProbMetric: 31.3112 - val_loss: 31.6908 - val_MinusLogProbMetric: 31.6908 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 140/1000
2023-10-27 01:55:47.750 
Epoch 140/1000 
	 loss: 31.3129, MinusLogProbMetric: 31.3129, val_loss: 32.9678, val_MinusLogProbMetric: 32.9678

Epoch 140: val_loss did not improve from 31.00973
196/196 - 43s - loss: 31.3129 - MinusLogProbMetric: 31.3129 - val_loss: 32.9678 - val_MinusLogProbMetric: 32.9678 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 141/1000
2023-10-27 01:56:30.377 
Epoch 141/1000 
	 loss: 31.3366, MinusLogProbMetric: 31.3366, val_loss: 30.9298, val_MinusLogProbMetric: 30.9298

Epoch 141: val_loss improved from 31.00973 to 30.92979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 31.3366 - MinusLogProbMetric: 31.3366 - val_loss: 30.9298 - val_MinusLogProbMetric: 30.9298 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 142/1000
2023-10-27 01:57:13.895 
Epoch 142/1000 
	 loss: 31.3694, MinusLogProbMetric: 31.3694, val_loss: 34.1828, val_MinusLogProbMetric: 34.1828

Epoch 142: val_loss did not improve from 30.92979
196/196 - 43s - loss: 31.3694 - MinusLogProbMetric: 31.3694 - val_loss: 34.1828 - val_MinusLogProbMetric: 34.1828 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 143/1000
2023-10-27 01:57:55.983 
Epoch 143/1000 
	 loss: 31.2532, MinusLogProbMetric: 31.2532, val_loss: 33.7017, val_MinusLogProbMetric: 33.7017

Epoch 143: val_loss did not improve from 30.92979
196/196 - 42s - loss: 31.2532 - MinusLogProbMetric: 31.2532 - val_loss: 33.7017 - val_MinusLogProbMetric: 33.7017 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 144/1000
2023-10-27 01:58:37.938 
Epoch 144/1000 
	 loss: 31.1663, MinusLogProbMetric: 31.1663, val_loss: 32.5693, val_MinusLogProbMetric: 32.5693

Epoch 144: val_loss did not improve from 30.92979
196/196 - 42s - loss: 31.1663 - MinusLogProbMetric: 31.1663 - val_loss: 32.5693 - val_MinusLogProbMetric: 32.5693 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 145/1000
2023-10-27 01:59:20.250 
Epoch 145/1000 
	 loss: 31.4085, MinusLogProbMetric: 31.4085, val_loss: 30.8477, val_MinusLogProbMetric: 30.8477

Epoch 145: val_loss improved from 30.92979 to 30.84765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 31.4085 - MinusLogProbMetric: 31.4085 - val_loss: 30.8477 - val_MinusLogProbMetric: 30.8477 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 146/1000
2023-10-27 02:00:03.394 
Epoch 146/1000 
	 loss: 31.0602, MinusLogProbMetric: 31.0602, val_loss: 32.9661, val_MinusLogProbMetric: 32.9661

Epoch 146: val_loss did not improve from 30.84765
196/196 - 42s - loss: 31.0602 - MinusLogProbMetric: 31.0602 - val_loss: 32.9661 - val_MinusLogProbMetric: 32.9661 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 147/1000
2023-10-27 02:00:45.729 
Epoch 147/1000 
	 loss: 31.4228, MinusLogProbMetric: 31.4228, val_loss: 31.1589, val_MinusLogProbMetric: 31.1589

Epoch 147: val_loss did not improve from 30.84765
196/196 - 42s - loss: 31.4228 - MinusLogProbMetric: 31.4228 - val_loss: 31.1589 - val_MinusLogProbMetric: 31.1589 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 148/1000
2023-10-27 02:01:28.348 
Epoch 148/1000 
	 loss: 30.9905, MinusLogProbMetric: 30.9905, val_loss: 30.7257, val_MinusLogProbMetric: 30.7257

Epoch 148: val_loss improved from 30.84765 to 30.72570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 30.9905 - MinusLogProbMetric: 30.9905 - val_loss: 30.7257 - val_MinusLogProbMetric: 30.7257 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 149/1000
2023-10-27 02:02:11.608 
Epoch 149/1000 
	 loss: 31.2927, MinusLogProbMetric: 31.2927, val_loss: 31.2987, val_MinusLogProbMetric: 31.2987

Epoch 149: val_loss did not improve from 30.72570
196/196 - 42s - loss: 31.2927 - MinusLogProbMetric: 31.2927 - val_loss: 31.2987 - val_MinusLogProbMetric: 31.2987 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 150/1000
2023-10-27 02:02:54.004 
Epoch 150/1000 
	 loss: 31.1342, MinusLogProbMetric: 31.1342, val_loss: 30.9935, val_MinusLogProbMetric: 30.9935

Epoch 150: val_loss did not improve from 30.72570
196/196 - 42s - loss: 31.1342 - MinusLogProbMetric: 31.1342 - val_loss: 30.9935 - val_MinusLogProbMetric: 30.9935 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 151/1000
2023-10-27 02:03:32.421 
Epoch 151/1000 
	 loss: 31.2096, MinusLogProbMetric: 31.2096, val_loss: 32.0228, val_MinusLogProbMetric: 32.0228

Epoch 151: val_loss did not improve from 30.72570
196/196 - 38s - loss: 31.2096 - MinusLogProbMetric: 31.2096 - val_loss: 32.0228 - val_MinusLogProbMetric: 32.0228 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 152/1000
2023-10-27 02:04:05.824 
Epoch 152/1000 
	 loss: 31.0702, MinusLogProbMetric: 31.0702, val_loss: 30.8225, val_MinusLogProbMetric: 30.8225

Epoch 152: val_loss did not improve from 30.72570
196/196 - 33s - loss: 31.0702 - MinusLogProbMetric: 31.0702 - val_loss: 30.8225 - val_MinusLogProbMetric: 30.8225 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 153/1000
2023-10-27 02:04:47.574 
Epoch 153/1000 
	 loss: 31.1546, MinusLogProbMetric: 31.1546, val_loss: 30.6028, val_MinusLogProbMetric: 30.6028

Epoch 153: val_loss improved from 30.72570 to 30.60280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 31.1546 - MinusLogProbMetric: 31.1546 - val_loss: 30.6028 - val_MinusLogProbMetric: 30.6028 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 154/1000
2023-10-27 02:05:30.428 
Epoch 154/1000 
	 loss: 31.3064, MinusLogProbMetric: 31.3064, val_loss: 31.3803, val_MinusLogProbMetric: 31.3803

Epoch 154: val_loss did not improve from 30.60280
196/196 - 42s - loss: 31.3064 - MinusLogProbMetric: 31.3064 - val_loss: 31.3803 - val_MinusLogProbMetric: 31.3803 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 155/1000
2023-10-27 02:06:12.507 
Epoch 155/1000 
	 loss: 30.8939, MinusLogProbMetric: 30.8939, val_loss: 31.3797, val_MinusLogProbMetric: 31.3797

Epoch 155: val_loss did not improve from 30.60280
196/196 - 42s - loss: 30.8939 - MinusLogProbMetric: 30.8939 - val_loss: 31.3797 - val_MinusLogProbMetric: 31.3797 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 156/1000
2023-10-27 02:06:54.307 
Epoch 156/1000 
	 loss: 31.1198, MinusLogProbMetric: 31.1198, val_loss: 31.1902, val_MinusLogProbMetric: 31.1902

Epoch 156: val_loss did not improve from 30.60280
196/196 - 42s - loss: 31.1198 - MinusLogProbMetric: 31.1198 - val_loss: 31.1902 - val_MinusLogProbMetric: 31.1902 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 157/1000
2023-10-27 02:07:36.743 
Epoch 157/1000 
	 loss: 31.0395, MinusLogProbMetric: 31.0395, val_loss: 30.5427, val_MinusLogProbMetric: 30.5427

Epoch 157: val_loss improved from 30.60280 to 30.54266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 31.0395 - MinusLogProbMetric: 31.0395 - val_loss: 30.5427 - val_MinusLogProbMetric: 30.5427 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 158/1000
2023-10-27 02:08:19.979 
Epoch 158/1000 
	 loss: 30.9181, MinusLogProbMetric: 30.9181, val_loss: 31.7826, val_MinusLogProbMetric: 31.7826

Epoch 158: val_loss did not improve from 30.54266
196/196 - 42s - loss: 30.9181 - MinusLogProbMetric: 30.9181 - val_loss: 31.7826 - val_MinusLogProbMetric: 31.7826 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 159/1000
2023-10-27 02:09:01.928 
Epoch 159/1000 
	 loss: 31.0323, MinusLogProbMetric: 31.0323, val_loss: 30.4092, val_MinusLogProbMetric: 30.4092

Epoch 159: val_loss improved from 30.54266 to 30.40923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 31.0323 - MinusLogProbMetric: 31.0323 - val_loss: 30.4092 - val_MinusLogProbMetric: 30.4092 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 160/1000
2023-10-27 02:09:44.534 
Epoch 160/1000 
	 loss: 31.1130, MinusLogProbMetric: 31.1130, val_loss: 31.4349, val_MinusLogProbMetric: 31.4349

Epoch 160: val_loss did not improve from 30.40923
196/196 - 42s - loss: 31.1130 - MinusLogProbMetric: 31.1130 - val_loss: 31.4349 - val_MinusLogProbMetric: 31.4349 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 161/1000
2023-10-27 02:10:26.657 
Epoch 161/1000 
	 loss: 30.9055, MinusLogProbMetric: 30.9055, val_loss: 31.3238, val_MinusLogProbMetric: 31.3238

Epoch 161: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.9055 - MinusLogProbMetric: 30.9055 - val_loss: 31.3238 - val_MinusLogProbMetric: 31.3238 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 162/1000
2023-10-27 02:11:08.719 
Epoch 162/1000 
	 loss: 30.9285, MinusLogProbMetric: 30.9285, val_loss: 31.3929, val_MinusLogProbMetric: 31.3929

Epoch 162: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.9285 - MinusLogProbMetric: 30.9285 - val_loss: 31.3929 - val_MinusLogProbMetric: 31.3929 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 163/1000
2023-10-27 02:11:50.727 
Epoch 163/1000 
	 loss: 31.0902, MinusLogProbMetric: 31.0902, val_loss: 31.2074, val_MinusLogProbMetric: 31.2074

Epoch 163: val_loss did not improve from 30.40923
196/196 - 42s - loss: 31.0902 - MinusLogProbMetric: 31.0902 - val_loss: 31.2074 - val_MinusLogProbMetric: 31.2074 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 164/1000
2023-10-27 02:12:32.896 
Epoch 164/1000 
	 loss: 30.7316, MinusLogProbMetric: 30.7316, val_loss: 30.7324, val_MinusLogProbMetric: 30.7324

Epoch 164: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.7316 - MinusLogProbMetric: 30.7316 - val_loss: 30.7324 - val_MinusLogProbMetric: 30.7324 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 165/1000
2023-10-27 02:13:15.250 
Epoch 165/1000 
	 loss: 30.8632, MinusLogProbMetric: 30.8632, val_loss: 31.3512, val_MinusLogProbMetric: 31.3512

Epoch 165: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.8632 - MinusLogProbMetric: 30.8632 - val_loss: 31.3512 - val_MinusLogProbMetric: 31.3512 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 166/1000
2023-10-27 02:13:57.637 
Epoch 166/1000 
	 loss: 31.0569, MinusLogProbMetric: 31.0569, val_loss: 30.8613, val_MinusLogProbMetric: 30.8613

Epoch 166: val_loss did not improve from 30.40923
196/196 - 42s - loss: 31.0569 - MinusLogProbMetric: 31.0569 - val_loss: 30.8613 - val_MinusLogProbMetric: 30.8613 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 167/1000
2023-10-27 02:14:39.608 
Epoch 167/1000 
	 loss: 30.8109, MinusLogProbMetric: 30.8109, val_loss: 30.9171, val_MinusLogProbMetric: 30.9171

Epoch 167: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.8109 - MinusLogProbMetric: 30.8109 - val_loss: 30.9171 - val_MinusLogProbMetric: 30.9171 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 168/1000
2023-10-27 02:15:21.961 
Epoch 168/1000 
	 loss: 31.1025, MinusLogProbMetric: 31.1025, val_loss: 30.5983, val_MinusLogProbMetric: 30.5983

Epoch 168: val_loss did not improve from 30.40923
196/196 - 42s - loss: 31.1025 - MinusLogProbMetric: 31.1025 - val_loss: 30.5983 - val_MinusLogProbMetric: 30.5983 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 169/1000
2023-10-27 02:16:04.457 
Epoch 169/1000 
	 loss: 30.7333, MinusLogProbMetric: 30.7333, val_loss: 32.0812, val_MinusLogProbMetric: 32.0812

Epoch 169: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.7333 - MinusLogProbMetric: 30.7333 - val_loss: 32.0812 - val_MinusLogProbMetric: 32.0812 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 170/1000
2023-10-27 02:16:46.564 
Epoch 170/1000 
	 loss: 30.7426, MinusLogProbMetric: 30.7426, val_loss: 31.5808, val_MinusLogProbMetric: 31.5808

Epoch 170: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.7426 - MinusLogProbMetric: 30.7426 - val_loss: 31.5808 - val_MinusLogProbMetric: 31.5808 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 171/1000
2023-10-27 02:17:29.069 
Epoch 171/1000 
	 loss: 30.9571, MinusLogProbMetric: 30.9571, val_loss: 30.5148, val_MinusLogProbMetric: 30.5148

Epoch 171: val_loss did not improve from 30.40923
196/196 - 43s - loss: 30.9571 - MinusLogProbMetric: 30.9571 - val_loss: 30.5148 - val_MinusLogProbMetric: 30.5148 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 172/1000
2023-10-27 02:18:11.187 
Epoch 172/1000 
	 loss: 30.7415, MinusLogProbMetric: 30.7415, val_loss: 31.5916, val_MinusLogProbMetric: 31.5916

Epoch 172: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.7415 - MinusLogProbMetric: 30.7415 - val_loss: 31.5916 - val_MinusLogProbMetric: 31.5916 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 173/1000
2023-10-27 02:18:52.630 
Epoch 173/1000 
	 loss: 30.6355, MinusLogProbMetric: 30.6355, val_loss: 31.3734, val_MinusLogProbMetric: 31.3734

Epoch 173: val_loss did not improve from 30.40923
196/196 - 41s - loss: 30.6355 - MinusLogProbMetric: 30.6355 - val_loss: 31.3734 - val_MinusLogProbMetric: 31.3734 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 174/1000
2023-10-27 02:19:34.156 
Epoch 174/1000 
	 loss: 30.8852, MinusLogProbMetric: 30.8852, val_loss: 31.1149, val_MinusLogProbMetric: 31.1149

Epoch 174: val_loss did not improve from 30.40923
196/196 - 42s - loss: 30.8852 - MinusLogProbMetric: 30.8852 - val_loss: 31.1149 - val_MinusLogProbMetric: 31.1149 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 175/1000
2023-10-27 02:20:16.927 
Epoch 175/1000 
	 loss: 30.5759, MinusLogProbMetric: 30.5759, val_loss: 30.4055, val_MinusLogProbMetric: 30.4055

Epoch 175: val_loss improved from 30.40923 to 30.40550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 30.5759 - MinusLogProbMetric: 30.5759 - val_loss: 30.4055 - val_MinusLogProbMetric: 30.4055 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 176/1000
2023-10-27 02:20:59.744 
Epoch 176/1000 
	 loss: 30.7795, MinusLogProbMetric: 30.7795, val_loss: 31.5343, val_MinusLogProbMetric: 31.5343

Epoch 176: val_loss did not improve from 30.40550
196/196 - 42s - loss: 30.7795 - MinusLogProbMetric: 30.7795 - val_loss: 31.5343 - val_MinusLogProbMetric: 31.5343 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 177/1000
2023-10-27 02:21:41.937 
Epoch 177/1000 
	 loss: 30.7518, MinusLogProbMetric: 30.7518, val_loss: 30.6801, val_MinusLogProbMetric: 30.6801

Epoch 177: val_loss did not improve from 30.40550
196/196 - 42s - loss: 30.7518 - MinusLogProbMetric: 30.7518 - val_loss: 30.6801 - val_MinusLogProbMetric: 30.6801 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 178/1000
2023-10-27 02:22:24.409 
Epoch 178/1000 
	 loss: 30.6346, MinusLogProbMetric: 30.6346, val_loss: 30.9666, val_MinusLogProbMetric: 30.9666

Epoch 178: val_loss did not improve from 30.40550
196/196 - 42s - loss: 30.6346 - MinusLogProbMetric: 30.6346 - val_loss: 30.9666 - val_MinusLogProbMetric: 30.9666 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 179/1000
2023-10-27 02:23:06.603 
Epoch 179/1000 
	 loss: 30.8468, MinusLogProbMetric: 30.8468, val_loss: 31.9600, val_MinusLogProbMetric: 31.9600

Epoch 179: val_loss did not improve from 30.40550
196/196 - 42s - loss: 30.8468 - MinusLogProbMetric: 30.8468 - val_loss: 31.9600 - val_MinusLogProbMetric: 31.9600 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 180/1000
2023-10-27 02:23:49.072 
Epoch 180/1000 
	 loss: 30.5739, MinusLogProbMetric: 30.5739, val_loss: 31.3972, val_MinusLogProbMetric: 31.3972

Epoch 180: val_loss did not improve from 30.40550
196/196 - 42s - loss: 30.5739 - MinusLogProbMetric: 30.5739 - val_loss: 31.3972 - val_MinusLogProbMetric: 31.3972 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 181/1000
2023-10-27 02:24:31.415 
Epoch 181/1000 
	 loss: 30.9193, MinusLogProbMetric: 30.9193, val_loss: 30.9866, val_MinusLogProbMetric: 30.9866

Epoch 181: val_loss did not improve from 30.40550
196/196 - 42s - loss: 30.9193 - MinusLogProbMetric: 30.9193 - val_loss: 30.9866 - val_MinusLogProbMetric: 30.9866 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 182/1000
2023-10-27 02:25:13.577 
Epoch 182/1000 
	 loss: 30.6100, MinusLogProbMetric: 30.6100, val_loss: 30.4083, val_MinusLogProbMetric: 30.4083

Epoch 182: val_loss did not improve from 30.40550
196/196 - 42s - loss: 30.6100 - MinusLogProbMetric: 30.6100 - val_loss: 30.4083 - val_MinusLogProbMetric: 30.4083 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 183/1000
2023-10-27 02:25:55.657 
Epoch 183/1000 
	 loss: 30.5543, MinusLogProbMetric: 30.5543, val_loss: 30.2643, val_MinusLogProbMetric: 30.2643

Epoch 183: val_loss improved from 30.40550 to 30.26432, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 30.5543 - MinusLogProbMetric: 30.5543 - val_loss: 30.2643 - val_MinusLogProbMetric: 30.2643 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 184/1000
2023-10-27 02:26:38.727 
Epoch 184/1000 
	 loss: 30.3985, MinusLogProbMetric: 30.3985, val_loss: 31.6053, val_MinusLogProbMetric: 31.6053

Epoch 184: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.3985 - MinusLogProbMetric: 30.3985 - val_loss: 31.6053 - val_MinusLogProbMetric: 31.6053 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 185/1000
2023-10-27 02:27:20.679 
Epoch 185/1000 
	 loss: 30.9973, MinusLogProbMetric: 30.9973, val_loss: 31.0972, val_MinusLogProbMetric: 31.0972

Epoch 185: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.9973 - MinusLogProbMetric: 30.9973 - val_loss: 31.0972 - val_MinusLogProbMetric: 31.0972 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 186/1000
2023-10-27 02:28:03.128 
Epoch 186/1000 
	 loss: 30.4377, MinusLogProbMetric: 30.4377, val_loss: 30.3818, val_MinusLogProbMetric: 30.3818

Epoch 186: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.4377 - MinusLogProbMetric: 30.4377 - val_loss: 30.3818 - val_MinusLogProbMetric: 30.3818 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 187/1000
2023-10-27 02:28:45.412 
Epoch 187/1000 
	 loss: 30.7791, MinusLogProbMetric: 30.7791, val_loss: 30.5651, val_MinusLogProbMetric: 30.5651

Epoch 187: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.7791 - MinusLogProbMetric: 30.7791 - val_loss: 30.5651 - val_MinusLogProbMetric: 30.5651 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 188/1000
2023-10-27 02:29:27.630 
Epoch 188/1000 
	 loss: 30.4619, MinusLogProbMetric: 30.4619, val_loss: 30.5585, val_MinusLogProbMetric: 30.5585

Epoch 188: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.4619 - MinusLogProbMetric: 30.4619 - val_loss: 30.5585 - val_MinusLogProbMetric: 30.5585 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 189/1000
2023-10-27 02:30:09.645 
Epoch 189/1000 
	 loss: 30.4432, MinusLogProbMetric: 30.4432, val_loss: 30.5287, val_MinusLogProbMetric: 30.5287

Epoch 189: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.4432 - MinusLogProbMetric: 30.4432 - val_loss: 30.5287 - val_MinusLogProbMetric: 30.5287 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 190/1000
2023-10-27 02:30:51.870 
Epoch 190/1000 
	 loss: 30.3781, MinusLogProbMetric: 30.3781, val_loss: 30.3960, val_MinusLogProbMetric: 30.3960

Epoch 190: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.3781 - MinusLogProbMetric: 30.3781 - val_loss: 30.3960 - val_MinusLogProbMetric: 30.3960 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 191/1000
2023-10-27 02:31:34.044 
Epoch 191/1000 
	 loss: 30.6610, MinusLogProbMetric: 30.6610, val_loss: 30.5790, val_MinusLogProbMetric: 30.5790

Epoch 191: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.6610 - MinusLogProbMetric: 30.6610 - val_loss: 30.5790 - val_MinusLogProbMetric: 30.5790 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 192/1000
2023-10-27 02:32:15.943 
Epoch 192/1000 
	 loss: 30.3338, MinusLogProbMetric: 30.3338, val_loss: 30.5635, val_MinusLogProbMetric: 30.5635

Epoch 192: val_loss did not improve from 30.26432
196/196 - 42s - loss: 30.3338 - MinusLogProbMetric: 30.3338 - val_loss: 30.5635 - val_MinusLogProbMetric: 30.5635 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 193/1000
2023-10-27 02:32:58.351 
Epoch 193/1000 
	 loss: 30.3211, MinusLogProbMetric: 30.3211, val_loss: 30.2481, val_MinusLogProbMetric: 30.2481

Epoch 193: val_loss improved from 30.26432 to 30.24810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 30.3211 - MinusLogProbMetric: 30.3211 - val_loss: 30.2481 - val_MinusLogProbMetric: 30.2481 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 194/1000
2023-10-27 02:33:41.872 
Epoch 194/1000 
	 loss: 30.3899, MinusLogProbMetric: 30.3899, val_loss: 30.3429, val_MinusLogProbMetric: 30.3429

Epoch 194: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.3899 - MinusLogProbMetric: 30.3899 - val_loss: 30.3429 - val_MinusLogProbMetric: 30.3429 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 195/1000
2023-10-27 02:34:24.236 
Epoch 195/1000 
	 loss: 30.4083, MinusLogProbMetric: 30.4083, val_loss: 30.2926, val_MinusLogProbMetric: 30.2926

Epoch 195: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.4083 - MinusLogProbMetric: 30.4083 - val_loss: 30.2926 - val_MinusLogProbMetric: 30.2926 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 196/1000
2023-10-27 02:35:06.102 
Epoch 196/1000 
	 loss: 30.4887, MinusLogProbMetric: 30.4887, val_loss: 30.2778, val_MinusLogProbMetric: 30.2778

Epoch 196: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.4887 - MinusLogProbMetric: 30.4887 - val_loss: 30.2778 - val_MinusLogProbMetric: 30.2778 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 197/1000
2023-10-27 02:35:48.331 
Epoch 197/1000 
	 loss: 30.4719, MinusLogProbMetric: 30.4719, val_loss: 30.5985, val_MinusLogProbMetric: 30.5985

Epoch 197: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.4719 - MinusLogProbMetric: 30.4719 - val_loss: 30.5985 - val_MinusLogProbMetric: 30.5985 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 198/1000
2023-10-27 02:36:30.386 
Epoch 198/1000 
	 loss: 30.3560, MinusLogProbMetric: 30.3560, val_loss: 30.3220, val_MinusLogProbMetric: 30.3220

Epoch 198: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.3560 - MinusLogProbMetric: 30.3560 - val_loss: 30.3220 - val_MinusLogProbMetric: 30.3220 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 199/1000
2023-10-27 02:37:12.328 
Epoch 199/1000 
	 loss: 30.3595, MinusLogProbMetric: 30.3595, val_loss: 30.2685, val_MinusLogProbMetric: 30.2685

Epoch 199: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.3595 - MinusLogProbMetric: 30.3595 - val_loss: 30.2685 - val_MinusLogProbMetric: 30.2685 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 200/1000
2023-10-27 02:37:54.547 
Epoch 200/1000 
	 loss: 30.5277, MinusLogProbMetric: 30.5277, val_loss: 31.4311, val_MinusLogProbMetric: 31.4311

Epoch 200: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.5277 - MinusLogProbMetric: 30.5277 - val_loss: 31.4311 - val_MinusLogProbMetric: 31.4311 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 201/1000
2023-10-27 02:38:36.619 
Epoch 201/1000 
	 loss: 30.5604, MinusLogProbMetric: 30.5604, val_loss: 30.6602, val_MinusLogProbMetric: 30.6602

Epoch 201: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.5604 - MinusLogProbMetric: 30.5604 - val_loss: 30.6602 - val_MinusLogProbMetric: 30.6602 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 202/1000
2023-10-27 02:39:18.290 
Epoch 202/1000 
	 loss: 30.3618, MinusLogProbMetric: 30.3618, val_loss: 30.8663, val_MinusLogProbMetric: 30.8663

Epoch 202: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.3618 - MinusLogProbMetric: 30.3618 - val_loss: 30.8663 - val_MinusLogProbMetric: 30.8663 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 203/1000
2023-10-27 02:40:00.333 
Epoch 203/1000 
	 loss: 30.2191, MinusLogProbMetric: 30.2191, val_loss: 30.3046, val_MinusLogProbMetric: 30.3046

Epoch 203: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.2191 - MinusLogProbMetric: 30.2191 - val_loss: 30.3046 - val_MinusLogProbMetric: 30.3046 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 204/1000
2023-10-27 02:40:42.523 
Epoch 204/1000 
	 loss: 30.4947, MinusLogProbMetric: 30.4947, val_loss: 30.5384, val_MinusLogProbMetric: 30.5384

Epoch 204: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.4947 - MinusLogProbMetric: 30.4947 - val_loss: 30.5384 - val_MinusLogProbMetric: 30.5384 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 205/1000
2023-10-27 02:41:24.819 
Epoch 205/1000 
	 loss: 30.3477, MinusLogProbMetric: 30.3477, val_loss: 30.9675, val_MinusLogProbMetric: 30.9675

Epoch 205: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.3477 - MinusLogProbMetric: 30.3477 - val_loss: 30.9675 - val_MinusLogProbMetric: 30.9675 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 206/1000
2023-10-27 02:42:07.092 
Epoch 206/1000 
	 loss: 30.3267, MinusLogProbMetric: 30.3267, val_loss: 31.1863, val_MinusLogProbMetric: 31.1863

Epoch 206: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.3267 - MinusLogProbMetric: 30.3267 - val_loss: 31.1863 - val_MinusLogProbMetric: 31.1863 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 207/1000
2023-10-27 02:42:49.509 
Epoch 207/1000 
	 loss: 30.6174, MinusLogProbMetric: 30.6174, val_loss: 30.8386, val_MinusLogProbMetric: 30.8386

Epoch 207: val_loss did not improve from 30.24810
196/196 - 42s - loss: 30.6174 - MinusLogProbMetric: 30.6174 - val_loss: 30.8386 - val_MinusLogProbMetric: 30.8386 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 208/1000
2023-10-27 02:43:31.829 
Epoch 208/1000 
	 loss: 30.3539, MinusLogProbMetric: 30.3539, val_loss: 30.0297, val_MinusLogProbMetric: 30.0297

Epoch 208: val_loss improved from 30.24810 to 30.02968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 30.3539 - MinusLogProbMetric: 30.3539 - val_loss: 30.0297 - val_MinusLogProbMetric: 30.0297 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 209/1000
2023-10-27 02:44:14.221 
Epoch 209/1000 
	 loss: 30.2435, MinusLogProbMetric: 30.2435, val_loss: 30.4474, val_MinusLogProbMetric: 30.4474

Epoch 209: val_loss did not improve from 30.02968
196/196 - 42s - loss: 30.2435 - MinusLogProbMetric: 30.2435 - val_loss: 30.4474 - val_MinusLogProbMetric: 30.4474 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 210/1000
2023-10-27 02:44:56.562 
Epoch 210/1000 
	 loss: 30.4223, MinusLogProbMetric: 30.4223, val_loss: 29.7493, val_MinusLogProbMetric: 29.7493

Epoch 210: val_loss improved from 30.02968 to 29.74932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 30.4223 - MinusLogProbMetric: 30.4223 - val_loss: 29.7493 - val_MinusLogProbMetric: 29.7493 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 211/1000
2023-10-27 02:45:39.548 
Epoch 211/1000 
	 loss: 30.3257, MinusLogProbMetric: 30.3257, val_loss: 30.4778, val_MinusLogProbMetric: 30.4778

Epoch 211: val_loss did not improve from 29.74932
196/196 - 42s - loss: 30.3257 - MinusLogProbMetric: 30.3257 - val_loss: 30.4778 - val_MinusLogProbMetric: 30.4778 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 212/1000
2023-10-27 02:46:20.895 
Epoch 212/1000 
	 loss: 30.2534, MinusLogProbMetric: 30.2534, val_loss: 30.1148, val_MinusLogProbMetric: 30.1148

Epoch 212: val_loss did not improve from 29.74932
196/196 - 41s - loss: 30.2534 - MinusLogProbMetric: 30.2534 - val_loss: 30.1148 - val_MinusLogProbMetric: 30.1148 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 213/1000
2023-10-27 02:47:02.804 
Epoch 213/1000 
	 loss: 30.2159, MinusLogProbMetric: 30.2159, val_loss: 30.3879, val_MinusLogProbMetric: 30.3879

Epoch 213: val_loss did not improve from 29.74932
196/196 - 42s - loss: 30.2159 - MinusLogProbMetric: 30.2159 - val_loss: 30.3879 - val_MinusLogProbMetric: 30.3879 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 214/1000
2023-10-27 02:47:43.773 
Epoch 214/1000 
	 loss: 30.2303, MinusLogProbMetric: 30.2303, val_loss: 30.7570, val_MinusLogProbMetric: 30.7570

Epoch 214: val_loss did not improve from 29.74932
196/196 - 41s - loss: 30.2303 - MinusLogProbMetric: 30.2303 - val_loss: 30.7570 - val_MinusLogProbMetric: 30.7570 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 215/1000
2023-10-27 02:48:25.472 
Epoch 215/1000 
	 loss: 30.4109, MinusLogProbMetric: 30.4109, val_loss: 30.0444, val_MinusLogProbMetric: 30.0444

Epoch 215: val_loss did not improve from 29.74932
196/196 - 42s - loss: 30.4109 - MinusLogProbMetric: 30.4109 - val_loss: 30.0444 - val_MinusLogProbMetric: 30.0444 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 216/1000
2023-10-27 02:49:07.121 
Epoch 216/1000 
	 loss: 30.3668, MinusLogProbMetric: 30.3668, val_loss: 30.1017, val_MinusLogProbMetric: 30.1017

Epoch 216: val_loss did not improve from 29.74932
196/196 - 42s - loss: 30.3668 - MinusLogProbMetric: 30.3668 - val_loss: 30.1017 - val_MinusLogProbMetric: 30.1017 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 217/1000
2023-10-27 02:49:49.616 
Epoch 217/1000 
	 loss: 30.1243, MinusLogProbMetric: 30.1243, val_loss: 30.0496, val_MinusLogProbMetric: 30.0496

Epoch 217: val_loss did not improve from 29.74932
196/196 - 42s - loss: 30.1243 - MinusLogProbMetric: 30.1243 - val_loss: 30.0496 - val_MinusLogProbMetric: 30.0496 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 218/1000
2023-10-27 02:50:30.632 
Epoch 218/1000 
	 loss: 30.1175, MinusLogProbMetric: 30.1175, val_loss: 30.4648, val_MinusLogProbMetric: 30.4648

Epoch 218: val_loss did not improve from 29.74932
196/196 - 41s - loss: 30.1175 - MinusLogProbMetric: 30.1175 - val_loss: 30.4648 - val_MinusLogProbMetric: 30.4648 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 219/1000
2023-10-27 02:51:09.862 
Epoch 219/1000 
	 loss: 30.4313, MinusLogProbMetric: 30.4313, val_loss: 30.3334, val_MinusLogProbMetric: 30.3334

Epoch 219: val_loss did not improve from 29.74932
196/196 - 39s - loss: 30.4313 - MinusLogProbMetric: 30.4313 - val_loss: 30.3334 - val_MinusLogProbMetric: 30.3334 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 220/1000
2023-10-27 02:51:43.230 
Epoch 220/1000 
	 loss: 30.0935, MinusLogProbMetric: 30.0935, val_loss: 31.1631, val_MinusLogProbMetric: 31.1631

Epoch 220: val_loss did not improve from 29.74932
196/196 - 33s - loss: 30.0935 - MinusLogProbMetric: 30.0935 - val_loss: 31.1631 - val_MinusLogProbMetric: 31.1631 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 221/1000
2023-10-27 02:52:16.910 
Epoch 221/1000 
	 loss: 30.1902, MinusLogProbMetric: 30.1902, val_loss: 30.0915, val_MinusLogProbMetric: 30.0915

Epoch 221: val_loss did not improve from 29.74932
196/196 - 34s - loss: 30.1902 - MinusLogProbMetric: 30.1902 - val_loss: 30.0915 - val_MinusLogProbMetric: 30.0915 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 222/1000
2023-10-27 02:52:50.848 
Epoch 222/1000 
	 loss: 30.1865, MinusLogProbMetric: 30.1865, val_loss: 30.3891, val_MinusLogProbMetric: 30.3891

Epoch 222: val_loss did not improve from 29.74932
196/196 - 34s - loss: 30.1865 - MinusLogProbMetric: 30.1865 - val_loss: 30.3891 - val_MinusLogProbMetric: 30.3891 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 223/1000
2023-10-27 02:53:30.393 
Epoch 223/1000 
	 loss: 30.0154, MinusLogProbMetric: 30.0154, val_loss: 30.5183, val_MinusLogProbMetric: 30.5183

Epoch 223: val_loss did not improve from 29.74932
196/196 - 40s - loss: 30.0154 - MinusLogProbMetric: 30.0154 - val_loss: 30.5183 - val_MinusLogProbMetric: 30.5183 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 224/1000
2023-10-27 02:54:11.877 
Epoch 224/1000 
	 loss: 30.0707, MinusLogProbMetric: 30.0707, val_loss: 29.9417, val_MinusLogProbMetric: 29.9417

Epoch 224: val_loss did not improve from 29.74932
196/196 - 41s - loss: 30.0707 - MinusLogProbMetric: 30.0707 - val_loss: 29.9417 - val_MinusLogProbMetric: 29.9417 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 225/1000
2023-10-27 02:54:46.108 
Epoch 225/1000 
	 loss: 30.1823, MinusLogProbMetric: 30.1823, val_loss: 30.8048, val_MinusLogProbMetric: 30.8048

Epoch 225: val_loss did not improve from 29.74932
196/196 - 34s - loss: 30.1823 - MinusLogProbMetric: 30.1823 - val_loss: 30.8048 - val_MinusLogProbMetric: 30.8048 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 226/1000
2023-10-27 02:55:22.937 
Epoch 226/1000 
	 loss: 30.1834, MinusLogProbMetric: 30.1834, val_loss: 30.0344, val_MinusLogProbMetric: 30.0344

Epoch 226: val_loss did not improve from 29.74932
196/196 - 37s - loss: 30.1834 - MinusLogProbMetric: 30.1834 - val_loss: 30.0344 - val_MinusLogProbMetric: 30.0344 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 227/1000
2023-10-27 02:55:58.815 
Epoch 227/1000 
	 loss: 30.1859, MinusLogProbMetric: 30.1859, val_loss: 30.0212, val_MinusLogProbMetric: 30.0212

Epoch 227: val_loss did not improve from 29.74932
196/196 - 36s - loss: 30.1859 - MinusLogProbMetric: 30.1859 - val_loss: 30.0212 - val_MinusLogProbMetric: 30.0212 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 228/1000
2023-10-27 02:56:40.085 
Epoch 228/1000 
	 loss: 30.4675, MinusLogProbMetric: 30.4675, val_loss: 30.3512, val_MinusLogProbMetric: 30.3512

Epoch 228: val_loss did not improve from 29.74932
196/196 - 41s - loss: 30.4675 - MinusLogProbMetric: 30.4675 - val_loss: 30.3512 - val_MinusLogProbMetric: 30.3512 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 229/1000
2023-10-27 02:57:18.516 
Epoch 229/1000 
	 loss: 30.2476, MinusLogProbMetric: 30.2476, val_loss: 32.7569, val_MinusLogProbMetric: 32.7569

Epoch 229: val_loss did not improve from 29.74932
196/196 - 38s - loss: 30.2476 - MinusLogProbMetric: 30.2476 - val_loss: 32.7569 - val_MinusLogProbMetric: 32.7569 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 230/1000
2023-10-27 02:57:53.119 
Epoch 230/1000 
	 loss: 30.0377, MinusLogProbMetric: 30.0377, val_loss: 30.9671, val_MinusLogProbMetric: 30.9671

Epoch 230: val_loss did not improve from 29.74932
196/196 - 35s - loss: 30.0377 - MinusLogProbMetric: 30.0377 - val_loss: 30.9671 - val_MinusLogProbMetric: 30.9671 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 231/1000
2023-10-27 02:58:31.201 
Epoch 231/1000 
	 loss: 30.0186, MinusLogProbMetric: 30.0186, val_loss: 30.2604, val_MinusLogProbMetric: 30.2604

Epoch 231: val_loss did not improve from 29.74932
196/196 - 38s - loss: 30.0186 - MinusLogProbMetric: 30.0186 - val_loss: 30.2604 - val_MinusLogProbMetric: 30.2604 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 232/1000
2023-10-27 02:59:11.702 
Epoch 232/1000 
	 loss: 30.0921, MinusLogProbMetric: 30.0921, val_loss: 30.7414, val_MinusLogProbMetric: 30.7414

Epoch 232: val_loss did not improve from 29.74932
196/196 - 40s - loss: 30.0921 - MinusLogProbMetric: 30.0921 - val_loss: 30.7414 - val_MinusLogProbMetric: 30.7414 - lr: 0.0010 - 40s/epoch - 207ms/step
Epoch 233/1000
2023-10-27 02:59:49.296 
Epoch 233/1000 
	 loss: 30.2098, MinusLogProbMetric: 30.2098, val_loss: 32.0069, val_MinusLogProbMetric: 32.0069

Epoch 233: val_loss did not improve from 29.74932
196/196 - 38s - loss: 30.2098 - MinusLogProbMetric: 30.2098 - val_loss: 32.0069 - val_MinusLogProbMetric: 32.0069 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 234/1000
2023-10-27 03:00:22.888 
Epoch 234/1000 
	 loss: 30.1194, MinusLogProbMetric: 30.1194, val_loss: 29.8864, val_MinusLogProbMetric: 29.8864

Epoch 234: val_loss did not improve from 29.74932
196/196 - 34s - loss: 30.1194 - MinusLogProbMetric: 30.1194 - val_loss: 29.8864 - val_MinusLogProbMetric: 29.8864 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 235/1000
2023-10-27 03:00:57.333 
Epoch 235/1000 
	 loss: 30.0846, MinusLogProbMetric: 30.0846, val_loss: 30.4728, val_MinusLogProbMetric: 30.4728

Epoch 235: val_loss did not improve from 29.74932
196/196 - 34s - loss: 30.0846 - MinusLogProbMetric: 30.0846 - val_loss: 30.4728 - val_MinusLogProbMetric: 30.4728 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 236/1000
2023-10-27 03:01:34.179 
Epoch 236/1000 
	 loss: 29.8815, MinusLogProbMetric: 29.8815, val_loss: 29.9600, val_MinusLogProbMetric: 29.9600

Epoch 236: val_loss did not improve from 29.74932
196/196 - 37s - loss: 29.8815 - MinusLogProbMetric: 29.8815 - val_loss: 29.9600 - val_MinusLogProbMetric: 29.9600 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 237/1000
2023-10-27 03:02:15.010 
Epoch 237/1000 
	 loss: 30.1508, MinusLogProbMetric: 30.1508, val_loss: 29.7818, val_MinusLogProbMetric: 29.7818

Epoch 237: val_loss did not improve from 29.74932
196/196 - 41s - loss: 30.1508 - MinusLogProbMetric: 30.1508 - val_loss: 29.7818 - val_MinusLogProbMetric: 29.7818 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 238/1000
2023-10-27 03:02:49.535 
Epoch 238/1000 
	 loss: 30.0911, MinusLogProbMetric: 30.0911, val_loss: 30.6108, val_MinusLogProbMetric: 30.6108

Epoch 238: val_loss did not improve from 29.74932
196/196 - 35s - loss: 30.0911 - MinusLogProbMetric: 30.0911 - val_loss: 30.6108 - val_MinusLogProbMetric: 30.6108 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 239/1000
2023-10-27 03:03:24.712 
Epoch 239/1000 
	 loss: 29.8975, MinusLogProbMetric: 29.8975, val_loss: 30.1389, val_MinusLogProbMetric: 30.1389

Epoch 239: val_loss did not improve from 29.74932
196/196 - 35s - loss: 29.8975 - MinusLogProbMetric: 29.8975 - val_loss: 30.1389 - val_MinusLogProbMetric: 30.1389 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 240/1000
2023-10-27 03:04:01.689 
Epoch 240/1000 
	 loss: 30.3552, MinusLogProbMetric: 30.3552, val_loss: 30.1837, val_MinusLogProbMetric: 30.1837

Epoch 240: val_loss did not improve from 29.74932
196/196 - 37s - loss: 30.3552 - MinusLogProbMetric: 30.3552 - val_loss: 30.1837 - val_MinusLogProbMetric: 30.1837 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 241/1000
2023-10-27 03:04:41.436 
Epoch 241/1000 
	 loss: 29.8977, MinusLogProbMetric: 29.8977, val_loss: 29.4516, val_MinusLogProbMetric: 29.4516

Epoch 241: val_loss improved from 29.74932 to 29.45163, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 40s - loss: 29.8977 - MinusLogProbMetric: 29.8977 - val_loss: 29.4516 - val_MinusLogProbMetric: 29.4516 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 242/1000
2023-10-27 03:05:15.896 
Epoch 242/1000 
	 loss: 30.1035, MinusLogProbMetric: 30.1035, val_loss: 34.0142, val_MinusLogProbMetric: 34.0142

Epoch 242: val_loss did not improve from 29.45163
196/196 - 34s - loss: 30.1035 - MinusLogProbMetric: 30.1035 - val_loss: 34.0142 - val_MinusLogProbMetric: 34.0142 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 243/1000
2023-10-27 03:05:49.729 
Epoch 243/1000 
	 loss: 30.1237, MinusLogProbMetric: 30.1237, val_loss: 29.9784, val_MinusLogProbMetric: 29.9784

Epoch 243: val_loss did not improve from 29.45163
196/196 - 34s - loss: 30.1237 - MinusLogProbMetric: 30.1237 - val_loss: 29.9784 - val_MinusLogProbMetric: 29.9784 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 244/1000
2023-10-27 03:06:29.423 
Epoch 244/1000 
	 loss: 29.9759, MinusLogProbMetric: 29.9759, val_loss: 31.3252, val_MinusLogProbMetric: 31.3252

Epoch 244: val_loss did not improve from 29.45163
196/196 - 40s - loss: 29.9759 - MinusLogProbMetric: 29.9759 - val_loss: 31.3252 - val_MinusLogProbMetric: 31.3252 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 245/1000
2023-10-27 03:07:10.334 
Epoch 245/1000 
	 loss: 30.0336, MinusLogProbMetric: 30.0336, val_loss: 30.0063, val_MinusLogProbMetric: 30.0063

Epoch 245: val_loss did not improve from 29.45163
196/196 - 41s - loss: 30.0336 - MinusLogProbMetric: 30.0336 - val_loss: 30.0063 - val_MinusLogProbMetric: 30.0063 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 246/1000
2023-10-27 03:07:45.734 
Epoch 246/1000 
	 loss: 29.9206, MinusLogProbMetric: 29.9206, val_loss: 30.1897, val_MinusLogProbMetric: 30.1897

Epoch 246: val_loss did not improve from 29.45163
196/196 - 35s - loss: 29.9206 - MinusLogProbMetric: 29.9206 - val_loss: 30.1897 - val_MinusLogProbMetric: 30.1897 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 247/1000
2023-10-27 03:08:19.317 
Epoch 247/1000 
	 loss: 30.3514, MinusLogProbMetric: 30.3514, val_loss: 30.1076, val_MinusLogProbMetric: 30.1076

Epoch 247: val_loss did not improve from 29.45163
196/196 - 34s - loss: 30.3514 - MinusLogProbMetric: 30.3514 - val_loss: 30.1076 - val_MinusLogProbMetric: 30.1076 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 248/1000
2023-10-27 03:08:56.005 
Epoch 248/1000 
	 loss: 29.9768, MinusLogProbMetric: 29.9768, val_loss: 29.7681, val_MinusLogProbMetric: 29.7681

Epoch 248: val_loss did not improve from 29.45163
196/196 - 37s - loss: 29.9768 - MinusLogProbMetric: 29.9768 - val_loss: 29.7681 - val_MinusLogProbMetric: 29.7681 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 249/1000
2023-10-27 03:09:34.598 
Epoch 249/1000 
	 loss: 30.0455, MinusLogProbMetric: 30.0455, val_loss: 30.8564, val_MinusLogProbMetric: 30.8564

Epoch 249: val_loss did not improve from 29.45163
196/196 - 39s - loss: 30.0455 - MinusLogProbMetric: 30.0455 - val_loss: 30.8564 - val_MinusLogProbMetric: 30.8564 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 250/1000
2023-10-27 03:10:12.627 
Epoch 250/1000 
	 loss: 30.2109, MinusLogProbMetric: 30.2109, val_loss: 30.3784, val_MinusLogProbMetric: 30.3784

Epoch 250: val_loss did not improve from 29.45163
196/196 - 38s - loss: 30.2109 - MinusLogProbMetric: 30.2109 - val_loss: 30.3784 - val_MinusLogProbMetric: 30.3784 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 251/1000
2023-10-27 03:10:46.860 
Epoch 251/1000 
	 loss: 29.9207, MinusLogProbMetric: 29.9207, val_loss: 29.7132, val_MinusLogProbMetric: 29.7132

Epoch 251: val_loss did not improve from 29.45163
196/196 - 34s - loss: 29.9207 - MinusLogProbMetric: 29.9207 - val_loss: 29.7132 - val_MinusLogProbMetric: 29.7132 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 252/1000
2023-10-27 03:11:23.179 
Epoch 252/1000 
	 loss: 30.0002, MinusLogProbMetric: 30.0002, val_loss: 30.6339, val_MinusLogProbMetric: 30.6339

Epoch 252: val_loss did not improve from 29.45163
196/196 - 36s - loss: 30.0002 - MinusLogProbMetric: 30.0002 - val_loss: 30.6339 - val_MinusLogProbMetric: 30.6339 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 253/1000
2023-10-27 03:12:02.790 
Epoch 253/1000 
	 loss: 30.0776, MinusLogProbMetric: 30.0776, val_loss: 30.0975, val_MinusLogProbMetric: 30.0975

Epoch 253: val_loss did not improve from 29.45163
196/196 - 40s - loss: 30.0776 - MinusLogProbMetric: 30.0776 - val_loss: 30.0975 - val_MinusLogProbMetric: 30.0975 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 254/1000
2023-10-27 03:12:37.020 
Epoch 254/1000 
	 loss: 29.8896, MinusLogProbMetric: 29.8896, val_loss: 29.6315, val_MinusLogProbMetric: 29.6315

Epoch 254: val_loss did not improve from 29.45163
196/196 - 34s - loss: 29.8896 - MinusLogProbMetric: 29.8896 - val_loss: 29.6315 - val_MinusLogProbMetric: 29.6315 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 255/1000
2023-10-27 03:13:10.334 
Epoch 255/1000 
	 loss: 29.9807, MinusLogProbMetric: 29.9807, val_loss: 29.3306, val_MinusLogProbMetric: 29.3306

Epoch 255: val_loss improved from 29.45163 to 29.33060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 34s - loss: 29.9807 - MinusLogProbMetric: 29.9807 - val_loss: 29.3306 - val_MinusLogProbMetric: 29.3306 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 256/1000
2023-10-27 03:13:47.375 
Epoch 256/1000 
	 loss: 29.8512, MinusLogProbMetric: 29.8512, val_loss: 29.8169, val_MinusLogProbMetric: 29.8169

Epoch 256: val_loss did not improve from 29.33060
196/196 - 36s - loss: 29.8512 - MinusLogProbMetric: 29.8512 - val_loss: 29.8169 - val_MinusLogProbMetric: 29.8169 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 257/1000
2023-10-27 03:14:28.997 
Epoch 257/1000 
	 loss: 29.7729, MinusLogProbMetric: 29.7729, val_loss: 29.7793, val_MinusLogProbMetric: 29.7793

Epoch 257: val_loss did not improve from 29.33060
196/196 - 42s - loss: 29.7729 - MinusLogProbMetric: 29.7729 - val_loss: 29.7793 - val_MinusLogProbMetric: 29.7793 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 258/1000
2023-10-27 03:15:03.763 
Epoch 258/1000 
	 loss: 29.9301, MinusLogProbMetric: 29.9301, val_loss: 30.2554, val_MinusLogProbMetric: 30.2554

Epoch 258: val_loss did not improve from 29.33060
196/196 - 35s - loss: 29.9301 - MinusLogProbMetric: 29.9301 - val_loss: 30.2554 - val_MinusLogProbMetric: 30.2554 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 259/1000
2023-10-27 03:15:37.282 
Epoch 259/1000 
	 loss: 29.7562, MinusLogProbMetric: 29.7562, val_loss: 29.6053, val_MinusLogProbMetric: 29.6053

Epoch 259: val_loss did not improve from 29.33060
196/196 - 34s - loss: 29.7562 - MinusLogProbMetric: 29.7562 - val_loss: 29.6053 - val_MinusLogProbMetric: 29.6053 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 260/1000
2023-10-27 03:16:12.104 
Epoch 260/1000 
	 loss: 30.0440, MinusLogProbMetric: 30.0440, val_loss: 29.4092, val_MinusLogProbMetric: 29.4092

Epoch 260: val_loss did not improve from 29.33060
196/196 - 35s - loss: 30.0440 - MinusLogProbMetric: 30.0440 - val_loss: 29.4092 - val_MinusLogProbMetric: 29.4092 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 261/1000
2023-10-27 03:16:53.069 
Epoch 261/1000 
	 loss: 29.8224, MinusLogProbMetric: 29.8224, val_loss: 32.3805, val_MinusLogProbMetric: 32.3805

Epoch 261: val_loss did not improve from 29.33060
196/196 - 41s - loss: 29.8224 - MinusLogProbMetric: 29.8224 - val_loss: 32.3805 - val_MinusLogProbMetric: 32.3805 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 262/1000
2023-10-27 03:17:27.343 
Epoch 262/1000 
	 loss: 29.9204, MinusLogProbMetric: 29.9204, val_loss: 30.0136, val_MinusLogProbMetric: 30.0136

Epoch 262: val_loss did not improve from 29.33060
196/196 - 34s - loss: 29.9204 - MinusLogProbMetric: 29.9204 - val_loss: 30.0136 - val_MinusLogProbMetric: 30.0136 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 263/1000
2023-10-27 03:18:00.881 
Epoch 263/1000 
	 loss: 29.8337, MinusLogProbMetric: 29.8337, val_loss: 29.4565, val_MinusLogProbMetric: 29.4565

Epoch 263: val_loss did not improve from 29.33060
196/196 - 34s - loss: 29.8337 - MinusLogProbMetric: 29.8337 - val_loss: 29.4565 - val_MinusLogProbMetric: 29.4565 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 264/1000
2023-10-27 03:18:34.934 
Epoch 264/1000 
	 loss: 29.9115, MinusLogProbMetric: 29.9115, val_loss: 29.3263, val_MinusLogProbMetric: 29.3263

Epoch 264: val_loss improved from 29.33060 to 29.32635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 35s - loss: 29.9115 - MinusLogProbMetric: 29.9115 - val_loss: 29.3263 - val_MinusLogProbMetric: 29.3263 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 265/1000
2023-10-27 03:19:13.761 
Epoch 265/1000 
	 loss: 29.7399, MinusLogProbMetric: 29.7399, val_loss: 29.4561, val_MinusLogProbMetric: 29.4561

Epoch 265: val_loss did not improve from 29.32635
196/196 - 38s - loss: 29.7399 - MinusLogProbMetric: 29.7399 - val_loss: 29.4561 - val_MinusLogProbMetric: 29.4561 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 266/1000
2023-10-27 03:19:51.331 
Epoch 266/1000 
	 loss: 29.8589, MinusLogProbMetric: 29.8589, val_loss: 30.5627, val_MinusLogProbMetric: 30.5627

Epoch 266: val_loss did not improve from 29.32635
196/196 - 38s - loss: 29.8589 - MinusLogProbMetric: 29.8589 - val_loss: 30.5627 - val_MinusLogProbMetric: 30.5627 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 267/1000
2023-10-27 03:20:24.693 
Epoch 267/1000 
	 loss: 29.8901, MinusLogProbMetric: 29.8901, val_loss: 30.2646, val_MinusLogProbMetric: 30.2646

Epoch 267: val_loss did not improve from 29.32635
196/196 - 33s - loss: 29.8901 - MinusLogProbMetric: 29.8901 - val_loss: 30.2646 - val_MinusLogProbMetric: 30.2646 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 268/1000
2023-10-27 03:21:00.537 
Epoch 268/1000 
	 loss: 29.7782, MinusLogProbMetric: 29.7782, val_loss: 29.6924, val_MinusLogProbMetric: 29.6924

Epoch 268: val_loss did not improve from 29.32635
196/196 - 36s - loss: 29.7782 - MinusLogProbMetric: 29.7782 - val_loss: 29.6924 - val_MinusLogProbMetric: 29.6924 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 269/1000
2023-10-27 03:21:42.944 
Epoch 269/1000 
	 loss: 30.1733, MinusLogProbMetric: 30.1733, val_loss: 30.3441, val_MinusLogProbMetric: 30.3441

Epoch 269: val_loss did not improve from 29.32635
196/196 - 42s - loss: 30.1733 - MinusLogProbMetric: 30.1733 - val_loss: 30.3441 - val_MinusLogProbMetric: 30.3441 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 270/1000
2023-10-27 03:22:18.003 
Epoch 270/1000 
	 loss: 29.7579, MinusLogProbMetric: 29.7579, val_loss: 29.9320, val_MinusLogProbMetric: 29.9320

Epoch 270: val_loss did not improve from 29.32635
196/196 - 35s - loss: 29.7579 - MinusLogProbMetric: 29.7579 - val_loss: 29.9320 - val_MinusLogProbMetric: 29.9320 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 271/1000
2023-10-27 03:22:52.538 
Epoch 271/1000 
	 loss: 29.9997, MinusLogProbMetric: 29.9997, val_loss: 30.3934, val_MinusLogProbMetric: 30.3934

Epoch 271: val_loss did not improve from 29.32635
196/196 - 35s - loss: 29.9997 - MinusLogProbMetric: 29.9997 - val_loss: 30.3934 - val_MinusLogProbMetric: 30.3934 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 272/1000
2023-10-27 03:23:27.175 
Epoch 272/1000 
	 loss: 30.1994, MinusLogProbMetric: 30.1994, val_loss: 29.7166, val_MinusLogProbMetric: 29.7166

Epoch 272: val_loss did not improve from 29.32635
196/196 - 35s - loss: 30.1994 - MinusLogProbMetric: 30.1994 - val_loss: 29.7166 - val_MinusLogProbMetric: 29.7166 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 273/1000
2023-10-27 03:24:06.985 
Epoch 273/1000 
	 loss: 29.5295, MinusLogProbMetric: 29.5295, val_loss: 29.5455, val_MinusLogProbMetric: 29.5455

Epoch 273: val_loss did not improve from 29.32635
196/196 - 40s - loss: 29.5295 - MinusLogProbMetric: 29.5295 - val_loss: 29.5455 - val_MinusLogProbMetric: 29.5455 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 274/1000
2023-10-27 03:24:49.641 
Epoch 274/1000 
	 loss: 29.8879, MinusLogProbMetric: 29.8879, val_loss: 30.0949, val_MinusLogProbMetric: 30.0949

Epoch 274: val_loss did not improve from 29.32635
196/196 - 43s - loss: 29.8879 - MinusLogProbMetric: 29.8879 - val_loss: 30.0949 - val_MinusLogProbMetric: 30.0949 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 275/1000
2023-10-27 03:25:29.589 
Epoch 275/1000 
	 loss: 29.8431, MinusLogProbMetric: 29.8431, val_loss: 30.2198, val_MinusLogProbMetric: 30.2198

Epoch 275: val_loss did not improve from 29.32635
196/196 - 40s - loss: 29.8431 - MinusLogProbMetric: 29.8431 - val_loss: 30.2198 - val_MinusLogProbMetric: 30.2198 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 276/1000
2023-10-27 03:26:12.047 
Epoch 276/1000 
	 loss: 29.7427, MinusLogProbMetric: 29.7427, val_loss: 30.3981, val_MinusLogProbMetric: 30.3981

Epoch 276: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.7427 - MinusLogProbMetric: 29.7427 - val_loss: 30.3981 - val_MinusLogProbMetric: 30.3981 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 277/1000
2023-10-27 03:26:47.735 
Epoch 277/1000 
	 loss: 30.0230, MinusLogProbMetric: 30.0230, val_loss: 29.9124, val_MinusLogProbMetric: 29.9124

Epoch 277: val_loss did not improve from 29.32635
196/196 - 36s - loss: 30.0230 - MinusLogProbMetric: 30.0230 - val_loss: 29.9124 - val_MinusLogProbMetric: 29.9124 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 278/1000
2023-10-27 03:27:21.595 
Epoch 278/1000 
	 loss: 29.8856, MinusLogProbMetric: 29.8856, val_loss: 30.2411, val_MinusLogProbMetric: 30.2411

Epoch 278: val_loss did not improve from 29.32635
196/196 - 34s - loss: 29.8856 - MinusLogProbMetric: 29.8856 - val_loss: 30.2411 - val_MinusLogProbMetric: 30.2411 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 279/1000
2023-10-27 03:27:58.818 
Epoch 279/1000 
	 loss: 29.8154, MinusLogProbMetric: 29.8154, val_loss: 29.4862, val_MinusLogProbMetric: 29.4862

Epoch 279: val_loss did not improve from 29.32635
196/196 - 37s - loss: 29.8154 - MinusLogProbMetric: 29.8154 - val_loss: 29.4862 - val_MinusLogProbMetric: 29.4862 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 280/1000
2023-10-27 03:28:40.986 
Epoch 280/1000 
	 loss: 29.6230, MinusLogProbMetric: 29.6230, val_loss: 29.3325, val_MinusLogProbMetric: 29.3325

Epoch 280: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.6230 - MinusLogProbMetric: 29.6230 - val_loss: 29.3325 - val_MinusLogProbMetric: 29.3325 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 281/1000
2023-10-27 03:29:22.471 
Epoch 281/1000 
	 loss: 30.0117, MinusLogProbMetric: 30.0117, val_loss: 30.0288, val_MinusLogProbMetric: 30.0288

Epoch 281: val_loss did not improve from 29.32635
196/196 - 41s - loss: 30.0117 - MinusLogProbMetric: 30.0117 - val_loss: 30.0288 - val_MinusLogProbMetric: 30.0288 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 282/1000
2023-10-27 03:30:04.571 
Epoch 282/1000 
	 loss: 29.5277, MinusLogProbMetric: 29.5277, val_loss: 31.7463, val_MinusLogProbMetric: 31.7463

Epoch 282: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.5277 - MinusLogProbMetric: 29.5277 - val_loss: 31.7463 - val_MinusLogProbMetric: 31.7463 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 283/1000
2023-10-27 03:30:46.623 
Epoch 283/1000 
	 loss: 29.8247, MinusLogProbMetric: 29.8247, val_loss: 29.5581, val_MinusLogProbMetric: 29.5581

Epoch 283: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.8247 - MinusLogProbMetric: 29.8247 - val_loss: 29.5581 - val_MinusLogProbMetric: 29.5581 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 284/1000
2023-10-27 03:31:28.813 
Epoch 284/1000 
	 loss: 29.7181, MinusLogProbMetric: 29.7181, val_loss: 29.6797, val_MinusLogProbMetric: 29.6797

Epoch 284: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.7181 - MinusLogProbMetric: 29.7181 - val_loss: 29.6797 - val_MinusLogProbMetric: 29.6797 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 285/1000
2023-10-27 03:32:10.738 
Epoch 285/1000 
	 loss: 29.7947, MinusLogProbMetric: 29.7947, val_loss: 29.4544, val_MinusLogProbMetric: 29.4544

Epoch 285: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.7947 - MinusLogProbMetric: 29.7947 - val_loss: 29.4544 - val_MinusLogProbMetric: 29.4544 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 286/1000
2023-10-27 03:32:52.542 
Epoch 286/1000 
	 loss: 29.6028, MinusLogProbMetric: 29.6028, val_loss: 29.8967, val_MinusLogProbMetric: 29.8967

Epoch 286: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.6028 - MinusLogProbMetric: 29.6028 - val_loss: 29.8967 - val_MinusLogProbMetric: 29.8967 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 287/1000
2023-10-27 03:33:33.519 
Epoch 287/1000 
	 loss: 29.8225, MinusLogProbMetric: 29.8225, val_loss: 29.7915, val_MinusLogProbMetric: 29.7915

Epoch 287: val_loss did not improve from 29.32635
196/196 - 41s - loss: 29.8225 - MinusLogProbMetric: 29.8225 - val_loss: 29.7915 - val_MinusLogProbMetric: 29.7915 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 288/1000
2023-10-27 03:34:14.921 
Epoch 288/1000 
	 loss: 29.7324, MinusLogProbMetric: 29.7324, val_loss: 29.5400, val_MinusLogProbMetric: 29.5400

Epoch 288: val_loss did not improve from 29.32635
196/196 - 41s - loss: 29.7324 - MinusLogProbMetric: 29.7324 - val_loss: 29.5400 - val_MinusLogProbMetric: 29.5400 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 289/1000
2023-10-27 03:34:56.897 
Epoch 289/1000 
	 loss: 29.5060, MinusLogProbMetric: 29.5060, val_loss: 31.2442, val_MinusLogProbMetric: 31.2442

Epoch 289: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.5060 - MinusLogProbMetric: 29.5060 - val_loss: 31.2442 - val_MinusLogProbMetric: 31.2442 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 290/1000
2023-10-27 03:35:39.159 
Epoch 290/1000 
	 loss: 30.0072, MinusLogProbMetric: 30.0072, val_loss: 29.6925, val_MinusLogProbMetric: 29.6925

Epoch 290: val_loss did not improve from 29.32635
196/196 - 42s - loss: 30.0072 - MinusLogProbMetric: 30.0072 - val_loss: 29.6925 - val_MinusLogProbMetric: 29.6925 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 291/1000
2023-10-27 03:36:21.269 
Epoch 291/1000 
	 loss: 29.6374, MinusLogProbMetric: 29.6374, val_loss: 29.7638, val_MinusLogProbMetric: 29.7638

Epoch 291: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.6374 - MinusLogProbMetric: 29.6374 - val_loss: 29.7638 - val_MinusLogProbMetric: 29.7638 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 292/1000
2023-10-27 03:37:03.180 
Epoch 292/1000 
	 loss: 29.5508, MinusLogProbMetric: 29.5508, val_loss: 29.9532, val_MinusLogProbMetric: 29.9532

Epoch 292: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.5508 - MinusLogProbMetric: 29.5508 - val_loss: 29.9532 - val_MinusLogProbMetric: 29.9532 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 293/1000
2023-10-27 03:37:44.969 
Epoch 293/1000 
	 loss: 29.7596, MinusLogProbMetric: 29.7596, val_loss: 30.0953, val_MinusLogProbMetric: 30.0953

Epoch 293: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.7596 - MinusLogProbMetric: 29.7596 - val_loss: 30.0953 - val_MinusLogProbMetric: 30.0953 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 294/1000
2023-10-27 03:38:27.019 
Epoch 294/1000 
	 loss: 29.7342, MinusLogProbMetric: 29.7342, val_loss: 30.3719, val_MinusLogProbMetric: 30.3719

Epoch 294: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.7342 - MinusLogProbMetric: 29.7342 - val_loss: 30.3719 - val_MinusLogProbMetric: 30.3719 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 295/1000
2023-10-27 03:39:08.870 
Epoch 295/1000 
	 loss: 29.6958, MinusLogProbMetric: 29.6958, val_loss: 29.7236, val_MinusLogProbMetric: 29.7236

Epoch 295: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.6958 - MinusLogProbMetric: 29.6958 - val_loss: 29.7236 - val_MinusLogProbMetric: 29.7236 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 296/1000
2023-10-27 03:39:50.537 
Epoch 296/1000 
	 loss: 29.5093, MinusLogProbMetric: 29.5093, val_loss: 29.5877, val_MinusLogProbMetric: 29.5877

Epoch 296: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.5093 - MinusLogProbMetric: 29.5093 - val_loss: 29.5877 - val_MinusLogProbMetric: 29.5877 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 297/1000
2023-10-27 03:40:32.395 
Epoch 297/1000 
	 loss: 30.3390, MinusLogProbMetric: 30.3390, val_loss: 29.8210, val_MinusLogProbMetric: 29.8210

Epoch 297: val_loss did not improve from 29.32635
196/196 - 42s - loss: 30.3390 - MinusLogProbMetric: 30.3390 - val_loss: 29.8210 - val_MinusLogProbMetric: 29.8210 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 298/1000
2023-10-27 03:41:14.296 
Epoch 298/1000 
	 loss: 29.6884, MinusLogProbMetric: 29.6884, val_loss: 30.2162, val_MinusLogProbMetric: 30.2162

Epoch 298: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.6884 - MinusLogProbMetric: 29.6884 - val_loss: 30.2162 - val_MinusLogProbMetric: 30.2162 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 299/1000
2023-10-27 03:41:56.521 
Epoch 299/1000 
	 loss: 29.4825, MinusLogProbMetric: 29.4825, val_loss: 29.7919, val_MinusLogProbMetric: 29.7919

Epoch 299: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.4825 - MinusLogProbMetric: 29.4825 - val_loss: 29.7919 - val_MinusLogProbMetric: 29.7919 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 300/1000
2023-10-27 03:42:37.694 
Epoch 300/1000 
	 loss: 29.9794, MinusLogProbMetric: 29.9794, val_loss: 30.6937, val_MinusLogProbMetric: 30.6937

Epoch 300: val_loss did not improve from 29.32635
196/196 - 41s - loss: 29.9794 - MinusLogProbMetric: 29.9794 - val_loss: 30.6937 - val_MinusLogProbMetric: 30.6937 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 301/1000
2023-10-27 03:43:20.371 
Epoch 301/1000 
	 loss: 29.5772, MinusLogProbMetric: 29.5772, val_loss: 32.9500, val_MinusLogProbMetric: 32.9500

Epoch 301: val_loss did not improve from 29.32635
196/196 - 43s - loss: 29.5772 - MinusLogProbMetric: 29.5772 - val_loss: 32.9500 - val_MinusLogProbMetric: 32.9500 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 302/1000
2023-10-27 03:44:02.346 
Epoch 302/1000 
	 loss: 29.8200, MinusLogProbMetric: 29.8200, val_loss: 29.9402, val_MinusLogProbMetric: 29.9402

Epoch 302: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.8200 - MinusLogProbMetric: 29.8200 - val_loss: 29.9402 - val_MinusLogProbMetric: 29.9402 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 303/1000
2023-10-27 03:44:44.557 
Epoch 303/1000 
	 loss: 29.6292, MinusLogProbMetric: 29.6292, val_loss: 29.5049, val_MinusLogProbMetric: 29.5049

Epoch 303: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.6292 - MinusLogProbMetric: 29.6292 - val_loss: 29.5049 - val_MinusLogProbMetric: 29.5049 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 304/1000
2023-10-27 03:45:26.379 
Epoch 304/1000 
	 loss: 29.7116, MinusLogProbMetric: 29.7116, val_loss: 29.8058, val_MinusLogProbMetric: 29.8058

Epoch 304: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.7116 - MinusLogProbMetric: 29.7116 - val_loss: 29.8058 - val_MinusLogProbMetric: 29.8058 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 305/1000
2023-10-27 03:46:08.130 
Epoch 305/1000 
	 loss: 29.7409, MinusLogProbMetric: 29.7409, val_loss: 29.5794, val_MinusLogProbMetric: 29.5794

Epoch 305: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.7409 - MinusLogProbMetric: 29.7409 - val_loss: 29.5794 - val_MinusLogProbMetric: 29.5794 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 306/1000
2023-10-27 03:46:46.726 
Epoch 306/1000 
	 loss: 29.5852, MinusLogProbMetric: 29.5852, val_loss: 30.4704, val_MinusLogProbMetric: 30.4704

Epoch 306: val_loss did not improve from 29.32635
196/196 - 39s - loss: 29.5852 - MinusLogProbMetric: 29.5852 - val_loss: 30.4704 - val_MinusLogProbMetric: 30.4704 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 307/1000
2023-10-27 03:47:23.275 
Epoch 307/1000 
	 loss: 29.6366, MinusLogProbMetric: 29.6366, val_loss: 30.6287, val_MinusLogProbMetric: 30.6287

Epoch 307: val_loss did not improve from 29.32635
196/196 - 37s - loss: 29.6366 - MinusLogProbMetric: 29.6366 - val_loss: 30.6287 - val_MinusLogProbMetric: 30.6287 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 308/1000
2023-10-27 03:47:59.290 
Epoch 308/1000 
	 loss: 29.5000, MinusLogProbMetric: 29.5000, val_loss: 29.5067, val_MinusLogProbMetric: 29.5067

Epoch 308: val_loss did not improve from 29.32635
196/196 - 36s - loss: 29.5000 - MinusLogProbMetric: 29.5000 - val_loss: 29.5067 - val_MinusLogProbMetric: 29.5067 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 309/1000
2023-10-27 03:48:38.583 
Epoch 309/1000 
	 loss: 29.7844, MinusLogProbMetric: 29.7844, val_loss: 29.3662, val_MinusLogProbMetric: 29.3662

Epoch 309: val_loss did not improve from 29.32635
196/196 - 39s - loss: 29.7844 - MinusLogProbMetric: 29.7844 - val_loss: 29.3662 - val_MinusLogProbMetric: 29.3662 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 310/1000
2023-10-27 03:49:20.289 
Epoch 310/1000 
	 loss: 29.4974, MinusLogProbMetric: 29.4974, val_loss: 29.9120, val_MinusLogProbMetric: 29.9120

Epoch 310: val_loss did not improve from 29.32635
196/196 - 42s - loss: 29.4974 - MinusLogProbMetric: 29.4974 - val_loss: 29.9120 - val_MinusLogProbMetric: 29.9120 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 311/1000
2023-10-27 03:49:56.943 
Epoch 311/1000 
	 loss: 29.8041, MinusLogProbMetric: 29.8041, val_loss: 31.0655, val_MinusLogProbMetric: 31.0655

Epoch 311: val_loss did not improve from 29.32635
196/196 - 37s - loss: 29.8041 - MinusLogProbMetric: 29.8041 - val_loss: 31.0655 - val_MinusLogProbMetric: 31.0655 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 312/1000
2023-10-27 03:50:33.135 
Epoch 312/1000 
	 loss: 29.4940, MinusLogProbMetric: 29.4940, val_loss: 33.0269, val_MinusLogProbMetric: 33.0269

Epoch 312: val_loss did not improve from 29.32635
196/196 - 36s - loss: 29.4940 - MinusLogProbMetric: 29.4940 - val_loss: 33.0269 - val_MinusLogProbMetric: 33.0269 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 313/1000
2023-10-27 03:51:09.222 
Epoch 313/1000 
	 loss: 29.7375, MinusLogProbMetric: 29.7375, val_loss: 30.0031, val_MinusLogProbMetric: 30.0031

Epoch 313: val_loss did not improve from 29.32635
196/196 - 36s - loss: 29.7375 - MinusLogProbMetric: 29.7375 - val_loss: 30.0031 - val_MinusLogProbMetric: 30.0031 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 314/1000
2023-10-27 03:51:49.863 
Epoch 314/1000 
	 loss: 29.8939, MinusLogProbMetric: 29.8939, val_loss: 29.9780, val_MinusLogProbMetric: 29.9780

Epoch 314: val_loss did not improve from 29.32635
196/196 - 41s - loss: 29.8939 - MinusLogProbMetric: 29.8939 - val_loss: 29.9780 - val_MinusLogProbMetric: 29.9780 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 315/1000
2023-10-27 03:52:30.073 
Epoch 315/1000 
	 loss: 28.6752, MinusLogProbMetric: 28.6752, val_loss: 28.7230, val_MinusLogProbMetric: 28.7230

Epoch 315: val_loss improved from 29.32635 to 28.72299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 41s - loss: 28.6752 - MinusLogProbMetric: 28.6752 - val_loss: 28.7230 - val_MinusLogProbMetric: 28.7230 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 316/1000
2023-10-27 03:53:07.732 
Epoch 316/1000 
	 loss: 28.6456, MinusLogProbMetric: 28.6456, val_loss: 28.8138, val_MinusLogProbMetric: 28.8138

Epoch 316: val_loss did not improve from 28.72299
196/196 - 37s - loss: 28.6456 - MinusLogProbMetric: 28.6456 - val_loss: 28.8138 - val_MinusLogProbMetric: 28.8138 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 317/1000
2023-10-27 03:53:44.871 
Epoch 317/1000 
	 loss: 28.7223, MinusLogProbMetric: 28.7223, val_loss: 28.6631, val_MinusLogProbMetric: 28.6631

Epoch 317: val_loss improved from 28.72299 to 28.66308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 38s - loss: 28.7223 - MinusLogProbMetric: 28.7223 - val_loss: 28.6631 - val_MinusLogProbMetric: 28.6631 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 318/1000
2023-10-27 03:54:21.001 
Epoch 318/1000 
	 loss: 28.6757, MinusLogProbMetric: 28.6757, val_loss: 28.7026, val_MinusLogProbMetric: 28.7026

Epoch 318: val_loss did not improve from 28.66308
196/196 - 36s - loss: 28.6757 - MinusLogProbMetric: 28.6757 - val_loss: 28.7026 - val_MinusLogProbMetric: 28.7026 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 319/1000
2023-10-27 03:54:55.958 
Epoch 319/1000 
	 loss: 28.7838, MinusLogProbMetric: 28.7838, val_loss: 28.8340, val_MinusLogProbMetric: 28.8340

Epoch 319: val_loss did not improve from 28.66308
196/196 - 35s - loss: 28.7838 - MinusLogProbMetric: 28.7838 - val_loss: 28.8340 - val_MinusLogProbMetric: 28.8340 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 320/1000
2023-10-27 03:55:30.642 
Epoch 320/1000 
	 loss: 28.6370, MinusLogProbMetric: 28.6370, val_loss: 29.0922, val_MinusLogProbMetric: 29.0922

Epoch 320: val_loss did not improve from 28.66308
196/196 - 35s - loss: 28.6370 - MinusLogProbMetric: 28.6370 - val_loss: 29.0922 - val_MinusLogProbMetric: 29.0922 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 321/1000
2023-10-27 03:56:05.646 
Epoch 321/1000 
	 loss: 28.6785, MinusLogProbMetric: 28.6785, val_loss: 29.0527, val_MinusLogProbMetric: 29.0527

Epoch 321: val_loss did not improve from 28.66308
196/196 - 35s - loss: 28.6785 - MinusLogProbMetric: 28.6785 - val_loss: 29.0527 - val_MinusLogProbMetric: 29.0527 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 322/1000
2023-10-27 03:56:41.078 
Epoch 322/1000 
	 loss: 28.8284, MinusLogProbMetric: 28.8284, val_loss: 28.7673, val_MinusLogProbMetric: 28.7673

Epoch 322: val_loss did not improve from 28.66308
196/196 - 35s - loss: 28.8284 - MinusLogProbMetric: 28.8284 - val_loss: 28.7673 - val_MinusLogProbMetric: 28.7673 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 323/1000
2023-10-27 03:57:16.438 
Epoch 323/1000 
	 loss: 28.7664, MinusLogProbMetric: 28.7664, val_loss: 29.2699, val_MinusLogProbMetric: 29.2699

Epoch 323: val_loss did not improve from 28.66308
196/196 - 35s - loss: 28.7664 - MinusLogProbMetric: 28.7664 - val_loss: 29.2699 - val_MinusLogProbMetric: 29.2699 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 324/1000
2023-10-27 03:57:51.568 
Epoch 324/1000 
	 loss: 28.6639, MinusLogProbMetric: 28.6639, val_loss: 28.7347, val_MinusLogProbMetric: 28.7347

Epoch 324: val_loss did not improve from 28.66308
196/196 - 35s - loss: 28.6639 - MinusLogProbMetric: 28.6639 - val_loss: 28.7347 - val_MinusLogProbMetric: 28.7347 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 325/1000
2023-10-27 03:58:26.487 
Epoch 325/1000 
	 loss: 28.7474, MinusLogProbMetric: 28.7474, val_loss: 28.7647, val_MinusLogProbMetric: 28.7647

Epoch 325: val_loss did not improve from 28.66308
196/196 - 35s - loss: 28.7474 - MinusLogProbMetric: 28.7474 - val_loss: 28.7647 - val_MinusLogProbMetric: 28.7647 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 326/1000
2023-10-27 03:59:01.674 
Epoch 326/1000 
	 loss: 28.7167, MinusLogProbMetric: 28.7167, val_loss: 28.5828, val_MinusLogProbMetric: 28.5828

Epoch 326: val_loss improved from 28.66308 to 28.58281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 36s - loss: 28.7167 - MinusLogProbMetric: 28.7167 - val_loss: 28.5828 - val_MinusLogProbMetric: 28.5828 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 327/1000
2023-10-27 03:59:37.960 
Epoch 327/1000 
	 loss: 28.7654, MinusLogProbMetric: 28.7654, val_loss: 28.7443, val_MinusLogProbMetric: 28.7443

Epoch 327: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.7654 - MinusLogProbMetric: 28.7654 - val_loss: 28.7443 - val_MinusLogProbMetric: 28.7443 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 328/1000
2023-10-27 04:00:13.993 
Epoch 328/1000 
	 loss: 28.8078, MinusLogProbMetric: 28.8078, val_loss: 28.7650, val_MinusLogProbMetric: 28.7650

Epoch 328: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.8078 - MinusLogProbMetric: 28.8078 - val_loss: 28.7650 - val_MinusLogProbMetric: 28.7650 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 329/1000
2023-10-27 04:00:49.345 
Epoch 329/1000 
	 loss: 28.5895, MinusLogProbMetric: 28.5895, val_loss: 28.8869, val_MinusLogProbMetric: 28.8869

Epoch 329: val_loss did not improve from 28.58281
196/196 - 35s - loss: 28.5895 - MinusLogProbMetric: 28.5895 - val_loss: 28.8869 - val_MinusLogProbMetric: 28.8869 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 330/1000
2023-10-27 04:01:24.245 
Epoch 330/1000 
	 loss: 28.7457, MinusLogProbMetric: 28.7457, val_loss: 28.7915, val_MinusLogProbMetric: 28.7915

Epoch 330: val_loss did not improve from 28.58281
196/196 - 35s - loss: 28.7457 - MinusLogProbMetric: 28.7457 - val_loss: 28.7915 - val_MinusLogProbMetric: 28.7915 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 331/1000
2023-10-27 04:01:58.977 
Epoch 331/1000 
	 loss: 28.6847, MinusLogProbMetric: 28.6847, val_loss: 28.8946, val_MinusLogProbMetric: 28.8946

Epoch 331: val_loss did not improve from 28.58281
196/196 - 35s - loss: 28.6847 - MinusLogProbMetric: 28.6847 - val_loss: 28.8946 - val_MinusLogProbMetric: 28.8946 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 332/1000
2023-10-27 04:02:34.465 
Epoch 332/1000 
	 loss: 28.8374, MinusLogProbMetric: 28.8374, val_loss: 28.6976, val_MinusLogProbMetric: 28.6976

Epoch 332: val_loss did not improve from 28.58281
196/196 - 35s - loss: 28.8374 - MinusLogProbMetric: 28.8374 - val_loss: 28.6976 - val_MinusLogProbMetric: 28.6976 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 333/1000
2023-10-27 04:03:10.638 
Epoch 333/1000 
	 loss: 28.5741, MinusLogProbMetric: 28.5741, val_loss: 28.5859, val_MinusLogProbMetric: 28.5859

Epoch 333: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.5741 - MinusLogProbMetric: 28.5741 - val_loss: 28.5859 - val_MinusLogProbMetric: 28.5859 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 334/1000
2023-10-27 04:03:48.246 
Epoch 334/1000 
	 loss: 28.6383, MinusLogProbMetric: 28.6383, val_loss: 28.7428, val_MinusLogProbMetric: 28.7428

Epoch 334: val_loss did not improve from 28.58281
196/196 - 38s - loss: 28.6383 - MinusLogProbMetric: 28.6383 - val_loss: 28.7428 - val_MinusLogProbMetric: 28.7428 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 335/1000
2023-10-27 04:04:24.105 
Epoch 335/1000 
	 loss: 28.7056, MinusLogProbMetric: 28.7056, val_loss: 28.7129, val_MinusLogProbMetric: 28.7129

Epoch 335: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.7056 - MinusLogProbMetric: 28.7056 - val_loss: 28.7129 - val_MinusLogProbMetric: 28.7129 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 336/1000
2023-10-27 04:04:59.847 
Epoch 336/1000 
	 loss: 28.6714, MinusLogProbMetric: 28.6714, val_loss: 28.7566, val_MinusLogProbMetric: 28.7566

Epoch 336: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.6714 - MinusLogProbMetric: 28.6714 - val_loss: 28.7566 - val_MinusLogProbMetric: 28.7566 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 337/1000
2023-10-27 04:05:35.732 
Epoch 337/1000 
	 loss: 28.6510, MinusLogProbMetric: 28.6510, val_loss: 28.9008, val_MinusLogProbMetric: 28.9008

Epoch 337: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.6510 - MinusLogProbMetric: 28.6510 - val_loss: 28.9008 - val_MinusLogProbMetric: 28.9008 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 338/1000
2023-10-27 04:06:13.723 
Epoch 338/1000 
	 loss: 28.8965, MinusLogProbMetric: 28.8965, val_loss: 28.7791, val_MinusLogProbMetric: 28.7791

Epoch 338: val_loss did not improve from 28.58281
196/196 - 38s - loss: 28.8965 - MinusLogProbMetric: 28.8965 - val_loss: 28.7791 - val_MinusLogProbMetric: 28.7791 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 339/1000
2023-10-27 04:06:49.738 
Epoch 339/1000 
	 loss: 28.6184, MinusLogProbMetric: 28.6184, val_loss: 29.7547, val_MinusLogProbMetric: 29.7547

Epoch 339: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.6184 - MinusLogProbMetric: 28.6184 - val_loss: 29.7547 - val_MinusLogProbMetric: 29.7547 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 340/1000
2023-10-27 04:07:26.408 
Epoch 340/1000 
	 loss: 28.6424, MinusLogProbMetric: 28.6424, val_loss: 28.6356, val_MinusLogProbMetric: 28.6356

Epoch 340: val_loss did not improve from 28.58281
196/196 - 37s - loss: 28.6424 - MinusLogProbMetric: 28.6424 - val_loss: 28.6356 - val_MinusLogProbMetric: 28.6356 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 341/1000
2023-10-27 04:08:02.261 
Epoch 341/1000 
	 loss: 28.5777, MinusLogProbMetric: 28.5777, val_loss: 28.7062, val_MinusLogProbMetric: 28.7062

Epoch 341: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.5777 - MinusLogProbMetric: 28.5777 - val_loss: 28.7062 - val_MinusLogProbMetric: 28.7062 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 342/1000
2023-10-27 04:08:38.188 
Epoch 342/1000 
	 loss: 28.7444, MinusLogProbMetric: 28.7444, val_loss: 28.7902, val_MinusLogProbMetric: 28.7902

Epoch 342: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.7444 - MinusLogProbMetric: 28.7444 - val_loss: 28.7902 - val_MinusLogProbMetric: 28.7902 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 343/1000
2023-10-27 04:09:14.975 
Epoch 343/1000 
	 loss: 28.7744, MinusLogProbMetric: 28.7744, val_loss: 28.6338, val_MinusLogProbMetric: 28.6338

Epoch 343: val_loss did not improve from 28.58281
196/196 - 37s - loss: 28.7744 - MinusLogProbMetric: 28.7744 - val_loss: 28.6338 - val_MinusLogProbMetric: 28.6338 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 344/1000
2023-10-27 04:09:55.662 
Epoch 344/1000 
	 loss: 28.7662, MinusLogProbMetric: 28.7662, val_loss: 28.7231, val_MinusLogProbMetric: 28.7231

Epoch 344: val_loss did not improve from 28.58281
196/196 - 41s - loss: 28.7662 - MinusLogProbMetric: 28.7662 - val_loss: 28.7231 - val_MinusLogProbMetric: 28.7231 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 345/1000
2023-10-27 04:10:31.604 
Epoch 345/1000 
	 loss: 28.5977, MinusLogProbMetric: 28.5977, val_loss: 28.9762, val_MinusLogProbMetric: 28.9762

Epoch 345: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.5977 - MinusLogProbMetric: 28.5977 - val_loss: 28.9762 - val_MinusLogProbMetric: 28.9762 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 346/1000
2023-10-27 04:11:07.690 
Epoch 346/1000 
	 loss: 28.6399, MinusLogProbMetric: 28.6399, val_loss: 28.9359, val_MinusLogProbMetric: 28.9359

Epoch 346: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.6399 - MinusLogProbMetric: 28.6399 - val_loss: 28.9359 - val_MinusLogProbMetric: 28.9359 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 347/1000
2023-10-27 04:11:43.976 
Epoch 347/1000 
	 loss: 28.6439, MinusLogProbMetric: 28.6439, val_loss: 28.8951, val_MinusLogProbMetric: 28.8951

Epoch 347: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.6439 - MinusLogProbMetric: 28.6439 - val_loss: 28.8951 - val_MinusLogProbMetric: 28.8951 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 348/1000
2023-10-27 04:12:25.411 
Epoch 348/1000 
	 loss: 28.8077, MinusLogProbMetric: 28.8077, val_loss: 29.0544, val_MinusLogProbMetric: 29.0544

Epoch 348: val_loss did not improve from 28.58281
196/196 - 41s - loss: 28.8077 - MinusLogProbMetric: 28.8077 - val_loss: 29.0544 - val_MinusLogProbMetric: 29.0544 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 349/1000
2023-10-27 04:13:03.221 
Epoch 349/1000 
	 loss: 28.6100, MinusLogProbMetric: 28.6100, val_loss: 29.0640, val_MinusLogProbMetric: 29.0640

Epoch 349: val_loss did not improve from 28.58281
196/196 - 38s - loss: 28.6100 - MinusLogProbMetric: 28.6100 - val_loss: 29.0640 - val_MinusLogProbMetric: 29.0640 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 350/1000
2023-10-27 04:13:39.576 
Epoch 350/1000 
	 loss: 28.6935, MinusLogProbMetric: 28.6935, val_loss: 28.7308, val_MinusLogProbMetric: 28.7308

Epoch 350: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.6935 - MinusLogProbMetric: 28.6935 - val_loss: 28.7308 - val_MinusLogProbMetric: 28.7308 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 351/1000
2023-10-27 04:14:14.415 
Epoch 351/1000 
	 loss: 28.7353, MinusLogProbMetric: 28.7353, val_loss: 28.6433, val_MinusLogProbMetric: 28.6433

Epoch 351: val_loss did not improve from 28.58281
196/196 - 35s - loss: 28.7353 - MinusLogProbMetric: 28.7353 - val_loss: 28.6433 - val_MinusLogProbMetric: 28.6433 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 352/1000
2023-10-27 04:14:50.734 
Epoch 352/1000 
	 loss: 28.5518, MinusLogProbMetric: 28.5518, val_loss: 30.9503, val_MinusLogProbMetric: 30.9503

Epoch 352: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.5518 - MinusLogProbMetric: 28.5518 - val_loss: 30.9503 - val_MinusLogProbMetric: 30.9503 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 353/1000
2023-10-27 04:15:31.209 
Epoch 353/1000 
	 loss: 28.7530, MinusLogProbMetric: 28.7530, val_loss: 28.5920, val_MinusLogProbMetric: 28.5920

Epoch 353: val_loss did not improve from 28.58281
196/196 - 40s - loss: 28.7530 - MinusLogProbMetric: 28.7530 - val_loss: 28.5920 - val_MinusLogProbMetric: 28.5920 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 354/1000
2023-10-27 04:16:09.031 
Epoch 354/1000 
	 loss: 28.5609, MinusLogProbMetric: 28.5609, val_loss: 28.6380, val_MinusLogProbMetric: 28.6380

Epoch 354: val_loss did not improve from 28.58281
196/196 - 38s - loss: 28.5609 - MinusLogProbMetric: 28.5609 - val_loss: 28.6380 - val_MinusLogProbMetric: 28.6380 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 355/1000
2023-10-27 04:16:45.860 
Epoch 355/1000 
	 loss: 28.7522, MinusLogProbMetric: 28.7522, val_loss: 28.7691, val_MinusLogProbMetric: 28.7691

Epoch 355: val_loss did not improve from 28.58281
196/196 - 37s - loss: 28.7522 - MinusLogProbMetric: 28.7522 - val_loss: 28.7691 - val_MinusLogProbMetric: 28.7691 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 356/1000
2023-10-27 04:17:22.303 
Epoch 356/1000 
	 loss: 28.7842, MinusLogProbMetric: 28.7842, val_loss: 28.6951, val_MinusLogProbMetric: 28.6951

Epoch 356: val_loss did not improve from 28.58281
196/196 - 36s - loss: 28.7842 - MinusLogProbMetric: 28.7842 - val_loss: 28.6951 - val_MinusLogProbMetric: 28.6951 - lr: 5.0000e-04 - 36s/epoch - 186ms/step
Epoch 357/1000
2023-10-27 04:18:00.101 
Epoch 357/1000 
	 loss: 28.5203, MinusLogProbMetric: 28.5203, val_loss: 28.5429, val_MinusLogProbMetric: 28.5429

Epoch 357: val_loss improved from 28.58281 to 28.54287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 39s - loss: 28.5203 - MinusLogProbMetric: 28.5203 - val_loss: 28.5429 - val_MinusLogProbMetric: 28.5429 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 358/1000
2023-10-27 04:18:42.043 
Epoch 358/1000 
	 loss: 28.6234, MinusLogProbMetric: 28.6234, val_loss: 28.5854, val_MinusLogProbMetric: 28.5854

Epoch 358: val_loss did not improve from 28.54287
196/196 - 41s - loss: 28.6234 - MinusLogProbMetric: 28.6234 - val_loss: 28.5854 - val_MinusLogProbMetric: 28.5854 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 359/1000
2023-10-27 04:19:20.315 
Epoch 359/1000 
	 loss: 28.5734, MinusLogProbMetric: 28.5734, val_loss: 28.7573, val_MinusLogProbMetric: 28.7573

Epoch 359: val_loss did not improve from 28.54287
196/196 - 38s - loss: 28.5734 - MinusLogProbMetric: 28.5734 - val_loss: 28.7573 - val_MinusLogProbMetric: 28.7573 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 360/1000
2023-10-27 04:19:57.575 
Epoch 360/1000 
	 loss: 28.5432, MinusLogProbMetric: 28.5432, val_loss: 28.5099, val_MinusLogProbMetric: 28.5099

Epoch 360: val_loss improved from 28.54287 to 28.50994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 38s - loss: 28.5432 - MinusLogProbMetric: 28.5432 - val_loss: 28.5099 - val_MinusLogProbMetric: 28.5099 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 361/1000
2023-10-27 04:20:37.248 
Epoch 361/1000 
	 loss: 28.8094, MinusLogProbMetric: 28.8094, val_loss: 28.5318, val_MinusLogProbMetric: 28.5318

Epoch 361: val_loss did not improve from 28.50994
196/196 - 39s - loss: 28.8094 - MinusLogProbMetric: 28.8094 - val_loss: 28.5318 - val_MinusLogProbMetric: 28.5318 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 362/1000
2023-10-27 04:21:13.153 
Epoch 362/1000 
	 loss: 28.5602, MinusLogProbMetric: 28.5602, val_loss: 28.8150, val_MinusLogProbMetric: 28.8150

Epoch 362: val_loss did not improve from 28.50994
196/196 - 36s - loss: 28.5602 - MinusLogProbMetric: 28.5602 - val_loss: 28.8150 - val_MinusLogProbMetric: 28.8150 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 363/1000
2023-10-27 04:21:54.622 
Epoch 363/1000 
	 loss: 28.5510, MinusLogProbMetric: 28.5510, val_loss: 28.5252, val_MinusLogProbMetric: 28.5252

Epoch 363: val_loss did not improve from 28.50994
196/196 - 41s - loss: 28.5510 - MinusLogProbMetric: 28.5510 - val_loss: 28.5252 - val_MinusLogProbMetric: 28.5252 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 364/1000
2023-10-27 04:22:31.671 
Epoch 364/1000 
	 loss: 28.6050, MinusLogProbMetric: 28.6050, val_loss: 28.6934, val_MinusLogProbMetric: 28.6934

Epoch 364: val_loss did not improve from 28.50994
196/196 - 37s - loss: 28.6050 - MinusLogProbMetric: 28.6050 - val_loss: 28.6934 - val_MinusLogProbMetric: 28.6934 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 365/1000
2023-10-27 04:23:10.967 
Epoch 365/1000 
	 loss: 28.7152, MinusLogProbMetric: 28.7152, val_loss: 28.6643, val_MinusLogProbMetric: 28.6643

Epoch 365: val_loss did not improve from 28.50994
196/196 - 39s - loss: 28.7152 - MinusLogProbMetric: 28.7152 - val_loss: 28.6643 - val_MinusLogProbMetric: 28.6643 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 366/1000
2023-10-27 04:23:43.809 
Epoch 366/1000 
	 loss: 28.5096, MinusLogProbMetric: 28.5096, val_loss: 28.9524, val_MinusLogProbMetric: 28.9524

Epoch 366: val_loss did not improve from 28.50994
196/196 - 33s - loss: 28.5096 - MinusLogProbMetric: 28.5096 - val_loss: 28.9524 - val_MinusLogProbMetric: 28.9524 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 367/1000
2023-10-27 04:24:16.683 
Epoch 367/1000 
	 loss: 28.5930, MinusLogProbMetric: 28.5930, val_loss: 28.6036, val_MinusLogProbMetric: 28.6036

Epoch 367: val_loss did not improve from 28.50994
196/196 - 33s - loss: 28.5930 - MinusLogProbMetric: 28.5930 - val_loss: 28.6036 - val_MinusLogProbMetric: 28.6036 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 368/1000
2023-10-27 04:24:49.741 
Epoch 368/1000 
	 loss: 28.7210, MinusLogProbMetric: 28.7210, val_loss: 28.6491, val_MinusLogProbMetric: 28.6491

Epoch 368: val_loss did not improve from 28.50994
196/196 - 33s - loss: 28.7210 - MinusLogProbMetric: 28.7210 - val_loss: 28.6491 - val_MinusLogProbMetric: 28.6491 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 369/1000
2023-10-27 04:25:22.899 
Epoch 369/1000 
	 loss: 28.7338, MinusLogProbMetric: 28.7338, val_loss: 28.5556, val_MinusLogProbMetric: 28.5556

Epoch 369: val_loss did not improve from 28.50994
196/196 - 33s - loss: 28.7338 - MinusLogProbMetric: 28.7338 - val_loss: 28.5556 - val_MinusLogProbMetric: 28.5556 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 370/1000
2023-10-27 04:26:01.622 
Epoch 370/1000 
	 loss: 28.5403, MinusLogProbMetric: 28.5403, val_loss: 28.6094, val_MinusLogProbMetric: 28.6094

Epoch 370: val_loss did not improve from 28.50994
196/196 - 39s - loss: 28.5403 - MinusLogProbMetric: 28.5403 - val_loss: 28.6094 - val_MinusLogProbMetric: 28.6094 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 371/1000
2023-10-27 04:26:41.205 
Epoch 371/1000 
	 loss: 28.6625, MinusLogProbMetric: 28.6625, val_loss: 28.8682, val_MinusLogProbMetric: 28.8682

Epoch 371: val_loss did not improve from 28.50994
196/196 - 40s - loss: 28.6625 - MinusLogProbMetric: 28.6625 - val_loss: 28.8682 - val_MinusLogProbMetric: 28.8682 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 372/1000
2023-10-27 04:27:19.194 
Epoch 372/1000 
	 loss: 28.7845, MinusLogProbMetric: 28.7845, val_loss: 29.3117, val_MinusLogProbMetric: 29.3117

Epoch 372: val_loss did not improve from 28.50994
196/196 - 38s - loss: 28.7845 - MinusLogProbMetric: 28.7845 - val_loss: 29.3117 - val_MinusLogProbMetric: 29.3117 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 373/1000
2023-10-27 04:27:59.385 
Epoch 373/1000 
	 loss: 28.5585, MinusLogProbMetric: 28.5585, val_loss: 28.6760, val_MinusLogProbMetric: 28.6760

Epoch 373: val_loss did not improve from 28.50994
196/196 - 40s - loss: 28.5585 - MinusLogProbMetric: 28.5585 - val_loss: 28.6760 - val_MinusLogProbMetric: 28.6760 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 374/1000
2023-10-27 04:28:36.035 
Epoch 374/1000 
	 loss: 28.6238, MinusLogProbMetric: 28.6238, val_loss: 28.6887, val_MinusLogProbMetric: 28.6887

Epoch 374: val_loss did not improve from 28.50994
196/196 - 37s - loss: 28.6238 - MinusLogProbMetric: 28.6238 - val_loss: 28.6887 - val_MinusLogProbMetric: 28.6887 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 375/1000
2023-10-27 04:29:16.593 
Epoch 375/1000 
	 loss: 28.5946, MinusLogProbMetric: 28.5946, val_loss: 28.7315, val_MinusLogProbMetric: 28.7315

Epoch 375: val_loss did not improve from 28.50994
196/196 - 41s - loss: 28.5946 - MinusLogProbMetric: 28.5946 - val_loss: 28.7315 - val_MinusLogProbMetric: 28.7315 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 376/1000
2023-10-27 04:29:54.277 
Epoch 376/1000 
	 loss: 28.5627, MinusLogProbMetric: 28.5627, val_loss: 29.0093, val_MinusLogProbMetric: 29.0093

Epoch 376: val_loss did not improve from 28.50994
196/196 - 38s - loss: 28.5627 - MinusLogProbMetric: 28.5627 - val_loss: 29.0093 - val_MinusLogProbMetric: 29.0093 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 377/1000
2023-10-27 04:30:32.337 
Epoch 377/1000 
	 loss: 28.7766, MinusLogProbMetric: 28.7766, val_loss: 29.4406, val_MinusLogProbMetric: 29.4406

Epoch 377: val_loss did not improve from 28.50994
196/196 - 38s - loss: 28.7766 - MinusLogProbMetric: 28.7766 - val_loss: 29.4406 - val_MinusLogProbMetric: 29.4406 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 378/1000
2023-10-27 04:31:11.200 
Epoch 378/1000 
	 loss: 28.5981, MinusLogProbMetric: 28.5981, val_loss: 29.0674, val_MinusLogProbMetric: 29.0674

Epoch 378: val_loss did not improve from 28.50994
196/196 - 39s - loss: 28.5981 - MinusLogProbMetric: 28.5981 - val_loss: 29.0674 - val_MinusLogProbMetric: 29.0674 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 379/1000
2023-10-27 04:31:49.442 
Epoch 379/1000 
	 loss: 28.5553, MinusLogProbMetric: 28.5553, val_loss: 28.6312, val_MinusLogProbMetric: 28.6312

Epoch 379: val_loss did not improve from 28.50994
196/196 - 38s - loss: 28.5553 - MinusLogProbMetric: 28.5553 - val_loss: 28.6312 - val_MinusLogProbMetric: 28.6312 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 380/1000
2023-10-27 04:32:30.360 
Epoch 380/1000 
	 loss: 28.8130, MinusLogProbMetric: 28.8130, val_loss: 28.6058, val_MinusLogProbMetric: 28.6058

Epoch 380: val_loss did not improve from 28.50994
196/196 - 41s - loss: 28.8130 - MinusLogProbMetric: 28.8130 - val_loss: 28.6058 - val_MinusLogProbMetric: 28.6058 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 381/1000
2023-10-27 04:33:07.840 
Epoch 381/1000 
	 loss: 28.5821, MinusLogProbMetric: 28.5821, val_loss: 28.5498, val_MinusLogProbMetric: 28.5498

Epoch 381: val_loss did not improve from 28.50994
196/196 - 37s - loss: 28.5821 - MinusLogProbMetric: 28.5821 - val_loss: 28.5498 - val_MinusLogProbMetric: 28.5498 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 382/1000
2023-10-27 04:33:46.114 
Epoch 382/1000 
	 loss: 28.5580, MinusLogProbMetric: 28.5580, val_loss: 31.5076, val_MinusLogProbMetric: 31.5076

Epoch 382: val_loss did not improve from 28.50994
196/196 - 38s - loss: 28.5580 - MinusLogProbMetric: 28.5580 - val_loss: 31.5076 - val_MinusLogProbMetric: 31.5076 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 383/1000
2023-10-27 04:34:22.804 
Epoch 383/1000 
	 loss: 28.5799, MinusLogProbMetric: 28.5799, val_loss: 29.2889, val_MinusLogProbMetric: 29.2889

Epoch 383: val_loss did not improve from 28.50994
196/196 - 37s - loss: 28.5799 - MinusLogProbMetric: 28.5799 - val_loss: 29.2889 - val_MinusLogProbMetric: 29.2889 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 384/1000
2023-10-27 04:35:00.593 
Epoch 384/1000 
	 loss: 28.6399, MinusLogProbMetric: 28.6399, val_loss: 28.5787, val_MinusLogProbMetric: 28.5787

Epoch 384: val_loss did not improve from 28.50994
196/196 - 38s - loss: 28.6399 - MinusLogProbMetric: 28.6399 - val_loss: 28.5787 - val_MinusLogProbMetric: 28.5787 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 385/1000
2023-10-27 04:35:39.577 
Epoch 385/1000 
	 loss: 28.4856, MinusLogProbMetric: 28.4856, val_loss: 28.9503, val_MinusLogProbMetric: 28.9503

Epoch 385: val_loss did not improve from 28.50994
196/196 - 39s - loss: 28.4856 - MinusLogProbMetric: 28.4856 - val_loss: 28.9503 - val_MinusLogProbMetric: 28.9503 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 386/1000
2023-10-27 04:36:16.901 
Epoch 386/1000 
	 loss: 28.7266, MinusLogProbMetric: 28.7266, val_loss: 28.6698, val_MinusLogProbMetric: 28.6698

Epoch 386: val_loss did not improve from 28.50994
196/196 - 37s - loss: 28.7266 - MinusLogProbMetric: 28.7266 - val_loss: 28.6698 - val_MinusLogProbMetric: 28.6698 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 387/1000
2023-10-27 04:36:56.789 
Epoch 387/1000 
	 loss: 28.6138, MinusLogProbMetric: 28.6138, val_loss: 28.9797, val_MinusLogProbMetric: 28.9797

Epoch 387: val_loss did not improve from 28.50994
196/196 - 40s - loss: 28.6138 - MinusLogProbMetric: 28.6138 - val_loss: 28.9797 - val_MinusLogProbMetric: 28.9797 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 388/1000
2023-10-27 04:37:32.888 
Epoch 388/1000 
	 loss: 28.6060, MinusLogProbMetric: 28.6060, val_loss: 28.7113, val_MinusLogProbMetric: 28.7113

Epoch 388: val_loss did not improve from 28.50994
196/196 - 36s - loss: 28.6060 - MinusLogProbMetric: 28.6060 - val_loss: 28.7113 - val_MinusLogProbMetric: 28.7113 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 389/1000
2023-10-27 04:38:12.515 
Epoch 389/1000 
	 loss: 28.7746, MinusLogProbMetric: 28.7746, val_loss: 28.4839, val_MinusLogProbMetric: 28.4839

Epoch 389: val_loss improved from 28.50994 to 28.48395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 40s - loss: 28.7746 - MinusLogProbMetric: 28.7746 - val_loss: 28.4839 - val_MinusLogProbMetric: 28.4839 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 390/1000
2023-10-27 04:38:55.096 
Epoch 390/1000 
	 loss: 28.5454, MinusLogProbMetric: 28.5454, val_loss: 28.6029, val_MinusLogProbMetric: 28.6029

Epoch 390: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.5454 - MinusLogProbMetric: 28.5454 - val_loss: 28.6029 - val_MinusLogProbMetric: 28.6029 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 391/1000
2023-10-27 04:39:33.678 
Epoch 391/1000 
	 loss: 28.4664, MinusLogProbMetric: 28.4664, val_loss: 31.8857, val_MinusLogProbMetric: 31.8857

Epoch 391: val_loss did not improve from 28.48395
196/196 - 39s - loss: 28.4664 - MinusLogProbMetric: 28.4664 - val_loss: 31.8857 - val_MinusLogProbMetric: 31.8857 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 392/1000
2023-10-27 04:40:12.303 
Epoch 392/1000 
	 loss: 28.7183, MinusLogProbMetric: 28.7183, val_loss: 28.7897, val_MinusLogProbMetric: 28.7897

Epoch 392: val_loss did not improve from 28.48395
196/196 - 39s - loss: 28.7183 - MinusLogProbMetric: 28.7183 - val_loss: 28.7897 - val_MinusLogProbMetric: 28.7897 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 393/1000
2023-10-27 04:40:54.527 
Epoch 393/1000 
	 loss: 28.5264, MinusLogProbMetric: 28.5264, val_loss: 28.5581, val_MinusLogProbMetric: 28.5581

Epoch 393: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.5264 - MinusLogProbMetric: 28.5264 - val_loss: 28.5581 - val_MinusLogProbMetric: 28.5581 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 394/1000
2023-10-27 04:41:35.693 
Epoch 394/1000 
	 loss: 28.6436, MinusLogProbMetric: 28.6436, val_loss: 28.7031, val_MinusLogProbMetric: 28.7031

Epoch 394: val_loss did not improve from 28.48395
196/196 - 41s - loss: 28.6436 - MinusLogProbMetric: 28.6436 - val_loss: 28.7031 - val_MinusLogProbMetric: 28.7031 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 395/1000
2023-10-27 04:42:18.157 
Epoch 395/1000 
	 loss: 28.4818, MinusLogProbMetric: 28.4818, val_loss: 28.7955, val_MinusLogProbMetric: 28.7955

Epoch 395: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.4818 - MinusLogProbMetric: 28.4818 - val_loss: 28.7955 - val_MinusLogProbMetric: 28.7955 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 396/1000
2023-10-27 04:42:59.617 
Epoch 396/1000 
	 loss: 28.6229, MinusLogProbMetric: 28.6229, val_loss: 28.7945, val_MinusLogProbMetric: 28.7945

Epoch 396: val_loss did not improve from 28.48395
196/196 - 41s - loss: 28.6229 - MinusLogProbMetric: 28.6229 - val_loss: 28.7945 - val_MinusLogProbMetric: 28.7945 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 397/1000
2023-10-27 04:43:42.094 
Epoch 397/1000 
	 loss: 28.7317, MinusLogProbMetric: 28.7317, val_loss: 28.9225, val_MinusLogProbMetric: 28.9225

Epoch 397: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.7317 - MinusLogProbMetric: 28.7317 - val_loss: 28.9225 - val_MinusLogProbMetric: 28.9225 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 398/1000
2023-10-27 04:44:23.885 
Epoch 398/1000 
	 loss: 28.5117, MinusLogProbMetric: 28.5117, val_loss: 28.7274, val_MinusLogProbMetric: 28.7274

Epoch 398: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.5117 - MinusLogProbMetric: 28.5117 - val_loss: 28.7274 - val_MinusLogProbMetric: 28.7274 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 399/1000
2023-10-27 04:45:05.906 
Epoch 399/1000 
	 loss: 28.7774, MinusLogProbMetric: 28.7774, val_loss: 28.6962, val_MinusLogProbMetric: 28.6962

Epoch 399: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.7774 - MinusLogProbMetric: 28.7774 - val_loss: 28.6962 - val_MinusLogProbMetric: 28.6962 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 400/1000
2023-10-27 04:45:47.802 
Epoch 400/1000 
	 loss: 28.5660, MinusLogProbMetric: 28.5660, val_loss: 28.8072, val_MinusLogProbMetric: 28.8072

Epoch 400: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.5660 - MinusLogProbMetric: 28.5660 - val_loss: 28.8072 - val_MinusLogProbMetric: 28.8072 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 401/1000
2023-10-27 04:46:30.084 
Epoch 401/1000 
	 loss: 28.4975, MinusLogProbMetric: 28.4975, val_loss: 28.6893, val_MinusLogProbMetric: 28.6893

Epoch 401: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.4975 - MinusLogProbMetric: 28.4975 - val_loss: 28.6893 - val_MinusLogProbMetric: 28.6893 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 402/1000
2023-10-27 04:47:11.083 
Epoch 402/1000 
	 loss: 28.7343, MinusLogProbMetric: 28.7343, val_loss: 28.7291, val_MinusLogProbMetric: 28.7291

Epoch 402: val_loss did not improve from 28.48395
196/196 - 41s - loss: 28.7343 - MinusLogProbMetric: 28.7343 - val_loss: 28.7291 - val_MinusLogProbMetric: 28.7291 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 403/1000
2023-10-27 04:47:53.585 
Epoch 403/1000 
	 loss: 28.4981, MinusLogProbMetric: 28.4981, val_loss: 28.9290, val_MinusLogProbMetric: 28.9290

Epoch 403: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.4981 - MinusLogProbMetric: 28.4981 - val_loss: 28.9290 - val_MinusLogProbMetric: 28.9290 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 404/1000
2023-10-27 04:48:34.084 
Epoch 404/1000 
	 loss: 28.5141, MinusLogProbMetric: 28.5141, val_loss: 28.7580, val_MinusLogProbMetric: 28.7580

Epoch 404: val_loss did not improve from 28.48395
196/196 - 40s - loss: 28.5141 - MinusLogProbMetric: 28.5141 - val_loss: 28.7580 - val_MinusLogProbMetric: 28.7580 - lr: 5.0000e-04 - 40s/epoch - 207ms/step
Epoch 405/1000
2023-10-27 04:49:15.561 
Epoch 405/1000 
	 loss: 28.5626, MinusLogProbMetric: 28.5626, val_loss: 28.5967, val_MinusLogProbMetric: 28.5967

Epoch 405: val_loss did not improve from 28.48395
196/196 - 41s - loss: 28.5626 - MinusLogProbMetric: 28.5626 - val_loss: 28.5967 - val_MinusLogProbMetric: 28.5967 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 406/1000
2023-10-27 04:49:57.118 
Epoch 406/1000 
	 loss: 28.6110, MinusLogProbMetric: 28.6110, val_loss: 28.6038, val_MinusLogProbMetric: 28.6038

Epoch 406: val_loss did not improve from 28.48395
196/196 - 42s - loss: 28.6110 - MinusLogProbMetric: 28.6110 - val_loss: 28.6038 - val_MinusLogProbMetric: 28.6038 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 407/1000
2023-10-27 04:50:39.035 
Epoch 407/1000 
	 loss: 28.5231, MinusLogProbMetric: 28.5231, val_loss: 28.4540, val_MinusLogProbMetric: 28.4540

Epoch 407: val_loss improved from 28.48395 to 28.45398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.5231 - MinusLogProbMetric: 28.5231 - val_loss: 28.4540 - val_MinusLogProbMetric: 28.4540 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 408/1000
2023-10-27 04:51:21.215 
Epoch 408/1000 
	 loss: 28.5584, MinusLogProbMetric: 28.5584, val_loss: 29.0152, val_MinusLogProbMetric: 29.0152

Epoch 408: val_loss did not improve from 28.45398
196/196 - 41s - loss: 28.5584 - MinusLogProbMetric: 28.5584 - val_loss: 29.0152 - val_MinusLogProbMetric: 29.0152 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 409/1000
2023-10-27 04:52:01.898 
Epoch 409/1000 
	 loss: 28.5266, MinusLogProbMetric: 28.5266, val_loss: 28.4956, val_MinusLogProbMetric: 28.4956

Epoch 409: val_loss did not improve from 28.45398
196/196 - 41s - loss: 28.5266 - MinusLogProbMetric: 28.5266 - val_loss: 28.4956 - val_MinusLogProbMetric: 28.4956 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 410/1000
2023-10-27 04:52:43.886 
Epoch 410/1000 
	 loss: 28.6020, MinusLogProbMetric: 28.6020, val_loss: 28.7755, val_MinusLogProbMetric: 28.7755

Epoch 410: val_loss did not improve from 28.45398
196/196 - 42s - loss: 28.6020 - MinusLogProbMetric: 28.6020 - val_loss: 28.7755 - val_MinusLogProbMetric: 28.7755 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 411/1000
2023-10-27 04:53:25.580 
Epoch 411/1000 
	 loss: 28.5977, MinusLogProbMetric: 28.5977, val_loss: 28.7816, val_MinusLogProbMetric: 28.7816

Epoch 411: val_loss did not improve from 28.45398
196/196 - 42s - loss: 28.5977 - MinusLogProbMetric: 28.5977 - val_loss: 28.7816 - val_MinusLogProbMetric: 28.7816 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 412/1000
2023-10-27 04:54:07.307 
Epoch 412/1000 
	 loss: 28.5396, MinusLogProbMetric: 28.5396, val_loss: 28.4960, val_MinusLogProbMetric: 28.4960

Epoch 412: val_loss did not improve from 28.45398
196/196 - 42s - loss: 28.5396 - MinusLogProbMetric: 28.5396 - val_loss: 28.4960 - val_MinusLogProbMetric: 28.4960 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 413/1000
2023-10-27 04:54:49.890 
Epoch 413/1000 
	 loss: 28.5792, MinusLogProbMetric: 28.5792, val_loss: 28.7443, val_MinusLogProbMetric: 28.7443

Epoch 413: val_loss did not improve from 28.45398
196/196 - 43s - loss: 28.5792 - MinusLogProbMetric: 28.5792 - val_loss: 28.7443 - val_MinusLogProbMetric: 28.7443 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 414/1000
2023-10-27 04:55:32.872 
Epoch 414/1000 
	 loss: 28.4841, MinusLogProbMetric: 28.4841, val_loss: 28.8020, val_MinusLogProbMetric: 28.8020

Epoch 414: val_loss did not improve from 28.45398
196/196 - 43s - loss: 28.4841 - MinusLogProbMetric: 28.4841 - val_loss: 28.8020 - val_MinusLogProbMetric: 28.8020 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 415/1000
2023-10-27 04:56:15.235 
Epoch 415/1000 
	 loss: 28.8429, MinusLogProbMetric: 28.8429, val_loss: 28.7677, val_MinusLogProbMetric: 28.7677

Epoch 415: val_loss did not improve from 28.45398
196/196 - 42s - loss: 28.8429 - MinusLogProbMetric: 28.8429 - val_loss: 28.7677 - val_MinusLogProbMetric: 28.7677 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 416/1000
2023-10-27 04:56:57.213 
Epoch 416/1000 
	 loss: 28.4768, MinusLogProbMetric: 28.4768, val_loss: 29.0188, val_MinusLogProbMetric: 29.0188

Epoch 416: val_loss did not improve from 28.45398
196/196 - 42s - loss: 28.4768 - MinusLogProbMetric: 28.4768 - val_loss: 29.0188 - val_MinusLogProbMetric: 29.0188 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 417/1000
2023-10-27 04:57:38.847 
Epoch 417/1000 
	 loss: 28.7078, MinusLogProbMetric: 28.7078, val_loss: 28.6961, val_MinusLogProbMetric: 28.6961

Epoch 417: val_loss did not improve from 28.45398
196/196 - 42s - loss: 28.7078 - MinusLogProbMetric: 28.7078 - val_loss: 28.6961 - val_MinusLogProbMetric: 28.6961 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 418/1000
2023-10-27 04:58:19.856 
Epoch 418/1000 
	 loss: 28.5948, MinusLogProbMetric: 28.5948, val_loss: 30.1723, val_MinusLogProbMetric: 30.1723

Epoch 418: val_loss did not improve from 28.45398
196/196 - 41s - loss: 28.5948 - MinusLogProbMetric: 28.5948 - val_loss: 30.1723 - val_MinusLogProbMetric: 30.1723 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 419/1000
2023-10-27 04:59:00.360 
Epoch 419/1000 
	 loss: 28.6009, MinusLogProbMetric: 28.6009, val_loss: 28.6216, val_MinusLogProbMetric: 28.6216

Epoch 419: val_loss did not improve from 28.45398
196/196 - 41s - loss: 28.6009 - MinusLogProbMetric: 28.6009 - val_loss: 28.6216 - val_MinusLogProbMetric: 28.6216 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 420/1000
2023-10-27 04:59:42.540 
Epoch 420/1000 
	 loss: 28.5172, MinusLogProbMetric: 28.5172, val_loss: 28.5197, val_MinusLogProbMetric: 28.5197

Epoch 420: val_loss did not improve from 28.45398
196/196 - 42s - loss: 28.5172 - MinusLogProbMetric: 28.5172 - val_loss: 28.5197 - val_MinusLogProbMetric: 28.5197 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 421/1000
2023-10-27 05:00:24.139 
Epoch 421/1000 
	 loss: 28.6906, MinusLogProbMetric: 28.6906, val_loss: 28.9379, val_MinusLogProbMetric: 28.9379

Epoch 421: val_loss did not improve from 28.45398
196/196 - 42s - loss: 28.6906 - MinusLogProbMetric: 28.6906 - val_loss: 28.9379 - val_MinusLogProbMetric: 28.9379 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 422/1000
2023-10-27 05:01:05.631 
Epoch 422/1000 
	 loss: 28.4858, MinusLogProbMetric: 28.4858, val_loss: 28.4065, val_MinusLogProbMetric: 28.4065

Epoch 422: val_loss improved from 28.45398 to 28.40652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 28.4858 - MinusLogProbMetric: 28.4858 - val_loss: 28.4065 - val_MinusLogProbMetric: 28.4065 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 423/1000
2023-10-27 05:01:48.259 
Epoch 423/1000 
	 loss: 28.6472, MinusLogProbMetric: 28.6472, val_loss: 28.6792, val_MinusLogProbMetric: 28.6792

Epoch 423: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.6472 - MinusLogProbMetric: 28.6472 - val_loss: 28.6792 - val_MinusLogProbMetric: 28.6792 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 424/1000
2023-10-27 05:02:30.386 
Epoch 424/1000 
	 loss: 28.5136, MinusLogProbMetric: 28.5136, val_loss: 28.5826, val_MinusLogProbMetric: 28.5826

Epoch 424: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5136 - MinusLogProbMetric: 28.5136 - val_loss: 28.5826 - val_MinusLogProbMetric: 28.5826 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 425/1000
2023-10-27 05:03:12.199 
Epoch 425/1000 
	 loss: 28.5066, MinusLogProbMetric: 28.5066, val_loss: 29.3826, val_MinusLogProbMetric: 29.3826

Epoch 425: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5066 - MinusLogProbMetric: 28.5066 - val_loss: 29.3826 - val_MinusLogProbMetric: 29.3826 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 426/1000
2023-10-27 05:03:54.585 
Epoch 426/1000 
	 loss: 28.5405, MinusLogProbMetric: 28.5405, val_loss: 29.1000, val_MinusLogProbMetric: 29.1000

Epoch 426: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5405 - MinusLogProbMetric: 28.5405 - val_loss: 29.1000 - val_MinusLogProbMetric: 29.1000 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 427/1000
2023-10-27 05:04:36.042 
Epoch 427/1000 
	 loss: 28.4912, MinusLogProbMetric: 28.4912, val_loss: 28.5784, val_MinusLogProbMetric: 28.5784

Epoch 427: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.4912 - MinusLogProbMetric: 28.4912 - val_loss: 28.5784 - val_MinusLogProbMetric: 28.5784 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 428/1000
2023-10-27 05:05:17.769 
Epoch 428/1000 
	 loss: 28.5307, MinusLogProbMetric: 28.5307, val_loss: 29.2806, val_MinusLogProbMetric: 29.2806

Epoch 428: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5307 - MinusLogProbMetric: 28.5307 - val_loss: 29.2806 - val_MinusLogProbMetric: 29.2806 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 429/1000
2023-10-27 05:05:59.870 
Epoch 429/1000 
	 loss: 28.7221, MinusLogProbMetric: 28.7221, val_loss: 28.7838, val_MinusLogProbMetric: 28.7838

Epoch 429: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.7221 - MinusLogProbMetric: 28.7221 - val_loss: 28.7838 - val_MinusLogProbMetric: 28.7838 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 430/1000
2023-10-27 05:06:42.215 
Epoch 430/1000 
	 loss: 28.5693, MinusLogProbMetric: 28.5693, val_loss: 29.4690, val_MinusLogProbMetric: 29.4690

Epoch 430: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5693 - MinusLogProbMetric: 28.5693 - val_loss: 29.4690 - val_MinusLogProbMetric: 29.4690 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 431/1000
2023-10-27 05:07:24.567 
Epoch 431/1000 
	 loss: 28.4952, MinusLogProbMetric: 28.4952, val_loss: 29.0003, val_MinusLogProbMetric: 29.0003

Epoch 431: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4952 - MinusLogProbMetric: 28.4952 - val_loss: 29.0003 - val_MinusLogProbMetric: 29.0003 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 432/1000
2023-10-27 05:08:06.714 
Epoch 432/1000 
	 loss: 28.5269, MinusLogProbMetric: 28.5269, val_loss: 28.8846, val_MinusLogProbMetric: 28.8846

Epoch 432: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5269 - MinusLogProbMetric: 28.5269 - val_loss: 28.8846 - val_MinusLogProbMetric: 28.8846 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 433/1000
2023-10-27 05:08:48.811 
Epoch 433/1000 
	 loss: 28.4775, MinusLogProbMetric: 28.4775, val_loss: 28.6459, val_MinusLogProbMetric: 28.6459

Epoch 433: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4775 - MinusLogProbMetric: 28.4775 - val_loss: 28.6459 - val_MinusLogProbMetric: 28.6459 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 434/1000
2023-10-27 05:09:31.072 
Epoch 434/1000 
	 loss: 28.5860, MinusLogProbMetric: 28.5860, val_loss: 28.4643, val_MinusLogProbMetric: 28.4643

Epoch 434: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5860 - MinusLogProbMetric: 28.5860 - val_loss: 28.4643 - val_MinusLogProbMetric: 28.4643 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 435/1000
2023-10-27 05:10:13.459 
Epoch 435/1000 
	 loss: 28.5066, MinusLogProbMetric: 28.5066, val_loss: 28.5225, val_MinusLogProbMetric: 28.5225

Epoch 435: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5066 - MinusLogProbMetric: 28.5066 - val_loss: 28.5225 - val_MinusLogProbMetric: 28.5225 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 436/1000
2023-10-27 05:10:55.279 
Epoch 436/1000 
	 loss: 28.4850, MinusLogProbMetric: 28.4850, val_loss: 28.4378, val_MinusLogProbMetric: 28.4378

Epoch 436: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4850 - MinusLogProbMetric: 28.4850 - val_loss: 28.4378 - val_MinusLogProbMetric: 28.4378 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 437/1000
2023-10-27 05:11:37.867 
Epoch 437/1000 
	 loss: 28.7063, MinusLogProbMetric: 28.7063, val_loss: 28.5723, val_MinusLogProbMetric: 28.5723

Epoch 437: val_loss did not improve from 28.40652
196/196 - 43s - loss: 28.7063 - MinusLogProbMetric: 28.7063 - val_loss: 28.5723 - val_MinusLogProbMetric: 28.5723 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 438/1000
2023-10-27 05:12:20.235 
Epoch 438/1000 
	 loss: 28.6383, MinusLogProbMetric: 28.6383, val_loss: 28.6733, val_MinusLogProbMetric: 28.6733

Epoch 438: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.6383 - MinusLogProbMetric: 28.6383 - val_loss: 28.6733 - val_MinusLogProbMetric: 28.6733 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 439/1000
2023-10-27 05:13:02.557 
Epoch 439/1000 
	 loss: 28.4793, MinusLogProbMetric: 28.4793, val_loss: 28.6424, val_MinusLogProbMetric: 28.6424

Epoch 439: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4793 - MinusLogProbMetric: 28.4793 - val_loss: 28.6424 - val_MinusLogProbMetric: 28.6424 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 440/1000
2023-10-27 05:13:44.943 
Epoch 440/1000 
	 loss: 28.5549, MinusLogProbMetric: 28.5549, val_loss: 28.6584, val_MinusLogProbMetric: 28.6584

Epoch 440: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5549 - MinusLogProbMetric: 28.5549 - val_loss: 28.6584 - val_MinusLogProbMetric: 28.6584 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 441/1000
2023-10-27 05:14:27.268 
Epoch 441/1000 
	 loss: 28.6469, MinusLogProbMetric: 28.6469, val_loss: 28.6735, val_MinusLogProbMetric: 28.6735

Epoch 441: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.6469 - MinusLogProbMetric: 28.6469 - val_loss: 28.6735 - val_MinusLogProbMetric: 28.6735 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 442/1000
2023-10-27 05:15:09.454 
Epoch 442/1000 
	 loss: 28.4002, MinusLogProbMetric: 28.4002, val_loss: 28.4416, val_MinusLogProbMetric: 28.4416

Epoch 442: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4002 - MinusLogProbMetric: 28.4002 - val_loss: 28.4416 - val_MinusLogProbMetric: 28.4416 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 443/1000
2023-10-27 05:15:51.910 
Epoch 443/1000 
	 loss: 28.7188, MinusLogProbMetric: 28.7188, val_loss: 28.6442, val_MinusLogProbMetric: 28.6442

Epoch 443: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.7188 - MinusLogProbMetric: 28.7188 - val_loss: 28.6442 - val_MinusLogProbMetric: 28.6442 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 444/1000
2023-10-27 05:16:33.353 
Epoch 444/1000 
	 loss: 28.5953, MinusLogProbMetric: 28.5953, val_loss: 29.1848, val_MinusLogProbMetric: 29.1848

Epoch 444: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.5953 - MinusLogProbMetric: 28.5953 - val_loss: 29.1848 - val_MinusLogProbMetric: 29.1848 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 445/1000
2023-10-27 05:17:14.345 
Epoch 445/1000 
	 loss: 28.5063, MinusLogProbMetric: 28.5063, val_loss: 28.9174, val_MinusLogProbMetric: 28.9174

Epoch 445: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.5063 - MinusLogProbMetric: 28.5063 - val_loss: 28.9174 - val_MinusLogProbMetric: 28.9174 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 446/1000
2023-10-27 05:17:56.455 
Epoch 446/1000 
	 loss: 28.5732, MinusLogProbMetric: 28.5732, val_loss: 29.3252, val_MinusLogProbMetric: 29.3252

Epoch 446: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5732 - MinusLogProbMetric: 28.5732 - val_loss: 29.3252 - val_MinusLogProbMetric: 29.3252 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 447/1000
2023-10-27 05:18:38.496 
Epoch 447/1000 
	 loss: 28.5978, MinusLogProbMetric: 28.5978, val_loss: 28.6339, val_MinusLogProbMetric: 28.6339

Epoch 447: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5978 - MinusLogProbMetric: 28.5978 - val_loss: 28.6339 - val_MinusLogProbMetric: 28.6339 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 448/1000
2023-10-27 05:19:20.663 
Epoch 448/1000 
	 loss: 28.6293, MinusLogProbMetric: 28.6293, val_loss: 28.7541, val_MinusLogProbMetric: 28.7541

Epoch 448: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.6293 - MinusLogProbMetric: 28.6293 - val_loss: 28.7541 - val_MinusLogProbMetric: 28.7541 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 449/1000
2023-10-27 05:20:01.852 
Epoch 449/1000 
	 loss: 28.6197, MinusLogProbMetric: 28.6197, val_loss: 30.6551, val_MinusLogProbMetric: 30.6551

Epoch 449: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.6197 - MinusLogProbMetric: 28.6197 - val_loss: 30.6551 - val_MinusLogProbMetric: 30.6551 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 450/1000
2023-10-27 05:20:43.539 
Epoch 450/1000 
	 loss: 28.5773, MinusLogProbMetric: 28.5773, val_loss: 28.5034, val_MinusLogProbMetric: 28.5034

Epoch 450: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5773 - MinusLogProbMetric: 28.5773 - val_loss: 28.5034 - val_MinusLogProbMetric: 28.5034 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 451/1000
2023-10-27 05:21:25.355 
Epoch 451/1000 
	 loss: 28.4904, MinusLogProbMetric: 28.4904, val_loss: 28.4877, val_MinusLogProbMetric: 28.4877

Epoch 451: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4904 - MinusLogProbMetric: 28.4904 - val_loss: 28.4877 - val_MinusLogProbMetric: 28.4877 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 452/1000
2023-10-27 05:22:06.924 
Epoch 452/1000 
	 loss: 28.5125, MinusLogProbMetric: 28.5125, val_loss: 28.8345, val_MinusLogProbMetric: 28.8345

Epoch 452: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5125 - MinusLogProbMetric: 28.5125 - val_loss: 28.8345 - val_MinusLogProbMetric: 28.8345 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 453/1000
2023-10-27 05:22:48.513 
Epoch 453/1000 
	 loss: 28.4271, MinusLogProbMetric: 28.4271, val_loss: 28.6250, val_MinusLogProbMetric: 28.6250

Epoch 453: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4271 - MinusLogProbMetric: 28.4271 - val_loss: 28.6250 - val_MinusLogProbMetric: 28.6250 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 454/1000
2023-10-27 05:23:31.185 
Epoch 454/1000 
	 loss: 28.5017, MinusLogProbMetric: 28.5017, val_loss: 28.4432, val_MinusLogProbMetric: 28.4432

Epoch 454: val_loss did not improve from 28.40652
196/196 - 43s - loss: 28.5017 - MinusLogProbMetric: 28.5017 - val_loss: 28.4432 - val_MinusLogProbMetric: 28.4432 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 455/1000
2023-10-27 05:24:12.986 
Epoch 455/1000 
	 loss: 28.6149, MinusLogProbMetric: 28.6149, val_loss: 28.7340, val_MinusLogProbMetric: 28.7340

Epoch 455: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.6149 - MinusLogProbMetric: 28.6149 - val_loss: 28.7340 - val_MinusLogProbMetric: 28.7340 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 456/1000
2023-10-27 05:24:54.673 
Epoch 456/1000 
	 loss: 28.3866, MinusLogProbMetric: 28.3866, val_loss: 28.6935, val_MinusLogProbMetric: 28.6935

Epoch 456: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.3866 - MinusLogProbMetric: 28.3866 - val_loss: 28.6935 - val_MinusLogProbMetric: 28.6935 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 457/1000
2023-10-27 05:25:35.814 
Epoch 457/1000 
	 loss: 28.6406, MinusLogProbMetric: 28.6406, val_loss: 28.7719, val_MinusLogProbMetric: 28.7719

Epoch 457: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.6406 - MinusLogProbMetric: 28.6406 - val_loss: 28.7719 - val_MinusLogProbMetric: 28.7719 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 458/1000
2023-10-27 05:26:11.417 
Epoch 458/1000 
	 loss: 28.5642, MinusLogProbMetric: 28.5642, val_loss: 29.1782, val_MinusLogProbMetric: 29.1782

Epoch 458: val_loss did not improve from 28.40652
196/196 - 36s - loss: 28.5642 - MinusLogProbMetric: 28.5642 - val_loss: 29.1782 - val_MinusLogProbMetric: 29.1782 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 459/1000
2023-10-27 05:26:46.418 
Epoch 459/1000 
	 loss: 28.4988, MinusLogProbMetric: 28.4988, val_loss: 28.6653, val_MinusLogProbMetric: 28.6653

Epoch 459: val_loss did not improve from 28.40652
196/196 - 35s - loss: 28.4988 - MinusLogProbMetric: 28.4988 - val_loss: 28.6653 - val_MinusLogProbMetric: 28.6653 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 460/1000
2023-10-27 05:27:21.814 
Epoch 460/1000 
	 loss: 28.5045, MinusLogProbMetric: 28.5045, val_loss: 28.7182, val_MinusLogProbMetric: 28.7182

Epoch 460: val_loss did not improve from 28.40652
196/196 - 35s - loss: 28.5045 - MinusLogProbMetric: 28.5045 - val_loss: 28.7182 - val_MinusLogProbMetric: 28.7182 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 461/1000
2023-10-27 05:28:02.465 
Epoch 461/1000 
	 loss: 28.4400, MinusLogProbMetric: 28.4400, val_loss: 30.9893, val_MinusLogProbMetric: 30.9893

Epoch 461: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.4400 - MinusLogProbMetric: 28.4400 - val_loss: 30.9893 - val_MinusLogProbMetric: 30.9893 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 462/1000
2023-10-27 05:28:44.568 
Epoch 462/1000 
	 loss: 28.7395, MinusLogProbMetric: 28.7395, val_loss: 28.6808, val_MinusLogProbMetric: 28.6808

Epoch 462: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.7395 - MinusLogProbMetric: 28.7395 - val_loss: 28.6808 - val_MinusLogProbMetric: 28.6808 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 463/1000
2023-10-27 05:29:25.550 
Epoch 463/1000 
	 loss: 28.4228, MinusLogProbMetric: 28.4228, val_loss: 28.4919, val_MinusLogProbMetric: 28.4919

Epoch 463: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.4228 - MinusLogProbMetric: 28.4228 - val_loss: 28.4919 - val_MinusLogProbMetric: 28.4919 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 464/1000
2023-10-27 05:30:07.618 
Epoch 464/1000 
	 loss: 28.4699, MinusLogProbMetric: 28.4699, val_loss: 28.6534, val_MinusLogProbMetric: 28.6534

Epoch 464: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4699 - MinusLogProbMetric: 28.4699 - val_loss: 28.6534 - val_MinusLogProbMetric: 28.6534 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 465/1000
2023-10-27 05:30:49.087 
Epoch 465/1000 
	 loss: 28.5353, MinusLogProbMetric: 28.5353, val_loss: 29.4494, val_MinusLogProbMetric: 29.4494

Epoch 465: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.5353 - MinusLogProbMetric: 28.5353 - val_loss: 29.4494 - val_MinusLogProbMetric: 29.4494 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 466/1000
2023-10-27 05:31:31.687 
Epoch 466/1000 
	 loss: 28.4680, MinusLogProbMetric: 28.4680, val_loss: 28.5334, val_MinusLogProbMetric: 28.5334

Epoch 466: val_loss did not improve from 28.40652
196/196 - 43s - loss: 28.4680 - MinusLogProbMetric: 28.4680 - val_loss: 28.5334 - val_MinusLogProbMetric: 28.5334 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 467/1000
2023-10-27 05:32:14.366 
Epoch 467/1000 
	 loss: 28.3996, MinusLogProbMetric: 28.3996, val_loss: 28.5776, val_MinusLogProbMetric: 28.5776

Epoch 467: val_loss did not improve from 28.40652
196/196 - 43s - loss: 28.3996 - MinusLogProbMetric: 28.3996 - val_loss: 28.5776 - val_MinusLogProbMetric: 28.5776 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 468/1000
2023-10-27 05:32:56.351 
Epoch 468/1000 
	 loss: 28.7596, MinusLogProbMetric: 28.7596, val_loss: 28.5354, val_MinusLogProbMetric: 28.5354

Epoch 468: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.7596 - MinusLogProbMetric: 28.7596 - val_loss: 28.5354 - val_MinusLogProbMetric: 28.5354 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 469/1000
2023-10-27 05:33:37.720 
Epoch 469/1000 
	 loss: 28.5427, MinusLogProbMetric: 28.5427, val_loss: 29.9014, val_MinusLogProbMetric: 29.9014

Epoch 469: val_loss did not improve from 28.40652
196/196 - 41s - loss: 28.5427 - MinusLogProbMetric: 28.5427 - val_loss: 29.9014 - val_MinusLogProbMetric: 29.9014 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 470/1000
2023-10-27 05:34:19.406 
Epoch 470/1000 
	 loss: 28.5453, MinusLogProbMetric: 28.5453, val_loss: 28.4742, val_MinusLogProbMetric: 28.4742

Epoch 470: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.5453 - MinusLogProbMetric: 28.5453 - val_loss: 28.4742 - val_MinusLogProbMetric: 28.4742 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 471/1000
2023-10-27 05:35:01.161 
Epoch 471/1000 
	 loss: 28.4919, MinusLogProbMetric: 28.4919, val_loss: 28.7387, val_MinusLogProbMetric: 28.7387

Epoch 471: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.4919 - MinusLogProbMetric: 28.4919 - val_loss: 28.7387 - val_MinusLogProbMetric: 28.7387 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 472/1000
2023-10-27 05:35:43.522 
Epoch 472/1000 
	 loss: 28.6131, MinusLogProbMetric: 28.6131, val_loss: 28.5736, val_MinusLogProbMetric: 28.5736

Epoch 472: val_loss did not improve from 28.40652
196/196 - 42s - loss: 28.6131 - MinusLogProbMetric: 28.6131 - val_loss: 28.5736 - val_MinusLogProbMetric: 28.5736 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 473/1000
2023-10-27 05:36:25.883 
Epoch 473/1000 
	 loss: 28.1353, MinusLogProbMetric: 28.1353, val_loss: 28.3191, val_MinusLogProbMetric: 28.3191

Epoch 473: val_loss improved from 28.40652 to 28.31912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.1353 - MinusLogProbMetric: 28.1353 - val_loss: 28.3191 - val_MinusLogProbMetric: 28.3191 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 474/1000
2023-10-27 05:37:09.061 
Epoch 474/1000 
	 loss: 28.1024, MinusLogProbMetric: 28.1024, val_loss: 28.2501, val_MinusLogProbMetric: 28.2501

Epoch 474: val_loss improved from 28.31912 to 28.25013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.1024 - MinusLogProbMetric: 28.1024 - val_loss: 28.2501 - val_MinusLogProbMetric: 28.2501 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 475/1000
2023-10-27 05:37:51.511 
Epoch 475/1000 
	 loss: 28.0909, MinusLogProbMetric: 28.0909, val_loss: 28.3761, val_MinusLogProbMetric: 28.3761

Epoch 475: val_loss did not improve from 28.25013
196/196 - 42s - loss: 28.0909 - MinusLogProbMetric: 28.0909 - val_loss: 28.3761 - val_MinusLogProbMetric: 28.3761 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 476/1000
2023-10-27 05:38:33.803 
Epoch 476/1000 
	 loss: 28.0927, MinusLogProbMetric: 28.0927, val_loss: 28.3712, val_MinusLogProbMetric: 28.3712

Epoch 476: val_loss did not improve from 28.25013
196/196 - 42s - loss: 28.0927 - MinusLogProbMetric: 28.0927 - val_loss: 28.3712 - val_MinusLogProbMetric: 28.3712 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 477/1000
2023-10-27 05:39:15.323 
Epoch 477/1000 
	 loss: 28.1043, MinusLogProbMetric: 28.1043, val_loss: 28.4574, val_MinusLogProbMetric: 28.4574

Epoch 477: val_loss did not improve from 28.25013
196/196 - 42s - loss: 28.1043 - MinusLogProbMetric: 28.1043 - val_loss: 28.4574 - val_MinusLogProbMetric: 28.4574 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 478/1000
2023-10-27 05:39:57.653 
Epoch 478/1000 
	 loss: 28.0988, MinusLogProbMetric: 28.0988, val_loss: 28.2268, val_MinusLogProbMetric: 28.2268

Epoch 478: val_loss improved from 28.25013 to 28.22680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.0988 - MinusLogProbMetric: 28.0988 - val_loss: 28.2268 - val_MinusLogProbMetric: 28.2268 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 479/1000
2023-10-27 05:40:40.435 
Epoch 479/1000 
	 loss: 28.0955, MinusLogProbMetric: 28.0955, val_loss: 28.2528, val_MinusLogProbMetric: 28.2528

Epoch 479: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.0955 - MinusLogProbMetric: 28.0955 - val_loss: 28.2528 - val_MinusLogProbMetric: 28.2528 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 480/1000
2023-10-27 05:41:23.082 
Epoch 480/1000 
	 loss: 28.1193, MinusLogProbMetric: 28.1193, val_loss: 28.6354, val_MinusLogProbMetric: 28.6354

Epoch 480: val_loss did not improve from 28.22680
196/196 - 43s - loss: 28.1193 - MinusLogProbMetric: 28.1193 - val_loss: 28.6354 - val_MinusLogProbMetric: 28.6354 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 481/1000
2023-10-27 05:42:04.773 
Epoch 481/1000 
	 loss: 28.0876, MinusLogProbMetric: 28.0876, val_loss: 28.3533, val_MinusLogProbMetric: 28.3533

Epoch 481: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.0876 - MinusLogProbMetric: 28.0876 - val_loss: 28.3533 - val_MinusLogProbMetric: 28.3533 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 482/1000
2023-10-27 05:42:46.467 
Epoch 482/1000 
	 loss: 28.1329, MinusLogProbMetric: 28.1329, val_loss: 28.4829, val_MinusLogProbMetric: 28.4829

Epoch 482: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.1329 - MinusLogProbMetric: 28.1329 - val_loss: 28.4829 - val_MinusLogProbMetric: 28.4829 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 483/1000
2023-10-27 05:43:28.312 
Epoch 483/1000 
	 loss: 28.1384, MinusLogProbMetric: 28.1384, val_loss: 28.2927, val_MinusLogProbMetric: 28.2927

Epoch 483: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.1384 - MinusLogProbMetric: 28.1384 - val_loss: 28.2927 - val_MinusLogProbMetric: 28.2927 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 484/1000
2023-10-27 05:44:10.430 
Epoch 484/1000 
	 loss: 28.1349, MinusLogProbMetric: 28.1349, val_loss: 28.4007, val_MinusLogProbMetric: 28.4007

Epoch 484: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.1349 - MinusLogProbMetric: 28.1349 - val_loss: 28.4007 - val_MinusLogProbMetric: 28.4007 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 485/1000
2023-10-27 05:44:51.603 
Epoch 485/1000 
	 loss: 28.0980, MinusLogProbMetric: 28.0980, val_loss: 28.6873, val_MinusLogProbMetric: 28.6873

Epoch 485: val_loss did not improve from 28.22680
196/196 - 41s - loss: 28.0980 - MinusLogProbMetric: 28.0980 - val_loss: 28.6873 - val_MinusLogProbMetric: 28.6873 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 486/1000
2023-10-27 05:45:33.574 
Epoch 486/1000 
	 loss: 28.1205, MinusLogProbMetric: 28.1205, val_loss: 28.3038, val_MinusLogProbMetric: 28.3038

Epoch 486: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.1205 - MinusLogProbMetric: 28.1205 - val_loss: 28.3038 - val_MinusLogProbMetric: 28.3038 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 487/1000
2023-10-27 05:46:15.364 
Epoch 487/1000 
	 loss: 28.0850, MinusLogProbMetric: 28.0850, val_loss: 28.3242, val_MinusLogProbMetric: 28.3242

Epoch 487: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.0850 - MinusLogProbMetric: 28.0850 - val_loss: 28.3242 - val_MinusLogProbMetric: 28.3242 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 488/1000
2023-10-27 05:46:57.245 
Epoch 488/1000 
	 loss: 28.1220, MinusLogProbMetric: 28.1220, val_loss: 28.2675, val_MinusLogProbMetric: 28.2675

Epoch 488: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.1220 - MinusLogProbMetric: 28.1220 - val_loss: 28.2675 - val_MinusLogProbMetric: 28.2675 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 489/1000
2023-10-27 05:47:39.042 
Epoch 489/1000 
	 loss: 28.0995, MinusLogProbMetric: 28.0995, val_loss: 28.2298, val_MinusLogProbMetric: 28.2298

Epoch 489: val_loss did not improve from 28.22680
196/196 - 42s - loss: 28.0995 - MinusLogProbMetric: 28.0995 - val_loss: 28.2298 - val_MinusLogProbMetric: 28.2298 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 490/1000
2023-10-27 05:48:20.582 
Epoch 490/1000 
	 loss: 28.1066, MinusLogProbMetric: 28.1066, val_loss: 28.2100, val_MinusLogProbMetric: 28.2100

Epoch 490: val_loss improved from 28.22680 to 28.21004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 28.1066 - MinusLogProbMetric: 28.1066 - val_loss: 28.2100 - val_MinusLogProbMetric: 28.2100 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 491/1000
2023-10-27 05:49:03.021 
Epoch 491/1000 
	 loss: 28.0988, MinusLogProbMetric: 28.0988, val_loss: 28.3087, val_MinusLogProbMetric: 28.3087

Epoch 491: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0988 - MinusLogProbMetric: 28.0988 - val_loss: 28.3087 - val_MinusLogProbMetric: 28.3087 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 492/1000
2023-10-27 05:49:45.340 
Epoch 492/1000 
	 loss: 28.0832, MinusLogProbMetric: 28.0832, val_loss: 28.3637, val_MinusLogProbMetric: 28.3637

Epoch 492: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0832 - MinusLogProbMetric: 28.0832 - val_loss: 28.3637 - val_MinusLogProbMetric: 28.3637 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 493/1000
2023-10-27 05:50:27.780 
Epoch 493/1000 
	 loss: 28.0799, MinusLogProbMetric: 28.0799, val_loss: 28.2278, val_MinusLogProbMetric: 28.2278

Epoch 493: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0799 - MinusLogProbMetric: 28.0799 - val_loss: 28.2278 - val_MinusLogProbMetric: 28.2278 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 494/1000
2023-10-27 05:51:09.702 
Epoch 494/1000 
	 loss: 28.1143, MinusLogProbMetric: 28.1143, val_loss: 28.4493, val_MinusLogProbMetric: 28.4493

Epoch 494: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.1143 - MinusLogProbMetric: 28.1143 - val_loss: 28.4493 - val_MinusLogProbMetric: 28.4493 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 495/1000
2023-10-27 05:51:51.469 
Epoch 495/1000 
	 loss: 28.0913, MinusLogProbMetric: 28.0913, val_loss: 28.3117, val_MinusLogProbMetric: 28.3117

Epoch 495: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0913 - MinusLogProbMetric: 28.0913 - val_loss: 28.3117 - val_MinusLogProbMetric: 28.3117 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 496/1000
2023-10-27 05:52:33.917 
Epoch 496/1000 
	 loss: 28.0669, MinusLogProbMetric: 28.0669, val_loss: 28.2983, val_MinusLogProbMetric: 28.2983

Epoch 496: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0669 - MinusLogProbMetric: 28.0669 - val_loss: 28.2983 - val_MinusLogProbMetric: 28.2983 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 497/1000
2023-10-27 05:53:15.967 
Epoch 497/1000 
	 loss: 28.0831, MinusLogProbMetric: 28.0831, val_loss: 28.2139, val_MinusLogProbMetric: 28.2139

Epoch 497: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0831 - MinusLogProbMetric: 28.0831 - val_loss: 28.2139 - val_MinusLogProbMetric: 28.2139 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 498/1000
2023-10-27 05:53:57.945 
Epoch 498/1000 
	 loss: 28.0928, MinusLogProbMetric: 28.0928, val_loss: 28.2112, val_MinusLogProbMetric: 28.2112

Epoch 498: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0928 - MinusLogProbMetric: 28.0928 - val_loss: 28.2112 - val_MinusLogProbMetric: 28.2112 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 499/1000
2023-10-27 05:54:40.054 
Epoch 499/1000 
	 loss: 28.1008, MinusLogProbMetric: 28.1008, val_loss: 28.2441, val_MinusLogProbMetric: 28.2441

Epoch 499: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.1008 - MinusLogProbMetric: 28.1008 - val_loss: 28.2441 - val_MinusLogProbMetric: 28.2441 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 500/1000
2023-10-27 05:55:21.633 
Epoch 500/1000 
	 loss: 28.0927, MinusLogProbMetric: 28.0927, val_loss: 28.3048, val_MinusLogProbMetric: 28.3048

Epoch 500: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0927 - MinusLogProbMetric: 28.0927 - val_loss: 28.3048 - val_MinusLogProbMetric: 28.3048 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 501/1000
2023-10-27 05:56:03.266 
Epoch 501/1000 
	 loss: 28.0849, MinusLogProbMetric: 28.0849, val_loss: 28.2512, val_MinusLogProbMetric: 28.2512

Epoch 501: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0849 - MinusLogProbMetric: 28.0849 - val_loss: 28.2512 - val_MinusLogProbMetric: 28.2512 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 502/1000
2023-10-27 05:56:45.493 
Epoch 502/1000 
	 loss: 28.0892, MinusLogProbMetric: 28.0892, val_loss: 28.2600, val_MinusLogProbMetric: 28.2600

Epoch 502: val_loss did not improve from 28.21004
196/196 - 42s - loss: 28.0892 - MinusLogProbMetric: 28.0892 - val_loss: 28.2600 - val_MinusLogProbMetric: 28.2600 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 503/1000
2023-10-27 05:57:28.105 
Epoch 503/1000 
	 loss: 28.0792, MinusLogProbMetric: 28.0792, val_loss: 28.2071, val_MinusLogProbMetric: 28.2071

Epoch 503: val_loss improved from 28.21004 to 28.20709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 28.0792 - MinusLogProbMetric: 28.0792 - val_loss: 28.2071 - val_MinusLogProbMetric: 28.2071 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 504/1000
2023-10-27 05:58:11.611 
Epoch 504/1000 
	 loss: 28.0913, MinusLogProbMetric: 28.0913, val_loss: 28.2911, val_MinusLogProbMetric: 28.2911

Epoch 504: val_loss did not improve from 28.20709
196/196 - 43s - loss: 28.0913 - MinusLogProbMetric: 28.0913 - val_loss: 28.2911 - val_MinusLogProbMetric: 28.2911 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 505/1000
2023-10-27 05:58:53.556 
Epoch 505/1000 
	 loss: 28.0676, MinusLogProbMetric: 28.0676, val_loss: 28.2674, val_MinusLogProbMetric: 28.2674

Epoch 505: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.0676 - MinusLogProbMetric: 28.0676 - val_loss: 28.2674 - val_MinusLogProbMetric: 28.2674 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 506/1000
2023-10-27 05:59:35.332 
Epoch 506/1000 
	 loss: 28.1106, MinusLogProbMetric: 28.1106, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 506: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.1106 - MinusLogProbMetric: 28.1106 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 507/1000
2023-10-27 06:00:17.738 
Epoch 507/1000 
	 loss: 28.0962, MinusLogProbMetric: 28.0962, val_loss: 28.4429, val_MinusLogProbMetric: 28.4429

Epoch 507: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.0962 - MinusLogProbMetric: 28.0962 - val_loss: 28.4429 - val_MinusLogProbMetric: 28.4429 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 508/1000
2023-10-27 06:00:59.856 
Epoch 508/1000 
	 loss: 28.1041, MinusLogProbMetric: 28.1041, val_loss: 28.2632, val_MinusLogProbMetric: 28.2632

Epoch 508: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.1041 - MinusLogProbMetric: 28.1041 - val_loss: 28.2632 - val_MinusLogProbMetric: 28.2632 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 509/1000
2023-10-27 06:01:41.293 
Epoch 509/1000 
	 loss: 28.0854, MinusLogProbMetric: 28.0854, val_loss: 28.2327, val_MinusLogProbMetric: 28.2327

Epoch 509: val_loss did not improve from 28.20709
196/196 - 41s - loss: 28.0854 - MinusLogProbMetric: 28.0854 - val_loss: 28.2327 - val_MinusLogProbMetric: 28.2327 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 510/1000
2023-10-27 06:02:23.363 
Epoch 510/1000 
	 loss: 28.0561, MinusLogProbMetric: 28.0561, val_loss: 28.2500, val_MinusLogProbMetric: 28.2500

Epoch 510: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.0561 - MinusLogProbMetric: 28.0561 - val_loss: 28.2500 - val_MinusLogProbMetric: 28.2500 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 511/1000
2023-10-27 06:03:05.686 
Epoch 511/1000 
	 loss: 28.0725, MinusLogProbMetric: 28.0725, val_loss: 28.3384, val_MinusLogProbMetric: 28.3384

Epoch 511: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.0725 - MinusLogProbMetric: 28.0725 - val_loss: 28.3384 - val_MinusLogProbMetric: 28.3384 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 512/1000
2023-10-27 06:03:47.245 
Epoch 512/1000 
	 loss: 28.0810, MinusLogProbMetric: 28.0810, val_loss: 28.5063, val_MinusLogProbMetric: 28.5063

Epoch 512: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.0810 - MinusLogProbMetric: 28.0810 - val_loss: 28.5063 - val_MinusLogProbMetric: 28.5063 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 513/1000
2023-10-27 06:04:28.850 
Epoch 513/1000 
	 loss: 28.1273, MinusLogProbMetric: 28.1273, val_loss: 28.3151, val_MinusLogProbMetric: 28.3151

Epoch 513: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.1273 - MinusLogProbMetric: 28.1273 - val_loss: 28.3151 - val_MinusLogProbMetric: 28.3151 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 514/1000
2023-10-27 06:05:10.739 
Epoch 514/1000 
	 loss: 28.0724, MinusLogProbMetric: 28.0724, val_loss: 28.2434, val_MinusLogProbMetric: 28.2434

Epoch 514: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.0724 - MinusLogProbMetric: 28.0724 - val_loss: 28.2434 - val_MinusLogProbMetric: 28.2434 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 515/1000
2023-10-27 06:05:53.024 
Epoch 515/1000 
	 loss: 28.0842, MinusLogProbMetric: 28.0842, val_loss: 28.2536, val_MinusLogProbMetric: 28.2536

Epoch 515: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.0842 - MinusLogProbMetric: 28.0842 - val_loss: 28.2536 - val_MinusLogProbMetric: 28.2536 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 516/1000
2023-10-27 06:06:34.406 
Epoch 516/1000 
	 loss: 28.0692, MinusLogProbMetric: 28.0692, val_loss: 28.2780, val_MinusLogProbMetric: 28.2780

Epoch 516: val_loss did not improve from 28.20709
196/196 - 41s - loss: 28.0692 - MinusLogProbMetric: 28.0692 - val_loss: 28.2780 - val_MinusLogProbMetric: 28.2780 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 517/1000
2023-10-27 06:07:16.293 
Epoch 517/1000 
	 loss: 28.0677, MinusLogProbMetric: 28.0677, val_loss: 28.2585, val_MinusLogProbMetric: 28.2585

Epoch 517: val_loss did not improve from 28.20709
196/196 - 42s - loss: 28.0677 - MinusLogProbMetric: 28.0677 - val_loss: 28.2585 - val_MinusLogProbMetric: 28.2585 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 518/1000
2023-10-27 06:07:57.302 
Epoch 518/1000 
	 loss: 28.0912, MinusLogProbMetric: 28.0912, val_loss: 28.2630, val_MinusLogProbMetric: 28.2630

Epoch 518: val_loss did not improve from 28.20709
196/196 - 41s - loss: 28.0912 - MinusLogProbMetric: 28.0912 - val_loss: 28.2630 - val_MinusLogProbMetric: 28.2630 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 519/1000
2023-10-27 06:08:38.517 
Epoch 519/1000 
	 loss: 28.0465, MinusLogProbMetric: 28.0465, val_loss: 28.3605, val_MinusLogProbMetric: 28.3605

Epoch 519: val_loss did not improve from 28.20709
196/196 - 41s - loss: 28.0465 - MinusLogProbMetric: 28.0465 - val_loss: 28.3605 - val_MinusLogProbMetric: 28.3605 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 520/1000
2023-10-27 06:09:21.137 
Epoch 520/1000 
	 loss: 28.0628, MinusLogProbMetric: 28.0628, val_loss: 28.2794, val_MinusLogProbMetric: 28.2794

Epoch 520: val_loss did not improve from 28.20709
196/196 - 43s - loss: 28.0628 - MinusLogProbMetric: 28.0628 - val_loss: 28.2794 - val_MinusLogProbMetric: 28.2794 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 521/1000
2023-10-27 06:10:02.983 
Epoch 521/1000 
	 loss: 28.1348, MinusLogProbMetric: 28.1348, val_loss: 28.1896, val_MinusLogProbMetric: 28.1896

Epoch 521: val_loss improved from 28.20709 to 28.18956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.1348 - MinusLogProbMetric: 28.1348 - val_loss: 28.1896 - val_MinusLogProbMetric: 28.1896 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 522/1000
2023-10-27 06:10:43.516 
Epoch 522/1000 
	 loss: 28.0829, MinusLogProbMetric: 28.0829, val_loss: 28.2636, val_MinusLogProbMetric: 28.2636

Epoch 522: val_loss did not improve from 28.18956
196/196 - 40s - loss: 28.0829 - MinusLogProbMetric: 28.0829 - val_loss: 28.2636 - val_MinusLogProbMetric: 28.2636 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 523/1000
2023-10-27 06:11:25.100 
Epoch 523/1000 
	 loss: 28.1053, MinusLogProbMetric: 28.1053, val_loss: 28.3309, val_MinusLogProbMetric: 28.3309

Epoch 523: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.1053 - MinusLogProbMetric: 28.1053 - val_loss: 28.3309 - val_MinusLogProbMetric: 28.3309 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 524/1000
2023-10-27 06:12:07.594 
Epoch 524/1000 
	 loss: 28.0680, MinusLogProbMetric: 28.0680, val_loss: 28.4135, val_MinusLogProbMetric: 28.4135

Epoch 524: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0680 - MinusLogProbMetric: 28.0680 - val_loss: 28.4135 - val_MinusLogProbMetric: 28.4135 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 525/1000
2023-10-27 06:12:49.767 
Epoch 525/1000 
	 loss: 28.1136, MinusLogProbMetric: 28.1136, val_loss: 28.3459, val_MinusLogProbMetric: 28.3459

Epoch 525: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.1136 - MinusLogProbMetric: 28.1136 - val_loss: 28.3459 - val_MinusLogProbMetric: 28.3459 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 526/1000
2023-10-27 06:13:31.632 
Epoch 526/1000 
	 loss: 28.0898, MinusLogProbMetric: 28.0898, val_loss: 28.2828, val_MinusLogProbMetric: 28.2828

Epoch 526: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0898 - MinusLogProbMetric: 28.0898 - val_loss: 28.2828 - val_MinusLogProbMetric: 28.2828 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 527/1000
2023-10-27 06:14:13.410 
Epoch 527/1000 
	 loss: 28.0651, MinusLogProbMetric: 28.0651, val_loss: 28.2148, val_MinusLogProbMetric: 28.2148

Epoch 527: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0651 - MinusLogProbMetric: 28.0651 - val_loss: 28.2148 - val_MinusLogProbMetric: 28.2148 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 528/1000
2023-10-27 06:14:55.338 
Epoch 528/1000 
	 loss: 28.0614, MinusLogProbMetric: 28.0614, val_loss: 28.2062, val_MinusLogProbMetric: 28.2062

Epoch 528: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0614 - MinusLogProbMetric: 28.0614 - val_loss: 28.2062 - val_MinusLogProbMetric: 28.2062 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 529/1000
2023-10-27 06:15:37.515 
Epoch 529/1000 
	 loss: 28.0866, MinusLogProbMetric: 28.0866, val_loss: 28.2458, val_MinusLogProbMetric: 28.2458

Epoch 529: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0866 - MinusLogProbMetric: 28.0866 - val_loss: 28.2458 - val_MinusLogProbMetric: 28.2458 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 530/1000
2023-10-27 06:16:18.887 
Epoch 530/1000 
	 loss: 28.0414, MinusLogProbMetric: 28.0414, val_loss: 28.3332, val_MinusLogProbMetric: 28.3332

Epoch 530: val_loss did not improve from 28.18956
196/196 - 41s - loss: 28.0414 - MinusLogProbMetric: 28.0414 - val_loss: 28.3332 - val_MinusLogProbMetric: 28.3332 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 531/1000
2023-10-27 06:17:01.359 
Epoch 531/1000 
	 loss: 28.0627, MinusLogProbMetric: 28.0627, val_loss: 28.2956, val_MinusLogProbMetric: 28.2956

Epoch 531: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0627 - MinusLogProbMetric: 28.0627 - val_loss: 28.2956 - val_MinusLogProbMetric: 28.2956 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 532/1000
2023-10-27 06:17:43.525 
Epoch 532/1000 
	 loss: 28.0817, MinusLogProbMetric: 28.0817, val_loss: 28.2385, val_MinusLogProbMetric: 28.2385

Epoch 532: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0817 - MinusLogProbMetric: 28.0817 - val_loss: 28.2385 - val_MinusLogProbMetric: 28.2385 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 533/1000
2023-10-27 06:18:25.787 
Epoch 533/1000 
	 loss: 28.0642, MinusLogProbMetric: 28.0642, val_loss: 28.1988, val_MinusLogProbMetric: 28.1988

Epoch 533: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0642 - MinusLogProbMetric: 28.0642 - val_loss: 28.1988 - val_MinusLogProbMetric: 28.1988 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 534/1000
2023-10-27 06:19:07.706 
Epoch 534/1000 
	 loss: 28.0708, MinusLogProbMetric: 28.0708, val_loss: 28.7445, val_MinusLogProbMetric: 28.7445

Epoch 534: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0708 - MinusLogProbMetric: 28.0708 - val_loss: 28.7445 - val_MinusLogProbMetric: 28.7445 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 535/1000
2023-10-27 06:19:49.996 
Epoch 535/1000 
	 loss: 28.0927, MinusLogProbMetric: 28.0927, val_loss: 28.2231, val_MinusLogProbMetric: 28.2231

Epoch 535: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0927 - MinusLogProbMetric: 28.0927 - val_loss: 28.2231 - val_MinusLogProbMetric: 28.2231 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 536/1000
2023-10-27 06:20:31.756 
Epoch 536/1000 
	 loss: 28.0914, MinusLogProbMetric: 28.0914, val_loss: 28.2423, val_MinusLogProbMetric: 28.2423

Epoch 536: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0914 - MinusLogProbMetric: 28.0914 - val_loss: 28.2423 - val_MinusLogProbMetric: 28.2423 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 537/1000
2023-10-27 06:21:13.854 
Epoch 537/1000 
	 loss: 28.0646, MinusLogProbMetric: 28.0646, val_loss: 28.1955, val_MinusLogProbMetric: 28.1955

Epoch 537: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0646 - MinusLogProbMetric: 28.0646 - val_loss: 28.1955 - val_MinusLogProbMetric: 28.1955 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 538/1000
2023-10-27 06:21:56.272 
Epoch 538/1000 
	 loss: 28.0794, MinusLogProbMetric: 28.0794, val_loss: 28.4144, val_MinusLogProbMetric: 28.4144

Epoch 538: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0794 - MinusLogProbMetric: 28.0794 - val_loss: 28.4144 - val_MinusLogProbMetric: 28.4144 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 539/1000
2023-10-27 06:22:38.047 
Epoch 539/1000 
	 loss: 28.0816, MinusLogProbMetric: 28.0816, val_loss: 28.2958, val_MinusLogProbMetric: 28.2958

Epoch 539: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0816 - MinusLogProbMetric: 28.0816 - val_loss: 28.2958 - val_MinusLogProbMetric: 28.2958 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 540/1000
2023-10-27 06:23:20.190 
Epoch 540/1000 
	 loss: 28.0763, MinusLogProbMetric: 28.0763, val_loss: 28.3173, val_MinusLogProbMetric: 28.3173

Epoch 540: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0763 - MinusLogProbMetric: 28.0763 - val_loss: 28.3173 - val_MinusLogProbMetric: 28.3173 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 541/1000
2023-10-27 06:24:02.423 
Epoch 541/1000 
	 loss: 28.0916, MinusLogProbMetric: 28.0916, val_loss: 28.3048, val_MinusLogProbMetric: 28.3048

Epoch 541: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0916 - MinusLogProbMetric: 28.0916 - val_loss: 28.3048 - val_MinusLogProbMetric: 28.3048 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 542/1000
2023-10-27 06:24:44.802 
Epoch 542/1000 
	 loss: 28.0919, MinusLogProbMetric: 28.0919, val_loss: 28.2451, val_MinusLogProbMetric: 28.2451

Epoch 542: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0919 - MinusLogProbMetric: 28.0919 - val_loss: 28.2451 - val_MinusLogProbMetric: 28.2451 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 543/1000
2023-10-27 06:25:26.935 
Epoch 543/1000 
	 loss: 28.0699, MinusLogProbMetric: 28.0699, val_loss: 28.3280, val_MinusLogProbMetric: 28.3280

Epoch 543: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0699 - MinusLogProbMetric: 28.0699 - val_loss: 28.3280 - val_MinusLogProbMetric: 28.3280 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 544/1000
2023-10-27 06:26:09.249 
Epoch 544/1000 
	 loss: 28.0612, MinusLogProbMetric: 28.0612, val_loss: 28.2600, val_MinusLogProbMetric: 28.2600

Epoch 544: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0612 - MinusLogProbMetric: 28.0612 - val_loss: 28.2600 - val_MinusLogProbMetric: 28.2600 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 545/1000
2023-10-27 06:26:51.072 
Epoch 545/1000 
	 loss: 28.0790, MinusLogProbMetric: 28.0790, val_loss: 28.3726, val_MinusLogProbMetric: 28.3726

Epoch 545: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0790 - MinusLogProbMetric: 28.0790 - val_loss: 28.3726 - val_MinusLogProbMetric: 28.3726 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 546/1000
2023-10-27 06:27:33.517 
Epoch 546/1000 
	 loss: 28.0740, MinusLogProbMetric: 28.0740, val_loss: 28.1978, val_MinusLogProbMetric: 28.1978

Epoch 546: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0740 - MinusLogProbMetric: 28.0740 - val_loss: 28.1978 - val_MinusLogProbMetric: 28.1978 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 547/1000
2023-10-27 06:28:14.620 
Epoch 547/1000 
	 loss: 28.0693, MinusLogProbMetric: 28.0693, val_loss: 28.3503, val_MinusLogProbMetric: 28.3503

Epoch 547: val_loss did not improve from 28.18956
196/196 - 41s - loss: 28.0693 - MinusLogProbMetric: 28.0693 - val_loss: 28.3503 - val_MinusLogProbMetric: 28.3503 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 548/1000
2023-10-27 06:28:50.804 
Epoch 548/1000 
	 loss: 28.0648, MinusLogProbMetric: 28.0648, val_loss: 28.3687, val_MinusLogProbMetric: 28.3687

Epoch 548: val_loss did not improve from 28.18956
196/196 - 36s - loss: 28.0648 - MinusLogProbMetric: 28.0648 - val_loss: 28.3687 - val_MinusLogProbMetric: 28.3687 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 549/1000
2023-10-27 06:29:32.326 
Epoch 549/1000 
	 loss: 28.0606, MinusLogProbMetric: 28.0606, val_loss: 28.2447, val_MinusLogProbMetric: 28.2447

Epoch 549: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0606 - MinusLogProbMetric: 28.0606 - val_loss: 28.2447 - val_MinusLogProbMetric: 28.2447 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 550/1000
2023-10-27 06:30:10.114 
Epoch 550/1000 
	 loss: 28.0789, MinusLogProbMetric: 28.0789, val_loss: 28.2019, val_MinusLogProbMetric: 28.2019

Epoch 550: val_loss did not improve from 28.18956
196/196 - 38s - loss: 28.0789 - MinusLogProbMetric: 28.0789 - val_loss: 28.2019 - val_MinusLogProbMetric: 28.2019 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 551/1000
2023-10-27 06:30:49.323 
Epoch 551/1000 
	 loss: 28.0562, MinusLogProbMetric: 28.0562, val_loss: 28.3844, val_MinusLogProbMetric: 28.3844

Epoch 551: val_loss did not improve from 28.18956
196/196 - 39s - loss: 28.0562 - MinusLogProbMetric: 28.0562 - val_loss: 28.3844 - val_MinusLogProbMetric: 28.3844 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 552/1000
2023-10-27 06:31:31.776 
Epoch 552/1000 
	 loss: 28.0833, MinusLogProbMetric: 28.0833, val_loss: 28.4864, val_MinusLogProbMetric: 28.4864

Epoch 552: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0833 - MinusLogProbMetric: 28.0833 - val_loss: 28.4864 - val_MinusLogProbMetric: 28.4864 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 553/1000
2023-10-27 06:32:13.945 
Epoch 553/1000 
	 loss: 28.1004, MinusLogProbMetric: 28.1004, val_loss: 28.3979, val_MinusLogProbMetric: 28.3979

Epoch 553: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.1004 - MinusLogProbMetric: 28.1004 - val_loss: 28.3979 - val_MinusLogProbMetric: 28.3979 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 554/1000
2023-10-27 06:32:55.637 
Epoch 554/1000 
	 loss: 28.0632, MinusLogProbMetric: 28.0632, val_loss: 28.2621, val_MinusLogProbMetric: 28.2621

Epoch 554: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0632 - MinusLogProbMetric: 28.0632 - val_loss: 28.2621 - val_MinusLogProbMetric: 28.2621 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 555/1000
2023-10-27 06:33:36.637 
Epoch 555/1000 
	 loss: 28.0786, MinusLogProbMetric: 28.0786, val_loss: 28.3297, val_MinusLogProbMetric: 28.3297

Epoch 555: val_loss did not improve from 28.18956
196/196 - 41s - loss: 28.0786 - MinusLogProbMetric: 28.0786 - val_loss: 28.3297 - val_MinusLogProbMetric: 28.3297 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 556/1000
2023-10-27 06:34:17.954 
Epoch 556/1000 
	 loss: 28.0749, MinusLogProbMetric: 28.0749, val_loss: 28.3619, val_MinusLogProbMetric: 28.3619

Epoch 556: val_loss did not improve from 28.18956
196/196 - 41s - loss: 28.0749 - MinusLogProbMetric: 28.0749 - val_loss: 28.3619 - val_MinusLogProbMetric: 28.3619 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 557/1000
2023-10-27 06:34:59.442 
Epoch 557/1000 
	 loss: 28.0823, MinusLogProbMetric: 28.0823, val_loss: 28.1993, val_MinusLogProbMetric: 28.1993

Epoch 557: val_loss did not improve from 28.18956
196/196 - 41s - loss: 28.0823 - MinusLogProbMetric: 28.0823 - val_loss: 28.1993 - val_MinusLogProbMetric: 28.1993 - lr: 2.5000e-04 - 41s/epoch - 212ms/step
Epoch 558/1000
2023-10-27 06:35:39.973 
Epoch 558/1000 
	 loss: 28.0392, MinusLogProbMetric: 28.0392, val_loss: 28.2813, val_MinusLogProbMetric: 28.2813

Epoch 558: val_loss did not improve from 28.18956
196/196 - 41s - loss: 28.0392 - MinusLogProbMetric: 28.0392 - val_loss: 28.2813 - val_MinusLogProbMetric: 28.2813 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 559/1000
2023-10-27 06:36:22.029 
Epoch 559/1000 
	 loss: 28.1045, MinusLogProbMetric: 28.1045, val_loss: 28.2947, val_MinusLogProbMetric: 28.2947

Epoch 559: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.1045 - MinusLogProbMetric: 28.1045 - val_loss: 28.2947 - val_MinusLogProbMetric: 28.2947 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 560/1000
2023-10-27 06:37:04.127 
Epoch 560/1000 
	 loss: 28.0587, MinusLogProbMetric: 28.0587, val_loss: 28.2962, val_MinusLogProbMetric: 28.2962

Epoch 560: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0587 - MinusLogProbMetric: 28.0587 - val_loss: 28.2962 - val_MinusLogProbMetric: 28.2962 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 561/1000
2023-10-27 06:37:46.152 
Epoch 561/1000 
	 loss: 28.0482, MinusLogProbMetric: 28.0482, val_loss: 28.1933, val_MinusLogProbMetric: 28.1933

Epoch 561: val_loss did not improve from 28.18956
196/196 - 42s - loss: 28.0482 - MinusLogProbMetric: 28.0482 - val_loss: 28.1933 - val_MinusLogProbMetric: 28.1933 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 562/1000
2023-10-27 06:38:27.756 
Epoch 562/1000 
	 loss: 28.0622, MinusLogProbMetric: 28.0622, val_loss: 28.1821, val_MinusLogProbMetric: 28.1821

Epoch 562: val_loss improved from 28.18956 to 28.18207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 28.0622 - MinusLogProbMetric: 28.0622 - val_loss: 28.1821 - val_MinusLogProbMetric: 28.1821 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 563/1000
2023-10-27 06:39:10.856 
Epoch 563/1000 
	 loss: 28.0521, MinusLogProbMetric: 28.0521, val_loss: 28.1766, val_MinusLogProbMetric: 28.1766

Epoch 563: val_loss improved from 28.18207 to 28.17655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.0521 - MinusLogProbMetric: 28.0521 - val_loss: 28.1766 - val_MinusLogProbMetric: 28.1766 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 564/1000
2023-10-27 06:39:53.256 
Epoch 564/1000 
	 loss: 28.0439, MinusLogProbMetric: 28.0439, val_loss: 28.3511, val_MinusLogProbMetric: 28.3511

Epoch 564: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0439 - MinusLogProbMetric: 28.0439 - val_loss: 28.3511 - val_MinusLogProbMetric: 28.3511 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 565/1000
2023-10-27 06:40:35.449 
Epoch 565/1000 
	 loss: 28.0682, MinusLogProbMetric: 28.0682, val_loss: 28.2333, val_MinusLogProbMetric: 28.2333

Epoch 565: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0682 - MinusLogProbMetric: 28.0682 - val_loss: 28.2333 - val_MinusLogProbMetric: 28.2333 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 566/1000
2023-10-27 06:41:17.046 
Epoch 566/1000 
	 loss: 28.0739, MinusLogProbMetric: 28.0739, val_loss: 28.3274, val_MinusLogProbMetric: 28.3274

Epoch 566: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0739 - MinusLogProbMetric: 28.0739 - val_loss: 28.3274 - val_MinusLogProbMetric: 28.3274 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 567/1000
2023-10-27 06:41:59.006 
Epoch 567/1000 
	 loss: 28.0600, MinusLogProbMetric: 28.0600, val_loss: 28.3088, val_MinusLogProbMetric: 28.3088

Epoch 567: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0600 - MinusLogProbMetric: 28.0600 - val_loss: 28.3088 - val_MinusLogProbMetric: 28.3088 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 568/1000
2023-10-27 06:42:39.704 
Epoch 568/1000 
	 loss: 28.0543, MinusLogProbMetric: 28.0543, val_loss: 28.1837, val_MinusLogProbMetric: 28.1837

Epoch 568: val_loss did not improve from 28.17655
196/196 - 41s - loss: 28.0543 - MinusLogProbMetric: 28.0543 - val_loss: 28.1837 - val_MinusLogProbMetric: 28.1837 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 569/1000
2023-10-27 06:43:21.856 
Epoch 569/1000 
	 loss: 28.0744, MinusLogProbMetric: 28.0744, val_loss: 28.3193, val_MinusLogProbMetric: 28.3193

Epoch 569: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0744 - MinusLogProbMetric: 28.0744 - val_loss: 28.3193 - val_MinusLogProbMetric: 28.3193 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 570/1000
2023-10-27 06:44:03.821 
Epoch 570/1000 
	 loss: 28.0854, MinusLogProbMetric: 28.0854, val_loss: 28.2527, val_MinusLogProbMetric: 28.2527

Epoch 570: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0854 - MinusLogProbMetric: 28.0854 - val_loss: 28.2527 - val_MinusLogProbMetric: 28.2527 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 571/1000
2023-10-27 06:44:45.962 
Epoch 571/1000 
	 loss: 28.0487, MinusLogProbMetric: 28.0487, val_loss: 28.4026, val_MinusLogProbMetric: 28.4026

Epoch 571: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0487 - MinusLogProbMetric: 28.0487 - val_loss: 28.4026 - val_MinusLogProbMetric: 28.4026 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 572/1000
2023-10-27 06:45:28.227 
Epoch 572/1000 
	 loss: 28.0498, MinusLogProbMetric: 28.0498, val_loss: 28.4861, val_MinusLogProbMetric: 28.4861

Epoch 572: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0498 - MinusLogProbMetric: 28.0498 - val_loss: 28.4861 - val_MinusLogProbMetric: 28.4861 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 573/1000
2023-10-27 06:46:10.670 
Epoch 573/1000 
	 loss: 28.0925, MinusLogProbMetric: 28.0925, val_loss: 28.2653, val_MinusLogProbMetric: 28.2653

Epoch 573: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0925 - MinusLogProbMetric: 28.0925 - val_loss: 28.2653 - val_MinusLogProbMetric: 28.2653 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 574/1000
2023-10-27 06:46:53.171 
Epoch 574/1000 
	 loss: 28.0766, MinusLogProbMetric: 28.0766, val_loss: 28.2272, val_MinusLogProbMetric: 28.2272

Epoch 574: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0766 - MinusLogProbMetric: 28.0766 - val_loss: 28.2272 - val_MinusLogProbMetric: 28.2272 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 575/1000
2023-10-27 06:47:35.154 
Epoch 575/1000 
	 loss: 28.0619, MinusLogProbMetric: 28.0619, val_loss: 28.2411, val_MinusLogProbMetric: 28.2411

Epoch 575: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0619 - MinusLogProbMetric: 28.0619 - val_loss: 28.2411 - val_MinusLogProbMetric: 28.2411 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 576/1000
2023-10-27 06:48:16.855 
Epoch 576/1000 
	 loss: 28.0640, MinusLogProbMetric: 28.0640, val_loss: 28.2654, val_MinusLogProbMetric: 28.2654

Epoch 576: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0640 - MinusLogProbMetric: 28.0640 - val_loss: 28.2654 - val_MinusLogProbMetric: 28.2654 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 577/1000
2023-10-27 06:48:58.625 
Epoch 577/1000 
	 loss: 28.0728, MinusLogProbMetric: 28.0728, val_loss: 28.3946, val_MinusLogProbMetric: 28.3946

Epoch 577: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0728 - MinusLogProbMetric: 28.0728 - val_loss: 28.3946 - val_MinusLogProbMetric: 28.3946 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 578/1000
2023-10-27 06:49:41.088 
Epoch 578/1000 
	 loss: 28.1056, MinusLogProbMetric: 28.1056, val_loss: 28.2404, val_MinusLogProbMetric: 28.2404

Epoch 578: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.1056 - MinusLogProbMetric: 28.1056 - val_loss: 28.2404 - val_MinusLogProbMetric: 28.2404 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 579/1000
2023-10-27 06:50:22.938 
Epoch 579/1000 
	 loss: 28.0571, MinusLogProbMetric: 28.0571, val_loss: 28.2864, val_MinusLogProbMetric: 28.2864

Epoch 579: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0571 - MinusLogProbMetric: 28.0571 - val_loss: 28.2864 - val_MinusLogProbMetric: 28.2864 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 580/1000
2023-10-27 06:51:04.244 
Epoch 580/1000 
	 loss: 28.0577, MinusLogProbMetric: 28.0577, val_loss: 28.2758, val_MinusLogProbMetric: 28.2758

Epoch 580: val_loss did not improve from 28.17655
196/196 - 41s - loss: 28.0577 - MinusLogProbMetric: 28.0577 - val_loss: 28.2758 - val_MinusLogProbMetric: 28.2758 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 581/1000
2023-10-27 06:51:45.568 
Epoch 581/1000 
	 loss: 28.0669, MinusLogProbMetric: 28.0669, val_loss: 28.3098, val_MinusLogProbMetric: 28.3098

Epoch 581: val_loss did not improve from 28.17655
196/196 - 41s - loss: 28.0669 - MinusLogProbMetric: 28.0669 - val_loss: 28.3098 - val_MinusLogProbMetric: 28.3098 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 582/1000
2023-10-27 06:52:27.222 
Epoch 582/1000 
	 loss: 28.0491, MinusLogProbMetric: 28.0491, val_loss: 28.3028, val_MinusLogProbMetric: 28.3028

Epoch 582: val_loss did not improve from 28.17655
196/196 - 42s - loss: 28.0491 - MinusLogProbMetric: 28.0491 - val_loss: 28.3028 - val_MinusLogProbMetric: 28.3028 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 583/1000
2023-10-27 06:53:07.619 
Epoch 583/1000 
	 loss: 28.0763, MinusLogProbMetric: 28.0763, val_loss: 28.1840, val_MinusLogProbMetric: 28.1840

Epoch 583: val_loss did not improve from 28.17655
196/196 - 40s - loss: 28.0763 - MinusLogProbMetric: 28.0763 - val_loss: 28.1840 - val_MinusLogProbMetric: 28.1840 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 584/1000
2023-10-27 06:53:49.111 
Epoch 584/1000 
	 loss: 28.0594, MinusLogProbMetric: 28.0594, val_loss: 28.1755, val_MinusLogProbMetric: 28.1755

Epoch 584: val_loss improved from 28.17655 to 28.17548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 28.0594 - MinusLogProbMetric: 28.0594 - val_loss: 28.1755 - val_MinusLogProbMetric: 28.1755 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 585/1000
2023-10-27 06:54:31.655 
Epoch 585/1000 
	 loss: 28.0515, MinusLogProbMetric: 28.0515, val_loss: 28.3221, val_MinusLogProbMetric: 28.3221

Epoch 585: val_loss did not improve from 28.17548
196/196 - 42s - loss: 28.0515 - MinusLogProbMetric: 28.0515 - val_loss: 28.3221 - val_MinusLogProbMetric: 28.3221 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 586/1000
2023-10-27 06:55:13.535 
Epoch 586/1000 
	 loss: 28.0721, MinusLogProbMetric: 28.0721, val_loss: 28.2828, val_MinusLogProbMetric: 28.2828

Epoch 586: val_loss did not improve from 28.17548
196/196 - 42s - loss: 28.0721 - MinusLogProbMetric: 28.0721 - val_loss: 28.2828 - val_MinusLogProbMetric: 28.2828 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 587/1000
2023-10-27 06:55:55.441 
Epoch 587/1000 
	 loss: 28.0513, MinusLogProbMetric: 28.0513, val_loss: 28.2787, val_MinusLogProbMetric: 28.2787

Epoch 587: val_loss did not improve from 28.17548
196/196 - 42s - loss: 28.0513 - MinusLogProbMetric: 28.0513 - val_loss: 28.2787 - val_MinusLogProbMetric: 28.2787 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 588/1000
2023-10-27 06:56:36.905 
Epoch 588/1000 
	 loss: 28.0769, MinusLogProbMetric: 28.0769, val_loss: 28.2179, val_MinusLogProbMetric: 28.2179

Epoch 588: val_loss did not improve from 28.17548
196/196 - 41s - loss: 28.0769 - MinusLogProbMetric: 28.0769 - val_loss: 28.2179 - val_MinusLogProbMetric: 28.2179 - lr: 2.5000e-04 - 41s/epoch - 212ms/step
Epoch 589/1000
2023-10-27 06:57:18.587 
Epoch 589/1000 
	 loss: 28.0267, MinusLogProbMetric: 28.0267, val_loss: 28.1748, val_MinusLogProbMetric: 28.1748

Epoch 589: val_loss improved from 28.17548 to 28.17477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 28.0267 - MinusLogProbMetric: 28.0267 - val_loss: 28.1748 - val_MinusLogProbMetric: 28.1748 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 590/1000
2023-10-27 06:58:01.075 
Epoch 590/1000 
	 loss: 28.0254, MinusLogProbMetric: 28.0254, val_loss: 28.2357, val_MinusLogProbMetric: 28.2357

Epoch 590: val_loss did not improve from 28.17477
196/196 - 42s - loss: 28.0254 - MinusLogProbMetric: 28.0254 - val_loss: 28.2357 - val_MinusLogProbMetric: 28.2357 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 591/1000
2023-10-27 06:58:41.748 
Epoch 591/1000 
	 loss: 28.0791, MinusLogProbMetric: 28.0791, val_loss: 28.3220, val_MinusLogProbMetric: 28.3220

Epoch 591: val_loss did not improve from 28.17477
196/196 - 41s - loss: 28.0791 - MinusLogProbMetric: 28.0791 - val_loss: 28.3220 - val_MinusLogProbMetric: 28.3220 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 592/1000
2023-10-27 06:59:23.596 
Epoch 592/1000 
	 loss: 28.0467, MinusLogProbMetric: 28.0467, val_loss: 28.1945, val_MinusLogProbMetric: 28.1945

Epoch 592: val_loss did not improve from 28.17477
196/196 - 42s - loss: 28.0467 - MinusLogProbMetric: 28.0467 - val_loss: 28.1945 - val_MinusLogProbMetric: 28.1945 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 593/1000
2023-10-27 07:00:05.091 
Epoch 593/1000 
	 loss: 28.0662, MinusLogProbMetric: 28.0662, val_loss: 28.3210, val_MinusLogProbMetric: 28.3210

Epoch 593: val_loss did not improve from 28.17477
196/196 - 41s - loss: 28.0662 - MinusLogProbMetric: 28.0662 - val_loss: 28.3210 - val_MinusLogProbMetric: 28.3210 - lr: 2.5000e-04 - 41s/epoch - 212ms/step
Epoch 594/1000
2023-10-27 07:00:47.366 
Epoch 594/1000 
	 loss: 28.0434, MinusLogProbMetric: 28.0434, val_loss: 28.3322, val_MinusLogProbMetric: 28.3322

Epoch 594: val_loss did not improve from 28.17477
196/196 - 42s - loss: 28.0434 - MinusLogProbMetric: 28.0434 - val_loss: 28.3322 - val_MinusLogProbMetric: 28.3322 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 595/1000
2023-10-27 07:01:29.173 
Epoch 595/1000 
	 loss: 28.0472, MinusLogProbMetric: 28.0472, val_loss: 28.2656, val_MinusLogProbMetric: 28.2656

Epoch 595: val_loss did not improve from 28.17477
196/196 - 42s - loss: 28.0472 - MinusLogProbMetric: 28.0472 - val_loss: 28.2656 - val_MinusLogProbMetric: 28.2656 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 596/1000
2023-10-27 07:02:11.658 
Epoch 596/1000 
	 loss: 28.0572, MinusLogProbMetric: 28.0572, val_loss: 28.2551, val_MinusLogProbMetric: 28.2551

Epoch 596: val_loss did not improve from 28.17477
196/196 - 42s - loss: 28.0572 - MinusLogProbMetric: 28.0572 - val_loss: 28.2551 - val_MinusLogProbMetric: 28.2551 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 597/1000
2023-10-27 07:02:52.753 
Epoch 597/1000 
	 loss: 28.0420, MinusLogProbMetric: 28.0420, val_loss: 28.1996, val_MinusLogProbMetric: 28.1996

Epoch 597: val_loss did not improve from 28.17477
196/196 - 41s - loss: 28.0420 - MinusLogProbMetric: 28.0420 - val_loss: 28.1996 - val_MinusLogProbMetric: 28.1996 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 598/1000
2023-10-27 07:03:35.121 
Epoch 598/1000 
	 loss: 28.0686, MinusLogProbMetric: 28.0686, val_loss: 28.3297, val_MinusLogProbMetric: 28.3297

Epoch 598: val_loss did not improve from 28.17477
196/196 - 42s - loss: 28.0686 - MinusLogProbMetric: 28.0686 - val_loss: 28.3297 - val_MinusLogProbMetric: 28.3297 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 599/1000
2023-10-27 07:04:16.542 
Epoch 599/1000 
	 loss: 28.0398, MinusLogProbMetric: 28.0398, val_loss: 28.3373, val_MinusLogProbMetric: 28.3373

Epoch 599: val_loss did not improve from 28.17477
196/196 - 41s - loss: 28.0398 - MinusLogProbMetric: 28.0398 - val_loss: 28.3373 - val_MinusLogProbMetric: 28.3373 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 600/1000
2023-10-27 07:04:58.611 
Epoch 600/1000 
	 loss: 28.0649, MinusLogProbMetric: 28.0649, val_loss: 28.4302, val_MinusLogProbMetric: 28.4302

Epoch 600: val_loss did not improve from 28.17477
196/196 - 42s - loss: 28.0649 - MinusLogProbMetric: 28.0649 - val_loss: 28.4302 - val_MinusLogProbMetric: 28.4302 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 601/1000
2023-10-27 07:05:40.026 
Epoch 601/1000 
	 loss: 28.0322, MinusLogProbMetric: 28.0322, val_loss: 28.1612, val_MinusLogProbMetric: 28.1612

Epoch 601: val_loss improved from 28.17477 to 28.16124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 28.0322 - MinusLogProbMetric: 28.0322 - val_loss: 28.1612 - val_MinusLogProbMetric: 28.1612 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 602/1000
2023-10-27 07:06:18.336 
Epoch 602/1000 
	 loss: 28.0475, MinusLogProbMetric: 28.0475, val_loss: 28.2451, val_MinusLogProbMetric: 28.2451

Epoch 602: val_loss did not improve from 28.16124
196/196 - 38s - loss: 28.0475 - MinusLogProbMetric: 28.0475 - val_loss: 28.2451 - val_MinusLogProbMetric: 28.2451 - lr: 2.5000e-04 - 38s/epoch - 191ms/step
Epoch 603/1000
2023-10-27 07:06:53.570 
Epoch 603/1000 
	 loss: 28.0347, MinusLogProbMetric: 28.0347, val_loss: 28.2790, val_MinusLogProbMetric: 28.2790

Epoch 603: val_loss did not improve from 28.16124
196/196 - 35s - loss: 28.0347 - MinusLogProbMetric: 28.0347 - val_loss: 28.2790 - val_MinusLogProbMetric: 28.2790 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 604/1000
2023-10-27 07:07:27.372 
Epoch 604/1000 
	 loss: 28.0328, MinusLogProbMetric: 28.0328, val_loss: 28.2007, val_MinusLogProbMetric: 28.2007

Epoch 604: val_loss did not improve from 28.16124
196/196 - 34s - loss: 28.0328 - MinusLogProbMetric: 28.0328 - val_loss: 28.2007 - val_MinusLogProbMetric: 28.2007 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 605/1000
2023-10-27 07:08:01.652 
Epoch 605/1000 
	 loss: 28.0455, MinusLogProbMetric: 28.0455, val_loss: 28.2799, val_MinusLogProbMetric: 28.2799

Epoch 605: val_loss did not improve from 28.16124
196/196 - 34s - loss: 28.0455 - MinusLogProbMetric: 28.0455 - val_loss: 28.2799 - val_MinusLogProbMetric: 28.2799 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 606/1000
2023-10-27 07:08:43.814 
Epoch 606/1000 
	 loss: 28.0609, MinusLogProbMetric: 28.0609, val_loss: 28.4711, val_MinusLogProbMetric: 28.4711

Epoch 606: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0609 - MinusLogProbMetric: 28.0609 - val_loss: 28.4711 - val_MinusLogProbMetric: 28.4711 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 607/1000
2023-10-27 07:09:25.874 
Epoch 607/1000 
	 loss: 28.1113, MinusLogProbMetric: 28.1113, val_loss: 28.3464, val_MinusLogProbMetric: 28.3464

Epoch 607: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.1113 - MinusLogProbMetric: 28.1113 - val_loss: 28.3464 - val_MinusLogProbMetric: 28.3464 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 608/1000
2023-10-27 07:10:07.306 
Epoch 608/1000 
	 loss: 28.0304, MinusLogProbMetric: 28.0304, val_loss: 28.2600, val_MinusLogProbMetric: 28.2600

Epoch 608: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0304 - MinusLogProbMetric: 28.0304 - val_loss: 28.2600 - val_MinusLogProbMetric: 28.2600 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 609/1000
2023-10-27 07:10:49.255 
Epoch 609/1000 
	 loss: 28.0442, MinusLogProbMetric: 28.0442, val_loss: 28.1928, val_MinusLogProbMetric: 28.1928

Epoch 609: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0442 - MinusLogProbMetric: 28.0442 - val_loss: 28.1928 - val_MinusLogProbMetric: 28.1928 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 610/1000
2023-10-27 07:11:31.010 
Epoch 610/1000 
	 loss: 28.0615, MinusLogProbMetric: 28.0615, val_loss: 28.2090, val_MinusLogProbMetric: 28.2090

Epoch 610: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0615 - MinusLogProbMetric: 28.0615 - val_loss: 28.2090 - val_MinusLogProbMetric: 28.2090 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 611/1000
2023-10-27 07:12:12.341 
Epoch 611/1000 
	 loss: 28.0220, MinusLogProbMetric: 28.0220, val_loss: 28.3523, val_MinusLogProbMetric: 28.3523

Epoch 611: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0220 - MinusLogProbMetric: 28.0220 - val_loss: 28.3523 - val_MinusLogProbMetric: 28.3523 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 612/1000
2023-10-27 07:12:54.252 
Epoch 612/1000 
	 loss: 28.0420, MinusLogProbMetric: 28.0420, val_loss: 28.4028, val_MinusLogProbMetric: 28.4028

Epoch 612: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0420 - MinusLogProbMetric: 28.0420 - val_loss: 28.4028 - val_MinusLogProbMetric: 28.4028 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 613/1000
2023-10-27 07:13:33.321 
Epoch 613/1000 
	 loss: 28.0483, MinusLogProbMetric: 28.0483, val_loss: 28.2806, val_MinusLogProbMetric: 28.2806

Epoch 613: val_loss did not improve from 28.16124
196/196 - 39s - loss: 28.0483 - MinusLogProbMetric: 28.0483 - val_loss: 28.2806 - val_MinusLogProbMetric: 28.2806 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 614/1000
2023-10-27 07:14:08.079 
Epoch 614/1000 
	 loss: 28.0770, MinusLogProbMetric: 28.0770, val_loss: 28.2070, val_MinusLogProbMetric: 28.2070

Epoch 614: val_loss did not improve from 28.16124
196/196 - 35s - loss: 28.0770 - MinusLogProbMetric: 28.0770 - val_loss: 28.2070 - val_MinusLogProbMetric: 28.2070 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 615/1000
2023-10-27 07:14:43.264 
Epoch 615/1000 
	 loss: 28.0731, MinusLogProbMetric: 28.0731, val_loss: 28.4049, val_MinusLogProbMetric: 28.4049

Epoch 615: val_loss did not improve from 28.16124
196/196 - 35s - loss: 28.0731 - MinusLogProbMetric: 28.0731 - val_loss: 28.4049 - val_MinusLogProbMetric: 28.4049 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 616/1000
2023-10-27 07:15:20.200 
Epoch 616/1000 
	 loss: 28.0498, MinusLogProbMetric: 28.0498, val_loss: 28.2531, val_MinusLogProbMetric: 28.2531

Epoch 616: val_loss did not improve from 28.16124
196/196 - 37s - loss: 28.0498 - MinusLogProbMetric: 28.0498 - val_loss: 28.2531 - val_MinusLogProbMetric: 28.2531 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 617/1000
2023-10-27 07:16:00.804 
Epoch 617/1000 
	 loss: 28.0574, MinusLogProbMetric: 28.0574, val_loss: 28.3956, val_MinusLogProbMetric: 28.3956

Epoch 617: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0574 - MinusLogProbMetric: 28.0574 - val_loss: 28.3956 - val_MinusLogProbMetric: 28.3956 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 618/1000
2023-10-27 07:16:41.630 
Epoch 618/1000 
	 loss: 28.0813, MinusLogProbMetric: 28.0813, val_loss: 28.4534, val_MinusLogProbMetric: 28.4534

Epoch 618: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0813 - MinusLogProbMetric: 28.0813 - val_loss: 28.4534 - val_MinusLogProbMetric: 28.4534 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 619/1000
2023-10-27 07:17:22.971 
Epoch 619/1000 
	 loss: 28.0608, MinusLogProbMetric: 28.0608, val_loss: 28.3585, val_MinusLogProbMetric: 28.3585

Epoch 619: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0608 - MinusLogProbMetric: 28.0608 - val_loss: 28.3585 - val_MinusLogProbMetric: 28.3585 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 620/1000
2023-10-27 07:18:04.458 
Epoch 620/1000 
	 loss: 28.0680, MinusLogProbMetric: 28.0680, val_loss: 28.4027, val_MinusLogProbMetric: 28.4027

Epoch 620: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0680 - MinusLogProbMetric: 28.0680 - val_loss: 28.4027 - val_MinusLogProbMetric: 28.4027 - lr: 2.5000e-04 - 41s/epoch - 212ms/step
Epoch 621/1000
2023-10-27 07:18:43.677 
Epoch 621/1000 
	 loss: 28.0608, MinusLogProbMetric: 28.0608, val_loss: 28.2049, val_MinusLogProbMetric: 28.2049

Epoch 621: val_loss did not improve from 28.16124
196/196 - 39s - loss: 28.0608 - MinusLogProbMetric: 28.0608 - val_loss: 28.2049 - val_MinusLogProbMetric: 28.2049 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 622/1000
2023-10-27 07:19:24.532 
Epoch 622/1000 
	 loss: 28.0405, MinusLogProbMetric: 28.0405, val_loss: 28.2391, val_MinusLogProbMetric: 28.2391

Epoch 622: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0405 - MinusLogProbMetric: 28.0405 - val_loss: 28.2391 - val_MinusLogProbMetric: 28.2391 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 623/1000
2023-10-27 07:20:06.509 
Epoch 623/1000 
	 loss: 28.0983, MinusLogProbMetric: 28.0983, val_loss: 28.3805, val_MinusLogProbMetric: 28.3805

Epoch 623: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0983 - MinusLogProbMetric: 28.0983 - val_loss: 28.3805 - val_MinusLogProbMetric: 28.3805 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 624/1000
2023-10-27 07:20:48.003 
Epoch 624/1000 
	 loss: 28.0401, MinusLogProbMetric: 28.0401, val_loss: 28.2658, val_MinusLogProbMetric: 28.2658

Epoch 624: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0401 - MinusLogProbMetric: 28.0401 - val_loss: 28.2658 - val_MinusLogProbMetric: 28.2658 - lr: 2.5000e-04 - 41s/epoch - 212ms/step
Epoch 625/1000
2023-10-27 07:21:30.275 
Epoch 625/1000 
	 loss: 28.0881, MinusLogProbMetric: 28.0881, val_loss: 28.2971, val_MinusLogProbMetric: 28.2971

Epoch 625: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0881 - MinusLogProbMetric: 28.0881 - val_loss: 28.2971 - val_MinusLogProbMetric: 28.2971 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 626/1000
2023-10-27 07:22:12.267 
Epoch 626/1000 
	 loss: 28.0664, MinusLogProbMetric: 28.0664, val_loss: 28.3082, val_MinusLogProbMetric: 28.3082

Epoch 626: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0664 - MinusLogProbMetric: 28.0664 - val_loss: 28.3082 - val_MinusLogProbMetric: 28.3082 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 627/1000
2023-10-27 07:22:54.519 
Epoch 627/1000 
	 loss: 28.1019, MinusLogProbMetric: 28.1019, val_loss: 28.4336, val_MinusLogProbMetric: 28.4336

Epoch 627: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.1019 - MinusLogProbMetric: 28.1019 - val_loss: 28.4336 - val_MinusLogProbMetric: 28.4336 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 628/1000
2023-10-27 07:23:36.316 
Epoch 628/1000 
	 loss: 28.0700, MinusLogProbMetric: 28.0700, val_loss: 28.2037, val_MinusLogProbMetric: 28.2037

Epoch 628: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0700 - MinusLogProbMetric: 28.0700 - val_loss: 28.2037 - val_MinusLogProbMetric: 28.2037 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 629/1000
2023-10-27 07:24:18.647 
Epoch 629/1000 
	 loss: 28.0262, MinusLogProbMetric: 28.0262, val_loss: 28.3890, val_MinusLogProbMetric: 28.3890

Epoch 629: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0262 - MinusLogProbMetric: 28.0262 - val_loss: 28.3890 - val_MinusLogProbMetric: 28.3890 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 630/1000
2023-10-27 07:24:58.067 
Epoch 630/1000 
	 loss: 28.0588, MinusLogProbMetric: 28.0588, val_loss: 28.2105, val_MinusLogProbMetric: 28.2105

Epoch 630: val_loss did not improve from 28.16124
196/196 - 39s - loss: 28.0588 - MinusLogProbMetric: 28.0588 - val_loss: 28.2105 - val_MinusLogProbMetric: 28.2105 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 631/1000
2023-10-27 07:25:39.919 
Epoch 631/1000 
	 loss: 28.0504, MinusLogProbMetric: 28.0504, val_loss: 28.4672, val_MinusLogProbMetric: 28.4672

Epoch 631: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0504 - MinusLogProbMetric: 28.0504 - val_loss: 28.4672 - val_MinusLogProbMetric: 28.4672 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 632/1000
2023-10-27 07:26:21.811 
Epoch 632/1000 
	 loss: 28.0305, MinusLogProbMetric: 28.0305, val_loss: 28.2448, val_MinusLogProbMetric: 28.2448

Epoch 632: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0305 - MinusLogProbMetric: 28.0305 - val_loss: 28.2448 - val_MinusLogProbMetric: 28.2448 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 633/1000
2023-10-27 07:27:03.035 
Epoch 633/1000 
	 loss: 28.0596, MinusLogProbMetric: 28.0596, val_loss: 28.3376, val_MinusLogProbMetric: 28.3376

Epoch 633: val_loss did not improve from 28.16124
196/196 - 41s - loss: 28.0596 - MinusLogProbMetric: 28.0596 - val_loss: 28.3376 - val_MinusLogProbMetric: 28.3376 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 634/1000
2023-10-27 07:27:45.134 
Epoch 634/1000 
	 loss: 28.0605, MinusLogProbMetric: 28.0605, val_loss: 28.1676, val_MinusLogProbMetric: 28.1676

Epoch 634: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0605 - MinusLogProbMetric: 28.0605 - val_loss: 28.1676 - val_MinusLogProbMetric: 28.1676 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 635/1000
2023-10-27 07:28:27.677 
Epoch 635/1000 
	 loss: 28.0314, MinusLogProbMetric: 28.0314, val_loss: 28.2169, val_MinusLogProbMetric: 28.2169

Epoch 635: val_loss did not improve from 28.16124
196/196 - 43s - loss: 28.0314 - MinusLogProbMetric: 28.0314 - val_loss: 28.2169 - val_MinusLogProbMetric: 28.2169 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 636/1000
2023-10-27 07:29:09.762 
Epoch 636/1000 
	 loss: 28.0634, MinusLogProbMetric: 28.0634, val_loss: 28.3653, val_MinusLogProbMetric: 28.3653

Epoch 636: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0634 - MinusLogProbMetric: 28.0634 - val_loss: 28.3653 - val_MinusLogProbMetric: 28.3653 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 637/1000
2023-10-27 07:29:51.904 
Epoch 637/1000 
	 loss: 28.0635, MinusLogProbMetric: 28.0635, val_loss: 28.3727, val_MinusLogProbMetric: 28.3727

Epoch 637: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0635 - MinusLogProbMetric: 28.0635 - val_loss: 28.3727 - val_MinusLogProbMetric: 28.3727 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 638/1000
2023-10-27 07:30:33.795 
Epoch 638/1000 
	 loss: 28.0507, MinusLogProbMetric: 28.0507, val_loss: 28.3096, val_MinusLogProbMetric: 28.3096

Epoch 638: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0507 - MinusLogProbMetric: 28.0507 - val_loss: 28.3096 - val_MinusLogProbMetric: 28.3096 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 639/1000
2023-10-27 07:31:15.742 
Epoch 639/1000 
	 loss: 28.0490, MinusLogProbMetric: 28.0490, val_loss: 28.1911, val_MinusLogProbMetric: 28.1911

Epoch 639: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0490 - MinusLogProbMetric: 28.0490 - val_loss: 28.1911 - val_MinusLogProbMetric: 28.1911 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 640/1000
2023-10-27 07:31:57.446 
Epoch 640/1000 
	 loss: 28.0208, MinusLogProbMetric: 28.0208, val_loss: 28.2707, val_MinusLogProbMetric: 28.2707

Epoch 640: val_loss did not improve from 28.16124
196/196 - 42s - loss: 28.0208 - MinusLogProbMetric: 28.0208 - val_loss: 28.2707 - val_MinusLogProbMetric: 28.2707 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 641/1000
2023-10-27 07:32:39.181 
Epoch 641/1000 
	 loss: 28.0516, MinusLogProbMetric: 28.0516, val_loss: 28.1592, val_MinusLogProbMetric: 28.1592

Epoch 641: val_loss improved from 28.16124 to 28.15924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.0516 - MinusLogProbMetric: 28.0516 - val_loss: 28.1592 - val_MinusLogProbMetric: 28.1592 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 642/1000
2023-10-27 07:33:22.066 
Epoch 642/1000 
	 loss: 28.0367, MinusLogProbMetric: 28.0367, val_loss: 28.1864, val_MinusLogProbMetric: 28.1864

Epoch 642: val_loss did not improve from 28.15924
196/196 - 42s - loss: 28.0367 - MinusLogProbMetric: 28.0367 - val_loss: 28.1864 - val_MinusLogProbMetric: 28.1864 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 643/1000
2023-10-27 07:34:04.080 
Epoch 643/1000 
	 loss: 28.0126, MinusLogProbMetric: 28.0126, val_loss: 28.2214, val_MinusLogProbMetric: 28.2214

Epoch 643: val_loss did not improve from 28.15924
196/196 - 42s - loss: 28.0126 - MinusLogProbMetric: 28.0126 - val_loss: 28.2214 - val_MinusLogProbMetric: 28.2214 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 644/1000
2023-10-27 07:34:46.027 
Epoch 644/1000 
	 loss: 28.0168, MinusLogProbMetric: 28.0168, val_loss: 28.1678, val_MinusLogProbMetric: 28.1678

Epoch 644: val_loss did not improve from 28.15924
196/196 - 42s - loss: 28.0168 - MinusLogProbMetric: 28.0168 - val_loss: 28.1678 - val_MinusLogProbMetric: 28.1678 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 645/1000
2023-10-27 07:35:28.038 
Epoch 645/1000 
	 loss: 28.1060, MinusLogProbMetric: 28.1060, val_loss: 28.2683, val_MinusLogProbMetric: 28.2683

Epoch 645: val_loss did not improve from 28.15924
196/196 - 42s - loss: 28.1060 - MinusLogProbMetric: 28.1060 - val_loss: 28.2683 - val_MinusLogProbMetric: 28.2683 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 646/1000
2023-10-27 07:36:09.958 
Epoch 646/1000 
	 loss: 28.0651, MinusLogProbMetric: 28.0651, val_loss: 28.3281, val_MinusLogProbMetric: 28.3281

Epoch 646: val_loss did not improve from 28.15924
196/196 - 42s - loss: 28.0651 - MinusLogProbMetric: 28.0651 - val_loss: 28.3281 - val_MinusLogProbMetric: 28.3281 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 647/1000
2023-10-27 07:36:51.504 
Epoch 647/1000 
	 loss: 28.0076, MinusLogProbMetric: 28.0076, val_loss: 28.2733, val_MinusLogProbMetric: 28.2733

Epoch 647: val_loss did not improve from 28.15924
196/196 - 42s - loss: 28.0076 - MinusLogProbMetric: 28.0076 - val_loss: 28.2733 - val_MinusLogProbMetric: 28.2733 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 648/1000
2023-10-27 07:37:32.946 
Epoch 648/1000 
	 loss: 28.0922, MinusLogProbMetric: 28.0922, val_loss: 28.1843, val_MinusLogProbMetric: 28.1843

Epoch 648: val_loss did not improve from 28.15924
196/196 - 41s - loss: 28.0922 - MinusLogProbMetric: 28.0922 - val_loss: 28.1843 - val_MinusLogProbMetric: 28.1843 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 649/1000
2023-10-27 07:38:13.826 
Epoch 649/1000 
	 loss: 28.0133, MinusLogProbMetric: 28.0133, val_loss: 28.3363, val_MinusLogProbMetric: 28.3363

Epoch 649: val_loss did not improve from 28.15924
196/196 - 41s - loss: 28.0133 - MinusLogProbMetric: 28.0133 - val_loss: 28.3363 - val_MinusLogProbMetric: 28.3363 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 650/1000
2023-10-27 07:38:48.656 
Epoch 650/1000 
	 loss: 28.0394, MinusLogProbMetric: 28.0394, val_loss: 28.2523, val_MinusLogProbMetric: 28.2523

Epoch 650: val_loss did not improve from 28.15924
196/196 - 35s - loss: 28.0394 - MinusLogProbMetric: 28.0394 - val_loss: 28.2523 - val_MinusLogProbMetric: 28.2523 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 651/1000
2023-10-27 07:39:21.662 
Epoch 651/1000 
	 loss: 28.0132, MinusLogProbMetric: 28.0132, val_loss: 28.3818, val_MinusLogProbMetric: 28.3818

Epoch 651: val_loss did not improve from 28.15924
196/196 - 33s - loss: 28.0132 - MinusLogProbMetric: 28.0132 - val_loss: 28.3818 - val_MinusLogProbMetric: 28.3818 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 652/1000
2023-10-27 07:39:54.618 
Epoch 652/1000 
	 loss: 28.0288, MinusLogProbMetric: 28.0288, val_loss: 28.2796, val_MinusLogProbMetric: 28.2796

Epoch 652: val_loss did not improve from 28.15924
196/196 - 33s - loss: 28.0288 - MinusLogProbMetric: 28.0288 - val_loss: 28.2796 - val_MinusLogProbMetric: 28.2796 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 653/1000
2023-10-27 07:40:31.072 
Epoch 653/1000 
	 loss: 28.1875, MinusLogProbMetric: 28.1875, val_loss: 28.1836, val_MinusLogProbMetric: 28.1836

Epoch 653: val_loss did not improve from 28.15924
196/196 - 36s - loss: 28.1875 - MinusLogProbMetric: 28.1875 - val_loss: 28.1836 - val_MinusLogProbMetric: 28.1836 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 654/1000
2023-10-27 07:41:13.342 
Epoch 654/1000 
	 loss: 28.0431, MinusLogProbMetric: 28.0431, val_loss: 28.2530, val_MinusLogProbMetric: 28.2530

Epoch 654: val_loss did not improve from 28.15924
196/196 - 42s - loss: 28.0431 - MinusLogProbMetric: 28.0431 - val_loss: 28.2530 - val_MinusLogProbMetric: 28.2530 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 655/1000
2023-10-27 07:41:55.131 
Epoch 655/1000 
	 loss: 28.0355, MinusLogProbMetric: 28.0355, val_loss: 28.2394, val_MinusLogProbMetric: 28.2394

Epoch 655: val_loss did not improve from 28.15924
196/196 - 42s - loss: 28.0355 - MinusLogProbMetric: 28.0355 - val_loss: 28.2394 - val_MinusLogProbMetric: 28.2394 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 656/1000
2023-10-27 07:42:35.691 
Epoch 656/1000 
	 loss: 28.0018, MinusLogProbMetric: 28.0018, val_loss: 28.2621, val_MinusLogProbMetric: 28.2621

Epoch 656: val_loss did not improve from 28.15924
196/196 - 41s - loss: 28.0018 - MinusLogProbMetric: 28.0018 - val_loss: 28.2621 - val_MinusLogProbMetric: 28.2621 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 657/1000
2023-10-27 07:43:17.652 
Epoch 657/1000 
	 loss: 28.0382, MinusLogProbMetric: 28.0382, val_loss: 28.1529, val_MinusLogProbMetric: 28.1529

Epoch 657: val_loss improved from 28.15924 to 28.15289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.0382 - MinusLogProbMetric: 28.0382 - val_loss: 28.1529 - val_MinusLogProbMetric: 28.1529 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 658/1000
2023-10-27 07:43:59.818 
Epoch 658/1000 
	 loss: 28.0212, MinusLogProbMetric: 28.0212, val_loss: 28.1730, val_MinusLogProbMetric: 28.1730

Epoch 658: val_loss did not improve from 28.15289
196/196 - 41s - loss: 28.0212 - MinusLogProbMetric: 28.0212 - val_loss: 28.1730 - val_MinusLogProbMetric: 28.1730 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 659/1000
2023-10-27 07:44:42.412 
Epoch 659/1000 
	 loss: 28.0321, MinusLogProbMetric: 28.0321, val_loss: 28.6030, val_MinusLogProbMetric: 28.6030

Epoch 659: val_loss did not improve from 28.15289
196/196 - 43s - loss: 28.0321 - MinusLogProbMetric: 28.0321 - val_loss: 28.6030 - val_MinusLogProbMetric: 28.6030 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 660/1000
2023-10-27 07:45:24.129 
Epoch 660/1000 
	 loss: 28.0442, MinusLogProbMetric: 28.0442, val_loss: 28.1849, val_MinusLogProbMetric: 28.1849

Epoch 660: val_loss did not improve from 28.15289
196/196 - 42s - loss: 28.0442 - MinusLogProbMetric: 28.0442 - val_loss: 28.1849 - val_MinusLogProbMetric: 28.1849 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 661/1000
2023-10-27 07:46:05.755 
Epoch 661/1000 
	 loss: 28.0382, MinusLogProbMetric: 28.0382, val_loss: 28.3884, val_MinusLogProbMetric: 28.3884

Epoch 661: val_loss did not improve from 28.15289
196/196 - 42s - loss: 28.0382 - MinusLogProbMetric: 28.0382 - val_loss: 28.3884 - val_MinusLogProbMetric: 28.3884 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 662/1000
2023-10-27 07:46:47.363 
Epoch 662/1000 
	 loss: 28.0544, MinusLogProbMetric: 28.0544, val_loss: 28.3687, val_MinusLogProbMetric: 28.3687

Epoch 662: val_loss did not improve from 28.15289
196/196 - 42s - loss: 28.0544 - MinusLogProbMetric: 28.0544 - val_loss: 28.3687 - val_MinusLogProbMetric: 28.3687 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 663/1000
2023-10-27 07:47:28.193 
Epoch 663/1000 
	 loss: 28.0480, MinusLogProbMetric: 28.0480, val_loss: 28.2838, val_MinusLogProbMetric: 28.2838

Epoch 663: val_loss did not improve from 28.15289
196/196 - 41s - loss: 28.0480 - MinusLogProbMetric: 28.0480 - val_loss: 28.2838 - val_MinusLogProbMetric: 28.2838 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 664/1000
2023-10-27 07:48:10.465 
Epoch 664/1000 
	 loss: 28.0337, MinusLogProbMetric: 28.0337, val_loss: 28.1835, val_MinusLogProbMetric: 28.1835

Epoch 664: val_loss did not improve from 28.15289
196/196 - 42s - loss: 28.0337 - MinusLogProbMetric: 28.0337 - val_loss: 28.1835 - val_MinusLogProbMetric: 28.1835 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 665/1000
2023-10-27 07:48:52.647 
Epoch 665/1000 
	 loss: 28.0559, MinusLogProbMetric: 28.0559, val_loss: 28.5227, val_MinusLogProbMetric: 28.5227

Epoch 665: val_loss did not improve from 28.15289
196/196 - 42s - loss: 28.0559 - MinusLogProbMetric: 28.0559 - val_loss: 28.5227 - val_MinusLogProbMetric: 28.5227 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 666/1000
2023-10-27 07:49:34.260 
Epoch 666/1000 
	 loss: 28.0428, MinusLogProbMetric: 28.0428, val_loss: 28.5366, val_MinusLogProbMetric: 28.5366

Epoch 666: val_loss did not improve from 28.15289
196/196 - 42s - loss: 28.0428 - MinusLogProbMetric: 28.0428 - val_loss: 28.5366 - val_MinusLogProbMetric: 28.5366 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 667/1000
2023-10-27 07:50:16.541 
Epoch 667/1000 
	 loss: 28.0350, MinusLogProbMetric: 28.0350, val_loss: 28.1495, val_MinusLogProbMetric: 28.1495

Epoch 667: val_loss improved from 28.15289 to 28.14945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 43s - loss: 28.0350 - MinusLogProbMetric: 28.0350 - val_loss: 28.1495 - val_MinusLogProbMetric: 28.1495 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 668/1000
2023-10-27 07:50:58.908 
Epoch 668/1000 
	 loss: 28.0291, MinusLogProbMetric: 28.0291, val_loss: 28.1888, val_MinusLogProbMetric: 28.1888

Epoch 668: val_loss did not improve from 28.14945
196/196 - 42s - loss: 28.0291 - MinusLogProbMetric: 28.0291 - val_loss: 28.1888 - val_MinusLogProbMetric: 28.1888 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 669/1000
2023-10-27 07:51:40.533 
Epoch 669/1000 
	 loss: 28.0522, MinusLogProbMetric: 28.0522, val_loss: 28.1520, val_MinusLogProbMetric: 28.1520

Epoch 669: val_loss did not improve from 28.14945
196/196 - 42s - loss: 28.0522 - MinusLogProbMetric: 28.0522 - val_loss: 28.1520 - val_MinusLogProbMetric: 28.1520 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 670/1000
2023-10-27 07:52:22.439 
Epoch 670/1000 
	 loss: 28.0405, MinusLogProbMetric: 28.0405, val_loss: 28.3043, val_MinusLogProbMetric: 28.3043

Epoch 670: val_loss did not improve from 28.14945
196/196 - 42s - loss: 28.0405 - MinusLogProbMetric: 28.0405 - val_loss: 28.3043 - val_MinusLogProbMetric: 28.3043 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 671/1000
2023-10-27 07:53:04.848 
Epoch 671/1000 
	 loss: 28.0247, MinusLogProbMetric: 28.0247, val_loss: 28.3008, val_MinusLogProbMetric: 28.3008

Epoch 671: val_loss did not improve from 28.14945
196/196 - 42s - loss: 28.0247 - MinusLogProbMetric: 28.0247 - val_loss: 28.3008 - val_MinusLogProbMetric: 28.3008 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 672/1000
2023-10-27 07:53:46.596 
Epoch 672/1000 
	 loss: 28.0633, MinusLogProbMetric: 28.0633, val_loss: 28.2005, val_MinusLogProbMetric: 28.2005

Epoch 672: val_loss did not improve from 28.14945
196/196 - 42s - loss: 28.0633 - MinusLogProbMetric: 28.0633 - val_loss: 28.2005 - val_MinusLogProbMetric: 28.2005 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 673/1000
2023-10-27 07:54:28.920 
Epoch 673/1000 
	 loss: 28.0284, MinusLogProbMetric: 28.0284, val_loss: 28.1895, val_MinusLogProbMetric: 28.1895

Epoch 673: val_loss did not improve from 28.14945
196/196 - 42s - loss: 28.0284 - MinusLogProbMetric: 28.0284 - val_loss: 28.1895 - val_MinusLogProbMetric: 28.1895 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 674/1000
2023-10-27 07:55:10.493 
Epoch 674/1000 
	 loss: 28.0021, MinusLogProbMetric: 28.0021, val_loss: 28.1494, val_MinusLogProbMetric: 28.1494

Epoch 674: val_loss improved from 28.14945 to 28.14942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 28.0021 - MinusLogProbMetric: 28.0021 - val_loss: 28.1494 - val_MinusLogProbMetric: 28.1494 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 675/1000
2023-10-27 07:55:51.365 
Epoch 675/1000 
	 loss: 28.0680, MinusLogProbMetric: 28.0680, val_loss: 28.6417, val_MinusLogProbMetric: 28.6417

Epoch 675: val_loss did not improve from 28.14942
196/196 - 40s - loss: 28.0680 - MinusLogProbMetric: 28.0680 - val_loss: 28.6417 - val_MinusLogProbMetric: 28.6417 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 676/1000
2023-10-27 07:56:28.591 
Epoch 676/1000 
	 loss: 28.0261, MinusLogProbMetric: 28.0261, val_loss: 28.2092, val_MinusLogProbMetric: 28.2092

Epoch 676: val_loss did not improve from 28.14942
196/196 - 37s - loss: 28.0261 - MinusLogProbMetric: 28.0261 - val_loss: 28.2092 - val_MinusLogProbMetric: 28.2092 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 677/1000
2023-10-27 07:57:02.768 
Epoch 677/1000 
	 loss: 28.0139, MinusLogProbMetric: 28.0139, val_loss: 28.2317, val_MinusLogProbMetric: 28.2317

Epoch 677: val_loss did not improve from 28.14942
196/196 - 34s - loss: 28.0139 - MinusLogProbMetric: 28.0139 - val_loss: 28.2317 - val_MinusLogProbMetric: 28.2317 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 678/1000
2023-10-27 07:57:36.454 
Epoch 678/1000 
	 loss: 28.0314, MinusLogProbMetric: 28.0314, val_loss: 28.2052, val_MinusLogProbMetric: 28.2052

Epoch 678: val_loss did not improve from 28.14942
196/196 - 34s - loss: 28.0314 - MinusLogProbMetric: 28.0314 - val_loss: 28.2052 - val_MinusLogProbMetric: 28.2052 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 679/1000
2023-10-27 07:58:16.715 
Epoch 679/1000 
	 loss: 28.0327, MinusLogProbMetric: 28.0327, val_loss: 28.5097, val_MinusLogProbMetric: 28.5097

Epoch 679: val_loss did not improve from 28.14942
196/196 - 40s - loss: 28.0327 - MinusLogProbMetric: 28.0327 - val_loss: 28.5097 - val_MinusLogProbMetric: 28.5097 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 680/1000
2023-10-27 07:58:53.615 
Epoch 680/1000 
	 loss: 28.0283, MinusLogProbMetric: 28.0283, val_loss: 28.1278, val_MinusLogProbMetric: 28.1278

Epoch 680: val_loss improved from 28.14942 to 28.12780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 38s - loss: 28.0283 - MinusLogProbMetric: 28.0283 - val_loss: 28.1278 - val_MinusLogProbMetric: 28.1278 - lr: 2.5000e-04 - 38s/epoch - 191ms/step
Epoch 681/1000
2023-10-27 07:59:27.934 
Epoch 681/1000 
	 loss: 28.0176, MinusLogProbMetric: 28.0176, val_loss: 28.4143, val_MinusLogProbMetric: 28.4143

Epoch 681: val_loss did not improve from 28.12780
196/196 - 34s - loss: 28.0176 - MinusLogProbMetric: 28.0176 - val_loss: 28.4143 - val_MinusLogProbMetric: 28.4143 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 682/1000
2023-10-27 08:00:02.204 
Epoch 682/1000 
	 loss: 28.0582, MinusLogProbMetric: 28.0582, val_loss: 28.1781, val_MinusLogProbMetric: 28.1781

Epoch 682: val_loss did not improve from 28.12780
196/196 - 34s - loss: 28.0582 - MinusLogProbMetric: 28.0582 - val_loss: 28.1781 - val_MinusLogProbMetric: 28.1781 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 683/1000
2023-10-27 08:00:40.620 
Epoch 683/1000 
	 loss: 28.0089, MinusLogProbMetric: 28.0089, val_loss: 28.2932, val_MinusLogProbMetric: 28.2932

Epoch 683: val_loss did not improve from 28.12780
196/196 - 38s - loss: 28.0089 - MinusLogProbMetric: 28.0089 - val_loss: 28.2932 - val_MinusLogProbMetric: 28.2932 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 684/1000
2023-10-27 08:01:15.339 
Epoch 684/1000 
	 loss: 28.0423, MinusLogProbMetric: 28.0423, val_loss: 28.1440, val_MinusLogProbMetric: 28.1440

Epoch 684: val_loss did not improve from 28.12780
196/196 - 35s - loss: 28.0423 - MinusLogProbMetric: 28.0423 - val_loss: 28.1440 - val_MinusLogProbMetric: 28.1440 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 685/1000
2023-10-27 08:01:48.583 
Epoch 685/1000 
	 loss: 28.0176, MinusLogProbMetric: 28.0176, val_loss: 28.8166, val_MinusLogProbMetric: 28.8166

Epoch 685: val_loss did not improve from 28.12780
196/196 - 33s - loss: 28.0176 - MinusLogProbMetric: 28.0176 - val_loss: 28.8166 - val_MinusLogProbMetric: 28.8166 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 686/1000
2023-10-27 08:02:22.397 
Epoch 686/1000 
	 loss: 28.0405, MinusLogProbMetric: 28.0405, val_loss: 28.1747, val_MinusLogProbMetric: 28.1747

Epoch 686: val_loss did not improve from 28.12780
196/196 - 34s - loss: 28.0405 - MinusLogProbMetric: 28.0405 - val_loss: 28.1747 - val_MinusLogProbMetric: 28.1747 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 687/1000
2023-10-27 08:03:02.773 
Epoch 687/1000 
	 loss: 27.9853, MinusLogProbMetric: 27.9853, val_loss: 28.2623, val_MinusLogProbMetric: 28.2623

Epoch 687: val_loss did not improve from 28.12780
196/196 - 40s - loss: 27.9853 - MinusLogProbMetric: 27.9853 - val_loss: 28.2623 - val_MinusLogProbMetric: 28.2623 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 688/1000
2023-10-27 08:03:40.164 
Epoch 688/1000 
	 loss: 28.0272, MinusLogProbMetric: 28.0272, val_loss: 28.3246, val_MinusLogProbMetric: 28.3246

Epoch 688: val_loss did not improve from 28.12780
196/196 - 37s - loss: 28.0272 - MinusLogProbMetric: 28.0272 - val_loss: 28.3246 - val_MinusLogProbMetric: 28.3246 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 689/1000
2023-10-27 08:04:15.183 
Epoch 689/1000 
	 loss: 27.9994, MinusLogProbMetric: 27.9994, val_loss: 28.3242, val_MinusLogProbMetric: 28.3242

Epoch 689: val_loss did not improve from 28.12780
196/196 - 35s - loss: 27.9994 - MinusLogProbMetric: 27.9994 - val_loss: 28.3242 - val_MinusLogProbMetric: 28.3242 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 690/1000
2023-10-27 08:04:48.054 
Epoch 690/1000 
	 loss: 28.0188, MinusLogProbMetric: 28.0188, val_loss: 28.5080, val_MinusLogProbMetric: 28.5080

Epoch 690: val_loss did not improve from 28.12780
196/196 - 33s - loss: 28.0188 - MinusLogProbMetric: 28.0188 - val_loss: 28.5080 - val_MinusLogProbMetric: 28.5080 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 691/1000
2023-10-27 08:05:22.649 
Epoch 691/1000 
	 loss: 28.0233, MinusLogProbMetric: 28.0233, val_loss: 28.3267, val_MinusLogProbMetric: 28.3267

Epoch 691: val_loss did not improve from 28.12780
196/196 - 35s - loss: 28.0233 - MinusLogProbMetric: 28.0233 - val_loss: 28.3267 - val_MinusLogProbMetric: 28.3267 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 692/1000
2023-10-27 08:06:03.360 
Epoch 692/1000 
	 loss: 28.0345, MinusLogProbMetric: 28.0345, val_loss: 28.1639, val_MinusLogProbMetric: 28.1639

Epoch 692: val_loss did not improve from 28.12780
196/196 - 41s - loss: 28.0345 - MinusLogProbMetric: 28.0345 - val_loss: 28.1639 - val_MinusLogProbMetric: 28.1639 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 693/1000
2023-10-27 08:06:37.678 
Epoch 693/1000 
	 loss: 28.0130, MinusLogProbMetric: 28.0130, val_loss: 28.2760, val_MinusLogProbMetric: 28.2760

Epoch 693: val_loss did not improve from 28.12780
196/196 - 34s - loss: 28.0130 - MinusLogProbMetric: 28.0130 - val_loss: 28.2760 - val_MinusLogProbMetric: 28.2760 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 694/1000
2023-10-27 08:07:11.673 
Epoch 694/1000 
	 loss: 28.0126, MinusLogProbMetric: 28.0126, val_loss: 28.2328, val_MinusLogProbMetric: 28.2328

Epoch 694: val_loss did not improve from 28.12780
196/196 - 34s - loss: 28.0126 - MinusLogProbMetric: 28.0126 - val_loss: 28.2328 - val_MinusLogProbMetric: 28.2328 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 695/1000
2023-10-27 08:07:47.600 
Epoch 695/1000 
	 loss: 27.9992, MinusLogProbMetric: 27.9992, val_loss: 28.1530, val_MinusLogProbMetric: 28.1530

Epoch 695: val_loss did not improve from 28.12780
196/196 - 36s - loss: 27.9992 - MinusLogProbMetric: 27.9992 - val_loss: 28.1530 - val_MinusLogProbMetric: 28.1530 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 696/1000
2023-10-27 08:08:27.766 
Epoch 696/1000 
	 loss: 28.0185, MinusLogProbMetric: 28.0185, val_loss: 28.1834, val_MinusLogProbMetric: 28.1834

Epoch 696: val_loss did not improve from 28.12780
196/196 - 40s - loss: 28.0185 - MinusLogProbMetric: 28.0185 - val_loss: 28.1834 - val_MinusLogProbMetric: 28.1834 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 697/1000
2023-10-27 08:09:05.357 
Epoch 697/1000 
	 loss: 28.0205, MinusLogProbMetric: 28.0205, val_loss: 28.2198, val_MinusLogProbMetric: 28.2198

Epoch 697: val_loss did not improve from 28.12780
196/196 - 38s - loss: 28.0205 - MinusLogProbMetric: 28.0205 - val_loss: 28.2198 - val_MinusLogProbMetric: 28.2198 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 698/1000
2023-10-27 08:09:42.075 
Epoch 698/1000 
	 loss: 28.0052, MinusLogProbMetric: 28.0052, val_loss: 28.1445, val_MinusLogProbMetric: 28.1445

Epoch 698: val_loss did not improve from 28.12780
196/196 - 37s - loss: 28.0052 - MinusLogProbMetric: 28.0052 - val_loss: 28.1445 - val_MinusLogProbMetric: 28.1445 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 699/1000
2023-10-27 08:10:17.162 
Epoch 699/1000 
	 loss: 27.9834, MinusLogProbMetric: 27.9834, val_loss: 28.2541, val_MinusLogProbMetric: 28.2541

Epoch 699: val_loss did not improve from 28.12780
196/196 - 35s - loss: 27.9834 - MinusLogProbMetric: 27.9834 - val_loss: 28.2541 - val_MinusLogProbMetric: 28.2541 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 700/1000
2023-10-27 08:10:57.504 
Epoch 700/1000 
	 loss: 28.0005, MinusLogProbMetric: 28.0005, val_loss: 28.1709, val_MinusLogProbMetric: 28.1709

Epoch 700: val_loss did not improve from 28.12780
196/196 - 40s - loss: 28.0005 - MinusLogProbMetric: 28.0005 - val_loss: 28.1709 - val_MinusLogProbMetric: 28.1709 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 701/1000
2023-10-27 08:11:34.795 
Epoch 701/1000 
	 loss: 27.9821, MinusLogProbMetric: 27.9821, val_loss: 28.1671, val_MinusLogProbMetric: 28.1671

Epoch 701: val_loss did not improve from 28.12780
196/196 - 37s - loss: 27.9821 - MinusLogProbMetric: 27.9821 - val_loss: 28.1671 - val_MinusLogProbMetric: 28.1671 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 702/1000
2023-10-27 08:12:11.855 
Epoch 702/1000 
	 loss: 27.9974, MinusLogProbMetric: 27.9974, val_loss: 28.3258, val_MinusLogProbMetric: 28.3258

Epoch 702: val_loss did not improve from 28.12780
196/196 - 37s - loss: 27.9974 - MinusLogProbMetric: 27.9974 - val_loss: 28.3258 - val_MinusLogProbMetric: 28.3258 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 703/1000
2023-10-27 08:12:47.551 
Epoch 703/1000 
	 loss: 28.0229, MinusLogProbMetric: 28.0229, val_loss: 28.3643, val_MinusLogProbMetric: 28.3643

Epoch 703: val_loss did not improve from 28.12780
196/196 - 36s - loss: 28.0229 - MinusLogProbMetric: 28.0229 - val_loss: 28.3643 - val_MinusLogProbMetric: 28.3643 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 704/1000
2023-10-27 08:13:28.332 
Epoch 704/1000 
	 loss: 27.9975, MinusLogProbMetric: 27.9975, val_loss: 28.2417, val_MinusLogProbMetric: 28.2417

Epoch 704: val_loss did not improve from 28.12780
196/196 - 41s - loss: 27.9975 - MinusLogProbMetric: 27.9975 - val_loss: 28.2417 - val_MinusLogProbMetric: 28.2417 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 705/1000
2023-10-27 08:14:08.026 
Epoch 705/1000 
	 loss: 27.9997, MinusLogProbMetric: 27.9997, val_loss: 28.2292, val_MinusLogProbMetric: 28.2292

Epoch 705: val_loss did not improve from 28.12780
196/196 - 40s - loss: 27.9997 - MinusLogProbMetric: 27.9997 - val_loss: 28.2292 - val_MinusLogProbMetric: 28.2292 - lr: 2.5000e-04 - 40s/epoch - 202ms/step
Epoch 706/1000
2023-10-27 08:14:41.627 
Epoch 706/1000 
	 loss: 28.0034, MinusLogProbMetric: 28.0034, val_loss: 28.1868, val_MinusLogProbMetric: 28.1868

Epoch 706: val_loss did not improve from 28.12780
196/196 - 34s - loss: 28.0034 - MinusLogProbMetric: 28.0034 - val_loss: 28.1868 - val_MinusLogProbMetric: 28.1868 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 707/1000
2023-10-27 08:15:15.096 
Epoch 707/1000 
	 loss: 28.0189, MinusLogProbMetric: 28.0189, val_loss: 28.2714, val_MinusLogProbMetric: 28.2714

Epoch 707: val_loss did not improve from 28.12780
196/196 - 33s - loss: 28.0189 - MinusLogProbMetric: 28.0189 - val_loss: 28.2714 - val_MinusLogProbMetric: 28.2714 - lr: 2.5000e-04 - 33s/epoch - 171ms/step
Epoch 708/1000
2023-10-27 08:15:51.913 
Epoch 708/1000 
	 loss: 28.0606, MinusLogProbMetric: 28.0606, val_loss: 28.1879, val_MinusLogProbMetric: 28.1879

Epoch 708: val_loss did not improve from 28.12780
196/196 - 37s - loss: 28.0606 - MinusLogProbMetric: 28.0606 - val_loss: 28.1879 - val_MinusLogProbMetric: 28.1879 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 709/1000
2023-10-27 08:16:33.223 
Epoch 709/1000 
	 loss: 28.0082, MinusLogProbMetric: 28.0082, val_loss: 28.3285, val_MinusLogProbMetric: 28.3285

Epoch 709: val_loss did not improve from 28.12780
196/196 - 41s - loss: 28.0082 - MinusLogProbMetric: 28.0082 - val_loss: 28.3285 - val_MinusLogProbMetric: 28.3285 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 710/1000
2023-10-27 08:17:07.679 
Epoch 710/1000 
	 loss: 28.0321, MinusLogProbMetric: 28.0321, val_loss: 28.1984, val_MinusLogProbMetric: 28.1984

Epoch 710: val_loss did not improve from 28.12780
196/196 - 34s - loss: 28.0321 - MinusLogProbMetric: 28.0321 - val_loss: 28.1984 - val_MinusLogProbMetric: 28.1984 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 711/1000
2023-10-27 08:17:46.604 
Epoch 711/1000 
	 loss: 28.0282, MinusLogProbMetric: 28.0282, val_loss: 28.2099, val_MinusLogProbMetric: 28.2099

Epoch 711: val_loss did not improve from 28.12780
196/196 - 39s - loss: 28.0282 - MinusLogProbMetric: 28.0282 - val_loss: 28.2099 - val_MinusLogProbMetric: 28.2099 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 712/1000
2023-10-27 08:18:25.545 
Epoch 712/1000 
	 loss: 28.0149, MinusLogProbMetric: 28.0149, val_loss: 28.2130, val_MinusLogProbMetric: 28.2130

Epoch 712: val_loss did not improve from 28.12780
196/196 - 39s - loss: 28.0149 - MinusLogProbMetric: 28.0149 - val_loss: 28.2130 - val_MinusLogProbMetric: 28.2130 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 713/1000
2023-10-27 08:19:05.908 
Epoch 713/1000 
	 loss: 28.0086, MinusLogProbMetric: 28.0086, val_loss: 28.3640, val_MinusLogProbMetric: 28.3640

Epoch 713: val_loss did not improve from 28.12780
196/196 - 40s - loss: 28.0086 - MinusLogProbMetric: 28.0086 - val_loss: 28.3640 - val_MinusLogProbMetric: 28.3640 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 714/1000
2023-10-27 08:19:45.984 
Epoch 714/1000 
	 loss: 28.0475, MinusLogProbMetric: 28.0475, val_loss: 28.1997, val_MinusLogProbMetric: 28.1997

Epoch 714: val_loss did not improve from 28.12780
196/196 - 40s - loss: 28.0475 - MinusLogProbMetric: 28.0475 - val_loss: 28.1997 - val_MinusLogProbMetric: 28.1997 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 715/1000
2023-10-27 08:20:22.870 
Epoch 715/1000 
	 loss: 28.0027, MinusLogProbMetric: 28.0027, val_loss: 28.2868, val_MinusLogProbMetric: 28.2868

Epoch 715: val_loss did not improve from 28.12780
196/196 - 37s - loss: 28.0027 - MinusLogProbMetric: 28.0027 - val_loss: 28.2868 - val_MinusLogProbMetric: 28.2868 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 716/1000
2023-10-27 08:21:00.839 
Epoch 716/1000 
	 loss: 28.0489, MinusLogProbMetric: 28.0489, val_loss: 28.2164, val_MinusLogProbMetric: 28.2164

Epoch 716: val_loss did not improve from 28.12780
196/196 - 38s - loss: 28.0489 - MinusLogProbMetric: 28.0489 - val_loss: 28.2164 - val_MinusLogProbMetric: 28.2164 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 717/1000
2023-10-27 08:21:38.200 
Epoch 717/1000 
	 loss: 28.0387, MinusLogProbMetric: 28.0387, val_loss: 28.2868, val_MinusLogProbMetric: 28.2868

Epoch 717: val_loss did not improve from 28.12780
196/196 - 37s - loss: 28.0387 - MinusLogProbMetric: 28.0387 - val_loss: 28.2868 - val_MinusLogProbMetric: 28.2868 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 718/1000
2023-10-27 08:22:19.969 
Epoch 718/1000 
	 loss: 28.0072, MinusLogProbMetric: 28.0072, val_loss: 28.2894, val_MinusLogProbMetric: 28.2894

Epoch 718: val_loss did not improve from 28.12780
196/196 - 42s - loss: 28.0072 - MinusLogProbMetric: 28.0072 - val_loss: 28.2894 - val_MinusLogProbMetric: 28.2894 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 719/1000
2023-10-27 08:22:57.696 
Epoch 719/1000 
	 loss: 28.0281, MinusLogProbMetric: 28.0281, val_loss: 28.1520, val_MinusLogProbMetric: 28.1520

Epoch 719: val_loss did not improve from 28.12780
196/196 - 38s - loss: 28.0281 - MinusLogProbMetric: 28.0281 - val_loss: 28.1520 - val_MinusLogProbMetric: 28.1520 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 720/1000
2023-10-27 08:23:34.862 
Epoch 720/1000 
	 loss: 28.0232, MinusLogProbMetric: 28.0232, val_loss: 28.1579, val_MinusLogProbMetric: 28.1579

Epoch 720: val_loss did not improve from 28.12780
196/196 - 37s - loss: 28.0232 - MinusLogProbMetric: 28.0232 - val_loss: 28.1579 - val_MinusLogProbMetric: 28.1579 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 721/1000
2023-10-27 08:24:12.825 
Epoch 721/1000 
	 loss: 28.0181, MinusLogProbMetric: 28.0181, val_loss: 28.3342, val_MinusLogProbMetric: 28.3342

Epoch 721: val_loss did not improve from 28.12780
196/196 - 38s - loss: 28.0181 - MinusLogProbMetric: 28.0181 - val_loss: 28.3342 - val_MinusLogProbMetric: 28.3342 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 722/1000
2023-10-27 08:24:51.524 
Epoch 722/1000 
	 loss: 28.0651, MinusLogProbMetric: 28.0651, val_loss: 28.3984, val_MinusLogProbMetric: 28.3984

Epoch 722: val_loss did not improve from 28.12780
196/196 - 39s - loss: 28.0651 - MinusLogProbMetric: 28.0651 - val_loss: 28.3984 - val_MinusLogProbMetric: 28.3984 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 723/1000
2023-10-27 08:25:32.964 
Epoch 723/1000 
	 loss: 28.0054, MinusLogProbMetric: 28.0054, val_loss: 28.3008, val_MinusLogProbMetric: 28.3008

Epoch 723: val_loss did not improve from 28.12780
196/196 - 41s - loss: 28.0054 - MinusLogProbMetric: 28.0054 - val_loss: 28.3008 - val_MinusLogProbMetric: 28.3008 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 724/1000
2023-10-27 08:26:09.182 
Epoch 724/1000 
	 loss: 28.0517, MinusLogProbMetric: 28.0517, val_loss: 28.1888, val_MinusLogProbMetric: 28.1888

Epoch 724: val_loss did not improve from 28.12780
196/196 - 36s - loss: 28.0517 - MinusLogProbMetric: 28.0517 - val_loss: 28.1888 - val_MinusLogProbMetric: 28.1888 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 725/1000
2023-10-27 08:26:48.759 
Epoch 725/1000 
	 loss: 27.9956, MinusLogProbMetric: 27.9956, val_loss: 28.1602, val_MinusLogProbMetric: 28.1602

Epoch 725: val_loss did not improve from 28.12780
196/196 - 40s - loss: 27.9956 - MinusLogProbMetric: 27.9956 - val_loss: 28.1602 - val_MinusLogProbMetric: 28.1602 - lr: 2.5000e-04 - 40s/epoch - 202ms/step
Epoch 726/1000
2023-10-27 08:27:27.043 
Epoch 726/1000 
	 loss: 28.0221, MinusLogProbMetric: 28.0221, val_loss: 28.2430, val_MinusLogProbMetric: 28.2430

Epoch 726: val_loss did not improve from 28.12780
196/196 - 38s - loss: 28.0221 - MinusLogProbMetric: 28.0221 - val_loss: 28.2430 - val_MinusLogProbMetric: 28.2430 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 727/1000
2023-10-27 08:28:08.437 
Epoch 727/1000 
	 loss: 27.9957, MinusLogProbMetric: 27.9957, val_loss: 28.3689, val_MinusLogProbMetric: 28.3689

Epoch 727: val_loss did not improve from 28.12780
196/196 - 41s - loss: 27.9957 - MinusLogProbMetric: 27.9957 - val_loss: 28.3689 - val_MinusLogProbMetric: 28.3689 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 728/1000
2023-10-27 08:28:47.827 
Epoch 728/1000 
	 loss: 28.0059, MinusLogProbMetric: 28.0059, val_loss: 28.2169, val_MinusLogProbMetric: 28.2169

Epoch 728: val_loss did not improve from 28.12780
196/196 - 39s - loss: 28.0059 - MinusLogProbMetric: 28.0059 - val_loss: 28.2169 - val_MinusLogProbMetric: 28.2169 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 729/1000
2023-10-27 08:29:23.675 
Epoch 729/1000 
	 loss: 28.0343, MinusLogProbMetric: 28.0343, val_loss: 28.2749, val_MinusLogProbMetric: 28.2749

Epoch 729: val_loss did not improve from 28.12780
196/196 - 36s - loss: 28.0343 - MinusLogProbMetric: 28.0343 - val_loss: 28.2749 - val_MinusLogProbMetric: 28.2749 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 730/1000
2023-10-27 08:30:03.340 
Epoch 730/1000 
	 loss: 27.9937, MinusLogProbMetric: 27.9937, val_loss: 28.0933, val_MinusLogProbMetric: 28.0933

Epoch 730: val_loss improved from 28.12780 to 28.09325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 40s - loss: 27.9937 - MinusLogProbMetric: 27.9937 - val_loss: 28.0933 - val_MinusLogProbMetric: 28.0933 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 731/1000
2023-10-27 08:30:43.638 
Epoch 731/1000 
	 loss: 27.9906, MinusLogProbMetric: 27.9906, val_loss: 28.1561, val_MinusLogProbMetric: 28.1561

Epoch 731: val_loss did not improve from 28.09325
196/196 - 40s - loss: 27.9906 - MinusLogProbMetric: 27.9906 - val_loss: 28.1561 - val_MinusLogProbMetric: 28.1561 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 732/1000
2023-10-27 08:31:24.020 
Epoch 732/1000 
	 loss: 28.0190, MinusLogProbMetric: 28.0190, val_loss: 28.1799, val_MinusLogProbMetric: 28.1799

Epoch 732: val_loss did not improve from 28.09325
196/196 - 40s - loss: 28.0190 - MinusLogProbMetric: 28.0190 - val_loss: 28.1799 - val_MinusLogProbMetric: 28.1799 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 733/1000
2023-10-27 08:31:57.460 
Epoch 733/1000 
	 loss: 28.0185, MinusLogProbMetric: 28.0185, val_loss: 28.2791, val_MinusLogProbMetric: 28.2791

Epoch 733: val_loss did not improve from 28.09325
196/196 - 33s - loss: 28.0185 - MinusLogProbMetric: 28.0185 - val_loss: 28.2791 - val_MinusLogProbMetric: 28.2791 - lr: 2.5000e-04 - 33s/epoch - 171ms/step
Epoch 734/1000
2023-10-27 08:32:30.627 
Epoch 734/1000 
	 loss: 28.0098, MinusLogProbMetric: 28.0098, val_loss: 28.2677, val_MinusLogProbMetric: 28.2677

Epoch 734: val_loss did not improve from 28.09325
196/196 - 33s - loss: 28.0098 - MinusLogProbMetric: 28.0098 - val_loss: 28.2677 - val_MinusLogProbMetric: 28.2677 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 735/1000
2023-10-27 08:33:06.195 
Epoch 735/1000 
	 loss: 28.0250, MinusLogProbMetric: 28.0250, val_loss: 28.2607, val_MinusLogProbMetric: 28.2607

Epoch 735: val_loss did not improve from 28.09325
196/196 - 36s - loss: 28.0250 - MinusLogProbMetric: 28.0250 - val_loss: 28.2607 - val_MinusLogProbMetric: 28.2607 - lr: 2.5000e-04 - 36s/epoch - 181ms/step
Epoch 736/1000
2023-10-27 08:33:44.213 
Epoch 736/1000 
	 loss: 28.0152, MinusLogProbMetric: 28.0152, val_loss: 28.1965, val_MinusLogProbMetric: 28.1965

Epoch 736: val_loss did not improve from 28.09325
196/196 - 38s - loss: 28.0152 - MinusLogProbMetric: 28.0152 - val_loss: 28.1965 - val_MinusLogProbMetric: 28.1965 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 737/1000
2023-10-27 08:34:17.092 
Epoch 737/1000 
	 loss: 28.0105, MinusLogProbMetric: 28.0105, val_loss: 28.2716, val_MinusLogProbMetric: 28.2716

Epoch 737: val_loss did not improve from 28.09325
196/196 - 33s - loss: 28.0105 - MinusLogProbMetric: 28.0105 - val_loss: 28.2716 - val_MinusLogProbMetric: 28.2716 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 738/1000
2023-10-27 08:34:53.848 
Epoch 738/1000 
	 loss: 27.9930, MinusLogProbMetric: 27.9930, val_loss: 28.3413, val_MinusLogProbMetric: 28.3413

Epoch 738: val_loss did not improve from 28.09325
196/196 - 37s - loss: 27.9930 - MinusLogProbMetric: 27.9930 - val_loss: 28.3413 - val_MinusLogProbMetric: 28.3413 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 739/1000
2023-10-27 08:35:28.399 
Epoch 739/1000 
	 loss: 28.0054, MinusLogProbMetric: 28.0054, val_loss: 28.1047, val_MinusLogProbMetric: 28.1047

Epoch 739: val_loss did not improve from 28.09325
196/196 - 35s - loss: 28.0054 - MinusLogProbMetric: 28.0054 - val_loss: 28.1047 - val_MinusLogProbMetric: 28.1047 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 740/1000
2023-10-27 08:36:10.743 
Epoch 740/1000 
	 loss: 27.9839, MinusLogProbMetric: 27.9839, val_loss: 28.4218, val_MinusLogProbMetric: 28.4218

Epoch 740: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.9839 - MinusLogProbMetric: 27.9839 - val_loss: 28.4218 - val_MinusLogProbMetric: 28.4218 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 741/1000
2023-10-27 08:36:50.861 
Epoch 741/1000 
	 loss: 28.0284, MinusLogProbMetric: 28.0284, val_loss: 28.5318, val_MinusLogProbMetric: 28.5318

Epoch 741: val_loss did not improve from 28.09325
196/196 - 40s - loss: 28.0284 - MinusLogProbMetric: 28.0284 - val_loss: 28.5318 - val_MinusLogProbMetric: 28.5318 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 742/1000
2023-10-27 08:37:28.620 
Epoch 742/1000 
	 loss: 28.0034, MinusLogProbMetric: 28.0034, val_loss: 28.4206, val_MinusLogProbMetric: 28.4206

Epoch 742: val_loss did not improve from 28.09325
196/196 - 38s - loss: 28.0034 - MinusLogProbMetric: 28.0034 - val_loss: 28.4206 - val_MinusLogProbMetric: 28.4206 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 743/1000
2023-10-27 08:38:06.218 
Epoch 743/1000 
	 loss: 28.0480, MinusLogProbMetric: 28.0480, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 743: val_loss did not improve from 28.09325
196/196 - 38s - loss: 28.0480 - MinusLogProbMetric: 28.0480 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 744/1000
2023-10-27 08:38:44.932 
Epoch 744/1000 
	 loss: 27.9744, MinusLogProbMetric: 27.9744, val_loss: 28.1414, val_MinusLogProbMetric: 28.1414

Epoch 744: val_loss did not improve from 28.09325
196/196 - 39s - loss: 27.9744 - MinusLogProbMetric: 27.9744 - val_loss: 28.1414 - val_MinusLogProbMetric: 28.1414 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 745/1000
2023-10-27 08:39:27.447 
Epoch 745/1000 
	 loss: 27.9961, MinusLogProbMetric: 27.9961, val_loss: 28.3332, val_MinusLogProbMetric: 28.3332

Epoch 745: val_loss did not improve from 28.09325
196/196 - 43s - loss: 27.9961 - MinusLogProbMetric: 27.9961 - val_loss: 28.3332 - val_MinusLogProbMetric: 28.3332 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 746/1000
2023-10-27 08:40:07.525 
Epoch 746/1000 
	 loss: 27.9977, MinusLogProbMetric: 27.9977, val_loss: 28.1751, val_MinusLogProbMetric: 28.1751

Epoch 746: val_loss did not improve from 28.09325
196/196 - 40s - loss: 27.9977 - MinusLogProbMetric: 27.9977 - val_loss: 28.1751 - val_MinusLogProbMetric: 28.1751 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 747/1000
2023-10-27 08:40:42.622 
Epoch 747/1000 
	 loss: 27.9955, MinusLogProbMetric: 27.9955, val_loss: 28.3440, val_MinusLogProbMetric: 28.3440

Epoch 747: val_loss did not improve from 28.09325
196/196 - 35s - loss: 27.9955 - MinusLogProbMetric: 27.9955 - val_loss: 28.3440 - val_MinusLogProbMetric: 28.3440 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 748/1000
2023-10-27 08:41:17.836 
Epoch 748/1000 
	 loss: 28.0328, MinusLogProbMetric: 28.0328, val_loss: 28.1607, val_MinusLogProbMetric: 28.1607

Epoch 748: val_loss did not improve from 28.09325
196/196 - 35s - loss: 28.0328 - MinusLogProbMetric: 28.0328 - val_loss: 28.1607 - val_MinusLogProbMetric: 28.1607 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 749/1000
2023-10-27 08:41:58.147 
Epoch 749/1000 
	 loss: 27.9873, MinusLogProbMetric: 27.9873, val_loss: 28.1836, val_MinusLogProbMetric: 28.1836

Epoch 749: val_loss did not improve from 28.09325
196/196 - 40s - loss: 27.9873 - MinusLogProbMetric: 27.9873 - val_loss: 28.1836 - val_MinusLogProbMetric: 28.1836 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 750/1000
2023-10-27 08:42:34.644 
Epoch 750/1000 
	 loss: 27.9855, MinusLogProbMetric: 27.9855, val_loss: 28.1681, val_MinusLogProbMetric: 28.1681

Epoch 750: val_loss did not improve from 28.09325
196/196 - 36s - loss: 27.9855 - MinusLogProbMetric: 27.9855 - val_loss: 28.1681 - val_MinusLogProbMetric: 28.1681 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 751/1000
2023-10-27 08:43:11.593 
Epoch 751/1000 
	 loss: 28.0142, MinusLogProbMetric: 28.0142, val_loss: 28.1876, val_MinusLogProbMetric: 28.1876

Epoch 751: val_loss did not improve from 28.09325
196/196 - 37s - loss: 28.0142 - MinusLogProbMetric: 28.0142 - val_loss: 28.1876 - val_MinusLogProbMetric: 28.1876 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 752/1000
2023-10-27 08:43:46.746 
Epoch 752/1000 
	 loss: 27.9812, MinusLogProbMetric: 27.9812, val_loss: 28.2646, val_MinusLogProbMetric: 28.2646

Epoch 752: val_loss did not improve from 28.09325
196/196 - 35s - loss: 27.9812 - MinusLogProbMetric: 27.9812 - val_loss: 28.2646 - val_MinusLogProbMetric: 28.2646 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 753/1000
2023-10-27 08:44:26.949 
Epoch 753/1000 
	 loss: 28.0104, MinusLogProbMetric: 28.0104, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 753: val_loss did not improve from 28.09325
196/196 - 40s - loss: 28.0104 - MinusLogProbMetric: 28.0104 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 754/1000
2023-10-27 08:45:06.183 
Epoch 754/1000 
	 loss: 28.0154, MinusLogProbMetric: 28.0154, val_loss: 28.3057, val_MinusLogProbMetric: 28.3057

Epoch 754: val_loss did not improve from 28.09325
196/196 - 39s - loss: 28.0154 - MinusLogProbMetric: 28.0154 - val_loss: 28.3057 - val_MinusLogProbMetric: 28.3057 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 755/1000
2023-10-27 08:45:44.575 
Epoch 755/1000 
	 loss: 27.9727, MinusLogProbMetric: 27.9727, val_loss: 28.2148, val_MinusLogProbMetric: 28.2148

Epoch 755: val_loss did not improve from 28.09325
196/196 - 38s - loss: 27.9727 - MinusLogProbMetric: 27.9727 - val_loss: 28.2148 - val_MinusLogProbMetric: 28.2148 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 756/1000
2023-10-27 08:46:21.005 
Epoch 756/1000 
	 loss: 28.0151, MinusLogProbMetric: 28.0151, val_loss: 28.2692, val_MinusLogProbMetric: 28.2692

Epoch 756: val_loss did not improve from 28.09325
196/196 - 36s - loss: 28.0151 - MinusLogProbMetric: 28.0151 - val_loss: 28.2692 - val_MinusLogProbMetric: 28.2692 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 757/1000
2023-10-27 08:46:59.206 
Epoch 757/1000 
	 loss: 28.0155, MinusLogProbMetric: 28.0155, val_loss: 28.3572, val_MinusLogProbMetric: 28.3572

Epoch 757: val_loss did not improve from 28.09325
196/196 - 38s - loss: 28.0155 - MinusLogProbMetric: 28.0155 - val_loss: 28.3572 - val_MinusLogProbMetric: 28.3572 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 758/1000
2023-10-27 08:47:39.302 
Epoch 758/1000 
	 loss: 28.0024, MinusLogProbMetric: 28.0024, val_loss: 28.2086, val_MinusLogProbMetric: 28.2086

Epoch 758: val_loss did not improve from 28.09325
196/196 - 40s - loss: 28.0024 - MinusLogProbMetric: 28.0024 - val_loss: 28.2086 - val_MinusLogProbMetric: 28.2086 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 759/1000
2023-10-27 08:48:21.085 
Epoch 759/1000 
	 loss: 27.9975, MinusLogProbMetric: 27.9975, val_loss: 28.1754, val_MinusLogProbMetric: 28.1754

Epoch 759: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.9975 - MinusLogProbMetric: 27.9975 - val_loss: 28.1754 - val_MinusLogProbMetric: 28.1754 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 760/1000
2023-10-27 08:48:54.376 
Epoch 760/1000 
	 loss: 27.9960, MinusLogProbMetric: 27.9960, val_loss: 28.1322, val_MinusLogProbMetric: 28.1322

Epoch 760: val_loss did not improve from 28.09325
196/196 - 33s - loss: 27.9960 - MinusLogProbMetric: 27.9960 - val_loss: 28.1322 - val_MinusLogProbMetric: 28.1322 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 761/1000
2023-10-27 08:49:28.419 
Epoch 761/1000 
	 loss: 27.9643, MinusLogProbMetric: 27.9643, val_loss: 28.1821, val_MinusLogProbMetric: 28.1821

Epoch 761: val_loss did not improve from 28.09325
196/196 - 34s - loss: 27.9643 - MinusLogProbMetric: 27.9643 - val_loss: 28.1821 - val_MinusLogProbMetric: 28.1821 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 762/1000
2023-10-27 08:50:06.795 
Epoch 762/1000 
	 loss: 28.0053, MinusLogProbMetric: 28.0053, val_loss: 28.2343, val_MinusLogProbMetric: 28.2343

Epoch 762: val_loss did not improve from 28.09325
196/196 - 38s - loss: 28.0053 - MinusLogProbMetric: 28.0053 - val_loss: 28.2343 - val_MinusLogProbMetric: 28.2343 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 763/1000
2023-10-27 08:50:46.186 
Epoch 763/1000 
	 loss: 27.9767, MinusLogProbMetric: 27.9767, val_loss: 28.1483, val_MinusLogProbMetric: 28.1483

Epoch 763: val_loss did not improve from 28.09325
196/196 - 39s - loss: 27.9767 - MinusLogProbMetric: 27.9767 - val_loss: 28.1483 - val_MinusLogProbMetric: 28.1483 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 764/1000
2023-10-27 08:51:27.812 
Epoch 764/1000 
	 loss: 27.9948, MinusLogProbMetric: 27.9948, val_loss: 28.2326, val_MinusLogProbMetric: 28.2326

Epoch 764: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.9948 - MinusLogProbMetric: 27.9948 - val_loss: 28.2326 - val_MinusLogProbMetric: 28.2326 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 765/1000
2023-10-27 08:52:10.346 
Epoch 765/1000 
	 loss: 27.9955, MinusLogProbMetric: 27.9955, val_loss: 28.1917, val_MinusLogProbMetric: 28.1917

Epoch 765: val_loss did not improve from 28.09325
196/196 - 43s - loss: 27.9955 - MinusLogProbMetric: 27.9955 - val_loss: 28.1917 - val_MinusLogProbMetric: 28.1917 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 766/1000
2023-10-27 08:52:51.015 
Epoch 766/1000 
	 loss: 27.9967, MinusLogProbMetric: 27.9967, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 766: val_loss did not improve from 28.09325
196/196 - 41s - loss: 27.9967 - MinusLogProbMetric: 27.9967 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 767/1000
2023-10-27 08:53:33.009 
Epoch 767/1000 
	 loss: 27.9944, MinusLogProbMetric: 27.9944, val_loss: 28.1865, val_MinusLogProbMetric: 28.1865

Epoch 767: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.9944 - MinusLogProbMetric: 27.9944 - val_loss: 28.1865 - val_MinusLogProbMetric: 28.1865 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 768/1000
2023-10-27 08:54:15.356 
Epoch 768/1000 
	 loss: 28.0096, MinusLogProbMetric: 28.0096, val_loss: 28.2872, val_MinusLogProbMetric: 28.2872

Epoch 768: val_loss did not improve from 28.09325
196/196 - 42s - loss: 28.0096 - MinusLogProbMetric: 28.0096 - val_loss: 28.2872 - val_MinusLogProbMetric: 28.2872 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 769/1000
2023-10-27 08:54:56.889 
Epoch 769/1000 
	 loss: 27.9752, MinusLogProbMetric: 27.9752, val_loss: 28.5517, val_MinusLogProbMetric: 28.5517

Epoch 769: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.9752 - MinusLogProbMetric: 27.9752 - val_loss: 28.5517 - val_MinusLogProbMetric: 28.5517 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 770/1000
2023-10-27 08:55:37.063 
Epoch 770/1000 
	 loss: 28.0251, MinusLogProbMetric: 28.0251, val_loss: 28.2896, val_MinusLogProbMetric: 28.2896

Epoch 770: val_loss did not improve from 28.09325
196/196 - 40s - loss: 28.0251 - MinusLogProbMetric: 28.0251 - val_loss: 28.2896 - val_MinusLogProbMetric: 28.2896 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 771/1000
2023-10-27 08:56:19.022 
Epoch 771/1000 
	 loss: 27.9964, MinusLogProbMetric: 27.9964, val_loss: 28.2136, val_MinusLogProbMetric: 28.2136

Epoch 771: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.9964 - MinusLogProbMetric: 27.9964 - val_loss: 28.2136 - val_MinusLogProbMetric: 28.2136 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 772/1000
2023-10-27 08:56:59.996 
Epoch 772/1000 
	 loss: 28.0236, MinusLogProbMetric: 28.0236, val_loss: 28.2895, val_MinusLogProbMetric: 28.2895

Epoch 772: val_loss did not improve from 28.09325
196/196 - 41s - loss: 28.0236 - MinusLogProbMetric: 28.0236 - val_loss: 28.2895 - val_MinusLogProbMetric: 28.2895 - lr: 2.5000e-04 - 41s/epoch - 209ms/step
Epoch 773/1000
2023-10-27 08:57:41.756 
Epoch 773/1000 
	 loss: 27.9948, MinusLogProbMetric: 27.9948, val_loss: 28.2378, val_MinusLogProbMetric: 28.2378

Epoch 773: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.9948 - MinusLogProbMetric: 27.9948 - val_loss: 28.2378 - val_MinusLogProbMetric: 28.2378 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 774/1000
2023-10-27 08:58:22.279 
Epoch 774/1000 
	 loss: 27.9900, MinusLogProbMetric: 27.9900, val_loss: 28.1304, val_MinusLogProbMetric: 28.1304

Epoch 774: val_loss did not improve from 28.09325
196/196 - 41s - loss: 27.9900 - MinusLogProbMetric: 27.9900 - val_loss: 28.1304 - val_MinusLogProbMetric: 28.1304 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 775/1000
2023-10-27 08:59:02.598 
Epoch 775/1000 
	 loss: 27.9740, MinusLogProbMetric: 27.9740, val_loss: 28.2044, val_MinusLogProbMetric: 28.2044

Epoch 775: val_loss did not improve from 28.09325
196/196 - 40s - loss: 27.9740 - MinusLogProbMetric: 27.9740 - val_loss: 28.2044 - val_MinusLogProbMetric: 28.2044 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 776/1000
2023-10-27 08:59:44.169 
Epoch 776/1000 
	 loss: 27.9937, MinusLogProbMetric: 27.9937, val_loss: 28.3214, val_MinusLogProbMetric: 28.3214

Epoch 776: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.9937 - MinusLogProbMetric: 27.9937 - val_loss: 28.3214 - val_MinusLogProbMetric: 28.3214 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 777/1000
2023-10-27 09:00:25.904 
Epoch 777/1000 
	 loss: 28.0046, MinusLogProbMetric: 28.0046, val_loss: 28.4202, val_MinusLogProbMetric: 28.4202

Epoch 777: val_loss did not improve from 28.09325
196/196 - 42s - loss: 28.0046 - MinusLogProbMetric: 28.0046 - val_loss: 28.4202 - val_MinusLogProbMetric: 28.4202 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 778/1000
2023-10-27 09:01:06.466 
Epoch 778/1000 
	 loss: 27.9859, MinusLogProbMetric: 27.9859, val_loss: 28.1923, val_MinusLogProbMetric: 28.1923

Epoch 778: val_loss did not improve from 28.09325
196/196 - 41s - loss: 27.9859 - MinusLogProbMetric: 27.9859 - val_loss: 28.1923 - val_MinusLogProbMetric: 28.1923 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 779/1000
2023-10-27 09:01:47.721 
Epoch 779/1000 
	 loss: 28.0055, MinusLogProbMetric: 28.0055, val_loss: 28.2464, val_MinusLogProbMetric: 28.2464

Epoch 779: val_loss did not improve from 28.09325
196/196 - 41s - loss: 28.0055 - MinusLogProbMetric: 28.0055 - val_loss: 28.2464 - val_MinusLogProbMetric: 28.2464 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 780/1000
2023-10-27 09:02:29.473 
Epoch 780/1000 
	 loss: 28.0038, MinusLogProbMetric: 28.0038, val_loss: 28.1750, val_MinusLogProbMetric: 28.1750

Epoch 780: val_loss did not improve from 28.09325
196/196 - 42s - loss: 28.0038 - MinusLogProbMetric: 28.0038 - val_loss: 28.1750 - val_MinusLogProbMetric: 28.1750 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 781/1000
2023-10-27 09:03:11.942 
Epoch 781/1000 
	 loss: 27.8650, MinusLogProbMetric: 27.8650, val_loss: 28.2053, val_MinusLogProbMetric: 28.2053

Epoch 781: val_loss did not improve from 28.09325
196/196 - 42s - loss: 27.8650 - MinusLogProbMetric: 27.8650 - val_loss: 28.2053 - val_MinusLogProbMetric: 28.2053 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 782/1000
2023-10-27 09:03:53.164 
Epoch 782/1000 
	 loss: 27.8613, MinusLogProbMetric: 27.8613, val_loss: 28.1109, val_MinusLogProbMetric: 28.1109

Epoch 782: val_loss did not improve from 28.09325
196/196 - 41s - loss: 27.8613 - MinusLogProbMetric: 27.8613 - val_loss: 28.1109 - val_MinusLogProbMetric: 28.1109 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 783/1000
2023-10-27 09:04:34.354 
Epoch 783/1000 
	 loss: 27.8655, MinusLogProbMetric: 27.8655, val_loss: 28.1664, val_MinusLogProbMetric: 28.1664

Epoch 783: val_loss did not improve from 28.09325
196/196 - 41s - loss: 27.8655 - MinusLogProbMetric: 27.8655 - val_loss: 28.1664 - val_MinusLogProbMetric: 28.1664 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 784/1000
2023-10-27 09:05:15.226 
Epoch 784/1000 
	 loss: 27.8653, MinusLogProbMetric: 27.8653, val_loss: 28.0732, val_MinusLogProbMetric: 28.0732

Epoch 784: val_loss improved from 28.09325 to 28.07325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 27.8653 - MinusLogProbMetric: 27.8653 - val_loss: 28.0732 - val_MinusLogProbMetric: 28.0732 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 785/1000
2023-10-27 09:05:57.109 
Epoch 785/1000 
	 loss: 27.8664, MinusLogProbMetric: 27.8664, val_loss: 28.0587, val_MinusLogProbMetric: 28.0587

Epoch 785: val_loss improved from 28.07325 to 28.05870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 27.8664 - MinusLogProbMetric: 27.8664 - val_loss: 28.0587 - val_MinusLogProbMetric: 28.0587 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 786/1000
2023-10-27 09:06:39.010 
Epoch 786/1000 
	 loss: 27.8595, MinusLogProbMetric: 27.8595, val_loss: 28.0866, val_MinusLogProbMetric: 28.0866

Epoch 786: val_loss did not improve from 28.05870
196/196 - 41s - loss: 27.8595 - MinusLogProbMetric: 27.8595 - val_loss: 28.0866 - val_MinusLogProbMetric: 28.0866 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 787/1000
2023-10-27 09:07:19.950 
Epoch 787/1000 
	 loss: 27.8510, MinusLogProbMetric: 27.8510, val_loss: 28.0614, val_MinusLogProbMetric: 28.0614

Epoch 787: val_loss did not improve from 28.05870
196/196 - 41s - loss: 27.8510 - MinusLogProbMetric: 27.8510 - val_loss: 28.0614 - val_MinusLogProbMetric: 28.0614 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 788/1000
2023-10-27 09:08:02.501 
Epoch 788/1000 
	 loss: 27.8520, MinusLogProbMetric: 27.8520, val_loss: 28.0671, val_MinusLogProbMetric: 28.0671

Epoch 788: val_loss did not improve from 28.05870
196/196 - 43s - loss: 27.8520 - MinusLogProbMetric: 27.8520 - val_loss: 28.0671 - val_MinusLogProbMetric: 28.0671 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 789/1000
2023-10-27 09:08:44.244 
Epoch 789/1000 
	 loss: 27.8458, MinusLogProbMetric: 27.8458, val_loss: 28.1320, val_MinusLogProbMetric: 28.1320

Epoch 789: val_loss did not improve from 28.05870
196/196 - 42s - loss: 27.8458 - MinusLogProbMetric: 27.8458 - val_loss: 28.1320 - val_MinusLogProbMetric: 28.1320 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 790/1000
2023-10-27 09:09:25.480 
Epoch 790/1000 
	 loss: 27.8669, MinusLogProbMetric: 27.8669, val_loss: 28.0639, val_MinusLogProbMetric: 28.0639

Epoch 790: val_loss did not improve from 28.05870
196/196 - 41s - loss: 27.8669 - MinusLogProbMetric: 27.8669 - val_loss: 28.0639 - val_MinusLogProbMetric: 28.0639 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 791/1000
2023-10-27 09:10:06.139 
Epoch 791/1000 
	 loss: 27.8504, MinusLogProbMetric: 27.8504, val_loss: 28.0595, val_MinusLogProbMetric: 28.0595

Epoch 791: val_loss did not improve from 28.05870
196/196 - 41s - loss: 27.8504 - MinusLogProbMetric: 27.8504 - val_loss: 28.0595 - val_MinusLogProbMetric: 28.0595 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 792/1000
2023-10-27 09:10:46.905 
Epoch 792/1000 
	 loss: 27.8598, MinusLogProbMetric: 27.8598, val_loss: 28.2388, val_MinusLogProbMetric: 28.2388

Epoch 792: val_loss did not improve from 28.05870
196/196 - 41s - loss: 27.8598 - MinusLogProbMetric: 27.8598 - val_loss: 28.2388 - val_MinusLogProbMetric: 28.2388 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 793/1000
2023-10-27 09:11:27.459 
Epoch 793/1000 
	 loss: 27.8598, MinusLogProbMetric: 27.8598, val_loss: 28.0890, val_MinusLogProbMetric: 28.0890

Epoch 793: val_loss did not improve from 28.05870
196/196 - 41s - loss: 27.8598 - MinusLogProbMetric: 27.8598 - val_loss: 28.0890 - val_MinusLogProbMetric: 28.0890 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 794/1000
2023-10-27 09:12:08.865 
Epoch 794/1000 
	 loss: 27.8539, MinusLogProbMetric: 27.8539, val_loss: 28.0838, val_MinusLogProbMetric: 28.0838

Epoch 794: val_loss did not improve from 28.05870
196/196 - 41s - loss: 27.8539 - MinusLogProbMetric: 27.8539 - val_loss: 28.0838 - val_MinusLogProbMetric: 28.0838 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 795/1000
2023-10-27 09:12:50.371 
Epoch 795/1000 
	 loss: 27.8555, MinusLogProbMetric: 27.8555, val_loss: 28.0706, val_MinusLogProbMetric: 28.0706

Epoch 795: val_loss did not improve from 28.05870
196/196 - 42s - loss: 27.8555 - MinusLogProbMetric: 27.8555 - val_loss: 28.0706 - val_MinusLogProbMetric: 28.0706 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 796/1000
2023-10-27 09:13:32.466 
Epoch 796/1000 
	 loss: 27.8550, MinusLogProbMetric: 27.8550, val_loss: 28.0976, val_MinusLogProbMetric: 28.0976

Epoch 796: val_loss did not improve from 28.05870
196/196 - 42s - loss: 27.8550 - MinusLogProbMetric: 27.8550 - val_loss: 28.0976 - val_MinusLogProbMetric: 28.0976 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 797/1000
2023-10-27 09:14:11.084 
Epoch 797/1000 
	 loss: 27.8561, MinusLogProbMetric: 27.8561, val_loss: 28.1308, val_MinusLogProbMetric: 28.1308

Epoch 797: val_loss did not improve from 28.05870
196/196 - 39s - loss: 27.8561 - MinusLogProbMetric: 27.8561 - val_loss: 28.1308 - val_MinusLogProbMetric: 28.1308 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 798/1000
2023-10-27 09:14:50.959 
Epoch 798/1000 
	 loss: 27.8716, MinusLogProbMetric: 27.8716, val_loss: 28.0694, val_MinusLogProbMetric: 28.0694

Epoch 798: val_loss did not improve from 28.05870
196/196 - 40s - loss: 27.8716 - MinusLogProbMetric: 27.8716 - val_loss: 28.0694 - val_MinusLogProbMetric: 28.0694 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 799/1000
2023-10-27 09:15:31.071 
Epoch 799/1000 
	 loss: 27.8652, MinusLogProbMetric: 27.8652, val_loss: 28.0815, val_MinusLogProbMetric: 28.0815

Epoch 799: val_loss did not improve from 28.05870
196/196 - 40s - loss: 27.8652 - MinusLogProbMetric: 27.8652 - val_loss: 28.0815 - val_MinusLogProbMetric: 28.0815 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 800/1000
2023-10-27 09:16:12.461 
Epoch 800/1000 
	 loss: 27.8588, MinusLogProbMetric: 27.8588, val_loss: 28.0785, val_MinusLogProbMetric: 28.0785

Epoch 800: val_loss did not improve from 28.05870
196/196 - 41s - loss: 27.8588 - MinusLogProbMetric: 27.8588 - val_loss: 28.0785 - val_MinusLogProbMetric: 28.0785 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 801/1000
2023-10-27 09:16:53.039 
Epoch 801/1000 
	 loss: 27.8494, MinusLogProbMetric: 27.8494, val_loss: 28.0582, val_MinusLogProbMetric: 28.0582

Epoch 801: val_loss improved from 28.05870 to 28.05822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 41s - loss: 27.8494 - MinusLogProbMetric: 27.8494 - val_loss: 28.0582 - val_MinusLogProbMetric: 28.0582 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 802/1000
2023-10-27 09:17:34.706 
Epoch 802/1000 
	 loss: 27.8591, MinusLogProbMetric: 27.8591, val_loss: 28.1080, val_MinusLogProbMetric: 28.1080

Epoch 802: val_loss did not improve from 28.05822
196/196 - 41s - loss: 27.8591 - MinusLogProbMetric: 27.8591 - val_loss: 28.1080 - val_MinusLogProbMetric: 28.1080 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 803/1000
2023-10-27 09:18:15.703 
Epoch 803/1000 
	 loss: 27.8698, MinusLogProbMetric: 27.8698, val_loss: 28.0818, val_MinusLogProbMetric: 28.0818

Epoch 803: val_loss did not improve from 28.05822
196/196 - 41s - loss: 27.8698 - MinusLogProbMetric: 27.8698 - val_loss: 28.0818 - val_MinusLogProbMetric: 28.0818 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 804/1000
2023-10-27 09:18:54.928 
Epoch 804/1000 
	 loss: 27.8755, MinusLogProbMetric: 27.8755, val_loss: 28.0510, val_MinusLogProbMetric: 28.0510

Epoch 804: val_loss improved from 28.05822 to 28.05103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 40s - loss: 27.8755 - MinusLogProbMetric: 27.8755 - val_loss: 28.0510 - val_MinusLogProbMetric: 28.0510 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 805/1000
2023-10-27 09:19:34.930 
Epoch 805/1000 
	 loss: 27.8517, MinusLogProbMetric: 27.8517, val_loss: 28.0925, val_MinusLogProbMetric: 28.0925

Epoch 805: val_loss did not improve from 28.05103
196/196 - 39s - loss: 27.8517 - MinusLogProbMetric: 27.8517 - val_loss: 28.0925 - val_MinusLogProbMetric: 28.0925 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 806/1000
2023-10-27 09:20:16.763 
Epoch 806/1000 
	 loss: 27.8569, MinusLogProbMetric: 27.8569, val_loss: 28.1389, val_MinusLogProbMetric: 28.1389

Epoch 806: val_loss did not improve from 28.05103
196/196 - 42s - loss: 27.8569 - MinusLogProbMetric: 27.8569 - val_loss: 28.1389 - val_MinusLogProbMetric: 28.1389 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 807/1000
2023-10-27 09:20:57.575 
Epoch 807/1000 
	 loss: 27.8524, MinusLogProbMetric: 27.8524, val_loss: 28.1497, val_MinusLogProbMetric: 28.1497

Epoch 807: val_loss did not improve from 28.05103
196/196 - 41s - loss: 27.8524 - MinusLogProbMetric: 27.8524 - val_loss: 28.1497 - val_MinusLogProbMetric: 28.1497 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 808/1000
2023-10-27 09:21:38.106 
Epoch 808/1000 
	 loss: 27.8588, MinusLogProbMetric: 27.8588, val_loss: 28.0720, val_MinusLogProbMetric: 28.0720

Epoch 808: val_loss did not improve from 28.05103
196/196 - 41s - loss: 27.8588 - MinusLogProbMetric: 27.8588 - val_loss: 28.0720 - val_MinusLogProbMetric: 28.0720 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 809/1000
2023-10-27 09:22:17.499 
Epoch 809/1000 
	 loss: 27.8612, MinusLogProbMetric: 27.8612, val_loss: 28.0576, val_MinusLogProbMetric: 28.0576

Epoch 809: val_loss did not improve from 28.05103
196/196 - 39s - loss: 27.8612 - MinusLogProbMetric: 27.8612 - val_loss: 28.0576 - val_MinusLogProbMetric: 28.0576 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 810/1000
2023-10-27 09:22:57.894 
Epoch 810/1000 
	 loss: 27.8607, MinusLogProbMetric: 27.8607, val_loss: 28.0546, val_MinusLogProbMetric: 28.0546

Epoch 810: val_loss did not improve from 28.05103
196/196 - 40s - loss: 27.8607 - MinusLogProbMetric: 27.8607 - val_loss: 28.0546 - val_MinusLogProbMetric: 28.0546 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 811/1000
2023-10-27 09:23:38.863 
Epoch 811/1000 
	 loss: 27.8459, MinusLogProbMetric: 27.8459, val_loss: 28.0612, val_MinusLogProbMetric: 28.0612

Epoch 811: val_loss did not improve from 28.05103
196/196 - 41s - loss: 27.8459 - MinusLogProbMetric: 27.8459 - val_loss: 28.0612 - val_MinusLogProbMetric: 28.0612 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 812/1000
2023-10-27 09:24:17.938 
Epoch 812/1000 
	 loss: 27.8539, MinusLogProbMetric: 27.8539, val_loss: 28.2085, val_MinusLogProbMetric: 28.2085

Epoch 812: val_loss did not improve from 28.05103
196/196 - 39s - loss: 27.8539 - MinusLogProbMetric: 27.8539 - val_loss: 28.2085 - val_MinusLogProbMetric: 28.2085 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 813/1000
2023-10-27 09:24:56.523 
Epoch 813/1000 
	 loss: 27.8593, MinusLogProbMetric: 27.8593, val_loss: 28.0628, val_MinusLogProbMetric: 28.0628

Epoch 813: val_loss did not improve from 28.05103
196/196 - 39s - loss: 27.8593 - MinusLogProbMetric: 27.8593 - val_loss: 28.0628 - val_MinusLogProbMetric: 28.0628 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 814/1000
2023-10-27 09:25:36.099 
Epoch 814/1000 
	 loss: 27.8518, MinusLogProbMetric: 27.8518, val_loss: 28.0472, val_MinusLogProbMetric: 28.0472

Epoch 814: val_loss improved from 28.05103 to 28.04724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 40s - loss: 27.8518 - MinusLogProbMetric: 27.8518 - val_loss: 28.0472 - val_MinusLogProbMetric: 28.0472 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 815/1000
2023-10-27 09:26:16.625 
Epoch 815/1000 
	 loss: 27.8535, MinusLogProbMetric: 27.8535, val_loss: 28.0913, val_MinusLogProbMetric: 28.0913

Epoch 815: val_loss did not improve from 28.04724
196/196 - 40s - loss: 27.8535 - MinusLogProbMetric: 27.8535 - val_loss: 28.0913 - val_MinusLogProbMetric: 28.0913 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 816/1000
2023-10-27 09:26:56.778 
Epoch 816/1000 
	 loss: 27.8530, MinusLogProbMetric: 27.8530, val_loss: 28.0766, val_MinusLogProbMetric: 28.0766

Epoch 816: val_loss did not improve from 28.04724
196/196 - 40s - loss: 27.8530 - MinusLogProbMetric: 27.8530 - val_loss: 28.0766 - val_MinusLogProbMetric: 28.0766 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 817/1000
2023-10-27 09:27:37.096 
Epoch 817/1000 
	 loss: 27.8588, MinusLogProbMetric: 27.8588, val_loss: 28.0528, val_MinusLogProbMetric: 28.0528

Epoch 817: val_loss did not improve from 28.04724
196/196 - 40s - loss: 27.8588 - MinusLogProbMetric: 27.8588 - val_loss: 28.0528 - val_MinusLogProbMetric: 28.0528 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 818/1000
2023-10-27 09:28:17.007 
Epoch 818/1000 
	 loss: 27.8550, MinusLogProbMetric: 27.8550, val_loss: 28.0728, val_MinusLogProbMetric: 28.0728

Epoch 818: val_loss did not improve from 28.04724
196/196 - 40s - loss: 27.8550 - MinusLogProbMetric: 27.8550 - val_loss: 28.0728 - val_MinusLogProbMetric: 28.0728 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 819/1000
2023-10-27 09:28:57.449 
Epoch 819/1000 
	 loss: 27.8548, MinusLogProbMetric: 27.8548, val_loss: 28.1684, val_MinusLogProbMetric: 28.1684

Epoch 819: val_loss did not improve from 28.04724
196/196 - 40s - loss: 27.8548 - MinusLogProbMetric: 27.8548 - val_loss: 28.1684 - val_MinusLogProbMetric: 28.1684 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 820/1000
2023-10-27 09:29:38.842 
Epoch 820/1000 
	 loss: 27.8515, MinusLogProbMetric: 27.8515, val_loss: 28.0464, val_MinusLogProbMetric: 28.0464

Epoch 820: val_loss improved from 28.04724 to 28.04644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 27.8515 - MinusLogProbMetric: 27.8515 - val_loss: 28.0464 - val_MinusLogProbMetric: 28.0464 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 821/1000
2023-10-27 09:30:20.759 
Epoch 821/1000 
	 loss: 27.8470, MinusLogProbMetric: 27.8470, val_loss: 28.0770, val_MinusLogProbMetric: 28.0770

Epoch 821: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8470 - MinusLogProbMetric: 27.8470 - val_loss: 28.0770 - val_MinusLogProbMetric: 28.0770 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 822/1000
2023-10-27 09:31:02.614 
Epoch 822/1000 
	 loss: 27.8463, MinusLogProbMetric: 27.8463, val_loss: 28.0862, val_MinusLogProbMetric: 28.0862

Epoch 822: val_loss did not improve from 28.04644
196/196 - 42s - loss: 27.8463 - MinusLogProbMetric: 27.8463 - val_loss: 28.0862 - val_MinusLogProbMetric: 28.0862 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 823/1000
2023-10-27 09:31:43.604 
Epoch 823/1000 
	 loss: 27.8492, MinusLogProbMetric: 27.8492, val_loss: 28.0531, val_MinusLogProbMetric: 28.0531

Epoch 823: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8492 - MinusLogProbMetric: 27.8492 - val_loss: 28.0531 - val_MinusLogProbMetric: 28.0531 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 824/1000
2023-10-27 09:32:23.211 
Epoch 824/1000 
	 loss: 27.8559, MinusLogProbMetric: 27.8559, val_loss: 28.1274, val_MinusLogProbMetric: 28.1274

Epoch 824: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8559 - MinusLogProbMetric: 27.8559 - val_loss: 28.1274 - val_MinusLogProbMetric: 28.1274 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 825/1000
2023-10-27 09:33:03.515 
Epoch 825/1000 
	 loss: 27.8632, MinusLogProbMetric: 27.8632, val_loss: 28.0825, val_MinusLogProbMetric: 28.0825

Epoch 825: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8632 - MinusLogProbMetric: 27.8632 - val_loss: 28.0825 - val_MinusLogProbMetric: 28.0825 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 826/1000
2023-10-27 09:33:44.992 
Epoch 826/1000 
	 loss: 27.8551, MinusLogProbMetric: 27.8551, val_loss: 28.0615, val_MinusLogProbMetric: 28.0615

Epoch 826: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8551 - MinusLogProbMetric: 27.8551 - val_loss: 28.0615 - val_MinusLogProbMetric: 28.0615 - lr: 1.2500e-04 - 41s/epoch - 212ms/step
Epoch 827/1000
2023-10-27 09:34:24.926 
Epoch 827/1000 
	 loss: 27.8571, MinusLogProbMetric: 27.8571, val_loss: 28.1050, val_MinusLogProbMetric: 28.1050

Epoch 827: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8571 - MinusLogProbMetric: 27.8571 - val_loss: 28.1050 - val_MinusLogProbMetric: 28.1050 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 828/1000
2023-10-27 09:35:06.127 
Epoch 828/1000 
	 loss: 27.8424, MinusLogProbMetric: 27.8424, val_loss: 28.0806, val_MinusLogProbMetric: 28.0806

Epoch 828: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8424 - MinusLogProbMetric: 27.8424 - val_loss: 28.0806 - val_MinusLogProbMetric: 28.0806 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 829/1000
2023-10-27 09:35:48.115 
Epoch 829/1000 
	 loss: 27.8579, MinusLogProbMetric: 27.8579, val_loss: 28.1154, val_MinusLogProbMetric: 28.1154

Epoch 829: val_loss did not improve from 28.04644
196/196 - 42s - loss: 27.8579 - MinusLogProbMetric: 27.8579 - val_loss: 28.1154 - val_MinusLogProbMetric: 28.1154 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 830/1000
2023-10-27 09:36:28.727 
Epoch 830/1000 
	 loss: 27.8517, MinusLogProbMetric: 27.8517, val_loss: 28.1155, val_MinusLogProbMetric: 28.1155

Epoch 830: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8517 - MinusLogProbMetric: 27.8517 - val_loss: 28.1155 - val_MinusLogProbMetric: 28.1155 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 831/1000
2023-10-27 09:37:08.335 
Epoch 831/1000 
	 loss: 27.8539, MinusLogProbMetric: 27.8539, val_loss: 28.0559, val_MinusLogProbMetric: 28.0559

Epoch 831: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8539 - MinusLogProbMetric: 27.8539 - val_loss: 28.0559 - val_MinusLogProbMetric: 28.0559 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 832/1000
2023-10-27 09:37:49.114 
Epoch 832/1000 
	 loss: 27.8564, MinusLogProbMetric: 27.8564, val_loss: 28.2457, val_MinusLogProbMetric: 28.2457

Epoch 832: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8564 - MinusLogProbMetric: 27.8564 - val_loss: 28.2457 - val_MinusLogProbMetric: 28.2457 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 833/1000
2023-10-27 09:38:29.978 
Epoch 833/1000 
	 loss: 27.8514, MinusLogProbMetric: 27.8514, val_loss: 28.0530, val_MinusLogProbMetric: 28.0530

Epoch 833: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8514 - MinusLogProbMetric: 27.8514 - val_loss: 28.0530 - val_MinusLogProbMetric: 28.0530 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 834/1000
2023-10-27 09:39:11.237 
Epoch 834/1000 
	 loss: 27.8538, MinusLogProbMetric: 27.8538, val_loss: 28.0843, val_MinusLogProbMetric: 28.0843

Epoch 834: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8538 - MinusLogProbMetric: 27.8538 - val_loss: 28.0843 - val_MinusLogProbMetric: 28.0843 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 835/1000
2023-10-27 09:39:51.639 
Epoch 835/1000 
	 loss: 27.8507, MinusLogProbMetric: 27.8507, val_loss: 28.0494, val_MinusLogProbMetric: 28.0494

Epoch 835: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8507 - MinusLogProbMetric: 27.8507 - val_loss: 28.0494 - val_MinusLogProbMetric: 28.0494 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 836/1000
2023-10-27 09:40:30.804 
Epoch 836/1000 
	 loss: 27.8471, MinusLogProbMetric: 27.8471, val_loss: 28.0764, val_MinusLogProbMetric: 28.0764

Epoch 836: val_loss did not improve from 28.04644
196/196 - 39s - loss: 27.8471 - MinusLogProbMetric: 27.8471 - val_loss: 28.0764 - val_MinusLogProbMetric: 28.0764 - lr: 1.2500e-04 - 39s/epoch - 200ms/step
Epoch 837/1000
2023-10-27 09:41:11.382 
Epoch 837/1000 
	 loss: 27.8431, MinusLogProbMetric: 27.8431, val_loss: 28.0867, val_MinusLogProbMetric: 28.0867

Epoch 837: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8431 - MinusLogProbMetric: 27.8431 - val_loss: 28.0867 - val_MinusLogProbMetric: 28.0867 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 838/1000
2023-10-27 09:41:49.974 
Epoch 838/1000 
	 loss: 27.8448, MinusLogProbMetric: 27.8448, val_loss: 28.0831, val_MinusLogProbMetric: 28.0831

Epoch 838: val_loss did not improve from 28.04644
196/196 - 39s - loss: 27.8448 - MinusLogProbMetric: 27.8448 - val_loss: 28.0831 - val_MinusLogProbMetric: 28.0831 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 839/1000
2023-10-27 09:42:30.112 
Epoch 839/1000 
	 loss: 27.8516, MinusLogProbMetric: 27.8516, val_loss: 28.0559, val_MinusLogProbMetric: 28.0559

Epoch 839: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8516 - MinusLogProbMetric: 27.8516 - val_loss: 28.0559 - val_MinusLogProbMetric: 28.0559 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 840/1000
2023-10-27 09:43:09.815 
Epoch 840/1000 
	 loss: 27.8455, MinusLogProbMetric: 27.8455, val_loss: 28.0732, val_MinusLogProbMetric: 28.0732

Epoch 840: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8455 - MinusLogProbMetric: 27.8455 - val_loss: 28.0732 - val_MinusLogProbMetric: 28.0732 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 841/1000
2023-10-27 09:43:49.585 
Epoch 841/1000 
	 loss: 27.8457, MinusLogProbMetric: 27.8457, val_loss: 28.0897, val_MinusLogProbMetric: 28.0897

Epoch 841: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8457 - MinusLogProbMetric: 27.8457 - val_loss: 28.0897 - val_MinusLogProbMetric: 28.0897 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 842/1000
2023-10-27 09:44:31.497 
Epoch 842/1000 
	 loss: 27.8459, MinusLogProbMetric: 27.8459, val_loss: 28.0765, val_MinusLogProbMetric: 28.0765

Epoch 842: val_loss did not improve from 28.04644
196/196 - 42s - loss: 27.8459 - MinusLogProbMetric: 27.8459 - val_loss: 28.0765 - val_MinusLogProbMetric: 28.0765 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 843/1000
2023-10-27 09:45:12.538 
Epoch 843/1000 
	 loss: 27.8619, MinusLogProbMetric: 27.8619, val_loss: 28.1438, val_MinusLogProbMetric: 28.1438

Epoch 843: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8619 - MinusLogProbMetric: 27.8619 - val_loss: 28.1438 - val_MinusLogProbMetric: 28.1438 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 844/1000
2023-10-27 09:45:53.212 
Epoch 844/1000 
	 loss: 27.8465, MinusLogProbMetric: 27.8465, val_loss: 28.1050, val_MinusLogProbMetric: 28.1050

Epoch 844: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8465 - MinusLogProbMetric: 27.8465 - val_loss: 28.1050 - val_MinusLogProbMetric: 28.1050 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 845/1000
2023-10-27 09:46:33.860 
Epoch 845/1000 
	 loss: 27.8478, MinusLogProbMetric: 27.8478, val_loss: 28.0793, val_MinusLogProbMetric: 28.0793

Epoch 845: val_loss did not improve from 28.04644
196/196 - 41s - loss: 27.8478 - MinusLogProbMetric: 27.8478 - val_loss: 28.0793 - val_MinusLogProbMetric: 28.0793 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 846/1000
2023-10-27 09:47:14.352 
Epoch 846/1000 
	 loss: 27.8519, MinusLogProbMetric: 27.8519, val_loss: 28.0746, val_MinusLogProbMetric: 28.0746

Epoch 846: val_loss did not improve from 28.04644
196/196 - 40s - loss: 27.8519 - MinusLogProbMetric: 27.8519 - val_loss: 28.0746 - val_MinusLogProbMetric: 28.0746 - lr: 1.2500e-04 - 40s/epoch - 207ms/step
Epoch 847/1000
2023-10-27 09:47:53.346 
Epoch 847/1000 
	 loss: 27.8507, MinusLogProbMetric: 27.8507, val_loss: 28.0677, val_MinusLogProbMetric: 28.0677

Epoch 847: val_loss did not improve from 28.04644
196/196 - 39s - loss: 27.8507 - MinusLogProbMetric: 27.8507 - val_loss: 28.0677 - val_MinusLogProbMetric: 28.0677 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 848/1000
2023-10-27 09:48:32.434 
Epoch 848/1000 
	 loss: 27.8520, MinusLogProbMetric: 27.8520, val_loss: 28.0388, val_MinusLogProbMetric: 28.0388

Epoch 848: val_loss improved from 28.04644 to 28.03884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 40s - loss: 27.8520 - MinusLogProbMetric: 27.8520 - val_loss: 28.0388 - val_MinusLogProbMetric: 28.0388 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 849/1000
2023-10-27 09:49:12.101 
Epoch 849/1000 
	 loss: 27.8475, MinusLogProbMetric: 27.8475, val_loss: 28.0919, val_MinusLogProbMetric: 28.0919

Epoch 849: val_loss did not improve from 28.03884
196/196 - 39s - loss: 27.8475 - MinusLogProbMetric: 27.8475 - val_loss: 28.0919 - val_MinusLogProbMetric: 28.0919 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 850/1000
2023-10-27 09:49:52.186 
Epoch 850/1000 
	 loss: 27.8533, MinusLogProbMetric: 27.8533, val_loss: 28.1047, val_MinusLogProbMetric: 28.1047

Epoch 850: val_loss did not improve from 28.03884
196/196 - 40s - loss: 27.8533 - MinusLogProbMetric: 27.8533 - val_loss: 28.1047 - val_MinusLogProbMetric: 28.1047 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 851/1000
2023-10-27 09:50:33.121 
Epoch 851/1000 
	 loss: 27.8556, MinusLogProbMetric: 27.8556, val_loss: 28.0886, val_MinusLogProbMetric: 28.0886

Epoch 851: val_loss did not improve from 28.03884
196/196 - 41s - loss: 27.8556 - MinusLogProbMetric: 27.8556 - val_loss: 28.0886 - val_MinusLogProbMetric: 28.0886 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 852/1000
2023-10-27 09:51:14.987 
Epoch 852/1000 
	 loss: 27.8539, MinusLogProbMetric: 27.8539, val_loss: 28.0428, val_MinusLogProbMetric: 28.0428

Epoch 852: val_loss did not improve from 28.03884
196/196 - 42s - loss: 27.8539 - MinusLogProbMetric: 27.8539 - val_loss: 28.0428 - val_MinusLogProbMetric: 28.0428 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 853/1000
2023-10-27 09:51:51.864 
Epoch 853/1000 
	 loss: 27.8392, MinusLogProbMetric: 27.8392, val_loss: 28.0577, val_MinusLogProbMetric: 28.0577

Epoch 853: val_loss did not improve from 28.03884
196/196 - 37s - loss: 27.8392 - MinusLogProbMetric: 27.8392 - val_loss: 28.0577 - val_MinusLogProbMetric: 28.0577 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 854/1000
2023-10-27 09:52:30.772 
Epoch 854/1000 
	 loss: 27.8571, MinusLogProbMetric: 27.8571, val_loss: 28.0951, val_MinusLogProbMetric: 28.0951

Epoch 854: val_loss did not improve from 28.03884
196/196 - 39s - loss: 27.8571 - MinusLogProbMetric: 27.8571 - val_loss: 28.0951 - val_MinusLogProbMetric: 28.0951 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 855/1000
2023-10-27 09:53:09.273 
Epoch 855/1000 
	 loss: 27.8522, MinusLogProbMetric: 27.8522, val_loss: 28.1372, val_MinusLogProbMetric: 28.1372

Epoch 855: val_loss did not improve from 28.03884
196/196 - 38s - loss: 27.8522 - MinusLogProbMetric: 27.8522 - val_loss: 28.1372 - val_MinusLogProbMetric: 28.1372 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 856/1000
2023-10-27 09:53:48.965 
Epoch 856/1000 
	 loss: 27.8549, MinusLogProbMetric: 27.8549, val_loss: 28.0439, val_MinusLogProbMetric: 28.0439

Epoch 856: val_loss did not improve from 28.03884
196/196 - 40s - loss: 27.8549 - MinusLogProbMetric: 27.8549 - val_loss: 28.0439 - val_MinusLogProbMetric: 28.0439 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 857/1000
2023-10-27 09:54:30.152 
Epoch 857/1000 
	 loss: 27.8563, MinusLogProbMetric: 27.8563, val_loss: 28.0969, val_MinusLogProbMetric: 28.0969

Epoch 857: val_loss did not improve from 28.03884
196/196 - 41s - loss: 27.8563 - MinusLogProbMetric: 27.8563 - val_loss: 28.0969 - val_MinusLogProbMetric: 28.0969 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 858/1000
2023-10-27 09:55:09.479 
Epoch 858/1000 
	 loss: 27.8457, MinusLogProbMetric: 27.8457, val_loss: 28.0729, val_MinusLogProbMetric: 28.0729

Epoch 858: val_loss did not improve from 28.03884
196/196 - 39s - loss: 27.8457 - MinusLogProbMetric: 27.8457 - val_loss: 28.0729 - val_MinusLogProbMetric: 28.0729 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 859/1000
2023-10-27 09:55:47.923 
Epoch 859/1000 
	 loss: 27.8546, MinusLogProbMetric: 27.8546, val_loss: 28.0569, val_MinusLogProbMetric: 28.0569

Epoch 859: val_loss did not improve from 28.03884
196/196 - 38s - loss: 27.8546 - MinusLogProbMetric: 27.8546 - val_loss: 28.0569 - val_MinusLogProbMetric: 28.0569 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 860/1000
2023-10-27 09:56:29.661 
Epoch 860/1000 
	 loss: 27.8524, MinusLogProbMetric: 27.8524, val_loss: 28.1152, val_MinusLogProbMetric: 28.1152

Epoch 860: val_loss did not improve from 28.03884
196/196 - 42s - loss: 27.8524 - MinusLogProbMetric: 27.8524 - val_loss: 28.1152 - val_MinusLogProbMetric: 28.1152 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 861/1000
2023-10-27 09:57:10.553 
Epoch 861/1000 
	 loss: 27.8520, MinusLogProbMetric: 27.8520, val_loss: 28.1610, val_MinusLogProbMetric: 28.1610

Epoch 861: val_loss did not improve from 28.03884
196/196 - 41s - loss: 27.8520 - MinusLogProbMetric: 27.8520 - val_loss: 28.1610 - val_MinusLogProbMetric: 28.1610 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 862/1000
2023-10-27 09:57:52.161 
Epoch 862/1000 
	 loss: 27.8578, MinusLogProbMetric: 27.8578, val_loss: 28.0864, val_MinusLogProbMetric: 28.0864

Epoch 862: val_loss did not improve from 28.03884
196/196 - 42s - loss: 27.8578 - MinusLogProbMetric: 27.8578 - val_loss: 28.0864 - val_MinusLogProbMetric: 28.0864 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 863/1000
2023-10-27 09:58:33.234 
Epoch 863/1000 
	 loss: 27.8449, MinusLogProbMetric: 27.8449, val_loss: 28.1068, val_MinusLogProbMetric: 28.1068

Epoch 863: val_loss did not improve from 28.03884
196/196 - 41s - loss: 27.8449 - MinusLogProbMetric: 27.8449 - val_loss: 28.1068 - val_MinusLogProbMetric: 28.1068 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 864/1000
2023-10-27 09:59:13.428 
Epoch 864/1000 
	 loss: 27.8621, MinusLogProbMetric: 27.8621, val_loss: 28.0356, val_MinusLogProbMetric: 28.0356

Epoch 864: val_loss improved from 28.03884 to 28.03565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 41s - loss: 27.8621 - MinusLogProbMetric: 27.8621 - val_loss: 28.0356 - val_MinusLogProbMetric: 28.0356 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 865/1000
2023-10-27 09:59:54.728 
Epoch 865/1000 
	 loss: 27.8519, MinusLogProbMetric: 27.8519, val_loss: 28.0533, val_MinusLogProbMetric: 28.0533

Epoch 865: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8519 - MinusLogProbMetric: 27.8519 - val_loss: 28.0533 - val_MinusLogProbMetric: 28.0533 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 866/1000
2023-10-27 10:00:34.869 
Epoch 866/1000 
	 loss: 27.8294, MinusLogProbMetric: 27.8294, val_loss: 28.1257, val_MinusLogProbMetric: 28.1257

Epoch 866: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8294 - MinusLogProbMetric: 27.8294 - val_loss: 28.1257 - val_MinusLogProbMetric: 28.1257 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 867/1000
2023-10-27 10:01:14.391 
Epoch 867/1000 
	 loss: 27.8423, MinusLogProbMetric: 27.8423, val_loss: 28.0863, val_MinusLogProbMetric: 28.0863

Epoch 867: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8423 - MinusLogProbMetric: 27.8423 - val_loss: 28.0863 - val_MinusLogProbMetric: 28.0863 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 868/1000
2023-10-27 10:01:53.908 
Epoch 868/1000 
	 loss: 27.8557, MinusLogProbMetric: 27.8557, val_loss: 28.0703, val_MinusLogProbMetric: 28.0703

Epoch 868: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8557 - MinusLogProbMetric: 27.8557 - val_loss: 28.0703 - val_MinusLogProbMetric: 28.0703 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 869/1000
2023-10-27 10:02:32.951 
Epoch 869/1000 
	 loss: 27.8522, MinusLogProbMetric: 27.8522, val_loss: 28.0444, val_MinusLogProbMetric: 28.0444

Epoch 869: val_loss did not improve from 28.03565
196/196 - 39s - loss: 27.8522 - MinusLogProbMetric: 27.8522 - val_loss: 28.0444 - val_MinusLogProbMetric: 28.0444 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 870/1000
2023-10-27 10:03:13.408 
Epoch 870/1000 
	 loss: 27.8545, MinusLogProbMetric: 27.8545, val_loss: 28.1128, val_MinusLogProbMetric: 28.1128

Epoch 870: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8545 - MinusLogProbMetric: 27.8545 - val_loss: 28.1128 - val_MinusLogProbMetric: 28.1128 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 871/1000
2023-10-27 10:03:53.873 
Epoch 871/1000 
	 loss: 27.8474, MinusLogProbMetric: 27.8474, val_loss: 28.0915, val_MinusLogProbMetric: 28.0915

Epoch 871: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8474 - MinusLogProbMetric: 27.8474 - val_loss: 28.0915 - val_MinusLogProbMetric: 28.0915 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 872/1000
2023-10-27 10:04:35.491 
Epoch 872/1000 
	 loss: 27.8360, MinusLogProbMetric: 27.8360, val_loss: 28.0836, val_MinusLogProbMetric: 28.0836

Epoch 872: val_loss did not improve from 28.03565
196/196 - 42s - loss: 27.8360 - MinusLogProbMetric: 27.8360 - val_loss: 28.0836 - val_MinusLogProbMetric: 28.0836 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 873/1000
2023-10-27 10:05:15.851 
Epoch 873/1000 
	 loss: 27.8436, MinusLogProbMetric: 27.8436, val_loss: 28.0551, val_MinusLogProbMetric: 28.0551

Epoch 873: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8436 - MinusLogProbMetric: 27.8436 - val_loss: 28.0551 - val_MinusLogProbMetric: 28.0551 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 874/1000
2023-10-27 10:05:54.620 
Epoch 874/1000 
	 loss: 27.8397, MinusLogProbMetric: 27.8397, val_loss: 28.0557, val_MinusLogProbMetric: 28.0557

Epoch 874: val_loss did not improve from 28.03565
196/196 - 39s - loss: 27.8397 - MinusLogProbMetric: 27.8397 - val_loss: 28.0557 - val_MinusLogProbMetric: 28.0557 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 875/1000
2023-10-27 10:06:36.119 
Epoch 875/1000 
	 loss: 27.8427, MinusLogProbMetric: 27.8427, val_loss: 28.0413, val_MinusLogProbMetric: 28.0413

Epoch 875: val_loss did not improve from 28.03565
196/196 - 41s - loss: 27.8427 - MinusLogProbMetric: 27.8427 - val_loss: 28.0413 - val_MinusLogProbMetric: 28.0413 - lr: 1.2500e-04 - 41s/epoch - 212ms/step
Epoch 876/1000
2023-10-27 10:07:15.903 
Epoch 876/1000 
	 loss: 27.8426, MinusLogProbMetric: 27.8426, val_loss: 28.0856, val_MinusLogProbMetric: 28.0856

Epoch 876: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8426 - MinusLogProbMetric: 27.8426 - val_loss: 28.0856 - val_MinusLogProbMetric: 28.0856 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 877/1000
2023-10-27 10:07:55.415 
Epoch 877/1000 
	 loss: 27.8548, MinusLogProbMetric: 27.8548, val_loss: 28.0657, val_MinusLogProbMetric: 28.0657

Epoch 877: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8548 - MinusLogProbMetric: 27.8548 - val_loss: 28.0657 - val_MinusLogProbMetric: 28.0657 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 878/1000
2023-10-27 10:08:35.868 
Epoch 878/1000 
	 loss: 27.8429, MinusLogProbMetric: 27.8429, val_loss: 28.0734, val_MinusLogProbMetric: 28.0734

Epoch 878: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8429 - MinusLogProbMetric: 27.8429 - val_loss: 28.0734 - val_MinusLogProbMetric: 28.0734 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 879/1000
2023-10-27 10:09:15.402 
Epoch 879/1000 
	 loss: 27.8527, MinusLogProbMetric: 27.8527, val_loss: 28.0816, val_MinusLogProbMetric: 28.0816

Epoch 879: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8527 - MinusLogProbMetric: 27.8527 - val_loss: 28.0816 - val_MinusLogProbMetric: 28.0816 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 880/1000
2023-10-27 10:09:56.240 
Epoch 880/1000 
	 loss: 27.8437, MinusLogProbMetric: 27.8437, val_loss: 28.0788, val_MinusLogProbMetric: 28.0788

Epoch 880: val_loss did not improve from 28.03565
196/196 - 41s - loss: 27.8437 - MinusLogProbMetric: 27.8437 - val_loss: 28.0788 - val_MinusLogProbMetric: 28.0788 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 881/1000
2023-10-27 10:10:36.473 
Epoch 881/1000 
	 loss: 27.8558, MinusLogProbMetric: 27.8558, val_loss: 28.0431, val_MinusLogProbMetric: 28.0431

Epoch 881: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8558 - MinusLogProbMetric: 27.8558 - val_loss: 28.0431 - val_MinusLogProbMetric: 28.0431 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 882/1000
2023-10-27 10:11:17.550 
Epoch 882/1000 
	 loss: 27.8463, MinusLogProbMetric: 27.8463, val_loss: 28.0761, val_MinusLogProbMetric: 28.0761

Epoch 882: val_loss did not improve from 28.03565
196/196 - 41s - loss: 27.8463 - MinusLogProbMetric: 27.8463 - val_loss: 28.0761 - val_MinusLogProbMetric: 28.0761 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 883/1000
2023-10-27 10:11:59.084 
Epoch 883/1000 
	 loss: 27.8408, MinusLogProbMetric: 27.8408, val_loss: 28.0556, val_MinusLogProbMetric: 28.0556

Epoch 883: val_loss did not improve from 28.03565
196/196 - 42s - loss: 27.8408 - MinusLogProbMetric: 27.8408 - val_loss: 28.0556 - val_MinusLogProbMetric: 28.0556 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 884/1000
2023-10-27 10:12:40.027 
Epoch 884/1000 
	 loss: 27.8438, MinusLogProbMetric: 27.8438, val_loss: 28.0441, val_MinusLogProbMetric: 28.0441

Epoch 884: val_loss did not improve from 28.03565
196/196 - 41s - loss: 27.8438 - MinusLogProbMetric: 27.8438 - val_loss: 28.0441 - val_MinusLogProbMetric: 28.0441 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 885/1000
2023-10-27 10:13:21.771 
Epoch 885/1000 
	 loss: 27.8430, MinusLogProbMetric: 27.8430, val_loss: 28.0374, val_MinusLogProbMetric: 28.0374

Epoch 885: val_loss did not improve from 28.03565
196/196 - 42s - loss: 27.8430 - MinusLogProbMetric: 27.8430 - val_loss: 28.0374 - val_MinusLogProbMetric: 28.0374 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 886/1000
2023-10-27 10:14:04.881 
Epoch 886/1000 
	 loss: 27.8407, MinusLogProbMetric: 27.8407, val_loss: 28.0473, val_MinusLogProbMetric: 28.0473

Epoch 886: val_loss did not improve from 28.03565
196/196 - 43s - loss: 27.8407 - MinusLogProbMetric: 27.8407 - val_loss: 28.0473 - val_MinusLogProbMetric: 28.0473 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 887/1000
2023-10-27 10:14:50.278 
Epoch 887/1000 
	 loss: 27.8569, MinusLogProbMetric: 27.8569, val_loss: 28.2038, val_MinusLogProbMetric: 28.2038

Epoch 887: val_loss did not improve from 28.03565
196/196 - 45s - loss: 27.8569 - MinusLogProbMetric: 27.8569 - val_loss: 28.2038 - val_MinusLogProbMetric: 28.2038 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 888/1000
2023-10-27 10:15:35.899 
Epoch 888/1000 
	 loss: 27.8503, MinusLogProbMetric: 27.8503, val_loss: 28.1904, val_MinusLogProbMetric: 28.1904

Epoch 888: val_loss did not improve from 28.03565
196/196 - 46s - loss: 27.8503 - MinusLogProbMetric: 27.8503 - val_loss: 28.1904 - val_MinusLogProbMetric: 28.1904 - lr: 1.2500e-04 - 46s/epoch - 233ms/step
Epoch 889/1000
2023-10-27 10:16:20.800 
Epoch 889/1000 
	 loss: 27.8419, MinusLogProbMetric: 27.8419, val_loss: 28.0782, val_MinusLogProbMetric: 28.0782

Epoch 889: val_loss did not improve from 28.03565
196/196 - 45s - loss: 27.8419 - MinusLogProbMetric: 27.8419 - val_loss: 28.0782 - val_MinusLogProbMetric: 28.0782 - lr: 1.2500e-04 - 45s/epoch - 229ms/step
Epoch 890/1000
2023-10-27 10:17:05.404 
Epoch 890/1000 
	 loss: 27.8406, MinusLogProbMetric: 27.8406, val_loss: 28.1001, val_MinusLogProbMetric: 28.1001

Epoch 890: val_loss did not improve from 28.03565
196/196 - 45s - loss: 27.8406 - MinusLogProbMetric: 27.8406 - val_loss: 28.1001 - val_MinusLogProbMetric: 28.1001 - lr: 1.2500e-04 - 45s/epoch - 227ms/step
Epoch 891/1000
2023-10-27 10:17:50.764 
Epoch 891/1000 
	 loss: 27.8495, MinusLogProbMetric: 27.8495, val_loss: 28.0924, val_MinusLogProbMetric: 28.0924

Epoch 891: val_loss did not improve from 28.03565
196/196 - 45s - loss: 27.8495 - MinusLogProbMetric: 27.8495 - val_loss: 28.0924 - val_MinusLogProbMetric: 28.0924 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 892/1000
2023-10-27 10:18:34.529 
Epoch 892/1000 
	 loss: 27.8488, MinusLogProbMetric: 27.8488, val_loss: 28.0524, val_MinusLogProbMetric: 28.0524

Epoch 892: val_loss did not improve from 28.03565
196/196 - 44s - loss: 27.8488 - MinusLogProbMetric: 27.8488 - val_loss: 28.0524 - val_MinusLogProbMetric: 28.0524 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 893/1000
2023-10-27 10:19:17.021 
Epoch 893/1000 
	 loss: 27.8529, MinusLogProbMetric: 27.8529, val_loss: 28.1637, val_MinusLogProbMetric: 28.1637

Epoch 893: val_loss did not improve from 28.03565
196/196 - 42s - loss: 27.8529 - MinusLogProbMetric: 27.8529 - val_loss: 28.1637 - val_MinusLogProbMetric: 28.1637 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 894/1000
2023-10-27 10:20:00.842 
Epoch 894/1000 
	 loss: 27.8532, MinusLogProbMetric: 27.8532, val_loss: 28.0765, val_MinusLogProbMetric: 28.0765

Epoch 894: val_loss did not improve from 28.03565
196/196 - 44s - loss: 27.8532 - MinusLogProbMetric: 27.8532 - val_loss: 28.0765 - val_MinusLogProbMetric: 28.0765 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 895/1000
2023-10-27 10:20:43.766 
Epoch 895/1000 
	 loss: 27.8550, MinusLogProbMetric: 27.8550, val_loss: 28.0546, val_MinusLogProbMetric: 28.0546

Epoch 895: val_loss did not improve from 28.03565
196/196 - 43s - loss: 27.8550 - MinusLogProbMetric: 27.8550 - val_loss: 28.0546 - val_MinusLogProbMetric: 28.0546 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 896/1000
2023-10-27 10:21:26.252 
Epoch 896/1000 
	 loss: 27.8551, MinusLogProbMetric: 27.8551, val_loss: 28.0428, val_MinusLogProbMetric: 28.0428

Epoch 896: val_loss did not improve from 28.03565
196/196 - 42s - loss: 27.8551 - MinusLogProbMetric: 27.8551 - val_loss: 28.0428 - val_MinusLogProbMetric: 28.0428 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 897/1000
2023-10-27 10:22:08.726 
Epoch 897/1000 
	 loss: 27.8453, MinusLogProbMetric: 27.8453, val_loss: 28.0894, val_MinusLogProbMetric: 28.0894

Epoch 897: val_loss did not improve from 28.03565
196/196 - 42s - loss: 27.8453 - MinusLogProbMetric: 27.8453 - val_loss: 28.0894 - val_MinusLogProbMetric: 28.0894 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 898/1000
2023-10-27 10:22:53.986 
Epoch 898/1000 
	 loss: 27.8518, MinusLogProbMetric: 27.8518, val_loss: 28.0791, val_MinusLogProbMetric: 28.0791

Epoch 898: val_loss did not improve from 28.03565
196/196 - 45s - loss: 27.8518 - MinusLogProbMetric: 27.8518 - val_loss: 28.0791 - val_MinusLogProbMetric: 28.0791 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 899/1000
2023-10-27 10:23:38.187 
Epoch 899/1000 
	 loss: 27.8604, MinusLogProbMetric: 27.8604, val_loss: 28.1410, val_MinusLogProbMetric: 28.1410

Epoch 899: val_loss did not improve from 28.03565
196/196 - 44s - loss: 27.8604 - MinusLogProbMetric: 27.8604 - val_loss: 28.1410 - val_MinusLogProbMetric: 28.1410 - lr: 1.2500e-04 - 44s/epoch - 225ms/step
Epoch 900/1000
2023-10-27 10:24:22.960 
Epoch 900/1000 
	 loss: 27.8676, MinusLogProbMetric: 27.8676, val_loss: 28.1338, val_MinusLogProbMetric: 28.1338

Epoch 900: val_loss did not improve from 28.03565
196/196 - 45s - loss: 27.8676 - MinusLogProbMetric: 27.8676 - val_loss: 28.1338 - val_MinusLogProbMetric: 28.1338 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 901/1000
2023-10-27 10:25:05.224 
Epoch 901/1000 
	 loss: 27.8513, MinusLogProbMetric: 27.8513, val_loss: 28.2023, val_MinusLogProbMetric: 28.2023

Epoch 901: val_loss did not improve from 28.03565
196/196 - 42s - loss: 27.8513 - MinusLogProbMetric: 27.8513 - val_loss: 28.2023 - val_MinusLogProbMetric: 28.2023 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 902/1000
2023-10-27 10:25:46.682 
Epoch 902/1000 
	 loss: 27.8595, MinusLogProbMetric: 27.8595, val_loss: 28.0534, val_MinusLogProbMetric: 28.0534

Epoch 902: val_loss did not improve from 28.03565
196/196 - 41s - loss: 27.8595 - MinusLogProbMetric: 27.8595 - val_loss: 28.0534 - val_MinusLogProbMetric: 28.0534 - lr: 1.2500e-04 - 41s/epoch - 212ms/step
Epoch 903/1000
2023-10-27 10:26:28.324 
Epoch 903/1000 
	 loss: 27.8497, MinusLogProbMetric: 27.8497, val_loss: 28.0967, val_MinusLogProbMetric: 28.0967

Epoch 903: val_loss did not improve from 28.03565
196/196 - 42s - loss: 27.8497 - MinusLogProbMetric: 27.8497 - val_loss: 28.0967 - val_MinusLogProbMetric: 28.0967 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 904/1000
2023-10-27 10:27:09.681 
Epoch 904/1000 
	 loss: 27.8520, MinusLogProbMetric: 27.8520, val_loss: 28.0902, val_MinusLogProbMetric: 28.0902

Epoch 904: val_loss did not improve from 28.03565
196/196 - 41s - loss: 27.8520 - MinusLogProbMetric: 27.8520 - val_loss: 28.0902 - val_MinusLogProbMetric: 28.0902 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 905/1000
2023-10-27 10:27:50.007 
Epoch 905/1000 
	 loss: 27.8668, MinusLogProbMetric: 27.8668, val_loss: 28.0622, val_MinusLogProbMetric: 28.0622

Epoch 905: val_loss did not improve from 28.03565
196/196 - 40s - loss: 27.8668 - MinusLogProbMetric: 27.8668 - val_loss: 28.0622 - val_MinusLogProbMetric: 28.0622 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 906/1000
2023-10-27 10:28:29.388 
Epoch 906/1000 
	 loss: 27.8490, MinusLogProbMetric: 27.8490, val_loss: 28.0541, val_MinusLogProbMetric: 28.0541

Epoch 906: val_loss did not improve from 28.03565
196/196 - 39s - loss: 27.8490 - MinusLogProbMetric: 27.8490 - val_loss: 28.0541 - val_MinusLogProbMetric: 28.0541 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 907/1000
2023-10-27 10:29:10.315 
Epoch 907/1000 
	 loss: 27.8323, MinusLogProbMetric: 27.8323, val_loss: 28.1819, val_MinusLogProbMetric: 28.1819

Epoch 907: val_loss did not improve from 28.03565
196/196 - 41s - loss: 27.8323 - MinusLogProbMetric: 27.8323 - val_loss: 28.1819 - val_MinusLogProbMetric: 28.1819 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 908/1000
2023-10-27 10:29:50.982 
Epoch 908/1000 
	 loss: 27.8486, MinusLogProbMetric: 27.8486, val_loss: 28.1948, val_MinusLogProbMetric: 28.1948

Epoch 908: val_loss did not improve from 28.03565
196/196 - 41s - loss: 27.8486 - MinusLogProbMetric: 27.8486 - val_loss: 28.1948 - val_MinusLogProbMetric: 28.1948 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 909/1000
2023-10-27 10:30:33.952 
Epoch 909/1000 
	 loss: 27.8401, MinusLogProbMetric: 27.8401, val_loss: 28.0246, val_MinusLogProbMetric: 28.0246

Epoch 909: val_loss improved from 28.03565 to 28.02462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 44s - loss: 27.8401 - MinusLogProbMetric: 27.8401 - val_loss: 28.0246 - val_MinusLogProbMetric: 28.0246 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 910/1000
2023-10-27 10:31:15.281 
Epoch 910/1000 
	 loss: 27.8450, MinusLogProbMetric: 27.8450, val_loss: 28.1154, val_MinusLogProbMetric: 28.1154

Epoch 910: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8450 - MinusLogProbMetric: 27.8450 - val_loss: 28.1154 - val_MinusLogProbMetric: 28.1154 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 911/1000
2023-10-27 10:31:54.925 
Epoch 911/1000 
	 loss: 27.8444, MinusLogProbMetric: 27.8444, val_loss: 28.0396, val_MinusLogProbMetric: 28.0396

Epoch 911: val_loss did not improve from 28.02462
196/196 - 40s - loss: 27.8444 - MinusLogProbMetric: 27.8444 - val_loss: 28.0396 - val_MinusLogProbMetric: 28.0396 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 912/1000
2023-10-27 10:32:36.706 
Epoch 912/1000 
	 loss: 27.8443, MinusLogProbMetric: 27.8443, val_loss: 28.1001, val_MinusLogProbMetric: 28.1001

Epoch 912: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8443 - MinusLogProbMetric: 27.8443 - val_loss: 28.1001 - val_MinusLogProbMetric: 28.1001 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 913/1000
2023-10-27 10:33:18.219 
Epoch 913/1000 
	 loss: 27.8733, MinusLogProbMetric: 27.8733, val_loss: 28.0590, val_MinusLogProbMetric: 28.0590

Epoch 913: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8733 - MinusLogProbMetric: 27.8733 - val_loss: 28.0590 - val_MinusLogProbMetric: 28.0590 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 914/1000
2023-10-27 10:33:59.980 
Epoch 914/1000 
	 loss: 27.8453, MinusLogProbMetric: 27.8453, val_loss: 28.0796, val_MinusLogProbMetric: 28.0796

Epoch 914: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8453 - MinusLogProbMetric: 27.8453 - val_loss: 28.0796 - val_MinusLogProbMetric: 28.0796 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 915/1000
2023-10-27 10:34:40.901 
Epoch 915/1000 
	 loss: 27.8383, MinusLogProbMetric: 27.8383, val_loss: 28.0900, val_MinusLogProbMetric: 28.0900

Epoch 915: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8383 - MinusLogProbMetric: 27.8383 - val_loss: 28.0900 - val_MinusLogProbMetric: 28.0900 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 916/1000
2023-10-27 10:35:23.058 
Epoch 916/1000 
	 loss: 27.8418, MinusLogProbMetric: 27.8418, val_loss: 28.0947, val_MinusLogProbMetric: 28.0947

Epoch 916: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8418 - MinusLogProbMetric: 27.8418 - val_loss: 28.0947 - val_MinusLogProbMetric: 28.0947 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 917/1000
2023-10-27 10:36:05.112 
Epoch 917/1000 
	 loss: 27.8449, MinusLogProbMetric: 27.8449, val_loss: 28.1057, val_MinusLogProbMetric: 28.1057

Epoch 917: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8449 - MinusLogProbMetric: 27.8449 - val_loss: 28.1057 - val_MinusLogProbMetric: 28.1057 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 918/1000
2023-10-27 10:36:46.713 
Epoch 918/1000 
	 loss: 27.8501, MinusLogProbMetric: 27.8501, val_loss: 28.0581, val_MinusLogProbMetric: 28.0581

Epoch 918: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8501 - MinusLogProbMetric: 27.8501 - val_loss: 28.0581 - val_MinusLogProbMetric: 28.0581 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 919/1000
2023-10-27 10:37:27.993 
Epoch 919/1000 
	 loss: 27.8369, MinusLogProbMetric: 27.8369, val_loss: 28.1361, val_MinusLogProbMetric: 28.1361

Epoch 919: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8369 - MinusLogProbMetric: 27.8369 - val_loss: 28.1361 - val_MinusLogProbMetric: 28.1361 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 920/1000
2023-10-27 10:38:08.645 
Epoch 920/1000 
	 loss: 27.8416, MinusLogProbMetric: 27.8416, val_loss: 28.0342, val_MinusLogProbMetric: 28.0342

Epoch 920: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8416 - MinusLogProbMetric: 27.8416 - val_loss: 28.0342 - val_MinusLogProbMetric: 28.0342 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 921/1000
2023-10-27 10:38:49.634 
Epoch 921/1000 
	 loss: 27.8333, MinusLogProbMetric: 27.8333, val_loss: 28.0753, val_MinusLogProbMetric: 28.0753

Epoch 921: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8333 - MinusLogProbMetric: 27.8333 - val_loss: 28.0753 - val_MinusLogProbMetric: 28.0753 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 922/1000
2023-10-27 10:39:30.757 
Epoch 922/1000 
	 loss: 27.8279, MinusLogProbMetric: 27.8279, val_loss: 28.0992, val_MinusLogProbMetric: 28.0992

Epoch 922: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8279 - MinusLogProbMetric: 27.8279 - val_loss: 28.0992 - val_MinusLogProbMetric: 28.0992 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 923/1000
2023-10-27 10:40:12.918 
Epoch 923/1000 
	 loss: 27.8405, MinusLogProbMetric: 27.8405, val_loss: 28.0917, val_MinusLogProbMetric: 28.0917

Epoch 923: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8405 - MinusLogProbMetric: 27.8405 - val_loss: 28.0917 - val_MinusLogProbMetric: 28.0917 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 924/1000
2023-10-27 10:40:54.818 
Epoch 924/1000 
	 loss: 27.8275, MinusLogProbMetric: 27.8275, val_loss: 28.0568, val_MinusLogProbMetric: 28.0568

Epoch 924: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8275 - MinusLogProbMetric: 27.8275 - val_loss: 28.0568 - val_MinusLogProbMetric: 28.0568 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 925/1000
2023-10-27 10:41:35.992 
Epoch 925/1000 
	 loss: 27.8324, MinusLogProbMetric: 27.8324, val_loss: 28.1514, val_MinusLogProbMetric: 28.1514

Epoch 925: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8324 - MinusLogProbMetric: 27.8324 - val_loss: 28.1514 - val_MinusLogProbMetric: 28.1514 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 926/1000
2023-10-27 10:42:16.210 
Epoch 926/1000 
	 loss: 27.8319, MinusLogProbMetric: 27.8319, val_loss: 28.0848, val_MinusLogProbMetric: 28.0848

Epoch 926: val_loss did not improve from 28.02462
196/196 - 40s - loss: 27.8319 - MinusLogProbMetric: 27.8319 - val_loss: 28.0848 - val_MinusLogProbMetric: 28.0848 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 927/1000
2023-10-27 10:42:57.295 
Epoch 927/1000 
	 loss: 27.8333, MinusLogProbMetric: 27.8333, val_loss: 28.0739, val_MinusLogProbMetric: 28.0739

Epoch 927: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8333 - MinusLogProbMetric: 27.8333 - val_loss: 28.0739 - val_MinusLogProbMetric: 28.0739 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 928/1000
2023-10-27 10:43:39.850 
Epoch 928/1000 
	 loss: 27.8325, MinusLogProbMetric: 27.8325, val_loss: 28.0455, val_MinusLogProbMetric: 28.0455

Epoch 928: val_loss did not improve from 28.02462
196/196 - 43s - loss: 27.8325 - MinusLogProbMetric: 27.8325 - val_loss: 28.0455 - val_MinusLogProbMetric: 28.0455 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 929/1000
2023-10-27 10:44:21.702 
Epoch 929/1000 
	 loss: 27.8344, MinusLogProbMetric: 27.8344, val_loss: 28.0576, val_MinusLogProbMetric: 28.0576

Epoch 929: val_loss did not improve from 28.02462
196/196 - 42s - loss: 27.8344 - MinusLogProbMetric: 27.8344 - val_loss: 28.0576 - val_MinusLogProbMetric: 28.0576 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 930/1000
2023-10-27 10:45:02.440 
Epoch 930/1000 
	 loss: 27.8417, MinusLogProbMetric: 27.8417, val_loss: 28.1182, val_MinusLogProbMetric: 28.1182

Epoch 930: val_loss did not improve from 28.02462
196/196 - 41s - loss: 27.8417 - MinusLogProbMetric: 27.8417 - val_loss: 28.1182 - val_MinusLogProbMetric: 28.1182 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 931/1000
2023-10-27 10:45:43.822 
Epoch 931/1000 
	 loss: 27.8256, MinusLogProbMetric: 27.8256, val_loss: 28.0203, val_MinusLogProbMetric: 28.0203

Epoch 931: val_loss improved from 28.02462 to 28.02027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 27.8256 - MinusLogProbMetric: 27.8256 - val_loss: 28.0203 - val_MinusLogProbMetric: 28.0203 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 932/1000
2023-10-27 10:46:25.898 
Epoch 932/1000 
	 loss: 27.8251, MinusLogProbMetric: 27.8251, val_loss: 28.0868, val_MinusLogProbMetric: 28.0868

Epoch 932: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8251 - MinusLogProbMetric: 27.8251 - val_loss: 28.0868 - val_MinusLogProbMetric: 28.0868 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 933/1000
2023-10-27 10:47:06.819 
Epoch 933/1000 
	 loss: 27.8281, MinusLogProbMetric: 27.8281, val_loss: 28.0299, val_MinusLogProbMetric: 28.0299

Epoch 933: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8281 - MinusLogProbMetric: 27.8281 - val_loss: 28.0299 - val_MinusLogProbMetric: 28.0299 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 934/1000
2023-10-27 10:47:47.466 
Epoch 934/1000 
	 loss: 27.8266, MinusLogProbMetric: 27.8266, val_loss: 28.0459, val_MinusLogProbMetric: 28.0459

Epoch 934: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8266 - MinusLogProbMetric: 27.8266 - val_loss: 28.0459 - val_MinusLogProbMetric: 28.0459 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 935/1000
2023-10-27 10:48:29.587 
Epoch 935/1000 
	 loss: 27.8371, MinusLogProbMetric: 27.8371, val_loss: 28.1889, val_MinusLogProbMetric: 28.1889

Epoch 935: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8371 - MinusLogProbMetric: 27.8371 - val_loss: 28.1889 - val_MinusLogProbMetric: 28.1889 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 936/1000
2023-10-27 10:49:11.103 
Epoch 936/1000 
	 loss: 27.8414, MinusLogProbMetric: 27.8414, val_loss: 28.0751, val_MinusLogProbMetric: 28.0751

Epoch 936: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8414 - MinusLogProbMetric: 27.8414 - val_loss: 28.0751 - val_MinusLogProbMetric: 28.0751 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 937/1000
2023-10-27 10:49:53.289 
Epoch 937/1000 
	 loss: 27.8268, MinusLogProbMetric: 27.8268, val_loss: 28.0551, val_MinusLogProbMetric: 28.0551

Epoch 937: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8268 - MinusLogProbMetric: 27.8268 - val_loss: 28.0551 - val_MinusLogProbMetric: 28.0551 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 938/1000
2023-10-27 10:50:33.684 
Epoch 938/1000 
	 loss: 27.8329, MinusLogProbMetric: 27.8329, val_loss: 28.0345, val_MinusLogProbMetric: 28.0345

Epoch 938: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8329 - MinusLogProbMetric: 27.8329 - val_loss: 28.0345 - val_MinusLogProbMetric: 28.0345 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 939/1000
2023-10-27 10:51:15.577 
Epoch 939/1000 
	 loss: 27.8219, MinusLogProbMetric: 27.8219, val_loss: 28.0314, val_MinusLogProbMetric: 28.0314

Epoch 939: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8219 - MinusLogProbMetric: 27.8219 - val_loss: 28.0314 - val_MinusLogProbMetric: 28.0314 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 940/1000
2023-10-27 10:51:55.780 
Epoch 940/1000 
	 loss: 27.8303, MinusLogProbMetric: 27.8303, val_loss: 28.0240, val_MinusLogProbMetric: 28.0240

Epoch 940: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8303 - MinusLogProbMetric: 27.8303 - val_loss: 28.0240 - val_MinusLogProbMetric: 28.0240 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 941/1000
2023-10-27 10:52:37.899 
Epoch 941/1000 
	 loss: 27.8416, MinusLogProbMetric: 27.8416, val_loss: 28.1039, val_MinusLogProbMetric: 28.1039

Epoch 941: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8416 - MinusLogProbMetric: 27.8416 - val_loss: 28.1039 - val_MinusLogProbMetric: 28.1039 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 942/1000
2023-10-27 10:53:18.621 
Epoch 942/1000 
	 loss: 27.8526, MinusLogProbMetric: 27.8526, val_loss: 28.0697, val_MinusLogProbMetric: 28.0697

Epoch 942: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8526 - MinusLogProbMetric: 27.8526 - val_loss: 28.0697 - val_MinusLogProbMetric: 28.0697 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 943/1000
2023-10-27 10:53:59.812 
Epoch 943/1000 
	 loss: 27.8345, MinusLogProbMetric: 27.8345, val_loss: 28.0534, val_MinusLogProbMetric: 28.0534

Epoch 943: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8345 - MinusLogProbMetric: 27.8345 - val_loss: 28.0534 - val_MinusLogProbMetric: 28.0534 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 944/1000
2023-10-27 10:54:41.328 
Epoch 944/1000 
	 loss: 27.8319, MinusLogProbMetric: 27.8319, val_loss: 28.0778, val_MinusLogProbMetric: 28.0778

Epoch 944: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8319 - MinusLogProbMetric: 27.8319 - val_loss: 28.0778 - val_MinusLogProbMetric: 28.0778 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 945/1000
2023-10-27 10:55:22.930 
Epoch 945/1000 
	 loss: 27.8417, MinusLogProbMetric: 27.8417, val_loss: 28.0566, val_MinusLogProbMetric: 28.0566

Epoch 945: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8417 - MinusLogProbMetric: 27.8417 - val_loss: 28.0566 - val_MinusLogProbMetric: 28.0566 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 946/1000
2023-10-27 10:56:04.659 
Epoch 946/1000 
	 loss: 27.8474, MinusLogProbMetric: 27.8474, val_loss: 28.0559, val_MinusLogProbMetric: 28.0559

Epoch 946: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8474 - MinusLogProbMetric: 27.8474 - val_loss: 28.0559 - val_MinusLogProbMetric: 28.0559 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 947/1000
2023-10-27 10:56:46.482 
Epoch 947/1000 
	 loss: 27.8302, MinusLogProbMetric: 27.8302, val_loss: 28.0464, val_MinusLogProbMetric: 28.0464

Epoch 947: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8302 - MinusLogProbMetric: 27.8302 - val_loss: 28.0464 - val_MinusLogProbMetric: 28.0464 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 948/1000
2023-10-27 10:57:26.744 
Epoch 948/1000 
	 loss: 27.8234, MinusLogProbMetric: 27.8234, val_loss: 28.0346, val_MinusLogProbMetric: 28.0346

Epoch 948: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8234 - MinusLogProbMetric: 27.8234 - val_loss: 28.0346 - val_MinusLogProbMetric: 28.0346 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 949/1000
2023-10-27 10:58:07.094 
Epoch 949/1000 
	 loss: 27.8295, MinusLogProbMetric: 27.8295, val_loss: 28.0224, val_MinusLogProbMetric: 28.0224

Epoch 949: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8295 - MinusLogProbMetric: 27.8295 - val_loss: 28.0224 - val_MinusLogProbMetric: 28.0224 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 950/1000
2023-10-27 10:58:48.543 
Epoch 950/1000 
	 loss: 27.8324, MinusLogProbMetric: 27.8324, val_loss: 28.0498, val_MinusLogProbMetric: 28.0498

Epoch 950: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8324 - MinusLogProbMetric: 27.8324 - val_loss: 28.0498 - val_MinusLogProbMetric: 28.0498 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 951/1000
2023-10-27 10:59:30.030 
Epoch 951/1000 
	 loss: 27.8256, MinusLogProbMetric: 27.8256, val_loss: 28.0348, val_MinusLogProbMetric: 28.0348

Epoch 951: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8256 - MinusLogProbMetric: 27.8256 - val_loss: 28.0348 - val_MinusLogProbMetric: 28.0348 - lr: 1.2500e-04 - 41s/epoch - 212ms/step
Epoch 952/1000
2023-10-27 11:00:11.685 
Epoch 952/1000 
	 loss: 27.8266, MinusLogProbMetric: 27.8266, val_loss: 28.1450, val_MinusLogProbMetric: 28.1450

Epoch 952: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8266 - MinusLogProbMetric: 27.8266 - val_loss: 28.1450 - val_MinusLogProbMetric: 28.1450 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 953/1000
2023-10-27 11:00:52.768 
Epoch 953/1000 
	 loss: 27.8387, MinusLogProbMetric: 27.8387, val_loss: 28.1433, val_MinusLogProbMetric: 28.1433

Epoch 953: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8387 - MinusLogProbMetric: 27.8387 - val_loss: 28.1433 - val_MinusLogProbMetric: 28.1433 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 954/1000
2023-10-27 11:01:34.533 
Epoch 954/1000 
	 loss: 27.8495, MinusLogProbMetric: 27.8495, val_loss: 28.1277, val_MinusLogProbMetric: 28.1277

Epoch 954: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8495 - MinusLogProbMetric: 27.8495 - val_loss: 28.1277 - val_MinusLogProbMetric: 28.1277 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 955/1000
2023-10-27 11:02:16.128 
Epoch 955/1000 
	 loss: 27.8439, MinusLogProbMetric: 27.8439, val_loss: 28.0923, val_MinusLogProbMetric: 28.0923

Epoch 955: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8439 - MinusLogProbMetric: 27.8439 - val_loss: 28.0923 - val_MinusLogProbMetric: 28.0923 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 956/1000
2023-10-27 11:02:58.557 
Epoch 956/1000 
	 loss: 27.8356, MinusLogProbMetric: 27.8356, val_loss: 28.0753, val_MinusLogProbMetric: 28.0753

Epoch 956: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8356 - MinusLogProbMetric: 27.8356 - val_loss: 28.0753 - val_MinusLogProbMetric: 28.0753 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 957/1000
2023-10-27 11:03:39.716 
Epoch 957/1000 
	 loss: 27.8292, MinusLogProbMetric: 27.8292, val_loss: 28.1727, val_MinusLogProbMetric: 28.1727

Epoch 957: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8292 - MinusLogProbMetric: 27.8292 - val_loss: 28.1727 - val_MinusLogProbMetric: 28.1727 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 958/1000
2023-10-27 11:04:20.991 
Epoch 958/1000 
	 loss: 27.8329, MinusLogProbMetric: 27.8329, val_loss: 28.0770, val_MinusLogProbMetric: 28.0770

Epoch 958: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8329 - MinusLogProbMetric: 27.8329 - val_loss: 28.0770 - val_MinusLogProbMetric: 28.0770 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 959/1000
2023-10-27 11:05:01.937 
Epoch 959/1000 
	 loss: 27.8429, MinusLogProbMetric: 27.8429, val_loss: 28.0514, val_MinusLogProbMetric: 28.0514

Epoch 959: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8429 - MinusLogProbMetric: 27.8429 - val_loss: 28.0514 - val_MinusLogProbMetric: 28.0514 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 960/1000
2023-10-27 11:05:42.051 
Epoch 960/1000 
	 loss: 27.8419, MinusLogProbMetric: 27.8419, val_loss: 28.2119, val_MinusLogProbMetric: 28.2119

Epoch 960: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8419 - MinusLogProbMetric: 27.8419 - val_loss: 28.2119 - val_MinusLogProbMetric: 28.2119 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 961/1000
2023-10-27 11:06:22.134 
Epoch 961/1000 
	 loss: 27.8419, MinusLogProbMetric: 27.8419, val_loss: 28.1637, val_MinusLogProbMetric: 28.1637

Epoch 961: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8419 - MinusLogProbMetric: 27.8419 - val_loss: 28.1637 - val_MinusLogProbMetric: 28.1637 - lr: 1.2500e-04 - 40s/epoch - 204ms/step
Epoch 962/1000
2023-10-27 11:07:03.584 
Epoch 962/1000 
	 loss: 27.8388, MinusLogProbMetric: 27.8388, val_loss: 28.0686, val_MinusLogProbMetric: 28.0686

Epoch 962: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8388 - MinusLogProbMetric: 27.8388 - val_loss: 28.0686 - val_MinusLogProbMetric: 28.0686 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 963/1000
2023-10-27 11:07:45.265 
Epoch 963/1000 
	 loss: 27.8527, MinusLogProbMetric: 27.8527, val_loss: 28.0829, val_MinusLogProbMetric: 28.0829

Epoch 963: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8527 - MinusLogProbMetric: 27.8527 - val_loss: 28.0829 - val_MinusLogProbMetric: 28.0829 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 964/1000
2023-10-27 11:08:26.962 
Epoch 964/1000 
	 loss: 27.8361, MinusLogProbMetric: 27.8361, val_loss: 28.0676, val_MinusLogProbMetric: 28.0676

Epoch 964: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8361 - MinusLogProbMetric: 27.8361 - val_loss: 28.0676 - val_MinusLogProbMetric: 28.0676 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 965/1000
2023-10-27 11:09:06.632 
Epoch 965/1000 
	 loss: 27.8382, MinusLogProbMetric: 27.8382, val_loss: 28.1232, val_MinusLogProbMetric: 28.1232

Epoch 965: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8382 - MinusLogProbMetric: 27.8382 - val_loss: 28.1232 - val_MinusLogProbMetric: 28.1232 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 966/1000
2023-10-27 11:09:46.959 
Epoch 966/1000 
	 loss: 27.8323, MinusLogProbMetric: 27.8323, val_loss: 28.0641, val_MinusLogProbMetric: 28.0641

Epoch 966: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8323 - MinusLogProbMetric: 27.8323 - val_loss: 28.0641 - val_MinusLogProbMetric: 28.0641 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 967/1000
2023-10-27 11:10:28.613 
Epoch 967/1000 
	 loss: 27.8377, MinusLogProbMetric: 27.8377, val_loss: 28.0747, val_MinusLogProbMetric: 28.0747

Epoch 967: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8377 - MinusLogProbMetric: 27.8377 - val_loss: 28.0747 - val_MinusLogProbMetric: 28.0747 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 968/1000
2023-10-27 11:11:09.534 
Epoch 968/1000 
	 loss: 27.8288, MinusLogProbMetric: 27.8288, val_loss: 28.1015, val_MinusLogProbMetric: 28.1015

Epoch 968: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8288 - MinusLogProbMetric: 27.8288 - val_loss: 28.1015 - val_MinusLogProbMetric: 28.1015 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 969/1000
2023-10-27 11:11:51.178 
Epoch 969/1000 
	 loss: 27.8351, MinusLogProbMetric: 27.8351, val_loss: 28.0877, val_MinusLogProbMetric: 28.0877

Epoch 969: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8351 - MinusLogProbMetric: 27.8351 - val_loss: 28.0877 - val_MinusLogProbMetric: 28.0877 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 970/1000
2023-10-27 11:12:33.524 
Epoch 970/1000 
	 loss: 27.8355, MinusLogProbMetric: 27.8355, val_loss: 28.0350, val_MinusLogProbMetric: 28.0350

Epoch 970: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8355 - MinusLogProbMetric: 27.8355 - val_loss: 28.0350 - val_MinusLogProbMetric: 28.0350 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 971/1000
2023-10-27 11:13:14.897 
Epoch 971/1000 
	 loss: 27.8373, MinusLogProbMetric: 27.8373, val_loss: 28.0535, val_MinusLogProbMetric: 28.0535

Epoch 971: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8373 - MinusLogProbMetric: 27.8373 - val_loss: 28.0535 - val_MinusLogProbMetric: 28.0535 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 972/1000
2023-10-27 11:13:56.477 
Epoch 972/1000 
	 loss: 27.8283, MinusLogProbMetric: 27.8283, val_loss: 28.0377, val_MinusLogProbMetric: 28.0377

Epoch 972: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8283 - MinusLogProbMetric: 27.8283 - val_loss: 28.0377 - val_MinusLogProbMetric: 28.0377 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 973/1000
2023-10-27 11:14:36.193 
Epoch 973/1000 
	 loss: 27.8351, MinusLogProbMetric: 27.8351, val_loss: 28.0660, val_MinusLogProbMetric: 28.0660

Epoch 973: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8351 - MinusLogProbMetric: 27.8351 - val_loss: 28.0660 - val_MinusLogProbMetric: 28.0660 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 974/1000
2023-10-27 11:15:16.421 
Epoch 974/1000 
	 loss: 27.8344, MinusLogProbMetric: 27.8344, val_loss: 28.0707, val_MinusLogProbMetric: 28.0707

Epoch 974: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8344 - MinusLogProbMetric: 27.8344 - val_loss: 28.0707 - val_MinusLogProbMetric: 28.0707 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 975/1000
2023-10-27 11:15:56.280 
Epoch 975/1000 
	 loss: 27.8283, MinusLogProbMetric: 27.8283, val_loss: 28.1025, val_MinusLogProbMetric: 28.1025

Epoch 975: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8283 - MinusLogProbMetric: 27.8283 - val_loss: 28.1025 - val_MinusLogProbMetric: 28.1025 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 976/1000
2023-10-27 11:16:36.399 
Epoch 976/1000 
	 loss: 27.8295, MinusLogProbMetric: 27.8295, val_loss: 28.0720, val_MinusLogProbMetric: 28.0720

Epoch 976: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8295 - MinusLogProbMetric: 27.8295 - val_loss: 28.0720 - val_MinusLogProbMetric: 28.0720 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 977/1000
2023-10-27 11:17:18.278 
Epoch 977/1000 
	 loss: 27.8276, MinusLogProbMetric: 27.8276, val_loss: 28.0644, val_MinusLogProbMetric: 28.0644

Epoch 977: val_loss did not improve from 28.02027
196/196 - 42s - loss: 27.8276 - MinusLogProbMetric: 27.8276 - val_loss: 28.0644 - val_MinusLogProbMetric: 28.0644 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 978/1000
2023-10-27 11:17:57.656 
Epoch 978/1000 
	 loss: 27.8301, MinusLogProbMetric: 27.8301, val_loss: 28.0647, val_MinusLogProbMetric: 28.0647

Epoch 978: val_loss did not improve from 28.02027
196/196 - 39s - loss: 27.8301 - MinusLogProbMetric: 27.8301 - val_loss: 28.0647 - val_MinusLogProbMetric: 28.0647 - lr: 1.2500e-04 - 39s/epoch - 201ms/step
Epoch 979/1000
2023-10-27 11:18:38.440 
Epoch 979/1000 
	 loss: 27.8382, MinusLogProbMetric: 27.8382, val_loss: 28.0809, val_MinusLogProbMetric: 28.0809

Epoch 979: val_loss did not improve from 28.02027
196/196 - 41s - loss: 27.8382 - MinusLogProbMetric: 27.8382 - val_loss: 28.0809 - val_MinusLogProbMetric: 28.0809 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 980/1000
2023-10-27 11:19:17.122 
Epoch 980/1000 
	 loss: 27.8352, MinusLogProbMetric: 27.8352, val_loss: 28.0449, val_MinusLogProbMetric: 28.0449

Epoch 980: val_loss did not improve from 28.02027
196/196 - 39s - loss: 27.8352 - MinusLogProbMetric: 27.8352 - val_loss: 28.0449 - val_MinusLogProbMetric: 28.0449 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 981/1000
2023-10-27 11:19:56.870 
Epoch 981/1000 
	 loss: 27.8274, MinusLogProbMetric: 27.8274, val_loss: 28.0628, val_MinusLogProbMetric: 28.0628

Epoch 981: val_loss did not improve from 28.02027
196/196 - 40s - loss: 27.8274 - MinusLogProbMetric: 27.8274 - val_loss: 28.0628 - val_MinusLogProbMetric: 28.0628 - lr: 1.2500e-04 - 40s/epoch - 203ms/step
Epoch 982/1000
2023-10-27 11:20:37.101 
Epoch 982/1000 
	 loss: 27.7818, MinusLogProbMetric: 27.7818, val_loss: 28.0122, val_MinusLogProbMetric: 28.0122

Epoch 982: val_loss improved from 28.02027 to 28.01215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 41s - loss: 27.7818 - MinusLogProbMetric: 27.7818 - val_loss: 28.0122 - val_MinusLogProbMetric: 28.0122 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Epoch 983/1000
2023-10-27 11:21:17.817 
Epoch 983/1000 
	 loss: 27.7798, MinusLogProbMetric: 27.7798, val_loss: 28.0203, val_MinusLogProbMetric: 28.0203

Epoch 983: val_loss did not improve from 28.01215
196/196 - 40s - loss: 27.7798 - MinusLogProbMetric: 27.7798 - val_loss: 28.0203 - val_MinusLogProbMetric: 28.0203 - lr: 6.2500e-05 - 40s/epoch - 204ms/step
Epoch 984/1000
2023-10-27 11:21:57.838 
Epoch 984/1000 
	 loss: 27.7850, MinusLogProbMetric: 27.7850, val_loss: 28.0111, val_MinusLogProbMetric: 28.0111

Epoch 984: val_loss improved from 28.01215 to 28.01113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 41s - loss: 27.7850 - MinusLogProbMetric: 27.7850 - val_loss: 28.0111 - val_MinusLogProbMetric: 28.0111 - lr: 6.2500e-05 - 41s/epoch - 208ms/step
Epoch 985/1000
2023-10-27 11:22:37.384 
Epoch 985/1000 
	 loss: 27.7778, MinusLogProbMetric: 27.7778, val_loss: 27.9972, val_MinusLogProbMetric: 27.9972

Epoch 985: val_loss improved from 28.01113 to 27.99725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 39s - loss: 27.7778 - MinusLogProbMetric: 27.7778 - val_loss: 27.9972 - val_MinusLogProbMetric: 27.9972 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 986/1000
2023-10-27 11:23:17.267 
Epoch 986/1000 
	 loss: 27.7822, MinusLogProbMetric: 27.7822, val_loss: 28.0323, val_MinusLogProbMetric: 28.0323

Epoch 986: val_loss did not improve from 27.99725
196/196 - 39s - loss: 27.7822 - MinusLogProbMetric: 27.7822 - val_loss: 28.0323 - val_MinusLogProbMetric: 28.0323 - lr: 6.2500e-05 - 39s/epoch - 200ms/step
Epoch 987/1000
2023-10-27 11:23:58.255 
Epoch 987/1000 
	 loss: 27.7876, MinusLogProbMetric: 27.7876, val_loss: 28.0154, val_MinusLogProbMetric: 28.0154

Epoch 987: val_loss did not improve from 27.99725
196/196 - 41s - loss: 27.7876 - MinusLogProbMetric: 27.7876 - val_loss: 28.0154 - val_MinusLogProbMetric: 28.0154 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Epoch 988/1000
2023-10-27 11:24:37.990 
Epoch 988/1000 
	 loss: 27.7812, MinusLogProbMetric: 27.7812, val_loss: 27.9989, val_MinusLogProbMetric: 27.9989

Epoch 988: val_loss did not improve from 27.99725
196/196 - 40s - loss: 27.7812 - MinusLogProbMetric: 27.7812 - val_loss: 27.9989 - val_MinusLogProbMetric: 27.9989 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 989/1000
2023-10-27 11:25:17.565 
Epoch 989/1000 
	 loss: 27.7836, MinusLogProbMetric: 27.7836, val_loss: 28.0192, val_MinusLogProbMetric: 28.0192

Epoch 989: val_loss did not improve from 27.99725
196/196 - 40s - loss: 27.7836 - MinusLogProbMetric: 27.7836 - val_loss: 28.0192 - val_MinusLogProbMetric: 28.0192 - lr: 6.2500e-05 - 40s/epoch - 202ms/step
Epoch 990/1000
2023-10-27 11:25:57.348 
Epoch 990/1000 
	 loss: 27.7845, MinusLogProbMetric: 27.7845, val_loss: 28.0257, val_MinusLogProbMetric: 28.0257

Epoch 990: val_loss did not improve from 27.99725
196/196 - 40s - loss: 27.7845 - MinusLogProbMetric: 27.7845 - val_loss: 28.0257 - val_MinusLogProbMetric: 28.0257 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 991/1000
2023-10-27 11:26:37.961 
Epoch 991/1000 
	 loss: 27.7826, MinusLogProbMetric: 27.7826, val_loss: 28.0036, val_MinusLogProbMetric: 28.0036

Epoch 991: val_loss did not improve from 27.99725
196/196 - 41s - loss: 27.7826 - MinusLogProbMetric: 27.7826 - val_loss: 28.0036 - val_MinusLogProbMetric: 28.0036 - lr: 6.2500e-05 - 41s/epoch - 207ms/step
Epoch 992/1000
2023-10-27 11:27:16.521 
Epoch 992/1000 
	 loss: 27.7806, MinusLogProbMetric: 27.7806, val_loss: 28.0271, val_MinusLogProbMetric: 28.0271

Epoch 992: val_loss did not improve from 27.99725
196/196 - 39s - loss: 27.7806 - MinusLogProbMetric: 27.7806 - val_loss: 28.0271 - val_MinusLogProbMetric: 28.0271 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 993/1000
2023-10-27 11:27:57.738 
Epoch 993/1000 
	 loss: 27.7842, MinusLogProbMetric: 27.7842, val_loss: 28.0060, val_MinusLogProbMetric: 28.0060

Epoch 993: val_loss did not improve from 27.99725
196/196 - 41s - loss: 27.7842 - MinusLogProbMetric: 27.7842 - val_loss: 28.0060 - val_MinusLogProbMetric: 28.0060 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 994/1000
2023-10-27 11:28:39.145 
Epoch 994/1000 
	 loss: 27.7792, MinusLogProbMetric: 27.7792, val_loss: 27.9955, val_MinusLogProbMetric: 27.9955

Epoch 994: val_loss improved from 27.99725 to 27.99553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_395/weights/best_weights.h5
196/196 - 42s - loss: 27.7792 - MinusLogProbMetric: 27.7792 - val_loss: 27.9955 - val_MinusLogProbMetric: 27.9955 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 995/1000
2023-10-27 11:29:20.154 
Epoch 995/1000 
	 loss: 27.7756, MinusLogProbMetric: 27.7756, val_loss: 28.0181, val_MinusLogProbMetric: 28.0181

Epoch 995: val_loss did not improve from 27.99553
196/196 - 40s - loss: 27.7756 - MinusLogProbMetric: 27.7756 - val_loss: 28.0181 - val_MinusLogProbMetric: 28.0181 - lr: 6.2500e-05 - 40s/epoch - 206ms/step
Epoch 996/1000
2023-10-27 11:30:02.324 
Epoch 996/1000 
	 loss: 27.7795, MinusLogProbMetric: 27.7795, val_loss: 28.0188, val_MinusLogProbMetric: 28.0188

Epoch 996: val_loss did not improve from 27.99553
196/196 - 42s - loss: 27.7795 - MinusLogProbMetric: 27.7795 - val_loss: 28.0188 - val_MinusLogProbMetric: 28.0188 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 997/1000
2023-10-27 11:30:41.676 
Epoch 997/1000 
	 loss: 27.7811, MinusLogProbMetric: 27.7811, val_loss: 28.0010, val_MinusLogProbMetric: 28.0010

Epoch 997: val_loss did not improve from 27.99553
196/196 - 39s - loss: 27.7811 - MinusLogProbMetric: 27.7811 - val_loss: 28.0010 - val_MinusLogProbMetric: 28.0010 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 998/1000
2023-10-27 11:31:21.563 
Epoch 998/1000 
	 loss: 27.7789, MinusLogProbMetric: 27.7789, val_loss: 28.0089, val_MinusLogProbMetric: 28.0089

Epoch 998: val_loss did not improve from 27.99553
196/196 - 40s - loss: 27.7789 - MinusLogProbMetric: 27.7789 - val_loss: 28.0089 - val_MinusLogProbMetric: 28.0089 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 999/1000
2023-10-27 11:32:02.318 
Epoch 999/1000 
	 loss: 27.7754, MinusLogProbMetric: 27.7754, val_loss: 28.0507, val_MinusLogProbMetric: 28.0507

Epoch 999: val_loss did not improve from 27.99553
196/196 - 41s - loss: 27.7754 - MinusLogProbMetric: 27.7754 - val_loss: 28.0507 - val_MinusLogProbMetric: 28.0507 - lr: 6.2500e-05 - 41s/epoch - 208ms/step
Epoch 1000/1000
2023-10-27 11:32:43.297 
Epoch 1000/1000 
	 loss: 27.7768, MinusLogProbMetric: 27.7768, val_loss: 28.0054, val_MinusLogProbMetric: 28.0054

Epoch 1000: val_loss did not improve from 27.99553
196/196 - 41s - loss: 27.7768 - MinusLogProbMetric: 27.7768 - val_loss: 28.0054 - val_MinusLogProbMetric: 28.0054 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 933.
Model trained in 40700.73 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.69 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.11 s.
===========
Run 395/720 done in 40708.27 s.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_399/ already exists.
Skipping it.
===========
Run 399/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_400/ already exists.
Skipping it.
===========
Run 400/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_401/ already exists.
Skipping it.
===========
Run 401/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_402/ already exists.
Skipping it.
===========
Run 402/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_403/ already exists.
Skipping it.
===========
Run 403/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_404/ already exists.
Skipping it.
===========
Run 404/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

===========
Generating train data for run 407.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_71"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_72 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  2653020   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f3743141a20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3743007580>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3743007580>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e02805ab0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3742709f30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f374270a4a0>, <keras.callbacks.ModelCheckpoint object at 0x7f374270a560>, <keras.callbacks.EarlyStopping object at 0x7f374270a7d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f374270a800>, <keras.callbacks.TerminateOnNaN object at 0x7f374270a440>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:32:54.721913
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:35:32.493 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 157s/epoch - 803ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 407.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_82"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_83 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  2653020   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f3dbe432890>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3dc664ab90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3dc664ab90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e25097460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3dbe468fd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3dbe469540>, <keras.callbacks.ModelCheckpoint object at 0x7f3dbe469600>, <keras.callbacks.EarlyStopping object at 0x7f3dbe469870>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3dbe4698a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3dbe4694e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:35:43.396122
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:38:22.310 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 159s/epoch - 810ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 407.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_93"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_94 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  2653020   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f35b436fdf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35b443bb20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35b443bb20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3db51f6770>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3db5031c30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3db50321a0>, <keras.callbacks.ModelCheckpoint object at 0x7f3db5032260>, <keras.callbacks.EarlyStopping object at 0x7f3db50324d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3db5032500>, <keras.callbacks.TerminateOnNaN object at 0x7f3db5032140>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:38:31.388329
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:41:07.939 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 156s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 156s/epoch - 798ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 407.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_104"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_105 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  2653020   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f3dbde6ada0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35d46c4880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35d46c4880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3dbdbcc5e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3dbdec94e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3dbdec9a50>, <keras.callbacks.ModelCheckpoint object at 0x7f3dbdec9b10>, <keras.callbacks.EarlyStopping object at 0x7f3dbdec9d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3dbdec9db0>, <keras.callbacks.TerminateOnNaN object at 0x7f3dbdec99f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:41:18.457079
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 196004 calls to <function Model.make_train_function.<locals>.train_function at 0x7f36e983fbe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:43:59.422 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 161s/epoch - 819ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 407.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_115"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_116 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f39d4468ac0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f38b40fe560>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f38b40fe560>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38b41b1c90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36e9708370>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36e97088e0>, <keras.callbacks.ModelCheckpoint object at 0x7f36e97089a0>, <keras.callbacks.EarlyStopping object at 0x7f36e9708c10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36e9708c40>, <keras.callbacks.TerminateOnNaN object at 0x7f36e9708880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:44:07.920435
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 196005 calls to <function Model.make_train_function.<locals>.train_function at 0x7f38f8d65360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:46:44.758 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 157s/epoch - 800ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 407.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f39341d1f60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f377426ee00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f377426ee00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38542ec700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f38d44b3490>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f38d44b2d70>, <keras.callbacks.ModelCheckpoint object at 0x7f38d44b2a70>, <keras.callbacks.EarlyStopping object at 0x7f38d44b14e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f38d44b15a0>, <keras.callbacks.TerminateOnNaN object at 0x7f38d44b3b50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:46:54.213875
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:49:31.369 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 157s/epoch - 801ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 407.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_137"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_138 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f36cbd15c60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f374aa79c30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f374aa79c30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36e92a2aa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36eb847880>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36eb847df0>, <keras.callbacks.ModelCheckpoint object at 0x7f36eb847eb0>, <keras.callbacks.EarlyStopping object at 0x7f36eb847f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36eb847f40>, <keras.callbacks.TerminateOnNaN object at 0x7f36eb847dc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:49:39.411390
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:52:16.433 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 157s/epoch - 801ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 407.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_148"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_149 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f379c5714b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f374087c850>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f374087c850>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3854308fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e01ab0be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e01ab1750>, <keras.callbacks.ModelCheckpoint object at 0x7f3e01ab18a0>, <keras.callbacks.EarlyStopping object at 0x7f3e01ab1c00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e01ab1ea0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e01ab1810>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:52:27.480554
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:55:05.880 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 158s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 158s/epoch - 807ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 407.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_159"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_160 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f373a5250f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36bb13ff40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36bb13ff40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38f40717e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36baf95150>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36baf956c0>, <keras.callbacks.ModelCheckpoint object at 0x7f36baf95780>, <keras.callbacks.EarlyStopping object at 0x7f36baf959f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36baf95a20>, <keras.callbacks.TerminateOnNaN object at 0x7f36baf95660>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:55:16.121138
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 11:57:53.855 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 158s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 158s/epoch - 805ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 407.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_170"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_171 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f381c3b7cd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f374afae020>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f374afae020>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38f8e98730>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f381c38d1b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f381c38cc40>, <keras.callbacks.ModelCheckpoint object at 0x7f381c38cb80>, <keras.callbacks.EarlyStopping object at 0x7f381c38c940>, <keras.callbacks.ReduceLROnPlateau object at 0x7f381c38c8b0>, <keras.callbacks.TerminateOnNaN object at 0x7f381c38ccd0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 11:58:05.295429
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:00:44.127 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 159s/epoch - 809ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 407.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_407/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_407
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_181"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_182 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f36ca972ef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36b9bcbdc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36b9bcbdc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f374aa4ebc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3670c43c40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_407/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3670c541f0>, <keras.callbacks.ModelCheckpoint object at 0x7f3670c542b0>, <keras.callbacks.EarlyStopping object at 0x7f3670c54520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3670c54550>, <keras.callbacks.TerminateOnNaN object at 0x7f3670c54190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_407/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 407/720 with hyperparameters:
timestamp = 2023-10-27 12:00:52.835820
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:03:35.733 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 13115.7061, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 13115.7061 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 163s/epoch - 830ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 407/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 408.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_192"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_193 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f36b88a7e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36e8be3250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36e8be3250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36c8bac970>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36b88cf610>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36b88cfb80>, <keras.callbacks.ModelCheckpoint object at 0x7f36b88cfc40>, <keras.callbacks.EarlyStopping object at 0x7f36b88cfeb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36b88cfee0>, <keras.callbacks.TerminateOnNaN object at 0x7f36b88cfb20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:03:46.911378
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:06:23.510 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 157s/epoch - 798ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 408.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_203"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_204 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f3da484bfa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36ca2bace0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36ca2bace0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3605853580>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d9bd1fd30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d9bd642e0>, <keras.callbacks.ModelCheckpoint object at 0x7f3d9bd643a0>, <keras.callbacks.EarlyStopping object at 0x7f3d9bd64610>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d9bd64640>, <keras.callbacks.TerminateOnNaN object at 0x7f3d9bd64280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:06:33.985589
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:09:11.300 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 157s/epoch - 802ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 408.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_214"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_215 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f3671ee6a40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3634b63e20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3634b63e20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3607e9a4d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3607c04640>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3607c04bb0>, <keras.callbacks.ModelCheckpoint object at 0x7f3607c04c70>, <keras.callbacks.EarlyStopping object at 0x7f3607c04ee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3607c04f10>, <keras.callbacks.TerminateOnNaN object at 0x7f3607c04b50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:09:20.066153
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:11:56.255 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 156s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 156s/epoch - 797ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 408.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_225"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_226 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f363647ab60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f363648b1c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f363648b1c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35e24ba4a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36b96bd2a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36b96bd810>, <keras.callbacks.ModelCheckpoint object at 0x7f36b96bd8d0>, <keras.callbacks.EarlyStopping object at 0x7f36b96bdb40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36b96bdb70>, <keras.callbacks.TerminateOnNaN object at 0x7f36b96bd7b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:12:07.415686
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:14:48.299 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 161s/epoch - 819ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 408.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_236"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_237 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f3d9b673ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36293cb490>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36293cb490>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f366108df90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36cb19c940>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36cb19ceb0>, <keras.callbacks.ModelCheckpoint object at 0x7f36cb19cf70>, <keras.callbacks.EarlyStopping object at 0x7f36cb19d1e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36cb19d210>, <keras.callbacks.TerminateOnNaN object at 0x7f36cb19ce50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:15:00.144448
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:17:41.218 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 161s/epoch - 820ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 408.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_247"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_248 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f36ea1938b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36ea1e1570>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36ea1e1570>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36608c87c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35bc3d28c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35bc3d2e30>, <keras.callbacks.ModelCheckpoint object at 0x7f35bc3d2ef0>, <keras.callbacks.EarlyStopping object at 0x7f35bc3d3160>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35bc3d3190>, <keras.callbacks.TerminateOnNaN object at 0x7f35bc3d2dd0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:17:49.986836
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:20:33.534 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 164s/epoch - 834ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 408.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_258"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_259 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f360711aa10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36ea0cdf00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36ea0cdf00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f37193cae00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36071dded0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36071de440>, <keras.callbacks.ModelCheckpoint object at 0x7f36071de500>, <keras.callbacks.EarlyStopping object at 0x7f36071de770>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36071de7a0>, <keras.callbacks.TerminateOnNaN object at 0x7f36071de3e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:20:43.604924
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:23:16.300 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 153s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 153s/epoch - 778ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 408.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_269"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_270 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f3d930365f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d931fc940>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d931fc940>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3660a54190>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3660a55d20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3660a57580>, <keras.callbacks.ModelCheckpoint object at 0x7f3660a55150>, <keras.callbacks.EarlyStopping object at 0x7f3660a56ef0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3660a551b0>, <keras.callbacks.TerminateOnNaN object at 0x7f3660a55b40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:23:31.566278
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:26:07.135 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 155s/epoch - 792ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 408.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_280"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_281 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f3d720b37c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f357d6de050>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f357d6de050>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f359c9b61a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d6944c3d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d6944c940>, <keras.callbacks.ModelCheckpoint object at 0x7f3d6944ca00>, <keras.callbacks.EarlyStopping object at 0x7f3d6944cc70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d6944cca0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d6944c8e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:26:19.012088
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:29:02.542 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 163s/epoch - 834ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 408.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_291"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_292 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f3661038f70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3594d030a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3594d030a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3594daa140>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3607052b00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3607053070>, <keras.callbacks.ModelCheckpoint object at 0x7f3607053130>, <keras.callbacks.EarlyStopping object at 0x7f36070533a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36070533d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3607053010>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:29:11.176817
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:31:58.696 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 168s/epoch - 855ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 408.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_408/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_408
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_302"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_303 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f35d61ba860>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36726e7a00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36726e7a00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3672322320>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35d61c8fa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_408/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35d61c9510>, <keras.callbacks.ModelCheckpoint object at 0x7f35d61c95d0>, <keras.callbacks.EarlyStopping object at 0x7f35d61c9840>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35d61c9870>, <keras.callbacks.TerminateOnNaN object at 0x7f35d61c94b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_408/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 408/720 with hyperparameters:
timestamp = 2023-10-27 12:32:10.379566
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:34:53.966 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11893.3955, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 11893.3955 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 163s/epoch - 834ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 408/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 409.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_308"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_309 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f36076b9c60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36cb10f820>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36cb10f820>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d722e0820>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3606ee6aa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3606ee46d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3606ee7610>, <keras.callbacks.EarlyStopping object at 0x7f3606ee4070>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3606ee7a00>, <keras.callbacks.TerminateOnNaN object at 0x7f3606ee5420>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:34:59.213401
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:35:58.897 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 60s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 60s/epoch - 304ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 409.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_314"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_315 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f3671493c70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3589b53010>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3589b53010>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f359c163be0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3671409f90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f367140a500>, <keras.callbacks.ModelCheckpoint object at 0x7f367140a5c0>, <keras.callbacks.EarlyStopping object at 0x7f367140a830>, <keras.callbacks.ReduceLROnPlateau object at 0x7f367140a860>, <keras.callbacks.TerminateOnNaN object at 0x7f367140a4a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:36:03.544299
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:37:02.421 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 59s/epoch - 300ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 409.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_320"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_321 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f3d6902b1c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35153dd240>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35153dd240>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d505c18a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d50461d50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d504622c0>, <keras.callbacks.ModelCheckpoint object at 0x7f3d50462380>, <keras.callbacks.EarlyStopping object at 0x7f3d504625f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d50462620>, <keras.callbacks.TerminateOnNaN object at 0x7f3d50462260>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:37:07.246802
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:38:06.618 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 59s/epoch - 303ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 409.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_326"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_327 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f3d47b80790>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34cdb17ee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34cdb17ee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f34c5822410>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d47465a20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d47465f90>, <keras.callbacks.ModelCheckpoint object at 0x7f3d47466050>, <keras.callbacks.EarlyStopping object at 0x7f3d474662c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d474662f0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d47465f30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:38:11.631686
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:39:22.834 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 71s/epoch - 362ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 409.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_332"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_333 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f37743f9ae0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36c86380a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36c86380a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f373a506590>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3719af1060>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3719af1b40>, <keras.callbacks.ModelCheckpoint object at 0x7f3719af0ee0>, <keras.callbacks.EarlyStopping object at 0x7f3719af1db0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3719af0850>, <keras.callbacks.TerminateOnNaN object at 0x7f3719af3730>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:39:27.600531
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:40:27.053 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 59s/epoch - 303ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 409.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_338"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_339 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f37197dbc10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3719401c00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3719401c00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36b908f0a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f37197aec50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f37197af1c0>, <keras.callbacks.ModelCheckpoint object at 0x7f37197af280>, <keras.callbacks.EarlyStopping object at 0x7f37197af4f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f37197af520>, <keras.callbacks.TerminateOnNaN object at 0x7f37197af160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:40:31.518479
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:41:31.764 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 60s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 60s/epoch - 307ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 409.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_344"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_345 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f34ccb23e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f357d887fd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f357d887fd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36bb3cffa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34cddf69b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f34cddf6f20>, <keras.callbacks.ModelCheckpoint object at 0x7f34cddf6fe0>, <keras.callbacks.EarlyStopping object at 0x7f34cddf7250>, <keras.callbacks.ReduceLROnPlateau object at 0x7f34cddf7280>, <keras.callbacks.TerminateOnNaN object at 0x7f34cddf6ec0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:41:35.615155
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:42:46.582 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 71s/epoch - 362ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 409.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_350"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_351 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f3ddf86d0c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f39340feb90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f39340feb90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f374aaa5390>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3854682380>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e248fa9b0>, <keras.callbacks.ModelCheckpoint object at 0x7f3e248f8c10>, <keras.callbacks.EarlyStopping object at 0x7f3e248fa1a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e248f80a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3e248f9210>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:42:52.049052
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:43:53.396 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 61s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 61s/epoch - 312ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 409.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_356"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_357 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f38d44f32e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3e02814430>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3e02814430>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f39d41e2bc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3e23824ca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3e23825840>, <keras.callbacks.ModelCheckpoint object at 0x7f3e23825b70>, <keras.callbacks.EarlyStopping object at 0x7f3e23825990>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3e23824a30>, <keras.callbacks.TerminateOnNaN object at 0x7f3e23827520>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:43:57.972497
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:44:57.921 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 60s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 60s/epoch - 305ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 409.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_362"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_363 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f3673bd5090>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36c8fc7a00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36c8fc7a00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3634cf6cb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f359c4b9db0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f359c4ba320>, <keras.callbacks.ModelCheckpoint object at 0x7f359c4ba3e0>, <keras.callbacks.EarlyStopping object at 0x7f359c4ba650>, <keras.callbacks.ReduceLROnPlateau object at 0x7f359c4ba680>, <keras.callbacks.TerminateOnNaN object at 0x7f359c4ba2c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:45:00.967372
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:46:06.187 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 65s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 65s/epoch - 332ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 409.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_409/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_409
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_368"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_369 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f35a4481f90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36ba0ad5a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36ba0ad5a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f373a66abf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f373a68a110>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_409/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f373a68a740>, <keras.callbacks.ModelCheckpoint object at 0x7f373a688a90>, <keras.callbacks.EarlyStopping object at 0x7f373a6895a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f373a689b10>, <keras.callbacks.TerminateOnNaN object at 0x7f373a688160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_409/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 409/720 with hyperparameters:
timestamp = 2023-10-27 12:46:10.050576
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:47:16.686 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10808.3701, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 10808.3701 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 66s/epoch - 339ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 409/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 410.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_410/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_410/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_410/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_410
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_374"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_375 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f39743a8040>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36042a7be0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36042a7be0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3e24b0fdf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36355fcee0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36355fc820>, <keras.callbacks.ModelCheckpoint object at 0x7f36355fd480>, <keras.callbacks.EarlyStopping object at 0x7f36355fd300>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36355fc2b0>, <keras.callbacks.TerminateOnNaN object at 0x7f36355fd390>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_410/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 410/720 with hyperparameters:
timestamp = 2023-10-27 12:47:21.234108
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 15: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:48:29.142 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6884.6699, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 6884.6699 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 68s/epoch - 346ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 410.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_410/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_410/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_410/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_410
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_380"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_381 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f359da8db40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3634e93bb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3634e93bb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36c9411ae0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3504245d50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35042462c0>, <keras.callbacks.ModelCheckpoint object at 0x7f3504246380>, <keras.callbacks.EarlyStopping object at 0x7f35042465f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3504246620>, <keras.callbacks.TerminateOnNaN object at 0x7f3504246260>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_410/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 410/720 with hyperparameters:
timestamp = 2023-10-27 12:48:34.171065
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
2023-10-27 12:50:03.244 
Epoch 1/1000 
	 loss: 1618.2189, MinusLogProbMetric: 1618.2189, val_loss: 419.1043, val_MinusLogProbMetric: 419.1043

Epoch 1: val_loss improved from inf to 419.10428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 89s - loss: 1618.2189 - MinusLogProbMetric: 1618.2189 - val_loss: 419.1043 - val_MinusLogProbMetric: 419.1043 - lr: 3.3333e-04 - 89s/epoch - 456ms/step
Epoch 2/1000
2023-10-27 12:50:37.627 
Epoch 2/1000 
	 loss: 353.2678, MinusLogProbMetric: 353.2678, val_loss: 280.4757, val_MinusLogProbMetric: 280.4757

Epoch 2: val_loss improved from 419.10428 to 280.47568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 353.2678 - MinusLogProbMetric: 353.2678 - val_loss: 280.4757 - val_MinusLogProbMetric: 280.4757 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 3/1000
2023-10-27 12:51:12.388 
Epoch 3/1000 
	 loss: 262.9400, MinusLogProbMetric: 262.9400, val_loss: 237.5012, val_MinusLogProbMetric: 237.5012

Epoch 3: val_loss improved from 280.47568 to 237.50121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 262.9400 - MinusLogProbMetric: 262.9400 - val_loss: 237.5012 - val_MinusLogProbMetric: 237.5012 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 4/1000
2023-10-27 12:51:47.398 
Epoch 4/1000 
	 loss: 210.8505, MinusLogProbMetric: 210.8505, val_loss: 193.6134, val_MinusLogProbMetric: 193.6134

Epoch 4: val_loss improved from 237.50121 to 193.61339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 210.8505 - MinusLogProbMetric: 210.8505 - val_loss: 193.6134 - val_MinusLogProbMetric: 193.6134 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 5/1000
2023-10-27 12:52:21.817 
Epoch 5/1000 
	 loss: 252.8890, MinusLogProbMetric: 252.8890, val_loss: 190.1744, val_MinusLogProbMetric: 190.1744

Epoch 5: val_loss improved from 193.61339 to 190.17436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 252.8890 - MinusLogProbMetric: 252.8890 - val_loss: 190.1744 - val_MinusLogProbMetric: 190.1744 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 6/1000
2023-10-27 12:52:57.178 
Epoch 6/1000 
	 loss: 175.9226, MinusLogProbMetric: 175.9226, val_loss: 165.6620, val_MinusLogProbMetric: 165.6620

Epoch 6: val_loss improved from 190.17436 to 165.66197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 175.9226 - MinusLogProbMetric: 175.9226 - val_loss: 165.6620 - val_MinusLogProbMetric: 165.6620 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 7/1000
2023-10-27 12:53:33.842 
Epoch 7/1000 
	 loss: 155.1657, MinusLogProbMetric: 155.1657, val_loss: 147.7072, val_MinusLogProbMetric: 147.7072

Epoch 7: val_loss improved from 165.66197 to 147.70715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 37s - loss: 155.1657 - MinusLogProbMetric: 155.1657 - val_loss: 147.7072 - val_MinusLogProbMetric: 147.7072 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 8/1000
2023-10-27 12:54:10.747 
Epoch 8/1000 
	 loss: 141.0187, MinusLogProbMetric: 141.0187, val_loss: 134.2077, val_MinusLogProbMetric: 134.2077

Epoch 8: val_loss improved from 147.70715 to 134.20775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 37s - loss: 141.0187 - MinusLogProbMetric: 141.0187 - val_loss: 134.2077 - val_MinusLogProbMetric: 134.2077 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 9/1000
2023-10-27 12:54:46.281 
Epoch 9/1000 
	 loss: 130.5665, MinusLogProbMetric: 130.5665, val_loss: 127.6740, val_MinusLogProbMetric: 127.6740

Epoch 9: val_loss improved from 134.20775 to 127.67398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 130.5665 - MinusLogProbMetric: 130.5665 - val_loss: 127.6740 - val_MinusLogProbMetric: 127.6740 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 10/1000
2023-10-27 12:55:21.712 
Epoch 10/1000 
	 loss: 120.6791, MinusLogProbMetric: 120.6791, val_loss: 117.8817, val_MinusLogProbMetric: 117.8817

Epoch 10: val_loss improved from 127.67398 to 117.88166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 120.6791 - MinusLogProbMetric: 120.6791 - val_loss: 117.8817 - val_MinusLogProbMetric: 117.8817 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 11/1000
2023-10-27 12:55:55.301 
Epoch 11/1000 
	 loss: 114.0159, MinusLogProbMetric: 114.0159, val_loss: 110.1465, val_MinusLogProbMetric: 110.1465

Epoch 11: val_loss improved from 117.88166 to 110.14648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 114.0159 - MinusLogProbMetric: 114.0159 - val_loss: 110.1465 - val_MinusLogProbMetric: 110.1465 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 12/1000
2023-10-27 12:56:26.019 
Epoch 12/1000 
	 loss: 107.4220, MinusLogProbMetric: 107.4220, val_loss: 105.3519, val_MinusLogProbMetric: 105.3519

Epoch 12: val_loss improved from 110.14648 to 105.35185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 31s - loss: 107.4220 - MinusLogProbMetric: 107.4220 - val_loss: 105.3519 - val_MinusLogProbMetric: 105.3519 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 13/1000
2023-10-27 12:56:58.497 
Epoch 13/1000 
	 loss: 102.3120, MinusLogProbMetric: 102.3120, val_loss: 100.0740, val_MinusLogProbMetric: 100.0740

Epoch 13: val_loss improved from 105.35185 to 100.07399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 32s - loss: 102.3120 - MinusLogProbMetric: 102.3120 - val_loss: 100.0740 - val_MinusLogProbMetric: 100.0740 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 14/1000
2023-10-27 12:57:28.259 
Epoch 14/1000 
	 loss: 97.7363, MinusLogProbMetric: 97.7363, val_loss: 95.5607, val_MinusLogProbMetric: 95.5607

Epoch 14: val_loss improved from 100.07399 to 95.56068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 30s - loss: 97.7363 - MinusLogProbMetric: 97.7363 - val_loss: 95.5607 - val_MinusLogProbMetric: 95.5607 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 15/1000
2023-10-27 12:57:56.550 
Epoch 15/1000 
	 loss: 94.3087, MinusLogProbMetric: 94.3087, val_loss: 92.9305, val_MinusLogProbMetric: 92.9305

Epoch 15: val_loss improved from 95.56068 to 92.93051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 94.3087 - MinusLogProbMetric: 94.3087 - val_loss: 92.9305 - val_MinusLogProbMetric: 92.9305 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 16/1000
2023-10-27 12:58:24.000 
Epoch 16/1000 
	 loss: 90.1007, MinusLogProbMetric: 90.1007, val_loss: 89.2843, val_MinusLogProbMetric: 89.2843

Epoch 16: val_loss improved from 92.93051 to 89.28430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 90.1007 - MinusLogProbMetric: 90.1007 - val_loss: 89.2843 - val_MinusLogProbMetric: 89.2843 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 17/1000
2023-10-27 12:58:51.358 
Epoch 17/1000 
	 loss: 101.2792, MinusLogProbMetric: 101.2792, val_loss: 93.4523, val_MinusLogProbMetric: 93.4523

Epoch 17: val_loss did not improve from 89.28430
196/196 - 27s - loss: 101.2792 - MinusLogProbMetric: 101.2792 - val_loss: 93.4523 - val_MinusLogProbMetric: 93.4523 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 18/1000
2023-10-27 12:59:22.328 
Epoch 18/1000 
	 loss: 94.7775, MinusLogProbMetric: 94.7775, val_loss: 90.3985, val_MinusLogProbMetric: 90.3985

Epoch 18: val_loss did not improve from 89.28430
196/196 - 31s - loss: 94.7775 - MinusLogProbMetric: 94.7775 - val_loss: 90.3985 - val_MinusLogProbMetric: 90.3985 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 19/1000
2023-10-27 12:59:55.203 
Epoch 19/1000 
	 loss: 87.3608, MinusLogProbMetric: 87.3608, val_loss: 84.7268, val_MinusLogProbMetric: 84.7268

Epoch 19: val_loss improved from 89.28430 to 84.72683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 33s - loss: 87.3608 - MinusLogProbMetric: 87.3608 - val_loss: 84.7268 - val_MinusLogProbMetric: 84.7268 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 20/1000
2023-10-27 13:00:22.681 
Epoch 20/1000 
	 loss: 83.9342, MinusLogProbMetric: 83.9342, val_loss: 84.0433, val_MinusLogProbMetric: 84.0433

Epoch 20: val_loss improved from 84.72683 to 84.04332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 83.9342 - MinusLogProbMetric: 83.9342 - val_loss: 84.0433 - val_MinusLogProbMetric: 84.0433 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 21/1000
2023-10-27 13:00:50.490 
Epoch 21/1000 
	 loss: 81.2670, MinusLogProbMetric: 81.2670, val_loss: 80.0830, val_MinusLogProbMetric: 80.0830

Epoch 21: val_loss improved from 84.04332 to 80.08298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 81.2670 - MinusLogProbMetric: 81.2670 - val_loss: 80.0830 - val_MinusLogProbMetric: 80.0830 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 22/1000
2023-10-27 13:01:19.415 
Epoch 22/1000 
	 loss: 79.1053, MinusLogProbMetric: 79.1053, val_loss: 78.7266, val_MinusLogProbMetric: 78.7266

Epoch 22: val_loss improved from 80.08298 to 78.72663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 29s - loss: 79.1053 - MinusLogProbMetric: 79.1053 - val_loss: 78.7266 - val_MinusLogProbMetric: 78.7266 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 23/1000
2023-10-27 13:01:51.519 
Epoch 23/1000 
	 loss: 77.2250, MinusLogProbMetric: 77.2250, val_loss: 77.0635, val_MinusLogProbMetric: 77.0635

Epoch 23: val_loss improved from 78.72663 to 77.06348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 32s - loss: 77.2250 - MinusLogProbMetric: 77.2250 - val_loss: 77.0635 - val_MinusLogProbMetric: 77.0635 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 24/1000
2023-10-27 13:02:20.432 
Epoch 24/1000 
	 loss: 75.5691, MinusLogProbMetric: 75.5691, val_loss: 75.7735, val_MinusLogProbMetric: 75.7735

Epoch 24: val_loss improved from 77.06348 to 75.77350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 29s - loss: 75.5691 - MinusLogProbMetric: 75.5691 - val_loss: 75.7735 - val_MinusLogProbMetric: 75.7735 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 25/1000
2023-10-27 13:02:47.623 
Epoch 25/1000 
	 loss: 74.0962, MinusLogProbMetric: 74.0962, val_loss: 72.9733, val_MinusLogProbMetric: 72.9733

Epoch 25: val_loss improved from 75.77350 to 72.97327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 74.0962 - MinusLogProbMetric: 74.0962 - val_loss: 72.9733 - val_MinusLogProbMetric: 72.9733 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 26/1000
2023-10-27 13:03:14.905 
Epoch 26/1000 
	 loss: 72.9592, MinusLogProbMetric: 72.9592, val_loss: 74.0976, val_MinusLogProbMetric: 74.0976

Epoch 26: val_loss did not improve from 72.97327
196/196 - 27s - loss: 72.9592 - MinusLogProbMetric: 72.9592 - val_loss: 74.0976 - val_MinusLogProbMetric: 74.0976 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 27/1000
2023-10-27 13:03:41.471 
Epoch 27/1000 
	 loss: 71.4455, MinusLogProbMetric: 71.4455, val_loss: 72.2821, val_MinusLogProbMetric: 72.2821

Epoch 27: val_loss improved from 72.97327 to 72.28211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 71.4455 - MinusLogProbMetric: 71.4455 - val_loss: 72.2821 - val_MinusLogProbMetric: 72.2821 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 28/1000
2023-10-27 13:04:09.144 
Epoch 28/1000 
	 loss: 70.4856, MinusLogProbMetric: 70.4856, val_loss: 69.9509, val_MinusLogProbMetric: 69.9509

Epoch 28: val_loss improved from 72.28211 to 69.95093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 70.4856 - MinusLogProbMetric: 70.4856 - val_loss: 69.9509 - val_MinusLogProbMetric: 69.9509 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 29/1000
2023-10-27 13:04:37.847 
Epoch 29/1000 
	 loss: 69.7172, MinusLogProbMetric: 69.7172, val_loss: 69.3077, val_MinusLogProbMetric: 69.3077

Epoch 29: val_loss improved from 69.95093 to 69.30774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 29s - loss: 69.7172 - MinusLogProbMetric: 69.7172 - val_loss: 69.3077 - val_MinusLogProbMetric: 69.3077 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 30/1000
2023-10-27 13:05:05.315 
Epoch 30/1000 
	 loss: 68.4588, MinusLogProbMetric: 68.4588, val_loss: 69.1728, val_MinusLogProbMetric: 69.1728

Epoch 30: val_loss improved from 69.30774 to 69.17278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 68.4588 - MinusLogProbMetric: 68.4588 - val_loss: 69.1728 - val_MinusLogProbMetric: 69.1728 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 31/1000
2023-10-27 13:05:32.818 
Epoch 31/1000 
	 loss: 67.5601, MinusLogProbMetric: 67.5601, val_loss: 67.3013, val_MinusLogProbMetric: 67.3013

Epoch 31: val_loss improved from 69.17278 to 67.30128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 67.5601 - MinusLogProbMetric: 67.5601 - val_loss: 67.3013 - val_MinusLogProbMetric: 67.3013 - lr: 3.3333e-04 - 28s/epoch - 140ms/step
Epoch 32/1000
2023-10-27 13:05:59.904 
Epoch 32/1000 
	 loss: 72.4136, MinusLogProbMetric: 72.4136, val_loss: 67.4449, val_MinusLogProbMetric: 67.4449

Epoch 32: val_loss did not improve from 67.30128
196/196 - 27s - loss: 72.4136 - MinusLogProbMetric: 72.4136 - val_loss: 67.4449 - val_MinusLogProbMetric: 67.4449 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 33/1000
2023-10-27 13:06:27.514 
Epoch 33/1000 
	 loss: 67.0209, MinusLogProbMetric: 67.0209, val_loss: 65.8742, val_MinusLogProbMetric: 65.8742

Epoch 33: val_loss improved from 67.30128 to 65.87418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 67.0209 - MinusLogProbMetric: 67.0209 - val_loss: 65.8742 - val_MinusLogProbMetric: 65.8742 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 34/1000
2023-10-27 13:06:54.565 
Epoch 34/1000 
	 loss: 65.9364, MinusLogProbMetric: 65.9364, val_loss: 64.5815, val_MinusLogProbMetric: 64.5815

Epoch 34: val_loss improved from 65.87418 to 64.58152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 65.9364 - MinusLogProbMetric: 65.9364 - val_loss: 64.5815 - val_MinusLogProbMetric: 64.5815 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 35/1000
2023-10-27 13:07:21.875 
Epoch 35/1000 
	 loss: 65.1071, MinusLogProbMetric: 65.1071, val_loss: 68.0486, val_MinusLogProbMetric: 68.0486

Epoch 35: val_loss did not improve from 64.58152
196/196 - 27s - loss: 65.1071 - MinusLogProbMetric: 65.1071 - val_loss: 68.0486 - val_MinusLogProbMetric: 68.0486 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 36/1000
2023-10-27 13:07:49.199 
Epoch 36/1000 
	 loss: 64.3933, MinusLogProbMetric: 64.3933, val_loss: 65.2378, val_MinusLogProbMetric: 65.2378

Epoch 36: val_loss did not improve from 64.58152
196/196 - 27s - loss: 64.3933 - MinusLogProbMetric: 64.3933 - val_loss: 65.2378 - val_MinusLogProbMetric: 65.2378 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 37/1000
2023-10-27 13:08:15.937 
Epoch 37/1000 
	 loss: 63.3809, MinusLogProbMetric: 63.3809, val_loss: 64.7971, val_MinusLogProbMetric: 64.7971

Epoch 37: val_loss did not improve from 64.58152
196/196 - 27s - loss: 63.3809 - MinusLogProbMetric: 63.3809 - val_loss: 64.7971 - val_MinusLogProbMetric: 64.7971 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 38/1000
2023-10-27 13:08:43.575 
Epoch 38/1000 
	 loss: 65.8443, MinusLogProbMetric: 65.8443, val_loss: 63.8021, val_MinusLogProbMetric: 63.8021

Epoch 38: val_loss improved from 64.58152 to 63.80213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 65.8443 - MinusLogProbMetric: 65.8443 - val_loss: 63.8021 - val_MinusLogProbMetric: 63.8021 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 39/1000
2023-10-27 13:09:11.151 
Epoch 39/1000 
	 loss: 62.8954, MinusLogProbMetric: 62.8954, val_loss: 63.6529, val_MinusLogProbMetric: 63.6529

Epoch 39: val_loss improved from 63.80213 to 63.65294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 62.8954 - MinusLogProbMetric: 62.8954 - val_loss: 63.6529 - val_MinusLogProbMetric: 63.6529 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 40/1000
2023-10-27 13:09:38.273 
Epoch 40/1000 
	 loss: 62.4169, MinusLogProbMetric: 62.4169, val_loss: 63.9863, val_MinusLogProbMetric: 63.9863

Epoch 40: val_loss did not improve from 63.65294
196/196 - 27s - loss: 62.4169 - MinusLogProbMetric: 62.4169 - val_loss: 63.9863 - val_MinusLogProbMetric: 63.9863 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 41/1000
2023-10-27 13:10:05.571 
Epoch 41/1000 
	 loss: 62.1530, MinusLogProbMetric: 62.1530, val_loss: 62.3577, val_MinusLogProbMetric: 62.3577

Epoch 41: val_loss improved from 63.65294 to 62.35767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 62.1530 - MinusLogProbMetric: 62.1530 - val_loss: 62.3577 - val_MinusLogProbMetric: 62.3577 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 42/1000
2023-10-27 13:10:33.857 
Epoch 42/1000 
	 loss: 61.4652, MinusLogProbMetric: 61.4652, val_loss: 61.7558, val_MinusLogProbMetric: 61.7558

Epoch 42: val_loss improved from 62.35767 to 61.75577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 61.4652 - MinusLogProbMetric: 61.4652 - val_loss: 61.7558 - val_MinusLogProbMetric: 61.7558 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 43/1000
2023-10-27 13:11:01.508 
Epoch 43/1000 
	 loss: 60.8467, MinusLogProbMetric: 60.8467, val_loss: 60.9054, val_MinusLogProbMetric: 60.9054

Epoch 43: val_loss improved from 61.75577 to 60.90545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 60.8467 - MinusLogProbMetric: 60.8467 - val_loss: 60.9054 - val_MinusLogProbMetric: 60.9054 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 44/1000
2023-10-27 13:11:28.916 
Epoch 44/1000 
	 loss: 60.1023, MinusLogProbMetric: 60.1023, val_loss: 60.4446, val_MinusLogProbMetric: 60.4446

Epoch 44: val_loss improved from 60.90545 to 60.44462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 60.1023 - MinusLogProbMetric: 60.1023 - val_loss: 60.4446 - val_MinusLogProbMetric: 60.4446 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 45/1000
2023-10-27 13:11:56.776 
Epoch 45/1000 
	 loss: 60.6666, MinusLogProbMetric: 60.6666, val_loss: 59.8748, val_MinusLogProbMetric: 59.8748

Epoch 45: val_loss improved from 60.44462 to 59.87476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 60.6666 - MinusLogProbMetric: 60.6666 - val_loss: 59.8748 - val_MinusLogProbMetric: 59.8748 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 46/1000
2023-10-27 13:12:24.356 
Epoch 46/1000 
	 loss: 59.5263, MinusLogProbMetric: 59.5263, val_loss: 59.0180, val_MinusLogProbMetric: 59.0180

Epoch 46: val_loss improved from 59.87476 to 59.01797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 59.5263 - MinusLogProbMetric: 59.5263 - val_loss: 59.0180 - val_MinusLogProbMetric: 59.0180 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 47/1000
2023-10-27 13:12:51.757 
Epoch 47/1000 
	 loss: 60.0607, MinusLogProbMetric: 60.0607, val_loss: 58.8085, val_MinusLogProbMetric: 58.8085

Epoch 47: val_loss improved from 59.01797 to 58.80853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 60.0607 - MinusLogProbMetric: 60.0607 - val_loss: 58.8085 - val_MinusLogProbMetric: 58.8085 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 48/1000
2023-10-27 13:13:19.554 
Epoch 48/1000 
	 loss: 58.9436, MinusLogProbMetric: 58.9436, val_loss: 58.4646, val_MinusLogProbMetric: 58.4646

Epoch 48: val_loss improved from 58.80853 to 58.46457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 58.9436 - MinusLogProbMetric: 58.9436 - val_loss: 58.4646 - val_MinusLogProbMetric: 58.4646 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 49/1000
2023-10-27 13:13:46.970 
Epoch 49/1000 
	 loss: 57.9085, MinusLogProbMetric: 57.9085, val_loss: 58.3776, val_MinusLogProbMetric: 58.3776

Epoch 49: val_loss improved from 58.46457 to 58.37759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 57.9085 - MinusLogProbMetric: 57.9085 - val_loss: 58.3776 - val_MinusLogProbMetric: 58.3776 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 50/1000
2023-10-27 13:14:14.163 
Epoch 50/1000 
	 loss: 58.0573, MinusLogProbMetric: 58.0573, val_loss: 58.6294, val_MinusLogProbMetric: 58.6294

Epoch 50: val_loss did not improve from 58.37759
196/196 - 27s - loss: 58.0573 - MinusLogProbMetric: 58.0573 - val_loss: 58.6294 - val_MinusLogProbMetric: 58.6294 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 51/1000
2023-10-27 13:14:41.291 
Epoch 51/1000 
	 loss: 57.6973, MinusLogProbMetric: 57.6973, val_loss: 57.9245, val_MinusLogProbMetric: 57.9245

Epoch 51: val_loss improved from 58.37759 to 57.92448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 57.6973 - MinusLogProbMetric: 57.6973 - val_loss: 57.9245 - val_MinusLogProbMetric: 57.9245 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 52/1000
2023-10-27 13:15:08.960 
Epoch 52/1000 
	 loss: 57.1380, MinusLogProbMetric: 57.1380, val_loss: 58.0620, val_MinusLogProbMetric: 58.0620

Epoch 52: val_loss did not improve from 57.92448
196/196 - 27s - loss: 57.1380 - MinusLogProbMetric: 57.1380 - val_loss: 58.0620 - val_MinusLogProbMetric: 58.0620 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 53/1000
2023-10-27 13:15:36.433 
Epoch 53/1000 
	 loss: 58.0903, MinusLogProbMetric: 58.0903, val_loss: 57.5338, val_MinusLogProbMetric: 57.5338

Epoch 53: val_loss improved from 57.92448 to 57.53382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 58.0903 - MinusLogProbMetric: 58.0903 - val_loss: 57.5338 - val_MinusLogProbMetric: 57.5338 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 54/1000
2023-10-27 13:16:03.853 
Epoch 54/1000 
	 loss: 56.6143, MinusLogProbMetric: 56.6143, val_loss: 56.7938, val_MinusLogProbMetric: 56.7938

Epoch 54: val_loss improved from 57.53382 to 56.79376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 56.6143 - MinusLogProbMetric: 56.6143 - val_loss: 56.7938 - val_MinusLogProbMetric: 56.7938 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 55/1000
2023-10-27 13:16:31.342 
Epoch 55/1000 
	 loss: 64.3726, MinusLogProbMetric: 64.3726, val_loss: 63.0579, val_MinusLogProbMetric: 63.0579

Epoch 55: val_loss did not improve from 56.79376
196/196 - 27s - loss: 64.3726 - MinusLogProbMetric: 64.3726 - val_loss: 63.0579 - val_MinusLogProbMetric: 63.0579 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 56/1000
2023-10-27 13:16:58.179 
Epoch 56/1000 
	 loss: 57.7302, MinusLogProbMetric: 57.7302, val_loss: 57.2646, val_MinusLogProbMetric: 57.2646

Epoch 56: val_loss did not improve from 56.79376
196/196 - 27s - loss: 57.7302 - MinusLogProbMetric: 57.7302 - val_loss: 57.2646 - val_MinusLogProbMetric: 57.2646 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 57/1000
2023-10-27 13:17:24.875 
Epoch 57/1000 
	 loss: 55.9408, MinusLogProbMetric: 55.9408, val_loss: 56.0993, val_MinusLogProbMetric: 56.0993

Epoch 57: val_loss improved from 56.79376 to 56.09929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 55.9408 - MinusLogProbMetric: 55.9408 - val_loss: 56.0993 - val_MinusLogProbMetric: 56.0993 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 58/1000
2023-10-27 13:17:52.519 
Epoch 58/1000 
	 loss: 55.5231, MinusLogProbMetric: 55.5231, val_loss: 55.5697, val_MinusLogProbMetric: 55.5697

Epoch 58: val_loss improved from 56.09929 to 55.56971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 55.5231 - MinusLogProbMetric: 55.5231 - val_loss: 55.5697 - val_MinusLogProbMetric: 55.5697 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 59/1000
2023-10-27 13:18:20.384 
Epoch 59/1000 
	 loss: 55.6642, MinusLogProbMetric: 55.6642, val_loss: 55.1403, val_MinusLogProbMetric: 55.1403

Epoch 59: val_loss improved from 55.56971 to 55.14032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 55.6642 - MinusLogProbMetric: 55.6642 - val_loss: 55.1403 - val_MinusLogProbMetric: 55.1403 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 60/1000
2023-10-27 13:18:48.322 
Epoch 60/1000 
	 loss: 55.3134, MinusLogProbMetric: 55.3134, val_loss: 56.9070, val_MinusLogProbMetric: 56.9070

Epoch 60: val_loss did not improve from 55.14032
196/196 - 27s - loss: 55.3134 - MinusLogProbMetric: 55.3134 - val_loss: 56.9070 - val_MinusLogProbMetric: 56.9070 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 61/1000
2023-10-27 13:19:15.889 
Epoch 61/1000 
	 loss: 55.4079, MinusLogProbMetric: 55.4079, val_loss: 55.3958, val_MinusLogProbMetric: 55.3958

Epoch 61: val_loss did not improve from 55.14032
196/196 - 28s - loss: 55.4079 - MinusLogProbMetric: 55.4079 - val_loss: 55.3958 - val_MinusLogProbMetric: 55.3958 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 62/1000
2023-10-27 13:19:45.486 
Epoch 62/1000 
	 loss: 55.0133, MinusLogProbMetric: 55.0133, val_loss: 56.7811, val_MinusLogProbMetric: 56.7811

Epoch 62: val_loss did not improve from 55.14032
196/196 - 30s - loss: 55.0133 - MinusLogProbMetric: 55.0133 - val_loss: 56.7811 - val_MinusLogProbMetric: 56.7811 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 63/1000
2023-10-27 13:20:12.400 
Epoch 63/1000 
	 loss: 54.4571, MinusLogProbMetric: 54.4571, val_loss: 54.9899, val_MinusLogProbMetric: 54.9899

Epoch 63: val_loss improved from 55.14032 to 54.98989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 54.4571 - MinusLogProbMetric: 54.4571 - val_loss: 54.9899 - val_MinusLogProbMetric: 54.9899 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 64/1000
2023-10-27 13:20:40.218 
Epoch 64/1000 
	 loss: 54.9976, MinusLogProbMetric: 54.9976, val_loss: 54.4550, val_MinusLogProbMetric: 54.4550

Epoch 64: val_loss improved from 54.98989 to 54.45502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 54.9976 - MinusLogProbMetric: 54.9976 - val_loss: 54.4550 - val_MinusLogProbMetric: 54.4550 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 65/1000
2023-10-27 13:21:07.775 
Epoch 65/1000 
	 loss: 54.1140, MinusLogProbMetric: 54.1140, val_loss: 55.1040, val_MinusLogProbMetric: 55.1040

Epoch 65: val_loss did not improve from 54.45502
196/196 - 27s - loss: 54.1140 - MinusLogProbMetric: 54.1140 - val_loss: 55.1040 - val_MinusLogProbMetric: 55.1040 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 66/1000
2023-10-27 13:21:34.137 
Epoch 66/1000 
	 loss: 54.1923, MinusLogProbMetric: 54.1923, val_loss: 54.0312, val_MinusLogProbMetric: 54.0312

Epoch 66: val_loss improved from 54.45502 to 54.03116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 54.1923 - MinusLogProbMetric: 54.1923 - val_loss: 54.0312 - val_MinusLogProbMetric: 54.0312 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 67/1000
2023-10-27 13:22:01.666 
Epoch 67/1000 
	 loss: 53.9064, MinusLogProbMetric: 53.9064, val_loss: 54.5551, val_MinusLogProbMetric: 54.5551

Epoch 67: val_loss did not improve from 54.03116
196/196 - 27s - loss: 53.9064 - MinusLogProbMetric: 53.9064 - val_loss: 54.5551 - val_MinusLogProbMetric: 54.5551 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 68/1000
2023-10-27 13:22:29.345 
Epoch 68/1000 
	 loss: 53.9331, MinusLogProbMetric: 53.9331, val_loss: 56.1957, val_MinusLogProbMetric: 56.1957

Epoch 68: val_loss did not improve from 54.03116
196/196 - 28s - loss: 53.9331 - MinusLogProbMetric: 53.9331 - val_loss: 56.1957 - val_MinusLogProbMetric: 56.1957 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 69/1000
2023-10-27 13:22:56.354 
Epoch 69/1000 
	 loss: 53.7382, MinusLogProbMetric: 53.7382, val_loss: 53.3647, val_MinusLogProbMetric: 53.3647

Epoch 69: val_loss improved from 54.03116 to 53.36469, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 53.7382 - MinusLogProbMetric: 53.7382 - val_loss: 53.3647 - val_MinusLogProbMetric: 53.3647 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 70/1000
2023-10-27 13:23:23.942 
Epoch 70/1000 
	 loss: 53.5433, MinusLogProbMetric: 53.5433, val_loss: 54.0443, val_MinusLogProbMetric: 54.0443

Epoch 70: val_loss did not improve from 53.36469
196/196 - 27s - loss: 53.5433 - MinusLogProbMetric: 53.5433 - val_loss: 54.0443 - val_MinusLogProbMetric: 54.0443 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 71/1000
2023-10-27 13:23:51.371 
Epoch 71/1000 
	 loss: 53.3064, MinusLogProbMetric: 53.3064, val_loss: 54.0917, val_MinusLogProbMetric: 54.0917

Epoch 71: val_loss did not improve from 53.36469
196/196 - 27s - loss: 53.3064 - MinusLogProbMetric: 53.3064 - val_loss: 54.0917 - val_MinusLogProbMetric: 54.0917 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 72/1000
2023-10-27 13:24:18.596 
Epoch 72/1000 
	 loss: 53.0979, MinusLogProbMetric: 53.0979, val_loss: 54.1254, val_MinusLogProbMetric: 54.1254

Epoch 72: val_loss did not improve from 53.36469
196/196 - 27s - loss: 53.0979 - MinusLogProbMetric: 53.0979 - val_loss: 54.1254 - val_MinusLogProbMetric: 54.1254 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 73/1000
2023-10-27 13:24:45.655 
Epoch 73/1000 
	 loss: 53.2537, MinusLogProbMetric: 53.2537, val_loss: 54.1151, val_MinusLogProbMetric: 54.1151

Epoch 73: val_loss did not improve from 53.36469
196/196 - 27s - loss: 53.2537 - MinusLogProbMetric: 53.2537 - val_loss: 54.1151 - val_MinusLogProbMetric: 54.1151 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 74/1000
2023-10-27 13:25:12.090 
Epoch 74/1000 
	 loss: 53.2907, MinusLogProbMetric: 53.2907, val_loss: 52.4024, val_MinusLogProbMetric: 52.4024

Epoch 74: val_loss improved from 53.36469 to 52.40239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 53.2907 - MinusLogProbMetric: 53.2907 - val_loss: 52.4024 - val_MinusLogProbMetric: 52.4024 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 75/1000
2023-10-27 13:25:39.307 
Epoch 75/1000 
	 loss: 52.5532, MinusLogProbMetric: 52.5532, val_loss: 53.1903, val_MinusLogProbMetric: 53.1903

Epoch 75: val_loss did not improve from 52.40239
196/196 - 27s - loss: 52.5532 - MinusLogProbMetric: 52.5532 - val_loss: 53.1903 - val_MinusLogProbMetric: 53.1903 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 76/1000
2023-10-27 13:26:09.006 
Epoch 76/1000 
	 loss: 52.6544, MinusLogProbMetric: 52.6544, val_loss: 52.9622, val_MinusLogProbMetric: 52.9622

Epoch 76: val_loss did not improve from 52.40239
196/196 - 30s - loss: 52.6544 - MinusLogProbMetric: 52.6544 - val_loss: 52.9622 - val_MinusLogProbMetric: 52.9622 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 77/1000
2023-10-27 13:26:37.443 
Epoch 77/1000 
	 loss: 53.0887, MinusLogProbMetric: 53.0887, val_loss: 52.0992, val_MinusLogProbMetric: 52.0992

Epoch 77: val_loss improved from 52.40239 to 52.09923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 29s - loss: 53.0887 - MinusLogProbMetric: 53.0887 - val_loss: 52.0992 - val_MinusLogProbMetric: 52.0992 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 78/1000
2023-10-27 13:27:05.034 
Epoch 78/1000 
	 loss: 52.3242, MinusLogProbMetric: 52.3242, val_loss: 52.5326, val_MinusLogProbMetric: 52.5326

Epoch 78: val_loss did not improve from 52.09923
196/196 - 27s - loss: 52.3242 - MinusLogProbMetric: 52.3242 - val_loss: 52.5326 - val_MinusLogProbMetric: 52.5326 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 79/1000
2023-10-27 13:27:32.026 
Epoch 79/1000 
	 loss: 52.1235, MinusLogProbMetric: 52.1235, val_loss: 52.5566, val_MinusLogProbMetric: 52.5566

Epoch 79: val_loss did not improve from 52.09923
196/196 - 27s - loss: 52.1235 - MinusLogProbMetric: 52.1235 - val_loss: 52.5566 - val_MinusLogProbMetric: 52.5566 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 80/1000
2023-10-27 13:27:59.176 
Epoch 80/1000 
	 loss: 52.1110, MinusLogProbMetric: 52.1110, val_loss: 52.8812, val_MinusLogProbMetric: 52.8812

Epoch 80: val_loss did not improve from 52.09923
196/196 - 27s - loss: 52.1110 - MinusLogProbMetric: 52.1110 - val_loss: 52.8812 - val_MinusLogProbMetric: 52.8812 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 81/1000
2023-10-27 13:28:26.401 
Epoch 81/1000 
	 loss: 52.0293, MinusLogProbMetric: 52.0293, val_loss: 54.0051, val_MinusLogProbMetric: 54.0051

Epoch 81: val_loss did not improve from 52.09923
196/196 - 27s - loss: 52.0293 - MinusLogProbMetric: 52.0293 - val_loss: 54.0051 - val_MinusLogProbMetric: 54.0051 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 82/1000
2023-10-27 13:28:53.552 
Epoch 82/1000 
	 loss: 51.8515, MinusLogProbMetric: 51.8515, val_loss: 53.4619, val_MinusLogProbMetric: 53.4619

Epoch 82: val_loss did not improve from 52.09923
196/196 - 27s - loss: 51.8515 - MinusLogProbMetric: 51.8515 - val_loss: 53.4619 - val_MinusLogProbMetric: 53.4619 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 83/1000
2023-10-27 13:29:22.763 
Epoch 83/1000 
	 loss: 51.6338, MinusLogProbMetric: 51.6338, val_loss: 51.9867, val_MinusLogProbMetric: 51.9867

Epoch 83: val_loss improved from 52.09923 to 51.98668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 30s - loss: 51.6338 - MinusLogProbMetric: 51.6338 - val_loss: 51.9867 - val_MinusLogProbMetric: 51.9867 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 84/1000
2023-10-27 13:29:53.761 
Epoch 84/1000 
	 loss: 51.8573, MinusLogProbMetric: 51.8573, val_loss: 52.3123, val_MinusLogProbMetric: 52.3123

Epoch 84: val_loss did not improve from 51.98668
196/196 - 31s - loss: 51.8573 - MinusLogProbMetric: 51.8573 - val_loss: 52.3123 - val_MinusLogProbMetric: 52.3123 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 85/1000
2023-10-27 13:30:20.852 
Epoch 85/1000 
	 loss: 52.4480, MinusLogProbMetric: 52.4480, val_loss: 51.8580, val_MinusLogProbMetric: 51.8580

Epoch 85: val_loss improved from 51.98668 to 51.85801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 52.4480 - MinusLogProbMetric: 52.4480 - val_loss: 51.8580 - val_MinusLogProbMetric: 51.8580 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 86/1000
2023-10-27 13:30:48.795 
Epoch 86/1000 
	 loss: 51.2676, MinusLogProbMetric: 51.2676, val_loss: 51.5795, val_MinusLogProbMetric: 51.5795

Epoch 86: val_loss improved from 51.85801 to 51.57954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 51.2676 - MinusLogProbMetric: 51.2676 - val_loss: 51.5795 - val_MinusLogProbMetric: 51.5795 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 87/1000
2023-10-27 13:31:16.382 
Epoch 87/1000 
	 loss: 51.2569, MinusLogProbMetric: 51.2569, val_loss: 51.8867, val_MinusLogProbMetric: 51.8867

Epoch 87: val_loss did not improve from 51.57954
196/196 - 27s - loss: 51.2569 - MinusLogProbMetric: 51.2569 - val_loss: 51.8867 - val_MinusLogProbMetric: 51.8867 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 88/1000
2023-10-27 13:31:45.991 
Epoch 88/1000 
	 loss: 51.1513, MinusLogProbMetric: 51.1513, val_loss: 51.8085, val_MinusLogProbMetric: 51.8085

Epoch 88: val_loss did not improve from 51.57954
196/196 - 30s - loss: 51.1513 - MinusLogProbMetric: 51.1513 - val_loss: 51.8085 - val_MinusLogProbMetric: 51.8085 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 89/1000
2023-10-27 13:32:19.533 
Epoch 89/1000 
	 loss: 51.1824, MinusLogProbMetric: 51.1824, val_loss: 53.3644, val_MinusLogProbMetric: 53.3644

Epoch 89: val_loss did not improve from 51.57954
196/196 - 34s - loss: 51.1824 - MinusLogProbMetric: 51.1824 - val_loss: 53.3644 - val_MinusLogProbMetric: 53.3644 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 90/1000
2023-10-27 13:32:49.339 
Epoch 90/1000 
	 loss: 51.3236, MinusLogProbMetric: 51.3236, val_loss: 52.3495, val_MinusLogProbMetric: 52.3495

Epoch 90: val_loss did not improve from 51.57954
196/196 - 30s - loss: 51.3236 - MinusLogProbMetric: 51.3236 - val_loss: 52.3495 - val_MinusLogProbMetric: 52.3495 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 91/1000
2023-10-27 13:33:15.578 
Epoch 91/1000 
	 loss: 50.9168, MinusLogProbMetric: 50.9168, val_loss: 52.3861, val_MinusLogProbMetric: 52.3861

Epoch 91: val_loss did not improve from 51.57954
196/196 - 26s - loss: 50.9168 - MinusLogProbMetric: 50.9168 - val_loss: 52.3861 - val_MinusLogProbMetric: 52.3861 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 92/1000
2023-10-27 13:33:42.268 
Epoch 92/1000 
	 loss: 52.2556, MinusLogProbMetric: 52.2556, val_loss: 51.3495, val_MinusLogProbMetric: 51.3495

Epoch 92: val_loss improved from 51.57954 to 51.34953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 52.2556 - MinusLogProbMetric: 52.2556 - val_loss: 51.3495 - val_MinusLogProbMetric: 51.3495 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 93/1000
2023-10-27 13:34:09.688 
Epoch 93/1000 
	 loss: 50.8040, MinusLogProbMetric: 50.8040, val_loss: 50.8553, val_MinusLogProbMetric: 50.8553

Epoch 93: val_loss improved from 51.34953 to 50.85534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 50.8040 - MinusLogProbMetric: 50.8040 - val_loss: 50.8553 - val_MinusLogProbMetric: 50.8553 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 94/1000
2023-10-27 13:34:39.483 
Epoch 94/1000 
	 loss: 51.0167, MinusLogProbMetric: 51.0167, val_loss: 51.6169, val_MinusLogProbMetric: 51.6169

Epoch 94: val_loss did not improve from 50.85534
196/196 - 29s - loss: 51.0167 - MinusLogProbMetric: 51.0167 - val_loss: 51.6169 - val_MinusLogProbMetric: 51.6169 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 95/1000
2023-10-27 13:35:10.468 
Epoch 95/1000 
	 loss: 50.7924, MinusLogProbMetric: 50.7924, val_loss: 51.1698, val_MinusLogProbMetric: 51.1698

Epoch 95: val_loss did not improve from 50.85534
196/196 - 31s - loss: 50.7924 - MinusLogProbMetric: 50.7924 - val_loss: 51.1698 - val_MinusLogProbMetric: 51.1698 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 96/1000
2023-10-27 13:35:39.168 
Epoch 96/1000 
	 loss: 50.5797, MinusLogProbMetric: 50.5797, val_loss: 50.0992, val_MinusLogProbMetric: 50.0992

Epoch 96: val_loss improved from 50.85534 to 50.09918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 29s - loss: 50.5797 - MinusLogProbMetric: 50.5797 - val_loss: 50.0992 - val_MinusLogProbMetric: 50.0992 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 97/1000
2023-10-27 13:36:06.822 
Epoch 97/1000 
	 loss: 51.5124, MinusLogProbMetric: 51.5124, val_loss: 50.1653, val_MinusLogProbMetric: 50.1653

Epoch 97: val_loss did not improve from 50.09918
196/196 - 27s - loss: 51.5124 - MinusLogProbMetric: 51.5124 - val_loss: 50.1653 - val_MinusLogProbMetric: 50.1653 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 98/1000
2023-10-27 13:36:33.622 
Epoch 98/1000 
	 loss: 50.2140, MinusLogProbMetric: 50.2140, val_loss: 51.2285, val_MinusLogProbMetric: 51.2285

Epoch 98: val_loss did not improve from 50.09918
196/196 - 27s - loss: 50.2140 - MinusLogProbMetric: 50.2140 - val_loss: 51.2285 - val_MinusLogProbMetric: 51.2285 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 99/1000
2023-10-27 13:37:00.692 
Epoch 99/1000 
	 loss: 50.1987, MinusLogProbMetric: 50.1987, val_loss: 50.4867, val_MinusLogProbMetric: 50.4867

Epoch 99: val_loss did not improve from 50.09918
196/196 - 27s - loss: 50.1987 - MinusLogProbMetric: 50.1987 - val_loss: 50.4867 - val_MinusLogProbMetric: 50.4867 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 100/1000
2023-10-27 13:37:28.602 
Epoch 100/1000 
	 loss: 50.1356, MinusLogProbMetric: 50.1356, val_loss: 49.7553, val_MinusLogProbMetric: 49.7553

Epoch 100: val_loss improved from 50.09918 to 49.75529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 50.1356 - MinusLogProbMetric: 50.1356 - val_loss: 49.7553 - val_MinusLogProbMetric: 49.7553 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 101/1000
2023-10-27 13:38:03.382 
Epoch 101/1000 
	 loss: 50.1586, MinusLogProbMetric: 50.1586, val_loss: 49.9932, val_MinusLogProbMetric: 49.9932

Epoch 101: val_loss did not improve from 49.75529
196/196 - 34s - loss: 50.1586 - MinusLogProbMetric: 50.1586 - val_loss: 49.9932 - val_MinusLogProbMetric: 49.9932 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 102/1000
2023-10-27 13:38:33.825 
Epoch 102/1000 
	 loss: 50.2357, MinusLogProbMetric: 50.2357, val_loss: 50.7334, val_MinusLogProbMetric: 50.7334

Epoch 102: val_loss did not improve from 49.75529
196/196 - 30s - loss: 50.2357 - MinusLogProbMetric: 50.2357 - val_loss: 50.7334 - val_MinusLogProbMetric: 50.7334 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 103/1000
2023-10-27 13:39:00.746 
Epoch 103/1000 
	 loss: 50.0679, MinusLogProbMetric: 50.0679, val_loss: 51.6463, val_MinusLogProbMetric: 51.6463

Epoch 103: val_loss did not improve from 49.75529
196/196 - 27s - loss: 50.0679 - MinusLogProbMetric: 50.0679 - val_loss: 51.6463 - val_MinusLogProbMetric: 51.6463 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 104/1000
2023-10-27 13:39:27.558 
Epoch 104/1000 
	 loss: 49.8735, MinusLogProbMetric: 49.8735, val_loss: 50.2036, val_MinusLogProbMetric: 50.2036

Epoch 104: val_loss did not improve from 49.75529
196/196 - 27s - loss: 49.8735 - MinusLogProbMetric: 49.8735 - val_loss: 50.2036 - val_MinusLogProbMetric: 50.2036 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 105/1000
2023-10-27 13:39:54.637 
Epoch 105/1000 
	 loss: 50.1233, MinusLogProbMetric: 50.1233, val_loss: 51.0492, val_MinusLogProbMetric: 51.0492

Epoch 105: val_loss did not improve from 49.75529
196/196 - 27s - loss: 50.1233 - MinusLogProbMetric: 50.1233 - val_loss: 51.0492 - val_MinusLogProbMetric: 51.0492 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 106/1000
2023-10-27 13:40:22.520 
Epoch 106/1000 
	 loss: 49.7516, MinusLogProbMetric: 49.7516, val_loss: 50.5272, val_MinusLogProbMetric: 50.5272

Epoch 106: val_loss did not improve from 49.75529
196/196 - 28s - loss: 49.7516 - MinusLogProbMetric: 49.7516 - val_loss: 50.5272 - val_MinusLogProbMetric: 50.5272 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 107/1000
2023-10-27 13:40:56.092 
Epoch 107/1000 
	 loss: 49.7331, MinusLogProbMetric: 49.7331, val_loss: 51.2263, val_MinusLogProbMetric: 51.2263

Epoch 107: val_loss did not improve from 49.75529
196/196 - 34s - loss: 49.7331 - MinusLogProbMetric: 49.7331 - val_loss: 51.2263 - val_MinusLogProbMetric: 51.2263 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 108/1000
2023-10-27 13:41:26.415 
Epoch 108/1000 
	 loss: 49.9146, MinusLogProbMetric: 49.9146, val_loss: 49.4557, val_MinusLogProbMetric: 49.4557

Epoch 108: val_loss improved from 49.75529 to 49.45570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 31s - loss: 49.9146 - MinusLogProbMetric: 49.9146 - val_loss: 49.4557 - val_MinusLogProbMetric: 49.4557 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 109/1000
2023-10-27 13:41:53.572 
Epoch 109/1000 
	 loss: 49.9210, MinusLogProbMetric: 49.9210, val_loss: 49.3085, val_MinusLogProbMetric: 49.3085

Epoch 109: val_loss improved from 49.45570 to 49.30848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 49.9210 - MinusLogProbMetric: 49.9210 - val_loss: 49.3085 - val_MinusLogProbMetric: 49.3085 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 110/1000
2023-10-27 13:42:21.150 
Epoch 110/1000 
	 loss: 49.7707, MinusLogProbMetric: 49.7707, val_loss: 49.7749, val_MinusLogProbMetric: 49.7749

Epoch 110: val_loss did not improve from 49.30848
196/196 - 27s - loss: 49.7707 - MinusLogProbMetric: 49.7707 - val_loss: 49.7749 - val_MinusLogProbMetric: 49.7749 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 111/1000
2023-10-27 13:42:48.043 
Epoch 111/1000 
	 loss: 49.3782, MinusLogProbMetric: 49.3782, val_loss: 50.1038, val_MinusLogProbMetric: 50.1038

Epoch 111: val_loss did not improve from 49.30848
196/196 - 27s - loss: 49.3782 - MinusLogProbMetric: 49.3782 - val_loss: 50.1038 - val_MinusLogProbMetric: 50.1038 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 112/1000
2023-10-27 13:43:15.201 
Epoch 112/1000 
	 loss: 49.6122, MinusLogProbMetric: 49.6122, val_loss: 49.2774, val_MinusLogProbMetric: 49.2774

Epoch 112: val_loss improved from 49.30848 to 49.27736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 49.6122 - MinusLogProbMetric: 49.6122 - val_loss: 49.2774 - val_MinusLogProbMetric: 49.2774 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 113/1000
2023-10-27 13:43:46.204 
Epoch 113/1000 
	 loss: 49.5449, MinusLogProbMetric: 49.5449, val_loss: 51.5587, val_MinusLogProbMetric: 51.5587

Epoch 113: val_loss did not improve from 49.27736
196/196 - 31s - loss: 49.5449 - MinusLogProbMetric: 49.5449 - val_loss: 51.5587 - val_MinusLogProbMetric: 51.5587 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 114/1000
2023-10-27 13:44:17.849 
Epoch 114/1000 
	 loss: 49.4650, MinusLogProbMetric: 49.4650, val_loss: 51.1902, val_MinusLogProbMetric: 51.1902

Epoch 114: val_loss did not improve from 49.27736
196/196 - 32s - loss: 49.4650 - MinusLogProbMetric: 49.4650 - val_loss: 51.1902 - val_MinusLogProbMetric: 51.1902 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 115/1000
2023-10-27 13:44:45.610 
Epoch 115/1000 
	 loss: 49.2276, MinusLogProbMetric: 49.2276, val_loss: 48.9840, val_MinusLogProbMetric: 48.9840

Epoch 115: val_loss improved from 49.27736 to 48.98399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 49.2276 - MinusLogProbMetric: 49.2276 - val_loss: 48.9840 - val_MinusLogProbMetric: 48.9840 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 116/1000
2023-10-27 13:45:13.363 
Epoch 116/1000 
	 loss: 49.3606, MinusLogProbMetric: 49.3606, val_loss: 49.0782, val_MinusLogProbMetric: 49.0782

Epoch 116: val_loss did not improve from 48.98399
196/196 - 27s - loss: 49.3606 - MinusLogProbMetric: 49.3606 - val_loss: 49.0782 - val_MinusLogProbMetric: 49.0782 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 117/1000
2023-10-27 13:45:44.180 
Epoch 117/1000 
	 loss: 49.0208, MinusLogProbMetric: 49.0208, val_loss: 49.4524, val_MinusLogProbMetric: 49.4524

Epoch 117: val_loss did not improve from 48.98399
196/196 - 31s - loss: 49.0208 - MinusLogProbMetric: 49.0208 - val_loss: 49.4524 - val_MinusLogProbMetric: 49.4524 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 118/1000
2023-10-27 13:46:11.059 
Epoch 118/1000 
	 loss: 49.3754, MinusLogProbMetric: 49.3754, val_loss: 50.5507, val_MinusLogProbMetric: 50.5507

Epoch 118: val_loss did not improve from 48.98399
196/196 - 27s - loss: 49.3754 - MinusLogProbMetric: 49.3754 - val_loss: 50.5507 - val_MinusLogProbMetric: 50.5507 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 119/1000
2023-10-27 13:46:40.694 
Epoch 119/1000 
	 loss: 49.0819, MinusLogProbMetric: 49.0819, val_loss: 49.4622, val_MinusLogProbMetric: 49.4622

Epoch 119: val_loss did not improve from 48.98399
196/196 - 30s - loss: 49.0819 - MinusLogProbMetric: 49.0819 - val_loss: 49.4622 - val_MinusLogProbMetric: 49.4622 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 120/1000
2023-10-27 13:47:13.842 
Epoch 120/1000 
	 loss: 49.4608, MinusLogProbMetric: 49.4608, val_loss: 49.5853, val_MinusLogProbMetric: 49.5853

Epoch 120: val_loss did not improve from 48.98399
196/196 - 33s - loss: 49.4608 - MinusLogProbMetric: 49.4608 - val_loss: 49.5853 - val_MinusLogProbMetric: 49.5853 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 121/1000
2023-10-27 13:47:40.996 
Epoch 121/1000 
	 loss: 48.8385, MinusLogProbMetric: 48.8385, val_loss: 50.2937, val_MinusLogProbMetric: 50.2937

Epoch 121: val_loss did not improve from 48.98399
196/196 - 27s - loss: 48.8385 - MinusLogProbMetric: 48.8385 - val_loss: 50.2937 - val_MinusLogProbMetric: 50.2937 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 122/1000
2023-10-27 13:48:12.341 
Epoch 122/1000 
	 loss: 49.1079, MinusLogProbMetric: 49.1079, val_loss: 48.4529, val_MinusLogProbMetric: 48.4529

Epoch 122: val_loss improved from 48.98399 to 48.45290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 32s - loss: 49.1079 - MinusLogProbMetric: 49.1079 - val_loss: 48.4529 - val_MinusLogProbMetric: 48.4529 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 123/1000
2023-10-27 13:48:43.293 
Epoch 123/1000 
	 loss: 49.0750, MinusLogProbMetric: 49.0750, val_loss: 49.8717, val_MinusLogProbMetric: 49.8717

Epoch 123: val_loss did not improve from 48.45290
196/196 - 30s - loss: 49.0750 - MinusLogProbMetric: 49.0750 - val_loss: 49.8717 - val_MinusLogProbMetric: 49.8717 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 124/1000
2023-10-27 13:49:09.999 
Epoch 124/1000 
	 loss: 48.7919, MinusLogProbMetric: 48.7919, val_loss: 49.4061, val_MinusLogProbMetric: 49.4061

Epoch 124: val_loss did not improve from 48.45290
196/196 - 27s - loss: 48.7919 - MinusLogProbMetric: 48.7919 - val_loss: 49.4061 - val_MinusLogProbMetric: 49.4061 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 125/1000
2023-10-27 13:49:40.721 
Epoch 125/1000 
	 loss: 49.2223, MinusLogProbMetric: 49.2223, val_loss: 49.4428, val_MinusLogProbMetric: 49.4428

Epoch 125: val_loss did not improve from 48.45290
196/196 - 31s - loss: 49.2223 - MinusLogProbMetric: 49.2223 - val_loss: 49.4428 - val_MinusLogProbMetric: 49.4428 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 126/1000
2023-10-27 13:50:10.024 
Epoch 126/1000 
	 loss: 48.5971, MinusLogProbMetric: 48.5971, val_loss: 50.0865, val_MinusLogProbMetric: 50.0865

Epoch 126: val_loss did not improve from 48.45290
196/196 - 29s - loss: 48.5971 - MinusLogProbMetric: 48.5971 - val_loss: 50.0865 - val_MinusLogProbMetric: 50.0865 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 127/1000
2023-10-27 13:50:36.969 
Epoch 127/1000 
	 loss: 48.6679, MinusLogProbMetric: 48.6679, val_loss: 49.2669, val_MinusLogProbMetric: 49.2669

Epoch 127: val_loss did not improve from 48.45290
196/196 - 27s - loss: 48.6679 - MinusLogProbMetric: 48.6679 - val_loss: 49.2669 - val_MinusLogProbMetric: 49.2669 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 128/1000
2023-10-27 13:51:07.619 
Epoch 128/1000 
	 loss: 48.9930, MinusLogProbMetric: 48.9930, val_loss: 49.0840, val_MinusLogProbMetric: 49.0840

Epoch 128: val_loss did not improve from 48.45290
196/196 - 31s - loss: 48.9930 - MinusLogProbMetric: 48.9930 - val_loss: 49.0840 - val_MinusLogProbMetric: 49.0840 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 129/1000
2023-10-27 13:51:38.614 
Epoch 129/1000 
	 loss: 49.0976, MinusLogProbMetric: 49.0976, val_loss: 48.8573, val_MinusLogProbMetric: 48.8573

Epoch 129: val_loss did not improve from 48.45290
196/196 - 31s - loss: 49.0976 - MinusLogProbMetric: 49.0976 - val_loss: 48.8573 - val_MinusLogProbMetric: 48.8573 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 130/1000
2023-10-27 13:52:05.559 
Epoch 130/1000 
	 loss: 48.3625, MinusLogProbMetric: 48.3625, val_loss: 50.6979, val_MinusLogProbMetric: 50.6979

Epoch 130: val_loss did not improve from 48.45290
196/196 - 27s - loss: 48.3625 - MinusLogProbMetric: 48.3625 - val_loss: 50.6979 - val_MinusLogProbMetric: 50.6979 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 131/1000
2023-10-27 13:52:38.111 
Epoch 131/1000 
	 loss: 48.5535, MinusLogProbMetric: 48.5535, val_loss: 48.3231, val_MinusLogProbMetric: 48.3231

Epoch 131: val_loss improved from 48.45290 to 48.32310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 33s - loss: 48.5535 - MinusLogProbMetric: 48.5535 - val_loss: 48.3231 - val_MinusLogProbMetric: 48.3231 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 132/1000
2023-10-27 13:53:07.005 
Epoch 132/1000 
	 loss: 48.4740, MinusLogProbMetric: 48.4740, val_loss: 48.4316, val_MinusLogProbMetric: 48.4316

Epoch 132: val_loss did not improve from 48.32310
196/196 - 28s - loss: 48.4740 - MinusLogProbMetric: 48.4740 - val_loss: 48.4316 - val_MinusLogProbMetric: 48.4316 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 133/1000
2023-10-27 13:53:34.257 
Epoch 133/1000 
	 loss: 48.2348, MinusLogProbMetric: 48.2348, val_loss: 49.3949, val_MinusLogProbMetric: 49.3949

Epoch 133: val_loss did not improve from 48.32310
196/196 - 27s - loss: 48.2348 - MinusLogProbMetric: 48.2348 - val_loss: 49.3949 - val_MinusLogProbMetric: 49.3949 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 134/1000
2023-10-27 13:54:07.017 
Epoch 134/1000 
	 loss: 49.5410, MinusLogProbMetric: 49.5410, val_loss: 49.7574, val_MinusLogProbMetric: 49.7574

Epoch 134: val_loss did not improve from 48.32310
196/196 - 33s - loss: 49.5410 - MinusLogProbMetric: 49.5410 - val_loss: 49.7574 - val_MinusLogProbMetric: 49.7574 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 135/1000
2023-10-27 13:54:34.683 
Epoch 135/1000 
	 loss: 48.6791, MinusLogProbMetric: 48.6791, val_loss: 48.3871, val_MinusLogProbMetric: 48.3871

Epoch 135: val_loss did not improve from 48.32310
196/196 - 28s - loss: 48.6791 - MinusLogProbMetric: 48.6791 - val_loss: 48.3871 - val_MinusLogProbMetric: 48.3871 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 136/1000
2023-10-27 13:55:02.148 
Epoch 136/1000 
	 loss: 48.3667, MinusLogProbMetric: 48.3667, val_loss: 48.7322, val_MinusLogProbMetric: 48.7322

Epoch 136: val_loss did not improve from 48.32310
196/196 - 27s - loss: 48.3667 - MinusLogProbMetric: 48.3667 - val_loss: 48.7322 - val_MinusLogProbMetric: 48.7322 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 137/1000
2023-10-27 13:55:34.294 
Epoch 137/1000 
	 loss: 48.3298, MinusLogProbMetric: 48.3298, val_loss: 48.2623, val_MinusLogProbMetric: 48.2623

Epoch 137: val_loss improved from 48.32310 to 48.26235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 33s - loss: 48.3298 - MinusLogProbMetric: 48.3298 - val_loss: 48.2623 - val_MinusLogProbMetric: 48.2623 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 138/1000
2023-10-27 13:56:01.372 
Epoch 138/1000 
	 loss: 48.2261, MinusLogProbMetric: 48.2261, val_loss: 49.0595, val_MinusLogProbMetric: 49.0595

Epoch 138: val_loss did not improve from 48.26235
196/196 - 27s - loss: 48.2261 - MinusLogProbMetric: 48.2261 - val_loss: 49.0595 - val_MinusLogProbMetric: 49.0595 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 139/1000
2023-10-27 13:56:28.787 
Epoch 139/1000 
	 loss: 48.1439, MinusLogProbMetric: 48.1439, val_loss: 48.8763, val_MinusLogProbMetric: 48.8763

Epoch 139: val_loss did not improve from 48.26235
196/196 - 27s - loss: 48.1439 - MinusLogProbMetric: 48.1439 - val_loss: 48.8763 - val_MinusLogProbMetric: 48.8763 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 140/1000
2023-10-27 13:56:57.073 
Epoch 140/1000 
	 loss: 48.5588, MinusLogProbMetric: 48.5588, val_loss: 48.3537, val_MinusLogProbMetric: 48.3537

Epoch 140: val_loss did not improve from 48.26235
196/196 - 28s - loss: 48.5588 - MinusLogProbMetric: 48.5588 - val_loss: 48.3537 - val_MinusLogProbMetric: 48.3537 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 141/1000
2023-10-27 13:57:24.544 
Epoch 141/1000 
	 loss: 48.9675, MinusLogProbMetric: 48.9675, val_loss: 49.9959, val_MinusLogProbMetric: 49.9959

Epoch 141: val_loss did not improve from 48.26235
196/196 - 27s - loss: 48.9675 - MinusLogProbMetric: 48.9675 - val_loss: 49.9959 - val_MinusLogProbMetric: 49.9959 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 142/1000
2023-10-27 13:57:51.848 
Epoch 142/1000 
	 loss: 47.8928, MinusLogProbMetric: 47.8928, val_loss: 47.8487, val_MinusLogProbMetric: 47.8487

Epoch 142: val_loss improved from 48.26235 to 47.84870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 47.8928 - MinusLogProbMetric: 47.8928 - val_loss: 47.8487 - val_MinusLogProbMetric: 47.8487 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 143/1000
2023-10-27 13:58:22.675 
Epoch 143/1000 
	 loss: 48.4130, MinusLogProbMetric: 48.4130, val_loss: 49.0121, val_MinusLogProbMetric: 49.0121

Epoch 143: val_loss did not improve from 47.84870
196/196 - 30s - loss: 48.4130 - MinusLogProbMetric: 48.4130 - val_loss: 49.0121 - val_MinusLogProbMetric: 49.0121 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 144/1000
2023-10-27 13:58:49.744 
Epoch 144/1000 
	 loss: 48.2061, MinusLogProbMetric: 48.2061, val_loss: 48.1701, val_MinusLogProbMetric: 48.1701

Epoch 144: val_loss did not improve from 47.84870
196/196 - 27s - loss: 48.2061 - MinusLogProbMetric: 48.2061 - val_loss: 48.1701 - val_MinusLogProbMetric: 48.1701 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 145/1000
2023-10-27 13:59:19.402 
Epoch 145/1000 
	 loss: 47.9401, MinusLogProbMetric: 47.9401, val_loss: 49.1437, val_MinusLogProbMetric: 49.1437

Epoch 145: val_loss did not improve from 47.84870
196/196 - 30s - loss: 47.9401 - MinusLogProbMetric: 47.9401 - val_loss: 49.1437 - val_MinusLogProbMetric: 49.1437 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 146/1000
2023-10-27 13:59:49.305 
Epoch 146/1000 
	 loss: 48.1968, MinusLogProbMetric: 48.1968, val_loss: 48.5914, val_MinusLogProbMetric: 48.5914

Epoch 146: val_loss did not improve from 47.84870
196/196 - 30s - loss: 48.1968 - MinusLogProbMetric: 48.1968 - val_loss: 48.5914 - val_MinusLogProbMetric: 48.5914 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 147/1000
2023-10-27 14:00:16.618 
Epoch 147/1000 
	 loss: 47.9517, MinusLogProbMetric: 47.9517, val_loss: 48.2520, val_MinusLogProbMetric: 48.2520

Epoch 147: val_loss did not improve from 47.84870
196/196 - 27s - loss: 47.9517 - MinusLogProbMetric: 47.9517 - val_loss: 48.2520 - val_MinusLogProbMetric: 48.2520 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 148/1000
2023-10-27 14:00:45.453 
Epoch 148/1000 
	 loss: 48.6957, MinusLogProbMetric: 48.6957, val_loss: 47.8959, val_MinusLogProbMetric: 47.8959

Epoch 148: val_loss did not improve from 47.84870
196/196 - 29s - loss: 48.6957 - MinusLogProbMetric: 48.6957 - val_loss: 47.8959 - val_MinusLogProbMetric: 47.8959 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 149/1000
2023-10-27 14:01:18.652 
Epoch 149/1000 
	 loss: 47.9434, MinusLogProbMetric: 47.9434, val_loss: 47.6951, val_MinusLogProbMetric: 47.6951

Epoch 149: val_loss improved from 47.84870 to 47.69508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 47.9434 - MinusLogProbMetric: 47.9434 - val_loss: 47.6951 - val_MinusLogProbMetric: 47.6951 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 150/1000
2023-10-27 14:01:46.705 
Epoch 150/1000 
	 loss: 47.9346, MinusLogProbMetric: 47.9346, val_loss: 48.4902, val_MinusLogProbMetric: 48.4902

Epoch 150: val_loss did not improve from 47.69508
196/196 - 28s - loss: 47.9346 - MinusLogProbMetric: 47.9346 - val_loss: 48.4902 - val_MinusLogProbMetric: 48.4902 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 151/1000
2023-10-27 14:02:14.146 
Epoch 151/1000 
	 loss: 47.7036, MinusLogProbMetric: 47.7036, val_loss: 47.6766, val_MinusLogProbMetric: 47.6766

Epoch 151: val_loss improved from 47.69508 to 47.67656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 47.7036 - MinusLogProbMetric: 47.7036 - val_loss: 47.6766 - val_MinusLogProbMetric: 47.6766 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 152/1000
2023-10-27 14:02:49.038 
Epoch 152/1000 
	 loss: 47.8228, MinusLogProbMetric: 47.8228, val_loss: 48.8573, val_MinusLogProbMetric: 48.8573

Epoch 152: val_loss did not improve from 47.67656
196/196 - 34s - loss: 47.8228 - MinusLogProbMetric: 47.8228 - val_loss: 48.8573 - val_MinusLogProbMetric: 48.8573 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 153/1000
2023-10-27 14:03:23.570 
Epoch 153/1000 
	 loss: 47.8296, MinusLogProbMetric: 47.8296, val_loss: 48.9886, val_MinusLogProbMetric: 48.9886

Epoch 153: val_loss did not improve from 47.67656
196/196 - 35s - loss: 47.8296 - MinusLogProbMetric: 47.8296 - val_loss: 48.9886 - val_MinusLogProbMetric: 48.9886 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 154/1000
2023-10-27 14:03:57.633 
Epoch 154/1000 
	 loss: 47.9626, MinusLogProbMetric: 47.9626, val_loss: 49.9817, val_MinusLogProbMetric: 49.9817

Epoch 154: val_loss did not improve from 47.67656
196/196 - 34s - loss: 47.9626 - MinusLogProbMetric: 47.9626 - val_loss: 49.9817 - val_MinusLogProbMetric: 49.9817 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 155/1000
2023-10-27 14:04:31.868 
Epoch 155/1000 
	 loss: 47.7118, MinusLogProbMetric: 47.7118, val_loss: 49.3598, val_MinusLogProbMetric: 49.3598

Epoch 155: val_loss did not improve from 47.67656
196/196 - 34s - loss: 47.7118 - MinusLogProbMetric: 47.7118 - val_loss: 49.3598 - val_MinusLogProbMetric: 49.3598 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 156/1000
2023-10-27 14:05:05.988 
Epoch 156/1000 
	 loss: 47.8989, MinusLogProbMetric: 47.8989, val_loss: 53.0555, val_MinusLogProbMetric: 53.0555

Epoch 156: val_loss did not improve from 47.67656
196/196 - 34s - loss: 47.8989 - MinusLogProbMetric: 47.8989 - val_loss: 53.0555 - val_MinusLogProbMetric: 53.0555 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 157/1000
2023-10-27 14:05:40.211 
Epoch 157/1000 
	 loss: 47.8247, MinusLogProbMetric: 47.8247, val_loss: 47.7610, val_MinusLogProbMetric: 47.7610

Epoch 157: val_loss did not improve from 47.67656
196/196 - 34s - loss: 47.8247 - MinusLogProbMetric: 47.8247 - val_loss: 47.7610 - val_MinusLogProbMetric: 47.7610 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 158/1000
2023-10-27 14:06:14.549 
Epoch 158/1000 
	 loss: 47.6375, MinusLogProbMetric: 47.6375, val_loss: 49.1967, val_MinusLogProbMetric: 49.1967

Epoch 158: val_loss did not improve from 47.67656
196/196 - 34s - loss: 47.6375 - MinusLogProbMetric: 47.6375 - val_loss: 49.1967 - val_MinusLogProbMetric: 49.1967 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 159/1000
2023-10-27 14:06:48.549 
Epoch 159/1000 
	 loss: 75.1474, MinusLogProbMetric: 75.1474, val_loss: 57.9505, val_MinusLogProbMetric: 57.9505

Epoch 159: val_loss did not improve from 47.67656
196/196 - 34s - loss: 75.1474 - MinusLogProbMetric: 75.1474 - val_loss: 57.9505 - val_MinusLogProbMetric: 57.9505 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 160/1000
2023-10-27 14:07:22.598 
Epoch 160/1000 
	 loss: 54.4676, MinusLogProbMetric: 54.4676, val_loss: 53.5123, val_MinusLogProbMetric: 53.5123

Epoch 160: val_loss did not improve from 47.67656
196/196 - 34s - loss: 54.4676 - MinusLogProbMetric: 54.4676 - val_loss: 53.5123 - val_MinusLogProbMetric: 53.5123 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 161/1000
2023-10-27 14:07:56.715 
Epoch 161/1000 
	 loss: 51.9749, MinusLogProbMetric: 51.9749, val_loss: 52.6457, val_MinusLogProbMetric: 52.6457

Epoch 161: val_loss did not improve from 47.67656
196/196 - 34s - loss: 51.9749 - MinusLogProbMetric: 51.9749 - val_loss: 52.6457 - val_MinusLogProbMetric: 52.6457 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 162/1000
2023-10-27 14:08:31.197 
Epoch 162/1000 
	 loss: 50.8872, MinusLogProbMetric: 50.8872, val_loss: 52.1447, val_MinusLogProbMetric: 52.1447

Epoch 162: val_loss did not improve from 47.67656
196/196 - 34s - loss: 50.8872 - MinusLogProbMetric: 50.8872 - val_loss: 52.1447 - val_MinusLogProbMetric: 52.1447 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 163/1000
2023-10-27 14:09:04.623 
Epoch 163/1000 
	 loss: 50.5691, MinusLogProbMetric: 50.5691, val_loss: 50.2971, val_MinusLogProbMetric: 50.2971

Epoch 163: val_loss did not improve from 47.67656
196/196 - 33s - loss: 50.5691 - MinusLogProbMetric: 50.5691 - val_loss: 50.2971 - val_MinusLogProbMetric: 50.2971 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 164/1000
2023-10-27 14:09:38.471 
Epoch 164/1000 
	 loss: 49.9543, MinusLogProbMetric: 49.9543, val_loss: 50.8464, val_MinusLogProbMetric: 50.8464

Epoch 164: val_loss did not improve from 47.67656
196/196 - 34s - loss: 49.9543 - MinusLogProbMetric: 49.9543 - val_loss: 50.8464 - val_MinusLogProbMetric: 50.8464 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 165/1000
2023-10-27 14:10:12.435 
Epoch 165/1000 
	 loss: 49.5732, MinusLogProbMetric: 49.5732, val_loss: 50.5434, val_MinusLogProbMetric: 50.5434

Epoch 165: val_loss did not improve from 47.67656
196/196 - 34s - loss: 49.5732 - MinusLogProbMetric: 49.5732 - val_loss: 50.5434 - val_MinusLogProbMetric: 50.5434 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 166/1000
2023-10-27 14:10:46.831 
Epoch 166/1000 
	 loss: 49.4251, MinusLogProbMetric: 49.4251, val_loss: 49.4739, val_MinusLogProbMetric: 49.4739

Epoch 166: val_loss did not improve from 47.67656
196/196 - 34s - loss: 49.4251 - MinusLogProbMetric: 49.4251 - val_loss: 49.4739 - val_MinusLogProbMetric: 49.4739 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 167/1000
2023-10-27 14:11:21.232 
Epoch 167/1000 
	 loss: 49.6123, MinusLogProbMetric: 49.6123, val_loss: 49.6798, val_MinusLogProbMetric: 49.6798

Epoch 167: val_loss did not improve from 47.67656
196/196 - 34s - loss: 49.6123 - MinusLogProbMetric: 49.6123 - val_loss: 49.6798 - val_MinusLogProbMetric: 49.6798 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 168/1000
2023-10-27 14:11:55.594 
Epoch 168/1000 
	 loss: 49.0004, MinusLogProbMetric: 49.0004, val_loss: 49.8935, val_MinusLogProbMetric: 49.8935

Epoch 168: val_loss did not improve from 47.67656
196/196 - 34s - loss: 49.0004 - MinusLogProbMetric: 49.0004 - val_loss: 49.8935 - val_MinusLogProbMetric: 49.8935 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 169/1000
2023-10-27 14:12:30.129 
Epoch 169/1000 
	 loss: 49.0720, MinusLogProbMetric: 49.0720, val_loss: 49.3170, val_MinusLogProbMetric: 49.3170

Epoch 169: val_loss did not improve from 47.67656
196/196 - 35s - loss: 49.0720 - MinusLogProbMetric: 49.0720 - val_loss: 49.3170 - val_MinusLogProbMetric: 49.3170 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 170/1000
2023-10-27 14:13:04.272 
Epoch 170/1000 
	 loss: 48.7643, MinusLogProbMetric: 48.7643, val_loss: 49.9962, val_MinusLogProbMetric: 49.9962

Epoch 170: val_loss did not improve from 47.67656
196/196 - 34s - loss: 48.7643 - MinusLogProbMetric: 48.7643 - val_loss: 49.9962 - val_MinusLogProbMetric: 49.9962 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 171/1000
2023-10-27 14:13:38.436 
Epoch 171/1000 
	 loss: 49.0355, MinusLogProbMetric: 49.0355, val_loss: 49.6997, val_MinusLogProbMetric: 49.6997

Epoch 171: val_loss did not improve from 47.67656
196/196 - 34s - loss: 49.0355 - MinusLogProbMetric: 49.0355 - val_loss: 49.6997 - val_MinusLogProbMetric: 49.6997 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 172/1000
2023-10-27 14:14:12.397 
Epoch 172/1000 
	 loss: 48.8842, MinusLogProbMetric: 48.8842, val_loss: 48.9233, val_MinusLogProbMetric: 48.9233

Epoch 172: val_loss did not improve from 47.67656
196/196 - 34s - loss: 48.8842 - MinusLogProbMetric: 48.8842 - val_loss: 48.9233 - val_MinusLogProbMetric: 48.9233 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 173/1000
2023-10-27 14:14:46.888 
Epoch 173/1000 
	 loss: 48.5495, MinusLogProbMetric: 48.5495, val_loss: 48.2864, val_MinusLogProbMetric: 48.2864

Epoch 173: val_loss did not improve from 47.67656
196/196 - 34s - loss: 48.5495 - MinusLogProbMetric: 48.5495 - val_loss: 48.2864 - val_MinusLogProbMetric: 48.2864 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 174/1000
2023-10-27 14:15:20.870 
Epoch 174/1000 
	 loss: 48.3258, MinusLogProbMetric: 48.3258, val_loss: 49.0844, val_MinusLogProbMetric: 49.0844

Epoch 174: val_loss did not improve from 47.67656
196/196 - 34s - loss: 48.3258 - MinusLogProbMetric: 48.3258 - val_loss: 49.0844 - val_MinusLogProbMetric: 49.0844 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 175/1000
2023-10-27 14:15:55.294 
Epoch 175/1000 
	 loss: 48.1132, MinusLogProbMetric: 48.1132, val_loss: 48.0827, val_MinusLogProbMetric: 48.0827

Epoch 175: val_loss did not improve from 47.67656
196/196 - 34s - loss: 48.1132 - MinusLogProbMetric: 48.1132 - val_loss: 48.0827 - val_MinusLogProbMetric: 48.0827 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 176/1000
2023-10-27 14:16:29.949 
Epoch 176/1000 
	 loss: 48.3808, MinusLogProbMetric: 48.3808, val_loss: 49.5562, val_MinusLogProbMetric: 49.5562

Epoch 176: val_loss did not improve from 47.67656
196/196 - 35s - loss: 48.3808 - MinusLogProbMetric: 48.3808 - val_loss: 49.5562 - val_MinusLogProbMetric: 49.5562 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 177/1000
2023-10-27 14:17:04.426 
Epoch 177/1000 
	 loss: 48.0225, MinusLogProbMetric: 48.0225, val_loss: 48.0141, val_MinusLogProbMetric: 48.0141

Epoch 177: val_loss did not improve from 47.67656
196/196 - 34s - loss: 48.0225 - MinusLogProbMetric: 48.0225 - val_loss: 48.0141 - val_MinusLogProbMetric: 48.0141 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 178/1000
2023-10-27 14:17:38.440 
Epoch 178/1000 
	 loss: 48.9400, MinusLogProbMetric: 48.9400, val_loss: 105.8984, val_MinusLogProbMetric: 105.8984

Epoch 178: val_loss did not improve from 47.67656
196/196 - 34s - loss: 48.9400 - MinusLogProbMetric: 48.9400 - val_loss: 105.8984 - val_MinusLogProbMetric: 105.8984 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 179/1000
2023-10-27 14:18:12.791 
Epoch 179/1000 
	 loss: 55.0782, MinusLogProbMetric: 55.0782, val_loss: 48.7393, val_MinusLogProbMetric: 48.7393

Epoch 179: val_loss did not improve from 47.67656
196/196 - 34s - loss: 55.0782 - MinusLogProbMetric: 55.0782 - val_loss: 48.7393 - val_MinusLogProbMetric: 48.7393 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 180/1000
2023-10-27 14:18:47.568 
Epoch 180/1000 
	 loss: 48.2215, MinusLogProbMetric: 48.2215, val_loss: 51.7555, val_MinusLogProbMetric: 51.7555

Epoch 180: val_loss did not improve from 47.67656
196/196 - 35s - loss: 48.2215 - MinusLogProbMetric: 48.2215 - val_loss: 51.7555 - val_MinusLogProbMetric: 51.7555 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 181/1000
2023-10-27 14:19:21.814 
Epoch 181/1000 
	 loss: 48.1850, MinusLogProbMetric: 48.1850, val_loss: 49.3029, val_MinusLogProbMetric: 49.3029

Epoch 181: val_loss did not improve from 47.67656
196/196 - 34s - loss: 48.1850 - MinusLogProbMetric: 48.1850 - val_loss: 49.3029 - val_MinusLogProbMetric: 49.3029 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 182/1000
2023-10-27 14:19:56.068 
Epoch 182/1000 
	 loss: 47.7796, MinusLogProbMetric: 47.7796, val_loss: 47.3187, val_MinusLogProbMetric: 47.3187

Epoch 182: val_loss improved from 47.67656 to 47.31868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 47.7796 - MinusLogProbMetric: 47.7796 - val_loss: 47.3187 - val_MinusLogProbMetric: 47.3187 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 183/1000
2023-10-27 14:20:31.147 
Epoch 183/1000 
	 loss: 47.8207, MinusLogProbMetric: 47.8207, val_loss: 47.6958, val_MinusLogProbMetric: 47.6958

Epoch 183: val_loss did not improve from 47.31868
196/196 - 34s - loss: 47.8207 - MinusLogProbMetric: 47.8207 - val_loss: 47.6958 - val_MinusLogProbMetric: 47.6958 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 184/1000
2023-10-27 14:21:05.741 
Epoch 184/1000 
	 loss: 47.8163, MinusLogProbMetric: 47.8163, val_loss: 48.7929, val_MinusLogProbMetric: 48.7929

Epoch 184: val_loss did not improve from 47.31868
196/196 - 35s - loss: 47.8163 - MinusLogProbMetric: 47.8163 - val_loss: 48.7929 - val_MinusLogProbMetric: 48.7929 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 185/1000
2023-10-27 14:21:40.221 
Epoch 185/1000 
	 loss: 47.6712, MinusLogProbMetric: 47.6712, val_loss: 48.5387, val_MinusLogProbMetric: 48.5387

Epoch 185: val_loss did not improve from 47.31868
196/196 - 34s - loss: 47.6712 - MinusLogProbMetric: 47.6712 - val_loss: 48.5387 - val_MinusLogProbMetric: 48.5387 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 186/1000
2023-10-27 14:22:14.291 
Epoch 186/1000 
	 loss: 48.0144, MinusLogProbMetric: 48.0144, val_loss: 48.4273, val_MinusLogProbMetric: 48.4273

Epoch 186: val_loss did not improve from 47.31868
196/196 - 34s - loss: 48.0144 - MinusLogProbMetric: 48.0144 - val_loss: 48.4273 - val_MinusLogProbMetric: 48.4273 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 187/1000
2023-10-27 14:22:48.417 
Epoch 187/1000 
	 loss: 47.6865, MinusLogProbMetric: 47.6865, val_loss: 47.9985, val_MinusLogProbMetric: 47.9985

Epoch 187: val_loss did not improve from 47.31868
196/196 - 34s - loss: 47.6865 - MinusLogProbMetric: 47.6865 - val_loss: 47.9985 - val_MinusLogProbMetric: 47.9985 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 188/1000
2023-10-27 14:23:22.653 
Epoch 188/1000 
	 loss: 47.8911, MinusLogProbMetric: 47.8911, val_loss: 47.1119, val_MinusLogProbMetric: 47.1119

Epoch 188: val_loss improved from 47.31868 to 47.11189, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 47.8911 - MinusLogProbMetric: 47.8911 - val_loss: 47.1119 - val_MinusLogProbMetric: 47.1119 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 189/1000
2023-10-27 14:23:57.181 
Epoch 189/1000 
	 loss: 47.5410, MinusLogProbMetric: 47.5410, val_loss: 47.9424, val_MinusLogProbMetric: 47.9424

Epoch 189: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.5410 - MinusLogProbMetric: 47.5410 - val_loss: 47.9424 - val_MinusLogProbMetric: 47.9424 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 190/1000
2023-10-27 14:24:31.643 
Epoch 190/1000 
	 loss: 47.3487, MinusLogProbMetric: 47.3487, val_loss: 47.5921, val_MinusLogProbMetric: 47.5921

Epoch 190: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.3487 - MinusLogProbMetric: 47.3487 - val_loss: 47.5921 - val_MinusLogProbMetric: 47.5921 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 191/1000
2023-10-27 14:25:06.116 
Epoch 191/1000 
	 loss: 47.7712, MinusLogProbMetric: 47.7712, val_loss: 49.7789, val_MinusLogProbMetric: 49.7789

Epoch 191: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.7712 - MinusLogProbMetric: 47.7712 - val_loss: 49.7789 - val_MinusLogProbMetric: 49.7789 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 192/1000
2023-10-27 14:25:40.311 
Epoch 192/1000 
	 loss: 47.3030, MinusLogProbMetric: 47.3030, val_loss: 47.7332, val_MinusLogProbMetric: 47.7332

Epoch 192: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.3030 - MinusLogProbMetric: 47.3030 - val_loss: 47.7332 - val_MinusLogProbMetric: 47.7332 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 193/1000
2023-10-27 14:26:14.189 
Epoch 193/1000 
	 loss: 47.2868, MinusLogProbMetric: 47.2868, val_loss: 48.1914, val_MinusLogProbMetric: 48.1914

Epoch 193: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.2868 - MinusLogProbMetric: 47.2868 - val_loss: 48.1914 - val_MinusLogProbMetric: 48.1914 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 194/1000
2023-10-27 14:26:48.705 
Epoch 194/1000 
	 loss: 47.6025, MinusLogProbMetric: 47.6025, val_loss: 47.5919, val_MinusLogProbMetric: 47.5919

Epoch 194: val_loss did not improve from 47.11189
196/196 - 35s - loss: 47.6025 - MinusLogProbMetric: 47.6025 - val_loss: 47.5919 - val_MinusLogProbMetric: 47.5919 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 195/1000
2023-10-27 14:27:21.907 
Epoch 195/1000 
	 loss: 47.4590, MinusLogProbMetric: 47.4590, val_loss: 47.7108, val_MinusLogProbMetric: 47.7108

Epoch 195: val_loss did not improve from 47.11189
196/196 - 33s - loss: 47.4590 - MinusLogProbMetric: 47.4590 - val_loss: 47.7108 - val_MinusLogProbMetric: 47.7108 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 196/1000
2023-10-27 14:27:56.288 
Epoch 196/1000 
	 loss: 47.6322, MinusLogProbMetric: 47.6322, val_loss: 49.1872, val_MinusLogProbMetric: 49.1872

Epoch 196: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.6322 - MinusLogProbMetric: 47.6322 - val_loss: 49.1872 - val_MinusLogProbMetric: 49.1872 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 197/1000
2023-10-27 14:28:30.408 
Epoch 197/1000 
	 loss: 47.2996, MinusLogProbMetric: 47.2996, val_loss: 48.0871, val_MinusLogProbMetric: 48.0871

Epoch 197: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.2996 - MinusLogProbMetric: 47.2996 - val_loss: 48.0871 - val_MinusLogProbMetric: 48.0871 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 198/1000
2023-10-27 14:29:04.909 
Epoch 198/1000 
	 loss: 47.1959, MinusLogProbMetric: 47.1959, val_loss: 48.1734, val_MinusLogProbMetric: 48.1734

Epoch 198: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.1959 - MinusLogProbMetric: 47.1959 - val_loss: 48.1734 - val_MinusLogProbMetric: 48.1734 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 199/1000
2023-10-27 14:29:39.044 
Epoch 199/1000 
	 loss: 47.4530, MinusLogProbMetric: 47.4530, val_loss: 47.5049, val_MinusLogProbMetric: 47.5049

Epoch 199: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.4530 - MinusLogProbMetric: 47.4530 - val_loss: 47.5049 - val_MinusLogProbMetric: 47.5049 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 200/1000
2023-10-27 14:30:13.383 
Epoch 200/1000 
	 loss: 47.0292, MinusLogProbMetric: 47.0292, val_loss: 47.1979, val_MinusLogProbMetric: 47.1979

Epoch 200: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.0292 - MinusLogProbMetric: 47.0292 - val_loss: 47.1979 - val_MinusLogProbMetric: 47.1979 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 201/1000
2023-10-27 14:30:47.569 
Epoch 201/1000 
	 loss: 46.8867, MinusLogProbMetric: 46.8867, val_loss: 47.7965, val_MinusLogProbMetric: 47.7965

Epoch 201: val_loss did not improve from 47.11189
196/196 - 34s - loss: 46.8867 - MinusLogProbMetric: 46.8867 - val_loss: 47.7965 - val_MinusLogProbMetric: 47.7965 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 202/1000
2023-10-27 14:31:21.605 
Epoch 202/1000 
	 loss: 47.2269, MinusLogProbMetric: 47.2269, val_loss: 47.7620, val_MinusLogProbMetric: 47.7620

Epoch 202: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.2269 - MinusLogProbMetric: 47.2269 - val_loss: 47.7620 - val_MinusLogProbMetric: 47.7620 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 203/1000
2023-10-27 14:31:55.708 
Epoch 203/1000 
	 loss: 46.9987, MinusLogProbMetric: 46.9987, val_loss: 47.2621, val_MinusLogProbMetric: 47.2621

Epoch 203: val_loss did not improve from 47.11189
196/196 - 34s - loss: 46.9987 - MinusLogProbMetric: 46.9987 - val_loss: 47.2621 - val_MinusLogProbMetric: 47.2621 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 204/1000
2023-10-27 14:32:29.870 
Epoch 204/1000 
	 loss: 47.2319, MinusLogProbMetric: 47.2319, val_loss: 47.3765, val_MinusLogProbMetric: 47.3765

Epoch 204: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.2319 - MinusLogProbMetric: 47.2319 - val_loss: 47.3765 - val_MinusLogProbMetric: 47.3765 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 205/1000
2023-10-27 14:33:04.307 
Epoch 205/1000 
	 loss: 47.0915, MinusLogProbMetric: 47.0915, val_loss: 51.6552, val_MinusLogProbMetric: 51.6552

Epoch 205: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.0915 - MinusLogProbMetric: 47.0915 - val_loss: 51.6552 - val_MinusLogProbMetric: 51.6552 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 206/1000
2023-10-27 14:33:38.799 
Epoch 206/1000 
	 loss: 47.0699, MinusLogProbMetric: 47.0699, val_loss: 48.1314, val_MinusLogProbMetric: 48.1314

Epoch 206: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.0699 - MinusLogProbMetric: 47.0699 - val_loss: 48.1314 - val_MinusLogProbMetric: 48.1314 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 207/1000
2023-10-27 14:34:13.192 
Epoch 207/1000 
	 loss: 47.3521, MinusLogProbMetric: 47.3521, val_loss: 49.5559, val_MinusLogProbMetric: 49.5559

Epoch 207: val_loss did not improve from 47.11189
196/196 - 34s - loss: 47.3521 - MinusLogProbMetric: 47.3521 - val_loss: 49.5559 - val_MinusLogProbMetric: 49.5559 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 208/1000
2023-10-27 14:34:47.694 
Epoch 208/1000 
	 loss: 47.2066, MinusLogProbMetric: 47.2066, val_loss: 47.1020, val_MinusLogProbMetric: 47.1020

Epoch 208: val_loss improved from 47.11189 to 47.10198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 47.2066 - MinusLogProbMetric: 47.2066 - val_loss: 47.1020 - val_MinusLogProbMetric: 47.1020 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 209/1000
2023-10-27 14:35:23.066 
Epoch 209/1000 
	 loss: 46.9217, MinusLogProbMetric: 46.9217, val_loss: 47.3710, val_MinusLogProbMetric: 47.3710

Epoch 209: val_loss did not improve from 47.10198
196/196 - 35s - loss: 46.9217 - MinusLogProbMetric: 46.9217 - val_loss: 47.3710 - val_MinusLogProbMetric: 47.3710 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 210/1000
2023-10-27 14:35:57.602 
Epoch 210/1000 
	 loss: 49.4885, MinusLogProbMetric: 49.4885, val_loss: 51.9642, val_MinusLogProbMetric: 51.9642

Epoch 210: val_loss did not improve from 47.10198
196/196 - 35s - loss: 49.4885 - MinusLogProbMetric: 49.4885 - val_loss: 51.9642 - val_MinusLogProbMetric: 51.9642 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 211/1000
2023-10-27 14:36:32.009 
Epoch 211/1000 
	 loss: 47.1018, MinusLogProbMetric: 47.1018, val_loss: 48.9824, val_MinusLogProbMetric: 48.9824

Epoch 211: val_loss did not improve from 47.10198
196/196 - 34s - loss: 47.1018 - MinusLogProbMetric: 47.1018 - val_loss: 48.9824 - val_MinusLogProbMetric: 48.9824 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 212/1000
2023-10-27 14:37:06.411 
Epoch 212/1000 
	 loss: 46.7949, MinusLogProbMetric: 46.7949, val_loss: 47.3771, val_MinusLogProbMetric: 47.3771

Epoch 212: val_loss did not improve from 47.10198
196/196 - 34s - loss: 46.7949 - MinusLogProbMetric: 46.7949 - val_loss: 47.3771 - val_MinusLogProbMetric: 47.3771 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 213/1000
2023-10-27 14:37:38.455 
Epoch 213/1000 
	 loss: 46.9690, MinusLogProbMetric: 46.9690, val_loss: 47.4700, val_MinusLogProbMetric: 47.4700

Epoch 213: val_loss did not improve from 47.10198
196/196 - 32s - loss: 46.9690 - MinusLogProbMetric: 46.9690 - val_loss: 47.4700 - val_MinusLogProbMetric: 47.4700 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 214/1000
2023-10-27 14:38:05.741 
Epoch 214/1000 
	 loss: 46.7400, MinusLogProbMetric: 46.7400, val_loss: 47.3199, val_MinusLogProbMetric: 47.3199

Epoch 214: val_loss did not improve from 47.10198
196/196 - 27s - loss: 46.7400 - MinusLogProbMetric: 46.7400 - val_loss: 47.3199 - val_MinusLogProbMetric: 47.3199 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 215/1000
2023-10-27 14:38:34.366 
Epoch 215/1000 
	 loss: 47.1078, MinusLogProbMetric: 47.1078, val_loss: 47.7542, val_MinusLogProbMetric: 47.7542

Epoch 215: val_loss did not improve from 47.10198
196/196 - 29s - loss: 47.1078 - MinusLogProbMetric: 47.1078 - val_loss: 47.7542 - val_MinusLogProbMetric: 47.7542 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 216/1000
2023-10-27 14:39:05.056 
Epoch 216/1000 
	 loss: 46.9604, MinusLogProbMetric: 46.9604, val_loss: 49.1782, val_MinusLogProbMetric: 49.1782

Epoch 216: val_loss did not improve from 47.10198
196/196 - 31s - loss: 46.9604 - MinusLogProbMetric: 46.9604 - val_loss: 49.1782 - val_MinusLogProbMetric: 49.1782 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 217/1000
2023-10-27 14:39:39.141 
Epoch 217/1000 
	 loss: 46.6881, MinusLogProbMetric: 46.6881, val_loss: 47.0299, val_MinusLogProbMetric: 47.0299

Epoch 217: val_loss improved from 47.10198 to 47.02989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 46.6881 - MinusLogProbMetric: 46.6881 - val_loss: 47.0299 - val_MinusLogProbMetric: 47.0299 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 218/1000
2023-10-27 14:40:13.854 
Epoch 218/1000 
	 loss: 46.8000, MinusLogProbMetric: 46.8000, val_loss: 48.4823, val_MinusLogProbMetric: 48.4823

Epoch 218: val_loss did not improve from 47.02989
196/196 - 34s - loss: 46.8000 - MinusLogProbMetric: 46.8000 - val_loss: 48.4823 - val_MinusLogProbMetric: 48.4823 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 219/1000
2023-10-27 14:40:47.894 
Epoch 219/1000 
	 loss: 46.8277, MinusLogProbMetric: 46.8277, val_loss: 49.0642, val_MinusLogProbMetric: 49.0642

Epoch 219: val_loss did not improve from 47.02989
196/196 - 34s - loss: 46.8277 - MinusLogProbMetric: 46.8277 - val_loss: 49.0642 - val_MinusLogProbMetric: 49.0642 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 220/1000
2023-10-27 14:41:22.214 
Epoch 220/1000 
	 loss: 46.3850, MinusLogProbMetric: 46.3850, val_loss: 45.9648, val_MinusLogProbMetric: 45.9648

Epoch 220: val_loss improved from 47.02989 to 45.96485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 46.3850 - MinusLogProbMetric: 46.3850 - val_loss: 45.9648 - val_MinusLogProbMetric: 45.9648 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 221/1000
2023-10-27 14:41:57.082 
Epoch 221/1000 
	 loss: 46.5499, MinusLogProbMetric: 46.5499, val_loss: 46.9467, val_MinusLogProbMetric: 46.9467

Epoch 221: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.5499 - MinusLogProbMetric: 46.5499 - val_loss: 46.9467 - val_MinusLogProbMetric: 46.9467 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 222/1000
2023-10-27 14:42:31.263 
Epoch 222/1000 
	 loss: 46.6806, MinusLogProbMetric: 46.6806, val_loss: 46.4286, val_MinusLogProbMetric: 46.4286

Epoch 222: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.6806 - MinusLogProbMetric: 46.6806 - val_loss: 46.4286 - val_MinusLogProbMetric: 46.4286 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 223/1000
2023-10-27 14:43:05.663 
Epoch 223/1000 
	 loss: 46.4047, MinusLogProbMetric: 46.4047, val_loss: 46.2826, val_MinusLogProbMetric: 46.2826

Epoch 223: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.4047 - MinusLogProbMetric: 46.4047 - val_loss: 46.2826 - val_MinusLogProbMetric: 46.2826 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 224/1000
2023-10-27 14:43:38.756 
Epoch 224/1000 
	 loss: 46.3193, MinusLogProbMetric: 46.3193, val_loss: 46.9260, val_MinusLogProbMetric: 46.9260

Epoch 224: val_loss did not improve from 45.96485
196/196 - 33s - loss: 46.3193 - MinusLogProbMetric: 46.3193 - val_loss: 46.9260 - val_MinusLogProbMetric: 46.9260 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 225/1000
2023-10-27 14:44:12.870 
Epoch 225/1000 
	 loss: 46.6371, MinusLogProbMetric: 46.6371, val_loss: 47.1294, val_MinusLogProbMetric: 47.1294

Epoch 225: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.6371 - MinusLogProbMetric: 46.6371 - val_loss: 47.1294 - val_MinusLogProbMetric: 47.1294 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 226/1000
2023-10-27 14:44:47.294 
Epoch 226/1000 
	 loss: 46.3279, MinusLogProbMetric: 46.3279, val_loss: 48.2197, val_MinusLogProbMetric: 48.2197

Epoch 226: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.3279 - MinusLogProbMetric: 46.3279 - val_loss: 48.2197 - val_MinusLogProbMetric: 48.2197 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 227/1000
2023-10-27 14:45:20.959 
Epoch 227/1000 
	 loss: 46.8664, MinusLogProbMetric: 46.8664, val_loss: 47.6563, val_MinusLogProbMetric: 47.6563

Epoch 227: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.8664 - MinusLogProbMetric: 46.8664 - val_loss: 47.6563 - val_MinusLogProbMetric: 47.6563 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 228/1000
2023-10-27 14:45:55.021 
Epoch 228/1000 
	 loss: 46.6154, MinusLogProbMetric: 46.6154, val_loss: 46.4464, val_MinusLogProbMetric: 46.4464

Epoch 228: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.6154 - MinusLogProbMetric: 46.6154 - val_loss: 46.4464 - val_MinusLogProbMetric: 46.4464 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 229/1000
2023-10-27 14:46:29.462 
Epoch 229/1000 
	 loss: 46.8705, MinusLogProbMetric: 46.8705, val_loss: 47.3400, val_MinusLogProbMetric: 47.3400

Epoch 229: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.8705 - MinusLogProbMetric: 46.8705 - val_loss: 47.3400 - val_MinusLogProbMetric: 47.3400 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 230/1000
2023-10-27 14:47:03.991 
Epoch 230/1000 
	 loss: 46.7046, MinusLogProbMetric: 46.7046, val_loss: 47.2604, val_MinusLogProbMetric: 47.2604

Epoch 230: val_loss did not improve from 45.96485
196/196 - 35s - loss: 46.7046 - MinusLogProbMetric: 46.7046 - val_loss: 47.2604 - val_MinusLogProbMetric: 47.2604 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 231/1000
2023-10-27 14:47:38.221 
Epoch 231/1000 
	 loss: 46.2761, MinusLogProbMetric: 46.2761, val_loss: 46.1336, val_MinusLogProbMetric: 46.1336

Epoch 231: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.2761 - MinusLogProbMetric: 46.2761 - val_loss: 46.1336 - val_MinusLogProbMetric: 46.1336 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 232/1000
2023-10-27 14:48:12.409 
Epoch 232/1000 
	 loss: 46.7924, MinusLogProbMetric: 46.7924, val_loss: 47.1503, val_MinusLogProbMetric: 47.1503

Epoch 232: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.7924 - MinusLogProbMetric: 46.7924 - val_loss: 47.1503 - val_MinusLogProbMetric: 47.1503 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 233/1000
2023-10-27 14:48:46.692 
Epoch 233/1000 
	 loss: 47.0374, MinusLogProbMetric: 47.0374, val_loss: 46.9611, val_MinusLogProbMetric: 46.9611

Epoch 233: val_loss did not improve from 45.96485
196/196 - 34s - loss: 47.0374 - MinusLogProbMetric: 47.0374 - val_loss: 46.9611 - val_MinusLogProbMetric: 46.9611 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 234/1000
2023-10-27 14:49:20.968 
Epoch 234/1000 
	 loss: 46.2581, MinusLogProbMetric: 46.2581, val_loss: 46.6269, val_MinusLogProbMetric: 46.6269

Epoch 234: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.2581 - MinusLogProbMetric: 46.2581 - val_loss: 46.6269 - val_MinusLogProbMetric: 46.6269 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 235/1000
2023-10-27 14:49:55.613 
Epoch 235/1000 
	 loss: 46.8275, MinusLogProbMetric: 46.8275, val_loss: 46.8209, val_MinusLogProbMetric: 46.8209

Epoch 235: val_loss did not improve from 45.96485
196/196 - 35s - loss: 46.8275 - MinusLogProbMetric: 46.8275 - val_loss: 46.8209 - val_MinusLogProbMetric: 46.8209 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 236/1000
2023-10-27 14:50:29.903 
Epoch 236/1000 
	 loss: 46.2545, MinusLogProbMetric: 46.2545, val_loss: 47.6863, val_MinusLogProbMetric: 47.6863

Epoch 236: val_loss did not improve from 45.96485
196/196 - 34s - loss: 46.2545 - MinusLogProbMetric: 46.2545 - val_loss: 47.6863 - val_MinusLogProbMetric: 47.6863 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 237/1000
2023-10-27 14:51:03.832 
Epoch 237/1000 
	 loss: 46.3063, MinusLogProbMetric: 46.3063, val_loss: 45.9591, val_MinusLogProbMetric: 45.9591

Epoch 237: val_loss improved from 45.96485 to 45.95913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 46.3063 - MinusLogProbMetric: 46.3063 - val_loss: 45.9591 - val_MinusLogProbMetric: 45.9591 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 238/1000
2023-10-27 14:51:37.644 
Epoch 238/1000 
	 loss: 46.5017, MinusLogProbMetric: 46.5017, val_loss: 46.8193, val_MinusLogProbMetric: 46.8193

Epoch 238: val_loss did not improve from 45.95913
196/196 - 33s - loss: 46.5017 - MinusLogProbMetric: 46.5017 - val_loss: 46.8193 - val_MinusLogProbMetric: 46.8193 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 239/1000
2023-10-27 14:52:12.149 
Epoch 239/1000 
	 loss: 46.3435, MinusLogProbMetric: 46.3435, val_loss: 47.6120, val_MinusLogProbMetric: 47.6120

Epoch 239: val_loss did not improve from 45.95913
196/196 - 35s - loss: 46.3435 - MinusLogProbMetric: 46.3435 - val_loss: 47.6120 - val_MinusLogProbMetric: 47.6120 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 240/1000
2023-10-27 14:52:46.333 
Epoch 240/1000 
	 loss: 46.1760, MinusLogProbMetric: 46.1760, val_loss: 48.1558, val_MinusLogProbMetric: 48.1558

Epoch 240: val_loss did not improve from 45.95913
196/196 - 34s - loss: 46.1760 - MinusLogProbMetric: 46.1760 - val_loss: 48.1558 - val_MinusLogProbMetric: 48.1558 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 241/1000
2023-10-27 14:53:20.969 
Epoch 241/1000 
	 loss: 46.0918, MinusLogProbMetric: 46.0918, val_loss: 46.2945, val_MinusLogProbMetric: 46.2945

Epoch 241: val_loss did not improve from 45.95913
196/196 - 35s - loss: 46.0918 - MinusLogProbMetric: 46.0918 - val_loss: 46.2945 - val_MinusLogProbMetric: 46.2945 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 242/1000
2023-10-27 14:53:55.315 
Epoch 242/1000 
	 loss: 54.9367, MinusLogProbMetric: 54.9367, val_loss: 53.7427, val_MinusLogProbMetric: 53.7427

Epoch 242: val_loss did not improve from 45.95913
196/196 - 34s - loss: 54.9367 - MinusLogProbMetric: 54.9367 - val_loss: 53.7427 - val_MinusLogProbMetric: 53.7427 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 243/1000
2023-10-27 14:54:29.854 
Epoch 243/1000 
	 loss: 49.6093, MinusLogProbMetric: 49.6093, val_loss: 47.1706, val_MinusLogProbMetric: 47.1706

Epoch 243: val_loss did not improve from 45.95913
196/196 - 35s - loss: 49.6093 - MinusLogProbMetric: 49.6093 - val_loss: 47.1706 - val_MinusLogProbMetric: 47.1706 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 244/1000
2023-10-27 14:55:04.142 
Epoch 244/1000 
	 loss: 46.7316, MinusLogProbMetric: 46.7316, val_loss: 47.6312, val_MinusLogProbMetric: 47.6312

Epoch 244: val_loss did not improve from 45.95913
196/196 - 34s - loss: 46.7316 - MinusLogProbMetric: 46.7316 - val_loss: 47.6312 - val_MinusLogProbMetric: 47.6312 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 245/1000
2023-10-27 14:55:38.254 
Epoch 245/1000 
	 loss: 46.5018, MinusLogProbMetric: 46.5018, val_loss: 46.7588, val_MinusLogProbMetric: 46.7588

Epoch 245: val_loss did not improve from 45.95913
196/196 - 34s - loss: 46.5018 - MinusLogProbMetric: 46.5018 - val_loss: 46.7588 - val_MinusLogProbMetric: 46.7588 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 246/1000
2023-10-27 14:56:12.545 
Epoch 246/1000 
	 loss: 46.5274, MinusLogProbMetric: 46.5274, val_loss: 46.4930, val_MinusLogProbMetric: 46.4930

Epoch 246: val_loss did not improve from 45.95913
196/196 - 34s - loss: 46.5274 - MinusLogProbMetric: 46.5274 - val_loss: 46.4930 - val_MinusLogProbMetric: 46.4930 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 247/1000
2023-10-27 14:56:45.886 
Epoch 247/1000 
	 loss: 46.1848, MinusLogProbMetric: 46.1848, val_loss: 45.9710, val_MinusLogProbMetric: 45.9710

Epoch 247: val_loss did not improve from 45.95913
196/196 - 33s - loss: 46.1848 - MinusLogProbMetric: 46.1848 - val_loss: 45.9710 - val_MinusLogProbMetric: 45.9710 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 248/1000
2023-10-27 14:57:12.724 
Epoch 248/1000 
	 loss: 46.0828, MinusLogProbMetric: 46.0828, val_loss: 48.9146, val_MinusLogProbMetric: 48.9146

Epoch 248: val_loss did not improve from 45.95913
196/196 - 27s - loss: 46.0828 - MinusLogProbMetric: 46.0828 - val_loss: 48.9146 - val_MinusLogProbMetric: 48.9146 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 249/1000
2023-10-27 14:57:39.550 
Epoch 249/1000 
	 loss: 46.4991, MinusLogProbMetric: 46.4991, val_loss: 46.3326, val_MinusLogProbMetric: 46.3326

Epoch 249: val_loss did not improve from 45.95913
196/196 - 27s - loss: 46.4991 - MinusLogProbMetric: 46.4991 - val_loss: 46.3326 - val_MinusLogProbMetric: 46.3326 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 250/1000
2023-10-27 14:58:07.870 
Epoch 250/1000 
	 loss: 46.1012, MinusLogProbMetric: 46.1012, val_loss: 46.1727, val_MinusLogProbMetric: 46.1727

Epoch 250: val_loss did not improve from 45.95913
196/196 - 28s - loss: 46.1012 - MinusLogProbMetric: 46.1012 - val_loss: 46.1727 - val_MinusLogProbMetric: 46.1727 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 251/1000
2023-10-27 14:58:41.912 
Epoch 251/1000 
	 loss: 46.1026, MinusLogProbMetric: 46.1026, val_loss: 45.9284, val_MinusLogProbMetric: 45.9284

Epoch 251: val_loss improved from 45.95913 to 45.92837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 46.1026 - MinusLogProbMetric: 46.1026 - val_loss: 45.9284 - val_MinusLogProbMetric: 45.9284 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 252/1000
2023-10-27 14:59:16.683 
Epoch 252/1000 
	 loss: 46.1253, MinusLogProbMetric: 46.1253, val_loss: 46.4249, val_MinusLogProbMetric: 46.4249

Epoch 252: val_loss did not improve from 45.92837
196/196 - 34s - loss: 46.1253 - MinusLogProbMetric: 46.1253 - val_loss: 46.4249 - val_MinusLogProbMetric: 46.4249 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 253/1000
2023-10-27 14:59:44.785 
Epoch 253/1000 
	 loss: 45.9345, MinusLogProbMetric: 45.9345, val_loss: 46.2696, val_MinusLogProbMetric: 46.2696

Epoch 253: val_loss did not improve from 45.92837
196/196 - 28s - loss: 45.9345 - MinusLogProbMetric: 45.9345 - val_loss: 46.2696 - val_MinusLogProbMetric: 46.2696 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 254/1000
2023-10-27 15:00:17.635 
Epoch 254/1000 
	 loss: 46.0927, MinusLogProbMetric: 46.0927, val_loss: 46.4018, val_MinusLogProbMetric: 46.4018

Epoch 254: val_loss did not improve from 45.92837
196/196 - 33s - loss: 46.0927 - MinusLogProbMetric: 46.0927 - val_loss: 46.4018 - val_MinusLogProbMetric: 46.4018 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 255/1000
2023-10-27 15:00:45.572 
Epoch 255/1000 
	 loss: 46.1573, MinusLogProbMetric: 46.1573, val_loss: 46.3881, val_MinusLogProbMetric: 46.3881

Epoch 255: val_loss did not improve from 45.92837
196/196 - 28s - loss: 46.1573 - MinusLogProbMetric: 46.1573 - val_loss: 46.3881 - val_MinusLogProbMetric: 46.3881 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 256/1000
2023-10-27 15:01:16.355 
Epoch 256/1000 
	 loss: 45.7885, MinusLogProbMetric: 45.7885, val_loss: 46.1912, val_MinusLogProbMetric: 46.1912

Epoch 256: val_loss did not improve from 45.92837
196/196 - 31s - loss: 45.7885 - MinusLogProbMetric: 45.7885 - val_loss: 46.1912 - val_MinusLogProbMetric: 46.1912 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 257/1000
2023-10-27 15:01:50.232 
Epoch 257/1000 
	 loss: 46.0786, MinusLogProbMetric: 46.0786, val_loss: 46.0165, val_MinusLogProbMetric: 46.0165

Epoch 257: val_loss did not improve from 45.92837
196/196 - 34s - loss: 46.0786 - MinusLogProbMetric: 46.0786 - val_loss: 46.0165 - val_MinusLogProbMetric: 46.0165 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 258/1000
2023-10-27 15:02:23.311 
Epoch 258/1000 
	 loss: 45.8528, MinusLogProbMetric: 45.8528, val_loss: 45.4916, val_MinusLogProbMetric: 45.4916

Epoch 258: val_loss improved from 45.92837 to 45.49164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 45.8528 - MinusLogProbMetric: 45.8528 - val_loss: 45.4916 - val_MinusLogProbMetric: 45.4916 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 259/1000
2023-10-27 15:02:53.767 
Epoch 259/1000 
	 loss: 45.7696, MinusLogProbMetric: 45.7696, val_loss: 45.9157, val_MinusLogProbMetric: 45.9157

Epoch 259: val_loss did not improve from 45.49164
196/196 - 30s - loss: 45.7696 - MinusLogProbMetric: 45.7696 - val_loss: 45.9157 - val_MinusLogProbMetric: 45.9157 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 260/1000
2023-10-27 15:03:25.874 
Epoch 260/1000 
	 loss: 46.1625, MinusLogProbMetric: 46.1625, val_loss: 48.2123, val_MinusLogProbMetric: 48.2123

Epoch 260: val_loss did not improve from 45.49164
196/196 - 32s - loss: 46.1625 - MinusLogProbMetric: 46.1625 - val_loss: 48.2123 - val_MinusLogProbMetric: 48.2123 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 261/1000
2023-10-27 15:03:56.061 
Epoch 261/1000 
	 loss: 46.0552, MinusLogProbMetric: 46.0552, val_loss: 46.0509, val_MinusLogProbMetric: 46.0509

Epoch 261: val_loss did not improve from 45.49164
196/196 - 30s - loss: 46.0552 - MinusLogProbMetric: 46.0552 - val_loss: 46.0509 - val_MinusLogProbMetric: 46.0509 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 262/1000
2023-10-27 15:04:29.363 
Epoch 262/1000 
	 loss: 46.0586, MinusLogProbMetric: 46.0586, val_loss: 47.7311, val_MinusLogProbMetric: 47.7311

Epoch 262: val_loss did not improve from 45.49164
196/196 - 33s - loss: 46.0586 - MinusLogProbMetric: 46.0586 - val_loss: 47.7311 - val_MinusLogProbMetric: 47.7311 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 263/1000
2023-10-27 15:05:03.076 
Epoch 263/1000 
	 loss: 45.6828, MinusLogProbMetric: 45.6828, val_loss: 45.6033, val_MinusLogProbMetric: 45.6033

Epoch 263: val_loss did not improve from 45.49164
196/196 - 34s - loss: 45.6828 - MinusLogProbMetric: 45.6828 - val_loss: 45.6033 - val_MinusLogProbMetric: 45.6033 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 264/1000
2023-10-27 15:05:35.028 
Epoch 264/1000 
	 loss: 45.7583, MinusLogProbMetric: 45.7583, val_loss: 47.9272, val_MinusLogProbMetric: 47.9272

Epoch 264: val_loss did not improve from 45.49164
196/196 - 32s - loss: 45.7583 - MinusLogProbMetric: 45.7583 - val_loss: 47.9272 - val_MinusLogProbMetric: 47.9272 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 265/1000
2023-10-27 15:06:04.731 
Epoch 265/1000 
	 loss: 46.1145, MinusLogProbMetric: 46.1145, val_loss: 46.1211, val_MinusLogProbMetric: 46.1211

Epoch 265: val_loss did not improve from 45.49164
196/196 - 30s - loss: 46.1145 - MinusLogProbMetric: 46.1145 - val_loss: 46.1211 - val_MinusLogProbMetric: 46.1211 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 266/1000
2023-10-27 15:06:34.531 
Epoch 266/1000 
	 loss: 45.9832, MinusLogProbMetric: 45.9832, val_loss: 46.0667, val_MinusLogProbMetric: 46.0667

Epoch 266: val_loss did not improve from 45.49164
196/196 - 30s - loss: 45.9832 - MinusLogProbMetric: 45.9832 - val_loss: 46.0667 - val_MinusLogProbMetric: 46.0667 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 267/1000
2023-10-27 15:07:08.118 
Epoch 267/1000 
	 loss: 45.8121, MinusLogProbMetric: 45.8121, val_loss: 46.3555, val_MinusLogProbMetric: 46.3555

Epoch 267: val_loss did not improve from 45.49164
196/196 - 34s - loss: 45.8121 - MinusLogProbMetric: 45.8121 - val_loss: 46.3555 - val_MinusLogProbMetric: 46.3555 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 268/1000
2023-10-27 15:07:41.994 
Epoch 268/1000 
	 loss: 45.6494, MinusLogProbMetric: 45.6494, val_loss: 45.4594, val_MinusLogProbMetric: 45.4594

Epoch 268: val_loss improved from 45.49164 to 45.45943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 45.6494 - MinusLogProbMetric: 45.6494 - val_loss: 45.4594 - val_MinusLogProbMetric: 45.4594 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 269/1000
2023-10-27 15:08:11.865 
Epoch 269/1000 
	 loss: 45.5408, MinusLogProbMetric: 45.5408, val_loss: 46.2891, val_MinusLogProbMetric: 46.2891

Epoch 269: val_loss did not improve from 45.45943
196/196 - 29s - loss: 45.5408 - MinusLogProbMetric: 45.5408 - val_loss: 46.2891 - val_MinusLogProbMetric: 46.2891 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 270/1000
2023-10-27 15:08:40.704 
Epoch 270/1000 
	 loss: 45.6471, MinusLogProbMetric: 45.6471, val_loss: 47.2930, val_MinusLogProbMetric: 47.2930

Epoch 270: val_loss did not improve from 45.45943
196/196 - 29s - loss: 45.6471 - MinusLogProbMetric: 45.6471 - val_loss: 47.2930 - val_MinusLogProbMetric: 47.2930 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 271/1000
2023-10-27 15:09:08.222 
Epoch 271/1000 
	 loss: 45.4421, MinusLogProbMetric: 45.4421, val_loss: 45.7545, val_MinusLogProbMetric: 45.7545

Epoch 271: val_loss did not improve from 45.45943
196/196 - 28s - loss: 45.4421 - MinusLogProbMetric: 45.4421 - val_loss: 45.7545 - val_MinusLogProbMetric: 45.7545 - lr: 3.3333e-04 - 28s/epoch - 140ms/step
Epoch 272/1000
2023-10-27 15:09:41.509 
Epoch 272/1000 
	 loss: 45.8486, MinusLogProbMetric: 45.8486, val_loss: 45.3776, val_MinusLogProbMetric: 45.3776

Epoch 272: val_loss improved from 45.45943 to 45.37761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 45.8486 - MinusLogProbMetric: 45.8486 - val_loss: 45.3776 - val_MinusLogProbMetric: 45.3776 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 273/1000
2023-10-27 15:10:15.730 
Epoch 273/1000 
	 loss: 45.6001, MinusLogProbMetric: 45.6001, val_loss: 47.0234, val_MinusLogProbMetric: 47.0234

Epoch 273: val_loss did not improve from 45.37761
196/196 - 34s - loss: 45.6001 - MinusLogProbMetric: 45.6001 - val_loss: 47.0234 - val_MinusLogProbMetric: 47.0234 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 274/1000
2023-10-27 15:10:48.523 
Epoch 274/1000 
	 loss: 45.4720, MinusLogProbMetric: 45.4720, val_loss: 45.9402, val_MinusLogProbMetric: 45.9402

Epoch 274: val_loss did not improve from 45.37761
196/196 - 33s - loss: 45.4720 - MinusLogProbMetric: 45.4720 - val_loss: 45.9402 - val_MinusLogProbMetric: 45.9402 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 275/1000
2023-10-27 15:11:18.761 
Epoch 275/1000 
	 loss: 45.4099, MinusLogProbMetric: 45.4099, val_loss: 46.3396, val_MinusLogProbMetric: 46.3396

Epoch 275: val_loss did not improve from 45.37761
196/196 - 30s - loss: 45.4099 - MinusLogProbMetric: 45.4099 - val_loss: 46.3396 - val_MinusLogProbMetric: 46.3396 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 276/1000
2023-10-27 15:11:51.844 
Epoch 276/1000 
	 loss: 45.4996, MinusLogProbMetric: 45.4996, val_loss: 45.9761, val_MinusLogProbMetric: 45.9761

Epoch 276: val_loss did not improve from 45.37761
196/196 - 33s - loss: 45.4996 - MinusLogProbMetric: 45.4996 - val_loss: 45.9761 - val_MinusLogProbMetric: 45.9761 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 277/1000
2023-10-27 15:12:23.499 
Epoch 277/1000 
	 loss: 45.8483, MinusLogProbMetric: 45.8483, val_loss: 45.7152, val_MinusLogProbMetric: 45.7152

Epoch 277: val_loss did not improve from 45.37761
196/196 - 32s - loss: 45.8483 - MinusLogProbMetric: 45.8483 - val_loss: 45.7152 - val_MinusLogProbMetric: 45.7152 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 278/1000
2023-10-27 15:12:57.435 
Epoch 278/1000 
	 loss: 45.4323, MinusLogProbMetric: 45.4323, val_loss: 49.1445, val_MinusLogProbMetric: 49.1445

Epoch 278: val_loss did not improve from 45.37761
196/196 - 34s - loss: 45.4323 - MinusLogProbMetric: 45.4323 - val_loss: 49.1445 - val_MinusLogProbMetric: 49.1445 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 279/1000
2023-10-27 15:13:28.219 
Epoch 279/1000 
	 loss: 45.8734, MinusLogProbMetric: 45.8734, val_loss: 45.4774, val_MinusLogProbMetric: 45.4774

Epoch 279: val_loss did not improve from 45.37761
196/196 - 31s - loss: 45.8734 - MinusLogProbMetric: 45.8734 - val_loss: 45.4774 - val_MinusLogProbMetric: 45.4774 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 280/1000
2023-10-27 15:14:00.221 
Epoch 280/1000 
	 loss: 45.5875, MinusLogProbMetric: 45.5875, val_loss: 49.2525, val_MinusLogProbMetric: 49.2525

Epoch 280: val_loss did not improve from 45.37761
196/196 - 32s - loss: 45.5875 - MinusLogProbMetric: 45.5875 - val_loss: 49.2525 - val_MinusLogProbMetric: 49.2525 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 281/1000
2023-10-27 15:14:30.225 
Epoch 281/1000 
	 loss: 45.6630, MinusLogProbMetric: 45.6630, val_loss: 47.3238, val_MinusLogProbMetric: 47.3238

Epoch 281: val_loss did not improve from 45.37761
196/196 - 30s - loss: 45.6630 - MinusLogProbMetric: 45.6630 - val_loss: 47.3238 - val_MinusLogProbMetric: 47.3238 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 282/1000
2023-10-27 15:15:01.392 
Epoch 282/1000 
	 loss: 45.7060, MinusLogProbMetric: 45.7060, val_loss: 47.7204, val_MinusLogProbMetric: 47.7204

Epoch 282: val_loss did not improve from 45.37761
196/196 - 31s - loss: 45.7060 - MinusLogProbMetric: 45.7060 - val_loss: 47.7204 - val_MinusLogProbMetric: 47.7204 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 283/1000
2023-10-27 15:15:30.519 
Epoch 283/1000 
	 loss: 45.4260, MinusLogProbMetric: 45.4260, val_loss: 46.2159, val_MinusLogProbMetric: 46.2159

Epoch 283: val_loss did not improve from 45.37761
196/196 - 29s - loss: 45.4260 - MinusLogProbMetric: 45.4260 - val_loss: 46.2159 - val_MinusLogProbMetric: 46.2159 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 284/1000
2023-10-27 15:16:04.033 
Epoch 284/1000 
	 loss: 45.4159, MinusLogProbMetric: 45.4159, val_loss: 45.5970, val_MinusLogProbMetric: 45.5970

Epoch 284: val_loss did not improve from 45.37761
196/196 - 34s - loss: 45.4159 - MinusLogProbMetric: 45.4159 - val_loss: 45.5970 - val_MinusLogProbMetric: 45.5970 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 285/1000
2023-10-27 15:16:36.845 
Epoch 285/1000 
	 loss: 45.4063, MinusLogProbMetric: 45.4063, val_loss: 46.6261, val_MinusLogProbMetric: 46.6261

Epoch 285: val_loss did not improve from 45.37761
196/196 - 33s - loss: 45.4063 - MinusLogProbMetric: 45.4063 - val_loss: 46.6261 - val_MinusLogProbMetric: 46.6261 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 286/1000
2023-10-27 15:17:05.773 
Epoch 286/1000 
	 loss: 45.5317, MinusLogProbMetric: 45.5317, val_loss: 46.7422, val_MinusLogProbMetric: 46.7422

Epoch 286: val_loss did not improve from 45.37761
196/196 - 29s - loss: 45.5317 - MinusLogProbMetric: 45.5317 - val_loss: 46.7422 - val_MinusLogProbMetric: 46.7422 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 287/1000
2023-10-27 15:17:36.300 
Epoch 287/1000 
	 loss: 45.3271, MinusLogProbMetric: 45.3271, val_loss: 45.7520, val_MinusLogProbMetric: 45.7520

Epoch 287: val_loss did not improve from 45.37761
196/196 - 31s - loss: 45.3271 - MinusLogProbMetric: 45.3271 - val_loss: 45.7520 - val_MinusLogProbMetric: 45.7520 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 288/1000
2023-10-27 15:18:04.715 
Epoch 288/1000 
	 loss: 45.7626, MinusLogProbMetric: 45.7626, val_loss: 45.4368, val_MinusLogProbMetric: 45.4368

Epoch 288: val_loss did not improve from 45.37761
196/196 - 28s - loss: 45.7626 - MinusLogProbMetric: 45.7626 - val_loss: 45.4368 - val_MinusLogProbMetric: 45.4368 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 289/1000
2023-10-27 15:18:37.242 
Epoch 289/1000 
	 loss: 45.3393, MinusLogProbMetric: 45.3393, val_loss: 45.5939, val_MinusLogProbMetric: 45.5939

Epoch 289: val_loss did not improve from 45.37761
196/196 - 33s - loss: 45.3393 - MinusLogProbMetric: 45.3393 - val_loss: 45.5939 - val_MinusLogProbMetric: 45.5939 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 290/1000
2023-10-27 15:19:11.084 
Epoch 290/1000 
	 loss: 45.5399, MinusLogProbMetric: 45.5399, val_loss: 46.9103, val_MinusLogProbMetric: 46.9103

Epoch 290: val_loss did not improve from 45.37761
196/196 - 34s - loss: 45.5399 - MinusLogProbMetric: 45.5399 - val_loss: 46.9103 - val_MinusLogProbMetric: 46.9103 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 291/1000
2023-10-27 15:19:38.392 
Epoch 291/1000 
	 loss: 45.5576, MinusLogProbMetric: 45.5576, val_loss: 46.2786, val_MinusLogProbMetric: 46.2786

Epoch 291: val_loss did not improve from 45.37761
196/196 - 27s - loss: 45.5576 - MinusLogProbMetric: 45.5576 - val_loss: 46.2786 - val_MinusLogProbMetric: 46.2786 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 292/1000
2023-10-27 15:20:06.005 
Epoch 292/1000 
	 loss: 45.4203, MinusLogProbMetric: 45.4203, val_loss: 45.2173, val_MinusLogProbMetric: 45.2173

Epoch 292: val_loss improved from 45.37761 to 45.21732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 45.4203 - MinusLogProbMetric: 45.4203 - val_loss: 45.2173 - val_MinusLogProbMetric: 45.2173 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 293/1000
2023-10-27 15:20:37.771 
Epoch 293/1000 
	 loss: 45.5533, MinusLogProbMetric: 45.5533, val_loss: 46.2803, val_MinusLogProbMetric: 46.2803

Epoch 293: val_loss did not improve from 45.21732
196/196 - 31s - loss: 45.5533 - MinusLogProbMetric: 45.5533 - val_loss: 46.2803 - val_MinusLogProbMetric: 46.2803 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 294/1000
2023-10-27 15:21:11.431 
Epoch 294/1000 
	 loss: 45.6664, MinusLogProbMetric: 45.6664, val_loss: 45.2505, val_MinusLogProbMetric: 45.2505

Epoch 294: val_loss did not improve from 45.21732
196/196 - 34s - loss: 45.6664 - MinusLogProbMetric: 45.6664 - val_loss: 45.2505 - val_MinusLogProbMetric: 45.2505 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 295/1000
2023-10-27 15:21:43.986 
Epoch 295/1000 
	 loss: 45.2986, MinusLogProbMetric: 45.2986, val_loss: 45.3039, val_MinusLogProbMetric: 45.3039

Epoch 295: val_loss did not improve from 45.21732
196/196 - 33s - loss: 45.2986 - MinusLogProbMetric: 45.2986 - val_loss: 45.3039 - val_MinusLogProbMetric: 45.3039 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 296/1000
2023-10-27 15:22:10.994 
Epoch 296/1000 
	 loss: 45.5463, MinusLogProbMetric: 45.5463, val_loss: 47.1169, val_MinusLogProbMetric: 47.1169

Epoch 296: val_loss did not improve from 45.21732
196/196 - 27s - loss: 45.5463 - MinusLogProbMetric: 45.5463 - val_loss: 47.1169 - val_MinusLogProbMetric: 47.1169 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 297/1000
2023-10-27 15:22:43.343 
Epoch 297/1000 
	 loss: 45.2688, MinusLogProbMetric: 45.2688, val_loss: 45.5272, val_MinusLogProbMetric: 45.5272

Epoch 297: val_loss did not improve from 45.21732
196/196 - 32s - loss: 45.2688 - MinusLogProbMetric: 45.2688 - val_loss: 45.5272 - val_MinusLogProbMetric: 45.5272 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 298/1000
2023-10-27 15:23:11.663 
Epoch 298/1000 
	 loss: 45.4159, MinusLogProbMetric: 45.4159, val_loss: 44.9512, val_MinusLogProbMetric: 44.9512

Epoch 298: val_loss improved from 45.21732 to 44.95123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 29s - loss: 45.4159 - MinusLogProbMetric: 45.4159 - val_loss: 44.9512 - val_MinusLogProbMetric: 44.9512 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 299/1000
2023-10-27 15:23:42.208 
Epoch 299/1000 
	 loss: 45.3871, MinusLogProbMetric: 45.3871, val_loss: 45.8089, val_MinusLogProbMetric: 45.8089

Epoch 299: val_loss did not improve from 44.95123
196/196 - 30s - loss: 45.3871 - MinusLogProbMetric: 45.3871 - val_loss: 45.8089 - val_MinusLogProbMetric: 45.8089 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 300/1000
2023-10-27 15:24:14.966 
Epoch 300/1000 
	 loss: 45.2888, MinusLogProbMetric: 45.2888, val_loss: 46.7711, val_MinusLogProbMetric: 46.7711

Epoch 300: val_loss did not improve from 44.95123
196/196 - 33s - loss: 45.2888 - MinusLogProbMetric: 45.2888 - val_loss: 46.7711 - val_MinusLogProbMetric: 46.7711 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 301/1000
2023-10-27 15:24:45.365 
Epoch 301/1000 
	 loss: 45.2301, MinusLogProbMetric: 45.2301, val_loss: 46.4957, val_MinusLogProbMetric: 46.4957

Epoch 301: val_loss did not improve from 44.95123
196/196 - 30s - loss: 45.2301 - MinusLogProbMetric: 45.2301 - val_loss: 46.4957 - val_MinusLogProbMetric: 46.4957 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 302/1000
2023-10-27 15:25:12.193 
Epoch 302/1000 
	 loss: 45.4395, MinusLogProbMetric: 45.4395, val_loss: 45.5236, val_MinusLogProbMetric: 45.5236

Epoch 302: val_loss did not improve from 44.95123
196/196 - 27s - loss: 45.4395 - MinusLogProbMetric: 45.4395 - val_loss: 45.5236 - val_MinusLogProbMetric: 45.5236 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 303/1000
2023-10-27 15:25:40.661 
Epoch 303/1000 
	 loss: 45.3928, MinusLogProbMetric: 45.3928, val_loss: 45.6525, val_MinusLogProbMetric: 45.6525

Epoch 303: val_loss did not improve from 44.95123
196/196 - 28s - loss: 45.3928 - MinusLogProbMetric: 45.3928 - val_loss: 45.6525 - val_MinusLogProbMetric: 45.6525 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 304/1000
2023-10-27 15:26:12.854 
Epoch 304/1000 
	 loss: 45.3019, MinusLogProbMetric: 45.3019, val_loss: 45.7752, val_MinusLogProbMetric: 45.7752

Epoch 304: val_loss did not improve from 44.95123
196/196 - 32s - loss: 45.3019 - MinusLogProbMetric: 45.3019 - val_loss: 45.7752 - val_MinusLogProbMetric: 45.7752 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 305/1000
2023-10-27 15:26:46.670 
Epoch 305/1000 
	 loss: 45.2027, MinusLogProbMetric: 45.2027, val_loss: 45.9617, val_MinusLogProbMetric: 45.9617

Epoch 305: val_loss did not improve from 44.95123
196/196 - 34s - loss: 45.2027 - MinusLogProbMetric: 45.2027 - val_loss: 45.9617 - val_MinusLogProbMetric: 45.9617 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 306/1000
2023-10-27 15:27:19.364 
Epoch 306/1000 
	 loss: 45.3623, MinusLogProbMetric: 45.3623, val_loss: 45.6061, val_MinusLogProbMetric: 45.6061

Epoch 306: val_loss did not improve from 44.95123
196/196 - 33s - loss: 45.3623 - MinusLogProbMetric: 45.3623 - val_loss: 45.6061 - val_MinusLogProbMetric: 45.6061 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 307/1000
2023-10-27 15:27:49.332 
Epoch 307/1000 
	 loss: 45.2124, MinusLogProbMetric: 45.2124, val_loss: 46.1188, val_MinusLogProbMetric: 46.1188

Epoch 307: val_loss did not improve from 44.95123
196/196 - 30s - loss: 45.2124 - MinusLogProbMetric: 45.2124 - val_loss: 46.1188 - val_MinusLogProbMetric: 46.1188 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 308/1000
2023-10-27 15:28:17.301 
Epoch 308/1000 
	 loss: 45.3391, MinusLogProbMetric: 45.3391, val_loss: 47.1446, val_MinusLogProbMetric: 47.1446

Epoch 308: val_loss did not improve from 44.95123
196/196 - 28s - loss: 45.3391 - MinusLogProbMetric: 45.3391 - val_loss: 47.1446 - val_MinusLogProbMetric: 47.1446 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 309/1000
2023-10-27 15:28:46.342 
Epoch 309/1000 
	 loss: 45.4605, MinusLogProbMetric: 45.4605, val_loss: 45.2062, val_MinusLogProbMetric: 45.2062

Epoch 309: val_loss did not improve from 44.95123
196/196 - 29s - loss: 45.4605 - MinusLogProbMetric: 45.4605 - val_loss: 45.2062 - val_MinusLogProbMetric: 45.2062 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 310/1000
2023-10-27 15:29:20.251 
Epoch 310/1000 
	 loss: 45.3223, MinusLogProbMetric: 45.3223, val_loss: 45.4723, val_MinusLogProbMetric: 45.4723

Epoch 310: val_loss did not improve from 44.95123
196/196 - 34s - loss: 45.3223 - MinusLogProbMetric: 45.3223 - val_loss: 45.4723 - val_MinusLogProbMetric: 45.4723 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 311/1000
2023-10-27 15:29:52.703 
Epoch 311/1000 
	 loss: 44.8641, MinusLogProbMetric: 44.8641, val_loss: 44.7720, val_MinusLogProbMetric: 44.7720

Epoch 311: val_loss improved from 44.95123 to 44.77202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 33s - loss: 44.8641 - MinusLogProbMetric: 44.8641 - val_loss: 44.7720 - val_MinusLogProbMetric: 44.7720 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 312/1000
2023-10-27 15:30:21.678 
Epoch 312/1000 
	 loss: 45.3606, MinusLogProbMetric: 45.3606, val_loss: 44.8707, val_MinusLogProbMetric: 44.8707

Epoch 312: val_loss did not improve from 44.77202
196/196 - 28s - loss: 45.3606 - MinusLogProbMetric: 45.3606 - val_loss: 44.8707 - val_MinusLogProbMetric: 44.8707 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 313/1000
2023-10-27 15:30:49.716 
Epoch 313/1000 
	 loss: 45.2801, MinusLogProbMetric: 45.2801, val_loss: 46.2298, val_MinusLogProbMetric: 46.2298

Epoch 313: val_loss did not improve from 44.77202
196/196 - 28s - loss: 45.2801 - MinusLogProbMetric: 45.2801 - val_loss: 46.2298 - val_MinusLogProbMetric: 46.2298 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 314/1000
2023-10-27 15:31:22.408 
Epoch 314/1000 
	 loss: 45.2335, MinusLogProbMetric: 45.2335, val_loss: 49.2700, val_MinusLogProbMetric: 49.2700

Epoch 314: val_loss did not improve from 44.77202
196/196 - 33s - loss: 45.2335 - MinusLogProbMetric: 45.2335 - val_loss: 49.2700 - val_MinusLogProbMetric: 49.2700 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 315/1000
2023-10-27 15:31:52.646 
Epoch 315/1000 
	 loss: 45.0163, MinusLogProbMetric: 45.0163, val_loss: 47.4459, val_MinusLogProbMetric: 47.4459

Epoch 315: val_loss did not improve from 44.77202
196/196 - 30s - loss: 45.0163 - MinusLogProbMetric: 45.0163 - val_loss: 47.4459 - val_MinusLogProbMetric: 47.4459 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 316/1000
2023-10-27 15:32:26.003 
Epoch 316/1000 
	 loss: 45.4345, MinusLogProbMetric: 45.4345, val_loss: 45.4258, val_MinusLogProbMetric: 45.4258

Epoch 316: val_loss did not improve from 44.77202
196/196 - 33s - loss: 45.4345 - MinusLogProbMetric: 45.4345 - val_loss: 45.4258 - val_MinusLogProbMetric: 45.4258 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 317/1000
2023-10-27 15:32:57.849 
Epoch 317/1000 
	 loss: 45.2730, MinusLogProbMetric: 45.2730, val_loss: 44.7750, val_MinusLogProbMetric: 44.7750

Epoch 317: val_loss did not improve from 44.77202
196/196 - 32s - loss: 45.2730 - MinusLogProbMetric: 45.2730 - val_loss: 44.7750 - val_MinusLogProbMetric: 44.7750 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 318/1000
2023-10-27 15:33:26.133 
Epoch 318/1000 
	 loss: 45.0469, MinusLogProbMetric: 45.0469, val_loss: 45.5963, val_MinusLogProbMetric: 45.5963

Epoch 318: val_loss did not improve from 44.77202
196/196 - 28s - loss: 45.0469 - MinusLogProbMetric: 45.0469 - val_loss: 45.5963 - val_MinusLogProbMetric: 45.5963 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 319/1000
2023-10-27 15:33:55.739 
Epoch 319/1000 
	 loss: 45.0590, MinusLogProbMetric: 45.0590, val_loss: 46.9405, val_MinusLogProbMetric: 46.9405

Epoch 319: val_loss did not improve from 44.77202
196/196 - 30s - loss: 45.0590 - MinusLogProbMetric: 45.0590 - val_loss: 46.9405 - val_MinusLogProbMetric: 46.9405 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 320/1000
2023-10-27 15:34:27.953 
Epoch 320/1000 
	 loss: 45.1444, MinusLogProbMetric: 45.1444, val_loss: 46.1185, val_MinusLogProbMetric: 46.1185

Epoch 320: val_loss did not improve from 44.77202
196/196 - 32s - loss: 45.1444 - MinusLogProbMetric: 45.1444 - val_loss: 46.1185 - val_MinusLogProbMetric: 46.1185 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 321/1000
2023-10-27 15:34:59.731 
Epoch 321/1000 
	 loss: 45.0994, MinusLogProbMetric: 45.0994, val_loss: 46.3229, val_MinusLogProbMetric: 46.3229

Epoch 321: val_loss did not improve from 44.77202
196/196 - 32s - loss: 45.0994 - MinusLogProbMetric: 45.0994 - val_loss: 46.3229 - val_MinusLogProbMetric: 46.3229 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 322/1000
2023-10-27 15:35:33.927 
Epoch 322/1000 
	 loss: 45.0867, MinusLogProbMetric: 45.0867, val_loss: 46.4933, val_MinusLogProbMetric: 46.4933

Epoch 322: val_loss did not improve from 44.77202
196/196 - 34s - loss: 45.0867 - MinusLogProbMetric: 45.0867 - val_loss: 46.4933 - val_MinusLogProbMetric: 46.4933 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 323/1000
2023-10-27 15:36:06.402 
Epoch 323/1000 
	 loss: 45.1379, MinusLogProbMetric: 45.1379, val_loss: 45.5835, val_MinusLogProbMetric: 45.5835

Epoch 323: val_loss did not improve from 44.77202
196/196 - 32s - loss: 45.1379 - MinusLogProbMetric: 45.1379 - val_loss: 45.5835 - val_MinusLogProbMetric: 45.5835 - lr: 3.3333e-04 - 32s/epoch - 166ms/step
Epoch 324/1000
2023-10-27 15:36:35.586 
Epoch 324/1000 
	 loss: 44.8415, MinusLogProbMetric: 44.8415, val_loss: 44.8122, val_MinusLogProbMetric: 44.8122

Epoch 324: val_loss did not improve from 44.77202
196/196 - 29s - loss: 44.8415 - MinusLogProbMetric: 44.8415 - val_loss: 44.8122 - val_MinusLogProbMetric: 44.8122 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 325/1000
2023-10-27 15:37:05.690 
Epoch 325/1000 
	 loss: 45.0150, MinusLogProbMetric: 45.0150, val_loss: 45.4479, val_MinusLogProbMetric: 45.4479

Epoch 325: val_loss did not improve from 44.77202
196/196 - 30s - loss: 45.0150 - MinusLogProbMetric: 45.0150 - val_loss: 45.4479 - val_MinusLogProbMetric: 45.4479 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 326/1000
2023-10-27 15:37:34.486 
Epoch 326/1000 
	 loss: 45.0138, MinusLogProbMetric: 45.0138, val_loss: 46.9116, val_MinusLogProbMetric: 46.9116

Epoch 326: val_loss did not improve from 44.77202
196/196 - 29s - loss: 45.0138 - MinusLogProbMetric: 45.0138 - val_loss: 46.9116 - val_MinusLogProbMetric: 46.9116 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 327/1000
2023-10-27 15:38:07.238 
Epoch 327/1000 
	 loss: 44.8382, MinusLogProbMetric: 44.8382, val_loss: 45.6396, val_MinusLogProbMetric: 45.6396

Epoch 327: val_loss did not improve from 44.77202
196/196 - 33s - loss: 44.8382 - MinusLogProbMetric: 44.8382 - val_loss: 45.6396 - val_MinusLogProbMetric: 45.6396 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 328/1000
2023-10-27 15:38:41.061 
Epoch 328/1000 
	 loss: 44.8910, MinusLogProbMetric: 44.8910, val_loss: 45.3891, val_MinusLogProbMetric: 45.3891

Epoch 328: val_loss did not improve from 44.77202
196/196 - 34s - loss: 44.8910 - MinusLogProbMetric: 44.8910 - val_loss: 45.3891 - val_MinusLogProbMetric: 45.3891 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 329/1000
2023-10-27 15:39:15.427 
Epoch 329/1000 
	 loss: 44.9407, MinusLogProbMetric: 44.9407, val_loss: 48.0637, val_MinusLogProbMetric: 48.0637

Epoch 329: val_loss did not improve from 44.77202
196/196 - 34s - loss: 44.9407 - MinusLogProbMetric: 44.9407 - val_loss: 48.0637 - val_MinusLogProbMetric: 48.0637 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 330/1000
2023-10-27 15:39:45.027 
Epoch 330/1000 
	 loss: 45.2480, MinusLogProbMetric: 45.2480, val_loss: 44.7991, val_MinusLogProbMetric: 44.7991

Epoch 330: val_loss did not improve from 44.77202
196/196 - 30s - loss: 45.2480 - MinusLogProbMetric: 45.2480 - val_loss: 44.7991 - val_MinusLogProbMetric: 44.7991 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 331/1000
2023-10-27 15:40:15.478 
Epoch 331/1000 
	 loss: 45.1206, MinusLogProbMetric: 45.1206, val_loss: 48.2831, val_MinusLogProbMetric: 48.2831

Epoch 331: val_loss did not improve from 44.77202
196/196 - 30s - loss: 45.1206 - MinusLogProbMetric: 45.1206 - val_loss: 48.2831 - val_MinusLogProbMetric: 48.2831 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 332/1000
2023-10-27 15:40:45.574 
Epoch 332/1000 
	 loss: 45.2247, MinusLogProbMetric: 45.2247, val_loss: 46.5599, val_MinusLogProbMetric: 46.5599

Epoch 332: val_loss did not improve from 44.77202
196/196 - 30s - loss: 45.2247 - MinusLogProbMetric: 45.2247 - val_loss: 46.5599 - val_MinusLogProbMetric: 46.5599 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 333/1000
2023-10-27 15:41:18.626 
Epoch 333/1000 
	 loss: 44.7805, MinusLogProbMetric: 44.7805, val_loss: 44.9496, val_MinusLogProbMetric: 44.9496

Epoch 333: val_loss did not improve from 44.77202
196/196 - 33s - loss: 44.7805 - MinusLogProbMetric: 44.7805 - val_loss: 44.9496 - val_MinusLogProbMetric: 44.9496 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 334/1000
2023-10-27 15:41:52.007 
Epoch 334/1000 
	 loss: 45.5169, MinusLogProbMetric: 45.5169, val_loss: 45.4079, val_MinusLogProbMetric: 45.4079

Epoch 334: val_loss did not improve from 44.77202
196/196 - 33s - loss: 45.5169 - MinusLogProbMetric: 45.5169 - val_loss: 45.4079 - val_MinusLogProbMetric: 45.4079 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 335/1000
2023-10-27 15:42:20.902 
Epoch 335/1000 
	 loss: 44.8766, MinusLogProbMetric: 44.8766, val_loss: 46.3736, val_MinusLogProbMetric: 46.3736

Epoch 335: val_loss did not improve from 44.77202
196/196 - 29s - loss: 44.8766 - MinusLogProbMetric: 44.8766 - val_loss: 46.3736 - val_MinusLogProbMetric: 46.3736 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 336/1000
2023-10-27 15:42:50.412 
Epoch 336/1000 
	 loss: 45.1676, MinusLogProbMetric: 45.1676, val_loss: 47.8471, val_MinusLogProbMetric: 47.8471

Epoch 336: val_loss did not improve from 44.77202
196/196 - 30s - loss: 45.1676 - MinusLogProbMetric: 45.1676 - val_loss: 47.8471 - val_MinusLogProbMetric: 47.8471 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 337/1000
2023-10-27 15:43:23.096 
Epoch 337/1000 
	 loss: 44.9264, MinusLogProbMetric: 44.9264, val_loss: 45.0181, val_MinusLogProbMetric: 45.0181

Epoch 337: val_loss did not improve from 44.77202
196/196 - 33s - loss: 44.9264 - MinusLogProbMetric: 44.9264 - val_loss: 45.0181 - val_MinusLogProbMetric: 45.0181 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 338/1000
2023-10-27 15:43:55.919 
Epoch 338/1000 
	 loss: 44.9942, MinusLogProbMetric: 44.9942, val_loss: 46.7134, val_MinusLogProbMetric: 46.7134

Epoch 338: val_loss did not improve from 44.77202
196/196 - 33s - loss: 44.9942 - MinusLogProbMetric: 44.9942 - val_loss: 46.7134 - val_MinusLogProbMetric: 46.7134 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 339/1000
2023-10-27 15:44:27.655 
Epoch 339/1000 
	 loss: 45.0231, MinusLogProbMetric: 45.0231, val_loss: 45.1233, val_MinusLogProbMetric: 45.1233

Epoch 339: val_loss did not improve from 44.77202
196/196 - 32s - loss: 45.0231 - MinusLogProbMetric: 45.0231 - val_loss: 45.1233 - val_MinusLogProbMetric: 45.1233 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 340/1000
2023-10-27 15:45:01.401 
Epoch 340/1000 
	 loss: 44.8985, MinusLogProbMetric: 44.8985, val_loss: 48.0325, val_MinusLogProbMetric: 48.0325

Epoch 340: val_loss did not improve from 44.77202
196/196 - 34s - loss: 44.8985 - MinusLogProbMetric: 44.8985 - val_loss: 48.0325 - val_MinusLogProbMetric: 48.0325 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 341/1000
2023-10-27 15:45:35.143 
Epoch 341/1000 
	 loss: 44.9191, MinusLogProbMetric: 44.9191, val_loss: 47.8876, val_MinusLogProbMetric: 47.8876

Epoch 341: val_loss did not improve from 44.77202
196/196 - 34s - loss: 44.9191 - MinusLogProbMetric: 44.9191 - val_loss: 47.8876 - val_MinusLogProbMetric: 47.8876 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 342/1000
2023-10-27 15:46:07.476 
Epoch 342/1000 
	 loss: 45.0352, MinusLogProbMetric: 45.0352, val_loss: 46.0396, val_MinusLogProbMetric: 46.0396

Epoch 342: val_loss did not improve from 44.77202
196/196 - 32s - loss: 45.0352 - MinusLogProbMetric: 45.0352 - val_loss: 46.0396 - val_MinusLogProbMetric: 46.0396 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 343/1000
2023-10-27 15:46:38.079 
Epoch 343/1000 
	 loss: 44.7286, MinusLogProbMetric: 44.7286, val_loss: 45.1692, val_MinusLogProbMetric: 45.1692

Epoch 343: val_loss did not improve from 44.77202
196/196 - 31s - loss: 44.7286 - MinusLogProbMetric: 44.7286 - val_loss: 45.1692 - val_MinusLogProbMetric: 45.1692 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 344/1000
2023-10-27 15:47:08.405 
Epoch 344/1000 
	 loss: 44.9337, MinusLogProbMetric: 44.9337, val_loss: 45.8979, val_MinusLogProbMetric: 45.8979

Epoch 344: val_loss did not improve from 44.77202
196/196 - 30s - loss: 44.9337 - MinusLogProbMetric: 44.9337 - val_loss: 45.8979 - val_MinusLogProbMetric: 45.8979 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 345/1000
2023-10-27 15:47:42.687 
Epoch 345/1000 
	 loss: 44.7487, MinusLogProbMetric: 44.7487, val_loss: 46.4322, val_MinusLogProbMetric: 46.4322

Epoch 345: val_loss did not improve from 44.77202
196/196 - 34s - loss: 44.7487 - MinusLogProbMetric: 44.7487 - val_loss: 46.4322 - val_MinusLogProbMetric: 46.4322 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 346/1000
2023-10-27 15:48:15.971 
Epoch 346/1000 
	 loss: 44.7062, MinusLogProbMetric: 44.7062, val_loss: 45.1673, val_MinusLogProbMetric: 45.1673

Epoch 346: val_loss did not improve from 44.77202
196/196 - 33s - loss: 44.7062 - MinusLogProbMetric: 44.7062 - val_loss: 45.1673 - val_MinusLogProbMetric: 45.1673 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 347/1000
2023-10-27 15:48:48.588 
Epoch 347/1000 
	 loss: 44.9780, MinusLogProbMetric: 44.9780, val_loss: 44.6361, val_MinusLogProbMetric: 44.6361

Epoch 347: val_loss improved from 44.77202 to 44.63614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 33s - loss: 44.9780 - MinusLogProbMetric: 44.9780 - val_loss: 44.6361 - val_MinusLogProbMetric: 44.6361 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 348/1000
2023-10-27 15:49:22.179 
Epoch 348/1000 
	 loss: 44.9591, MinusLogProbMetric: 44.9591, val_loss: 44.4112, val_MinusLogProbMetric: 44.4112

Epoch 348: val_loss improved from 44.63614 to 44.41123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 44.9591 - MinusLogProbMetric: 44.9591 - val_loss: 44.4112 - val_MinusLogProbMetric: 44.4112 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 349/1000
2023-10-27 15:49:56.303 
Epoch 349/1000 
	 loss: 44.8610, MinusLogProbMetric: 44.8610, val_loss: 44.9538, val_MinusLogProbMetric: 44.9538

Epoch 349: val_loss did not improve from 44.41123
196/196 - 34s - loss: 44.8610 - MinusLogProbMetric: 44.8610 - val_loss: 44.9538 - val_MinusLogProbMetric: 44.9538 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 350/1000
2023-10-27 15:50:25.036 
Epoch 350/1000 
	 loss: 44.6435, MinusLogProbMetric: 44.6435, val_loss: 46.3987, val_MinusLogProbMetric: 46.3987

Epoch 350: val_loss did not improve from 44.41123
196/196 - 29s - loss: 44.6435 - MinusLogProbMetric: 44.6435 - val_loss: 46.3987 - val_MinusLogProbMetric: 46.3987 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 351/1000
2023-10-27 15:50:59.272 
Epoch 351/1000 
	 loss: 44.8394, MinusLogProbMetric: 44.8394, val_loss: 46.1611, val_MinusLogProbMetric: 46.1611

Epoch 351: val_loss did not improve from 44.41123
196/196 - 34s - loss: 44.8394 - MinusLogProbMetric: 44.8394 - val_loss: 46.1611 - val_MinusLogProbMetric: 46.1611 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 352/1000
2023-10-27 15:51:32.824 
Epoch 352/1000 
	 loss: 45.0321, MinusLogProbMetric: 45.0321, val_loss: 44.6064, val_MinusLogProbMetric: 44.6064

Epoch 352: val_loss did not improve from 44.41123
196/196 - 34s - loss: 45.0321 - MinusLogProbMetric: 45.0321 - val_loss: 44.6064 - val_MinusLogProbMetric: 44.6064 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 353/1000
2023-10-27 15:52:04.134 
Epoch 353/1000 
	 loss: 44.7083, MinusLogProbMetric: 44.7083, val_loss: 44.5111, val_MinusLogProbMetric: 44.5111

Epoch 353: val_loss did not improve from 44.41123
196/196 - 31s - loss: 44.7083 - MinusLogProbMetric: 44.7083 - val_loss: 44.5111 - val_MinusLogProbMetric: 44.5111 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 354/1000
2023-10-27 15:52:33.679 
Epoch 354/1000 
	 loss: 44.6255, MinusLogProbMetric: 44.6255, val_loss: 44.9480, val_MinusLogProbMetric: 44.9480

Epoch 354: val_loss did not improve from 44.41123
196/196 - 30s - loss: 44.6255 - MinusLogProbMetric: 44.6255 - val_loss: 44.9480 - val_MinusLogProbMetric: 44.9480 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 355/1000
2023-10-27 15:53:00.932 
Epoch 355/1000 
	 loss: 44.9567, MinusLogProbMetric: 44.9567, val_loss: 46.2486, val_MinusLogProbMetric: 46.2486

Epoch 355: val_loss did not improve from 44.41123
196/196 - 27s - loss: 44.9567 - MinusLogProbMetric: 44.9567 - val_loss: 46.2486 - val_MinusLogProbMetric: 46.2486 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 356/1000
2023-10-27 15:53:31.848 
Epoch 356/1000 
	 loss: 44.8140, MinusLogProbMetric: 44.8140, val_loss: 45.8175, val_MinusLogProbMetric: 45.8175

Epoch 356: val_loss did not improve from 44.41123
196/196 - 31s - loss: 44.8140 - MinusLogProbMetric: 44.8140 - val_loss: 45.8175 - val_MinusLogProbMetric: 45.8175 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 357/1000
2023-10-27 15:54:05.661 
Epoch 357/1000 
	 loss: 45.1367, MinusLogProbMetric: 45.1367, val_loss: 45.1591, val_MinusLogProbMetric: 45.1591

Epoch 357: val_loss did not improve from 44.41123
196/196 - 34s - loss: 45.1367 - MinusLogProbMetric: 45.1367 - val_loss: 45.1591 - val_MinusLogProbMetric: 45.1591 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 358/1000
2023-10-27 15:54:33.477 
Epoch 358/1000 
	 loss: 44.5274, MinusLogProbMetric: 44.5274, val_loss: 45.4138, val_MinusLogProbMetric: 45.4138

Epoch 358: val_loss did not improve from 44.41123
196/196 - 28s - loss: 44.5274 - MinusLogProbMetric: 44.5274 - val_loss: 45.4138 - val_MinusLogProbMetric: 45.4138 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 359/1000
2023-10-27 15:55:04.238 
Epoch 359/1000 
	 loss: 44.9474, MinusLogProbMetric: 44.9474, val_loss: 46.9921, val_MinusLogProbMetric: 46.9921

Epoch 359: val_loss did not improve from 44.41123
196/196 - 31s - loss: 44.9474 - MinusLogProbMetric: 44.9474 - val_loss: 46.9921 - val_MinusLogProbMetric: 46.9921 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 360/1000
2023-10-27 15:55:32.154 
Epoch 360/1000 
	 loss: 44.9203, MinusLogProbMetric: 44.9203, val_loss: 45.3546, val_MinusLogProbMetric: 45.3546

Epoch 360: val_loss did not improve from 44.41123
196/196 - 28s - loss: 44.9203 - MinusLogProbMetric: 44.9203 - val_loss: 45.3546 - val_MinusLogProbMetric: 45.3546 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 361/1000
2023-10-27 15:55:59.425 
Epoch 361/1000 
	 loss: 44.7505, MinusLogProbMetric: 44.7505, val_loss: 45.1678, val_MinusLogProbMetric: 45.1678

Epoch 361: val_loss did not improve from 44.41123
196/196 - 27s - loss: 44.7505 - MinusLogProbMetric: 44.7505 - val_loss: 45.1678 - val_MinusLogProbMetric: 45.1678 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 362/1000
2023-10-27 15:56:30.677 
Epoch 362/1000 
	 loss: 44.5265, MinusLogProbMetric: 44.5265, val_loss: 45.0049, val_MinusLogProbMetric: 45.0049

Epoch 362: val_loss did not improve from 44.41123
196/196 - 31s - loss: 44.5265 - MinusLogProbMetric: 44.5265 - val_loss: 45.0049 - val_MinusLogProbMetric: 45.0049 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 363/1000
2023-10-27 15:57:02.822 
Epoch 363/1000 
	 loss: 44.7750, MinusLogProbMetric: 44.7750, val_loss: 46.2041, val_MinusLogProbMetric: 46.2041

Epoch 363: val_loss did not improve from 44.41123
196/196 - 32s - loss: 44.7750 - MinusLogProbMetric: 44.7750 - val_loss: 46.2041 - val_MinusLogProbMetric: 46.2041 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 364/1000
2023-10-27 15:57:30.508 
Epoch 364/1000 
	 loss: 44.6448, MinusLogProbMetric: 44.6448, val_loss: 45.7446, val_MinusLogProbMetric: 45.7446

Epoch 364: val_loss did not improve from 44.41123
196/196 - 28s - loss: 44.6448 - MinusLogProbMetric: 44.6448 - val_loss: 45.7446 - val_MinusLogProbMetric: 45.7446 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 365/1000
2023-10-27 15:57:58.488 
Epoch 365/1000 
	 loss: 45.0246, MinusLogProbMetric: 45.0246, val_loss: 44.8305, val_MinusLogProbMetric: 44.8305

Epoch 365: val_loss did not improve from 44.41123
196/196 - 28s - loss: 45.0246 - MinusLogProbMetric: 45.0246 - val_loss: 44.8305 - val_MinusLogProbMetric: 44.8305 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 366/1000
2023-10-27 15:58:25.656 
Epoch 366/1000 
	 loss: 44.5422, MinusLogProbMetric: 44.5422, val_loss: 45.1236, val_MinusLogProbMetric: 45.1236

Epoch 366: val_loss did not improve from 44.41123
196/196 - 27s - loss: 44.5422 - MinusLogProbMetric: 44.5422 - val_loss: 45.1236 - val_MinusLogProbMetric: 45.1236 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 367/1000
2023-10-27 15:58:56.915 
Epoch 367/1000 
	 loss: 44.4576, MinusLogProbMetric: 44.4576, val_loss: 44.4119, val_MinusLogProbMetric: 44.4119

Epoch 367: val_loss did not improve from 44.41123
196/196 - 31s - loss: 44.4576 - MinusLogProbMetric: 44.4576 - val_loss: 44.4119 - val_MinusLogProbMetric: 44.4119 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 368/1000
2023-10-27 15:59:28.795 
Epoch 368/1000 
	 loss: 44.5761, MinusLogProbMetric: 44.5761, val_loss: 44.0590, val_MinusLogProbMetric: 44.0590

Epoch 368: val_loss improved from 44.41123 to 44.05896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 32s - loss: 44.5761 - MinusLogProbMetric: 44.5761 - val_loss: 44.0590 - val_MinusLogProbMetric: 44.0590 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 369/1000
2023-10-27 15:59:57.566 
Epoch 369/1000 
	 loss: 44.6165, MinusLogProbMetric: 44.6165, val_loss: 45.6712, val_MinusLogProbMetric: 45.6712

Epoch 369: val_loss did not improve from 44.05896
196/196 - 28s - loss: 44.6165 - MinusLogProbMetric: 44.6165 - val_loss: 45.6712 - val_MinusLogProbMetric: 45.6712 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 370/1000
2023-10-27 16:00:25.441 
Epoch 370/1000 
	 loss: 44.5227, MinusLogProbMetric: 44.5227, val_loss: 44.6702, val_MinusLogProbMetric: 44.6702

Epoch 370: val_loss did not improve from 44.05896
196/196 - 28s - loss: 44.5227 - MinusLogProbMetric: 44.5227 - val_loss: 44.6702 - val_MinusLogProbMetric: 44.6702 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 371/1000
2023-10-27 16:00:53.581 
Epoch 371/1000 
	 loss: 44.4124, MinusLogProbMetric: 44.4124, val_loss: 44.5503, val_MinusLogProbMetric: 44.5503

Epoch 371: val_loss did not improve from 44.05896
196/196 - 28s - loss: 44.4124 - MinusLogProbMetric: 44.4124 - val_loss: 44.5503 - val_MinusLogProbMetric: 44.5503 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 372/1000
2023-10-27 16:01:20.639 
Epoch 372/1000 
	 loss: 44.5050, MinusLogProbMetric: 44.5050, val_loss: 44.4446, val_MinusLogProbMetric: 44.4446

Epoch 372: val_loss did not improve from 44.05896
196/196 - 27s - loss: 44.5050 - MinusLogProbMetric: 44.5050 - val_loss: 44.4446 - val_MinusLogProbMetric: 44.4446 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 373/1000
2023-10-27 16:01:48.185 
Epoch 373/1000 
	 loss: 44.7925, MinusLogProbMetric: 44.7925, val_loss: 44.8655, val_MinusLogProbMetric: 44.8655

Epoch 373: val_loss did not improve from 44.05896
196/196 - 28s - loss: 44.7925 - MinusLogProbMetric: 44.7925 - val_loss: 44.8655 - val_MinusLogProbMetric: 44.8655 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 374/1000
2023-10-27 16:02:16.837 
Epoch 374/1000 
	 loss: 44.5891, MinusLogProbMetric: 44.5891, val_loss: 47.0118, val_MinusLogProbMetric: 47.0118

Epoch 374: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.5891 - MinusLogProbMetric: 44.5891 - val_loss: 47.0118 - val_MinusLogProbMetric: 47.0118 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 375/1000
2023-10-27 16:02:46.895 
Epoch 375/1000 
	 loss: 44.5256, MinusLogProbMetric: 44.5256, val_loss: 44.5398, val_MinusLogProbMetric: 44.5398

Epoch 375: val_loss did not improve from 44.05896
196/196 - 30s - loss: 44.5256 - MinusLogProbMetric: 44.5256 - val_loss: 44.5398 - val_MinusLogProbMetric: 44.5398 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 376/1000
2023-10-27 16:03:17.159 
Epoch 376/1000 
	 loss: 44.8205, MinusLogProbMetric: 44.8205, val_loss: 45.1275, val_MinusLogProbMetric: 45.1275

Epoch 376: val_loss did not improve from 44.05896
196/196 - 30s - loss: 44.8205 - MinusLogProbMetric: 44.8205 - val_loss: 45.1275 - val_MinusLogProbMetric: 45.1275 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 377/1000
2023-10-27 16:03:46.044 
Epoch 377/1000 
	 loss: 44.9030, MinusLogProbMetric: 44.9030, val_loss: 44.5996, val_MinusLogProbMetric: 44.5996

Epoch 377: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.9030 - MinusLogProbMetric: 44.9030 - val_loss: 44.5996 - val_MinusLogProbMetric: 44.5996 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 378/1000
2023-10-27 16:04:13.330 
Epoch 378/1000 
	 loss: 44.4720, MinusLogProbMetric: 44.4720, val_loss: 45.1507, val_MinusLogProbMetric: 45.1507

Epoch 378: val_loss did not improve from 44.05896
196/196 - 27s - loss: 44.4720 - MinusLogProbMetric: 44.4720 - val_loss: 45.1507 - val_MinusLogProbMetric: 45.1507 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 379/1000
2023-10-27 16:04:40.725 
Epoch 379/1000 
	 loss: 44.3681, MinusLogProbMetric: 44.3681, val_loss: 44.4913, val_MinusLogProbMetric: 44.4913

Epoch 379: val_loss did not improve from 44.05896
196/196 - 27s - loss: 44.3681 - MinusLogProbMetric: 44.3681 - val_loss: 44.4913 - val_MinusLogProbMetric: 44.4913 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 380/1000
2023-10-27 16:05:09.424 
Epoch 380/1000 
	 loss: 44.4936, MinusLogProbMetric: 44.4936, val_loss: 45.8042, val_MinusLogProbMetric: 45.8042

Epoch 380: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.4936 - MinusLogProbMetric: 44.4936 - val_loss: 45.8042 - val_MinusLogProbMetric: 45.8042 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 381/1000
2023-10-27 16:05:38.034 
Epoch 381/1000 
	 loss: 44.3032, MinusLogProbMetric: 44.3032, val_loss: 45.4830, val_MinusLogProbMetric: 45.4830

Epoch 381: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.3032 - MinusLogProbMetric: 44.3032 - val_loss: 45.4830 - val_MinusLogProbMetric: 45.4830 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 382/1000
2023-10-27 16:06:07.033 
Epoch 382/1000 
	 loss: 44.4576, MinusLogProbMetric: 44.4576, val_loss: 44.5316, val_MinusLogProbMetric: 44.5316

Epoch 382: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.4576 - MinusLogProbMetric: 44.4576 - val_loss: 44.5316 - val_MinusLogProbMetric: 44.5316 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 383/1000
2023-10-27 16:06:35.720 
Epoch 383/1000 
	 loss: 44.5390, MinusLogProbMetric: 44.5390, val_loss: 44.1821, val_MinusLogProbMetric: 44.1821

Epoch 383: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.5390 - MinusLogProbMetric: 44.5390 - val_loss: 44.1821 - val_MinusLogProbMetric: 44.1821 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 384/1000
2023-10-27 16:07:06.637 
Epoch 384/1000 
	 loss: 44.5385, MinusLogProbMetric: 44.5385, val_loss: 45.7229, val_MinusLogProbMetric: 45.7229

Epoch 384: val_loss did not improve from 44.05896
196/196 - 31s - loss: 44.5385 - MinusLogProbMetric: 44.5385 - val_loss: 45.7229 - val_MinusLogProbMetric: 45.7229 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 385/1000
2023-10-27 16:07:34.261 
Epoch 385/1000 
	 loss: 44.4031, MinusLogProbMetric: 44.4031, val_loss: 44.7916, val_MinusLogProbMetric: 44.7916

Epoch 385: val_loss did not improve from 44.05896
196/196 - 28s - loss: 44.4031 - MinusLogProbMetric: 44.4031 - val_loss: 44.7916 - val_MinusLogProbMetric: 44.7916 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 386/1000
2023-10-27 16:08:02.902 
Epoch 386/1000 
	 loss: 44.4135, MinusLogProbMetric: 44.4135, val_loss: 44.5997, val_MinusLogProbMetric: 44.5997

Epoch 386: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.4135 - MinusLogProbMetric: 44.4135 - val_loss: 44.5997 - val_MinusLogProbMetric: 44.5997 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 387/1000
2023-10-27 16:08:31.482 
Epoch 387/1000 
	 loss: 44.4663, MinusLogProbMetric: 44.4663, val_loss: 44.8098, val_MinusLogProbMetric: 44.8098

Epoch 387: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.4663 - MinusLogProbMetric: 44.4663 - val_loss: 44.8098 - val_MinusLogProbMetric: 44.8098 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 388/1000
2023-10-27 16:09:00.551 
Epoch 388/1000 
	 loss: 44.7103, MinusLogProbMetric: 44.7103, val_loss: 46.4675, val_MinusLogProbMetric: 46.4675

Epoch 388: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.7103 - MinusLogProbMetric: 44.7103 - val_loss: 46.4675 - val_MinusLogProbMetric: 46.4675 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 389/1000
2023-10-27 16:09:29.356 
Epoch 389/1000 
	 loss: 44.3872, MinusLogProbMetric: 44.3872, val_loss: 44.9726, val_MinusLogProbMetric: 44.9726

Epoch 389: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.3872 - MinusLogProbMetric: 44.3872 - val_loss: 44.9726 - val_MinusLogProbMetric: 44.9726 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 390/1000
2023-10-27 16:09:59.457 
Epoch 390/1000 
	 loss: 44.5879, MinusLogProbMetric: 44.5879, val_loss: 44.9076, val_MinusLogProbMetric: 44.9076

Epoch 390: val_loss did not improve from 44.05896
196/196 - 30s - loss: 44.5879 - MinusLogProbMetric: 44.5879 - val_loss: 44.9076 - val_MinusLogProbMetric: 44.9076 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 391/1000
2023-10-27 16:10:31.514 
Epoch 391/1000 
	 loss: 44.4493, MinusLogProbMetric: 44.4493, val_loss: 44.8144, val_MinusLogProbMetric: 44.8144

Epoch 391: val_loss did not improve from 44.05896
196/196 - 32s - loss: 44.4493 - MinusLogProbMetric: 44.4493 - val_loss: 44.8144 - val_MinusLogProbMetric: 44.8144 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 392/1000
2023-10-27 16:11:01.529 
Epoch 392/1000 
	 loss: 44.3825, MinusLogProbMetric: 44.3825, val_loss: 45.4335, val_MinusLogProbMetric: 45.4335

Epoch 392: val_loss did not improve from 44.05896
196/196 - 30s - loss: 44.3825 - MinusLogProbMetric: 44.3825 - val_loss: 45.4335 - val_MinusLogProbMetric: 45.4335 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 393/1000
2023-10-27 16:11:30.493 
Epoch 393/1000 
	 loss: 44.3596, MinusLogProbMetric: 44.3596, val_loss: 44.5610, val_MinusLogProbMetric: 44.5610

Epoch 393: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.3596 - MinusLogProbMetric: 44.3596 - val_loss: 44.5610 - val_MinusLogProbMetric: 44.5610 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 394/1000
2023-10-27 16:11:57.934 
Epoch 394/1000 
	 loss: 44.5126, MinusLogProbMetric: 44.5126, val_loss: 44.4632, val_MinusLogProbMetric: 44.4632

Epoch 394: val_loss did not improve from 44.05896
196/196 - 27s - loss: 44.5126 - MinusLogProbMetric: 44.5126 - val_loss: 44.4632 - val_MinusLogProbMetric: 44.4632 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 395/1000
2023-10-27 16:12:26.788 
Epoch 395/1000 
	 loss: 44.3729, MinusLogProbMetric: 44.3729, val_loss: 44.6283, val_MinusLogProbMetric: 44.6283

Epoch 395: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.3729 - MinusLogProbMetric: 44.3729 - val_loss: 44.6283 - val_MinusLogProbMetric: 44.6283 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 396/1000
2023-10-27 16:12:57.545 
Epoch 396/1000 
	 loss: 44.2421, MinusLogProbMetric: 44.2421, val_loss: 44.4544, val_MinusLogProbMetric: 44.4544

Epoch 396: val_loss did not improve from 44.05896
196/196 - 31s - loss: 44.2421 - MinusLogProbMetric: 44.2421 - val_loss: 44.4544 - val_MinusLogProbMetric: 44.4544 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 397/1000
2023-10-27 16:13:30.854 
Epoch 397/1000 
	 loss: 44.2290, MinusLogProbMetric: 44.2290, val_loss: 48.2011, val_MinusLogProbMetric: 48.2011

Epoch 397: val_loss did not improve from 44.05896
196/196 - 33s - loss: 44.2290 - MinusLogProbMetric: 44.2290 - val_loss: 48.2011 - val_MinusLogProbMetric: 48.2011 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 398/1000
2023-10-27 16:13:58.814 
Epoch 398/1000 
	 loss: 44.4268, MinusLogProbMetric: 44.4268, val_loss: 44.7512, val_MinusLogProbMetric: 44.7512

Epoch 398: val_loss did not improve from 44.05896
196/196 - 28s - loss: 44.4268 - MinusLogProbMetric: 44.4268 - val_loss: 44.7512 - val_MinusLogProbMetric: 44.7512 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 399/1000
2023-10-27 16:14:26.885 
Epoch 399/1000 
	 loss: 44.1489, MinusLogProbMetric: 44.1489, val_loss: 44.9096, val_MinusLogProbMetric: 44.9096

Epoch 399: val_loss did not improve from 44.05896
196/196 - 28s - loss: 44.1489 - MinusLogProbMetric: 44.1489 - val_loss: 44.9096 - val_MinusLogProbMetric: 44.9096 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 400/1000
2023-10-27 16:14:55.566 
Epoch 400/1000 
	 loss: 44.4720, MinusLogProbMetric: 44.4720, val_loss: 44.9003, val_MinusLogProbMetric: 44.9003

Epoch 400: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.4720 - MinusLogProbMetric: 44.4720 - val_loss: 44.9003 - val_MinusLogProbMetric: 44.9003 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 401/1000
2023-10-27 16:15:24.625 
Epoch 401/1000 
	 loss: 44.6033, MinusLogProbMetric: 44.6033, val_loss: 44.3714, val_MinusLogProbMetric: 44.3714

Epoch 401: val_loss did not improve from 44.05896
196/196 - 29s - loss: 44.6033 - MinusLogProbMetric: 44.6033 - val_loss: 44.3714 - val_MinusLogProbMetric: 44.3714 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 402/1000
2023-10-27 16:15:54.149 
Epoch 402/1000 
	 loss: 44.4313, MinusLogProbMetric: 44.4313, val_loss: 44.6532, val_MinusLogProbMetric: 44.6532

Epoch 402: val_loss did not improve from 44.05896
196/196 - 30s - loss: 44.4313 - MinusLogProbMetric: 44.4313 - val_loss: 44.6532 - val_MinusLogProbMetric: 44.6532 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 403/1000
2023-10-27 16:16:24.145 
Epoch 403/1000 
	 loss: 44.2523, MinusLogProbMetric: 44.2523, val_loss: 44.7934, val_MinusLogProbMetric: 44.7934

Epoch 403: val_loss did not improve from 44.05896
196/196 - 30s - loss: 44.2523 - MinusLogProbMetric: 44.2523 - val_loss: 44.7934 - val_MinusLogProbMetric: 44.7934 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 404/1000
2023-10-27 16:16:55.266 
Epoch 404/1000 
	 loss: 44.3453, MinusLogProbMetric: 44.3453, val_loss: 44.4844, val_MinusLogProbMetric: 44.4844

Epoch 404: val_loss did not improve from 44.05896
196/196 - 31s - loss: 44.3453 - MinusLogProbMetric: 44.3453 - val_loss: 44.4844 - val_MinusLogProbMetric: 44.4844 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 405/1000
2023-10-27 16:17:24.430 
Epoch 405/1000 
	 loss: 44.2674, MinusLogProbMetric: 44.2674, val_loss: 44.0324, val_MinusLogProbMetric: 44.0324

Epoch 405: val_loss improved from 44.05896 to 44.03238, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 30s - loss: 44.2674 - MinusLogProbMetric: 44.2674 - val_loss: 44.0324 - val_MinusLogProbMetric: 44.0324 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 406/1000
2023-10-27 16:17:53.822 
Epoch 406/1000 
	 loss: 44.5740, MinusLogProbMetric: 44.5740, val_loss: 44.5331, val_MinusLogProbMetric: 44.5331

Epoch 406: val_loss did not improve from 44.03238
196/196 - 29s - loss: 44.5740 - MinusLogProbMetric: 44.5740 - val_loss: 44.5331 - val_MinusLogProbMetric: 44.5331 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 407/1000
2023-10-27 16:18:22.824 
Epoch 407/1000 
	 loss: 44.1566, MinusLogProbMetric: 44.1566, val_loss: 44.9049, val_MinusLogProbMetric: 44.9049

Epoch 407: val_loss did not improve from 44.03238
196/196 - 29s - loss: 44.1566 - MinusLogProbMetric: 44.1566 - val_loss: 44.9049 - val_MinusLogProbMetric: 44.9049 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 408/1000
2023-10-27 16:18:51.766 
Epoch 408/1000 
	 loss: 44.4588, MinusLogProbMetric: 44.4588, val_loss: 46.1108, val_MinusLogProbMetric: 46.1108

Epoch 408: val_loss did not improve from 44.03238
196/196 - 29s - loss: 44.4588 - MinusLogProbMetric: 44.4588 - val_loss: 46.1108 - val_MinusLogProbMetric: 46.1108 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 409/1000
2023-10-27 16:19:24.402 
Epoch 409/1000 
	 loss: 44.6530, MinusLogProbMetric: 44.6530, val_loss: 46.3965, val_MinusLogProbMetric: 46.3965

Epoch 409: val_loss did not improve from 44.03238
196/196 - 33s - loss: 44.6530 - MinusLogProbMetric: 44.6530 - val_loss: 46.3965 - val_MinusLogProbMetric: 46.3965 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 410/1000
2023-10-27 16:19:56.061 
Epoch 410/1000 
	 loss: 44.3754, MinusLogProbMetric: 44.3754, val_loss: 43.9939, val_MinusLogProbMetric: 43.9939

Epoch 410: val_loss improved from 44.03238 to 43.99395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 32s - loss: 44.3754 - MinusLogProbMetric: 44.3754 - val_loss: 43.9939 - val_MinusLogProbMetric: 43.9939 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 411/1000
2023-10-27 16:20:24.183 
Epoch 411/1000 
	 loss: 44.4398, MinusLogProbMetric: 44.4398, val_loss: 44.6316, val_MinusLogProbMetric: 44.6316

Epoch 411: val_loss did not improve from 43.99395
196/196 - 28s - loss: 44.4398 - MinusLogProbMetric: 44.4398 - val_loss: 44.6316 - val_MinusLogProbMetric: 44.6316 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 412/1000
2023-10-27 16:20:52.147 
Epoch 412/1000 
	 loss: 44.0937, MinusLogProbMetric: 44.0937, val_loss: 45.2917, val_MinusLogProbMetric: 45.2917

Epoch 412: val_loss did not improve from 43.99395
196/196 - 28s - loss: 44.0937 - MinusLogProbMetric: 44.0937 - val_loss: 45.2917 - val_MinusLogProbMetric: 45.2917 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 413/1000
2023-10-27 16:21:20.649 
Epoch 413/1000 
	 loss: 44.1136, MinusLogProbMetric: 44.1136, val_loss: 46.6449, val_MinusLogProbMetric: 46.6449

Epoch 413: val_loss did not improve from 43.99395
196/196 - 28s - loss: 44.1136 - MinusLogProbMetric: 44.1136 - val_loss: 46.6449 - val_MinusLogProbMetric: 46.6449 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 414/1000
2023-10-27 16:21:49.492 
Epoch 414/1000 
	 loss: 44.2901, MinusLogProbMetric: 44.2901, val_loss: 43.9134, val_MinusLogProbMetric: 43.9134

Epoch 414: val_loss improved from 43.99395 to 43.91339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 29s - loss: 44.2901 - MinusLogProbMetric: 44.2901 - val_loss: 43.9134 - val_MinusLogProbMetric: 43.9134 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 415/1000
2023-10-27 16:22:20.001 
Epoch 415/1000 
	 loss: 44.0675, MinusLogProbMetric: 44.0675, val_loss: 45.9577, val_MinusLogProbMetric: 45.9577

Epoch 415: val_loss did not improve from 43.91339
196/196 - 30s - loss: 44.0675 - MinusLogProbMetric: 44.0675 - val_loss: 45.9577 - val_MinusLogProbMetric: 45.9577 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 416/1000
2023-10-27 16:22:51.447 
Epoch 416/1000 
	 loss: 44.2085, MinusLogProbMetric: 44.2085, val_loss: 44.6584, val_MinusLogProbMetric: 44.6584

Epoch 416: val_loss did not improve from 43.91339
196/196 - 31s - loss: 44.2085 - MinusLogProbMetric: 44.2085 - val_loss: 44.6584 - val_MinusLogProbMetric: 44.6584 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 417/1000
2023-10-27 16:23:20.701 
Epoch 417/1000 
	 loss: 44.4129, MinusLogProbMetric: 44.4129, val_loss: 45.2952, val_MinusLogProbMetric: 45.2952

Epoch 417: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.4129 - MinusLogProbMetric: 44.4129 - val_loss: 45.2952 - val_MinusLogProbMetric: 45.2952 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 418/1000
2023-10-27 16:23:49.524 
Epoch 418/1000 
	 loss: 44.4011, MinusLogProbMetric: 44.4011, val_loss: 44.2839, val_MinusLogProbMetric: 44.2839

Epoch 418: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.4011 - MinusLogProbMetric: 44.4011 - val_loss: 44.2839 - val_MinusLogProbMetric: 44.2839 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 419/1000
2023-10-27 16:24:18.433 
Epoch 419/1000 
	 loss: 44.4242, MinusLogProbMetric: 44.4242, val_loss: 44.2171, val_MinusLogProbMetric: 44.2171

Epoch 419: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.4242 - MinusLogProbMetric: 44.4242 - val_loss: 44.2171 - val_MinusLogProbMetric: 44.2171 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 420/1000
2023-10-27 16:24:47.122 
Epoch 420/1000 
	 loss: 44.3375, MinusLogProbMetric: 44.3375, val_loss: 44.7576, val_MinusLogProbMetric: 44.7576

Epoch 420: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.3375 - MinusLogProbMetric: 44.3375 - val_loss: 44.7576 - val_MinusLogProbMetric: 44.7576 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 421/1000
2023-10-27 16:25:16.517 
Epoch 421/1000 
	 loss: 44.1802, MinusLogProbMetric: 44.1802, val_loss: 44.0804, val_MinusLogProbMetric: 44.0804

Epoch 421: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.1802 - MinusLogProbMetric: 44.1802 - val_loss: 44.0804 - val_MinusLogProbMetric: 44.0804 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 422/1000
2023-10-27 16:25:45.780 
Epoch 422/1000 
	 loss: 44.1169, MinusLogProbMetric: 44.1169, val_loss: 44.9624, val_MinusLogProbMetric: 44.9624

Epoch 422: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.1169 - MinusLogProbMetric: 44.1169 - val_loss: 44.9624 - val_MinusLogProbMetric: 44.9624 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 423/1000
2023-10-27 16:26:15.658 
Epoch 423/1000 
	 loss: 44.2609, MinusLogProbMetric: 44.2609, val_loss: 44.5779, val_MinusLogProbMetric: 44.5779

Epoch 423: val_loss did not improve from 43.91339
196/196 - 30s - loss: 44.2609 - MinusLogProbMetric: 44.2609 - val_loss: 44.5779 - val_MinusLogProbMetric: 44.5779 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 424/1000
2023-10-27 16:26:44.811 
Epoch 424/1000 
	 loss: 44.4302, MinusLogProbMetric: 44.4302, val_loss: 44.5119, val_MinusLogProbMetric: 44.5119

Epoch 424: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.4302 - MinusLogProbMetric: 44.4302 - val_loss: 44.5119 - val_MinusLogProbMetric: 44.5119 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 425/1000
2023-10-27 16:27:13.247 
Epoch 425/1000 
	 loss: 44.0771, MinusLogProbMetric: 44.0771, val_loss: 46.8885, val_MinusLogProbMetric: 46.8885

Epoch 425: val_loss did not improve from 43.91339
196/196 - 28s - loss: 44.0771 - MinusLogProbMetric: 44.0771 - val_loss: 46.8885 - val_MinusLogProbMetric: 46.8885 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 426/1000
2023-10-27 16:27:42.102 
Epoch 426/1000 
	 loss: 44.5829, MinusLogProbMetric: 44.5829, val_loss: 46.0813, val_MinusLogProbMetric: 46.0813

Epoch 426: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.5829 - MinusLogProbMetric: 44.5829 - val_loss: 46.0813 - val_MinusLogProbMetric: 46.0813 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 427/1000
2023-10-27 16:28:10.742 
Epoch 427/1000 
	 loss: 44.2454, MinusLogProbMetric: 44.2454, val_loss: 44.0412, val_MinusLogProbMetric: 44.0412

Epoch 427: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.2454 - MinusLogProbMetric: 44.2454 - val_loss: 44.0412 - val_MinusLogProbMetric: 44.0412 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 428/1000
2023-10-27 16:28:39.225 
Epoch 428/1000 
	 loss: 44.1950, MinusLogProbMetric: 44.1950, val_loss: 44.4711, val_MinusLogProbMetric: 44.4711

Epoch 428: val_loss did not improve from 43.91339
196/196 - 28s - loss: 44.1950 - MinusLogProbMetric: 44.1950 - val_loss: 44.4711 - val_MinusLogProbMetric: 44.4711 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 429/1000
2023-10-27 16:29:07.100 
Epoch 429/1000 
	 loss: 44.0234, MinusLogProbMetric: 44.0234, val_loss: 44.1089, val_MinusLogProbMetric: 44.1089

Epoch 429: val_loss did not improve from 43.91339
196/196 - 28s - loss: 44.0234 - MinusLogProbMetric: 44.0234 - val_loss: 44.1089 - val_MinusLogProbMetric: 44.1089 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 430/1000
2023-10-27 16:29:35.985 
Epoch 430/1000 
	 loss: 44.2497, MinusLogProbMetric: 44.2497, val_loss: 44.9649, val_MinusLogProbMetric: 44.9649

Epoch 430: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.2497 - MinusLogProbMetric: 44.2497 - val_loss: 44.9649 - val_MinusLogProbMetric: 44.9649 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 431/1000
2023-10-27 16:30:05.239 
Epoch 431/1000 
	 loss: 44.0879, MinusLogProbMetric: 44.0879, val_loss: 45.0427, val_MinusLogProbMetric: 45.0427

Epoch 431: val_loss did not improve from 43.91339
196/196 - 29s - loss: 44.0879 - MinusLogProbMetric: 44.0879 - val_loss: 45.0427 - val_MinusLogProbMetric: 45.0427 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 432/1000
2023-10-27 16:30:34.189 
Epoch 432/1000 
	 loss: 43.8100, MinusLogProbMetric: 43.8100, val_loss: 44.6249, val_MinusLogProbMetric: 44.6249

Epoch 432: val_loss did not improve from 43.91339
196/196 - 29s - loss: 43.8100 - MinusLogProbMetric: 43.8100 - val_loss: 44.6249 - val_MinusLogProbMetric: 44.6249 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 433/1000
2023-10-27 16:31:04.717 
Epoch 433/1000 
	 loss: 44.2144, MinusLogProbMetric: 44.2144, val_loss: 44.8730, val_MinusLogProbMetric: 44.8730

Epoch 433: val_loss did not improve from 43.91339
196/196 - 31s - loss: 44.2144 - MinusLogProbMetric: 44.2144 - val_loss: 44.8730 - val_MinusLogProbMetric: 44.8730 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 434/1000
2023-10-27 16:31:38.576 
Epoch 434/1000 
	 loss: 44.0005, MinusLogProbMetric: 44.0005, val_loss: 44.8876, val_MinusLogProbMetric: 44.8876

Epoch 434: val_loss did not improve from 43.91339
196/196 - 34s - loss: 44.0005 - MinusLogProbMetric: 44.0005 - val_loss: 44.8876 - val_MinusLogProbMetric: 44.8876 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 435/1000
2023-10-27 16:32:05.775 
Epoch 435/1000 
	 loss: 43.9833, MinusLogProbMetric: 43.9833, val_loss: 45.9431, val_MinusLogProbMetric: 45.9431

Epoch 435: val_loss did not improve from 43.91339
196/196 - 27s - loss: 43.9833 - MinusLogProbMetric: 43.9833 - val_loss: 45.9431 - val_MinusLogProbMetric: 45.9431 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 436/1000
2023-10-27 16:32:32.463 
Epoch 436/1000 
	 loss: 44.2329, MinusLogProbMetric: 44.2329, val_loss: 43.8566, val_MinusLogProbMetric: 43.8566

Epoch 436: val_loss improved from 43.91339 to 43.85659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 27s - loss: 44.2329 - MinusLogProbMetric: 44.2329 - val_loss: 43.8566 - val_MinusLogProbMetric: 43.8566 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 437/1000
2023-10-27 16:33:00.809 
Epoch 437/1000 
	 loss: 44.0841, MinusLogProbMetric: 44.0841, val_loss: 43.8329, val_MinusLogProbMetric: 43.8329

Epoch 437: val_loss improved from 43.85659 to 43.83288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 28s - loss: 44.0841 - MinusLogProbMetric: 44.0841 - val_loss: 43.8329 - val_MinusLogProbMetric: 43.8329 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 438/1000
2023-10-27 16:33:29.336 
Epoch 438/1000 
	 loss: 44.0843, MinusLogProbMetric: 44.0843, val_loss: 44.5527, val_MinusLogProbMetric: 44.5527

Epoch 438: val_loss did not improve from 43.83288
196/196 - 28s - loss: 44.0843 - MinusLogProbMetric: 44.0843 - val_loss: 44.5527 - val_MinusLogProbMetric: 44.5527 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 439/1000
2023-10-27 16:33:58.352 
Epoch 439/1000 
	 loss: 43.8553, MinusLogProbMetric: 43.8553, val_loss: 43.8644, val_MinusLogProbMetric: 43.8644

Epoch 439: val_loss did not improve from 43.83288
196/196 - 29s - loss: 43.8553 - MinusLogProbMetric: 43.8553 - val_loss: 43.8644 - val_MinusLogProbMetric: 43.8644 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 440/1000
2023-10-27 16:34:28.475 
Epoch 440/1000 
	 loss: 44.0778, MinusLogProbMetric: 44.0778, val_loss: 43.8066, val_MinusLogProbMetric: 43.8066

Epoch 440: val_loss improved from 43.83288 to 43.80663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 31s - loss: 44.0778 - MinusLogProbMetric: 44.0778 - val_loss: 43.8066 - val_MinusLogProbMetric: 43.8066 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 441/1000
2023-10-27 16:35:01.615 
Epoch 441/1000 
	 loss: 44.1122, MinusLogProbMetric: 44.1122, val_loss: 44.1809, val_MinusLogProbMetric: 44.1809

Epoch 441: val_loss did not improve from 43.80663
196/196 - 33s - loss: 44.1122 - MinusLogProbMetric: 44.1122 - val_loss: 44.1809 - val_MinusLogProbMetric: 44.1809 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 442/1000
2023-10-27 16:35:34.312 
Epoch 442/1000 
	 loss: 44.0522, MinusLogProbMetric: 44.0522, val_loss: 44.7007, val_MinusLogProbMetric: 44.7007

Epoch 442: val_loss did not improve from 43.80663
196/196 - 33s - loss: 44.0522 - MinusLogProbMetric: 44.0522 - val_loss: 44.7007 - val_MinusLogProbMetric: 44.7007 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 443/1000
2023-10-27 16:36:08.144 
Epoch 443/1000 
	 loss: 44.1224, MinusLogProbMetric: 44.1224, val_loss: 45.5155, val_MinusLogProbMetric: 45.5155

Epoch 443: val_loss did not improve from 43.80663
196/196 - 34s - loss: 44.1224 - MinusLogProbMetric: 44.1224 - val_loss: 45.5155 - val_MinusLogProbMetric: 45.5155 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 444/1000
2023-10-27 16:36:41.750 
Epoch 444/1000 
	 loss: 44.0163, MinusLogProbMetric: 44.0163, val_loss: 44.8027, val_MinusLogProbMetric: 44.8027

Epoch 444: val_loss did not improve from 43.80663
196/196 - 34s - loss: 44.0163 - MinusLogProbMetric: 44.0163 - val_loss: 44.8027 - val_MinusLogProbMetric: 44.8027 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 445/1000
2023-10-27 16:37:15.486 
Epoch 445/1000 
	 loss: 44.5238, MinusLogProbMetric: 44.5238, val_loss: 44.1495, val_MinusLogProbMetric: 44.1495

Epoch 445: val_loss did not improve from 43.80663
196/196 - 34s - loss: 44.5238 - MinusLogProbMetric: 44.5238 - val_loss: 44.1495 - val_MinusLogProbMetric: 44.1495 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 446/1000
2023-10-27 16:37:49.305 
Epoch 446/1000 
	 loss: 43.5842, MinusLogProbMetric: 43.5842, val_loss: 43.9837, val_MinusLogProbMetric: 43.9837

Epoch 446: val_loss did not improve from 43.80663
196/196 - 34s - loss: 43.5842 - MinusLogProbMetric: 43.5842 - val_loss: 43.9837 - val_MinusLogProbMetric: 43.9837 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 447/1000
2023-10-27 16:38:23.083 
Epoch 447/1000 
	 loss: 44.1467, MinusLogProbMetric: 44.1467, val_loss: 46.1574, val_MinusLogProbMetric: 46.1574

Epoch 447: val_loss did not improve from 43.80663
196/196 - 34s - loss: 44.1467 - MinusLogProbMetric: 44.1467 - val_loss: 46.1574 - val_MinusLogProbMetric: 46.1574 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 448/1000
2023-10-27 16:38:56.681 
Epoch 448/1000 
	 loss: 44.2807, MinusLogProbMetric: 44.2807, val_loss: 44.7377, val_MinusLogProbMetric: 44.7377

Epoch 448: val_loss did not improve from 43.80663
196/196 - 34s - loss: 44.2807 - MinusLogProbMetric: 44.2807 - val_loss: 44.7377 - val_MinusLogProbMetric: 44.7377 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 449/1000
2023-10-27 16:39:30.686 
Epoch 449/1000 
	 loss: 43.9352, MinusLogProbMetric: 43.9352, val_loss: 44.9553, val_MinusLogProbMetric: 44.9553

Epoch 449: val_loss did not improve from 43.80663
196/196 - 34s - loss: 43.9352 - MinusLogProbMetric: 43.9352 - val_loss: 44.9553 - val_MinusLogProbMetric: 44.9553 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 450/1000
2023-10-27 16:40:04.528 
Epoch 450/1000 
	 loss: 43.8529, MinusLogProbMetric: 43.8529, val_loss: 44.0979, val_MinusLogProbMetric: 44.0979

Epoch 450: val_loss did not improve from 43.80663
196/196 - 34s - loss: 43.8529 - MinusLogProbMetric: 43.8529 - val_loss: 44.0979 - val_MinusLogProbMetric: 44.0979 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 451/1000
2023-10-27 16:40:38.117 
Epoch 451/1000 
	 loss: 44.1617, MinusLogProbMetric: 44.1617, val_loss: 45.2110, val_MinusLogProbMetric: 45.2110

Epoch 451: val_loss did not improve from 43.80663
196/196 - 34s - loss: 44.1617 - MinusLogProbMetric: 44.1617 - val_loss: 45.2110 - val_MinusLogProbMetric: 45.2110 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 452/1000
2023-10-27 16:41:12.076 
Epoch 452/1000 
	 loss: 44.3123, MinusLogProbMetric: 44.3123, val_loss: 46.2480, val_MinusLogProbMetric: 46.2480

Epoch 452: val_loss did not improve from 43.80663
196/196 - 34s - loss: 44.3123 - MinusLogProbMetric: 44.3123 - val_loss: 46.2480 - val_MinusLogProbMetric: 46.2480 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 453/1000
2023-10-27 16:41:46.033 
Epoch 453/1000 
	 loss: 43.8211, MinusLogProbMetric: 43.8211, val_loss: 44.0809, val_MinusLogProbMetric: 44.0809

Epoch 453: val_loss did not improve from 43.80663
196/196 - 34s - loss: 43.8211 - MinusLogProbMetric: 43.8211 - val_loss: 44.0809 - val_MinusLogProbMetric: 44.0809 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 454/1000
2023-10-27 16:42:15.719 
Epoch 454/1000 
	 loss: 43.9698, MinusLogProbMetric: 43.9698, val_loss: 43.7193, val_MinusLogProbMetric: 43.7193

Epoch 454: val_loss improved from 43.80663 to 43.71926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 30s - loss: 43.9698 - MinusLogProbMetric: 43.9698 - val_loss: 43.7193 - val_MinusLogProbMetric: 43.7193 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 455/1000
2023-10-27 16:42:44.856 
Epoch 455/1000 
	 loss: 43.8769, MinusLogProbMetric: 43.8769, val_loss: 43.8020, val_MinusLogProbMetric: 43.8020

Epoch 455: val_loss did not improve from 43.71926
196/196 - 29s - loss: 43.8769 - MinusLogProbMetric: 43.8769 - val_loss: 43.8020 - val_MinusLogProbMetric: 43.8020 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 456/1000
2023-10-27 16:43:13.761 
Epoch 456/1000 
	 loss: 43.9137, MinusLogProbMetric: 43.9137, val_loss: 44.9990, val_MinusLogProbMetric: 44.9990

Epoch 456: val_loss did not improve from 43.71926
196/196 - 29s - loss: 43.9137 - MinusLogProbMetric: 43.9137 - val_loss: 44.9990 - val_MinusLogProbMetric: 44.9990 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 457/1000
2023-10-27 16:43:42.762 
Epoch 457/1000 
	 loss: 43.9284, MinusLogProbMetric: 43.9284, val_loss: 44.9171, val_MinusLogProbMetric: 44.9171

Epoch 457: val_loss did not improve from 43.71926
196/196 - 29s - loss: 43.9284 - MinusLogProbMetric: 43.9284 - val_loss: 44.9171 - val_MinusLogProbMetric: 44.9171 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 458/1000
2023-10-27 16:44:12.978 
Epoch 458/1000 
	 loss: 44.2664, MinusLogProbMetric: 44.2664, val_loss: 45.1007, val_MinusLogProbMetric: 45.1007

Epoch 458: val_loss did not improve from 43.71926
196/196 - 30s - loss: 44.2664 - MinusLogProbMetric: 44.2664 - val_loss: 45.1007 - val_MinusLogProbMetric: 45.1007 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 459/1000
2023-10-27 16:44:45.938 
Epoch 459/1000 
	 loss: 43.7230, MinusLogProbMetric: 43.7230, val_loss: 44.2662, val_MinusLogProbMetric: 44.2662

Epoch 459: val_loss did not improve from 43.71926
196/196 - 33s - loss: 43.7230 - MinusLogProbMetric: 43.7230 - val_loss: 44.2662 - val_MinusLogProbMetric: 44.2662 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 460/1000
2023-10-27 16:45:19.497 
Epoch 460/1000 
	 loss: 44.1676, MinusLogProbMetric: 44.1676, val_loss: 44.4210, val_MinusLogProbMetric: 44.4210

Epoch 460: val_loss did not improve from 43.71926
196/196 - 34s - loss: 44.1676 - MinusLogProbMetric: 44.1676 - val_loss: 44.4210 - val_MinusLogProbMetric: 44.4210 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 461/1000
2023-10-27 16:45:53.700 
Epoch 461/1000 
	 loss: 43.9970, MinusLogProbMetric: 43.9970, val_loss: 44.6033, val_MinusLogProbMetric: 44.6033

Epoch 461: val_loss did not improve from 43.71926
196/196 - 34s - loss: 43.9970 - MinusLogProbMetric: 43.9970 - val_loss: 44.6033 - val_MinusLogProbMetric: 44.6033 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 462/1000
2023-10-27 16:46:27.454 
Epoch 462/1000 
	 loss: 43.9780, MinusLogProbMetric: 43.9780, val_loss: 46.7600, val_MinusLogProbMetric: 46.7600

Epoch 462: val_loss did not improve from 43.71926
196/196 - 34s - loss: 43.9780 - MinusLogProbMetric: 43.9780 - val_loss: 46.7600 - val_MinusLogProbMetric: 46.7600 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 463/1000
2023-10-27 16:47:00.005 
Epoch 463/1000 
	 loss: 44.0173, MinusLogProbMetric: 44.0173, val_loss: 44.2000, val_MinusLogProbMetric: 44.2000

Epoch 463: val_loss did not improve from 43.71926
196/196 - 33s - loss: 44.0173 - MinusLogProbMetric: 44.0173 - val_loss: 44.2000 - val_MinusLogProbMetric: 44.2000 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 464/1000
2023-10-27 16:47:33.760 
Epoch 464/1000 
	 loss: 44.3398, MinusLogProbMetric: 44.3398, val_loss: 44.6597, val_MinusLogProbMetric: 44.6597

Epoch 464: val_loss did not improve from 43.71926
196/196 - 34s - loss: 44.3398 - MinusLogProbMetric: 44.3398 - val_loss: 44.6597 - val_MinusLogProbMetric: 44.6597 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 465/1000
2023-10-27 16:48:07.941 
Epoch 465/1000 
	 loss: 43.6285, MinusLogProbMetric: 43.6285, val_loss: 46.4502, val_MinusLogProbMetric: 46.4502

Epoch 465: val_loss did not improve from 43.71926
196/196 - 34s - loss: 43.6285 - MinusLogProbMetric: 43.6285 - val_loss: 46.4502 - val_MinusLogProbMetric: 46.4502 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 466/1000
2023-10-27 16:48:42.094 
Epoch 466/1000 
	 loss: 43.9238, MinusLogProbMetric: 43.9238, val_loss: 44.0708, val_MinusLogProbMetric: 44.0708

Epoch 466: val_loss did not improve from 43.71926
196/196 - 34s - loss: 43.9238 - MinusLogProbMetric: 43.9238 - val_loss: 44.0708 - val_MinusLogProbMetric: 44.0708 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 467/1000
2023-10-27 16:49:15.572 
Epoch 467/1000 
	 loss: 43.9217, MinusLogProbMetric: 43.9217, val_loss: 45.1640, val_MinusLogProbMetric: 45.1640

Epoch 467: val_loss did not improve from 43.71926
196/196 - 33s - loss: 43.9217 - MinusLogProbMetric: 43.9217 - val_loss: 45.1640 - val_MinusLogProbMetric: 45.1640 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 468/1000
2023-10-27 16:49:49.742 
Epoch 468/1000 
	 loss: 44.3019, MinusLogProbMetric: 44.3019, val_loss: 45.5085, val_MinusLogProbMetric: 45.5085

Epoch 468: val_loss did not improve from 43.71926
196/196 - 34s - loss: 44.3019 - MinusLogProbMetric: 44.3019 - val_loss: 45.5085 - val_MinusLogProbMetric: 45.5085 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 469/1000
2023-10-27 16:50:23.348 
Epoch 469/1000 
	 loss: 43.8924, MinusLogProbMetric: 43.8924, val_loss: 43.9059, val_MinusLogProbMetric: 43.9059

Epoch 469: val_loss did not improve from 43.71926
196/196 - 34s - loss: 43.8924 - MinusLogProbMetric: 43.8924 - val_loss: 43.9059 - val_MinusLogProbMetric: 43.9059 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 470/1000
2023-10-27 16:50:57.136 
Epoch 470/1000 
	 loss: 43.9517, MinusLogProbMetric: 43.9517, val_loss: 43.6119, val_MinusLogProbMetric: 43.6119

Epoch 470: val_loss improved from 43.71926 to 43.61190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 43.9517 - MinusLogProbMetric: 43.9517 - val_loss: 43.6119 - val_MinusLogProbMetric: 43.6119 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 471/1000
2023-10-27 16:51:31.305 
Epoch 471/1000 
	 loss: 43.8160, MinusLogProbMetric: 43.8160, val_loss: 44.2812, val_MinusLogProbMetric: 44.2812

Epoch 471: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.8160 - MinusLogProbMetric: 43.8160 - val_loss: 44.2812 - val_MinusLogProbMetric: 44.2812 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 472/1000
2023-10-27 16:52:02.371 
Epoch 472/1000 
	 loss: 43.8375, MinusLogProbMetric: 43.8375, val_loss: 43.8344, val_MinusLogProbMetric: 43.8344

Epoch 472: val_loss did not improve from 43.61190
196/196 - 31s - loss: 43.8375 - MinusLogProbMetric: 43.8375 - val_loss: 43.8344 - val_MinusLogProbMetric: 43.8344 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 473/1000
2023-10-27 16:52:36.249 
Epoch 473/1000 
	 loss: 43.8965, MinusLogProbMetric: 43.8965, val_loss: 44.1742, val_MinusLogProbMetric: 44.1742

Epoch 473: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.8965 - MinusLogProbMetric: 43.8965 - val_loss: 44.1742 - val_MinusLogProbMetric: 44.1742 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 474/1000
2023-10-27 16:53:09.975 
Epoch 474/1000 
	 loss: 44.1071, MinusLogProbMetric: 44.1071, val_loss: 47.4607, val_MinusLogProbMetric: 47.4607

Epoch 474: val_loss did not improve from 43.61190
196/196 - 34s - loss: 44.1071 - MinusLogProbMetric: 44.1071 - val_loss: 47.4607 - val_MinusLogProbMetric: 47.4607 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 475/1000
2023-10-27 16:53:44.068 
Epoch 475/1000 
	 loss: 44.0273, MinusLogProbMetric: 44.0273, val_loss: 44.8525, val_MinusLogProbMetric: 44.8525

Epoch 475: val_loss did not improve from 43.61190
196/196 - 34s - loss: 44.0273 - MinusLogProbMetric: 44.0273 - val_loss: 44.8525 - val_MinusLogProbMetric: 44.8525 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 476/1000
2023-10-27 16:54:17.441 
Epoch 476/1000 
	 loss: 44.2248, MinusLogProbMetric: 44.2248, val_loss: 43.8100, val_MinusLogProbMetric: 43.8100

Epoch 476: val_loss did not improve from 43.61190
196/196 - 33s - loss: 44.2248 - MinusLogProbMetric: 44.2248 - val_loss: 43.8100 - val_MinusLogProbMetric: 43.8100 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 477/1000
2023-10-27 16:54:51.058 
Epoch 477/1000 
	 loss: 43.9350, MinusLogProbMetric: 43.9350, val_loss: 43.9458, val_MinusLogProbMetric: 43.9458

Epoch 477: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.9350 - MinusLogProbMetric: 43.9350 - val_loss: 43.9458 - val_MinusLogProbMetric: 43.9458 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 478/1000
2023-10-27 16:55:24.857 
Epoch 478/1000 
	 loss: 43.9687, MinusLogProbMetric: 43.9687, val_loss: 43.8404, val_MinusLogProbMetric: 43.8404

Epoch 478: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.9687 - MinusLogProbMetric: 43.9687 - val_loss: 43.8404 - val_MinusLogProbMetric: 43.8404 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 479/1000
2023-10-27 16:55:58.639 
Epoch 479/1000 
	 loss: 43.8935, MinusLogProbMetric: 43.8935, val_loss: 43.8427, val_MinusLogProbMetric: 43.8427

Epoch 479: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.8935 - MinusLogProbMetric: 43.8935 - val_loss: 43.8427 - val_MinusLogProbMetric: 43.8427 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 480/1000
2023-10-27 16:56:32.701 
Epoch 480/1000 
	 loss: 43.9475, MinusLogProbMetric: 43.9475, val_loss: 45.6476, val_MinusLogProbMetric: 45.6476

Epoch 480: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.9475 - MinusLogProbMetric: 43.9475 - val_loss: 45.6476 - val_MinusLogProbMetric: 45.6476 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 481/1000
2023-10-27 16:57:06.917 
Epoch 481/1000 
	 loss: 44.0177, MinusLogProbMetric: 44.0177, val_loss: 46.5059, val_MinusLogProbMetric: 46.5059

Epoch 481: val_loss did not improve from 43.61190
196/196 - 34s - loss: 44.0177 - MinusLogProbMetric: 44.0177 - val_loss: 46.5059 - val_MinusLogProbMetric: 46.5059 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 482/1000
2023-10-27 16:57:41.123 
Epoch 482/1000 
	 loss: 43.9850, MinusLogProbMetric: 43.9850, val_loss: 44.5078, val_MinusLogProbMetric: 44.5078

Epoch 482: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.9850 - MinusLogProbMetric: 43.9850 - val_loss: 44.5078 - val_MinusLogProbMetric: 44.5078 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 483/1000
2023-10-27 16:58:15.106 
Epoch 483/1000 
	 loss: 43.5581, MinusLogProbMetric: 43.5581, val_loss: 43.8192, val_MinusLogProbMetric: 43.8192

Epoch 483: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.5581 - MinusLogProbMetric: 43.5581 - val_loss: 43.8192 - val_MinusLogProbMetric: 43.8192 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 484/1000
2023-10-27 16:58:48.975 
Epoch 484/1000 
	 loss: 43.9852, MinusLogProbMetric: 43.9852, val_loss: 45.0878, val_MinusLogProbMetric: 45.0878

Epoch 484: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.9852 - MinusLogProbMetric: 43.9852 - val_loss: 45.0878 - val_MinusLogProbMetric: 45.0878 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 485/1000
2023-10-27 16:59:22.655 
Epoch 485/1000 
	 loss: 43.6827, MinusLogProbMetric: 43.6827, val_loss: 43.9900, val_MinusLogProbMetric: 43.9900

Epoch 485: val_loss did not improve from 43.61190
196/196 - 34s - loss: 43.6827 - MinusLogProbMetric: 43.6827 - val_loss: 43.9900 - val_MinusLogProbMetric: 43.9900 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 486/1000
2023-10-27 16:59:56.268 
Epoch 486/1000 
	 loss: 43.8249, MinusLogProbMetric: 43.8249, val_loss: 43.5761, val_MinusLogProbMetric: 43.5761

Epoch 486: val_loss improved from 43.61190 to 43.57606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 43.8249 - MinusLogProbMetric: 43.8249 - val_loss: 43.5761 - val_MinusLogProbMetric: 43.5761 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 487/1000
2023-10-27 17:00:29.940 
Epoch 487/1000 
	 loss: 44.0404, MinusLogProbMetric: 44.0404, val_loss: 45.2245, val_MinusLogProbMetric: 45.2245

Epoch 487: val_loss did not improve from 43.57606
196/196 - 33s - loss: 44.0404 - MinusLogProbMetric: 44.0404 - val_loss: 45.2245 - val_MinusLogProbMetric: 45.2245 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 488/1000
2023-10-27 17:01:03.720 
Epoch 488/1000 
	 loss: 43.6360, MinusLogProbMetric: 43.6360, val_loss: 43.6898, val_MinusLogProbMetric: 43.6898

Epoch 488: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.6360 - MinusLogProbMetric: 43.6360 - val_loss: 43.6898 - val_MinusLogProbMetric: 43.6898 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 489/1000
2023-10-27 17:01:37.177 
Epoch 489/1000 
	 loss: 44.0056, MinusLogProbMetric: 44.0056, val_loss: 44.2063, val_MinusLogProbMetric: 44.2063

Epoch 489: val_loss did not improve from 43.57606
196/196 - 33s - loss: 44.0056 - MinusLogProbMetric: 44.0056 - val_loss: 44.2063 - val_MinusLogProbMetric: 44.2063 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 490/1000
2023-10-27 17:02:11.033 
Epoch 490/1000 
	 loss: 44.2509, MinusLogProbMetric: 44.2509, val_loss: 44.5372, val_MinusLogProbMetric: 44.5372

Epoch 490: val_loss did not improve from 43.57606
196/196 - 34s - loss: 44.2509 - MinusLogProbMetric: 44.2509 - val_loss: 44.5372 - val_MinusLogProbMetric: 44.5372 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 491/1000
2023-10-27 17:02:44.923 
Epoch 491/1000 
	 loss: 43.6550, MinusLogProbMetric: 43.6550, val_loss: 43.6868, val_MinusLogProbMetric: 43.6868

Epoch 491: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.6550 - MinusLogProbMetric: 43.6550 - val_loss: 43.6868 - val_MinusLogProbMetric: 43.6868 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 492/1000
2023-10-27 17:03:17.728 
Epoch 492/1000 
	 loss: 44.2821, MinusLogProbMetric: 44.2821, val_loss: 45.8099, val_MinusLogProbMetric: 45.8099

Epoch 492: val_loss did not improve from 43.57606
196/196 - 33s - loss: 44.2821 - MinusLogProbMetric: 44.2821 - val_loss: 45.8099 - val_MinusLogProbMetric: 45.8099 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 493/1000
2023-10-27 17:03:51.443 
Epoch 493/1000 
	 loss: 44.1780, MinusLogProbMetric: 44.1780, val_loss: 44.0117, val_MinusLogProbMetric: 44.0117

Epoch 493: val_loss did not improve from 43.57606
196/196 - 34s - loss: 44.1780 - MinusLogProbMetric: 44.1780 - val_loss: 44.0117 - val_MinusLogProbMetric: 44.0117 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 494/1000
2023-10-27 17:04:25.134 
Epoch 494/1000 
	 loss: 43.9009, MinusLogProbMetric: 43.9009, val_loss: 45.3999, val_MinusLogProbMetric: 45.3999

Epoch 494: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.9009 - MinusLogProbMetric: 43.9009 - val_loss: 45.3999 - val_MinusLogProbMetric: 45.3999 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 495/1000
2023-10-27 17:04:58.746 
Epoch 495/1000 
	 loss: 43.7194, MinusLogProbMetric: 43.7194, val_loss: 44.4142, val_MinusLogProbMetric: 44.4142

Epoch 495: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.7194 - MinusLogProbMetric: 43.7194 - val_loss: 44.4142 - val_MinusLogProbMetric: 44.4142 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 496/1000
2023-10-27 17:05:31.759 
Epoch 496/1000 
	 loss: 43.8259, MinusLogProbMetric: 43.8259, val_loss: 44.4664, val_MinusLogProbMetric: 44.4664

Epoch 496: val_loss did not improve from 43.57606
196/196 - 33s - loss: 43.8259 - MinusLogProbMetric: 43.8259 - val_loss: 44.4664 - val_MinusLogProbMetric: 44.4664 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 497/1000
2023-10-27 17:06:00.280 
Epoch 497/1000 
	 loss: 43.9674, MinusLogProbMetric: 43.9674, val_loss: 44.1387, val_MinusLogProbMetric: 44.1387

Epoch 497: val_loss did not improve from 43.57606
196/196 - 29s - loss: 43.9674 - MinusLogProbMetric: 43.9674 - val_loss: 44.1387 - val_MinusLogProbMetric: 44.1387 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 498/1000
2023-10-27 17:06:31.153 
Epoch 498/1000 
	 loss: 43.8888, MinusLogProbMetric: 43.8888, val_loss: 45.5821, val_MinusLogProbMetric: 45.5821

Epoch 498: val_loss did not improve from 43.57606
196/196 - 31s - loss: 43.8888 - MinusLogProbMetric: 43.8888 - val_loss: 45.5821 - val_MinusLogProbMetric: 45.5821 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 499/1000
2023-10-27 17:07:00.653 
Epoch 499/1000 
	 loss: 44.5070, MinusLogProbMetric: 44.5070, val_loss: 44.5809, val_MinusLogProbMetric: 44.5809

Epoch 499: val_loss did not improve from 43.57606
196/196 - 29s - loss: 44.5070 - MinusLogProbMetric: 44.5070 - val_loss: 44.5809 - val_MinusLogProbMetric: 44.5809 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 500/1000
2023-10-27 17:07:31.919 
Epoch 500/1000 
	 loss: 43.9001, MinusLogProbMetric: 43.9001, val_loss: 45.2046, val_MinusLogProbMetric: 45.2046

Epoch 500: val_loss did not improve from 43.57606
196/196 - 31s - loss: 43.9001 - MinusLogProbMetric: 43.9001 - val_loss: 45.2046 - val_MinusLogProbMetric: 45.2046 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 501/1000
2023-10-27 17:08:01.772 
Epoch 501/1000 
	 loss: 44.0134, MinusLogProbMetric: 44.0134, val_loss: 45.5704, val_MinusLogProbMetric: 45.5704

Epoch 501: val_loss did not improve from 43.57606
196/196 - 30s - loss: 44.0134 - MinusLogProbMetric: 44.0134 - val_loss: 45.5704 - val_MinusLogProbMetric: 45.5704 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 502/1000
2023-10-27 17:08:32.095 
Epoch 502/1000 
	 loss: 43.9766, MinusLogProbMetric: 43.9766, val_loss: 45.9685, val_MinusLogProbMetric: 45.9685

Epoch 502: val_loss did not improve from 43.57606
196/196 - 30s - loss: 43.9766 - MinusLogProbMetric: 43.9766 - val_loss: 45.9685 - val_MinusLogProbMetric: 45.9685 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 503/1000
2023-10-27 17:09:02.738 
Epoch 503/1000 
	 loss: 43.7537, MinusLogProbMetric: 43.7537, val_loss: 45.9364, val_MinusLogProbMetric: 45.9364

Epoch 503: val_loss did not improve from 43.57606
196/196 - 31s - loss: 43.7537 - MinusLogProbMetric: 43.7537 - val_loss: 45.9364 - val_MinusLogProbMetric: 45.9364 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 504/1000
2023-10-27 17:09:32.328 
Epoch 504/1000 
	 loss: 43.6297, MinusLogProbMetric: 43.6297, val_loss: 44.1963, val_MinusLogProbMetric: 44.1963

Epoch 504: val_loss did not improve from 43.57606
196/196 - 30s - loss: 43.6297 - MinusLogProbMetric: 43.6297 - val_loss: 44.1963 - val_MinusLogProbMetric: 44.1963 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 505/1000
2023-10-27 17:10:02.477 
Epoch 505/1000 
	 loss: 44.2159, MinusLogProbMetric: 44.2159, val_loss: 44.4351, val_MinusLogProbMetric: 44.4351

Epoch 505: val_loss did not improve from 43.57606
196/196 - 30s - loss: 44.2159 - MinusLogProbMetric: 44.2159 - val_loss: 44.4351 - val_MinusLogProbMetric: 44.4351 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 506/1000
2023-10-27 17:10:32.572 
Epoch 506/1000 
	 loss: 43.9032, MinusLogProbMetric: 43.9032, val_loss: 43.9754, val_MinusLogProbMetric: 43.9754

Epoch 506: val_loss did not improve from 43.57606
196/196 - 30s - loss: 43.9032 - MinusLogProbMetric: 43.9032 - val_loss: 43.9754 - val_MinusLogProbMetric: 43.9754 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 507/1000
2023-10-27 17:11:03.913 
Epoch 507/1000 
	 loss: 43.7087, MinusLogProbMetric: 43.7087, val_loss: 45.4396, val_MinusLogProbMetric: 45.4396

Epoch 507: val_loss did not improve from 43.57606
196/196 - 31s - loss: 43.7087 - MinusLogProbMetric: 43.7087 - val_loss: 45.4396 - val_MinusLogProbMetric: 45.4396 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 508/1000
2023-10-27 17:11:32.960 
Epoch 508/1000 
	 loss: 43.7372, MinusLogProbMetric: 43.7372, val_loss: 43.8077, val_MinusLogProbMetric: 43.8077

Epoch 508: val_loss did not improve from 43.57606
196/196 - 29s - loss: 43.7372 - MinusLogProbMetric: 43.7372 - val_loss: 43.8077 - val_MinusLogProbMetric: 43.8077 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 509/1000
2023-10-27 17:12:04.573 
Epoch 509/1000 
	 loss: 44.0102, MinusLogProbMetric: 44.0102, val_loss: 46.1470, val_MinusLogProbMetric: 46.1470

Epoch 509: val_loss did not improve from 43.57606
196/196 - 32s - loss: 44.0102 - MinusLogProbMetric: 44.0102 - val_loss: 46.1470 - val_MinusLogProbMetric: 46.1470 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 510/1000
2023-10-27 17:12:33.302 
Epoch 510/1000 
	 loss: 43.6454, MinusLogProbMetric: 43.6454, val_loss: 45.7973, val_MinusLogProbMetric: 45.7973

Epoch 510: val_loss did not improve from 43.57606
196/196 - 29s - loss: 43.6454 - MinusLogProbMetric: 43.6454 - val_loss: 45.7973 - val_MinusLogProbMetric: 45.7973 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 511/1000
2023-10-27 17:13:03.329 
Epoch 511/1000 
	 loss: 44.4599, MinusLogProbMetric: 44.4599, val_loss: 44.7058, val_MinusLogProbMetric: 44.7058

Epoch 511: val_loss did not improve from 43.57606
196/196 - 30s - loss: 44.4599 - MinusLogProbMetric: 44.4599 - val_loss: 44.7058 - val_MinusLogProbMetric: 44.7058 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 512/1000
2023-10-27 17:13:32.456 
Epoch 512/1000 
	 loss: 46.1832, MinusLogProbMetric: 46.1832, val_loss: 44.8650, val_MinusLogProbMetric: 44.8650

Epoch 512: val_loss did not improve from 43.57606
196/196 - 29s - loss: 46.1832 - MinusLogProbMetric: 46.1832 - val_loss: 44.8650 - val_MinusLogProbMetric: 44.8650 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 513/1000
2023-10-27 17:14:01.215 
Epoch 513/1000 
	 loss: 44.0726, MinusLogProbMetric: 44.0726, val_loss: 43.8112, val_MinusLogProbMetric: 43.8112

Epoch 513: val_loss did not improve from 43.57606
196/196 - 29s - loss: 44.0726 - MinusLogProbMetric: 44.0726 - val_loss: 43.8112 - val_MinusLogProbMetric: 43.8112 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 514/1000
2023-10-27 17:14:30.045 
Epoch 514/1000 
	 loss: 43.8335, MinusLogProbMetric: 43.8335, val_loss: 43.9065, val_MinusLogProbMetric: 43.9065

Epoch 514: val_loss did not improve from 43.57606
196/196 - 29s - loss: 43.8335 - MinusLogProbMetric: 43.8335 - val_loss: 43.9065 - val_MinusLogProbMetric: 43.9065 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 515/1000
2023-10-27 17:14:58.957 
Epoch 515/1000 
	 loss: 44.2330, MinusLogProbMetric: 44.2330, val_loss: 45.0435, val_MinusLogProbMetric: 45.0435

Epoch 515: val_loss did not improve from 43.57606
196/196 - 29s - loss: 44.2330 - MinusLogProbMetric: 44.2330 - val_loss: 45.0435 - val_MinusLogProbMetric: 45.0435 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 516/1000
2023-10-27 17:15:27.124 
Epoch 516/1000 
	 loss: 44.0530, MinusLogProbMetric: 44.0530, val_loss: 43.8200, val_MinusLogProbMetric: 43.8200

Epoch 516: val_loss did not improve from 43.57606
196/196 - 28s - loss: 44.0530 - MinusLogProbMetric: 44.0530 - val_loss: 43.8200 - val_MinusLogProbMetric: 43.8200 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 517/1000
2023-10-27 17:15:58.412 
Epoch 517/1000 
	 loss: 44.0413, MinusLogProbMetric: 44.0413, val_loss: 43.9778, val_MinusLogProbMetric: 43.9778

Epoch 517: val_loss did not improve from 43.57606
196/196 - 31s - loss: 44.0413 - MinusLogProbMetric: 44.0413 - val_loss: 43.9778 - val_MinusLogProbMetric: 43.9778 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 518/1000
2023-10-27 17:16:27.428 
Epoch 518/1000 
	 loss: 44.0143, MinusLogProbMetric: 44.0143, val_loss: 44.4406, val_MinusLogProbMetric: 44.4406

Epoch 518: val_loss did not improve from 43.57606
196/196 - 29s - loss: 44.0143 - MinusLogProbMetric: 44.0143 - val_loss: 44.4406 - val_MinusLogProbMetric: 44.4406 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 519/1000
2023-10-27 17:16:57.177 
Epoch 519/1000 
	 loss: 43.6493, MinusLogProbMetric: 43.6493, val_loss: 45.0767, val_MinusLogProbMetric: 45.0767

Epoch 519: val_loss did not improve from 43.57606
196/196 - 30s - loss: 43.6493 - MinusLogProbMetric: 43.6493 - val_loss: 45.0767 - val_MinusLogProbMetric: 45.0767 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 520/1000
2023-10-27 17:17:25.627 
Epoch 520/1000 
	 loss: 43.8151, MinusLogProbMetric: 43.8151, val_loss: 44.6986, val_MinusLogProbMetric: 44.6986

Epoch 520: val_loss did not improve from 43.57606
196/196 - 28s - loss: 43.8151 - MinusLogProbMetric: 43.8151 - val_loss: 44.6986 - val_MinusLogProbMetric: 44.6986 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 521/1000
2023-10-27 17:17:54.367 
Epoch 521/1000 
	 loss: 43.8522, MinusLogProbMetric: 43.8522, val_loss: 45.2378, val_MinusLogProbMetric: 45.2378

Epoch 521: val_loss did not improve from 43.57606
196/196 - 29s - loss: 43.8522 - MinusLogProbMetric: 43.8522 - val_loss: 45.2378 - val_MinusLogProbMetric: 45.2378 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 522/1000
2023-10-27 17:18:23.123 
Epoch 522/1000 
	 loss: 43.7582, MinusLogProbMetric: 43.7582, val_loss: 44.5282, val_MinusLogProbMetric: 44.5282

Epoch 522: val_loss did not improve from 43.57606
196/196 - 29s - loss: 43.7582 - MinusLogProbMetric: 43.7582 - val_loss: 44.5282 - val_MinusLogProbMetric: 44.5282 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 523/1000
2023-10-27 17:18:50.791 
Epoch 523/1000 
	 loss: 43.8833, MinusLogProbMetric: 43.8833, val_loss: 44.0158, val_MinusLogProbMetric: 44.0158

Epoch 523: val_loss did not improve from 43.57606
196/196 - 28s - loss: 43.8833 - MinusLogProbMetric: 43.8833 - val_loss: 44.0158 - val_MinusLogProbMetric: 44.0158 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 524/1000
2023-10-27 17:19:23.559 
Epoch 524/1000 
	 loss: 43.7760, MinusLogProbMetric: 43.7760, val_loss: 44.3896, val_MinusLogProbMetric: 44.3896

Epoch 524: val_loss did not improve from 43.57606
196/196 - 33s - loss: 43.7760 - MinusLogProbMetric: 43.7760 - val_loss: 44.3896 - val_MinusLogProbMetric: 44.3896 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 525/1000
2023-10-27 17:19:54.858 
Epoch 525/1000 
	 loss: 44.2215, MinusLogProbMetric: 44.2215, val_loss: 44.2316, val_MinusLogProbMetric: 44.2316

Epoch 525: val_loss did not improve from 43.57606
196/196 - 31s - loss: 44.2215 - MinusLogProbMetric: 44.2215 - val_loss: 44.2316 - val_MinusLogProbMetric: 44.2316 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 526/1000
2023-10-27 17:20:28.588 
Epoch 526/1000 
	 loss: 43.9911, MinusLogProbMetric: 43.9911, val_loss: 43.8854, val_MinusLogProbMetric: 43.8854

Epoch 526: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.9911 - MinusLogProbMetric: 43.9911 - val_loss: 43.8854 - val_MinusLogProbMetric: 43.8854 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 527/1000
2023-10-27 17:21:01.337 
Epoch 527/1000 
	 loss: 44.2174, MinusLogProbMetric: 44.2174, val_loss: 44.0516, val_MinusLogProbMetric: 44.0516

Epoch 527: val_loss did not improve from 43.57606
196/196 - 33s - loss: 44.2174 - MinusLogProbMetric: 44.2174 - val_loss: 44.0516 - val_MinusLogProbMetric: 44.0516 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 528/1000
2023-10-27 17:21:34.772 
Epoch 528/1000 
	 loss: 43.3999, MinusLogProbMetric: 43.3999, val_loss: 44.5643, val_MinusLogProbMetric: 44.5643

Epoch 528: val_loss did not improve from 43.57606
196/196 - 33s - loss: 43.3999 - MinusLogProbMetric: 43.3999 - val_loss: 44.5643 - val_MinusLogProbMetric: 44.5643 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 529/1000
2023-10-27 17:22:08.653 
Epoch 529/1000 
	 loss: 43.9652, MinusLogProbMetric: 43.9652, val_loss: 43.9010, val_MinusLogProbMetric: 43.9010

Epoch 529: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.9652 - MinusLogProbMetric: 43.9652 - val_loss: 43.9010 - val_MinusLogProbMetric: 43.9010 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 530/1000
2023-10-27 17:22:42.459 
Epoch 530/1000 
	 loss: 43.7837, MinusLogProbMetric: 43.7837, val_loss: 44.1084, val_MinusLogProbMetric: 44.1084

Epoch 530: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.7837 - MinusLogProbMetric: 43.7837 - val_loss: 44.1084 - val_MinusLogProbMetric: 44.1084 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 531/1000
2023-10-27 17:23:16.413 
Epoch 531/1000 
	 loss: 43.9160, MinusLogProbMetric: 43.9160, val_loss: 46.5143, val_MinusLogProbMetric: 46.5143

Epoch 531: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.9160 - MinusLogProbMetric: 43.9160 - val_loss: 46.5143 - val_MinusLogProbMetric: 46.5143 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 532/1000
2023-10-27 17:23:50.058 
Epoch 532/1000 
	 loss: 43.7308, MinusLogProbMetric: 43.7308, val_loss: 46.0013, val_MinusLogProbMetric: 46.0013

Epoch 532: val_loss did not improve from 43.57606
196/196 - 34s - loss: 43.7308 - MinusLogProbMetric: 43.7308 - val_loss: 46.0013 - val_MinusLogProbMetric: 46.0013 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 533/1000
2023-10-27 17:24:23.535 
Epoch 533/1000 
	 loss: 43.6354, MinusLogProbMetric: 43.6354, val_loss: 44.2684, val_MinusLogProbMetric: 44.2684

Epoch 533: val_loss did not improve from 43.57606
196/196 - 33s - loss: 43.6354 - MinusLogProbMetric: 43.6354 - val_loss: 44.2684 - val_MinusLogProbMetric: 44.2684 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 534/1000
2023-10-27 17:24:57.197 
Epoch 534/1000 
	 loss: 43.7400, MinusLogProbMetric: 43.7400, val_loss: 43.5261, val_MinusLogProbMetric: 43.5261

Epoch 534: val_loss improved from 43.57606 to 43.52615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 43.7400 - MinusLogProbMetric: 43.7400 - val_loss: 43.5261 - val_MinusLogProbMetric: 43.5261 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 535/1000
2023-10-27 17:25:30.704 
Epoch 535/1000 
	 loss: 43.8808, MinusLogProbMetric: 43.8808, val_loss: 44.4916, val_MinusLogProbMetric: 44.4916

Epoch 535: val_loss did not improve from 43.52615
196/196 - 33s - loss: 43.8808 - MinusLogProbMetric: 43.8808 - val_loss: 44.4916 - val_MinusLogProbMetric: 44.4916 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 536/1000
2023-10-27 17:26:04.524 
Epoch 536/1000 
	 loss: 43.5884, MinusLogProbMetric: 43.5884, val_loss: 43.6343, val_MinusLogProbMetric: 43.6343

Epoch 536: val_loss did not improve from 43.52615
196/196 - 34s - loss: 43.5884 - MinusLogProbMetric: 43.5884 - val_loss: 43.6343 - val_MinusLogProbMetric: 43.6343 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 537/1000
2023-10-27 17:26:37.748 
Epoch 537/1000 
	 loss: 43.7557, MinusLogProbMetric: 43.7557, val_loss: 44.3082, val_MinusLogProbMetric: 44.3082

Epoch 537: val_loss did not improve from 43.52615
196/196 - 33s - loss: 43.7557 - MinusLogProbMetric: 43.7557 - val_loss: 44.3082 - val_MinusLogProbMetric: 44.3082 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 538/1000
2023-10-27 17:27:11.476 
Epoch 538/1000 
	 loss: 43.6017, MinusLogProbMetric: 43.6017, val_loss: 46.3252, val_MinusLogProbMetric: 46.3252

Epoch 538: val_loss did not improve from 43.52615
196/196 - 34s - loss: 43.6017 - MinusLogProbMetric: 43.6017 - val_loss: 46.3252 - val_MinusLogProbMetric: 46.3252 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 539/1000
2023-10-27 17:27:44.672 
Epoch 539/1000 
	 loss: 43.6066, MinusLogProbMetric: 43.6066, val_loss: 44.9032, val_MinusLogProbMetric: 44.9032

Epoch 539: val_loss did not improve from 43.52615
196/196 - 33s - loss: 43.6066 - MinusLogProbMetric: 43.6066 - val_loss: 44.9032 - val_MinusLogProbMetric: 44.9032 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 540/1000
2023-10-27 17:28:17.511 
Epoch 540/1000 
	 loss: 43.9103, MinusLogProbMetric: 43.9103, val_loss: 45.2847, val_MinusLogProbMetric: 45.2847

Epoch 540: val_loss did not improve from 43.52615
196/196 - 33s - loss: 43.9103 - MinusLogProbMetric: 43.9103 - val_loss: 45.2847 - val_MinusLogProbMetric: 45.2847 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 541/1000
2023-10-27 17:28:51.306 
Epoch 541/1000 
	 loss: 43.9053, MinusLogProbMetric: 43.9053, val_loss: 44.0596, val_MinusLogProbMetric: 44.0596

Epoch 541: val_loss did not improve from 43.52615
196/196 - 34s - loss: 43.9053 - MinusLogProbMetric: 43.9053 - val_loss: 44.0596 - val_MinusLogProbMetric: 44.0596 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 542/1000
2023-10-27 17:29:24.223 
Epoch 542/1000 
	 loss: 43.8185, MinusLogProbMetric: 43.8185, val_loss: 43.5980, val_MinusLogProbMetric: 43.5980

Epoch 542: val_loss did not improve from 43.52615
196/196 - 33s - loss: 43.8185 - MinusLogProbMetric: 43.8185 - val_loss: 43.5980 - val_MinusLogProbMetric: 43.5980 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 543/1000
2023-10-27 17:29:57.304 
Epoch 543/1000 
	 loss: 44.4421, MinusLogProbMetric: 44.4421, val_loss: 47.1647, val_MinusLogProbMetric: 47.1647

Epoch 543: val_loss did not improve from 43.52615
196/196 - 33s - loss: 44.4421 - MinusLogProbMetric: 44.4421 - val_loss: 47.1647 - val_MinusLogProbMetric: 47.1647 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 544/1000
2023-10-27 17:30:30.908 
Epoch 544/1000 
	 loss: 43.7752, MinusLogProbMetric: 43.7752, val_loss: 43.4104, val_MinusLogProbMetric: 43.4104

Epoch 544: val_loss improved from 43.52615 to 43.41037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 43.7752 - MinusLogProbMetric: 43.7752 - val_loss: 43.4104 - val_MinusLogProbMetric: 43.4104 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 545/1000
2023-10-27 17:31:04.839 
Epoch 545/1000 
	 loss: 43.6286, MinusLogProbMetric: 43.6286, val_loss: 44.5222, val_MinusLogProbMetric: 44.5222

Epoch 545: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.6286 - MinusLogProbMetric: 43.6286 - val_loss: 44.5222 - val_MinusLogProbMetric: 44.5222 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 546/1000
2023-10-27 17:31:38.092 
Epoch 546/1000 
	 loss: 43.7487, MinusLogProbMetric: 43.7487, val_loss: 44.4644, val_MinusLogProbMetric: 44.4644

Epoch 546: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.7487 - MinusLogProbMetric: 43.7487 - val_loss: 44.4644 - val_MinusLogProbMetric: 44.4644 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 547/1000
2023-10-27 17:32:11.677 
Epoch 547/1000 
	 loss: 43.8476, MinusLogProbMetric: 43.8476, val_loss: 44.0631, val_MinusLogProbMetric: 44.0631

Epoch 547: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.8476 - MinusLogProbMetric: 43.8476 - val_loss: 44.0631 - val_MinusLogProbMetric: 44.0631 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 548/1000
2023-10-27 17:32:44.894 
Epoch 548/1000 
	 loss: 43.5975, MinusLogProbMetric: 43.5975, val_loss: 44.6869, val_MinusLogProbMetric: 44.6869

Epoch 548: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.5975 - MinusLogProbMetric: 43.5975 - val_loss: 44.6869 - val_MinusLogProbMetric: 44.6869 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 549/1000
2023-10-27 17:33:18.506 
Epoch 549/1000 
	 loss: 44.0524, MinusLogProbMetric: 44.0524, val_loss: 44.2719, val_MinusLogProbMetric: 44.2719

Epoch 549: val_loss did not improve from 43.41037
196/196 - 34s - loss: 44.0524 - MinusLogProbMetric: 44.0524 - val_loss: 44.2719 - val_MinusLogProbMetric: 44.2719 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 550/1000
2023-10-27 17:33:52.282 
Epoch 550/1000 
	 loss: 43.7633, MinusLogProbMetric: 43.7633, val_loss: 44.8256, val_MinusLogProbMetric: 44.8256

Epoch 550: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.7633 - MinusLogProbMetric: 43.7633 - val_loss: 44.8256 - val_MinusLogProbMetric: 44.8256 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 551/1000
2023-10-27 17:34:25.950 
Epoch 551/1000 
	 loss: 43.5570, MinusLogProbMetric: 43.5570, val_loss: 43.9806, val_MinusLogProbMetric: 43.9806

Epoch 551: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.5570 - MinusLogProbMetric: 43.5570 - val_loss: 43.9806 - val_MinusLogProbMetric: 43.9806 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 552/1000
2023-10-27 17:34:59.518 
Epoch 552/1000 
	 loss: 43.3767, MinusLogProbMetric: 43.3767, val_loss: 44.3896, val_MinusLogProbMetric: 44.3896

Epoch 552: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.3767 - MinusLogProbMetric: 43.3767 - val_loss: 44.3896 - val_MinusLogProbMetric: 44.3896 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 553/1000
2023-10-27 17:35:33.176 
Epoch 553/1000 
	 loss: 43.7046, MinusLogProbMetric: 43.7046, val_loss: 44.8826, val_MinusLogProbMetric: 44.8826

Epoch 553: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.7046 - MinusLogProbMetric: 43.7046 - val_loss: 44.8826 - val_MinusLogProbMetric: 44.8826 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 554/1000
2023-10-27 17:36:06.978 
Epoch 554/1000 
	 loss: 43.8368, MinusLogProbMetric: 43.8368, val_loss: 45.0744, val_MinusLogProbMetric: 45.0744

Epoch 554: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.8368 - MinusLogProbMetric: 43.8368 - val_loss: 45.0744 - val_MinusLogProbMetric: 45.0744 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 555/1000
2023-10-27 17:36:40.311 
Epoch 555/1000 
	 loss: 43.4147, MinusLogProbMetric: 43.4147, val_loss: 43.9995, val_MinusLogProbMetric: 43.9995

Epoch 555: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.4147 - MinusLogProbMetric: 43.4147 - val_loss: 43.9995 - val_MinusLogProbMetric: 43.9995 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 556/1000
2023-10-27 17:37:13.756 
Epoch 556/1000 
	 loss: 43.9120, MinusLogProbMetric: 43.9120, val_loss: 45.3311, val_MinusLogProbMetric: 45.3311

Epoch 556: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.9120 - MinusLogProbMetric: 43.9120 - val_loss: 45.3311 - val_MinusLogProbMetric: 45.3311 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 557/1000
2023-10-27 17:37:47.857 
Epoch 557/1000 
	 loss: 43.8252, MinusLogProbMetric: 43.8252, val_loss: 45.3078, val_MinusLogProbMetric: 45.3078

Epoch 557: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.8252 - MinusLogProbMetric: 43.8252 - val_loss: 45.3078 - val_MinusLogProbMetric: 45.3078 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 558/1000
2023-10-27 17:38:21.387 
Epoch 558/1000 
	 loss: 43.7077, MinusLogProbMetric: 43.7077, val_loss: 43.9749, val_MinusLogProbMetric: 43.9749

Epoch 558: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.7077 - MinusLogProbMetric: 43.7077 - val_loss: 43.9749 - val_MinusLogProbMetric: 43.9749 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 559/1000
2023-10-27 17:38:48.469 
Epoch 559/1000 
	 loss: 43.5075, MinusLogProbMetric: 43.5075, val_loss: 45.3766, val_MinusLogProbMetric: 45.3766

Epoch 559: val_loss did not improve from 43.41037
196/196 - 27s - loss: 43.5075 - MinusLogProbMetric: 43.5075 - val_loss: 45.3766 - val_MinusLogProbMetric: 45.3766 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 560/1000
2023-10-27 17:39:15.117 
Epoch 560/1000 
	 loss: 43.8255, MinusLogProbMetric: 43.8255, val_loss: 44.8803, val_MinusLogProbMetric: 44.8803

Epoch 560: val_loss did not improve from 43.41037
196/196 - 27s - loss: 43.8255 - MinusLogProbMetric: 43.8255 - val_loss: 44.8803 - val_MinusLogProbMetric: 44.8803 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 561/1000
2023-10-27 17:39:41.816 
Epoch 561/1000 
	 loss: 43.7466, MinusLogProbMetric: 43.7466, val_loss: 44.0251, val_MinusLogProbMetric: 44.0251

Epoch 561: val_loss did not improve from 43.41037
196/196 - 27s - loss: 43.7466 - MinusLogProbMetric: 43.7466 - val_loss: 44.0251 - val_MinusLogProbMetric: 44.0251 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 562/1000
2023-10-27 17:40:08.413 
Epoch 562/1000 
	 loss: 43.4723, MinusLogProbMetric: 43.4723, val_loss: 43.4113, val_MinusLogProbMetric: 43.4113

Epoch 562: val_loss did not improve from 43.41037
196/196 - 27s - loss: 43.4723 - MinusLogProbMetric: 43.4723 - val_loss: 43.4113 - val_MinusLogProbMetric: 43.4113 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 563/1000
2023-10-27 17:40:36.773 
Epoch 563/1000 
	 loss: 44.1102, MinusLogProbMetric: 44.1102, val_loss: 45.5770, val_MinusLogProbMetric: 45.5770

Epoch 563: val_loss did not improve from 43.41037
196/196 - 28s - loss: 44.1102 - MinusLogProbMetric: 44.1102 - val_loss: 45.5770 - val_MinusLogProbMetric: 45.5770 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 564/1000
2023-10-27 17:41:11.222 
Epoch 564/1000 
	 loss: 43.6517, MinusLogProbMetric: 43.6517, val_loss: 44.7344, val_MinusLogProbMetric: 44.7344

Epoch 564: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.6517 - MinusLogProbMetric: 43.6517 - val_loss: 44.7344 - val_MinusLogProbMetric: 44.7344 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 565/1000
2023-10-27 17:41:45.228 
Epoch 565/1000 
	 loss: 43.7414, MinusLogProbMetric: 43.7414, val_loss: 43.4715, val_MinusLogProbMetric: 43.4715

Epoch 565: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.7414 - MinusLogProbMetric: 43.7414 - val_loss: 43.4715 - val_MinusLogProbMetric: 43.4715 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 566/1000
2023-10-27 17:42:19.271 
Epoch 566/1000 
	 loss: 43.5283, MinusLogProbMetric: 43.5283, val_loss: 45.9290, val_MinusLogProbMetric: 45.9290

Epoch 566: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.5283 - MinusLogProbMetric: 43.5283 - val_loss: 45.9290 - val_MinusLogProbMetric: 45.9290 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 567/1000
2023-10-27 17:42:53.224 
Epoch 567/1000 
	 loss: 43.6881, MinusLogProbMetric: 43.6881, val_loss: 43.8310, val_MinusLogProbMetric: 43.8310

Epoch 567: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.6881 - MinusLogProbMetric: 43.6881 - val_loss: 43.8310 - val_MinusLogProbMetric: 43.8310 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 568/1000
2023-10-27 17:43:26.976 
Epoch 568/1000 
	 loss: 43.6781, MinusLogProbMetric: 43.6781, val_loss: 44.0586, val_MinusLogProbMetric: 44.0586

Epoch 568: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.6781 - MinusLogProbMetric: 43.6781 - val_loss: 44.0586 - val_MinusLogProbMetric: 44.0586 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 569/1000
2023-10-27 17:44:00.823 
Epoch 569/1000 
	 loss: 43.8710, MinusLogProbMetric: 43.8710, val_loss: 45.8370, val_MinusLogProbMetric: 45.8370

Epoch 569: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.8710 - MinusLogProbMetric: 43.8710 - val_loss: 45.8370 - val_MinusLogProbMetric: 45.8370 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 570/1000
2023-10-27 17:44:34.027 
Epoch 570/1000 
	 loss: 43.8521, MinusLogProbMetric: 43.8521, val_loss: 44.0074, val_MinusLogProbMetric: 44.0074

Epoch 570: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.8521 - MinusLogProbMetric: 43.8521 - val_loss: 44.0074 - val_MinusLogProbMetric: 44.0074 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 571/1000
2023-10-27 17:45:07.795 
Epoch 571/1000 
	 loss: 43.4576, MinusLogProbMetric: 43.4576, val_loss: 43.5653, val_MinusLogProbMetric: 43.5653

Epoch 571: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.4576 - MinusLogProbMetric: 43.4576 - val_loss: 43.5653 - val_MinusLogProbMetric: 43.5653 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 572/1000
2023-10-27 17:45:41.777 
Epoch 572/1000 
	 loss: 43.7252, MinusLogProbMetric: 43.7252, val_loss: 44.0266, val_MinusLogProbMetric: 44.0266

Epoch 572: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.7252 - MinusLogProbMetric: 43.7252 - val_loss: 44.0266 - val_MinusLogProbMetric: 44.0266 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 573/1000
2023-10-27 17:46:15.195 
Epoch 573/1000 
	 loss: 43.4651, MinusLogProbMetric: 43.4651, val_loss: 44.3796, val_MinusLogProbMetric: 44.3796

Epoch 573: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.4651 - MinusLogProbMetric: 43.4651 - val_loss: 44.3796 - val_MinusLogProbMetric: 44.3796 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 574/1000
2023-10-27 17:46:48.806 
Epoch 574/1000 
	 loss: 43.6009, MinusLogProbMetric: 43.6009, val_loss: 44.2223, val_MinusLogProbMetric: 44.2223

Epoch 574: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.6009 - MinusLogProbMetric: 43.6009 - val_loss: 44.2223 - val_MinusLogProbMetric: 44.2223 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 575/1000
2023-10-27 17:47:21.933 
Epoch 575/1000 
	 loss: 43.5464, MinusLogProbMetric: 43.5464, val_loss: 44.0550, val_MinusLogProbMetric: 44.0550

Epoch 575: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.5464 - MinusLogProbMetric: 43.5464 - val_loss: 44.0550 - val_MinusLogProbMetric: 44.0550 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 576/1000
2023-10-27 17:47:54.974 
Epoch 576/1000 
	 loss: 43.4436, MinusLogProbMetric: 43.4436, val_loss: 43.4778, val_MinusLogProbMetric: 43.4778

Epoch 576: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.4436 - MinusLogProbMetric: 43.4436 - val_loss: 43.4778 - val_MinusLogProbMetric: 43.4778 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 577/1000
2023-10-27 17:48:23.816 
Epoch 577/1000 
	 loss: 43.5802, MinusLogProbMetric: 43.5802, val_loss: 43.6450, val_MinusLogProbMetric: 43.6450

Epoch 577: val_loss did not improve from 43.41037
196/196 - 29s - loss: 43.5802 - MinusLogProbMetric: 43.5802 - val_loss: 43.6450 - val_MinusLogProbMetric: 43.6450 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 578/1000
2023-10-27 17:48:53.813 
Epoch 578/1000 
	 loss: 43.6229, MinusLogProbMetric: 43.6229, val_loss: 44.0919, val_MinusLogProbMetric: 44.0919

Epoch 578: val_loss did not improve from 43.41037
196/196 - 30s - loss: 43.6229 - MinusLogProbMetric: 43.6229 - val_loss: 44.0919 - val_MinusLogProbMetric: 44.0919 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 579/1000
2023-10-27 17:49:22.549 
Epoch 579/1000 
	 loss: 43.4551, MinusLogProbMetric: 43.4551, val_loss: 44.0365, val_MinusLogProbMetric: 44.0365

Epoch 579: val_loss did not improve from 43.41037
196/196 - 29s - loss: 43.4551 - MinusLogProbMetric: 43.4551 - val_loss: 44.0365 - val_MinusLogProbMetric: 44.0365 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 580/1000
2023-10-27 17:49:53.247 
Epoch 580/1000 
	 loss: 43.3880, MinusLogProbMetric: 43.3880, val_loss: 43.7030, val_MinusLogProbMetric: 43.7030

Epoch 580: val_loss did not improve from 43.41037
196/196 - 31s - loss: 43.3880 - MinusLogProbMetric: 43.3880 - val_loss: 43.7030 - val_MinusLogProbMetric: 43.7030 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 581/1000
2023-10-27 17:50:24.801 
Epoch 581/1000 
	 loss: 44.1828, MinusLogProbMetric: 44.1828, val_loss: 43.7643, val_MinusLogProbMetric: 43.7643

Epoch 581: val_loss did not improve from 43.41037
196/196 - 32s - loss: 44.1828 - MinusLogProbMetric: 44.1828 - val_loss: 43.7643 - val_MinusLogProbMetric: 43.7643 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 582/1000
2023-10-27 17:50:57.962 
Epoch 582/1000 
	 loss: 43.6009, MinusLogProbMetric: 43.6009, val_loss: 43.8451, val_MinusLogProbMetric: 43.8451

Epoch 582: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.6009 - MinusLogProbMetric: 43.6009 - val_loss: 43.8451 - val_MinusLogProbMetric: 43.8451 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 583/1000
2023-10-27 17:51:31.655 
Epoch 583/1000 
	 loss: 43.6838, MinusLogProbMetric: 43.6838, val_loss: 43.8254, val_MinusLogProbMetric: 43.8254

Epoch 583: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.6838 - MinusLogProbMetric: 43.6838 - val_loss: 43.8254 - val_MinusLogProbMetric: 43.8254 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 584/1000
2023-10-27 17:52:05.502 
Epoch 584/1000 
	 loss: 43.5568, MinusLogProbMetric: 43.5568, val_loss: 43.8579, val_MinusLogProbMetric: 43.8579

Epoch 584: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.5568 - MinusLogProbMetric: 43.5568 - val_loss: 43.8579 - val_MinusLogProbMetric: 43.8579 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 585/1000
2023-10-27 17:52:39.146 
Epoch 585/1000 
	 loss: 43.3115, MinusLogProbMetric: 43.3115, val_loss: 43.6471, val_MinusLogProbMetric: 43.6471

Epoch 585: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.3115 - MinusLogProbMetric: 43.3115 - val_loss: 43.6471 - val_MinusLogProbMetric: 43.6471 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 586/1000
2023-10-27 17:53:12.575 
Epoch 586/1000 
	 loss: 43.6082, MinusLogProbMetric: 43.6082, val_loss: 44.2456, val_MinusLogProbMetric: 44.2456

Epoch 586: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.6082 - MinusLogProbMetric: 43.6082 - val_loss: 44.2456 - val_MinusLogProbMetric: 44.2456 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 587/1000
2023-10-27 17:53:45.355 
Epoch 587/1000 
	 loss: 43.4468, MinusLogProbMetric: 43.4468, val_loss: 43.5875, val_MinusLogProbMetric: 43.5875

Epoch 587: val_loss did not improve from 43.41037
196/196 - 33s - loss: 43.4468 - MinusLogProbMetric: 43.4468 - val_loss: 43.5875 - val_MinusLogProbMetric: 43.5875 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 588/1000
2023-10-27 17:54:19.112 
Epoch 588/1000 
	 loss: 43.3827, MinusLogProbMetric: 43.3827, val_loss: 45.4165, val_MinusLogProbMetric: 45.4165

Epoch 588: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.3827 - MinusLogProbMetric: 43.3827 - val_loss: 45.4165 - val_MinusLogProbMetric: 45.4165 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 589/1000
2023-10-27 17:54:53.006 
Epoch 589/1000 
	 loss: 43.5698, MinusLogProbMetric: 43.5698, val_loss: 45.7585, val_MinusLogProbMetric: 45.7585

Epoch 589: val_loss did not improve from 43.41037
196/196 - 34s - loss: 43.5698 - MinusLogProbMetric: 43.5698 - val_loss: 45.7585 - val_MinusLogProbMetric: 45.7585 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 590/1000
2023-10-27 17:55:26.801 
Epoch 590/1000 
	 loss: 43.4132, MinusLogProbMetric: 43.4132, val_loss: 43.2286, val_MinusLogProbMetric: 43.2286

Epoch 590: val_loss improved from 43.41037 to 43.22861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 43.4132 - MinusLogProbMetric: 43.4132 - val_loss: 43.2286 - val_MinusLogProbMetric: 43.2286 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 591/1000
2023-10-27 17:55:59.167 
Epoch 591/1000 
	 loss: 43.8775, MinusLogProbMetric: 43.8775, val_loss: 43.7880, val_MinusLogProbMetric: 43.7880

Epoch 591: val_loss did not improve from 43.22861
196/196 - 32s - loss: 43.8775 - MinusLogProbMetric: 43.8775 - val_loss: 43.7880 - val_MinusLogProbMetric: 43.7880 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 592/1000
2023-10-27 17:56:32.079 
Epoch 592/1000 
	 loss: 43.6629, MinusLogProbMetric: 43.6629, val_loss: 43.4822, val_MinusLogProbMetric: 43.4822

Epoch 592: val_loss did not improve from 43.22861
196/196 - 33s - loss: 43.6629 - MinusLogProbMetric: 43.6629 - val_loss: 43.4822 - val_MinusLogProbMetric: 43.4822 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 593/1000
2023-10-27 17:57:04.716 
Epoch 593/1000 
	 loss: 43.6337, MinusLogProbMetric: 43.6337, val_loss: 43.6526, val_MinusLogProbMetric: 43.6526

Epoch 593: val_loss did not improve from 43.22861
196/196 - 33s - loss: 43.6337 - MinusLogProbMetric: 43.6337 - val_loss: 43.6526 - val_MinusLogProbMetric: 43.6526 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 594/1000
2023-10-27 17:57:38.106 
Epoch 594/1000 
	 loss: 43.2018, MinusLogProbMetric: 43.2018, val_loss: 43.6070, val_MinusLogProbMetric: 43.6070

Epoch 594: val_loss did not improve from 43.22861
196/196 - 33s - loss: 43.2018 - MinusLogProbMetric: 43.2018 - val_loss: 43.6070 - val_MinusLogProbMetric: 43.6070 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 595/1000
2023-10-27 17:58:11.750 
Epoch 595/1000 
	 loss: 43.4524, MinusLogProbMetric: 43.4524, val_loss: 44.4475, val_MinusLogProbMetric: 44.4475

Epoch 595: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.4524 - MinusLogProbMetric: 43.4524 - val_loss: 44.4475 - val_MinusLogProbMetric: 44.4475 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 596/1000
2023-10-27 17:58:45.069 
Epoch 596/1000 
	 loss: 43.6882, MinusLogProbMetric: 43.6882, val_loss: 44.1144, val_MinusLogProbMetric: 44.1144

Epoch 596: val_loss did not improve from 43.22861
196/196 - 33s - loss: 43.6882 - MinusLogProbMetric: 43.6882 - val_loss: 44.1144 - val_MinusLogProbMetric: 44.1144 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 597/1000
2023-10-27 17:59:18.836 
Epoch 597/1000 
	 loss: 43.1995, MinusLogProbMetric: 43.1995, val_loss: 43.8411, val_MinusLogProbMetric: 43.8411

Epoch 597: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.1995 - MinusLogProbMetric: 43.1995 - val_loss: 43.8411 - val_MinusLogProbMetric: 43.8411 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 598/1000
2023-10-27 17:59:52.718 
Epoch 598/1000 
	 loss: 43.5673, MinusLogProbMetric: 43.5673, val_loss: 45.9760, val_MinusLogProbMetric: 45.9760

Epoch 598: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.5673 - MinusLogProbMetric: 43.5673 - val_loss: 45.9760 - val_MinusLogProbMetric: 45.9760 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 599/1000
2023-10-27 18:00:26.138 
Epoch 599/1000 
	 loss: 43.8137, MinusLogProbMetric: 43.8137, val_loss: 43.6493, val_MinusLogProbMetric: 43.6493

Epoch 599: val_loss did not improve from 43.22861
196/196 - 33s - loss: 43.8137 - MinusLogProbMetric: 43.8137 - val_loss: 43.6493 - val_MinusLogProbMetric: 43.6493 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 600/1000
2023-10-27 18:01:00.209 
Epoch 600/1000 
	 loss: 43.3402, MinusLogProbMetric: 43.3402, val_loss: 43.2358, val_MinusLogProbMetric: 43.2358

Epoch 600: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.3402 - MinusLogProbMetric: 43.3402 - val_loss: 43.2358 - val_MinusLogProbMetric: 43.2358 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 601/1000
2023-10-27 18:01:33.893 
Epoch 601/1000 
	 loss: 43.7775, MinusLogProbMetric: 43.7775, val_loss: 44.3917, val_MinusLogProbMetric: 44.3917

Epoch 601: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.7775 - MinusLogProbMetric: 43.7775 - val_loss: 44.3917 - val_MinusLogProbMetric: 44.3917 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 602/1000
2023-10-27 18:02:07.626 
Epoch 602/1000 
	 loss: 43.4316, MinusLogProbMetric: 43.4316, val_loss: 44.3015, val_MinusLogProbMetric: 44.3015

Epoch 602: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.4316 - MinusLogProbMetric: 43.4316 - val_loss: 44.3015 - val_MinusLogProbMetric: 44.3015 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 603/1000
2023-10-27 18:02:41.135 
Epoch 603/1000 
	 loss: 43.6169, MinusLogProbMetric: 43.6169, val_loss: 44.0321, val_MinusLogProbMetric: 44.0321

Epoch 603: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.6169 - MinusLogProbMetric: 43.6169 - val_loss: 44.0321 - val_MinusLogProbMetric: 44.0321 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 604/1000
2023-10-27 18:03:14.522 
Epoch 604/1000 
	 loss: 43.3090, MinusLogProbMetric: 43.3090, val_loss: 44.5655, val_MinusLogProbMetric: 44.5655

Epoch 604: val_loss did not improve from 43.22861
196/196 - 33s - loss: 43.3090 - MinusLogProbMetric: 43.3090 - val_loss: 44.5655 - val_MinusLogProbMetric: 44.5655 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 605/1000
2023-10-27 18:03:48.451 
Epoch 605/1000 
	 loss: 43.5604, MinusLogProbMetric: 43.5604, val_loss: 44.7409, val_MinusLogProbMetric: 44.7409

Epoch 605: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.5604 - MinusLogProbMetric: 43.5604 - val_loss: 44.7409 - val_MinusLogProbMetric: 44.7409 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 606/1000
2023-10-27 18:04:22.391 
Epoch 606/1000 
	 loss: 43.8566, MinusLogProbMetric: 43.8566, val_loss: 45.6322, val_MinusLogProbMetric: 45.6322

Epoch 606: val_loss did not improve from 43.22861
196/196 - 34s - loss: 43.8566 - MinusLogProbMetric: 43.8566 - val_loss: 45.6322 - val_MinusLogProbMetric: 45.6322 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 607/1000
2023-10-27 18:04:56.162 
Epoch 607/1000 
	 loss: 43.1111, MinusLogProbMetric: 43.1111, val_loss: 43.1266, val_MinusLogProbMetric: 43.1266

Epoch 607: val_loss improved from 43.22861 to 43.12660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 43.1111 - MinusLogProbMetric: 43.1111 - val_loss: 43.1266 - val_MinusLogProbMetric: 43.1266 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 608/1000
2023-10-27 18:05:30.399 
Epoch 608/1000 
	 loss: 43.7339, MinusLogProbMetric: 43.7339, val_loss: 43.8755, val_MinusLogProbMetric: 43.8755

Epoch 608: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.7339 - MinusLogProbMetric: 43.7339 - val_loss: 43.8755 - val_MinusLogProbMetric: 43.8755 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 609/1000
2023-10-27 18:06:04.073 
Epoch 609/1000 
	 loss: 43.5895, MinusLogProbMetric: 43.5895, val_loss: 45.1287, val_MinusLogProbMetric: 45.1287

Epoch 609: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.5895 - MinusLogProbMetric: 43.5895 - val_loss: 45.1287 - val_MinusLogProbMetric: 45.1287 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 610/1000
2023-10-27 18:06:37.587 
Epoch 610/1000 
	 loss: 43.2801, MinusLogProbMetric: 43.2801, val_loss: 43.4904, val_MinusLogProbMetric: 43.4904

Epoch 610: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.2801 - MinusLogProbMetric: 43.2801 - val_loss: 43.4904 - val_MinusLogProbMetric: 43.4904 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 611/1000
2023-10-27 18:07:11.410 
Epoch 611/1000 
	 loss: 43.4162, MinusLogProbMetric: 43.4162, val_loss: 46.6676, val_MinusLogProbMetric: 46.6676

Epoch 611: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.4162 - MinusLogProbMetric: 43.4162 - val_loss: 46.6676 - val_MinusLogProbMetric: 46.6676 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 612/1000
2023-10-27 18:07:45.349 
Epoch 612/1000 
	 loss: 43.4633, MinusLogProbMetric: 43.4633, val_loss: 43.5315, val_MinusLogProbMetric: 43.5315

Epoch 612: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.4633 - MinusLogProbMetric: 43.4633 - val_loss: 43.5315 - val_MinusLogProbMetric: 43.5315 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 613/1000
2023-10-27 18:08:19.434 
Epoch 613/1000 
	 loss: 43.6174, MinusLogProbMetric: 43.6174, val_loss: 43.3130, val_MinusLogProbMetric: 43.3130

Epoch 613: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.6174 - MinusLogProbMetric: 43.6174 - val_loss: 43.3130 - val_MinusLogProbMetric: 43.3130 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 614/1000
2023-10-27 18:08:53.503 
Epoch 614/1000 
	 loss: 43.0971, MinusLogProbMetric: 43.0971, val_loss: 43.1832, val_MinusLogProbMetric: 43.1832

Epoch 614: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.0971 - MinusLogProbMetric: 43.0971 - val_loss: 43.1832 - val_MinusLogProbMetric: 43.1832 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 615/1000
2023-10-27 18:09:27.346 
Epoch 615/1000 
	 loss: 43.5146, MinusLogProbMetric: 43.5146, val_loss: 45.0032, val_MinusLogProbMetric: 45.0032

Epoch 615: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.5146 - MinusLogProbMetric: 43.5146 - val_loss: 45.0032 - val_MinusLogProbMetric: 45.0032 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 616/1000
2023-10-27 18:10:01.253 
Epoch 616/1000 
	 loss: 43.5230, MinusLogProbMetric: 43.5230, val_loss: 44.4417, val_MinusLogProbMetric: 44.4417

Epoch 616: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.5230 - MinusLogProbMetric: 43.5230 - val_loss: 44.4417 - val_MinusLogProbMetric: 44.4417 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 617/1000
2023-10-27 18:10:35.007 
Epoch 617/1000 
	 loss: 43.2624, MinusLogProbMetric: 43.2624, val_loss: 43.7464, val_MinusLogProbMetric: 43.7464

Epoch 617: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.2624 - MinusLogProbMetric: 43.2624 - val_loss: 43.7464 - val_MinusLogProbMetric: 43.7464 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 618/1000
2023-10-27 18:11:08.636 
Epoch 618/1000 
	 loss: 43.3350, MinusLogProbMetric: 43.3350, val_loss: 43.7479, val_MinusLogProbMetric: 43.7479

Epoch 618: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.3350 - MinusLogProbMetric: 43.3350 - val_loss: 43.7479 - val_MinusLogProbMetric: 43.7479 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 619/1000
2023-10-27 18:11:41.837 
Epoch 619/1000 
	 loss: 43.3879, MinusLogProbMetric: 43.3879, val_loss: 43.9281, val_MinusLogProbMetric: 43.9281

Epoch 619: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.3879 - MinusLogProbMetric: 43.3879 - val_loss: 43.9281 - val_MinusLogProbMetric: 43.9281 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 620/1000
2023-10-27 18:12:15.030 
Epoch 620/1000 
	 loss: 43.2543, MinusLogProbMetric: 43.2543, val_loss: 43.1578, val_MinusLogProbMetric: 43.1578

Epoch 620: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.2543 - MinusLogProbMetric: 43.2543 - val_loss: 43.1578 - val_MinusLogProbMetric: 43.1578 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 621/1000
2023-10-27 18:12:49.279 
Epoch 621/1000 
	 loss: 43.6011, MinusLogProbMetric: 43.6011, val_loss: 43.5821, val_MinusLogProbMetric: 43.5821

Epoch 621: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.6011 - MinusLogProbMetric: 43.6011 - val_loss: 43.5821 - val_MinusLogProbMetric: 43.5821 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 622/1000
2023-10-27 18:13:23.444 
Epoch 622/1000 
	 loss: 42.9036, MinusLogProbMetric: 42.9036, val_loss: 43.2896, val_MinusLogProbMetric: 43.2896

Epoch 622: val_loss did not improve from 43.12660
196/196 - 34s - loss: 42.9036 - MinusLogProbMetric: 42.9036 - val_loss: 43.2896 - val_MinusLogProbMetric: 43.2896 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 623/1000
2023-10-27 18:13:56.849 
Epoch 623/1000 
	 loss: 43.4692, MinusLogProbMetric: 43.4692, val_loss: 45.5944, val_MinusLogProbMetric: 45.5944

Epoch 623: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.4692 - MinusLogProbMetric: 43.4692 - val_loss: 45.5944 - val_MinusLogProbMetric: 45.5944 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 624/1000
2023-10-27 18:14:29.739 
Epoch 624/1000 
	 loss: 43.5642, MinusLogProbMetric: 43.5642, val_loss: 43.6670, val_MinusLogProbMetric: 43.6670

Epoch 624: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.5642 - MinusLogProbMetric: 43.5642 - val_loss: 43.6670 - val_MinusLogProbMetric: 43.6670 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 625/1000
2023-10-27 18:15:02.690 
Epoch 625/1000 
	 loss: 43.0933, MinusLogProbMetric: 43.0933, val_loss: 43.4110, val_MinusLogProbMetric: 43.4110

Epoch 625: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.0933 - MinusLogProbMetric: 43.0933 - val_loss: 43.4110 - val_MinusLogProbMetric: 43.4110 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 626/1000
2023-10-27 18:15:36.298 
Epoch 626/1000 
	 loss: 43.2187, MinusLogProbMetric: 43.2187, val_loss: 43.4128, val_MinusLogProbMetric: 43.4128

Epoch 626: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.2187 - MinusLogProbMetric: 43.2187 - val_loss: 43.4128 - val_MinusLogProbMetric: 43.4128 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 627/1000
2023-10-27 18:16:10.110 
Epoch 627/1000 
	 loss: 43.3981, MinusLogProbMetric: 43.3981, val_loss: 45.1810, val_MinusLogProbMetric: 45.1810

Epoch 627: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.3981 - MinusLogProbMetric: 43.3981 - val_loss: 45.1810 - val_MinusLogProbMetric: 45.1810 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 628/1000
2023-10-27 18:16:42.826 
Epoch 628/1000 
	 loss: 43.2282, MinusLogProbMetric: 43.2282, val_loss: 45.1662, val_MinusLogProbMetric: 45.1662

Epoch 628: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.2282 - MinusLogProbMetric: 43.2282 - val_loss: 45.1662 - val_MinusLogProbMetric: 45.1662 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 629/1000
2023-10-27 18:17:16.700 
Epoch 629/1000 
	 loss: 43.1707, MinusLogProbMetric: 43.1707, val_loss: 43.6045, val_MinusLogProbMetric: 43.6045

Epoch 629: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.1707 - MinusLogProbMetric: 43.1707 - val_loss: 43.6045 - val_MinusLogProbMetric: 43.6045 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 630/1000
2023-10-27 18:17:49.978 
Epoch 630/1000 
	 loss: 43.2287, MinusLogProbMetric: 43.2287, val_loss: 43.6679, val_MinusLogProbMetric: 43.6679

Epoch 630: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.2287 - MinusLogProbMetric: 43.2287 - val_loss: 43.6679 - val_MinusLogProbMetric: 43.6679 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 631/1000
2023-10-27 18:18:23.720 
Epoch 631/1000 
	 loss: 43.6177, MinusLogProbMetric: 43.6177, val_loss: 43.8908, val_MinusLogProbMetric: 43.8908

Epoch 631: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.6177 - MinusLogProbMetric: 43.6177 - val_loss: 43.8908 - val_MinusLogProbMetric: 43.8908 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 632/1000
2023-10-27 18:18:57.386 
Epoch 632/1000 
	 loss: 43.8260, MinusLogProbMetric: 43.8260, val_loss: 43.4727, val_MinusLogProbMetric: 43.4727

Epoch 632: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.8260 - MinusLogProbMetric: 43.8260 - val_loss: 43.4727 - val_MinusLogProbMetric: 43.4727 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 633/1000
2023-10-27 18:19:31.122 
Epoch 633/1000 
	 loss: 43.3669, MinusLogProbMetric: 43.3669, val_loss: 43.3457, val_MinusLogProbMetric: 43.3457

Epoch 633: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.3669 - MinusLogProbMetric: 43.3669 - val_loss: 43.3457 - val_MinusLogProbMetric: 43.3457 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 634/1000
2023-10-27 18:20:04.585 
Epoch 634/1000 
	 loss: 43.6186, MinusLogProbMetric: 43.6186, val_loss: 45.1356, val_MinusLogProbMetric: 45.1356

Epoch 634: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.6186 - MinusLogProbMetric: 43.6186 - val_loss: 45.1356 - val_MinusLogProbMetric: 45.1356 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 635/1000
2023-10-27 18:20:38.136 
Epoch 635/1000 
	 loss: 43.2035, MinusLogProbMetric: 43.2035, val_loss: 45.3056, val_MinusLogProbMetric: 45.3056

Epoch 635: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.2035 - MinusLogProbMetric: 43.2035 - val_loss: 45.3056 - val_MinusLogProbMetric: 45.3056 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 636/1000
2023-10-27 18:21:11.623 
Epoch 636/1000 
	 loss: 43.4068, MinusLogProbMetric: 43.4068, val_loss: 43.1600, val_MinusLogProbMetric: 43.1600

Epoch 636: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.4068 - MinusLogProbMetric: 43.4068 - val_loss: 43.1600 - val_MinusLogProbMetric: 43.1600 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 637/1000
2023-10-27 18:21:45.046 
Epoch 637/1000 
	 loss: 43.1918, MinusLogProbMetric: 43.1918, val_loss: 43.9105, val_MinusLogProbMetric: 43.9105

Epoch 637: val_loss did not improve from 43.12660
196/196 - 33s - loss: 43.1918 - MinusLogProbMetric: 43.1918 - val_loss: 43.9105 - val_MinusLogProbMetric: 43.9105 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 638/1000
2023-10-27 18:22:18.731 
Epoch 638/1000 
	 loss: 43.1109, MinusLogProbMetric: 43.1109, val_loss: 43.2428, val_MinusLogProbMetric: 43.2428

Epoch 638: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.1109 - MinusLogProbMetric: 43.1109 - val_loss: 43.2428 - val_MinusLogProbMetric: 43.2428 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 639/1000
2023-10-27 18:22:52.262 
Epoch 639/1000 
	 loss: 43.2538, MinusLogProbMetric: 43.2538, val_loss: 45.4652, val_MinusLogProbMetric: 45.4652

Epoch 639: val_loss did not improve from 43.12660
196/196 - 34s - loss: 43.2538 - MinusLogProbMetric: 43.2538 - val_loss: 45.4652 - val_MinusLogProbMetric: 45.4652 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 640/1000
2023-10-27 18:23:26.278 
Epoch 640/1000 
	 loss: 43.3718, MinusLogProbMetric: 43.3718, val_loss: 42.9287, val_MinusLogProbMetric: 42.9287

Epoch 640: val_loss improved from 43.12660 to 42.92867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 43.3718 - MinusLogProbMetric: 43.3718 - val_loss: 42.9287 - val_MinusLogProbMetric: 42.9287 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 641/1000
2023-10-27 18:24:00.608 
Epoch 641/1000 
	 loss: 43.0559, MinusLogProbMetric: 43.0559, val_loss: 43.2100, val_MinusLogProbMetric: 43.2100

Epoch 641: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.0559 - MinusLogProbMetric: 43.0559 - val_loss: 43.2100 - val_MinusLogProbMetric: 43.2100 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 642/1000
2023-10-27 18:24:34.603 
Epoch 642/1000 
	 loss: 43.4845, MinusLogProbMetric: 43.4845, val_loss: 44.7126, val_MinusLogProbMetric: 44.7126

Epoch 642: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.4845 - MinusLogProbMetric: 43.4845 - val_loss: 44.7126 - val_MinusLogProbMetric: 44.7126 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 643/1000
2023-10-27 18:25:08.378 
Epoch 643/1000 
	 loss: 43.1674, MinusLogProbMetric: 43.1674, val_loss: 44.8860, val_MinusLogProbMetric: 44.8860

Epoch 643: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.1674 - MinusLogProbMetric: 43.1674 - val_loss: 44.8860 - val_MinusLogProbMetric: 44.8860 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 644/1000
2023-10-27 18:25:42.132 
Epoch 644/1000 
	 loss: 43.0913, MinusLogProbMetric: 43.0913, val_loss: 43.4282, val_MinusLogProbMetric: 43.4282

Epoch 644: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.0913 - MinusLogProbMetric: 43.0913 - val_loss: 43.4282 - val_MinusLogProbMetric: 43.4282 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 645/1000
2023-10-27 18:26:16.121 
Epoch 645/1000 
	 loss: 43.0165, MinusLogProbMetric: 43.0165, val_loss: 45.8707, val_MinusLogProbMetric: 45.8707

Epoch 645: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.0165 - MinusLogProbMetric: 43.0165 - val_loss: 45.8707 - val_MinusLogProbMetric: 45.8707 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 646/1000
2023-10-27 18:26:50.297 
Epoch 646/1000 
	 loss: 43.3141, MinusLogProbMetric: 43.3141, val_loss: 45.1217, val_MinusLogProbMetric: 45.1217

Epoch 646: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.3141 - MinusLogProbMetric: 43.3141 - val_loss: 45.1217 - val_MinusLogProbMetric: 45.1217 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 647/1000
2023-10-27 18:27:24.138 
Epoch 647/1000 
	 loss: 43.3222, MinusLogProbMetric: 43.3222, val_loss: 44.1676, val_MinusLogProbMetric: 44.1676

Epoch 647: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.3222 - MinusLogProbMetric: 43.3222 - val_loss: 44.1676 - val_MinusLogProbMetric: 44.1676 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 648/1000
2023-10-27 18:27:58.213 
Epoch 648/1000 
	 loss: 43.2077, MinusLogProbMetric: 43.2077, val_loss: 43.4894, val_MinusLogProbMetric: 43.4894

Epoch 648: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.2077 - MinusLogProbMetric: 43.2077 - val_loss: 43.4894 - val_MinusLogProbMetric: 43.4894 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 649/1000
2023-10-27 18:28:31.310 
Epoch 649/1000 
	 loss: 43.2867, MinusLogProbMetric: 43.2867, val_loss: 44.0463, val_MinusLogProbMetric: 44.0463

Epoch 649: val_loss did not improve from 42.92867
196/196 - 33s - loss: 43.2867 - MinusLogProbMetric: 43.2867 - val_loss: 44.0463 - val_MinusLogProbMetric: 44.0463 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 650/1000
2023-10-27 18:29:05.644 
Epoch 650/1000 
	 loss: 42.8951, MinusLogProbMetric: 42.8951, val_loss: 43.1870, val_MinusLogProbMetric: 43.1870

Epoch 650: val_loss did not improve from 42.92867
196/196 - 34s - loss: 42.8951 - MinusLogProbMetric: 42.8951 - val_loss: 43.1870 - val_MinusLogProbMetric: 43.1870 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 651/1000
2023-10-27 18:29:39.720 
Epoch 651/1000 
	 loss: 43.1273, MinusLogProbMetric: 43.1273, val_loss: 43.4187, val_MinusLogProbMetric: 43.4187

Epoch 651: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.1273 - MinusLogProbMetric: 43.1273 - val_loss: 43.4187 - val_MinusLogProbMetric: 43.4187 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 652/1000
2023-10-27 18:30:13.608 
Epoch 652/1000 
	 loss: 43.4321, MinusLogProbMetric: 43.4321, val_loss: 43.1688, val_MinusLogProbMetric: 43.1688

Epoch 652: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.4321 - MinusLogProbMetric: 43.4321 - val_loss: 43.1688 - val_MinusLogProbMetric: 43.1688 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 653/1000
2023-10-27 18:30:47.373 
Epoch 653/1000 
	 loss: 43.1772, MinusLogProbMetric: 43.1772, val_loss: 42.9456, val_MinusLogProbMetric: 42.9456

Epoch 653: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.1772 - MinusLogProbMetric: 43.1772 - val_loss: 42.9456 - val_MinusLogProbMetric: 42.9456 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 654/1000
2023-10-27 18:31:20.707 
Epoch 654/1000 
	 loss: 43.5784, MinusLogProbMetric: 43.5784, val_loss: 44.0314, val_MinusLogProbMetric: 44.0314

Epoch 654: val_loss did not improve from 42.92867
196/196 - 33s - loss: 43.5784 - MinusLogProbMetric: 43.5784 - val_loss: 44.0314 - val_MinusLogProbMetric: 44.0314 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 655/1000
2023-10-27 18:31:54.522 
Epoch 655/1000 
	 loss: 43.0472, MinusLogProbMetric: 43.0472, val_loss: 43.7166, val_MinusLogProbMetric: 43.7166

Epoch 655: val_loss did not improve from 42.92867
196/196 - 34s - loss: 43.0472 - MinusLogProbMetric: 43.0472 - val_loss: 43.7166 - val_MinusLogProbMetric: 43.7166 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 656/1000
2023-10-27 18:32:28.423 
Epoch 656/1000 
	 loss: 43.1108, MinusLogProbMetric: 43.1108, val_loss: 42.8565, val_MinusLogProbMetric: 42.8565

Epoch 656: val_loss improved from 42.92867 to 42.85645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 43.1108 - MinusLogProbMetric: 43.1108 - val_loss: 42.8565 - val_MinusLogProbMetric: 42.8565 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 657/1000
2023-10-27 18:33:02.840 
Epoch 657/1000 
	 loss: 42.9908, MinusLogProbMetric: 42.9908, val_loss: 43.4172, val_MinusLogProbMetric: 43.4172

Epoch 657: val_loss did not improve from 42.85645
196/196 - 34s - loss: 42.9908 - MinusLogProbMetric: 42.9908 - val_loss: 43.4172 - val_MinusLogProbMetric: 43.4172 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 658/1000
2023-10-27 18:33:36.943 
Epoch 658/1000 
	 loss: 43.6877, MinusLogProbMetric: 43.6877, val_loss: 43.0036, val_MinusLogProbMetric: 43.0036

Epoch 658: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.6877 - MinusLogProbMetric: 43.6877 - val_loss: 43.0036 - val_MinusLogProbMetric: 43.0036 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 659/1000
2023-10-27 18:34:10.807 
Epoch 659/1000 
	 loss: 43.1833, MinusLogProbMetric: 43.1833, val_loss: 43.3858, val_MinusLogProbMetric: 43.3858

Epoch 659: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.1833 - MinusLogProbMetric: 43.1833 - val_loss: 43.3858 - val_MinusLogProbMetric: 43.3858 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 660/1000
2023-10-27 18:34:44.875 
Epoch 660/1000 
	 loss: 43.2048, MinusLogProbMetric: 43.2048, val_loss: 43.6406, val_MinusLogProbMetric: 43.6406

Epoch 660: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.2048 - MinusLogProbMetric: 43.2048 - val_loss: 43.6406 - val_MinusLogProbMetric: 43.6406 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 661/1000
2023-10-27 18:35:18.678 
Epoch 661/1000 
	 loss: 43.0566, MinusLogProbMetric: 43.0566, val_loss: 44.7531, val_MinusLogProbMetric: 44.7531

Epoch 661: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.0566 - MinusLogProbMetric: 43.0566 - val_loss: 44.7531 - val_MinusLogProbMetric: 44.7531 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 662/1000
2023-10-27 18:35:52.085 
Epoch 662/1000 
	 loss: 43.1649, MinusLogProbMetric: 43.1649, val_loss: 43.7981, val_MinusLogProbMetric: 43.7981

Epoch 662: val_loss did not improve from 42.85645
196/196 - 33s - loss: 43.1649 - MinusLogProbMetric: 43.1649 - val_loss: 43.7981 - val_MinusLogProbMetric: 43.7981 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 663/1000
2023-10-27 18:36:25.794 
Epoch 663/1000 
	 loss: 43.1836, MinusLogProbMetric: 43.1836, val_loss: 43.1519, val_MinusLogProbMetric: 43.1519

Epoch 663: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.1836 - MinusLogProbMetric: 43.1836 - val_loss: 43.1519 - val_MinusLogProbMetric: 43.1519 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 664/1000
2023-10-27 18:36:58.837 
Epoch 664/1000 
	 loss: 43.4249, MinusLogProbMetric: 43.4249, val_loss: 43.1772, val_MinusLogProbMetric: 43.1772

Epoch 664: val_loss did not improve from 42.85645
196/196 - 33s - loss: 43.4249 - MinusLogProbMetric: 43.4249 - val_loss: 43.1772 - val_MinusLogProbMetric: 43.1772 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 665/1000
2023-10-27 18:37:32.612 
Epoch 665/1000 
	 loss: 43.1490, MinusLogProbMetric: 43.1490, val_loss: 43.5448, val_MinusLogProbMetric: 43.5448

Epoch 665: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.1490 - MinusLogProbMetric: 43.1490 - val_loss: 43.5448 - val_MinusLogProbMetric: 43.5448 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 666/1000
2023-10-27 18:38:05.060 
Epoch 666/1000 
	 loss: 43.1053, MinusLogProbMetric: 43.1053, val_loss: 43.0505, val_MinusLogProbMetric: 43.0505

Epoch 666: val_loss did not improve from 42.85645
196/196 - 32s - loss: 43.1053 - MinusLogProbMetric: 43.1053 - val_loss: 43.0505 - val_MinusLogProbMetric: 43.0505 - lr: 3.3333e-04 - 32s/epoch - 166ms/step
Epoch 667/1000
2023-10-27 18:38:38.790 
Epoch 667/1000 
	 loss: 43.3302, MinusLogProbMetric: 43.3302, val_loss: 44.3991, val_MinusLogProbMetric: 44.3991

Epoch 667: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.3302 - MinusLogProbMetric: 43.3302 - val_loss: 44.3991 - val_MinusLogProbMetric: 44.3991 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 668/1000
2023-10-27 18:39:11.930 
Epoch 668/1000 
	 loss: 42.9493, MinusLogProbMetric: 42.9493, val_loss: 43.2594, val_MinusLogProbMetric: 43.2594

Epoch 668: val_loss did not improve from 42.85645
196/196 - 33s - loss: 42.9493 - MinusLogProbMetric: 42.9493 - val_loss: 43.2594 - val_MinusLogProbMetric: 43.2594 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 669/1000
2023-10-27 18:39:45.655 
Epoch 669/1000 
	 loss: 42.8058, MinusLogProbMetric: 42.8058, val_loss: 43.4153, val_MinusLogProbMetric: 43.4153

Epoch 669: val_loss did not improve from 42.85645
196/196 - 34s - loss: 42.8058 - MinusLogProbMetric: 42.8058 - val_loss: 43.4153 - val_MinusLogProbMetric: 43.4153 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 670/1000
2023-10-27 18:40:18.923 
Epoch 670/1000 
	 loss: 43.2381, MinusLogProbMetric: 43.2381, val_loss: 45.1889, val_MinusLogProbMetric: 45.1889

Epoch 670: val_loss did not improve from 42.85645
196/196 - 33s - loss: 43.2381 - MinusLogProbMetric: 43.2381 - val_loss: 45.1889 - val_MinusLogProbMetric: 45.1889 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 671/1000
2023-10-27 18:40:52.611 
Epoch 671/1000 
	 loss: 43.2129, MinusLogProbMetric: 43.2129, val_loss: 43.5607, val_MinusLogProbMetric: 43.5607

Epoch 671: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.2129 - MinusLogProbMetric: 43.2129 - val_loss: 43.5607 - val_MinusLogProbMetric: 43.5607 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 672/1000
2023-10-27 18:41:26.277 
Epoch 672/1000 
	 loss: 43.2940, MinusLogProbMetric: 43.2940, val_loss: 43.1107, val_MinusLogProbMetric: 43.1107

Epoch 672: val_loss did not improve from 42.85645
196/196 - 34s - loss: 43.2940 - MinusLogProbMetric: 43.2940 - val_loss: 43.1107 - val_MinusLogProbMetric: 43.1107 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 673/1000
2023-10-27 18:41:59.154 
Epoch 673/1000 
	 loss: 42.8620, MinusLogProbMetric: 42.8620, val_loss: 42.7638, val_MinusLogProbMetric: 42.7638

Epoch 673: val_loss improved from 42.85645 to 42.76380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 33s - loss: 42.8620 - MinusLogProbMetric: 42.8620 - val_loss: 42.7638 - val_MinusLogProbMetric: 42.7638 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 674/1000
2023-10-27 18:42:33.730 
Epoch 674/1000 
	 loss: 43.2489, MinusLogProbMetric: 43.2489, val_loss: 43.5617, val_MinusLogProbMetric: 43.5617

Epoch 674: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.2489 - MinusLogProbMetric: 43.2489 - val_loss: 43.5617 - val_MinusLogProbMetric: 43.5617 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 675/1000
2023-10-27 18:43:07.541 
Epoch 675/1000 
	 loss: 43.3583, MinusLogProbMetric: 43.3583, val_loss: 46.0746, val_MinusLogProbMetric: 46.0746

Epoch 675: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.3583 - MinusLogProbMetric: 43.3583 - val_loss: 46.0746 - val_MinusLogProbMetric: 46.0746 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 676/1000
2023-10-27 18:43:41.735 
Epoch 676/1000 
	 loss: 43.1738, MinusLogProbMetric: 43.1738, val_loss: 43.0396, val_MinusLogProbMetric: 43.0396

Epoch 676: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.1738 - MinusLogProbMetric: 43.1738 - val_loss: 43.0396 - val_MinusLogProbMetric: 43.0396 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 677/1000
2023-10-27 18:44:15.300 
Epoch 677/1000 
	 loss: 43.2835, MinusLogProbMetric: 43.2835, val_loss: 46.7092, val_MinusLogProbMetric: 46.7092

Epoch 677: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.2835 - MinusLogProbMetric: 43.2835 - val_loss: 46.7092 - val_MinusLogProbMetric: 46.7092 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 678/1000
2023-10-27 18:44:48.828 
Epoch 678/1000 
	 loss: 43.7284, MinusLogProbMetric: 43.7284, val_loss: 43.1876, val_MinusLogProbMetric: 43.1876

Epoch 678: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.7284 - MinusLogProbMetric: 43.7284 - val_loss: 43.1876 - val_MinusLogProbMetric: 43.1876 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 679/1000
2023-10-27 18:45:22.990 
Epoch 679/1000 
	 loss: 42.9173, MinusLogProbMetric: 42.9173, val_loss: 43.6411, val_MinusLogProbMetric: 43.6411

Epoch 679: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9173 - MinusLogProbMetric: 42.9173 - val_loss: 43.6411 - val_MinusLogProbMetric: 43.6411 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 680/1000
2023-10-27 18:45:56.369 
Epoch 680/1000 
	 loss: 43.2475, MinusLogProbMetric: 43.2475, val_loss: 43.9301, val_MinusLogProbMetric: 43.9301

Epoch 680: val_loss did not improve from 42.76380
196/196 - 33s - loss: 43.2475 - MinusLogProbMetric: 43.2475 - val_loss: 43.9301 - val_MinusLogProbMetric: 43.9301 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 681/1000
2023-10-27 18:46:30.040 
Epoch 681/1000 
	 loss: 42.9005, MinusLogProbMetric: 42.9005, val_loss: 43.1497, val_MinusLogProbMetric: 43.1497

Epoch 681: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9005 - MinusLogProbMetric: 42.9005 - val_loss: 43.1497 - val_MinusLogProbMetric: 43.1497 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 682/1000
2023-10-27 18:47:04.210 
Epoch 682/1000 
	 loss: 43.3039, MinusLogProbMetric: 43.3039, val_loss: 43.6698, val_MinusLogProbMetric: 43.6698

Epoch 682: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.3039 - MinusLogProbMetric: 43.3039 - val_loss: 43.6698 - val_MinusLogProbMetric: 43.6698 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 683/1000
2023-10-27 18:47:38.093 
Epoch 683/1000 
	 loss: 43.1435, MinusLogProbMetric: 43.1435, val_loss: 43.3642, val_MinusLogProbMetric: 43.3642

Epoch 683: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.1435 - MinusLogProbMetric: 43.1435 - val_loss: 43.3642 - val_MinusLogProbMetric: 43.3642 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 684/1000
2023-10-27 18:48:11.821 
Epoch 684/1000 
	 loss: 43.1915, MinusLogProbMetric: 43.1915, val_loss: 44.6422, val_MinusLogProbMetric: 44.6422

Epoch 684: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.1915 - MinusLogProbMetric: 43.1915 - val_loss: 44.6422 - val_MinusLogProbMetric: 44.6422 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 685/1000
2023-10-27 18:48:45.266 
Epoch 685/1000 
	 loss: 43.1774, MinusLogProbMetric: 43.1774, val_loss: 44.6723, val_MinusLogProbMetric: 44.6723

Epoch 685: val_loss did not improve from 42.76380
196/196 - 33s - loss: 43.1774 - MinusLogProbMetric: 43.1774 - val_loss: 44.6723 - val_MinusLogProbMetric: 44.6723 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 686/1000
2023-10-27 18:49:18.892 
Epoch 686/1000 
	 loss: 43.0316, MinusLogProbMetric: 43.0316, val_loss: 44.0135, val_MinusLogProbMetric: 44.0135

Epoch 686: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.0316 - MinusLogProbMetric: 43.0316 - val_loss: 44.0135 - val_MinusLogProbMetric: 44.0135 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 687/1000
2023-10-27 18:49:53.034 
Epoch 687/1000 
	 loss: 43.7416, MinusLogProbMetric: 43.7416, val_loss: 43.7864, val_MinusLogProbMetric: 43.7864

Epoch 687: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.7416 - MinusLogProbMetric: 43.7416 - val_loss: 43.7864 - val_MinusLogProbMetric: 43.7864 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 688/1000
2023-10-27 18:50:26.978 
Epoch 688/1000 
	 loss: 42.9088, MinusLogProbMetric: 42.9088, val_loss: 43.0650, val_MinusLogProbMetric: 43.0650

Epoch 688: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9088 - MinusLogProbMetric: 42.9088 - val_loss: 43.0650 - val_MinusLogProbMetric: 43.0650 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 689/1000
2023-10-27 18:51:00.738 
Epoch 689/1000 
	 loss: 42.9161, MinusLogProbMetric: 42.9161, val_loss: 43.3549, val_MinusLogProbMetric: 43.3549

Epoch 689: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9161 - MinusLogProbMetric: 42.9161 - val_loss: 43.3549 - val_MinusLogProbMetric: 43.3549 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 690/1000
2023-10-27 18:51:33.817 
Epoch 690/1000 
	 loss: 42.8648, MinusLogProbMetric: 42.8648, val_loss: 44.2858, val_MinusLogProbMetric: 44.2858

Epoch 690: val_loss did not improve from 42.76380
196/196 - 33s - loss: 42.8648 - MinusLogProbMetric: 42.8648 - val_loss: 44.2858 - val_MinusLogProbMetric: 44.2858 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 691/1000
2023-10-27 18:52:07.841 
Epoch 691/1000 
	 loss: 43.0443, MinusLogProbMetric: 43.0443, val_loss: 43.6329, val_MinusLogProbMetric: 43.6329

Epoch 691: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.0443 - MinusLogProbMetric: 43.0443 - val_loss: 43.6329 - val_MinusLogProbMetric: 43.6329 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 692/1000
2023-10-27 18:52:41.789 
Epoch 692/1000 
	 loss: 43.2535, MinusLogProbMetric: 43.2535, val_loss: 44.1667, val_MinusLogProbMetric: 44.1667

Epoch 692: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.2535 - MinusLogProbMetric: 43.2535 - val_loss: 44.1667 - val_MinusLogProbMetric: 44.1667 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 693/1000
2023-10-27 18:53:15.846 
Epoch 693/1000 
	 loss: 43.2105, MinusLogProbMetric: 43.2105, val_loss: 43.1261, val_MinusLogProbMetric: 43.1261

Epoch 693: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.2105 - MinusLogProbMetric: 43.2105 - val_loss: 43.1261 - val_MinusLogProbMetric: 43.1261 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 694/1000
2023-10-27 18:53:49.047 
Epoch 694/1000 
	 loss: 43.0205, MinusLogProbMetric: 43.0205, val_loss: 44.3773, val_MinusLogProbMetric: 44.3773

Epoch 694: val_loss did not improve from 42.76380
196/196 - 33s - loss: 43.0205 - MinusLogProbMetric: 43.0205 - val_loss: 44.3773 - val_MinusLogProbMetric: 44.3773 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 695/1000
2023-10-27 18:54:22.664 
Epoch 695/1000 
	 loss: 42.9109, MinusLogProbMetric: 42.9109, val_loss: 42.8514, val_MinusLogProbMetric: 42.8514

Epoch 695: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9109 - MinusLogProbMetric: 42.9109 - val_loss: 42.8514 - val_MinusLogProbMetric: 42.8514 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 696/1000
2023-10-27 18:54:56.058 
Epoch 696/1000 
	 loss: 43.1499, MinusLogProbMetric: 43.1499, val_loss: 44.5012, val_MinusLogProbMetric: 44.5012

Epoch 696: val_loss did not improve from 42.76380
196/196 - 33s - loss: 43.1499 - MinusLogProbMetric: 43.1499 - val_loss: 44.5012 - val_MinusLogProbMetric: 44.5012 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 697/1000
2023-10-27 18:55:29.788 
Epoch 697/1000 
	 loss: 43.0683, MinusLogProbMetric: 43.0683, val_loss: 43.1520, val_MinusLogProbMetric: 43.1520

Epoch 697: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.0683 - MinusLogProbMetric: 43.0683 - val_loss: 43.1520 - val_MinusLogProbMetric: 43.1520 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 698/1000
2023-10-27 18:56:03.401 
Epoch 698/1000 
	 loss: 42.8336, MinusLogProbMetric: 42.8336, val_loss: 43.7154, val_MinusLogProbMetric: 43.7154

Epoch 698: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.8336 - MinusLogProbMetric: 42.8336 - val_loss: 43.7154 - val_MinusLogProbMetric: 43.7154 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 699/1000
2023-10-27 18:56:37.116 
Epoch 699/1000 
	 loss: 42.8422, MinusLogProbMetric: 42.8422, val_loss: 43.2263, val_MinusLogProbMetric: 43.2263

Epoch 699: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.8422 - MinusLogProbMetric: 42.8422 - val_loss: 43.2263 - val_MinusLogProbMetric: 43.2263 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 700/1000
2023-10-27 18:57:10.826 
Epoch 700/1000 
	 loss: 43.0509, MinusLogProbMetric: 43.0509, val_loss: 44.8433, val_MinusLogProbMetric: 44.8433

Epoch 700: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.0509 - MinusLogProbMetric: 43.0509 - val_loss: 44.8433 - val_MinusLogProbMetric: 44.8433 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 701/1000
2023-10-27 18:57:43.367 
Epoch 701/1000 
	 loss: 43.2624, MinusLogProbMetric: 43.2624, val_loss: 46.4706, val_MinusLogProbMetric: 46.4706

Epoch 701: val_loss did not improve from 42.76380
196/196 - 33s - loss: 43.2624 - MinusLogProbMetric: 43.2624 - val_loss: 46.4706 - val_MinusLogProbMetric: 46.4706 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 702/1000
2023-10-27 18:58:17.304 
Epoch 702/1000 
	 loss: 42.9931, MinusLogProbMetric: 42.9931, val_loss: 43.1128, val_MinusLogProbMetric: 43.1128

Epoch 702: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9931 - MinusLogProbMetric: 42.9931 - val_loss: 43.1128 - val_MinusLogProbMetric: 43.1128 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 703/1000
2023-10-27 18:58:50.601 
Epoch 703/1000 
	 loss: 43.0308, MinusLogProbMetric: 43.0308, val_loss: 43.1033, val_MinusLogProbMetric: 43.1033

Epoch 703: val_loss did not improve from 42.76380
196/196 - 33s - loss: 43.0308 - MinusLogProbMetric: 43.0308 - val_loss: 43.1033 - val_MinusLogProbMetric: 43.1033 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 704/1000
2023-10-27 18:59:22.889 
Epoch 704/1000 
	 loss: 43.0037, MinusLogProbMetric: 43.0037, val_loss: 43.8361, val_MinusLogProbMetric: 43.8361

Epoch 704: val_loss did not improve from 42.76380
196/196 - 32s - loss: 43.0037 - MinusLogProbMetric: 43.0037 - val_loss: 43.8361 - val_MinusLogProbMetric: 43.8361 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 705/1000
2023-10-27 18:59:56.658 
Epoch 705/1000 
	 loss: 43.0763, MinusLogProbMetric: 43.0763, val_loss: 43.8637, val_MinusLogProbMetric: 43.8637

Epoch 705: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.0763 - MinusLogProbMetric: 43.0763 - val_loss: 43.8637 - val_MinusLogProbMetric: 43.8637 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 706/1000
2023-10-27 19:00:30.502 
Epoch 706/1000 
	 loss: 42.9782, MinusLogProbMetric: 42.9782, val_loss: 44.7975, val_MinusLogProbMetric: 44.7975

Epoch 706: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9782 - MinusLogProbMetric: 42.9782 - val_loss: 44.7975 - val_MinusLogProbMetric: 44.7975 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 707/1000
2023-10-27 19:01:04.364 
Epoch 707/1000 
	 loss: 43.1140, MinusLogProbMetric: 43.1140, val_loss: 43.2100, val_MinusLogProbMetric: 43.2100

Epoch 707: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.1140 - MinusLogProbMetric: 43.1140 - val_loss: 43.2100 - val_MinusLogProbMetric: 43.2100 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 708/1000
2023-10-27 19:01:37.306 
Epoch 708/1000 
	 loss: 42.9776, MinusLogProbMetric: 42.9776, val_loss: 43.8581, val_MinusLogProbMetric: 43.8581

Epoch 708: val_loss did not improve from 42.76380
196/196 - 33s - loss: 42.9776 - MinusLogProbMetric: 42.9776 - val_loss: 43.8581 - val_MinusLogProbMetric: 43.8581 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 709/1000
2023-10-27 19:02:11.041 
Epoch 709/1000 
	 loss: 43.3820, MinusLogProbMetric: 43.3820, val_loss: 43.8344, val_MinusLogProbMetric: 43.8344

Epoch 709: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.3820 - MinusLogProbMetric: 43.3820 - val_loss: 43.8344 - val_MinusLogProbMetric: 43.8344 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 710/1000
2023-10-27 19:02:45.062 
Epoch 710/1000 
	 loss: 42.8862, MinusLogProbMetric: 42.8862, val_loss: 43.2393, val_MinusLogProbMetric: 43.2393

Epoch 710: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.8862 - MinusLogProbMetric: 42.8862 - val_loss: 43.2393 - val_MinusLogProbMetric: 43.2393 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 711/1000
2023-10-27 19:03:19.039 
Epoch 711/1000 
	 loss: 43.4945, MinusLogProbMetric: 43.4945, val_loss: 43.4935, val_MinusLogProbMetric: 43.4935

Epoch 711: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.4945 - MinusLogProbMetric: 43.4945 - val_loss: 43.4935 - val_MinusLogProbMetric: 43.4935 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 712/1000
2023-10-27 19:03:52.891 
Epoch 712/1000 
	 loss: 43.0094, MinusLogProbMetric: 43.0094, val_loss: 43.2010, val_MinusLogProbMetric: 43.2010

Epoch 712: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.0094 - MinusLogProbMetric: 43.0094 - val_loss: 43.2010 - val_MinusLogProbMetric: 43.2010 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 713/1000
2023-10-27 19:04:25.060 
Epoch 713/1000 
	 loss: 43.1010, MinusLogProbMetric: 43.1010, val_loss: 44.3135, val_MinusLogProbMetric: 44.3135

Epoch 713: val_loss did not improve from 42.76380
196/196 - 32s - loss: 43.1010 - MinusLogProbMetric: 43.1010 - val_loss: 44.3135 - val_MinusLogProbMetric: 44.3135 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 714/1000
2023-10-27 19:04:59.032 
Epoch 714/1000 
	 loss: 42.7238, MinusLogProbMetric: 42.7238, val_loss: 43.3106, val_MinusLogProbMetric: 43.3106

Epoch 714: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.7238 - MinusLogProbMetric: 42.7238 - val_loss: 43.3106 - val_MinusLogProbMetric: 43.3106 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 715/1000
2023-10-27 19:05:32.698 
Epoch 715/1000 
	 loss: 42.9770, MinusLogProbMetric: 42.9770, val_loss: 44.6424, val_MinusLogProbMetric: 44.6424

Epoch 715: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9770 - MinusLogProbMetric: 42.9770 - val_loss: 44.6424 - val_MinusLogProbMetric: 44.6424 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 716/1000
2023-10-27 19:06:06.537 
Epoch 716/1000 
	 loss: 42.9177, MinusLogProbMetric: 42.9177, val_loss: 43.0149, val_MinusLogProbMetric: 43.0149

Epoch 716: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9177 - MinusLogProbMetric: 42.9177 - val_loss: 43.0149 - val_MinusLogProbMetric: 43.0149 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 717/1000
2023-10-27 19:06:40.135 
Epoch 717/1000 
	 loss: 43.0319, MinusLogProbMetric: 43.0319, val_loss: 43.0419, val_MinusLogProbMetric: 43.0419

Epoch 717: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.0319 - MinusLogProbMetric: 43.0319 - val_loss: 43.0419 - val_MinusLogProbMetric: 43.0419 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 718/1000
2023-10-27 19:07:14.324 
Epoch 718/1000 
	 loss: 42.5018, MinusLogProbMetric: 42.5018, val_loss: 43.8097, val_MinusLogProbMetric: 43.8097

Epoch 718: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.5018 - MinusLogProbMetric: 42.5018 - val_loss: 43.8097 - val_MinusLogProbMetric: 43.8097 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 719/1000
2023-10-27 19:07:48.664 
Epoch 719/1000 
	 loss: 43.4204, MinusLogProbMetric: 43.4204, val_loss: 42.9318, val_MinusLogProbMetric: 42.9318

Epoch 719: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.4204 - MinusLogProbMetric: 43.4204 - val_loss: 42.9318 - val_MinusLogProbMetric: 42.9318 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 720/1000
2023-10-27 19:08:22.428 
Epoch 720/1000 
	 loss: 42.9586, MinusLogProbMetric: 42.9586, val_loss: 43.7241, val_MinusLogProbMetric: 43.7241

Epoch 720: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.9586 - MinusLogProbMetric: 42.9586 - val_loss: 43.7241 - val_MinusLogProbMetric: 43.7241 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 721/1000
2023-10-27 19:08:56.385 
Epoch 721/1000 
	 loss: 43.0652, MinusLogProbMetric: 43.0652, val_loss: 42.7827, val_MinusLogProbMetric: 42.7827

Epoch 721: val_loss did not improve from 42.76380
196/196 - 34s - loss: 43.0652 - MinusLogProbMetric: 43.0652 - val_loss: 42.7827 - val_MinusLogProbMetric: 42.7827 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 722/1000
2023-10-27 19:09:29.823 
Epoch 722/1000 
	 loss: 43.0203, MinusLogProbMetric: 43.0203, val_loss: 42.8911, val_MinusLogProbMetric: 42.8911

Epoch 722: val_loss did not improve from 42.76380
196/196 - 33s - loss: 43.0203 - MinusLogProbMetric: 43.0203 - val_loss: 42.8911 - val_MinusLogProbMetric: 42.8911 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 723/1000
2023-10-27 19:10:03.634 
Epoch 723/1000 
	 loss: 42.8580, MinusLogProbMetric: 42.8580, val_loss: 43.8706, val_MinusLogProbMetric: 43.8706

Epoch 723: val_loss did not improve from 42.76380
196/196 - 34s - loss: 42.8580 - MinusLogProbMetric: 42.8580 - val_loss: 43.8706 - val_MinusLogProbMetric: 43.8706 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 724/1000
2023-10-27 19:10:37.565 
Epoch 724/1000 
	 loss: 41.5947, MinusLogProbMetric: 41.5947, val_loss: 42.2290, val_MinusLogProbMetric: 42.2290

Epoch 724: val_loss improved from 42.76380 to 42.22897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 41.5947 - MinusLogProbMetric: 41.5947 - val_loss: 42.2290 - val_MinusLogProbMetric: 42.2290 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 725/1000
2023-10-27 19:11:11.656 
Epoch 725/1000 
	 loss: 41.8830, MinusLogProbMetric: 41.8830, val_loss: 43.5394, val_MinusLogProbMetric: 43.5394

Epoch 725: val_loss did not improve from 42.22897
196/196 - 34s - loss: 41.8830 - MinusLogProbMetric: 41.8830 - val_loss: 43.5394 - val_MinusLogProbMetric: 43.5394 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 726/1000
2023-10-27 19:11:45.464 
Epoch 726/1000 
	 loss: 41.7060, MinusLogProbMetric: 41.7060, val_loss: 42.2852, val_MinusLogProbMetric: 42.2852

Epoch 726: val_loss did not improve from 42.22897
196/196 - 34s - loss: 41.7060 - MinusLogProbMetric: 41.7060 - val_loss: 42.2852 - val_MinusLogProbMetric: 42.2852 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 727/1000
2023-10-27 19:12:19.152 
Epoch 727/1000 
	 loss: 41.6934, MinusLogProbMetric: 41.6934, val_loss: 42.5405, val_MinusLogProbMetric: 42.5405

Epoch 727: val_loss did not improve from 42.22897
196/196 - 34s - loss: 41.6934 - MinusLogProbMetric: 41.6934 - val_loss: 42.5405 - val_MinusLogProbMetric: 42.5405 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 728/1000
2023-10-27 19:12:52.791 
Epoch 728/1000 
	 loss: 41.8380, MinusLogProbMetric: 41.8380, val_loss: 42.5182, val_MinusLogProbMetric: 42.5182

Epoch 728: val_loss did not improve from 42.22897
196/196 - 34s - loss: 41.8380 - MinusLogProbMetric: 41.8380 - val_loss: 42.5182 - val_MinusLogProbMetric: 42.5182 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 729/1000
2023-10-27 19:13:26.578 
Epoch 729/1000 
	 loss: 41.6585, MinusLogProbMetric: 41.6585, val_loss: 42.4424, val_MinusLogProbMetric: 42.4424

Epoch 729: val_loss did not improve from 42.22897
196/196 - 34s - loss: 41.6585 - MinusLogProbMetric: 41.6585 - val_loss: 42.4424 - val_MinusLogProbMetric: 42.4424 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 730/1000
2023-10-27 19:14:00.312 
Epoch 730/1000 
	 loss: 41.8901, MinusLogProbMetric: 41.8901, val_loss: 42.4250, val_MinusLogProbMetric: 42.4250

Epoch 730: val_loss did not improve from 42.22897
196/196 - 34s - loss: 41.8901 - MinusLogProbMetric: 41.8901 - val_loss: 42.4250 - val_MinusLogProbMetric: 42.4250 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 731/1000
2023-10-27 19:14:34.265 
Epoch 731/1000 
	 loss: 41.8006, MinusLogProbMetric: 41.8006, val_loss: 42.7514, val_MinusLogProbMetric: 42.7514

Epoch 731: val_loss did not improve from 42.22897
196/196 - 34s - loss: 41.8006 - MinusLogProbMetric: 41.8006 - val_loss: 42.7514 - val_MinusLogProbMetric: 42.7514 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 732/1000
2023-10-27 19:15:08.307 
Epoch 732/1000 
	 loss: 41.5834, MinusLogProbMetric: 41.5834, val_loss: 42.1886, val_MinusLogProbMetric: 42.1886

Epoch 732: val_loss improved from 42.22897 to 42.18855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 41.5834 - MinusLogProbMetric: 41.5834 - val_loss: 42.1886 - val_MinusLogProbMetric: 42.1886 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 733/1000
2023-10-27 19:15:42.804 
Epoch 733/1000 
	 loss: 41.7746, MinusLogProbMetric: 41.7746, val_loss: 44.0639, val_MinusLogProbMetric: 44.0639

Epoch 733: val_loss did not improve from 42.18855
196/196 - 34s - loss: 41.7746 - MinusLogProbMetric: 41.7746 - val_loss: 44.0639 - val_MinusLogProbMetric: 44.0639 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 734/1000
2023-10-27 19:16:16.643 
Epoch 734/1000 
	 loss: 41.8442, MinusLogProbMetric: 41.8442, val_loss: 42.2870, val_MinusLogProbMetric: 42.2870

Epoch 734: val_loss did not improve from 42.18855
196/196 - 34s - loss: 41.8442 - MinusLogProbMetric: 41.8442 - val_loss: 42.2870 - val_MinusLogProbMetric: 42.2870 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 735/1000
2023-10-27 19:16:50.218 
Epoch 735/1000 
	 loss: 41.8335, MinusLogProbMetric: 41.8335, val_loss: 42.1866, val_MinusLogProbMetric: 42.1866

Epoch 735: val_loss improved from 42.18855 to 42.18655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 41.8335 - MinusLogProbMetric: 41.8335 - val_loss: 42.1866 - val_MinusLogProbMetric: 42.1866 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 736/1000
2023-10-27 19:17:23.703 
Epoch 736/1000 
	 loss: 41.8706, MinusLogProbMetric: 41.8706, val_loss: 42.6282, val_MinusLogProbMetric: 42.6282

Epoch 736: val_loss did not improve from 42.18655
196/196 - 33s - loss: 41.8706 - MinusLogProbMetric: 41.8706 - val_loss: 42.6282 - val_MinusLogProbMetric: 42.6282 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 737/1000
2023-10-27 19:17:57.492 
Epoch 737/1000 
	 loss: 41.5914, MinusLogProbMetric: 41.5914, val_loss: 42.2031, val_MinusLogProbMetric: 42.2031

Epoch 737: val_loss did not improve from 42.18655
196/196 - 34s - loss: 41.5914 - MinusLogProbMetric: 41.5914 - val_loss: 42.2031 - val_MinusLogProbMetric: 42.2031 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 738/1000
2023-10-27 19:18:31.398 
Epoch 738/1000 
	 loss: 41.9406, MinusLogProbMetric: 41.9406, val_loss: 42.5321, val_MinusLogProbMetric: 42.5321

Epoch 738: val_loss did not improve from 42.18655
196/196 - 34s - loss: 41.9406 - MinusLogProbMetric: 41.9406 - val_loss: 42.5321 - val_MinusLogProbMetric: 42.5321 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 739/1000
2023-10-27 19:19:04.016 
Epoch 739/1000 
	 loss: 41.8859, MinusLogProbMetric: 41.8859, val_loss: 42.2933, val_MinusLogProbMetric: 42.2933

Epoch 739: val_loss did not improve from 42.18655
196/196 - 33s - loss: 41.8859 - MinusLogProbMetric: 41.8859 - val_loss: 42.2933 - val_MinusLogProbMetric: 42.2933 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 740/1000
2023-10-27 19:19:37.819 
Epoch 740/1000 
	 loss: 41.6948, MinusLogProbMetric: 41.6948, val_loss: 42.2629, val_MinusLogProbMetric: 42.2629

Epoch 740: val_loss did not improve from 42.18655
196/196 - 34s - loss: 41.6948 - MinusLogProbMetric: 41.6948 - val_loss: 42.2629 - val_MinusLogProbMetric: 42.2629 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 741/1000
2023-10-27 19:20:11.418 
Epoch 741/1000 
	 loss: 41.9559, MinusLogProbMetric: 41.9559, val_loss: 42.3141, val_MinusLogProbMetric: 42.3141

Epoch 741: val_loss did not improve from 42.18655
196/196 - 34s - loss: 41.9559 - MinusLogProbMetric: 41.9559 - val_loss: 42.3141 - val_MinusLogProbMetric: 42.3141 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 742/1000
2023-10-27 19:20:45.109 
Epoch 742/1000 
	 loss: 41.6293, MinusLogProbMetric: 41.6293, val_loss: 42.5473, val_MinusLogProbMetric: 42.5473

Epoch 742: val_loss did not improve from 42.18655
196/196 - 34s - loss: 41.6293 - MinusLogProbMetric: 41.6293 - val_loss: 42.5473 - val_MinusLogProbMetric: 42.5473 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 743/1000
2023-10-27 19:21:17.957 
Epoch 743/1000 
	 loss: 41.8967, MinusLogProbMetric: 41.8967, val_loss: 42.2612, val_MinusLogProbMetric: 42.2612

Epoch 743: val_loss did not improve from 42.18655
196/196 - 33s - loss: 41.8967 - MinusLogProbMetric: 41.8967 - val_loss: 42.2612 - val_MinusLogProbMetric: 42.2612 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 744/1000
2023-10-27 19:21:51.424 
Epoch 744/1000 
	 loss: 41.7523, MinusLogProbMetric: 41.7523, val_loss: 42.0232, val_MinusLogProbMetric: 42.0232

Epoch 744: val_loss improved from 42.18655 to 42.02320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 41.7523 - MinusLogProbMetric: 41.7523 - val_loss: 42.0232 - val_MinusLogProbMetric: 42.0232 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 745/1000
2023-10-27 19:22:25.922 
Epoch 745/1000 
	 loss: 41.8177, MinusLogProbMetric: 41.8177, val_loss: 42.4450, val_MinusLogProbMetric: 42.4450

Epoch 745: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8177 - MinusLogProbMetric: 41.8177 - val_loss: 42.4450 - val_MinusLogProbMetric: 42.4450 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 746/1000
2023-10-27 19:22:59.243 
Epoch 746/1000 
	 loss: 41.7263, MinusLogProbMetric: 41.7263, val_loss: 42.1334, val_MinusLogProbMetric: 42.1334

Epoch 746: val_loss did not improve from 42.02320
196/196 - 33s - loss: 41.7263 - MinusLogProbMetric: 41.7263 - val_loss: 42.1334 - val_MinusLogProbMetric: 42.1334 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 747/1000
2023-10-27 19:23:33.238 
Epoch 747/1000 
	 loss: 41.5815, MinusLogProbMetric: 41.5815, val_loss: 42.2514, val_MinusLogProbMetric: 42.2514

Epoch 747: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.5815 - MinusLogProbMetric: 41.5815 - val_loss: 42.2514 - val_MinusLogProbMetric: 42.2514 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 748/1000
2023-10-27 19:24:07.024 
Epoch 748/1000 
	 loss: 41.7577, MinusLogProbMetric: 41.7577, val_loss: 42.4704, val_MinusLogProbMetric: 42.4704

Epoch 748: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.7577 - MinusLogProbMetric: 41.7577 - val_loss: 42.4704 - val_MinusLogProbMetric: 42.4704 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 749/1000
2023-10-27 19:24:40.970 
Epoch 749/1000 
	 loss: 41.7505, MinusLogProbMetric: 41.7505, val_loss: 42.3311, val_MinusLogProbMetric: 42.3311

Epoch 749: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.7505 - MinusLogProbMetric: 41.7505 - val_loss: 42.3311 - val_MinusLogProbMetric: 42.3311 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 750/1000
2023-10-27 19:25:14.343 
Epoch 750/1000 
	 loss: 41.6797, MinusLogProbMetric: 41.6797, val_loss: 43.5891, val_MinusLogProbMetric: 43.5891

Epoch 750: val_loss did not improve from 42.02320
196/196 - 33s - loss: 41.6797 - MinusLogProbMetric: 41.6797 - val_loss: 43.5891 - val_MinusLogProbMetric: 43.5891 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 751/1000
2023-10-27 19:25:48.291 
Epoch 751/1000 
	 loss: 41.8115, MinusLogProbMetric: 41.8115, val_loss: 43.2694, val_MinusLogProbMetric: 43.2694

Epoch 751: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8115 - MinusLogProbMetric: 41.8115 - val_loss: 43.2694 - val_MinusLogProbMetric: 43.2694 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 752/1000
2023-10-27 19:26:21.860 
Epoch 752/1000 
	 loss: 41.6439, MinusLogProbMetric: 41.6439, val_loss: 42.5447, val_MinusLogProbMetric: 42.5447

Epoch 752: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.6439 - MinusLogProbMetric: 41.6439 - val_loss: 42.5447 - val_MinusLogProbMetric: 42.5447 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 753/1000
2023-10-27 19:26:55.939 
Epoch 753/1000 
	 loss: 41.7501, MinusLogProbMetric: 41.7501, val_loss: 42.2009, val_MinusLogProbMetric: 42.2009

Epoch 753: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.7501 - MinusLogProbMetric: 41.7501 - val_loss: 42.2009 - val_MinusLogProbMetric: 42.2009 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 754/1000
2023-10-27 19:27:29.752 
Epoch 754/1000 
	 loss: 41.9598, MinusLogProbMetric: 41.9598, val_loss: 42.1936, val_MinusLogProbMetric: 42.1936

Epoch 754: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.9598 - MinusLogProbMetric: 41.9598 - val_loss: 42.1936 - val_MinusLogProbMetric: 42.1936 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 755/1000
2023-10-27 19:28:03.757 
Epoch 755/1000 
	 loss: 41.6896, MinusLogProbMetric: 41.6896, val_loss: 42.4991, val_MinusLogProbMetric: 42.4991

Epoch 755: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.6896 - MinusLogProbMetric: 41.6896 - val_loss: 42.4991 - val_MinusLogProbMetric: 42.4991 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 756/1000
2023-10-27 19:28:38.007 
Epoch 756/1000 
	 loss: 41.8037, MinusLogProbMetric: 41.8037, val_loss: 42.1493, val_MinusLogProbMetric: 42.1493

Epoch 756: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8037 - MinusLogProbMetric: 41.8037 - val_loss: 42.1493 - val_MinusLogProbMetric: 42.1493 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 757/1000
2023-10-27 19:29:11.627 
Epoch 757/1000 
	 loss: 41.6327, MinusLogProbMetric: 41.6327, val_loss: 43.9347, val_MinusLogProbMetric: 43.9347

Epoch 757: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.6327 - MinusLogProbMetric: 41.6327 - val_loss: 43.9347 - val_MinusLogProbMetric: 43.9347 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 758/1000
2023-10-27 19:29:45.801 
Epoch 758/1000 
	 loss: 41.8324, MinusLogProbMetric: 41.8324, val_loss: 42.6804, val_MinusLogProbMetric: 42.6804

Epoch 758: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8324 - MinusLogProbMetric: 41.8324 - val_loss: 42.6804 - val_MinusLogProbMetric: 42.6804 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 759/1000
2023-10-27 19:30:19.655 
Epoch 759/1000 
	 loss: 41.8344, MinusLogProbMetric: 41.8344, val_loss: 42.1908, val_MinusLogProbMetric: 42.1908

Epoch 759: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8344 - MinusLogProbMetric: 41.8344 - val_loss: 42.1908 - val_MinusLogProbMetric: 42.1908 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 760/1000
2023-10-27 19:30:53.720 
Epoch 760/1000 
	 loss: 41.8998, MinusLogProbMetric: 41.8998, val_loss: 42.3251, val_MinusLogProbMetric: 42.3251

Epoch 760: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8998 - MinusLogProbMetric: 41.8998 - val_loss: 42.3251 - val_MinusLogProbMetric: 42.3251 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 761/1000
2023-10-27 19:31:27.661 
Epoch 761/1000 
	 loss: 41.8884, MinusLogProbMetric: 41.8884, val_loss: 42.1338, val_MinusLogProbMetric: 42.1338

Epoch 761: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8884 - MinusLogProbMetric: 41.8884 - val_loss: 42.1338 - val_MinusLogProbMetric: 42.1338 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 762/1000
2023-10-27 19:32:01.356 
Epoch 762/1000 
	 loss: 41.8212, MinusLogProbMetric: 41.8212, val_loss: 42.2221, val_MinusLogProbMetric: 42.2221

Epoch 762: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8212 - MinusLogProbMetric: 41.8212 - val_loss: 42.2221 - val_MinusLogProbMetric: 42.2221 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 763/1000
2023-10-27 19:32:35.116 
Epoch 763/1000 
	 loss: 41.7918, MinusLogProbMetric: 41.7918, val_loss: 42.1460, val_MinusLogProbMetric: 42.1460

Epoch 763: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.7918 - MinusLogProbMetric: 41.7918 - val_loss: 42.1460 - val_MinusLogProbMetric: 42.1460 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 764/1000
2023-10-27 19:33:09.043 
Epoch 764/1000 
	 loss: 41.6170, MinusLogProbMetric: 41.6170, val_loss: 42.1681, val_MinusLogProbMetric: 42.1681

Epoch 764: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.6170 - MinusLogProbMetric: 41.6170 - val_loss: 42.1681 - val_MinusLogProbMetric: 42.1681 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 765/1000
2023-10-27 19:33:43.007 
Epoch 765/1000 
	 loss: 41.9488, MinusLogProbMetric: 41.9488, val_loss: 42.5569, val_MinusLogProbMetric: 42.5569

Epoch 765: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.9488 - MinusLogProbMetric: 41.9488 - val_loss: 42.5569 - val_MinusLogProbMetric: 42.5569 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 766/1000
2023-10-27 19:34:16.805 
Epoch 766/1000 
	 loss: 41.8425, MinusLogProbMetric: 41.8425, val_loss: 42.1601, val_MinusLogProbMetric: 42.1601

Epoch 766: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8425 - MinusLogProbMetric: 41.8425 - val_loss: 42.1601 - val_MinusLogProbMetric: 42.1601 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 767/1000
2023-10-27 19:34:51.042 
Epoch 767/1000 
	 loss: 41.5715, MinusLogProbMetric: 41.5715, val_loss: 42.1261, val_MinusLogProbMetric: 42.1261

Epoch 767: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.5715 - MinusLogProbMetric: 41.5715 - val_loss: 42.1261 - val_MinusLogProbMetric: 42.1261 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 768/1000
2023-10-27 19:35:25.010 
Epoch 768/1000 
	 loss: 41.9287, MinusLogProbMetric: 41.9287, val_loss: 42.2622, val_MinusLogProbMetric: 42.2622

Epoch 768: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.9287 - MinusLogProbMetric: 41.9287 - val_loss: 42.2622 - val_MinusLogProbMetric: 42.2622 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 769/1000
2023-10-27 19:35:58.985 
Epoch 769/1000 
	 loss: 41.6127, MinusLogProbMetric: 41.6127, val_loss: 42.1867, val_MinusLogProbMetric: 42.1867

Epoch 769: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.6127 - MinusLogProbMetric: 41.6127 - val_loss: 42.1867 - val_MinusLogProbMetric: 42.1867 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 770/1000
2023-10-27 19:36:32.867 
Epoch 770/1000 
	 loss: 41.8357, MinusLogProbMetric: 41.8357, val_loss: 42.1892, val_MinusLogProbMetric: 42.1892

Epoch 770: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8357 - MinusLogProbMetric: 41.8357 - val_loss: 42.1892 - val_MinusLogProbMetric: 42.1892 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 771/1000
2023-10-27 19:37:06.643 
Epoch 771/1000 
	 loss: 41.5495, MinusLogProbMetric: 41.5495, val_loss: 42.1456, val_MinusLogProbMetric: 42.1456

Epoch 771: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.5495 - MinusLogProbMetric: 41.5495 - val_loss: 42.1456 - val_MinusLogProbMetric: 42.1456 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 772/1000
2023-10-27 19:37:40.950 
Epoch 772/1000 
	 loss: 42.2198, MinusLogProbMetric: 42.2198, val_loss: 42.7583, val_MinusLogProbMetric: 42.7583

Epoch 772: val_loss did not improve from 42.02320
196/196 - 34s - loss: 42.2198 - MinusLogProbMetric: 42.2198 - val_loss: 42.7583 - val_MinusLogProbMetric: 42.7583 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 773/1000
2023-10-27 19:38:14.913 
Epoch 773/1000 
	 loss: 41.8378, MinusLogProbMetric: 41.8378, val_loss: 44.0531, val_MinusLogProbMetric: 44.0531

Epoch 773: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8378 - MinusLogProbMetric: 41.8378 - val_loss: 44.0531 - val_MinusLogProbMetric: 44.0531 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 774/1000
2023-10-27 19:38:48.925 
Epoch 774/1000 
	 loss: 41.7670, MinusLogProbMetric: 41.7670, val_loss: 42.5962, val_MinusLogProbMetric: 42.5962

Epoch 774: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.7670 - MinusLogProbMetric: 41.7670 - val_loss: 42.5962 - val_MinusLogProbMetric: 42.5962 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 775/1000
2023-10-27 19:39:22.440 
Epoch 775/1000 
	 loss: 41.5607, MinusLogProbMetric: 41.5607, val_loss: 42.2967, val_MinusLogProbMetric: 42.2967

Epoch 775: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.5607 - MinusLogProbMetric: 41.5607 - val_loss: 42.2967 - val_MinusLogProbMetric: 42.2967 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 776/1000
2023-10-27 19:39:56.132 
Epoch 776/1000 
	 loss: 41.5097, MinusLogProbMetric: 41.5097, val_loss: 42.4089, val_MinusLogProbMetric: 42.4089

Epoch 776: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.5097 - MinusLogProbMetric: 41.5097 - val_loss: 42.4089 - val_MinusLogProbMetric: 42.4089 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 777/1000
2023-10-27 19:40:30.087 
Epoch 777/1000 
	 loss: 41.7436, MinusLogProbMetric: 41.7436, val_loss: 42.3418, val_MinusLogProbMetric: 42.3418

Epoch 777: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.7436 - MinusLogProbMetric: 41.7436 - val_loss: 42.3418 - val_MinusLogProbMetric: 42.3418 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 778/1000
2023-10-27 19:41:03.733 
Epoch 778/1000 
	 loss: 41.8945, MinusLogProbMetric: 41.8945, val_loss: 42.9767, val_MinusLogProbMetric: 42.9767

Epoch 778: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8945 - MinusLogProbMetric: 41.8945 - val_loss: 42.9767 - val_MinusLogProbMetric: 42.9767 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 779/1000
2023-10-27 19:41:37.987 
Epoch 779/1000 
	 loss: 41.6735, MinusLogProbMetric: 41.6735, val_loss: 42.5205, val_MinusLogProbMetric: 42.5205

Epoch 779: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.6735 - MinusLogProbMetric: 41.6735 - val_loss: 42.5205 - val_MinusLogProbMetric: 42.5205 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 780/1000
2023-10-27 19:42:12.212 
Epoch 780/1000 
	 loss: 41.6681, MinusLogProbMetric: 41.6681, val_loss: 45.0262, val_MinusLogProbMetric: 45.0262

Epoch 780: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.6681 - MinusLogProbMetric: 41.6681 - val_loss: 45.0262 - val_MinusLogProbMetric: 45.0262 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 781/1000
2023-10-27 19:42:46.090 
Epoch 781/1000 
	 loss: 41.8171, MinusLogProbMetric: 41.8171, val_loss: 43.4156, val_MinusLogProbMetric: 43.4156

Epoch 781: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8171 - MinusLogProbMetric: 41.8171 - val_loss: 43.4156 - val_MinusLogProbMetric: 43.4156 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 782/1000
2023-10-27 19:43:19.852 
Epoch 782/1000 
	 loss: 41.7065, MinusLogProbMetric: 41.7065, val_loss: 43.1552, val_MinusLogProbMetric: 43.1552

Epoch 782: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.7065 - MinusLogProbMetric: 41.7065 - val_loss: 43.1552 - val_MinusLogProbMetric: 43.1552 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 783/1000
2023-10-27 19:43:53.604 
Epoch 783/1000 
	 loss: 41.7775, MinusLogProbMetric: 41.7775, val_loss: 42.0660, val_MinusLogProbMetric: 42.0660

Epoch 783: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.7775 - MinusLogProbMetric: 41.7775 - val_loss: 42.0660 - val_MinusLogProbMetric: 42.0660 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 784/1000
2023-10-27 19:44:27.709 
Epoch 784/1000 
	 loss: 41.5912, MinusLogProbMetric: 41.5912, val_loss: 42.9540, val_MinusLogProbMetric: 42.9540

Epoch 784: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.5912 - MinusLogProbMetric: 41.5912 - val_loss: 42.9540 - val_MinusLogProbMetric: 42.9540 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 785/1000
2023-10-27 19:45:01.415 
Epoch 785/1000 
	 loss: 41.8683, MinusLogProbMetric: 41.8683, val_loss: 42.1149, val_MinusLogProbMetric: 42.1149

Epoch 785: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8683 - MinusLogProbMetric: 41.8683 - val_loss: 42.1149 - val_MinusLogProbMetric: 42.1149 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 786/1000
2023-10-27 19:45:34.062 
Epoch 786/1000 
	 loss: 41.7024, MinusLogProbMetric: 41.7024, val_loss: 42.2531, val_MinusLogProbMetric: 42.2531

Epoch 786: val_loss did not improve from 42.02320
196/196 - 33s - loss: 41.7024 - MinusLogProbMetric: 41.7024 - val_loss: 42.2531 - val_MinusLogProbMetric: 42.2531 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 787/1000
2023-10-27 19:46:07.909 
Epoch 787/1000 
	 loss: 41.8788, MinusLogProbMetric: 41.8788, val_loss: 42.4962, val_MinusLogProbMetric: 42.4962

Epoch 787: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.8788 - MinusLogProbMetric: 41.8788 - val_loss: 42.4962 - val_MinusLogProbMetric: 42.4962 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 788/1000
2023-10-27 19:46:41.465 
Epoch 788/1000 
	 loss: 41.4569, MinusLogProbMetric: 41.4569, val_loss: 42.2967, val_MinusLogProbMetric: 42.2967

Epoch 788: val_loss did not improve from 42.02320
196/196 - 34s - loss: 41.4569 - MinusLogProbMetric: 41.4569 - val_loss: 42.2967 - val_MinusLogProbMetric: 42.2967 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 789/1000
2023-10-27 19:47:16.422 
Epoch 789/1000 
	 loss: 41.6141, MinusLogProbMetric: 41.6141, val_loss: 42.1103, val_MinusLogProbMetric: 42.1103

Epoch 789: val_loss did not improve from 42.02320
196/196 - 35s - loss: 41.6141 - MinusLogProbMetric: 41.6141 - val_loss: 42.1103 - val_MinusLogProbMetric: 42.1103 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 790/1000
2023-10-27 19:47:51.028 
Epoch 790/1000 
	 loss: 41.6699, MinusLogProbMetric: 41.6699, val_loss: 42.1609, val_MinusLogProbMetric: 42.1609

Epoch 790: val_loss did not improve from 42.02320
196/196 - 35s - loss: 41.6699 - MinusLogProbMetric: 41.6699 - val_loss: 42.1609 - val_MinusLogProbMetric: 42.1609 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 791/1000
2023-10-27 19:48:25.829 
Epoch 791/1000 
	 loss: 41.5560, MinusLogProbMetric: 41.5560, val_loss: 42.1926, val_MinusLogProbMetric: 42.1926

Epoch 791: val_loss did not improve from 42.02320
196/196 - 35s - loss: 41.5560 - MinusLogProbMetric: 41.5560 - val_loss: 42.1926 - val_MinusLogProbMetric: 42.1926 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 792/1000
2023-10-27 19:48:59.592 
Epoch 792/1000 
	 loss: 41.7149, MinusLogProbMetric: 41.7149, val_loss: 41.9769, val_MinusLogProbMetric: 41.9769

Epoch 792: val_loss improved from 42.02320 to 41.97686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 41.7149 - MinusLogProbMetric: 41.7149 - val_loss: 41.9769 - val_MinusLogProbMetric: 41.9769 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 793/1000
2023-10-27 19:49:32.931 
Epoch 793/1000 
	 loss: 41.5686, MinusLogProbMetric: 41.5686, val_loss: 42.0863, val_MinusLogProbMetric: 42.0863

Epoch 793: val_loss did not improve from 41.97686
196/196 - 33s - loss: 41.5686 - MinusLogProbMetric: 41.5686 - val_loss: 42.0863 - val_MinusLogProbMetric: 42.0863 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 794/1000
2023-10-27 19:50:06.604 
Epoch 794/1000 
	 loss: 41.5734, MinusLogProbMetric: 41.5734, val_loss: 42.3058, val_MinusLogProbMetric: 42.3058

Epoch 794: val_loss did not improve from 41.97686
196/196 - 34s - loss: 41.5734 - MinusLogProbMetric: 41.5734 - val_loss: 42.3058 - val_MinusLogProbMetric: 42.3058 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 795/1000
2023-10-27 19:50:40.519 
Epoch 795/1000 
	 loss: 41.8989, MinusLogProbMetric: 41.8989, val_loss: 41.9727, val_MinusLogProbMetric: 41.9727

Epoch 795: val_loss improved from 41.97686 to 41.97266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 41.8989 - MinusLogProbMetric: 41.8989 - val_loss: 41.9727 - val_MinusLogProbMetric: 41.9727 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 796/1000
2023-10-27 19:51:14.913 
Epoch 796/1000 
	 loss: 41.6647, MinusLogProbMetric: 41.6647, val_loss: 42.1133, val_MinusLogProbMetric: 42.1133

Epoch 796: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.6647 - MinusLogProbMetric: 41.6647 - val_loss: 42.1133 - val_MinusLogProbMetric: 42.1133 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 797/1000
2023-10-27 19:51:48.690 
Epoch 797/1000 
	 loss: 41.5478, MinusLogProbMetric: 41.5478, val_loss: 42.7362, val_MinusLogProbMetric: 42.7362

Epoch 797: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.5478 - MinusLogProbMetric: 41.5478 - val_loss: 42.7362 - val_MinusLogProbMetric: 42.7362 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 798/1000
2023-10-27 19:52:22.786 
Epoch 798/1000 
	 loss: 41.8894, MinusLogProbMetric: 41.8894, val_loss: 44.6157, val_MinusLogProbMetric: 44.6157

Epoch 798: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.8894 - MinusLogProbMetric: 41.8894 - val_loss: 44.6157 - val_MinusLogProbMetric: 44.6157 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 799/1000
2023-10-27 19:52:56.970 
Epoch 799/1000 
	 loss: 41.7324, MinusLogProbMetric: 41.7324, val_loss: 42.9462, val_MinusLogProbMetric: 42.9462

Epoch 799: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.7324 - MinusLogProbMetric: 41.7324 - val_loss: 42.9462 - val_MinusLogProbMetric: 42.9462 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 800/1000
2023-10-27 19:53:31.040 
Epoch 800/1000 
	 loss: 42.0686, MinusLogProbMetric: 42.0686, val_loss: 42.5712, val_MinusLogProbMetric: 42.5712

Epoch 800: val_loss did not improve from 41.97266
196/196 - 34s - loss: 42.0686 - MinusLogProbMetric: 42.0686 - val_loss: 42.5712 - val_MinusLogProbMetric: 42.5712 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 801/1000
2023-10-27 19:54:05.016 
Epoch 801/1000 
	 loss: 41.4806, MinusLogProbMetric: 41.4806, val_loss: 42.6836, val_MinusLogProbMetric: 42.6836

Epoch 801: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.4806 - MinusLogProbMetric: 41.4806 - val_loss: 42.6836 - val_MinusLogProbMetric: 42.6836 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 802/1000
2023-10-27 19:54:38.088 
Epoch 802/1000 
	 loss: 41.5820, MinusLogProbMetric: 41.5820, val_loss: 43.8155, val_MinusLogProbMetric: 43.8155

Epoch 802: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.5820 - MinusLogProbMetric: 41.5820 - val_loss: 43.8155 - val_MinusLogProbMetric: 43.8155 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 803/1000
2023-10-27 19:55:11.834 
Epoch 803/1000 
	 loss: 41.7759, MinusLogProbMetric: 41.7759, val_loss: 42.5717, val_MinusLogProbMetric: 42.5717

Epoch 803: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.7759 - MinusLogProbMetric: 41.7759 - val_loss: 42.5717 - val_MinusLogProbMetric: 42.5717 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 804/1000
2023-10-27 19:55:43.868 
Epoch 804/1000 
	 loss: 41.9656, MinusLogProbMetric: 41.9656, val_loss: 42.2603, val_MinusLogProbMetric: 42.2603

Epoch 804: val_loss did not improve from 41.97266
196/196 - 32s - loss: 41.9656 - MinusLogProbMetric: 41.9656 - val_loss: 42.2603 - val_MinusLogProbMetric: 42.2603 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 805/1000
2023-10-27 19:56:12.194 
Epoch 805/1000 
	 loss: 41.4896, MinusLogProbMetric: 41.4896, val_loss: 42.0772, val_MinusLogProbMetric: 42.0772

Epoch 805: val_loss did not improve from 41.97266
196/196 - 28s - loss: 41.4896 - MinusLogProbMetric: 41.4896 - val_loss: 42.0772 - val_MinusLogProbMetric: 42.0772 - lr: 1.6667e-04 - 28s/epoch - 145ms/step
Epoch 806/1000
2023-10-27 19:56:40.599 
Epoch 806/1000 
	 loss: 41.4393, MinusLogProbMetric: 41.4393, val_loss: 42.0272, val_MinusLogProbMetric: 42.0272

Epoch 806: val_loss did not improve from 41.97266
196/196 - 28s - loss: 41.4393 - MinusLogProbMetric: 41.4393 - val_loss: 42.0272 - val_MinusLogProbMetric: 42.0272 - lr: 1.6667e-04 - 28s/epoch - 145ms/step
Epoch 807/1000
2023-10-27 19:57:09.251 
Epoch 807/1000 
	 loss: 41.6400, MinusLogProbMetric: 41.6400, val_loss: 42.1446, val_MinusLogProbMetric: 42.1446

Epoch 807: val_loss did not improve from 41.97266
196/196 - 29s - loss: 41.6400 - MinusLogProbMetric: 41.6400 - val_loss: 42.1446 - val_MinusLogProbMetric: 42.1446 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 808/1000
2023-10-27 19:57:37.878 
Epoch 808/1000 
	 loss: 41.5502, MinusLogProbMetric: 41.5502, val_loss: 42.2069, val_MinusLogProbMetric: 42.2069

Epoch 808: val_loss did not improve from 41.97266
196/196 - 29s - loss: 41.5502 - MinusLogProbMetric: 41.5502 - val_loss: 42.2069 - val_MinusLogProbMetric: 42.2069 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 809/1000
2023-10-27 19:58:10.518 
Epoch 809/1000 
	 loss: 41.6015, MinusLogProbMetric: 41.6015, val_loss: 42.2290, val_MinusLogProbMetric: 42.2290

Epoch 809: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.6015 - MinusLogProbMetric: 41.6015 - val_loss: 42.2290 - val_MinusLogProbMetric: 42.2290 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 810/1000
2023-10-27 19:58:44.086 
Epoch 810/1000 
	 loss: 41.4932, MinusLogProbMetric: 41.4932, val_loss: 42.0822, val_MinusLogProbMetric: 42.0822

Epoch 810: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.4932 - MinusLogProbMetric: 41.4932 - val_loss: 42.0822 - val_MinusLogProbMetric: 42.0822 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 811/1000
2023-10-27 19:59:18.255 
Epoch 811/1000 
	 loss: 41.5062, MinusLogProbMetric: 41.5062, val_loss: 42.5107, val_MinusLogProbMetric: 42.5107

Epoch 811: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.5062 - MinusLogProbMetric: 41.5062 - val_loss: 42.5107 - val_MinusLogProbMetric: 42.5107 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 812/1000
2023-10-27 19:59:51.438 
Epoch 812/1000 
	 loss: 41.9477, MinusLogProbMetric: 41.9477, val_loss: 42.3017, val_MinusLogProbMetric: 42.3017

Epoch 812: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.9477 - MinusLogProbMetric: 41.9477 - val_loss: 42.3017 - val_MinusLogProbMetric: 42.3017 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 813/1000
2023-10-27 20:00:24.460 
Epoch 813/1000 
	 loss: 41.5573, MinusLogProbMetric: 41.5573, val_loss: 42.0093, val_MinusLogProbMetric: 42.0093

Epoch 813: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.5573 - MinusLogProbMetric: 41.5573 - val_loss: 42.0093 - val_MinusLogProbMetric: 42.0093 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 814/1000
2023-10-27 20:00:54.115 
Epoch 814/1000 
	 loss: 41.5712, MinusLogProbMetric: 41.5712, val_loss: 42.1128, val_MinusLogProbMetric: 42.1128

Epoch 814: val_loss did not improve from 41.97266
196/196 - 30s - loss: 41.5712 - MinusLogProbMetric: 41.5712 - val_loss: 42.1128 - val_MinusLogProbMetric: 42.1128 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 815/1000
2023-10-27 20:01:23.102 
Epoch 815/1000 
	 loss: 41.6668, MinusLogProbMetric: 41.6668, val_loss: 43.2935, val_MinusLogProbMetric: 43.2935

Epoch 815: val_loss did not improve from 41.97266
196/196 - 29s - loss: 41.6668 - MinusLogProbMetric: 41.6668 - val_loss: 43.2935 - val_MinusLogProbMetric: 43.2935 - lr: 1.6667e-04 - 29s/epoch - 148ms/step
Epoch 816/1000
2023-10-27 20:01:51.814 
Epoch 816/1000 
	 loss: 41.5891, MinusLogProbMetric: 41.5891, val_loss: 42.0530, val_MinusLogProbMetric: 42.0530

Epoch 816: val_loss did not improve from 41.97266
196/196 - 29s - loss: 41.5891 - MinusLogProbMetric: 41.5891 - val_loss: 42.0530 - val_MinusLogProbMetric: 42.0530 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 817/1000
2023-10-27 20:02:20.831 
Epoch 817/1000 
	 loss: 41.6221, MinusLogProbMetric: 41.6221, val_loss: 43.4211, val_MinusLogProbMetric: 43.4211

Epoch 817: val_loss did not improve from 41.97266
196/196 - 29s - loss: 41.6221 - MinusLogProbMetric: 41.6221 - val_loss: 43.4211 - val_MinusLogProbMetric: 43.4211 - lr: 1.6667e-04 - 29s/epoch - 148ms/step
Epoch 818/1000
2023-10-27 20:02:52.423 
Epoch 818/1000 
	 loss: 41.5829, MinusLogProbMetric: 41.5829, val_loss: 42.1270, val_MinusLogProbMetric: 42.1270

Epoch 818: val_loss did not improve from 41.97266
196/196 - 32s - loss: 41.5829 - MinusLogProbMetric: 41.5829 - val_loss: 42.1270 - val_MinusLogProbMetric: 42.1270 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 819/1000
2023-10-27 20:03:25.419 
Epoch 819/1000 
	 loss: 42.2997, MinusLogProbMetric: 42.2997, val_loss: 42.4029, val_MinusLogProbMetric: 42.4029

Epoch 819: val_loss did not improve from 41.97266
196/196 - 33s - loss: 42.2997 - MinusLogProbMetric: 42.2997 - val_loss: 42.4029 - val_MinusLogProbMetric: 42.4029 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 820/1000
2023-10-27 20:03:58.748 
Epoch 820/1000 
	 loss: 41.4610, MinusLogProbMetric: 41.4610, val_loss: 42.2187, val_MinusLogProbMetric: 42.2187

Epoch 820: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.4610 - MinusLogProbMetric: 41.4610 - val_loss: 42.2187 - val_MinusLogProbMetric: 42.2187 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 821/1000
2023-10-27 20:04:32.765 
Epoch 821/1000 
	 loss: 41.7162, MinusLogProbMetric: 41.7162, val_loss: 42.0645, val_MinusLogProbMetric: 42.0645

Epoch 821: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.7162 - MinusLogProbMetric: 41.7162 - val_loss: 42.0645 - val_MinusLogProbMetric: 42.0645 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 822/1000
2023-10-27 20:05:06.360 
Epoch 822/1000 
	 loss: 41.7159, MinusLogProbMetric: 41.7159, val_loss: 42.1041, val_MinusLogProbMetric: 42.1041

Epoch 822: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.7159 - MinusLogProbMetric: 41.7159 - val_loss: 42.1041 - val_MinusLogProbMetric: 42.1041 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 823/1000
2023-10-27 20:05:40.318 
Epoch 823/1000 
	 loss: 41.5239, MinusLogProbMetric: 41.5239, val_loss: 42.0888, val_MinusLogProbMetric: 42.0888

Epoch 823: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.5239 - MinusLogProbMetric: 41.5239 - val_loss: 42.0888 - val_MinusLogProbMetric: 42.0888 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 824/1000
2023-10-27 20:06:13.297 
Epoch 824/1000 
	 loss: 41.8654, MinusLogProbMetric: 41.8654, val_loss: 42.0554, val_MinusLogProbMetric: 42.0554

Epoch 824: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.8654 - MinusLogProbMetric: 41.8654 - val_loss: 42.0554 - val_MinusLogProbMetric: 42.0554 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 825/1000
2023-10-27 20:06:46.641 
Epoch 825/1000 
	 loss: 41.7847, MinusLogProbMetric: 41.7847, val_loss: 42.7387, val_MinusLogProbMetric: 42.7387

Epoch 825: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.7847 - MinusLogProbMetric: 41.7847 - val_loss: 42.7387 - val_MinusLogProbMetric: 42.7387 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 826/1000
2023-10-27 20:07:20.738 
Epoch 826/1000 
	 loss: 41.5101, MinusLogProbMetric: 41.5101, val_loss: 42.5364, val_MinusLogProbMetric: 42.5364

Epoch 826: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.5101 - MinusLogProbMetric: 41.5101 - val_loss: 42.5364 - val_MinusLogProbMetric: 42.5364 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 827/1000
2023-10-27 20:07:54.476 
Epoch 827/1000 
	 loss: 41.5667, MinusLogProbMetric: 41.5667, val_loss: 42.2694, val_MinusLogProbMetric: 42.2694

Epoch 827: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.5667 - MinusLogProbMetric: 41.5667 - val_loss: 42.2694 - val_MinusLogProbMetric: 42.2694 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 828/1000
2023-10-27 20:08:28.058 
Epoch 828/1000 
	 loss: 41.6325, MinusLogProbMetric: 41.6325, val_loss: 42.0870, val_MinusLogProbMetric: 42.0870

Epoch 828: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.6325 - MinusLogProbMetric: 41.6325 - val_loss: 42.0870 - val_MinusLogProbMetric: 42.0870 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 829/1000
2023-10-27 20:09:01.827 
Epoch 829/1000 
	 loss: 41.6791, MinusLogProbMetric: 41.6791, val_loss: 42.1551, val_MinusLogProbMetric: 42.1551

Epoch 829: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.6791 - MinusLogProbMetric: 41.6791 - val_loss: 42.1551 - val_MinusLogProbMetric: 42.1551 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 830/1000
2023-10-27 20:09:34.913 
Epoch 830/1000 
	 loss: 42.0428, MinusLogProbMetric: 42.0428, val_loss: 42.1166, val_MinusLogProbMetric: 42.1166

Epoch 830: val_loss did not improve from 41.97266
196/196 - 33s - loss: 42.0428 - MinusLogProbMetric: 42.0428 - val_loss: 42.1166 - val_MinusLogProbMetric: 42.1166 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 831/1000
2023-10-27 20:10:08.204 
Epoch 831/1000 
	 loss: 41.4906, MinusLogProbMetric: 41.4906, val_loss: 42.3191, val_MinusLogProbMetric: 42.3191

Epoch 831: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.4906 - MinusLogProbMetric: 41.4906 - val_loss: 42.3191 - val_MinusLogProbMetric: 42.3191 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 832/1000
2023-10-27 20:10:42.313 
Epoch 832/1000 
	 loss: 41.8020, MinusLogProbMetric: 41.8020, val_loss: 42.3800, val_MinusLogProbMetric: 42.3800

Epoch 832: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.8020 - MinusLogProbMetric: 41.8020 - val_loss: 42.3800 - val_MinusLogProbMetric: 42.3800 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 833/1000
2023-10-27 20:11:16.381 
Epoch 833/1000 
	 loss: 41.7439, MinusLogProbMetric: 41.7439, val_loss: 42.1923, val_MinusLogProbMetric: 42.1923

Epoch 833: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.7439 - MinusLogProbMetric: 41.7439 - val_loss: 42.1923 - val_MinusLogProbMetric: 42.1923 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 834/1000
2023-10-27 20:11:49.985 
Epoch 834/1000 
	 loss: 41.7940, MinusLogProbMetric: 41.7940, val_loss: 42.1929, val_MinusLogProbMetric: 42.1929

Epoch 834: val_loss did not improve from 41.97266
196/196 - 34s - loss: 41.7940 - MinusLogProbMetric: 41.7940 - val_loss: 42.1929 - val_MinusLogProbMetric: 42.1929 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 835/1000
2023-10-27 20:12:23.247 
Epoch 835/1000 
	 loss: 41.5432, MinusLogProbMetric: 41.5432, val_loss: 42.3301, val_MinusLogProbMetric: 42.3301

Epoch 835: val_loss did not improve from 41.97266
196/196 - 33s - loss: 41.5432 - MinusLogProbMetric: 41.5432 - val_loss: 42.3301 - val_MinusLogProbMetric: 42.3301 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 836/1000
2023-10-27 20:12:56.177 
Epoch 836/1000 
	 loss: 41.7348, MinusLogProbMetric: 41.7348, val_loss: 41.8746, val_MinusLogProbMetric: 41.8746

Epoch 836: val_loss improved from 41.97266 to 41.87463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 33s - loss: 41.7348 - MinusLogProbMetric: 41.7348 - val_loss: 41.8746 - val_MinusLogProbMetric: 41.8746 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 837/1000
2023-10-27 20:13:28.761 
Epoch 837/1000 
	 loss: 41.6285, MinusLogProbMetric: 41.6285, val_loss: 42.1639, val_MinusLogProbMetric: 42.1639

Epoch 837: val_loss did not improve from 41.87463
196/196 - 32s - loss: 41.6285 - MinusLogProbMetric: 41.6285 - val_loss: 42.1639 - val_MinusLogProbMetric: 42.1639 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 838/1000
2023-10-27 20:14:02.401 
Epoch 838/1000 
	 loss: 41.7499, MinusLogProbMetric: 41.7499, val_loss: 44.7106, val_MinusLogProbMetric: 44.7106

Epoch 838: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.7499 - MinusLogProbMetric: 41.7499 - val_loss: 44.7106 - val_MinusLogProbMetric: 44.7106 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 839/1000
2023-10-27 20:14:36.585 
Epoch 839/1000 
	 loss: 41.7288, MinusLogProbMetric: 41.7288, val_loss: 42.6544, val_MinusLogProbMetric: 42.6544

Epoch 839: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.7288 - MinusLogProbMetric: 41.7288 - val_loss: 42.6544 - val_MinusLogProbMetric: 42.6544 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 840/1000
2023-10-27 20:15:11.207 
Epoch 840/1000 
	 loss: 41.8734, MinusLogProbMetric: 41.8734, val_loss: 41.9511, val_MinusLogProbMetric: 41.9511

Epoch 840: val_loss did not improve from 41.87463
196/196 - 35s - loss: 41.8734 - MinusLogProbMetric: 41.8734 - val_loss: 41.9511 - val_MinusLogProbMetric: 41.9511 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 841/1000
2023-10-27 20:15:45.020 
Epoch 841/1000 
	 loss: 41.5602, MinusLogProbMetric: 41.5602, val_loss: 42.8888, val_MinusLogProbMetric: 42.8888

Epoch 841: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.5602 - MinusLogProbMetric: 41.5602 - val_loss: 42.8888 - val_MinusLogProbMetric: 42.8888 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 842/1000
2023-10-27 20:16:19.275 
Epoch 842/1000 
	 loss: 41.7055, MinusLogProbMetric: 41.7055, val_loss: 42.5078, val_MinusLogProbMetric: 42.5078

Epoch 842: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.7055 - MinusLogProbMetric: 41.7055 - val_loss: 42.5078 - val_MinusLogProbMetric: 42.5078 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 843/1000
2023-10-27 20:16:53.170 
Epoch 843/1000 
	 loss: 41.5202, MinusLogProbMetric: 41.5202, val_loss: 42.1255, val_MinusLogProbMetric: 42.1255

Epoch 843: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.5202 - MinusLogProbMetric: 41.5202 - val_loss: 42.1255 - val_MinusLogProbMetric: 42.1255 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 844/1000
2023-10-27 20:17:27.083 
Epoch 844/1000 
	 loss: 41.4264, MinusLogProbMetric: 41.4264, val_loss: 42.6706, val_MinusLogProbMetric: 42.6706

Epoch 844: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.4264 - MinusLogProbMetric: 41.4264 - val_loss: 42.6706 - val_MinusLogProbMetric: 42.6706 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 845/1000
2023-10-27 20:18:00.721 
Epoch 845/1000 
	 loss: 41.6013, MinusLogProbMetric: 41.6013, val_loss: 42.0554, val_MinusLogProbMetric: 42.0554

Epoch 845: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.6013 - MinusLogProbMetric: 41.6013 - val_loss: 42.0554 - val_MinusLogProbMetric: 42.0554 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 846/1000
2023-10-27 20:18:34.357 
Epoch 846/1000 
	 loss: 41.3915, MinusLogProbMetric: 41.3915, val_loss: 43.5953, val_MinusLogProbMetric: 43.5953

Epoch 846: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.3915 - MinusLogProbMetric: 41.3915 - val_loss: 43.5953 - val_MinusLogProbMetric: 43.5953 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 847/1000
2023-10-27 20:19:08.515 
Epoch 847/1000 
	 loss: 41.7910, MinusLogProbMetric: 41.7910, val_loss: 41.9086, val_MinusLogProbMetric: 41.9086

Epoch 847: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.7910 - MinusLogProbMetric: 41.7910 - val_loss: 41.9086 - val_MinusLogProbMetric: 41.9086 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 848/1000
2023-10-27 20:19:41.672 
Epoch 848/1000 
	 loss: 41.8227, MinusLogProbMetric: 41.8227, val_loss: 42.3041, val_MinusLogProbMetric: 42.3041

Epoch 848: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.8227 - MinusLogProbMetric: 41.8227 - val_loss: 42.3041 - val_MinusLogProbMetric: 42.3041 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 849/1000
2023-10-27 20:20:15.430 
Epoch 849/1000 
	 loss: 41.4334, MinusLogProbMetric: 41.4334, val_loss: 42.0565, val_MinusLogProbMetric: 42.0565

Epoch 849: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.4334 - MinusLogProbMetric: 41.4334 - val_loss: 42.0565 - val_MinusLogProbMetric: 42.0565 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 850/1000
2023-10-27 20:20:49.310 
Epoch 850/1000 
	 loss: 41.8248, MinusLogProbMetric: 41.8248, val_loss: 42.0674, val_MinusLogProbMetric: 42.0674

Epoch 850: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.8248 - MinusLogProbMetric: 41.8248 - val_loss: 42.0674 - val_MinusLogProbMetric: 42.0674 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 851/1000
2023-10-27 20:21:22.673 
Epoch 851/1000 
	 loss: 41.5377, MinusLogProbMetric: 41.5377, val_loss: 42.6495, val_MinusLogProbMetric: 42.6495

Epoch 851: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.5377 - MinusLogProbMetric: 41.5377 - val_loss: 42.6495 - val_MinusLogProbMetric: 42.6495 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 852/1000
2023-10-27 20:21:56.504 
Epoch 852/1000 
	 loss: 42.0062, MinusLogProbMetric: 42.0062, val_loss: 42.1309, val_MinusLogProbMetric: 42.1309

Epoch 852: val_loss did not improve from 41.87463
196/196 - 34s - loss: 42.0062 - MinusLogProbMetric: 42.0062 - val_loss: 42.1309 - val_MinusLogProbMetric: 42.1309 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 853/1000
2023-10-27 20:22:30.551 
Epoch 853/1000 
	 loss: 41.5930, MinusLogProbMetric: 41.5930, val_loss: 42.1216, val_MinusLogProbMetric: 42.1216

Epoch 853: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.5930 - MinusLogProbMetric: 41.5930 - val_loss: 42.1216 - val_MinusLogProbMetric: 42.1216 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 854/1000
2023-10-27 20:23:03.171 
Epoch 854/1000 
	 loss: 41.7722, MinusLogProbMetric: 41.7722, val_loss: 42.1815, val_MinusLogProbMetric: 42.1815

Epoch 854: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.7722 - MinusLogProbMetric: 41.7722 - val_loss: 42.1815 - val_MinusLogProbMetric: 42.1815 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 855/1000
2023-10-27 20:23:36.596 
Epoch 855/1000 
	 loss: 41.7191, MinusLogProbMetric: 41.7191, val_loss: 42.0010, val_MinusLogProbMetric: 42.0010

Epoch 855: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.7191 - MinusLogProbMetric: 41.7191 - val_loss: 42.0010 - val_MinusLogProbMetric: 42.0010 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 856/1000
2023-10-27 20:24:09.996 
Epoch 856/1000 
	 loss: 41.3828, MinusLogProbMetric: 41.3828, val_loss: 41.9775, val_MinusLogProbMetric: 41.9775

Epoch 856: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.3828 - MinusLogProbMetric: 41.3828 - val_loss: 41.9775 - val_MinusLogProbMetric: 41.9775 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 857/1000
2023-10-27 20:24:43.686 
Epoch 857/1000 
	 loss: 41.5539, MinusLogProbMetric: 41.5539, val_loss: 44.1886, val_MinusLogProbMetric: 44.1886

Epoch 857: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.5539 - MinusLogProbMetric: 41.5539 - val_loss: 44.1886 - val_MinusLogProbMetric: 44.1886 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 858/1000
2023-10-27 20:25:17.971 
Epoch 858/1000 
	 loss: 41.9909, MinusLogProbMetric: 41.9909, val_loss: 44.8081, val_MinusLogProbMetric: 44.8081

Epoch 858: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.9909 - MinusLogProbMetric: 41.9909 - val_loss: 44.8081 - val_MinusLogProbMetric: 44.8081 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 859/1000
2023-10-27 20:25:51.856 
Epoch 859/1000 
	 loss: 41.4417, MinusLogProbMetric: 41.4417, val_loss: 42.1982, val_MinusLogProbMetric: 42.1982

Epoch 859: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.4417 - MinusLogProbMetric: 41.4417 - val_loss: 42.1982 - val_MinusLogProbMetric: 42.1982 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 860/1000
2023-10-27 20:26:24.511 
Epoch 860/1000 
	 loss: 41.4278, MinusLogProbMetric: 41.4278, val_loss: 41.9290, val_MinusLogProbMetric: 41.9290

Epoch 860: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.4278 - MinusLogProbMetric: 41.4278 - val_loss: 41.9290 - val_MinusLogProbMetric: 41.9290 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 861/1000
2023-10-27 20:26:58.375 
Epoch 861/1000 
	 loss: 41.7116, MinusLogProbMetric: 41.7116, val_loss: 42.3415, val_MinusLogProbMetric: 42.3415

Epoch 861: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.7116 - MinusLogProbMetric: 41.7116 - val_loss: 42.3415 - val_MinusLogProbMetric: 42.3415 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 862/1000
2023-10-27 20:27:31.678 
Epoch 862/1000 
	 loss: 41.7144, MinusLogProbMetric: 41.7144, val_loss: 42.2238, val_MinusLogProbMetric: 42.2238

Epoch 862: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.7144 - MinusLogProbMetric: 41.7144 - val_loss: 42.2238 - val_MinusLogProbMetric: 42.2238 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 863/1000
2023-10-27 20:28:05.025 
Epoch 863/1000 
	 loss: 41.9058, MinusLogProbMetric: 41.9058, val_loss: 42.8925, val_MinusLogProbMetric: 42.8925

Epoch 863: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.9058 - MinusLogProbMetric: 41.9058 - val_loss: 42.8925 - val_MinusLogProbMetric: 42.8925 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 864/1000
2023-10-27 20:28:38.656 
Epoch 864/1000 
	 loss: 41.6021, MinusLogProbMetric: 41.6021, val_loss: 42.0680, val_MinusLogProbMetric: 42.0680

Epoch 864: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.6021 - MinusLogProbMetric: 41.6021 - val_loss: 42.0680 - val_MinusLogProbMetric: 42.0680 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 865/1000
2023-10-27 20:29:11.991 
Epoch 865/1000 
	 loss: 41.3656, MinusLogProbMetric: 41.3656, val_loss: 42.0119, val_MinusLogProbMetric: 42.0119

Epoch 865: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.3656 - MinusLogProbMetric: 41.3656 - val_loss: 42.0119 - val_MinusLogProbMetric: 42.0119 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 866/1000
2023-10-27 20:29:45.310 
Epoch 866/1000 
	 loss: 41.9694, MinusLogProbMetric: 41.9694, val_loss: 42.1578, val_MinusLogProbMetric: 42.1578

Epoch 866: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.9694 - MinusLogProbMetric: 41.9694 - val_loss: 42.1578 - val_MinusLogProbMetric: 42.1578 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 867/1000
2023-10-27 20:30:18.906 
Epoch 867/1000 
	 loss: 41.6532, MinusLogProbMetric: 41.6532, val_loss: 42.1513, val_MinusLogProbMetric: 42.1513

Epoch 867: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.6532 - MinusLogProbMetric: 41.6532 - val_loss: 42.1513 - val_MinusLogProbMetric: 42.1513 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 868/1000
2023-10-27 20:30:52.833 
Epoch 868/1000 
	 loss: 41.5287, MinusLogProbMetric: 41.5287, val_loss: 43.5746, val_MinusLogProbMetric: 43.5746

Epoch 868: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.5287 - MinusLogProbMetric: 41.5287 - val_loss: 43.5746 - val_MinusLogProbMetric: 43.5746 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 869/1000
2023-10-27 20:31:27.499 
Epoch 869/1000 
	 loss: 41.6939, MinusLogProbMetric: 41.6939, val_loss: 42.9712, val_MinusLogProbMetric: 42.9712

Epoch 869: val_loss did not improve from 41.87463
196/196 - 35s - loss: 41.6939 - MinusLogProbMetric: 41.6939 - val_loss: 42.9712 - val_MinusLogProbMetric: 42.9712 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 870/1000
2023-10-27 20:32:01.829 
Epoch 870/1000 
	 loss: 41.7263, MinusLogProbMetric: 41.7263, val_loss: 42.5344, val_MinusLogProbMetric: 42.5344

Epoch 870: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.7263 - MinusLogProbMetric: 41.7263 - val_loss: 42.5344 - val_MinusLogProbMetric: 42.5344 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 871/1000
2023-10-27 20:32:35.417 
Epoch 871/1000 
	 loss: 41.5085, MinusLogProbMetric: 41.5085, val_loss: 42.9933, val_MinusLogProbMetric: 42.9933

Epoch 871: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.5085 - MinusLogProbMetric: 41.5085 - val_loss: 42.9933 - val_MinusLogProbMetric: 42.9933 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 872/1000
2023-10-27 20:33:09.042 
Epoch 872/1000 
	 loss: 41.7558, MinusLogProbMetric: 41.7558, val_loss: 42.1821, val_MinusLogProbMetric: 42.1821

Epoch 872: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.7558 - MinusLogProbMetric: 41.7558 - val_loss: 42.1821 - val_MinusLogProbMetric: 42.1821 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 873/1000
2023-10-27 20:33:42.569 
Epoch 873/1000 
	 loss: 41.4204, MinusLogProbMetric: 41.4204, val_loss: 42.2297, val_MinusLogProbMetric: 42.2297

Epoch 873: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.4204 - MinusLogProbMetric: 41.4204 - val_loss: 42.2297 - val_MinusLogProbMetric: 42.2297 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 874/1000
2023-10-27 20:34:16.498 
Epoch 874/1000 
	 loss: 41.6643, MinusLogProbMetric: 41.6643, val_loss: 43.1460, val_MinusLogProbMetric: 43.1460

Epoch 874: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.6643 - MinusLogProbMetric: 41.6643 - val_loss: 43.1460 - val_MinusLogProbMetric: 43.1460 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 875/1000
2023-10-27 20:34:50.197 
Epoch 875/1000 
	 loss: 42.0644, MinusLogProbMetric: 42.0644, val_loss: 42.2424, val_MinusLogProbMetric: 42.2424

Epoch 875: val_loss did not improve from 41.87463
196/196 - 34s - loss: 42.0644 - MinusLogProbMetric: 42.0644 - val_loss: 42.2424 - val_MinusLogProbMetric: 42.2424 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 876/1000
2023-10-27 20:35:23.902 
Epoch 876/1000 
	 loss: 41.5736, MinusLogProbMetric: 41.5736, val_loss: 43.3379, val_MinusLogProbMetric: 43.3379

Epoch 876: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.5736 - MinusLogProbMetric: 41.5736 - val_loss: 43.3379 - val_MinusLogProbMetric: 43.3379 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 877/1000
2023-10-27 20:35:57.808 
Epoch 877/1000 
	 loss: 41.8391, MinusLogProbMetric: 41.8391, val_loss: 42.1102, val_MinusLogProbMetric: 42.1102

Epoch 877: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.8391 - MinusLogProbMetric: 41.8391 - val_loss: 42.1102 - val_MinusLogProbMetric: 42.1102 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 878/1000
2023-10-27 20:36:30.784 
Epoch 878/1000 
	 loss: 41.6854, MinusLogProbMetric: 41.6854, val_loss: 42.2654, val_MinusLogProbMetric: 42.2654

Epoch 878: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.6854 - MinusLogProbMetric: 41.6854 - val_loss: 42.2654 - val_MinusLogProbMetric: 42.2654 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 879/1000
2023-10-27 20:37:05.085 
Epoch 879/1000 
	 loss: 41.6717, MinusLogProbMetric: 41.6717, val_loss: 42.3727, val_MinusLogProbMetric: 42.3727

Epoch 879: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.6717 - MinusLogProbMetric: 41.6717 - val_loss: 42.3727 - val_MinusLogProbMetric: 42.3727 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 880/1000
2023-10-27 20:37:38.209 
Epoch 880/1000 
	 loss: 41.5457, MinusLogProbMetric: 41.5457, val_loss: 42.4758, val_MinusLogProbMetric: 42.4758

Epoch 880: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.5457 - MinusLogProbMetric: 41.5457 - val_loss: 42.4758 - val_MinusLogProbMetric: 42.4758 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 881/1000
2023-10-27 20:38:12.087 
Epoch 881/1000 
	 loss: 41.7860, MinusLogProbMetric: 41.7860, val_loss: 43.0988, val_MinusLogProbMetric: 43.0988

Epoch 881: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.7860 - MinusLogProbMetric: 41.7860 - val_loss: 43.0988 - val_MinusLogProbMetric: 43.0988 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 882/1000
2023-10-27 20:38:45.958 
Epoch 882/1000 
	 loss: 41.4988, MinusLogProbMetric: 41.4988, val_loss: 43.8897, val_MinusLogProbMetric: 43.8897

Epoch 882: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.4988 - MinusLogProbMetric: 41.4988 - val_loss: 43.8897 - val_MinusLogProbMetric: 43.8897 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 883/1000
2023-10-27 20:39:19.514 
Epoch 883/1000 
	 loss: 41.4580, MinusLogProbMetric: 41.4580, val_loss: 42.5546, val_MinusLogProbMetric: 42.5546

Epoch 883: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.4580 - MinusLogProbMetric: 41.4580 - val_loss: 42.5546 - val_MinusLogProbMetric: 42.5546 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 884/1000
2023-10-27 20:39:52.828 
Epoch 884/1000 
	 loss: 41.7301, MinusLogProbMetric: 41.7301, val_loss: 43.3174, val_MinusLogProbMetric: 43.3174

Epoch 884: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.7301 - MinusLogProbMetric: 41.7301 - val_loss: 43.3174 - val_MinusLogProbMetric: 43.3174 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 885/1000
2023-10-27 20:40:26.605 
Epoch 885/1000 
	 loss: 41.8125, MinusLogProbMetric: 41.8125, val_loss: 42.2890, val_MinusLogProbMetric: 42.2890

Epoch 885: val_loss did not improve from 41.87463
196/196 - 34s - loss: 41.8125 - MinusLogProbMetric: 41.8125 - val_loss: 42.2890 - val_MinusLogProbMetric: 42.2890 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 886/1000
2023-10-27 20:40:59.564 
Epoch 886/1000 
	 loss: 41.6000, MinusLogProbMetric: 41.6000, val_loss: 42.1536, val_MinusLogProbMetric: 42.1536

Epoch 886: val_loss did not improve from 41.87463
196/196 - 33s - loss: 41.6000 - MinusLogProbMetric: 41.6000 - val_loss: 42.1536 - val_MinusLogProbMetric: 42.1536 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 887/1000
2023-10-27 20:41:33.332 
Epoch 887/1000 
	 loss: 41.0234, MinusLogProbMetric: 41.0234, val_loss: 41.8609, val_MinusLogProbMetric: 41.8609

Epoch 887: val_loss improved from 41.87463 to 41.86092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 41.0234 - MinusLogProbMetric: 41.0234 - val_loss: 41.8609 - val_MinusLogProbMetric: 41.8609 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 888/1000
2023-10-27 20:42:07.787 
Epoch 888/1000 
	 loss: 41.0068, MinusLogProbMetric: 41.0068, val_loss: 41.8040, val_MinusLogProbMetric: 41.8040

Epoch 888: val_loss improved from 41.86092 to 41.80400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 41.0068 - MinusLogProbMetric: 41.0068 - val_loss: 41.8040 - val_MinusLogProbMetric: 41.8040 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 889/1000
2023-10-27 20:42:42.582 
Epoch 889/1000 
	 loss: 40.9596, MinusLogProbMetric: 40.9596, val_loss: 42.0964, val_MinusLogProbMetric: 42.0964

Epoch 889: val_loss did not improve from 41.80400
196/196 - 34s - loss: 40.9596 - MinusLogProbMetric: 40.9596 - val_loss: 42.0964 - val_MinusLogProbMetric: 42.0964 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 890/1000
2023-10-27 20:43:16.140 
Epoch 890/1000 
	 loss: 40.9904, MinusLogProbMetric: 40.9904, val_loss: 41.7260, val_MinusLogProbMetric: 41.7260

Epoch 890: val_loss improved from 41.80400 to 41.72596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.9904 - MinusLogProbMetric: 40.9904 - val_loss: 41.7260 - val_MinusLogProbMetric: 41.7260 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 891/1000
2023-10-27 20:43:50.555 
Epoch 891/1000 
	 loss: 40.9872, MinusLogProbMetric: 40.9872, val_loss: 41.8739, val_MinusLogProbMetric: 41.8739

Epoch 891: val_loss did not improve from 41.72596
196/196 - 34s - loss: 40.9872 - MinusLogProbMetric: 40.9872 - val_loss: 41.8739 - val_MinusLogProbMetric: 41.8739 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 892/1000
2023-10-27 20:44:23.524 
Epoch 892/1000 
	 loss: 41.1493, MinusLogProbMetric: 41.1493, val_loss: 41.7794, val_MinusLogProbMetric: 41.7794

Epoch 892: val_loss did not improve from 41.72596
196/196 - 33s - loss: 41.1493 - MinusLogProbMetric: 41.1493 - val_loss: 41.7794 - val_MinusLogProbMetric: 41.7794 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 893/1000
2023-10-27 20:44:57.645 
Epoch 893/1000 
	 loss: 40.9866, MinusLogProbMetric: 40.9866, val_loss: 41.8109, val_MinusLogProbMetric: 41.8109

Epoch 893: val_loss did not improve from 41.72596
196/196 - 34s - loss: 40.9866 - MinusLogProbMetric: 40.9866 - val_loss: 41.8109 - val_MinusLogProbMetric: 41.8109 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 894/1000
2023-10-27 20:45:31.554 
Epoch 894/1000 
	 loss: 40.9771, MinusLogProbMetric: 40.9771, val_loss: 41.7165, val_MinusLogProbMetric: 41.7165

Epoch 894: val_loss improved from 41.72596 to 41.71655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.9771 - MinusLogProbMetric: 40.9771 - val_loss: 41.7165 - val_MinusLogProbMetric: 41.7165 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 895/1000
2023-10-27 20:46:06.297 
Epoch 895/1000 
	 loss: 41.2263, MinusLogProbMetric: 41.2263, val_loss: 41.8159, val_MinusLogProbMetric: 41.8159

Epoch 895: val_loss did not improve from 41.71655
196/196 - 34s - loss: 41.2263 - MinusLogProbMetric: 41.2263 - val_loss: 41.8159 - val_MinusLogProbMetric: 41.8159 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 896/1000
2023-10-27 20:46:40.018 
Epoch 896/1000 
	 loss: 40.9542, MinusLogProbMetric: 40.9542, val_loss: 41.7426, val_MinusLogProbMetric: 41.7426

Epoch 896: val_loss did not improve from 41.71655
196/196 - 34s - loss: 40.9542 - MinusLogProbMetric: 40.9542 - val_loss: 41.7426 - val_MinusLogProbMetric: 41.7426 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 897/1000
2023-10-27 20:47:14.173 
Epoch 897/1000 
	 loss: 40.9928, MinusLogProbMetric: 40.9928, val_loss: 41.8789, val_MinusLogProbMetric: 41.8789

Epoch 897: val_loss did not improve from 41.71655
196/196 - 34s - loss: 40.9928 - MinusLogProbMetric: 40.9928 - val_loss: 41.8789 - val_MinusLogProbMetric: 41.8789 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 898/1000
2023-10-27 20:47:47.924 
Epoch 898/1000 
	 loss: 40.9497, MinusLogProbMetric: 40.9497, val_loss: 41.7215, val_MinusLogProbMetric: 41.7215

Epoch 898: val_loss did not improve from 41.71655
196/196 - 34s - loss: 40.9497 - MinusLogProbMetric: 40.9497 - val_loss: 41.7215 - val_MinusLogProbMetric: 41.7215 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 899/1000
2023-10-27 20:48:21.372 
Epoch 899/1000 
	 loss: 41.0246, MinusLogProbMetric: 41.0246, val_loss: 41.7125, val_MinusLogProbMetric: 41.7125

Epoch 899: val_loss improved from 41.71655 to 41.71247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 41.0246 - MinusLogProbMetric: 41.0246 - val_loss: 41.7125 - val_MinusLogProbMetric: 41.7125 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 900/1000
2023-10-27 20:48:55.416 
Epoch 900/1000 
	 loss: 40.9881, MinusLogProbMetric: 40.9881, val_loss: 41.8000, val_MinusLogProbMetric: 41.8000

Epoch 900: val_loss did not improve from 41.71247
196/196 - 34s - loss: 40.9881 - MinusLogProbMetric: 40.9881 - val_loss: 41.8000 - val_MinusLogProbMetric: 41.8000 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 901/1000
2023-10-27 20:49:29.042 
Epoch 901/1000 
	 loss: 40.9861, MinusLogProbMetric: 40.9861, val_loss: 41.7628, val_MinusLogProbMetric: 41.7628

Epoch 901: val_loss did not improve from 41.71247
196/196 - 34s - loss: 40.9861 - MinusLogProbMetric: 40.9861 - val_loss: 41.7628 - val_MinusLogProbMetric: 41.7628 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 902/1000
2023-10-27 20:50:02.791 
Epoch 902/1000 
	 loss: 40.9885, MinusLogProbMetric: 40.9885, val_loss: 41.7570, val_MinusLogProbMetric: 41.7570

Epoch 902: val_loss did not improve from 41.71247
196/196 - 34s - loss: 40.9885 - MinusLogProbMetric: 40.9885 - val_loss: 41.7570 - val_MinusLogProbMetric: 41.7570 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 903/1000
2023-10-27 20:50:35.987 
Epoch 903/1000 
	 loss: 41.3756, MinusLogProbMetric: 41.3756, val_loss: 41.7239, val_MinusLogProbMetric: 41.7239

Epoch 903: val_loss did not improve from 41.71247
196/196 - 33s - loss: 41.3756 - MinusLogProbMetric: 41.3756 - val_loss: 41.7239 - val_MinusLogProbMetric: 41.7239 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 904/1000
2023-10-27 20:51:09.813 
Epoch 904/1000 
	 loss: 40.9639, MinusLogProbMetric: 40.9639, val_loss: 41.9666, val_MinusLogProbMetric: 41.9666

Epoch 904: val_loss did not improve from 41.71247
196/196 - 34s - loss: 40.9639 - MinusLogProbMetric: 40.9639 - val_loss: 41.9666 - val_MinusLogProbMetric: 41.9666 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 905/1000
2023-10-27 20:51:43.555 
Epoch 905/1000 
	 loss: 40.9775, MinusLogProbMetric: 40.9775, val_loss: 41.8155, val_MinusLogProbMetric: 41.8155

Epoch 905: val_loss did not improve from 41.71247
196/196 - 34s - loss: 40.9775 - MinusLogProbMetric: 40.9775 - val_loss: 41.8155 - val_MinusLogProbMetric: 41.8155 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 906/1000
2023-10-27 20:52:17.814 
Epoch 906/1000 
	 loss: 41.0280, MinusLogProbMetric: 41.0280, val_loss: 41.6605, val_MinusLogProbMetric: 41.6605

Epoch 906: val_loss improved from 41.71247 to 41.66045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 41.0280 - MinusLogProbMetric: 41.0280 - val_loss: 41.6605 - val_MinusLogProbMetric: 41.6605 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 907/1000
2023-10-27 20:52:51.986 
Epoch 907/1000 
	 loss: 40.9883, MinusLogProbMetric: 40.9883, val_loss: 42.6143, val_MinusLogProbMetric: 42.6143

Epoch 907: val_loss did not improve from 41.66045
196/196 - 34s - loss: 40.9883 - MinusLogProbMetric: 40.9883 - val_loss: 42.6143 - val_MinusLogProbMetric: 42.6143 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 908/1000
2023-10-27 20:53:24.979 
Epoch 908/1000 
	 loss: 41.2312, MinusLogProbMetric: 41.2312, val_loss: 41.7575, val_MinusLogProbMetric: 41.7575

Epoch 908: val_loss did not improve from 41.66045
196/196 - 33s - loss: 41.2312 - MinusLogProbMetric: 41.2312 - val_loss: 41.7575 - val_MinusLogProbMetric: 41.7575 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 909/1000
2023-10-27 20:53:58.818 
Epoch 909/1000 
	 loss: 40.9673, MinusLogProbMetric: 40.9673, val_loss: 41.7865, val_MinusLogProbMetric: 41.7865

Epoch 909: val_loss did not improve from 41.66045
196/196 - 34s - loss: 40.9673 - MinusLogProbMetric: 40.9673 - val_loss: 41.7865 - val_MinusLogProbMetric: 41.7865 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 910/1000
2023-10-27 20:54:32.698 
Epoch 910/1000 
	 loss: 40.9210, MinusLogProbMetric: 40.9210, val_loss: 41.8268, val_MinusLogProbMetric: 41.8268

Epoch 910: val_loss did not improve from 41.66045
196/196 - 34s - loss: 40.9210 - MinusLogProbMetric: 40.9210 - val_loss: 41.8268 - val_MinusLogProbMetric: 41.8268 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 911/1000
2023-10-27 20:55:06.753 
Epoch 911/1000 
	 loss: 40.9706, MinusLogProbMetric: 40.9706, val_loss: 41.7369, val_MinusLogProbMetric: 41.7369

Epoch 911: val_loss did not improve from 41.66045
196/196 - 34s - loss: 40.9706 - MinusLogProbMetric: 40.9706 - val_loss: 41.7369 - val_MinusLogProbMetric: 41.7369 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 912/1000
2023-10-27 20:55:40.510 
Epoch 912/1000 
	 loss: 40.9653, MinusLogProbMetric: 40.9653, val_loss: 41.6551, val_MinusLogProbMetric: 41.6551

Epoch 912: val_loss improved from 41.66045 to 41.65513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.9653 - MinusLogProbMetric: 40.9653 - val_loss: 41.6551 - val_MinusLogProbMetric: 41.6551 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 913/1000
2023-10-27 20:56:14.607 
Epoch 913/1000 
	 loss: 40.9493, MinusLogProbMetric: 40.9493, val_loss: 41.6677, val_MinusLogProbMetric: 41.6677

Epoch 913: val_loss did not improve from 41.65513
196/196 - 34s - loss: 40.9493 - MinusLogProbMetric: 40.9493 - val_loss: 41.6677 - val_MinusLogProbMetric: 41.6677 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 914/1000
2023-10-27 20:56:48.549 
Epoch 914/1000 
	 loss: 40.9844, MinusLogProbMetric: 40.9844, val_loss: 41.7862, val_MinusLogProbMetric: 41.7862

Epoch 914: val_loss did not improve from 41.65513
196/196 - 34s - loss: 40.9844 - MinusLogProbMetric: 40.9844 - val_loss: 41.7862 - val_MinusLogProbMetric: 41.7862 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 915/1000
2023-10-27 20:57:22.141 
Epoch 915/1000 
	 loss: 40.9916, MinusLogProbMetric: 40.9916, val_loss: 41.7596, val_MinusLogProbMetric: 41.7596

Epoch 915: val_loss did not improve from 41.65513
196/196 - 34s - loss: 40.9916 - MinusLogProbMetric: 40.9916 - val_loss: 41.7596 - val_MinusLogProbMetric: 41.7596 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 916/1000
2023-10-27 20:57:56.159 
Epoch 916/1000 
	 loss: 40.9462, MinusLogProbMetric: 40.9462, val_loss: 41.7283, val_MinusLogProbMetric: 41.7283

Epoch 916: val_loss did not improve from 41.65513
196/196 - 34s - loss: 40.9462 - MinusLogProbMetric: 40.9462 - val_loss: 41.7283 - val_MinusLogProbMetric: 41.7283 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 917/1000
2023-10-27 20:58:29.686 
Epoch 917/1000 
	 loss: 40.9431, MinusLogProbMetric: 40.9431, val_loss: 41.6943, val_MinusLogProbMetric: 41.6943

Epoch 917: val_loss did not improve from 41.65513
196/196 - 34s - loss: 40.9431 - MinusLogProbMetric: 40.9431 - val_loss: 41.6943 - val_MinusLogProbMetric: 41.6943 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 918/1000
2023-10-27 20:59:03.384 
Epoch 918/1000 
	 loss: 40.9556, MinusLogProbMetric: 40.9556, val_loss: 41.8457, val_MinusLogProbMetric: 41.8457

Epoch 918: val_loss did not improve from 41.65513
196/196 - 34s - loss: 40.9556 - MinusLogProbMetric: 40.9556 - val_loss: 41.8457 - val_MinusLogProbMetric: 41.8457 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 919/1000
2023-10-27 20:59:37.419 
Epoch 919/1000 
	 loss: 41.0899, MinusLogProbMetric: 41.0899, val_loss: 41.7407, val_MinusLogProbMetric: 41.7407

Epoch 919: val_loss did not improve from 41.65513
196/196 - 34s - loss: 41.0899 - MinusLogProbMetric: 41.0899 - val_loss: 41.7407 - val_MinusLogProbMetric: 41.7407 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 920/1000
2023-10-27 21:00:11.547 
Epoch 920/1000 
	 loss: 40.9583, MinusLogProbMetric: 40.9583, val_loss: 41.7537, val_MinusLogProbMetric: 41.7537

Epoch 920: val_loss did not improve from 41.65513
196/196 - 34s - loss: 40.9583 - MinusLogProbMetric: 40.9583 - val_loss: 41.7537 - val_MinusLogProbMetric: 41.7537 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 921/1000
2023-10-27 21:00:45.262 
Epoch 921/1000 
	 loss: 41.1018, MinusLogProbMetric: 41.1018, val_loss: 41.6399, val_MinusLogProbMetric: 41.6399

Epoch 921: val_loss improved from 41.65513 to 41.63995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 41.1018 - MinusLogProbMetric: 41.1018 - val_loss: 41.6399 - val_MinusLogProbMetric: 41.6399 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 922/1000
2023-10-27 21:01:19.363 
Epoch 922/1000 
	 loss: 40.9322, MinusLogProbMetric: 40.9322, val_loss: 41.7928, val_MinusLogProbMetric: 41.7928

Epoch 922: val_loss did not improve from 41.63995
196/196 - 33s - loss: 40.9322 - MinusLogProbMetric: 40.9322 - val_loss: 41.7928 - val_MinusLogProbMetric: 41.7928 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 923/1000
2023-10-27 21:01:53.346 
Epoch 923/1000 
	 loss: 40.9562, MinusLogProbMetric: 40.9562, val_loss: 41.6513, val_MinusLogProbMetric: 41.6513

Epoch 923: val_loss did not improve from 41.63995
196/196 - 34s - loss: 40.9562 - MinusLogProbMetric: 40.9562 - val_loss: 41.6513 - val_MinusLogProbMetric: 41.6513 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 924/1000
2023-10-27 21:02:27.203 
Epoch 924/1000 
	 loss: 40.9674, MinusLogProbMetric: 40.9674, val_loss: 41.6701, val_MinusLogProbMetric: 41.6701

Epoch 924: val_loss did not improve from 41.63995
196/196 - 34s - loss: 40.9674 - MinusLogProbMetric: 40.9674 - val_loss: 41.6701 - val_MinusLogProbMetric: 41.6701 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 925/1000
2023-10-27 21:03:01.046 
Epoch 925/1000 
	 loss: 41.0292, MinusLogProbMetric: 41.0292, val_loss: 42.8003, val_MinusLogProbMetric: 42.8003

Epoch 925: val_loss did not improve from 41.63995
196/196 - 34s - loss: 41.0292 - MinusLogProbMetric: 41.0292 - val_loss: 42.8003 - val_MinusLogProbMetric: 42.8003 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 926/1000
2023-10-27 21:03:34.935 
Epoch 926/1000 
	 loss: 41.0116, MinusLogProbMetric: 41.0116, val_loss: 41.7754, val_MinusLogProbMetric: 41.7754

Epoch 926: val_loss did not improve from 41.63995
196/196 - 34s - loss: 41.0116 - MinusLogProbMetric: 41.0116 - val_loss: 41.7754 - val_MinusLogProbMetric: 41.7754 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 927/1000
2023-10-27 21:04:07.942 
Epoch 927/1000 
	 loss: 40.9688, MinusLogProbMetric: 40.9688, val_loss: 41.6201, val_MinusLogProbMetric: 41.6201

Epoch 927: val_loss improved from 41.63995 to 41.62011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.9688 - MinusLogProbMetric: 40.9688 - val_loss: 41.6201 - val_MinusLogProbMetric: 41.6201 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 928/1000
2023-10-27 21:04:41.565 
Epoch 928/1000 
	 loss: 40.9268, MinusLogProbMetric: 40.9268, val_loss: 41.8266, val_MinusLogProbMetric: 41.8266

Epoch 928: val_loss did not improve from 41.62011
196/196 - 33s - loss: 40.9268 - MinusLogProbMetric: 40.9268 - val_loss: 41.8266 - val_MinusLogProbMetric: 41.8266 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 929/1000
2023-10-27 21:05:15.700 
Epoch 929/1000 
	 loss: 40.9244, MinusLogProbMetric: 40.9244, val_loss: 41.7201, val_MinusLogProbMetric: 41.7201

Epoch 929: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9244 - MinusLogProbMetric: 40.9244 - val_loss: 41.7201 - val_MinusLogProbMetric: 41.7201 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 930/1000
2023-10-27 21:05:49.937 
Epoch 930/1000 
	 loss: 40.9406, MinusLogProbMetric: 40.9406, val_loss: 41.6850, val_MinusLogProbMetric: 41.6850

Epoch 930: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9406 - MinusLogProbMetric: 40.9406 - val_loss: 41.6850 - val_MinusLogProbMetric: 41.6850 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 931/1000
2023-10-27 21:06:24.040 
Epoch 931/1000 
	 loss: 40.9510, MinusLogProbMetric: 40.9510, val_loss: 41.9292, val_MinusLogProbMetric: 41.9292

Epoch 931: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9510 - MinusLogProbMetric: 40.9510 - val_loss: 41.9292 - val_MinusLogProbMetric: 41.9292 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 932/1000
2023-10-27 21:06:58.164 
Epoch 932/1000 
	 loss: 40.9563, MinusLogProbMetric: 40.9563, val_loss: 42.2889, val_MinusLogProbMetric: 42.2889

Epoch 932: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9563 - MinusLogProbMetric: 40.9563 - val_loss: 42.2889 - val_MinusLogProbMetric: 42.2889 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 933/1000
2023-10-27 21:07:32.674 
Epoch 933/1000 
	 loss: 40.9228, MinusLogProbMetric: 40.9228, val_loss: 41.6785, val_MinusLogProbMetric: 41.6785

Epoch 933: val_loss did not improve from 41.62011
196/196 - 35s - loss: 40.9228 - MinusLogProbMetric: 40.9228 - val_loss: 41.6785 - val_MinusLogProbMetric: 41.6785 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 934/1000
2023-10-27 21:08:06.463 
Epoch 934/1000 
	 loss: 40.9343, MinusLogProbMetric: 40.9343, val_loss: 41.8157, val_MinusLogProbMetric: 41.8157

Epoch 934: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9343 - MinusLogProbMetric: 40.9343 - val_loss: 41.8157 - val_MinusLogProbMetric: 41.8157 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 935/1000
2023-10-27 21:08:40.149 
Epoch 935/1000 
	 loss: 40.9417, MinusLogProbMetric: 40.9417, val_loss: 41.9732, val_MinusLogProbMetric: 41.9732

Epoch 935: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9417 - MinusLogProbMetric: 40.9417 - val_loss: 41.9732 - val_MinusLogProbMetric: 41.9732 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 936/1000
2023-10-27 21:09:13.428 
Epoch 936/1000 
	 loss: 40.9709, MinusLogProbMetric: 40.9709, val_loss: 41.8933, val_MinusLogProbMetric: 41.8933

Epoch 936: val_loss did not improve from 41.62011
196/196 - 33s - loss: 40.9709 - MinusLogProbMetric: 40.9709 - val_loss: 41.8933 - val_MinusLogProbMetric: 41.8933 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 937/1000
2023-10-27 21:09:47.417 
Epoch 937/1000 
	 loss: 40.9643, MinusLogProbMetric: 40.9643, val_loss: 41.7202, val_MinusLogProbMetric: 41.7202

Epoch 937: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9643 - MinusLogProbMetric: 40.9643 - val_loss: 41.7202 - val_MinusLogProbMetric: 41.7202 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 938/1000
2023-10-27 21:10:21.294 
Epoch 938/1000 
	 loss: 40.9341, MinusLogProbMetric: 40.9341, val_loss: 41.7796, val_MinusLogProbMetric: 41.7796

Epoch 938: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9341 - MinusLogProbMetric: 40.9341 - val_loss: 41.7796 - val_MinusLogProbMetric: 41.7796 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 939/1000
2023-10-27 21:10:55.243 
Epoch 939/1000 
	 loss: 40.9661, MinusLogProbMetric: 40.9661, val_loss: 41.7276, val_MinusLogProbMetric: 41.7276

Epoch 939: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9661 - MinusLogProbMetric: 40.9661 - val_loss: 41.7276 - val_MinusLogProbMetric: 41.7276 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 940/1000
2023-10-27 21:11:28.537 
Epoch 940/1000 
	 loss: 41.1615, MinusLogProbMetric: 41.1615, val_loss: 41.7347, val_MinusLogProbMetric: 41.7347

Epoch 940: val_loss did not improve from 41.62011
196/196 - 33s - loss: 41.1615 - MinusLogProbMetric: 41.1615 - val_loss: 41.7347 - val_MinusLogProbMetric: 41.7347 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 941/1000
2023-10-27 21:12:02.048 
Epoch 941/1000 
	 loss: 40.9408, MinusLogProbMetric: 40.9408, val_loss: 41.7762, val_MinusLogProbMetric: 41.7762

Epoch 941: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9408 - MinusLogProbMetric: 40.9408 - val_loss: 41.7762 - val_MinusLogProbMetric: 41.7762 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 942/1000
2023-10-27 21:12:34.403 
Epoch 942/1000 
	 loss: 41.1676, MinusLogProbMetric: 41.1676, val_loss: 41.6974, val_MinusLogProbMetric: 41.6974

Epoch 942: val_loss did not improve from 41.62011
196/196 - 32s - loss: 41.1676 - MinusLogProbMetric: 41.1676 - val_loss: 41.6974 - val_MinusLogProbMetric: 41.6974 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 943/1000
2023-10-27 21:13:08.403 
Epoch 943/1000 
	 loss: 40.9021, MinusLogProbMetric: 40.9021, val_loss: 41.8260, val_MinusLogProbMetric: 41.8260

Epoch 943: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9021 - MinusLogProbMetric: 40.9021 - val_loss: 41.8260 - val_MinusLogProbMetric: 41.8260 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 944/1000
2023-10-27 21:13:41.733 
Epoch 944/1000 
	 loss: 40.9279, MinusLogProbMetric: 40.9279, val_loss: 41.8685, val_MinusLogProbMetric: 41.8685

Epoch 944: val_loss did not improve from 41.62011
196/196 - 33s - loss: 40.9279 - MinusLogProbMetric: 40.9279 - val_loss: 41.8685 - val_MinusLogProbMetric: 41.8685 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 945/1000
2023-10-27 21:14:15.521 
Epoch 945/1000 
	 loss: 40.9327, MinusLogProbMetric: 40.9327, val_loss: 41.9542, val_MinusLogProbMetric: 41.9542

Epoch 945: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9327 - MinusLogProbMetric: 40.9327 - val_loss: 41.9542 - val_MinusLogProbMetric: 41.9542 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 946/1000
2023-10-27 21:14:49.415 
Epoch 946/1000 
	 loss: 40.9554, MinusLogProbMetric: 40.9554, val_loss: 41.8303, val_MinusLogProbMetric: 41.8303

Epoch 946: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9554 - MinusLogProbMetric: 40.9554 - val_loss: 41.8303 - val_MinusLogProbMetric: 41.8303 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 947/1000
2023-10-27 21:15:23.173 
Epoch 947/1000 
	 loss: 41.0055, MinusLogProbMetric: 41.0055, val_loss: 43.3926, val_MinusLogProbMetric: 43.3926

Epoch 947: val_loss did not improve from 41.62011
196/196 - 34s - loss: 41.0055 - MinusLogProbMetric: 41.0055 - val_loss: 43.3926 - val_MinusLogProbMetric: 43.3926 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 948/1000
2023-10-27 21:15:57.042 
Epoch 948/1000 
	 loss: 41.1937, MinusLogProbMetric: 41.1937, val_loss: 41.6706, val_MinusLogProbMetric: 41.6706

Epoch 948: val_loss did not improve from 41.62011
196/196 - 34s - loss: 41.1937 - MinusLogProbMetric: 41.1937 - val_loss: 41.6706 - val_MinusLogProbMetric: 41.6706 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 949/1000
2023-10-27 21:16:30.784 
Epoch 949/1000 
	 loss: 40.8996, MinusLogProbMetric: 40.8996, val_loss: 41.7883, val_MinusLogProbMetric: 41.7883

Epoch 949: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.8996 - MinusLogProbMetric: 40.8996 - val_loss: 41.7883 - val_MinusLogProbMetric: 41.7883 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 950/1000
2023-10-27 21:17:04.563 
Epoch 950/1000 
	 loss: 40.9223, MinusLogProbMetric: 40.9223, val_loss: 41.7040, val_MinusLogProbMetric: 41.7040

Epoch 950: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9223 - MinusLogProbMetric: 40.9223 - val_loss: 41.7040 - val_MinusLogProbMetric: 41.7040 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 951/1000
2023-10-27 21:17:37.859 
Epoch 951/1000 
	 loss: 40.9281, MinusLogProbMetric: 40.9281, val_loss: 41.8182, val_MinusLogProbMetric: 41.8182

Epoch 951: val_loss did not improve from 41.62011
196/196 - 33s - loss: 40.9281 - MinusLogProbMetric: 40.9281 - val_loss: 41.8182 - val_MinusLogProbMetric: 41.8182 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 952/1000
2023-10-27 21:18:11.099 
Epoch 952/1000 
	 loss: 40.9348, MinusLogProbMetric: 40.9348, val_loss: 41.7228, val_MinusLogProbMetric: 41.7228

Epoch 952: val_loss did not improve from 41.62011
196/196 - 33s - loss: 40.9348 - MinusLogProbMetric: 40.9348 - val_loss: 41.7228 - val_MinusLogProbMetric: 41.7228 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 953/1000
2023-10-27 21:18:45.050 
Epoch 953/1000 
	 loss: 40.9398, MinusLogProbMetric: 40.9398, val_loss: 41.7122, val_MinusLogProbMetric: 41.7122

Epoch 953: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9398 - MinusLogProbMetric: 40.9398 - val_loss: 41.7122 - val_MinusLogProbMetric: 41.7122 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 954/1000
2023-10-27 21:19:18.740 
Epoch 954/1000 
	 loss: 40.9560, MinusLogProbMetric: 40.9560, val_loss: 41.6455, val_MinusLogProbMetric: 41.6455

Epoch 954: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9560 - MinusLogProbMetric: 40.9560 - val_loss: 41.6455 - val_MinusLogProbMetric: 41.6455 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 955/1000
2023-10-27 21:19:52.455 
Epoch 955/1000 
	 loss: 40.9765, MinusLogProbMetric: 40.9765, val_loss: 41.6466, val_MinusLogProbMetric: 41.6466

Epoch 955: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9765 - MinusLogProbMetric: 40.9765 - val_loss: 41.6466 - val_MinusLogProbMetric: 41.6466 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 956/1000
2023-10-27 21:20:26.229 
Epoch 956/1000 
	 loss: 40.9169, MinusLogProbMetric: 40.9169, val_loss: 41.7885, val_MinusLogProbMetric: 41.7885

Epoch 956: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9169 - MinusLogProbMetric: 40.9169 - val_loss: 41.7885 - val_MinusLogProbMetric: 41.7885 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 957/1000
2023-10-27 21:20:58.472 
Epoch 957/1000 
	 loss: 41.3018, MinusLogProbMetric: 41.3018, val_loss: 41.9430, val_MinusLogProbMetric: 41.9430

Epoch 957: val_loss did not improve from 41.62011
196/196 - 32s - loss: 41.3018 - MinusLogProbMetric: 41.3018 - val_loss: 41.9430 - val_MinusLogProbMetric: 41.9430 - lr: 8.3333e-05 - 32s/epoch - 164ms/step
Epoch 958/1000
2023-10-27 21:21:31.989 
Epoch 958/1000 
	 loss: 40.9035, MinusLogProbMetric: 40.9035, val_loss: 41.7476, val_MinusLogProbMetric: 41.7476

Epoch 958: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9035 - MinusLogProbMetric: 40.9035 - val_loss: 41.7476 - val_MinusLogProbMetric: 41.7476 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 959/1000
2023-10-27 21:22:05.786 
Epoch 959/1000 
	 loss: 40.9049, MinusLogProbMetric: 40.9049, val_loss: 41.7219, val_MinusLogProbMetric: 41.7219

Epoch 959: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9049 - MinusLogProbMetric: 40.9049 - val_loss: 41.7219 - val_MinusLogProbMetric: 41.7219 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 960/1000
2023-10-27 21:22:39.736 
Epoch 960/1000 
	 loss: 40.9157, MinusLogProbMetric: 40.9157, val_loss: 41.6479, val_MinusLogProbMetric: 41.6479

Epoch 960: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9157 - MinusLogProbMetric: 40.9157 - val_loss: 41.6479 - val_MinusLogProbMetric: 41.6479 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 961/1000
2023-10-27 21:23:13.658 
Epoch 961/1000 
	 loss: 40.9388, MinusLogProbMetric: 40.9388, val_loss: 41.8005, val_MinusLogProbMetric: 41.8005

Epoch 961: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9388 - MinusLogProbMetric: 40.9388 - val_loss: 41.8005 - val_MinusLogProbMetric: 41.8005 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 962/1000
2023-10-27 21:23:47.207 
Epoch 962/1000 
	 loss: 41.0262, MinusLogProbMetric: 41.0262, val_loss: 42.3355, val_MinusLogProbMetric: 42.3355

Epoch 962: val_loss did not improve from 41.62011
196/196 - 34s - loss: 41.0262 - MinusLogProbMetric: 41.0262 - val_loss: 42.3355 - val_MinusLogProbMetric: 42.3355 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 963/1000
2023-10-27 21:24:21.151 
Epoch 963/1000 
	 loss: 41.0486, MinusLogProbMetric: 41.0486, val_loss: 41.7268, val_MinusLogProbMetric: 41.7268

Epoch 963: val_loss did not improve from 41.62011
196/196 - 34s - loss: 41.0486 - MinusLogProbMetric: 41.0486 - val_loss: 41.7268 - val_MinusLogProbMetric: 41.7268 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 964/1000
2023-10-27 21:24:54.986 
Epoch 964/1000 
	 loss: 40.9035, MinusLogProbMetric: 40.9035, val_loss: 41.7222, val_MinusLogProbMetric: 41.7222

Epoch 964: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9035 - MinusLogProbMetric: 40.9035 - val_loss: 41.7222 - val_MinusLogProbMetric: 41.7222 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 965/1000
2023-10-27 21:25:29.119 
Epoch 965/1000 
	 loss: 40.9240, MinusLogProbMetric: 40.9240, val_loss: 41.6769, val_MinusLogProbMetric: 41.6769

Epoch 965: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9240 - MinusLogProbMetric: 40.9240 - val_loss: 41.6769 - val_MinusLogProbMetric: 41.6769 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 966/1000
2023-10-27 21:26:02.805 
Epoch 966/1000 
	 loss: 41.3416, MinusLogProbMetric: 41.3416, val_loss: 43.5311, val_MinusLogProbMetric: 43.5311

Epoch 966: val_loss did not improve from 41.62011
196/196 - 34s - loss: 41.3416 - MinusLogProbMetric: 41.3416 - val_loss: 43.5311 - val_MinusLogProbMetric: 43.5311 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 967/1000
2023-10-27 21:26:36.837 
Epoch 967/1000 
	 loss: 41.0764, MinusLogProbMetric: 41.0764, val_loss: 41.7444, val_MinusLogProbMetric: 41.7444

Epoch 967: val_loss did not improve from 41.62011
196/196 - 34s - loss: 41.0764 - MinusLogProbMetric: 41.0764 - val_loss: 41.7444 - val_MinusLogProbMetric: 41.7444 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 968/1000
2023-10-27 21:27:10.789 
Epoch 968/1000 
	 loss: 40.9022, MinusLogProbMetric: 40.9022, val_loss: 41.6902, val_MinusLogProbMetric: 41.6902

Epoch 968: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9022 - MinusLogProbMetric: 40.9022 - val_loss: 41.6902 - val_MinusLogProbMetric: 41.6902 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 969/1000
2023-10-27 21:27:44.720 
Epoch 969/1000 
	 loss: 40.8893, MinusLogProbMetric: 40.8893, val_loss: 41.6370, val_MinusLogProbMetric: 41.6370

Epoch 969: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.8893 - MinusLogProbMetric: 40.8893 - val_loss: 41.6370 - val_MinusLogProbMetric: 41.6370 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 970/1000
2023-10-27 21:28:18.468 
Epoch 970/1000 
	 loss: 40.9225, MinusLogProbMetric: 40.9225, val_loss: 41.8521, val_MinusLogProbMetric: 41.8521

Epoch 970: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9225 - MinusLogProbMetric: 40.9225 - val_loss: 41.8521 - val_MinusLogProbMetric: 41.8521 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 971/1000
2023-10-27 21:28:52.417 
Epoch 971/1000 
	 loss: 40.9456, MinusLogProbMetric: 40.9456, val_loss: 41.8331, val_MinusLogProbMetric: 41.8331

Epoch 971: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9456 - MinusLogProbMetric: 40.9456 - val_loss: 41.8331 - val_MinusLogProbMetric: 41.8331 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 972/1000
2023-10-27 21:29:26.002 
Epoch 972/1000 
	 loss: 41.1282, MinusLogProbMetric: 41.1282, val_loss: 41.7396, val_MinusLogProbMetric: 41.7396

Epoch 972: val_loss did not improve from 41.62011
196/196 - 34s - loss: 41.1282 - MinusLogProbMetric: 41.1282 - val_loss: 41.7396 - val_MinusLogProbMetric: 41.7396 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 973/1000
2023-10-27 21:30:00.116 
Epoch 973/1000 
	 loss: 40.9113, MinusLogProbMetric: 40.9113, val_loss: 41.8030, val_MinusLogProbMetric: 41.8030

Epoch 973: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9113 - MinusLogProbMetric: 40.9113 - val_loss: 41.8030 - val_MinusLogProbMetric: 41.8030 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 974/1000
2023-10-27 21:30:33.728 
Epoch 974/1000 
	 loss: 40.9117, MinusLogProbMetric: 40.9117, val_loss: 41.7271, val_MinusLogProbMetric: 41.7271

Epoch 974: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9117 - MinusLogProbMetric: 40.9117 - val_loss: 41.7271 - val_MinusLogProbMetric: 41.7271 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 975/1000
2023-10-27 21:31:07.654 
Epoch 975/1000 
	 loss: 40.9256, MinusLogProbMetric: 40.9256, val_loss: 42.1194, val_MinusLogProbMetric: 42.1194

Epoch 975: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9256 - MinusLogProbMetric: 40.9256 - val_loss: 42.1194 - val_MinusLogProbMetric: 42.1194 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 976/1000
2023-10-27 21:31:41.512 
Epoch 976/1000 
	 loss: 40.9155, MinusLogProbMetric: 40.9155, val_loss: 41.6808, val_MinusLogProbMetric: 41.6808

Epoch 976: val_loss did not improve from 41.62011
196/196 - 34s - loss: 40.9155 - MinusLogProbMetric: 40.9155 - val_loss: 41.6808 - val_MinusLogProbMetric: 41.6808 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 977/1000
2023-10-27 21:32:15.410 
Epoch 977/1000 
	 loss: 41.0859, MinusLogProbMetric: 41.0859, val_loss: 41.7959, val_MinusLogProbMetric: 41.7959

Epoch 977: val_loss did not improve from 41.62011
196/196 - 34s - loss: 41.0859 - MinusLogProbMetric: 41.0859 - val_loss: 41.7959 - val_MinusLogProbMetric: 41.7959 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 978/1000
2023-10-27 21:32:47.706 
Epoch 978/1000 
	 loss: 40.7591, MinusLogProbMetric: 40.7591, val_loss: 41.6208, val_MinusLogProbMetric: 41.6208

Epoch 978: val_loss did not improve from 41.62011
196/196 - 32s - loss: 40.7591 - MinusLogProbMetric: 40.7591 - val_loss: 41.6208 - val_MinusLogProbMetric: 41.6208 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 979/1000
2023-10-27 21:33:21.232 
Epoch 979/1000 
	 loss: 40.7513, MinusLogProbMetric: 40.7513, val_loss: 41.6178, val_MinusLogProbMetric: 41.6178

Epoch 979: val_loss improved from 41.62011 to 41.61780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.7513 - MinusLogProbMetric: 40.7513 - val_loss: 41.6178 - val_MinusLogProbMetric: 41.6178 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 980/1000
2023-10-27 21:33:55.331 
Epoch 980/1000 
	 loss: 40.7495, MinusLogProbMetric: 40.7495, val_loss: 41.5606, val_MinusLogProbMetric: 41.5606

Epoch 980: val_loss improved from 41.61780 to 41.56061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.7495 - MinusLogProbMetric: 40.7495 - val_loss: 41.5606 - val_MinusLogProbMetric: 41.5606 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 981/1000
2023-10-27 21:34:29.353 
Epoch 981/1000 
	 loss: 40.7569, MinusLogProbMetric: 40.7569, val_loss: 41.6317, val_MinusLogProbMetric: 41.6317

Epoch 981: val_loss did not improve from 41.56061
196/196 - 33s - loss: 40.7569 - MinusLogProbMetric: 40.7569 - val_loss: 41.6317 - val_MinusLogProbMetric: 41.6317 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 982/1000
2023-10-27 21:35:02.784 
Epoch 982/1000 
	 loss: 40.7467, MinusLogProbMetric: 40.7467, val_loss: 41.5736, val_MinusLogProbMetric: 41.5736

Epoch 982: val_loss did not improve from 41.56061
196/196 - 33s - loss: 40.7467 - MinusLogProbMetric: 40.7467 - val_loss: 41.5736 - val_MinusLogProbMetric: 41.5736 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 983/1000
2023-10-27 21:35:35.938 
Epoch 983/1000 
	 loss: 40.7491, MinusLogProbMetric: 40.7491, val_loss: 41.5531, val_MinusLogProbMetric: 41.5531

Epoch 983: val_loss improved from 41.56061 to 41.55311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.7491 - MinusLogProbMetric: 40.7491 - val_loss: 41.5531 - val_MinusLogProbMetric: 41.5531 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 984/1000
2023-10-27 21:36:09.491 
Epoch 984/1000 
	 loss: 40.7546, MinusLogProbMetric: 40.7546, val_loss: 41.6328, val_MinusLogProbMetric: 41.6328

Epoch 984: val_loss did not improve from 41.55311
196/196 - 33s - loss: 40.7546 - MinusLogProbMetric: 40.7546 - val_loss: 41.6328 - val_MinusLogProbMetric: 41.6328 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 985/1000
2023-10-27 21:36:43.490 
Epoch 985/1000 
	 loss: 40.7502, MinusLogProbMetric: 40.7502, val_loss: 41.6500, val_MinusLogProbMetric: 41.6500

Epoch 985: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7502 - MinusLogProbMetric: 40.7502 - val_loss: 41.6500 - val_MinusLogProbMetric: 41.6500 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 986/1000
2023-10-27 21:37:16.296 
Epoch 986/1000 
	 loss: 40.7600, MinusLogProbMetric: 40.7600, val_loss: 41.6099, val_MinusLogProbMetric: 41.6099

Epoch 986: val_loss did not improve from 41.55311
196/196 - 33s - loss: 40.7600 - MinusLogProbMetric: 40.7600 - val_loss: 41.6099 - val_MinusLogProbMetric: 41.6099 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 987/1000
2023-10-27 21:37:50.144 
Epoch 987/1000 
	 loss: 40.7457, MinusLogProbMetric: 40.7457, val_loss: 41.6190, val_MinusLogProbMetric: 41.6190

Epoch 987: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7457 - MinusLogProbMetric: 40.7457 - val_loss: 41.6190 - val_MinusLogProbMetric: 41.6190 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 988/1000
2023-10-27 21:38:24.190 
Epoch 988/1000 
	 loss: 40.7535, MinusLogProbMetric: 40.7535, val_loss: 41.5602, val_MinusLogProbMetric: 41.5602

Epoch 988: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7535 - MinusLogProbMetric: 40.7535 - val_loss: 41.5602 - val_MinusLogProbMetric: 41.5602 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 989/1000
2023-10-27 21:38:57.452 
Epoch 989/1000 
	 loss: 40.7371, MinusLogProbMetric: 40.7371, val_loss: 41.6613, val_MinusLogProbMetric: 41.6613

Epoch 989: val_loss did not improve from 41.55311
196/196 - 33s - loss: 40.7371 - MinusLogProbMetric: 40.7371 - val_loss: 41.6613 - val_MinusLogProbMetric: 41.6613 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 990/1000
2023-10-27 21:39:31.289 
Epoch 990/1000 
	 loss: 40.7456, MinusLogProbMetric: 40.7456, val_loss: 41.5943, val_MinusLogProbMetric: 41.5943

Epoch 990: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7456 - MinusLogProbMetric: 40.7456 - val_loss: 41.5943 - val_MinusLogProbMetric: 41.5943 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 991/1000
2023-10-27 21:40:04.857 
Epoch 991/1000 
	 loss: 40.7549, MinusLogProbMetric: 40.7549, val_loss: 41.5555, val_MinusLogProbMetric: 41.5555

Epoch 991: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7549 - MinusLogProbMetric: 40.7549 - val_loss: 41.5555 - val_MinusLogProbMetric: 41.5555 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 992/1000
2023-10-27 21:40:38.702 
Epoch 992/1000 
	 loss: 40.7482, MinusLogProbMetric: 40.7482, val_loss: 41.5678, val_MinusLogProbMetric: 41.5678

Epoch 992: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7482 - MinusLogProbMetric: 40.7482 - val_loss: 41.5678 - val_MinusLogProbMetric: 41.5678 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 993/1000
2023-10-27 21:41:12.477 
Epoch 993/1000 
	 loss: 40.7499, MinusLogProbMetric: 40.7499, val_loss: 41.6365, val_MinusLogProbMetric: 41.6365

Epoch 993: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7499 - MinusLogProbMetric: 40.7499 - val_loss: 41.6365 - val_MinusLogProbMetric: 41.6365 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 994/1000
2023-10-27 21:41:46.565 
Epoch 994/1000 
	 loss: 40.7351, MinusLogProbMetric: 40.7351, val_loss: 41.5728, val_MinusLogProbMetric: 41.5728

Epoch 994: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7351 - MinusLogProbMetric: 40.7351 - val_loss: 41.5728 - val_MinusLogProbMetric: 41.5728 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 995/1000
2023-10-27 21:42:20.336 
Epoch 995/1000 
	 loss: 40.7645, MinusLogProbMetric: 40.7645, val_loss: 41.5598, val_MinusLogProbMetric: 41.5598

Epoch 995: val_loss did not improve from 41.55311
196/196 - 34s - loss: 40.7645 - MinusLogProbMetric: 40.7645 - val_loss: 41.5598 - val_MinusLogProbMetric: 41.5598 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 996/1000
2023-10-27 21:42:54.058 
Epoch 996/1000 
	 loss: 40.7318, MinusLogProbMetric: 40.7318, val_loss: 41.5401, val_MinusLogProbMetric: 41.5401

Epoch 996: val_loss improved from 41.55311 to 41.54008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.7318 - MinusLogProbMetric: 40.7318 - val_loss: 41.5401 - val_MinusLogProbMetric: 41.5401 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 997/1000
2023-10-27 21:43:28.685 
Epoch 997/1000 
	 loss: 40.7366, MinusLogProbMetric: 40.7366, val_loss: 41.5392, val_MinusLogProbMetric: 41.5392

Epoch 997: val_loss improved from 41.54008 to 41.53923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 35s - loss: 40.7366 - MinusLogProbMetric: 40.7366 - val_loss: 41.5392 - val_MinusLogProbMetric: 41.5392 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 998/1000
2023-10-27 21:44:02.797 
Epoch 998/1000 
	 loss: 40.7399, MinusLogProbMetric: 40.7399, val_loss: 41.5878, val_MinusLogProbMetric: 41.5878

Epoch 998: val_loss did not improve from 41.53923
196/196 - 34s - loss: 40.7399 - MinusLogProbMetric: 40.7399 - val_loss: 41.5878 - val_MinusLogProbMetric: 41.5878 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 999/1000
2023-10-27 21:44:36.427 
Epoch 999/1000 
	 loss: 40.7382, MinusLogProbMetric: 40.7382, val_loss: 41.5294, val_MinusLogProbMetric: 41.5294

Epoch 999: val_loss improved from 41.53923 to 41.52941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_410/weights/best_weights.h5
196/196 - 34s - loss: 40.7382 - MinusLogProbMetric: 40.7382 - val_loss: 41.5294 - val_MinusLogProbMetric: 41.5294 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 1000/1000
2023-10-27 21:45:11.040 
Epoch 1000/1000 
	 loss: 40.7407, MinusLogProbMetric: 40.7407, val_loss: 41.5772, val_MinusLogProbMetric: 41.5772

Epoch 1000: val_loss did not improve from 41.52941
196/196 - 34s - loss: 40.7407 - MinusLogProbMetric: 40.7407 - val_loss: 41.5772 - val_MinusLogProbMetric: 41.5772 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 187.
Model trained in 32196.94 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.82 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.12 s.
===========
Run 410/720 done in 32275.35 s.
===========

Directory ../../results/CsplineN_new/run_411/ already exists.
Skipping it.
===========
Run 411/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_412/ already exists.
Skipping it.
===========
Run 412/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_413/ already exists.
Skipping it.
===========
Run 413/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_414/ already exists.
Skipping it.
===========
Run 414/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_415/ already exists.
Skipping it.
===========
Run 415/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_416/ already exists.
Skipping it.
===========
Run 416/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_417/ already exists.
Skipping it.
===========
Run 417/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_418/ already exists.
Skipping it.
===========
Run 418/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_419/ already exists.
Skipping it.
===========
Run 419/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_420/ already exists.
Skipping it.
===========
Run 420/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_421/ already exists.
Skipping it.
===========
Run 421/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_422/ already exists.
Skipping it.
===========
Run 422/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_423/ already exists.
Skipping it.
===========
Run 423/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_424/ already exists.
Skipping it.
===========
Run 424/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_425/ already exists.
Skipping it.
===========
Run 425/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_426/ already exists.
Skipping it.
===========
Run 426/720 already exists. Skipping it.
===========

===========
Generating train data for run 427.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_386"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_387 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f351490dfc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f350dc97bb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f350dc97bb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3514ef8640>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f350db81e10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f350db82380>, <keras.callbacks.ModelCheckpoint object at 0x7f350db82440>, <keras.callbacks.EarlyStopping object at 0x7f350db826b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f350db826e0>, <keras.callbacks.TerminateOnNaN object at 0x7f350db82320>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:45:17.561852
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:46:40.253 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 83s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 83s/epoch - 421ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 427.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_392"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_393 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f374aa4e4a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3604dde7a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3604dde7a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36e96640d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36eab70520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36eab70a90>, <keras.callbacks.ModelCheckpoint object at 0x7f36eab70b50>, <keras.callbacks.EarlyStopping object at 0x7f36eab70dc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36eab70df0>, <keras.callbacks.TerminateOnNaN object at 0x7f36eab70a30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:46:46.165867
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:47:59.709 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 73s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 73s/epoch - 374ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 427.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_398"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_399 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f3de889f490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35d6803cd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35d6803cd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f34bd3bb2e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f357c8b6470>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f357c8b69e0>, <keras.callbacks.ModelCheckpoint object at 0x7f357c8b6aa0>, <keras.callbacks.EarlyStopping object at 0x7f357c8b6d10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f357c8b6d40>, <keras.callbacks.TerminateOnNaN object at 0x7f357c8b6980>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:48:06.212101
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:49:20.197 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 74s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 74s/epoch - 377ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 427.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_404"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_405 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f36b9c790c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3607834520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3607834520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f354c5c3010>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f354c5c11e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f354c5c2530>, <keras.callbacks.ModelCheckpoint object at 0x7f354c5c2e90>, <keras.callbacks.EarlyStopping object at 0x7f354c5c2b60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f354c5c34f0>, <keras.callbacks.TerminateOnNaN object at 0x7f354c5c3400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:49:31.344190
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:50:47.750 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 76s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 76s/epoch - 389ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 427.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_410"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_411 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f36e987bc40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3607874910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3607874910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35d70542e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d9bb5add0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d9bb5b340>, <keras.callbacks.ModelCheckpoint object at 0x7f3d9bb5b400>, <keras.callbacks.EarlyStopping object at 0x7f3d9bb5b670>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d9bb5b6a0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d9bb5b2e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:50:54.031412
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:52:05.950 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 72s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 72s/epoch - 366ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 427.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_416"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_417 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f3635480970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3635430160>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3635430160>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f356cfb2bc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35948dc220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35948dc790>, <keras.callbacks.ModelCheckpoint object at 0x7f35948dc850>, <keras.callbacks.EarlyStopping object at 0x7f35948dcac0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35948dcaf0>, <keras.callbacks.TerminateOnNaN object at 0x7f35948dc730>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:52:11.942258
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:53:32.538 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 80s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 80s/epoch - 411ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 427.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_422"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_423 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f36eabf9ed0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f38ec62a680>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f38ec62a680>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f399467dff0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36078754e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35e3dab0d0>, <keras.callbacks.ModelCheckpoint object at 0x7f35e3daaad0>, <keras.callbacks.EarlyStopping object at 0x7f35e3da9ba0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35e3da9ea0>, <keras.callbacks.TerminateOnNaN object at 0x7f35e3da9f00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:53:38.621972
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:54:51.188 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 72s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 72s/epoch - 369ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 427.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_428"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_429 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f35d4efb8e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3742864b80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3742864b80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3de8b06aa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36281f57b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36281f5d20>, <keras.callbacks.ModelCheckpoint object at 0x7f36281f5de0>, <keras.callbacks.EarlyStopping object at 0x7f36281f6050>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36281f6080>, <keras.callbacks.TerminateOnNaN object at 0x7f36281f5cc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:54:57.311878
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:56:10.026 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 73s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 73s/epoch - 370ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 427.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_434"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_435 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f34cd653640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f355cd31690>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f355cd31690>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35d532d120>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3505bc6ce0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3505bc7250>, <keras.callbacks.ModelCheckpoint object at 0x7f3505bc7310>, <keras.callbacks.EarlyStopping object at 0x7f3505bc7580>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3505bc75b0>, <keras.callbacks.TerminateOnNaN object at 0x7f3505bc71f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:56:15.952669
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:57:36.156 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 80s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 80s/epoch - 408ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 427.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_440"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_441 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f3d8aa8f6a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3589f33cd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3589f33cd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f39945acfd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3660d46aa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3660d47010>, <keras.callbacks.ModelCheckpoint object at 0x7f3660d470d0>, <keras.callbacks.EarlyStopping object at 0x7f3660d47340>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3660d47370>, <keras.callbacks.TerminateOnNaN object at 0x7f3660d46fb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:57:41.722260
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 21:58:54.380 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 72s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 72s/epoch - 370ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 427.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_427/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_427
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_446"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_447 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f350c15a230>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35d4911b40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35d4911b40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f356caece20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35954a7dc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_427/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35954a45e0>, <keras.callbacks.ModelCheckpoint object at 0x7f35954a6fe0>, <keras.callbacks.EarlyStopping object at 0x7f35954a7880>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35954a79a0>, <keras.callbacks.TerminateOnNaN object at 0x7f35954a48e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_427/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 427/720 with hyperparameters:
timestamp = 2023-10-27 21:59:00.020018
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:00:22.314 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11236.6523, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 82s - loss: nan - MinusLogProbMetric: 11236.6523 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 82s/epoch - 419ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 427/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 428.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_428
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_452"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_453 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f36cb348b80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34c4249a20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34c4249a20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3588ec7e50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f39545f0ca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f39545f1f60>, <keras.callbacks.ModelCheckpoint object at 0x7f39545f1870>, <keras.callbacks.EarlyStopping object at 0x7f39545f0eb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f39545f3eb0>, <keras.callbacks.TerminateOnNaN object at 0x7f39545f1930>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_428/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 428/720 with hyperparameters:
timestamp = 2023-10-27 22:00:28.676260
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:01:40.997 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9038.4316, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 72s - loss: nan - MinusLogProbMetric: 9038.4316 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 72s/epoch - 368ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 428.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_428
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_458"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_459 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f357d795300>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3df137a230>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3df137a230>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3df13a3af0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3595097160>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35950976d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3595097790>, <keras.callbacks.EarlyStopping object at 0x7f3595097a00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3595097a30>, <keras.callbacks.TerminateOnNaN object at 0x7f3595097670>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_428/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 428/720 with hyperparameters:
timestamp = 2023-10-27 22:01:46.436818
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 10: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 22:03:02.233 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 8436.7412, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 76s - loss: nan - MinusLogProbMetric: 8436.7412 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 76s/epoch - 386ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 428.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_428
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_464"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_465 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f36b81565c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36701f2f80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36701f2f80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f366011fbb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f373856d330>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f373856d8a0>, <keras.callbacks.ModelCheckpoint object at 0x7f373856d960>, <keras.callbacks.EarlyStopping object at 0x7f373856dbd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f373856dc00>, <keras.callbacks.TerminateOnNaN object at 0x7f373856d840>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_428/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 428/720 with hyperparameters:
timestamp = 2023-10-27 22:03:07.947781
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
2023-10-27 22:05:15.263 
Epoch 1/1000 
	 loss: 2915.2346, MinusLogProbMetric: 2915.2346, val_loss: 758.8270, val_MinusLogProbMetric: 758.8270

Epoch 1: val_loss improved from inf to 758.82703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 128s - loss: 2915.2346 - MinusLogProbMetric: 2915.2346 - val_loss: 758.8270 - val_MinusLogProbMetric: 758.8270 - lr: 1.1111e-04 - 128s/epoch - 652ms/step
Epoch 2/1000
2023-10-27 22:05:58.228 
Epoch 2/1000 
	 loss: 664.8011, MinusLogProbMetric: 664.8011, val_loss: 484.0505, val_MinusLogProbMetric: 484.0505

Epoch 2: val_loss improved from 758.82703 to 484.05054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 664.8011 - MinusLogProbMetric: 664.8011 - val_loss: 484.0505 - val_MinusLogProbMetric: 484.0505 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 3/1000
2023-10-27 22:06:40.289 
Epoch 3/1000 
	 loss: 453.6269, MinusLogProbMetric: 453.6269, val_loss: 425.1223, val_MinusLogProbMetric: 425.1223

Epoch 3: val_loss improved from 484.05054 to 425.12231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 453.6269 - MinusLogProbMetric: 453.6269 - val_loss: 425.1223 - val_MinusLogProbMetric: 425.1223 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 4/1000
2023-10-27 22:07:22.673 
Epoch 4/1000 
	 loss: 386.2017, MinusLogProbMetric: 386.2017, val_loss: 366.4294, val_MinusLogProbMetric: 366.4294

Epoch 4: val_loss improved from 425.12231 to 366.42944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 386.2017 - MinusLogProbMetric: 386.2017 - val_loss: 366.4294 - val_MinusLogProbMetric: 366.4294 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 5/1000
2023-10-27 22:08:06.396 
Epoch 5/1000 
	 loss: 348.7591, MinusLogProbMetric: 348.7591, val_loss: 350.3706, val_MinusLogProbMetric: 350.3706

Epoch 5: val_loss improved from 366.42944 to 350.37061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 348.7591 - MinusLogProbMetric: 348.7591 - val_loss: 350.3706 - val_MinusLogProbMetric: 350.3706 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 6/1000
2023-10-27 22:08:49.018 
Epoch 6/1000 
	 loss: 316.6537, MinusLogProbMetric: 316.6537, val_loss: 300.6906, val_MinusLogProbMetric: 300.6906

Epoch 6: val_loss improved from 350.37061 to 300.69064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 316.6537 - MinusLogProbMetric: 316.6537 - val_loss: 300.6906 - val_MinusLogProbMetric: 300.6906 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 7/1000
2023-10-27 22:09:32.529 
Epoch 7/1000 
	 loss: 292.1689, MinusLogProbMetric: 292.1689, val_loss: 278.0879, val_MinusLogProbMetric: 278.0879

Epoch 7: val_loss improved from 300.69064 to 278.08792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 292.1689 - MinusLogProbMetric: 292.1689 - val_loss: 278.0879 - val_MinusLogProbMetric: 278.0879 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 8/1000
2023-10-27 22:10:14.729 
Epoch 8/1000 
	 loss: 271.8231, MinusLogProbMetric: 271.8231, val_loss: 258.4101, val_MinusLogProbMetric: 258.4101

Epoch 8: val_loss improved from 278.08792 to 258.41013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 271.8231 - MinusLogProbMetric: 271.8231 - val_loss: 258.4101 - val_MinusLogProbMetric: 258.4101 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 9/1000
2023-10-27 22:10:57.646 
Epoch 9/1000 
	 loss: 253.3383, MinusLogProbMetric: 253.3383, val_loss: 243.3564, val_MinusLogProbMetric: 243.3564

Epoch 9: val_loss improved from 258.41013 to 243.35641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 253.3383 - MinusLogProbMetric: 253.3383 - val_loss: 243.3564 - val_MinusLogProbMetric: 243.3564 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 10/1000
2023-10-27 22:11:39.559 
Epoch 10/1000 
	 loss: 239.3984, MinusLogProbMetric: 239.3984, val_loss: 229.8511, val_MinusLogProbMetric: 229.8511

Epoch 10: val_loss improved from 243.35641 to 229.85107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 239.3984 - MinusLogProbMetric: 239.3984 - val_loss: 229.8511 - val_MinusLogProbMetric: 229.8511 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 11/1000
2023-10-27 22:12:22.937 
Epoch 11/1000 
	 loss: 238.6780, MinusLogProbMetric: 238.6780, val_loss: 223.5805, val_MinusLogProbMetric: 223.5805

Epoch 11: val_loss improved from 229.85107 to 223.58046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 238.6780 - MinusLogProbMetric: 238.6780 - val_loss: 223.5805 - val_MinusLogProbMetric: 223.5805 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 12/1000
2023-10-27 22:13:06.226 
Epoch 12/1000 
	 loss: 216.6354, MinusLogProbMetric: 216.6354, val_loss: 209.8507, val_MinusLogProbMetric: 209.8507

Epoch 12: val_loss improved from 223.58046 to 209.85074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 216.6354 - MinusLogProbMetric: 216.6354 - val_loss: 209.8507 - val_MinusLogProbMetric: 209.8507 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 13/1000
2023-10-27 22:13:48.982 
Epoch 13/1000 
	 loss: 206.6473, MinusLogProbMetric: 206.6473, val_loss: 200.9585, val_MinusLogProbMetric: 200.9585

Epoch 13: val_loss improved from 209.85074 to 200.95853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 206.6473 - MinusLogProbMetric: 206.6473 - val_loss: 200.9585 - val_MinusLogProbMetric: 200.9585 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 14/1000
2023-10-27 22:14:31.245 
Epoch 14/1000 
	 loss: 197.6897, MinusLogProbMetric: 197.6897, val_loss: 192.9363, val_MinusLogProbMetric: 192.9363

Epoch 14: val_loss improved from 200.95853 to 192.93628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 197.6897 - MinusLogProbMetric: 197.6897 - val_loss: 192.9363 - val_MinusLogProbMetric: 192.9363 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 15/1000
2023-10-27 22:15:11.986 
Epoch 15/1000 
	 loss: 190.3795, MinusLogProbMetric: 190.3795, val_loss: 187.8215, val_MinusLogProbMetric: 187.8215

Epoch 15: val_loss improved from 192.93628 to 187.82150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 190.3795 - MinusLogProbMetric: 190.3795 - val_loss: 187.8215 - val_MinusLogProbMetric: 187.8215 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 16/1000
2023-10-27 22:15:54.234 
Epoch 16/1000 
	 loss: 184.3917, MinusLogProbMetric: 184.3917, val_loss: 180.6999, val_MinusLogProbMetric: 180.6999

Epoch 16: val_loss improved from 187.82150 to 180.69994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 184.3917 - MinusLogProbMetric: 184.3917 - val_loss: 180.6999 - val_MinusLogProbMetric: 180.6999 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 17/1000
2023-10-27 22:16:36.437 
Epoch 17/1000 
	 loss: 178.0444, MinusLogProbMetric: 178.0444, val_loss: 177.7098, val_MinusLogProbMetric: 177.7098

Epoch 17: val_loss improved from 180.69994 to 177.70984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 178.0444 - MinusLogProbMetric: 178.0444 - val_loss: 177.7098 - val_MinusLogProbMetric: 177.7098 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 18/1000
2023-10-27 22:17:19.683 
Epoch 18/1000 
	 loss: 172.1712, MinusLogProbMetric: 172.1712, val_loss: 169.9083, val_MinusLogProbMetric: 169.9083

Epoch 18: val_loss improved from 177.70984 to 169.90826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 172.1712 - MinusLogProbMetric: 172.1712 - val_loss: 169.9083 - val_MinusLogProbMetric: 169.9083 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 19/1000
2023-10-27 22:18:03.274 
Epoch 19/1000 
	 loss: 167.8512, MinusLogProbMetric: 167.8512, val_loss: 165.7086, val_MinusLogProbMetric: 165.7086

Epoch 19: val_loss improved from 169.90826 to 165.70860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 167.8512 - MinusLogProbMetric: 167.8512 - val_loss: 165.7086 - val_MinusLogProbMetric: 165.7086 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 20/1000
2023-10-27 22:18:45.944 
Epoch 20/1000 
	 loss: 162.9083, MinusLogProbMetric: 162.9083, val_loss: 162.8909, val_MinusLogProbMetric: 162.8909

Epoch 20: val_loss improved from 165.70860 to 162.89093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 162.9083 - MinusLogProbMetric: 162.9083 - val_loss: 162.8909 - val_MinusLogProbMetric: 162.8909 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 21/1000
2023-10-27 22:19:27.818 
Epoch 21/1000 
	 loss: 159.4021, MinusLogProbMetric: 159.4021, val_loss: 156.1540, val_MinusLogProbMetric: 156.1540

Epoch 21: val_loss improved from 162.89093 to 156.15398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 159.4021 - MinusLogProbMetric: 159.4021 - val_loss: 156.1540 - val_MinusLogProbMetric: 156.1540 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 22/1000
2023-10-27 22:20:10.440 
Epoch 22/1000 
	 loss: 154.1923, MinusLogProbMetric: 154.1923, val_loss: 153.4796, val_MinusLogProbMetric: 153.4796

Epoch 22: val_loss improved from 156.15398 to 153.47957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 154.1923 - MinusLogProbMetric: 154.1923 - val_loss: 153.4796 - val_MinusLogProbMetric: 153.4796 - lr: 1.1111e-04 - 42s/epoch - 217ms/step
Epoch 23/1000
2023-10-27 22:20:54.376 
Epoch 23/1000 
	 loss: 151.0851, MinusLogProbMetric: 151.0851, val_loss: 149.0421, val_MinusLogProbMetric: 149.0421

Epoch 23: val_loss improved from 153.47957 to 149.04213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 151.0851 - MinusLogProbMetric: 151.0851 - val_loss: 149.0421 - val_MinusLogProbMetric: 149.0421 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 24/1000
2023-10-27 22:21:36.760 
Epoch 24/1000 
	 loss: 147.1601, MinusLogProbMetric: 147.1601, val_loss: 145.9267, val_MinusLogProbMetric: 145.9267

Epoch 24: val_loss improved from 149.04213 to 145.92665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 147.1601 - MinusLogProbMetric: 147.1601 - val_loss: 145.9267 - val_MinusLogProbMetric: 145.9267 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 25/1000
2023-10-27 22:22:20.747 
Epoch 25/1000 
	 loss: 144.0367, MinusLogProbMetric: 144.0367, val_loss: 142.5715, val_MinusLogProbMetric: 142.5715

Epoch 25: val_loss improved from 145.92665 to 142.57147, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 144.0367 - MinusLogProbMetric: 144.0367 - val_loss: 142.5715 - val_MinusLogProbMetric: 142.5715 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 26/1000
2023-10-27 22:23:04.742 
Epoch 26/1000 
	 loss: 140.7254, MinusLogProbMetric: 140.7254, val_loss: 139.6345, val_MinusLogProbMetric: 139.6345

Epoch 26: val_loss improved from 142.57147 to 139.63452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 140.7254 - MinusLogProbMetric: 140.7254 - val_loss: 139.6345 - val_MinusLogProbMetric: 139.6345 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 27/1000
2023-10-27 22:23:49.204 
Epoch 27/1000 
	 loss: 139.4310, MinusLogProbMetric: 139.4310, val_loss: 136.9199, val_MinusLogProbMetric: 136.9199

Epoch 27: val_loss improved from 139.63452 to 136.91989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 45s - loss: 139.4310 - MinusLogProbMetric: 139.4310 - val_loss: 136.9199 - val_MinusLogProbMetric: 136.9199 - lr: 1.1111e-04 - 45s/epoch - 227ms/step
Epoch 28/1000
2023-10-27 22:24:33.116 
Epoch 28/1000 
	 loss: 134.9468, MinusLogProbMetric: 134.9468, val_loss: 133.8661, val_MinusLogProbMetric: 133.8661

Epoch 28: val_loss improved from 136.91989 to 133.86606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 134.9468 - MinusLogProbMetric: 134.9468 - val_loss: 133.8661 - val_MinusLogProbMetric: 133.8661 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 29/1000
2023-10-27 22:25:15.462 
Epoch 29/1000 
	 loss: 132.5580, MinusLogProbMetric: 132.5580, val_loss: 132.0308, val_MinusLogProbMetric: 132.0308

Epoch 29: val_loss improved from 133.86606 to 132.03076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 132.5580 - MinusLogProbMetric: 132.5580 - val_loss: 132.0308 - val_MinusLogProbMetric: 132.0308 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 30/1000
2023-10-27 22:25:57.855 
Epoch 30/1000 
	 loss: 130.2185, MinusLogProbMetric: 130.2185, val_loss: 128.8876, val_MinusLogProbMetric: 128.8876

Epoch 30: val_loss improved from 132.03076 to 128.88757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 130.2185 - MinusLogProbMetric: 130.2185 - val_loss: 128.8876 - val_MinusLogProbMetric: 128.8876 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 31/1000
2023-10-27 22:26:41.125 
Epoch 31/1000 
	 loss: 127.4432, MinusLogProbMetric: 127.4432, val_loss: 126.2253, val_MinusLogProbMetric: 126.2253

Epoch 31: val_loss improved from 128.88757 to 126.22530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 127.4432 - MinusLogProbMetric: 127.4432 - val_loss: 126.2253 - val_MinusLogProbMetric: 126.2253 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 32/1000
2023-10-27 22:27:23.922 
Epoch 32/1000 
	 loss: 124.9366, MinusLogProbMetric: 124.9366, val_loss: 124.0876, val_MinusLogProbMetric: 124.0876

Epoch 32: val_loss improved from 126.22530 to 124.08759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 124.9366 - MinusLogProbMetric: 124.9366 - val_loss: 124.0876 - val_MinusLogProbMetric: 124.0876 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 33/1000
2023-10-27 22:28:05.302 
Epoch 33/1000 
	 loss: 122.9350, MinusLogProbMetric: 122.9350, val_loss: 122.4743, val_MinusLogProbMetric: 122.4743

Epoch 33: val_loss improved from 124.08759 to 122.47430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 122.9350 - MinusLogProbMetric: 122.9350 - val_loss: 122.4743 - val_MinusLogProbMetric: 122.4743 - lr: 1.1111e-04 - 41s/epoch - 212ms/step
Epoch 34/1000
2023-10-27 22:28:48.459 
Epoch 34/1000 
	 loss: 120.9772, MinusLogProbMetric: 120.9772, val_loss: 122.2865, val_MinusLogProbMetric: 122.2865

Epoch 34: val_loss improved from 122.47430 to 122.28654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 120.9772 - MinusLogProbMetric: 120.9772 - val_loss: 122.2865 - val_MinusLogProbMetric: 122.2865 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 35/1000
2023-10-27 22:29:29.730 
Epoch 35/1000 
	 loss: 118.7163, MinusLogProbMetric: 118.7163, val_loss: 117.9786, val_MinusLogProbMetric: 117.9786

Epoch 35: val_loss improved from 122.28654 to 117.97860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 118.7163 - MinusLogProbMetric: 118.7163 - val_loss: 117.9786 - val_MinusLogProbMetric: 117.9786 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 36/1000
2023-10-27 22:30:05.037 
Epoch 36/1000 
	 loss: 116.7552, MinusLogProbMetric: 116.7552, val_loss: 116.8711, val_MinusLogProbMetric: 116.8711

Epoch 36: val_loss improved from 117.97860 to 116.87112, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 116.7552 - MinusLogProbMetric: 116.7552 - val_loss: 116.8711 - val_MinusLogProbMetric: 116.8711 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 37/1000
2023-10-27 22:30:40.378 
Epoch 37/1000 
	 loss: 115.1745, MinusLogProbMetric: 115.1745, val_loss: 114.3625, val_MinusLogProbMetric: 114.3625

Epoch 37: val_loss improved from 116.87112 to 114.36246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 115.1745 - MinusLogProbMetric: 115.1745 - val_loss: 114.3625 - val_MinusLogProbMetric: 114.3625 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 38/1000
2023-10-27 22:31:18.677 
Epoch 38/1000 
	 loss: 113.4075, MinusLogProbMetric: 113.4075, val_loss: 114.3357, val_MinusLogProbMetric: 114.3357

Epoch 38: val_loss improved from 114.36246 to 114.33568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 113.4075 - MinusLogProbMetric: 113.4075 - val_loss: 114.3357 - val_MinusLogProbMetric: 114.3357 - lr: 1.1111e-04 - 38s/epoch - 195ms/step
Epoch 39/1000
2023-10-27 22:31:56.860 
Epoch 39/1000 
	 loss: 111.7734, MinusLogProbMetric: 111.7734, val_loss: 111.4195, val_MinusLogProbMetric: 111.4195

Epoch 39: val_loss improved from 114.33568 to 111.41946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 111.7734 - MinusLogProbMetric: 111.7734 - val_loss: 111.4195 - val_MinusLogProbMetric: 111.4195 - lr: 1.1111e-04 - 38s/epoch - 195ms/step
Epoch 40/1000
2023-10-27 22:32:32.295 
Epoch 40/1000 
	 loss: 110.2569, MinusLogProbMetric: 110.2569, val_loss: 109.2628, val_MinusLogProbMetric: 109.2628

Epoch 40: val_loss improved from 111.41946 to 109.26276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 110.2569 - MinusLogProbMetric: 110.2569 - val_loss: 109.2628 - val_MinusLogProbMetric: 109.2628 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 41/1000
2023-10-27 22:33:09.129 
Epoch 41/1000 
	 loss: 108.4960, MinusLogProbMetric: 108.4960, val_loss: 108.2654, val_MinusLogProbMetric: 108.2654

Epoch 41: val_loss improved from 109.26276 to 108.26545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 108.4960 - MinusLogProbMetric: 108.4960 - val_loss: 108.2654 - val_MinusLogProbMetric: 108.2654 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 42/1000
2023-10-27 22:33:46.295 
Epoch 42/1000 
	 loss: 107.1611, MinusLogProbMetric: 107.1611, val_loss: 107.6300, val_MinusLogProbMetric: 107.6300

Epoch 42: val_loss improved from 108.26545 to 107.63002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 107.1611 - MinusLogProbMetric: 107.1611 - val_loss: 107.6300 - val_MinusLogProbMetric: 107.6300 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 43/1000
2023-10-27 22:34:25.193 
Epoch 43/1000 
	 loss: 105.7070, MinusLogProbMetric: 105.7070, val_loss: 105.3741, val_MinusLogProbMetric: 105.3741

Epoch 43: val_loss improved from 107.63002 to 105.37411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 39s - loss: 105.7070 - MinusLogProbMetric: 105.7070 - val_loss: 105.3741 - val_MinusLogProbMetric: 105.3741 - lr: 1.1111e-04 - 39s/epoch - 199ms/step
Epoch 44/1000
2023-10-27 22:35:00.989 
Epoch 44/1000 
	 loss: 104.4112, MinusLogProbMetric: 104.4112, val_loss: 104.2977, val_MinusLogProbMetric: 104.2977

Epoch 44: val_loss improved from 105.37411 to 104.29767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 104.4112 - MinusLogProbMetric: 104.4112 - val_loss: 104.2977 - val_MinusLogProbMetric: 104.2977 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 45/1000
2023-10-27 22:35:37.574 
Epoch 45/1000 
	 loss: 102.9317, MinusLogProbMetric: 102.9317, val_loss: 103.0958, val_MinusLogProbMetric: 103.0958

Epoch 45: val_loss improved from 104.29767 to 103.09576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 102.9317 - MinusLogProbMetric: 102.9317 - val_loss: 103.0958 - val_MinusLogProbMetric: 103.0958 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 46/1000
2023-10-27 22:36:15.189 
Epoch 46/1000 
	 loss: 101.6279, MinusLogProbMetric: 101.6279, val_loss: 101.3922, val_MinusLogProbMetric: 101.3922

Epoch 46: val_loss improved from 103.09576 to 101.39221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 101.6279 - MinusLogProbMetric: 101.6279 - val_loss: 101.3922 - val_MinusLogProbMetric: 101.3922 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 47/1000
2023-10-27 22:36:52.880 
Epoch 47/1000 
	 loss: 100.3046, MinusLogProbMetric: 100.3046, val_loss: 100.2425, val_MinusLogProbMetric: 100.2425

Epoch 47: val_loss improved from 101.39221 to 100.24251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 100.3046 - MinusLogProbMetric: 100.3046 - val_loss: 100.2425 - val_MinusLogProbMetric: 100.2425 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 48/1000
2023-10-27 22:37:29.329 
Epoch 48/1000 
	 loss: 99.0708, MinusLogProbMetric: 99.0708, val_loss: 99.3018, val_MinusLogProbMetric: 99.3018

Epoch 48: val_loss improved from 100.24251 to 99.30183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 99.0708 - MinusLogProbMetric: 99.0708 - val_loss: 99.3018 - val_MinusLogProbMetric: 99.3018 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 49/1000
2023-10-27 22:38:05.898 
Epoch 49/1000 
	 loss: 98.0205, MinusLogProbMetric: 98.0205, val_loss: 98.0748, val_MinusLogProbMetric: 98.0748

Epoch 49: val_loss improved from 99.30183 to 98.07481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 98.0205 - MinusLogProbMetric: 98.0205 - val_loss: 98.0748 - val_MinusLogProbMetric: 98.0748 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 50/1000
2023-10-27 22:38:44.328 
Epoch 50/1000 
	 loss: 97.0334, MinusLogProbMetric: 97.0334, val_loss: 97.8744, val_MinusLogProbMetric: 97.8744

Epoch 50: val_loss improved from 98.07481 to 97.87442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 97.0334 - MinusLogProbMetric: 97.0334 - val_loss: 97.8744 - val_MinusLogProbMetric: 97.8744 - lr: 1.1111e-04 - 38s/epoch - 195ms/step
Epoch 51/1000
2023-10-27 22:39:20.997 
Epoch 51/1000 
	 loss: 95.7295, MinusLogProbMetric: 95.7295, val_loss: 95.5741, val_MinusLogProbMetric: 95.5741

Epoch 51: val_loss improved from 97.87442 to 95.57407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 95.7295 - MinusLogProbMetric: 95.7295 - val_loss: 95.5741 - val_MinusLogProbMetric: 95.5741 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 52/1000
2023-10-27 22:39:57.053 
Epoch 52/1000 
	 loss: 94.9194, MinusLogProbMetric: 94.9194, val_loss: 94.9830, val_MinusLogProbMetric: 94.9830

Epoch 52: val_loss improved from 95.57407 to 94.98300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 94.9194 - MinusLogProbMetric: 94.9194 - val_loss: 94.9830 - val_MinusLogProbMetric: 94.9830 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 53/1000
2023-10-27 22:40:33.960 
Epoch 53/1000 
	 loss: 93.6047, MinusLogProbMetric: 93.6047, val_loss: 94.2908, val_MinusLogProbMetric: 94.2908

Epoch 53: val_loss improved from 94.98300 to 94.29081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 93.6047 - MinusLogProbMetric: 93.6047 - val_loss: 94.2908 - val_MinusLogProbMetric: 94.2908 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 54/1000
2023-10-27 22:41:10.055 
Epoch 54/1000 
	 loss: 92.7109, MinusLogProbMetric: 92.7109, val_loss: 92.9376, val_MinusLogProbMetric: 92.9376

Epoch 54: val_loss improved from 94.29081 to 92.93759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 92.7109 - MinusLogProbMetric: 92.7109 - val_loss: 92.9376 - val_MinusLogProbMetric: 92.9376 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 55/1000
2023-10-27 22:41:45.961 
Epoch 55/1000 
	 loss: 91.6762, MinusLogProbMetric: 91.6762, val_loss: 92.8639, val_MinusLogProbMetric: 92.8639

Epoch 55: val_loss improved from 92.93759 to 92.86393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 91.6762 - MinusLogProbMetric: 91.6762 - val_loss: 92.8639 - val_MinusLogProbMetric: 92.8639 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 56/1000
2023-10-27 22:42:22.724 
Epoch 56/1000 
	 loss: 90.7861, MinusLogProbMetric: 90.7861, val_loss: 91.0093, val_MinusLogProbMetric: 91.0093

Epoch 56: val_loss improved from 92.86393 to 91.00933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 90.7861 - MinusLogProbMetric: 90.7861 - val_loss: 91.0093 - val_MinusLogProbMetric: 91.0093 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 57/1000
2023-10-27 22:42:59.712 
Epoch 57/1000 
	 loss: 89.8735, MinusLogProbMetric: 89.8735, val_loss: 90.3776, val_MinusLogProbMetric: 90.3776

Epoch 57: val_loss improved from 91.00933 to 90.37762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 89.8735 - MinusLogProbMetric: 89.8735 - val_loss: 90.3776 - val_MinusLogProbMetric: 90.3776 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 58/1000
2023-10-27 22:43:37.022 
Epoch 58/1000 
	 loss: 88.9576, MinusLogProbMetric: 88.9576, val_loss: 88.9283, val_MinusLogProbMetric: 88.9283

Epoch 58: val_loss improved from 90.37762 to 88.92832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 88.9576 - MinusLogProbMetric: 88.9576 - val_loss: 88.9283 - val_MinusLogProbMetric: 88.9283 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 59/1000
2023-10-27 22:44:12.894 
Epoch 59/1000 
	 loss: 88.1060, MinusLogProbMetric: 88.1060, val_loss: 88.0989, val_MinusLogProbMetric: 88.0989

Epoch 59: val_loss improved from 88.92832 to 88.09889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 88.1060 - MinusLogProbMetric: 88.1060 - val_loss: 88.0989 - val_MinusLogProbMetric: 88.0989 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 60/1000
2023-10-27 22:44:49.877 
Epoch 60/1000 
	 loss: 87.4474, MinusLogProbMetric: 87.4474, val_loss: 87.5487, val_MinusLogProbMetric: 87.5487

Epoch 60: val_loss improved from 88.09889 to 87.54866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 87.4474 - MinusLogProbMetric: 87.4474 - val_loss: 87.5487 - val_MinusLogProbMetric: 87.5487 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 61/1000
2023-10-27 22:45:27.793 
Epoch 61/1000 
	 loss: 86.4960, MinusLogProbMetric: 86.4960, val_loss: 86.3049, val_MinusLogProbMetric: 86.3049

Epoch 61: val_loss improved from 87.54866 to 86.30488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 86.4960 - MinusLogProbMetric: 86.4960 - val_loss: 86.3049 - val_MinusLogProbMetric: 86.3049 - lr: 1.1111e-04 - 38s/epoch - 195ms/step
Epoch 62/1000
2023-10-27 22:46:05.916 
Epoch 62/1000 
	 loss: 85.8798, MinusLogProbMetric: 85.8798, val_loss: 86.6571, val_MinusLogProbMetric: 86.6571

Epoch 62: val_loss did not improve from 86.30488
196/196 - 37s - loss: 85.8798 - MinusLogProbMetric: 85.8798 - val_loss: 86.6571 - val_MinusLogProbMetric: 86.6571 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 63/1000
2023-10-27 22:46:41.600 
Epoch 63/1000 
	 loss: 85.1550, MinusLogProbMetric: 85.1550, val_loss: 85.6488, val_MinusLogProbMetric: 85.6488

Epoch 63: val_loss improved from 86.30488 to 85.64882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 85.1550 - MinusLogProbMetric: 85.1550 - val_loss: 85.6488 - val_MinusLogProbMetric: 85.6488 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 64/1000
2023-10-27 22:47:19.460 
Epoch 64/1000 
	 loss: 84.3864, MinusLogProbMetric: 84.3864, val_loss: 84.8596, val_MinusLogProbMetric: 84.8596

Epoch 64: val_loss improved from 85.64882 to 84.85965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 84.3864 - MinusLogProbMetric: 84.3864 - val_loss: 84.8596 - val_MinusLogProbMetric: 84.8596 - lr: 1.1111e-04 - 38s/epoch - 193ms/step
Epoch 65/1000
2023-10-27 22:47:58.697 
Epoch 65/1000 
	 loss: 83.7064, MinusLogProbMetric: 83.7064, val_loss: 84.0680, val_MinusLogProbMetric: 84.0680

Epoch 65: val_loss improved from 84.85965 to 84.06802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 39s - loss: 83.7064 - MinusLogProbMetric: 83.7064 - val_loss: 84.0680 - val_MinusLogProbMetric: 84.0680 - lr: 1.1111e-04 - 39s/epoch - 200ms/step
Epoch 66/1000
2023-10-27 22:48:34.224 
Epoch 66/1000 
	 loss: 82.9775, MinusLogProbMetric: 82.9775, val_loss: 82.9094, val_MinusLogProbMetric: 82.9094

Epoch 66: val_loss improved from 84.06802 to 82.90941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 82.9775 - MinusLogProbMetric: 82.9775 - val_loss: 82.9094 - val_MinusLogProbMetric: 82.9094 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 67/1000
2023-10-27 22:49:09.918 
Epoch 67/1000 
	 loss: 82.3058, MinusLogProbMetric: 82.3058, val_loss: 82.8338, val_MinusLogProbMetric: 82.8338

Epoch 67: val_loss improved from 82.90941 to 82.83382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 82.3058 - MinusLogProbMetric: 82.3058 - val_loss: 82.8338 - val_MinusLogProbMetric: 82.8338 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 68/1000
2023-10-27 22:49:48.241 
Epoch 68/1000 
	 loss: 81.6872, MinusLogProbMetric: 81.6872, val_loss: 81.6349, val_MinusLogProbMetric: 81.6349

Epoch 68: val_loss improved from 82.83382 to 81.63492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 81.6872 - MinusLogProbMetric: 81.6872 - val_loss: 81.6349 - val_MinusLogProbMetric: 81.6349 - lr: 1.1111e-04 - 38s/epoch - 196ms/step
Epoch 69/1000
2023-10-27 22:50:24.555 
Epoch 69/1000 
	 loss: 80.9911, MinusLogProbMetric: 80.9911, val_loss: 81.4085, val_MinusLogProbMetric: 81.4085

Epoch 69: val_loss improved from 81.63492 to 81.40853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 80.9911 - MinusLogProbMetric: 80.9911 - val_loss: 81.4085 - val_MinusLogProbMetric: 81.4085 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 70/1000
2023-10-27 22:51:00.725 
Epoch 70/1000 
	 loss: 80.4643, MinusLogProbMetric: 80.4643, val_loss: 80.9127, val_MinusLogProbMetric: 80.9127

Epoch 70: val_loss improved from 81.40853 to 80.91273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 80.4643 - MinusLogProbMetric: 80.4643 - val_loss: 80.9127 - val_MinusLogProbMetric: 80.9127 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 71/1000
2023-10-27 22:51:36.407 
Epoch 71/1000 
	 loss: 79.8903, MinusLogProbMetric: 79.8903, val_loss: 80.2048, val_MinusLogProbMetric: 80.2048

Epoch 71: val_loss improved from 80.91273 to 80.20483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 79.8903 - MinusLogProbMetric: 79.8903 - val_loss: 80.2048 - val_MinusLogProbMetric: 80.2048 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 72/1000
2023-10-27 22:52:15.207 
Epoch 72/1000 
	 loss: 79.7565, MinusLogProbMetric: 79.7565, val_loss: 79.6549, val_MinusLogProbMetric: 79.6549

Epoch 72: val_loss improved from 80.20483 to 79.65492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 39s - loss: 79.7565 - MinusLogProbMetric: 79.7565 - val_loss: 79.6549 - val_MinusLogProbMetric: 79.6549 - lr: 1.1111e-04 - 39s/epoch - 199ms/step
Epoch 73/1000
2023-10-27 22:52:52.533 
Epoch 73/1000 
	 loss: 78.6957, MinusLogProbMetric: 78.6957, val_loss: 79.1654, val_MinusLogProbMetric: 79.1654

Epoch 73: val_loss improved from 79.65492 to 79.16539, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 78.6957 - MinusLogProbMetric: 78.6957 - val_loss: 79.1654 - val_MinusLogProbMetric: 79.1654 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 74/1000
2023-10-27 22:53:28.411 
Epoch 74/1000 
	 loss: 78.1837, MinusLogProbMetric: 78.1837, val_loss: 78.5687, val_MinusLogProbMetric: 78.5687

Epoch 74: val_loss improved from 79.16539 to 78.56873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 78.1837 - MinusLogProbMetric: 78.1837 - val_loss: 78.5687 - val_MinusLogProbMetric: 78.5687 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 75/1000
2023-10-27 22:54:05.096 
Epoch 75/1000 
	 loss: 77.9813, MinusLogProbMetric: 77.9813, val_loss: 77.9165, val_MinusLogProbMetric: 77.9165

Epoch 75: val_loss improved from 78.56873 to 77.91646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 77.9813 - MinusLogProbMetric: 77.9813 - val_loss: 77.9165 - val_MinusLogProbMetric: 77.9165 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 76/1000
2023-10-27 22:54:43.821 
Epoch 76/1000 
	 loss: 77.2948, MinusLogProbMetric: 77.2948, val_loss: 77.7685, val_MinusLogProbMetric: 77.7685

Epoch 76: val_loss improved from 77.91646 to 77.76849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 39s - loss: 77.2948 - MinusLogProbMetric: 77.2948 - val_loss: 77.7685 - val_MinusLogProbMetric: 77.7685 - lr: 1.1111e-04 - 39s/epoch - 198ms/step
Epoch 77/1000
2023-10-27 22:55:20.155 
Epoch 77/1000 
	 loss: 76.9311, MinusLogProbMetric: 76.9311, val_loss: 77.0872, val_MinusLogProbMetric: 77.0872

Epoch 77: val_loss improved from 77.76849 to 77.08719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 76.9311 - MinusLogProbMetric: 76.9311 - val_loss: 77.0872 - val_MinusLogProbMetric: 77.0872 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 78/1000
2023-10-27 22:55:55.943 
Epoch 78/1000 
	 loss: 76.2640, MinusLogProbMetric: 76.2640, val_loss: 76.5677, val_MinusLogProbMetric: 76.5677

Epoch 78: val_loss improved from 77.08719 to 76.56769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 76.2640 - MinusLogProbMetric: 76.2640 - val_loss: 76.5677 - val_MinusLogProbMetric: 76.5677 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 79/1000
2023-10-27 22:56:38.972 
Epoch 79/1000 
	 loss: 101.6004, MinusLogProbMetric: 101.6004, val_loss: 86.8960, val_MinusLogProbMetric: 86.8960

Epoch 79: val_loss did not improve from 76.56769
196/196 - 42s - loss: 101.6004 - MinusLogProbMetric: 101.6004 - val_loss: 86.8960 - val_MinusLogProbMetric: 86.8960 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 80/1000
2023-10-27 22:57:15.821 
Epoch 80/1000 
	 loss: 83.8644, MinusLogProbMetric: 83.8644, val_loss: 82.1991, val_MinusLogProbMetric: 82.1991

Epoch 80: val_loss did not improve from 76.56769
196/196 - 37s - loss: 83.8644 - MinusLogProbMetric: 83.8644 - val_loss: 82.1991 - val_MinusLogProbMetric: 82.1991 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 81/1000
2023-10-27 22:57:50.535 
Epoch 81/1000 
	 loss: 80.7439, MinusLogProbMetric: 80.7439, val_loss: 80.3911, val_MinusLogProbMetric: 80.3911

Epoch 81: val_loss did not improve from 76.56769
196/196 - 35s - loss: 80.7439 - MinusLogProbMetric: 80.7439 - val_loss: 80.3911 - val_MinusLogProbMetric: 80.3911 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 82/1000
2023-10-27 22:58:27.434 
Epoch 82/1000 
	 loss: 78.8472, MinusLogProbMetric: 78.8472, val_loss: 79.4109, val_MinusLogProbMetric: 79.4109

Epoch 82: val_loss did not improve from 76.56769
196/196 - 37s - loss: 78.8472 - MinusLogProbMetric: 78.8472 - val_loss: 79.4109 - val_MinusLogProbMetric: 79.4109 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 83/1000
2023-10-27 22:59:05.756 
Epoch 83/1000 
	 loss: 77.5647, MinusLogProbMetric: 77.5647, val_loss: 78.0301, val_MinusLogProbMetric: 78.0301

Epoch 83: val_loss did not improve from 76.56769
196/196 - 38s - loss: 77.5647 - MinusLogProbMetric: 77.5647 - val_loss: 78.0301 - val_MinusLogProbMetric: 78.0301 - lr: 1.1111e-04 - 38s/epoch - 195ms/step
Epoch 84/1000
2023-10-27 22:59:41.387 
Epoch 84/1000 
	 loss: 76.5122, MinusLogProbMetric: 76.5122, val_loss: 76.6084, val_MinusLogProbMetric: 76.6084

Epoch 84: val_loss did not improve from 76.56769
196/196 - 36s - loss: 76.5122 - MinusLogProbMetric: 76.5122 - val_loss: 76.6084 - val_MinusLogProbMetric: 76.6084 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 85/1000
2023-10-27 23:00:16.460 
Epoch 85/1000 
	 loss: 75.7166, MinusLogProbMetric: 75.7166, val_loss: 75.8318, val_MinusLogProbMetric: 75.8318

Epoch 85: val_loss improved from 76.56769 to 75.83177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 75.7166 - MinusLogProbMetric: 75.7166 - val_loss: 75.8318 - val_MinusLogProbMetric: 75.8318 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 86/1000
2023-10-27 23:00:53.519 
Epoch 86/1000 
	 loss: 75.1500, MinusLogProbMetric: 75.1500, val_loss: 75.1830, val_MinusLogProbMetric: 75.1830

Epoch 86: val_loss improved from 75.83177 to 75.18302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 75.1500 - MinusLogProbMetric: 75.1500 - val_loss: 75.1830 - val_MinusLogProbMetric: 75.1830 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 87/1000
2023-10-27 23:01:33.676 
Epoch 87/1000 
	 loss: 74.5050, MinusLogProbMetric: 74.5050, val_loss: 74.5175, val_MinusLogProbMetric: 74.5175

Epoch 87: val_loss improved from 75.18302 to 74.51749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 40s - loss: 74.5050 - MinusLogProbMetric: 74.5050 - val_loss: 74.5175 - val_MinusLogProbMetric: 74.5175 - lr: 1.1111e-04 - 40s/epoch - 204ms/step
Epoch 88/1000
2023-10-27 23:02:09.801 
Epoch 88/1000 
	 loss: 73.8856, MinusLogProbMetric: 73.8856, val_loss: 74.3948, val_MinusLogProbMetric: 74.3948

Epoch 88: val_loss improved from 74.51749 to 74.39478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 73.8856 - MinusLogProbMetric: 73.8856 - val_loss: 74.3948 - val_MinusLogProbMetric: 74.3948 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 89/1000
2023-10-27 23:02:45.296 
Epoch 89/1000 
	 loss: 73.5157, MinusLogProbMetric: 73.5157, val_loss: 74.2458, val_MinusLogProbMetric: 74.2458

Epoch 89: val_loss improved from 74.39478 to 74.24581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 73.5157 - MinusLogProbMetric: 73.5157 - val_loss: 74.2458 - val_MinusLogProbMetric: 74.2458 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 90/1000
2023-10-27 23:03:25.262 
Epoch 90/1000 
	 loss: 73.2201, MinusLogProbMetric: 73.2201, val_loss: 74.7664, val_MinusLogProbMetric: 74.7664

Epoch 90: val_loss did not improve from 74.24581
196/196 - 39s - loss: 73.2201 - MinusLogProbMetric: 73.2201 - val_loss: 74.7664 - val_MinusLogProbMetric: 74.7664 - lr: 1.1111e-04 - 39s/epoch - 201ms/step
Epoch 91/1000
2023-10-27 23:04:04.033 
Epoch 91/1000 
	 loss: 72.6655, MinusLogProbMetric: 72.6655, val_loss: 72.9873, val_MinusLogProbMetric: 72.9873

Epoch 91: val_loss improved from 74.24581 to 72.98732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 39s - loss: 72.6655 - MinusLogProbMetric: 72.6655 - val_loss: 72.9873 - val_MinusLogProbMetric: 72.9873 - lr: 1.1111e-04 - 39s/epoch - 201ms/step
Epoch 92/1000
2023-10-27 23:04:40.625 
Epoch 92/1000 
	 loss: 72.2396, MinusLogProbMetric: 72.2396, val_loss: 73.0943, val_MinusLogProbMetric: 73.0943

Epoch 92: val_loss did not improve from 72.98732
196/196 - 36s - loss: 72.2396 - MinusLogProbMetric: 72.2396 - val_loss: 73.0943 - val_MinusLogProbMetric: 73.0943 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 93/1000
2023-10-27 23:05:16.379 
Epoch 93/1000 
	 loss: 71.9542, MinusLogProbMetric: 71.9542, val_loss: 72.4718, val_MinusLogProbMetric: 72.4718

Epoch 93: val_loss improved from 72.98732 to 72.47180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 71.9542 - MinusLogProbMetric: 71.9542 - val_loss: 72.4718 - val_MinusLogProbMetric: 72.4718 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 94/1000
2023-10-27 23:05:57.919 
Epoch 94/1000 
	 loss: 71.4203, MinusLogProbMetric: 71.4203, val_loss: 71.9895, val_MinusLogProbMetric: 71.9895

Epoch 94: val_loss improved from 72.47180 to 71.98952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 71.4203 - MinusLogProbMetric: 71.4203 - val_loss: 71.9895 - val_MinusLogProbMetric: 71.9895 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 95/1000
2023-10-27 23:06:34.047 
Epoch 95/1000 
	 loss: 71.2540, MinusLogProbMetric: 71.2540, val_loss: 71.6138, val_MinusLogProbMetric: 71.6138

Epoch 95: val_loss improved from 71.98952 to 71.61380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 71.2540 - MinusLogProbMetric: 71.2540 - val_loss: 71.6138 - val_MinusLogProbMetric: 71.6138 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 96/1000
2023-10-27 23:07:09.851 
Epoch 96/1000 
	 loss: 70.7866, MinusLogProbMetric: 70.7866, val_loss: 71.0579, val_MinusLogProbMetric: 71.0579

Epoch 96: val_loss improved from 71.61380 to 71.05788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 70.7866 - MinusLogProbMetric: 70.7866 - val_loss: 71.0579 - val_MinusLogProbMetric: 71.0579 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 97/1000
2023-10-27 23:07:47.590 
Epoch 97/1000 
	 loss: 70.5052, MinusLogProbMetric: 70.5052, val_loss: 70.9372, val_MinusLogProbMetric: 70.9372

Epoch 97: val_loss improved from 71.05788 to 70.93723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 70.5052 - MinusLogProbMetric: 70.5052 - val_loss: 70.9372 - val_MinusLogProbMetric: 70.9372 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 98/1000
2023-10-27 23:08:26.075 
Epoch 98/1000 
	 loss: 70.2659, MinusLogProbMetric: 70.2659, val_loss: 71.0520, val_MinusLogProbMetric: 71.0520

Epoch 98: val_loss did not improve from 70.93723
196/196 - 38s - loss: 70.2659 - MinusLogProbMetric: 70.2659 - val_loss: 71.0520 - val_MinusLogProbMetric: 71.0520 - lr: 1.1111e-04 - 38s/epoch - 194ms/step
Epoch 99/1000
2023-10-27 23:09:01.759 
Epoch 99/1000 
	 loss: 69.9249, MinusLogProbMetric: 69.9249, val_loss: 70.2660, val_MinusLogProbMetric: 70.2660

Epoch 99: val_loss improved from 70.93723 to 70.26600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 69.9249 - MinusLogProbMetric: 69.9249 - val_loss: 70.2660 - val_MinusLogProbMetric: 70.2660 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 100/1000
2023-10-27 23:09:38.415 
Epoch 100/1000 
	 loss: 69.6420, MinusLogProbMetric: 69.6420, val_loss: 70.4068, val_MinusLogProbMetric: 70.4068

Epoch 100: val_loss did not improve from 70.26600
196/196 - 36s - loss: 69.6420 - MinusLogProbMetric: 69.6420 - val_loss: 70.4068 - val_MinusLogProbMetric: 70.4068 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 101/1000
2023-10-27 23:10:15.399 
Epoch 101/1000 
	 loss: 69.3357, MinusLogProbMetric: 69.3357, val_loss: 70.2547, val_MinusLogProbMetric: 70.2547

Epoch 101: val_loss improved from 70.26600 to 70.25468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 69.3357 - MinusLogProbMetric: 69.3357 - val_loss: 70.2547 - val_MinusLogProbMetric: 70.2547 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 102/1000
2023-10-27 23:10:55.146 
Epoch 102/1000 
	 loss: 69.3064, MinusLogProbMetric: 69.3064, val_loss: 69.4143, val_MinusLogProbMetric: 69.4143

Epoch 102: val_loss improved from 70.25468 to 69.41433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 40s - loss: 69.3064 - MinusLogProbMetric: 69.3064 - val_loss: 69.4143 - val_MinusLogProbMetric: 69.4143 - lr: 1.1111e-04 - 40s/epoch - 203ms/step
Epoch 103/1000
2023-10-27 23:11:30.601 
Epoch 103/1000 
	 loss: 68.7610, MinusLogProbMetric: 68.7610, val_loss: 69.2665, val_MinusLogProbMetric: 69.2665

Epoch 103: val_loss improved from 69.41433 to 69.26647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 68.7610 - MinusLogProbMetric: 68.7610 - val_loss: 69.2665 - val_MinusLogProbMetric: 69.2665 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 104/1000
2023-10-27 23:12:07.311 
Epoch 104/1000 
	 loss: 68.7926, MinusLogProbMetric: 68.7926, val_loss: 69.0508, val_MinusLogProbMetric: 69.0508

Epoch 104: val_loss improved from 69.26647 to 69.05078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 68.7926 - MinusLogProbMetric: 68.7926 - val_loss: 69.0508 - val_MinusLogProbMetric: 69.0508 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 105/1000
2023-10-27 23:12:49.703 
Epoch 105/1000 
	 loss: 68.2783, MinusLogProbMetric: 68.2783, val_loss: 69.3289, val_MinusLogProbMetric: 69.3289

Epoch 105: val_loss did not improve from 69.05078
196/196 - 42s - loss: 68.2783 - MinusLogProbMetric: 68.2783 - val_loss: 69.3289 - val_MinusLogProbMetric: 69.3289 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 106/1000
2023-10-27 23:13:32.370 
Epoch 106/1000 
	 loss: 68.0563, MinusLogProbMetric: 68.0563, val_loss: 68.6171, val_MinusLogProbMetric: 68.6171

Epoch 106: val_loss improved from 69.05078 to 68.61709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 68.0563 - MinusLogProbMetric: 68.0563 - val_loss: 68.6171 - val_MinusLogProbMetric: 68.6171 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 107/1000
2023-10-27 23:14:16.271 
Epoch 107/1000 
	 loss: 67.8480, MinusLogProbMetric: 67.8480, val_loss: 68.5694, val_MinusLogProbMetric: 68.5694

Epoch 107: val_loss improved from 68.61709 to 68.56939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 67.8480 - MinusLogProbMetric: 67.8480 - val_loss: 68.5694 - val_MinusLogProbMetric: 68.5694 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 108/1000
2023-10-27 23:15:00.014 
Epoch 108/1000 
	 loss: 67.6512, MinusLogProbMetric: 67.6512, val_loss: 67.7660, val_MinusLogProbMetric: 67.7660

Epoch 108: val_loss improved from 68.56939 to 67.76595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 67.6512 - MinusLogProbMetric: 67.6512 - val_loss: 67.7660 - val_MinusLogProbMetric: 67.7660 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 109/1000
2023-10-27 23:15:43.836 
Epoch 109/1000 
	 loss: 67.3571, MinusLogProbMetric: 67.3571, val_loss: 67.8288, val_MinusLogProbMetric: 67.8288

Epoch 109: val_loss did not improve from 67.76595
196/196 - 43s - loss: 67.3571 - MinusLogProbMetric: 67.3571 - val_loss: 67.8288 - val_MinusLogProbMetric: 67.8288 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 110/1000
2023-10-27 23:16:27.660 
Epoch 110/1000 
	 loss: 67.0813, MinusLogProbMetric: 67.0813, val_loss: 67.7839, val_MinusLogProbMetric: 67.7839

Epoch 110: val_loss did not improve from 67.76595
196/196 - 44s - loss: 67.0813 - MinusLogProbMetric: 67.0813 - val_loss: 67.7839 - val_MinusLogProbMetric: 67.7839 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 111/1000
2023-10-27 23:17:11.530 
Epoch 111/1000 
	 loss: 66.8597, MinusLogProbMetric: 66.8597, val_loss: 67.0558, val_MinusLogProbMetric: 67.0558

Epoch 111: val_loss improved from 67.76595 to 67.05583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 45s - loss: 66.8597 - MinusLogProbMetric: 66.8597 - val_loss: 67.0558 - val_MinusLogProbMetric: 67.0558 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 112/1000
2023-10-27 23:17:55.547 
Epoch 112/1000 
	 loss: 66.6592, MinusLogProbMetric: 66.6592, val_loss: 67.7418, val_MinusLogProbMetric: 67.7418

Epoch 112: val_loss did not improve from 67.05583
196/196 - 43s - loss: 66.6592 - MinusLogProbMetric: 66.6592 - val_loss: 67.7418 - val_MinusLogProbMetric: 67.7418 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 113/1000
2023-10-27 23:18:37.758 
Epoch 113/1000 
	 loss: 66.8320, MinusLogProbMetric: 66.8320, val_loss: 67.0559, val_MinusLogProbMetric: 67.0559

Epoch 113: val_loss did not improve from 67.05583
196/196 - 42s - loss: 66.8320 - MinusLogProbMetric: 66.8320 - val_loss: 67.0559 - val_MinusLogProbMetric: 67.0559 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 114/1000
2023-10-27 23:19:19.951 
Epoch 114/1000 
	 loss: 66.2402, MinusLogProbMetric: 66.2402, val_loss: 66.5708, val_MinusLogProbMetric: 66.5708

Epoch 114: val_loss improved from 67.05583 to 66.57079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 66.2402 - MinusLogProbMetric: 66.2402 - val_loss: 66.5708 - val_MinusLogProbMetric: 66.5708 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 115/1000
2023-10-27 23:20:04.038 
Epoch 115/1000 
	 loss: 65.9705, MinusLogProbMetric: 65.9705, val_loss: 66.3450, val_MinusLogProbMetric: 66.3450

Epoch 115: val_loss improved from 66.57079 to 66.34499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 65.9705 - MinusLogProbMetric: 65.9705 - val_loss: 66.3450 - val_MinusLogProbMetric: 66.3450 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 116/1000
2023-10-27 23:20:47.448 
Epoch 116/1000 
	 loss: 65.7308, MinusLogProbMetric: 65.7308, val_loss: 66.1886, val_MinusLogProbMetric: 66.1886

Epoch 116: val_loss improved from 66.34499 to 66.18862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 65.7308 - MinusLogProbMetric: 65.7308 - val_loss: 66.1886 - val_MinusLogProbMetric: 66.1886 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 117/1000
2023-10-27 23:21:32.333 
Epoch 117/1000 
	 loss: 65.7033, MinusLogProbMetric: 65.7033, val_loss: 65.9841, val_MinusLogProbMetric: 65.9841

Epoch 117: val_loss improved from 66.18862 to 65.98408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 45s - loss: 65.7033 - MinusLogProbMetric: 65.7033 - val_loss: 65.9841 - val_MinusLogProbMetric: 65.9841 - lr: 1.1111e-04 - 45s/epoch - 229ms/step
Epoch 118/1000
2023-10-27 23:22:16.703 
Epoch 118/1000 
	 loss: 66.4136, MinusLogProbMetric: 66.4136, val_loss: 66.2717, val_MinusLogProbMetric: 66.2717

Epoch 118: val_loss did not improve from 65.98408
196/196 - 44s - loss: 66.4136 - MinusLogProbMetric: 66.4136 - val_loss: 66.2717 - val_MinusLogProbMetric: 66.2717 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 119/1000
2023-10-27 23:22:59.039 
Epoch 119/1000 
	 loss: 65.4293, MinusLogProbMetric: 65.4293, val_loss: 65.6793, val_MinusLogProbMetric: 65.6793

Epoch 119: val_loss improved from 65.98408 to 65.67934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 65.4293 - MinusLogProbMetric: 65.4293 - val_loss: 65.6793 - val_MinusLogProbMetric: 65.6793 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 120/1000
2023-10-27 23:23:43.577 
Epoch 120/1000 
	 loss: 64.9910, MinusLogProbMetric: 64.9910, val_loss: 66.7991, val_MinusLogProbMetric: 66.7991

Epoch 120: val_loss did not improve from 65.67934
196/196 - 44s - loss: 64.9910 - MinusLogProbMetric: 64.9910 - val_loss: 66.7991 - val_MinusLogProbMetric: 66.7991 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 121/1000
2023-10-27 23:24:26.953 
Epoch 121/1000 
	 loss: 65.0085, MinusLogProbMetric: 65.0085, val_loss: 66.6891, val_MinusLogProbMetric: 66.6891

Epoch 121: val_loss did not improve from 65.67934
196/196 - 43s - loss: 65.0085 - MinusLogProbMetric: 65.0085 - val_loss: 66.6891 - val_MinusLogProbMetric: 66.6891 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 122/1000
2023-10-27 23:25:09.157 
Epoch 122/1000 
	 loss: 64.5083, MinusLogProbMetric: 64.5083, val_loss: 65.1304, val_MinusLogProbMetric: 65.1304

Epoch 122: val_loss improved from 65.67934 to 65.13036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 64.5083 - MinusLogProbMetric: 64.5083 - val_loss: 65.1304 - val_MinusLogProbMetric: 65.1304 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 123/1000
2023-10-27 23:25:53.514 
Epoch 123/1000 
	 loss: 64.4469, MinusLogProbMetric: 64.4469, val_loss: 65.3160, val_MinusLogProbMetric: 65.3160

Epoch 123: val_loss did not improve from 65.13036
196/196 - 44s - loss: 64.4469 - MinusLogProbMetric: 64.4469 - val_loss: 65.3160 - val_MinusLogProbMetric: 65.3160 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 124/1000
2023-10-27 23:26:37.286 
Epoch 124/1000 
	 loss: 64.3382, MinusLogProbMetric: 64.3382, val_loss: 64.7624, val_MinusLogProbMetric: 64.7624

Epoch 124: val_loss improved from 65.13036 to 64.76236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 45s - loss: 64.3382 - MinusLogProbMetric: 64.3382 - val_loss: 64.7624 - val_MinusLogProbMetric: 64.7624 - lr: 1.1111e-04 - 45s/epoch - 227ms/step
Epoch 125/1000
2023-10-27 23:27:21.517 
Epoch 125/1000 
	 loss: 64.1520, MinusLogProbMetric: 64.1520, val_loss: 64.6883, val_MinusLogProbMetric: 64.6883

Epoch 125: val_loss improved from 64.76236 to 64.68834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 64.1520 - MinusLogProbMetric: 64.1520 - val_loss: 64.6883 - val_MinusLogProbMetric: 64.6883 - lr: 1.1111e-04 - 44s/epoch - 226ms/step
Epoch 126/1000
2023-10-27 23:28:04.791 
Epoch 126/1000 
	 loss: 64.0339, MinusLogProbMetric: 64.0339, val_loss: 71.0924, val_MinusLogProbMetric: 71.0924

Epoch 126: val_loss did not improve from 64.68834
196/196 - 42s - loss: 64.0339 - MinusLogProbMetric: 64.0339 - val_loss: 71.0924 - val_MinusLogProbMetric: 71.0924 - lr: 1.1111e-04 - 42s/epoch - 217ms/step
Epoch 127/1000
2023-10-27 23:28:46.489 
Epoch 127/1000 
	 loss: 63.9343, MinusLogProbMetric: 63.9343, val_loss: 64.7763, val_MinusLogProbMetric: 64.7763

Epoch 127: val_loss did not improve from 64.68834
196/196 - 42s - loss: 63.9343 - MinusLogProbMetric: 63.9343 - val_loss: 64.7763 - val_MinusLogProbMetric: 64.7763 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 128/1000
2023-10-27 23:29:28.438 
Epoch 128/1000 
	 loss: 63.5745, MinusLogProbMetric: 63.5745, val_loss: 64.2313, val_MinusLogProbMetric: 64.2313

Epoch 128: val_loss improved from 64.68834 to 64.23127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 63.5745 - MinusLogProbMetric: 63.5745 - val_loss: 64.2313 - val_MinusLogProbMetric: 64.2313 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 129/1000
2023-10-27 23:30:12.175 
Epoch 129/1000 
	 loss: 63.4508, MinusLogProbMetric: 63.4508, val_loss: 64.0740, val_MinusLogProbMetric: 64.0740

Epoch 129: val_loss improved from 64.23127 to 64.07401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 63.4508 - MinusLogProbMetric: 63.4508 - val_loss: 64.0740 - val_MinusLogProbMetric: 64.0740 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 130/1000
2023-10-27 23:30:56.029 
Epoch 130/1000 
	 loss: 63.4562, MinusLogProbMetric: 63.4562, val_loss: 64.0385, val_MinusLogProbMetric: 64.0385

Epoch 130: val_loss improved from 64.07401 to 64.03848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 63.4562 - MinusLogProbMetric: 63.4562 - val_loss: 64.0385 - val_MinusLogProbMetric: 64.0385 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 131/1000
2023-10-27 23:31:40.337 
Epoch 131/1000 
	 loss: 63.1374, MinusLogProbMetric: 63.1374, val_loss: 63.6057, val_MinusLogProbMetric: 63.6057

Epoch 131: val_loss improved from 64.03848 to 63.60572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 63.1374 - MinusLogProbMetric: 63.1374 - val_loss: 63.6057 - val_MinusLogProbMetric: 63.6057 - lr: 1.1111e-04 - 44s/epoch - 227ms/step
Epoch 132/1000
2023-10-27 23:32:24.876 
Epoch 132/1000 
	 loss: 63.1735, MinusLogProbMetric: 63.1735, val_loss: 63.6051, val_MinusLogProbMetric: 63.6051

Epoch 132: val_loss improved from 63.60572 to 63.60515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 63.1735 - MinusLogProbMetric: 63.1735 - val_loss: 63.6051 - val_MinusLogProbMetric: 63.6051 - lr: 1.1111e-04 - 44s/epoch - 227ms/step
Epoch 133/1000
2023-10-27 23:33:09.474 
Epoch 133/1000 
	 loss: 62.8217, MinusLogProbMetric: 62.8217, val_loss: 63.4005, val_MinusLogProbMetric: 63.4005

Epoch 133: val_loss improved from 63.60515 to 63.40048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 45s - loss: 62.8217 - MinusLogProbMetric: 62.8217 - val_loss: 63.4005 - val_MinusLogProbMetric: 63.4005 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 134/1000
2023-10-27 23:33:53.628 
Epoch 134/1000 
	 loss: 62.7037, MinusLogProbMetric: 62.7037, val_loss: 63.8874, val_MinusLogProbMetric: 63.8874

Epoch 134: val_loss did not improve from 63.40048
196/196 - 43s - loss: 62.7037 - MinusLogProbMetric: 62.7037 - val_loss: 63.8874 - val_MinusLogProbMetric: 63.8874 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 135/1000
2023-10-27 23:34:37.331 
Epoch 135/1000 
	 loss: 62.5483, MinusLogProbMetric: 62.5483, val_loss: 63.0848, val_MinusLogProbMetric: 63.0848

Epoch 135: val_loss improved from 63.40048 to 63.08481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 62.5483 - MinusLogProbMetric: 62.5483 - val_loss: 63.0848 - val_MinusLogProbMetric: 63.0848 - lr: 1.1111e-04 - 44s/epoch - 227ms/step
Epoch 136/1000
2023-10-27 23:35:20.563 
Epoch 136/1000 
	 loss: 62.4657, MinusLogProbMetric: 62.4657, val_loss: 63.7671, val_MinusLogProbMetric: 63.7671

Epoch 136: val_loss did not improve from 63.08481
196/196 - 43s - loss: 62.4657 - MinusLogProbMetric: 62.4657 - val_loss: 63.7671 - val_MinusLogProbMetric: 63.7671 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 137/1000
2023-10-27 23:36:02.860 
Epoch 137/1000 
	 loss: 62.3601, MinusLogProbMetric: 62.3601, val_loss: 63.0707, val_MinusLogProbMetric: 63.0707

Epoch 137: val_loss improved from 63.08481 to 63.07071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 62.3601 - MinusLogProbMetric: 62.3601 - val_loss: 63.0707 - val_MinusLogProbMetric: 63.0707 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 138/1000
2023-10-27 23:36:47.100 
Epoch 138/1000 
	 loss: 62.1202, MinusLogProbMetric: 62.1202, val_loss: 62.5015, val_MinusLogProbMetric: 62.5015

Epoch 138: val_loss improved from 63.07071 to 62.50150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 62.1202 - MinusLogProbMetric: 62.1202 - val_loss: 62.5015 - val_MinusLogProbMetric: 62.5015 - lr: 1.1111e-04 - 44s/epoch - 226ms/step
Epoch 139/1000
2023-10-27 23:37:30.258 
Epoch 139/1000 
	 loss: 62.2349, MinusLogProbMetric: 62.2349, val_loss: 63.0891, val_MinusLogProbMetric: 63.0891

Epoch 139: val_loss did not improve from 62.50150
196/196 - 42s - loss: 62.2349 - MinusLogProbMetric: 62.2349 - val_loss: 63.0891 - val_MinusLogProbMetric: 63.0891 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 140/1000
2023-10-27 23:38:13.854 
Epoch 140/1000 
	 loss: 62.0325, MinusLogProbMetric: 62.0325, val_loss: 62.4404, val_MinusLogProbMetric: 62.4404

Epoch 140: val_loss improved from 62.50150 to 62.44035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 62.0325 - MinusLogProbMetric: 62.0325 - val_loss: 62.4404 - val_MinusLogProbMetric: 62.4404 - lr: 1.1111e-04 - 44s/epoch - 226ms/step
Epoch 141/1000
2023-10-27 23:38:57.260 
Epoch 141/1000 
	 loss: 61.7463, MinusLogProbMetric: 61.7463, val_loss: 62.5041, val_MinusLogProbMetric: 62.5041

Epoch 141: val_loss did not improve from 62.44035
196/196 - 43s - loss: 61.7463 - MinusLogProbMetric: 61.7463 - val_loss: 62.5041 - val_MinusLogProbMetric: 62.5041 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 142/1000
2023-10-27 23:39:40.646 
Epoch 142/1000 
	 loss: 61.6467, MinusLogProbMetric: 61.6467, val_loss: 62.4137, val_MinusLogProbMetric: 62.4137

Epoch 142: val_loss improved from 62.44035 to 62.41366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 61.6467 - MinusLogProbMetric: 61.6467 - val_loss: 62.4137 - val_MinusLogProbMetric: 62.4137 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 143/1000
2023-10-27 23:40:23.693 
Epoch 143/1000 
	 loss: 61.4358, MinusLogProbMetric: 61.4358, val_loss: 62.2621, val_MinusLogProbMetric: 62.2621

Epoch 143: val_loss improved from 62.41366 to 62.26214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 61.4358 - MinusLogProbMetric: 61.4358 - val_loss: 62.2621 - val_MinusLogProbMetric: 62.2621 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 144/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 79: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 23:40:43.630 
Epoch 144/1000 
	 loss: nan, MinusLogProbMetric: 73.9896, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 144: val_loss did not improve from 62.26214
196/196 - 19s - loss: nan - MinusLogProbMetric: 73.9896 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 19s/epoch - 98ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 428.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_428
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_470"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_471 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f36eb99b880>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34cd2f0940>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34cd2f0940>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3634c36f50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3604ddd180>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3604ddc3d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3604dde8f0>, <keras.callbacks.EarlyStopping object at 0x7f3604dde200>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3604dde020>, <keras.callbacks.TerminateOnNaN object at 0x7f3604dddcc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 428/720 with hyperparameters:
timestamp = 2023-10-27 23:40:50.449518
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
2023-10-27 23:42:48.062 
Epoch 1/1000 
	 loss: 89.8364, MinusLogProbMetric: 89.8364, val_loss: 63.9647, val_MinusLogProbMetric: 63.9647

Epoch 1: val_loss improved from inf to 63.96469, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 118s - loss: 89.8364 - MinusLogProbMetric: 89.8364 - val_loss: 63.9647 - val_MinusLogProbMetric: 63.9647 - lr: 3.7037e-05 - 118s/epoch - 602ms/step
Epoch 2/1000
2023-10-27 23:43:32.623 
Epoch 2/1000 
	 loss: 62.0782, MinusLogProbMetric: 62.0782, val_loss: 62.8953, val_MinusLogProbMetric: 62.8953

Epoch 2: val_loss improved from 63.96469 to 62.89531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 62.0782 - MinusLogProbMetric: 62.0782 - val_loss: 62.8953 - val_MinusLogProbMetric: 62.8953 - lr: 3.7037e-05 - 44s/epoch - 227ms/step
Epoch 3/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 149: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 23:44:07.385 
Epoch 3/1000 
	 loss: nan, MinusLogProbMetric: 76.5787, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 3: val_loss did not improve from 62.89531
196/196 - 34s - loss: nan - MinusLogProbMetric: 76.5787 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 34s/epoch - 174ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 428.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_428
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_476"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_477 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7f357df73df0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f362830fd90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f362830fd90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36b9864b80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f350570ffd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35057e8580>, <keras.callbacks.ModelCheckpoint object at 0x7f35057e8640>, <keras.callbacks.EarlyStopping object at 0x7f35057e88b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35057e88e0>, <keras.callbacks.TerminateOnNaN object at 0x7f35057e8520>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 428/720 with hyperparameters:
timestamp = 2023-10-27 23:44:13.984163
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
2023-10-27 23:46:25.460 
Epoch 1/1000 
	 loss: 70.4288, MinusLogProbMetric: 70.4288, val_loss: 61.1017, val_MinusLogProbMetric: 61.1017

Epoch 1: val_loss improved from inf to 61.10165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 132s - loss: 70.4288 - MinusLogProbMetric: 70.4288 - val_loss: 61.1017 - val_MinusLogProbMetric: 61.1017 - lr: 1.2346e-05 - 132s/epoch - 673ms/step
Epoch 2/1000
2023-10-27 23:47:08.784 
Epoch 2/1000 
	 loss: 60.7509, MinusLogProbMetric: 60.7509, val_loss: 61.2588, val_MinusLogProbMetric: 61.2588

Epoch 2: val_loss did not improve from 61.10165
196/196 - 42s - loss: 60.7509 - MinusLogProbMetric: 60.7509 - val_loss: 61.2588 - val_MinusLogProbMetric: 61.2588 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 3/1000
2023-10-27 23:47:53.273 
Epoch 3/1000 
	 loss: 59.8902, MinusLogProbMetric: 59.8902, val_loss: 60.3198, val_MinusLogProbMetric: 60.3198

Epoch 3: val_loss improved from 61.10165 to 60.31983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 45s - loss: 59.8902 - MinusLogProbMetric: 59.8902 - val_loss: 60.3198 - val_MinusLogProbMetric: 60.3198 - lr: 1.2346e-05 - 45s/epoch - 231ms/step
Epoch 4/1000
2023-10-27 23:48:35.966 
Epoch 4/1000 
	 loss: 59.3982, MinusLogProbMetric: 59.3982, val_loss: 60.1371, val_MinusLogProbMetric: 60.1371

Epoch 4: val_loss improved from 60.31983 to 60.13706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 59.3982 - MinusLogProbMetric: 59.3982 - val_loss: 60.1371 - val_MinusLogProbMetric: 60.1371 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 5/1000
2023-10-27 23:49:21.322 
Epoch 5/1000 
	 loss: 59.0762, MinusLogProbMetric: 59.0762, val_loss: 60.5131, val_MinusLogProbMetric: 60.5131

Epoch 5: val_loss did not improve from 60.13706
196/196 - 45s - loss: 59.0762 - MinusLogProbMetric: 59.0762 - val_loss: 60.5131 - val_MinusLogProbMetric: 60.5131 - lr: 1.2346e-05 - 45s/epoch - 227ms/step
Epoch 6/1000
2023-10-27 23:50:05.460 
Epoch 6/1000 
	 loss: 59.8213, MinusLogProbMetric: 59.8213, val_loss: 59.6543, val_MinusLogProbMetric: 59.6543

Epoch 6: val_loss improved from 60.13706 to 59.65427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 45s - loss: 59.8213 - MinusLogProbMetric: 59.8213 - val_loss: 59.6543 - val_MinusLogProbMetric: 59.6543 - lr: 1.2346e-05 - 45s/epoch - 229ms/step
Epoch 7/1000
2023-10-27 23:50:49.433 
Epoch 7/1000 
	 loss: 58.7284, MinusLogProbMetric: 58.7284, val_loss: 59.4247, val_MinusLogProbMetric: 59.4247

Epoch 7: val_loss improved from 59.65427 to 59.42471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 58.7284 - MinusLogProbMetric: 58.7284 - val_loss: 59.4247 - val_MinusLogProbMetric: 59.4247 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 8/1000
2023-10-27 23:51:32.858 
Epoch 8/1000 
	 loss: 58.4356, MinusLogProbMetric: 58.4356, val_loss: 58.8288, val_MinusLogProbMetric: 58.8288

Epoch 8: val_loss improved from 59.42471 to 58.82881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 58.4356 - MinusLogProbMetric: 58.4356 - val_loss: 58.8288 - val_MinusLogProbMetric: 58.8288 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 9/1000
2023-10-27 23:52:16.800 
Epoch 9/1000 
	 loss: 58.2329, MinusLogProbMetric: 58.2329, val_loss: 58.6192, val_MinusLogProbMetric: 58.6192

Epoch 9: val_loss improved from 58.82881 to 58.61917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 58.2329 - MinusLogProbMetric: 58.2329 - val_loss: 58.6192 - val_MinusLogProbMetric: 58.6192 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 10/1000
2023-10-27 23:53:01.859 
Epoch 10/1000 
	 loss: 59.4189, MinusLogProbMetric: 59.4189, val_loss: 61.0426, val_MinusLogProbMetric: 61.0426

Epoch 10: val_loss did not improve from 58.61917
196/196 - 44s - loss: 59.4189 - MinusLogProbMetric: 59.4189 - val_loss: 61.0426 - val_MinusLogProbMetric: 61.0426 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 11/1000
2023-10-27 23:53:45.085 
Epoch 11/1000 
	 loss: 63.0493, MinusLogProbMetric: 63.0493, val_loss: 59.9645, val_MinusLogProbMetric: 59.9645

Epoch 11: val_loss did not improve from 58.61917
196/196 - 43s - loss: 63.0493 - MinusLogProbMetric: 63.0493 - val_loss: 59.9645 - val_MinusLogProbMetric: 59.9645 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 12/1000
2023-10-27 23:54:27.745 
Epoch 12/1000 
	 loss: 58.7394, MinusLogProbMetric: 58.7394, val_loss: 59.2296, val_MinusLogProbMetric: 59.2296

Epoch 12: val_loss did not improve from 58.61917
196/196 - 43s - loss: 58.7394 - MinusLogProbMetric: 58.7394 - val_loss: 59.2296 - val_MinusLogProbMetric: 59.2296 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 13/1000
2023-10-27 23:55:10.749 
Epoch 13/1000 
	 loss: 58.1430, MinusLogProbMetric: 58.1430, val_loss: 59.0685, val_MinusLogProbMetric: 59.0685

Epoch 13: val_loss did not improve from 58.61917
196/196 - 43s - loss: 58.1430 - MinusLogProbMetric: 58.1430 - val_loss: 59.0685 - val_MinusLogProbMetric: 59.0685 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 14/1000
2023-10-27 23:55:53.887 
Epoch 14/1000 
	 loss: 57.7651, MinusLogProbMetric: 57.7651, val_loss: 58.0938, val_MinusLogProbMetric: 58.0938

Epoch 14: val_loss improved from 58.61917 to 58.09378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 57.7651 - MinusLogProbMetric: 57.7651 - val_loss: 58.0938 - val_MinusLogProbMetric: 58.0938 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 15/1000
2023-10-27 23:56:37.268 
Epoch 15/1000 
	 loss: 57.3777, MinusLogProbMetric: 57.3777, val_loss: 57.8093, val_MinusLogProbMetric: 57.8093

Epoch 15: val_loss improved from 58.09378 to 57.80933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 57.3777 - MinusLogProbMetric: 57.3777 - val_loss: 57.8093 - val_MinusLogProbMetric: 57.8093 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 16/1000
2023-10-27 23:57:20.653 
Epoch 16/1000 
	 loss: 57.1725, MinusLogProbMetric: 57.1725, val_loss: 58.0153, val_MinusLogProbMetric: 58.0153

Epoch 16: val_loss did not improve from 57.80933
196/196 - 43s - loss: 57.1725 - MinusLogProbMetric: 57.1725 - val_loss: 58.0153 - val_MinusLogProbMetric: 58.0153 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 17/1000
2023-10-27 23:58:03.365 
Epoch 17/1000 
	 loss: 57.0578, MinusLogProbMetric: 57.0578, val_loss: 58.2225, val_MinusLogProbMetric: 58.2225

Epoch 17: val_loss did not improve from 57.80933
196/196 - 43s - loss: 57.0578 - MinusLogProbMetric: 57.0578 - val_loss: 58.2225 - val_MinusLogProbMetric: 58.2225 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 18/1000
2023-10-27 23:58:47.795 
Epoch 18/1000 
	 loss: 56.7843, MinusLogProbMetric: 56.7843, val_loss: 57.7928, val_MinusLogProbMetric: 57.7928

Epoch 18: val_loss improved from 57.80933 to 57.79276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 45s - loss: 56.7843 - MinusLogProbMetric: 56.7843 - val_loss: 57.7928 - val_MinusLogProbMetric: 57.7928 - lr: 1.2346e-05 - 45s/epoch - 230ms/step
Epoch 19/1000
2023-10-27 23:59:31.264 
Epoch 19/1000 
	 loss: 56.5762, MinusLogProbMetric: 56.5762, val_loss: 56.9773, val_MinusLogProbMetric: 56.9773

Epoch 19: val_loss improved from 57.79276 to 56.97728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 56.5762 - MinusLogProbMetric: 56.5762 - val_loss: 56.9773 - val_MinusLogProbMetric: 56.9773 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 20/1000
2023-10-28 00:00:14.806 
Epoch 20/1000 
	 loss: 56.5542, MinusLogProbMetric: 56.5542, val_loss: 57.0836, val_MinusLogProbMetric: 57.0836

Epoch 20: val_loss did not improve from 56.97728
196/196 - 43s - loss: 56.5542 - MinusLogProbMetric: 56.5542 - val_loss: 57.0836 - val_MinusLogProbMetric: 57.0836 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 21/1000
2023-10-28 00:00:58.547 
Epoch 21/1000 
	 loss: 56.2381, MinusLogProbMetric: 56.2381, val_loss: 57.1339, val_MinusLogProbMetric: 57.1339

Epoch 21: val_loss did not improve from 56.97728
196/196 - 44s - loss: 56.2381 - MinusLogProbMetric: 56.2381 - val_loss: 57.1339 - val_MinusLogProbMetric: 57.1339 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 22/1000
2023-10-28 00:01:41.290 
Epoch 22/1000 
	 loss: 56.1693, MinusLogProbMetric: 56.1693, val_loss: 57.3658, val_MinusLogProbMetric: 57.3658

Epoch 22: val_loss did not improve from 56.97728
196/196 - 43s - loss: 56.1693 - MinusLogProbMetric: 56.1693 - val_loss: 57.3658 - val_MinusLogProbMetric: 57.3658 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 23/1000
2023-10-28 00:02:24.935 
Epoch 23/1000 
	 loss: 55.9601, MinusLogProbMetric: 55.9601, val_loss: 56.8740, val_MinusLogProbMetric: 56.8740

Epoch 23: val_loss improved from 56.97728 to 56.87396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 55.9601 - MinusLogProbMetric: 55.9601 - val_loss: 56.8740 - val_MinusLogProbMetric: 56.8740 - lr: 1.2346e-05 - 44s/epoch - 227ms/step
Epoch 24/1000
2023-10-28 00:03:09.204 
Epoch 24/1000 
	 loss: 55.8598, MinusLogProbMetric: 55.8598, val_loss: 56.3910, val_MinusLogProbMetric: 56.3910

Epoch 24: val_loss improved from 56.87396 to 56.39098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 55.8598 - MinusLogProbMetric: 55.8598 - val_loss: 56.3910 - val_MinusLogProbMetric: 56.3910 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 25/1000
2023-10-28 00:03:52.408 
Epoch 25/1000 
	 loss: 55.7378, MinusLogProbMetric: 55.7378, val_loss: 56.6721, val_MinusLogProbMetric: 56.6721

Epoch 25: val_loss did not improve from 56.39098
196/196 - 42s - loss: 55.7378 - MinusLogProbMetric: 55.7378 - val_loss: 56.6721 - val_MinusLogProbMetric: 56.6721 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 26/1000
2023-10-28 00:04:35.704 
Epoch 26/1000 
	 loss: 55.6221, MinusLogProbMetric: 55.6221, val_loss: 56.1752, val_MinusLogProbMetric: 56.1752

Epoch 26: val_loss improved from 56.39098 to 56.17523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 55.6221 - MinusLogProbMetric: 55.6221 - val_loss: 56.1752 - val_MinusLogProbMetric: 56.1752 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 27/1000
2023-10-28 00:05:19.798 
Epoch 27/1000 
	 loss: 55.6159, MinusLogProbMetric: 55.6159, val_loss: 56.5111, val_MinusLogProbMetric: 56.5111

Epoch 27: val_loss did not improve from 56.17523
196/196 - 43s - loss: 55.6159 - MinusLogProbMetric: 55.6159 - val_loss: 56.5111 - val_MinusLogProbMetric: 56.5111 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 28/1000
2023-10-28 00:06:02.776 
Epoch 28/1000 
	 loss: 55.8611, MinusLogProbMetric: 55.8611, val_loss: 56.0258, val_MinusLogProbMetric: 56.0258

Epoch 28: val_loss improved from 56.17523 to 56.02575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 55.8611 - MinusLogProbMetric: 55.8611 - val_loss: 56.0258 - val_MinusLogProbMetric: 56.0258 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 29/1000
2023-10-28 00:06:47.111 
Epoch 29/1000 
	 loss: 55.4617, MinusLogProbMetric: 55.4617, val_loss: 55.9578, val_MinusLogProbMetric: 55.9578

Epoch 29: val_loss improved from 56.02575 to 55.95778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 55.4617 - MinusLogProbMetric: 55.4617 - val_loss: 55.9578 - val_MinusLogProbMetric: 55.9578 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 30/1000
2023-10-28 00:07:30.688 
Epoch 30/1000 
	 loss: 55.1939, MinusLogProbMetric: 55.1939, val_loss: 56.0589, val_MinusLogProbMetric: 56.0589

Epoch 30: val_loss did not improve from 55.95778
196/196 - 43s - loss: 55.1939 - MinusLogProbMetric: 55.1939 - val_loss: 56.0589 - val_MinusLogProbMetric: 56.0589 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 31/1000
2023-10-28 00:08:13.046 
Epoch 31/1000 
	 loss: 55.0223, MinusLogProbMetric: 55.0223, val_loss: 55.5123, val_MinusLogProbMetric: 55.5123

Epoch 31: val_loss improved from 55.95778 to 55.51235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 55.0223 - MinusLogProbMetric: 55.0223 - val_loss: 55.5123 - val_MinusLogProbMetric: 55.5123 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 32/1000
2023-10-28 00:08:57.248 
Epoch 32/1000 
	 loss: 54.8055, MinusLogProbMetric: 54.8055, val_loss: 55.3646, val_MinusLogProbMetric: 55.3646

Epoch 32: val_loss improved from 55.51235 to 55.36455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 54.8055 - MinusLogProbMetric: 54.8055 - val_loss: 55.3646 - val_MinusLogProbMetric: 55.3646 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 33/1000
2023-10-28 00:09:41.288 
Epoch 33/1000 
	 loss: 54.7100, MinusLogProbMetric: 54.7100, val_loss: 55.6067, val_MinusLogProbMetric: 55.6067

Epoch 33: val_loss did not improve from 55.36455
196/196 - 43s - loss: 54.7100 - MinusLogProbMetric: 54.7100 - val_loss: 55.6067 - val_MinusLogProbMetric: 55.6067 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 34/1000
2023-10-28 00:10:24.268 
Epoch 34/1000 
	 loss: 54.8638, MinusLogProbMetric: 54.8638, val_loss: 55.3339, val_MinusLogProbMetric: 55.3339

Epoch 34: val_loss improved from 55.36455 to 55.33389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 54.8638 - MinusLogProbMetric: 54.8638 - val_loss: 55.3339 - val_MinusLogProbMetric: 55.3339 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 35/1000
2023-10-28 00:11:08.267 
Epoch 35/1000 
	 loss: 54.5025, MinusLogProbMetric: 54.5025, val_loss: 55.0153, val_MinusLogProbMetric: 55.0153

Epoch 35: val_loss improved from 55.33389 to 55.01527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 54.5025 - MinusLogProbMetric: 54.5025 - val_loss: 55.0153 - val_MinusLogProbMetric: 55.0153 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 36/1000
2023-10-28 00:11:51.342 
Epoch 36/1000 
	 loss: 54.3431, MinusLogProbMetric: 54.3431, val_loss: 55.9188, val_MinusLogProbMetric: 55.9188

Epoch 36: val_loss did not improve from 55.01527
196/196 - 42s - loss: 54.3431 - MinusLogProbMetric: 54.3431 - val_loss: 55.9188 - val_MinusLogProbMetric: 55.9188 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 37/1000
2023-10-28 00:12:33.220 
Epoch 37/1000 
	 loss: 54.3726, MinusLogProbMetric: 54.3726, val_loss: 54.8678, val_MinusLogProbMetric: 54.8678

Epoch 37: val_loss improved from 55.01527 to 54.86782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 54.3726 - MinusLogProbMetric: 54.3726 - val_loss: 54.8678 - val_MinusLogProbMetric: 54.8678 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 38/1000
2023-10-28 00:13:17.816 
Epoch 38/1000 
	 loss: 54.0663, MinusLogProbMetric: 54.0663, val_loss: 54.8903, val_MinusLogProbMetric: 54.8903

Epoch 38: val_loss did not improve from 54.86782
196/196 - 44s - loss: 54.0663 - MinusLogProbMetric: 54.0663 - val_loss: 54.8903 - val_MinusLogProbMetric: 54.8903 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 39/1000
2023-10-28 00:14:00.246 
Epoch 39/1000 
	 loss: 54.0341, MinusLogProbMetric: 54.0341, val_loss: 54.8014, val_MinusLogProbMetric: 54.8014

Epoch 39: val_loss improved from 54.86782 to 54.80143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 54.0341 - MinusLogProbMetric: 54.0341 - val_loss: 54.8014 - val_MinusLogProbMetric: 54.8014 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 40/1000
2023-10-28 00:14:43.511 
Epoch 40/1000 
	 loss: 54.0349, MinusLogProbMetric: 54.0349, val_loss: 54.5545, val_MinusLogProbMetric: 54.5545

Epoch 40: val_loss improved from 54.80143 to 54.55445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 54.0349 - MinusLogProbMetric: 54.0349 - val_loss: 54.5545 - val_MinusLogProbMetric: 54.5545 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 41/1000
2023-10-28 00:15:25.921 
Epoch 41/1000 
	 loss: 53.8421, MinusLogProbMetric: 53.8421, val_loss: 55.6894, val_MinusLogProbMetric: 55.6894

Epoch 41: val_loss did not improve from 54.55445
196/196 - 42s - loss: 53.8421 - MinusLogProbMetric: 53.8421 - val_loss: 55.6894 - val_MinusLogProbMetric: 55.6894 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 42/1000
2023-10-28 00:16:08.885 
Epoch 42/1000 
	 loss: 53.9246, MinusLogProbMetric: 53.9246, val_loss: 54.4594, val_MinusLogProbMetric: 54.4594

Epoch 42: val_loss improved from 54.55445 to 54.45935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 53.9246 - MinusLogProbMetric: 53.9246 - val_loss: 54.4594 - val_MinusLogProbMetric: 54.4594 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 43/1000
2023-10-28 00:16:53.429 
Epoch 43/1000 
	 loss: 53.6654, MinusLogProbMetric: 53.6654, val_loss: 55.2377, val_MinusLogProbMetric: 55.2377

Epoch 43: val_loss did not improve from 54.45935
196/196 - 44s - loss: 53.6654 - MinusLogProbMetric: 53.6654 - val_loss: 55.2377 - val_MinusLogProbMetric: 55.2377 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 44/1000
2023-10-28 00:17:36.747 
Epoch 44/1000 
	 loss: 53.6254, MinusLogProbMetric: 53.6254, val_loss: 54.1831, val_MinusLogProbMetric: 54.1831

Epoch 44: val_loss improved from 54.45935 to 54.18309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 53.6254 - MinusLogProbMetric: 53.6254 - val_loss: 54.1831 - val_MinusLogProbMetric: 54.1831 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 45/1000
2023-10-28 00:18:21.207 
Epoch 45/1000 
	 loss: 53.4706, MinusLogProbMetric: 53.4706, val_loss: 54.4568, val_MinusLogProbMetric: 54.4568

Epoch 45: val_loss did not improve from 54.18309
196/196 - 44s - loss: 53.4706 - MinusLogProbMetric: 53.4706 - val_loss: 54.4568 - val_MinusLogProbMetric: 54.4568 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 46/1000
2023-10-28 00:19:03.617 
Epoch 46/1000 
	 loss: 53.5248, MinusLogProbMetric: 53.5248, val_loss: 53.9232, val_MinusLogProbMetric: 53.9232

Epoch 46: val_loss improved from 54.18309 to 53.92320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 53.5248 - MinusLogProbMetric: 53.5248 - val_loss: 53.9232 - val_MinusLogProbMetric: 53.9232 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 47/1000
2023-10-28 00:19:47.426 
Epoch 47/1000 
	 loss: 53.3233, MinusLogProbMetric: 53.3233, val_loss: 54.1111, val_MinusLogProbMetric: 54.1111

Epoch 47: val_loss did not improve from 53.92320
196/196 - 43s - loss: 53.3233 - MinusLogProbMetric: 53.3233 - val_loss: 54.1111 - val_MinusLogProbMetric: 54.1111 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 48/1000
2023-10-28 00:20:28.082 
Epoch 48/1000 
	 loss: 53.2724, MinusLogProbMetric: 53.2724, val_loss: 53.9905, val_MinusLogProbMetric: 53.9905

Epoch 48: val_loss did not improve from 53.92320
196/196 - 41s - loss: 53.2724 - MinusLogProbMetric: 53.2724 - val_loss: 53.9905 - val_MinusLogProbMetric: 53.9905 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 49/1000
2023-10-28 00:21:11.287 
Epoch 49/1000 
	 loss: 53.0747, MinusLogProbMetric: 53.0747, val_loss: 53.7691, val_MinusLogProbMetric: 53.7691

Epoch 49: val_loss improved from 53.92320 to 53.76914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 53.0747 - MinusLogProbMetric: 53.0747 - val_loss: 53.7691 - val_MinusLogProbMetric: 53.7691 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 50/1000
2023-10-28 00:21:54.848 
Epoch 50/1000 
	 loss: 53.0840, MinusLogProbMetric: 53.0840, val_loss: 53.7651, val_MinusLogProbMetric: 53.7651

Epoch 50: val_loss improved from 53.76914 to 53.76515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 53.0840 - MinusLogProbMetric: 53.0840 - val_loss: 53.7651 - val_MinusLogProbMetric: 53.7651 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 51/1000
2023-10-28 00:22:37.893 
Epoch 51/1000 
	 loss: 53.0004, MinusLogProbMetric: 53.0004, val_loss: 54.0304, val_MinusLogProbMetric: 54.0304

Epoch 51: val_loss did not improve from 53.76515
196/196 - 42s - loss: 53.0004 - MinusLogProbMetric: 53.0004 - val_loss: 54.0304 - val_MinusLogProbMetric: 54.0304 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 52/1000
2023-10-28 00:23:21.452 
Epoch 52/1000 
	 loss: 52.9437, MinusLogProbMetric: 52.9437, val_loss: 53.7655, val_MinusLogProbMetric: 53.7655

Epoch 52: val_loss did not improve from 53.76515
196/196 - 44s - loss: 52.9437 - MinusLogProbMetric: 52.9437 - val_loss: 53.7655 - val_MinusLogProbMetric: 53.7655 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 53/1000
2023-10-28 00:24:04.973 
Epoch 53/1000 
	 loss: 53.0141, MinusLogProbMetric: 53.0141, val_loss: 53.6566, val_MinusLogProbMetric: 53.6566

Epoch 53: val_loss improved from 53.76515 to 53.65660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 53.0141 - MinusLogProbMetric: 53.0141 - val_loss: 53.6566 - val_MinusLogProbMetric: 53.6566 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 54/1000
2023-10-28 00:24:49.042 
Epoch 54/1000 
	 loss: 52.8715, MinusLogProbMetric: 52.8715, val_loss: 53.4340, val_MinusLogProbMetric: 53.4340

Epoch 54: val_loss improved from 53.65660 to 53.43399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 52.8715 - MinusLogProbMetric: 52.8715 - val_loss: 53.4340 - val_MinusLogProbMetric: 53.4340 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 55/1000
2023-10-28 00:25:33.253 
Epoch 55/1000 
	 loss: 52.6947, MinusLogProbMetric: 52.6947, val_loss: 53.4929, val_MinusLogProbMetric: 53.4929

Epoch 55: val_loss did not improve from 53.43399
196/196 - 43s - loss: 52.6947 - MinusLogProbMetric: 52.6947 - val_loss: 53.4929 - val_MinusLogProbMetric: 53.4929 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 56/1000
2023-10-28 00:26:16.005 
Epoch 56/1000 
	 loss: 52.7070, MinusLogProbMetric: 52.7070, val_loss: 53.6220, val_MinusLogProbMetric: 53.6220

Epoch 56: val_loss did not improve from 53.43399
196/196 - 43s - loss: 52.7070 - MinusLogProbMetric: 52.7070 - val_loss: 53.6220 - val_MinusLogProbMetric: 53.6220 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 57/1000
2023-10-28 00:26:58.501 
Epoch 57/1000 
	 loss: 52.5192, MinusLogProbMetric: 52.5192, val_loss: 53.4986, val_MinusLogProbMetric: 53.4986

Epoch 57: val_loss did not improve from 53.43399
196/196 - 42s - loss: 52.5192 - MinusLogProbMetric: 52.5192 - val_loss: 53.4986 - val_MinusLogProbMetric: 53.4986 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 58/1000
2023-10-28 00:27:41.170 
Epoch 58/1000 
	 loss: 52.5141, MinusLogProbMetric: 52.5141, val_loss: 53.1053, val_MinusLogProbMetric: 53.1053

Epoch 58: val_loss improved from 53.43399 to 53.10529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 52.5141 - MinusLogProbMetric: 52.5141 - val_loss: 53.1053 - val_MinusLogProbMetric: 53.1053 - lr: 1.2346e-05 - 43s/epoch - 222ms/step
Epoch 59/1000
2023-10-28 00:28:25.739 
Epoch 59/1000 
	 loss: 52.3549, MinusLogProbMetric: 52.3549, val_loss: 53.5201, val_MinusLogProbMetric: 53.5201

Epoch 59: val_loss did not improve from 53.10529
196/196 - 44s - loss: 52.3549 - MinusLogProbMetric: 52.3549 - val_loss: 53.5201 - val_MinusLogProbMetric: 53.5201 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 60/1000
2023-10-28 00:29:08.935 
Epoch 60/1000 
	 loss: 52.3842, MinusLogProbMetric: 52.3842, val_loss: 53.3380, val_MinusLogProbMetric: 53.3380

Epoch 60: val_loss did not improve from 53.10529
196/196 - 43s - loss: 52.3842 - MinusLogProbMetric: 52.3842 - val_loss: 53.3380 - val_MinusLogProbMetric: 53.3380 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 61/1000
2023-10-28 00:29:52.152 
Epoch 61/1000 
	 loss: 52.3020, MinusLogProbMetric: 52.3020, val_loss: 52.8398, val_MinusLogProbMetric: 52.8398

Epoch 61: val_loss improved from 53.10529 to 52.83983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 52.3020 - MinusLogProbMetric: 52.3020 - val_loss: 52.8398 - val_MinusLogProbMetric: 52.8398 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 62/1000
2023-10-28 00:30:35.398 
Epoch 62/1000 
	 loss: 52.2832, MinusLogProbMetric: 52.2832, val_loss: 53.6766, val_MinusLogProbMetric: 53.6766

Epoch 62: val_loss did not improve from 52.83983
196/196 - 43s - loss: 52.2832 - MinusLogProbMetric: 52.2832 - val_loss: 53.6766 - val_MinusLogProbMetric: 53.6766 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 63/1000
2023-10-28 00:31:18.579 
Epoch 63/1000 
	 loss: 52.3570, MinusLogProbMetric: 52.3570, val_loss: 52.7140, val_MinusLogProbMetric: 52.7140

Epoch 63: val_loss improved from 52.83983 to 52.71398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 52.3570 - MinusLogProbMetric: 52.3570 - val_loss: 52.7140 - val_MinusLogProbMetric: 52.7140 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 64/1000
2023-10-28 00:32:01.817 
Epoch 64/1000 
	 loss: 52.6785, MinusLogProbMetric: 52.6785, val_loss: 52.8564, val_MinusLogProbMetric: 52.8564

Epoch 64: val_loss did not improve from 52.71398
196/196 - 42s - loss: 52.6785 - MinusLogProbMetric: 52.6785 - val_loss: 52.8564 - val_MinusLogProbMetric: 52.8564 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 65/1000
2023-10-28 00:32:44.260 
Epoch 65/1000 
	 loss: 52.0219, MinusLogProbMetric: 52.0219, val_loss: 52.7405, val_MinusLogProbMetric: 52.7405

Epoch 65: val_loss did not improve from 52.71398
196/196 - 42s - loss: 52.0219 - MinusLogProbMetric: 52.0219 - val_loss: 52.7405 - val_MinusLogProbMetric: 52.7405 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 66/1000
2023-10-28 00:33:26.833 
Epoch 66/1000 
	 loss: 51.9874, MinusLogProbMetric: 51.9874, val_loss: 52.5629, val_MinusLogProbMetric: 52.5629

Epoch 66: val_loss improved from 52.71398 to 52.56293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 51.9874 - MinusLogProbMetric: 51.9874 - val_loss: 52.5629 - val_MinusLogProbMetric: 52.5629 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 67/1000
2023-10-28 00:34:10.389 
Epoch 67/1000 
	 loss: 52.1570, MinusLogProbMetric: 52.1570, val_loss: 52.7131, val_MinusLogProbMetric: 52.7131

Epoch 67: val_loss did not improve from 52.56293
196/196 - 43s - loss: 52.1570 - MinusLogProbMetric: 52.1570 - val_loss: 52.7131 - val_MinusLogProbMetric: 52.7131 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 68/1000
2023-10-28 00:34:53.214 
Epoch 68/1000 
	 loss: 51.8844, MinusLogProbMetric: 51.8844, val_loss: 52.5301, val_MinusLogProbMetric: 52.5301

Epoch 68: val_loss improved from 52.56293 to 52.53008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 51.8844 - MinusLogProbMetric: 51.8844 - val_loss: 52.5301 - val_MinusLogProbMetric: 52.5301 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 69/1000
2023-10-28 00:35:36.711 
Epoch 69/1000 
	 loss: 51.8230, MinusLogProbMetric: 51.8230, val_loss: 52.5600, val_MinusLogProbMetric: 52.5600

Epoch 69: val_loss did not improve from 52.53008
196/196 - 43s - loss: 51.8230 - MinusLogProbMetric: 51.8230 - val_loss: 52.5600 - val_MinusLogProbMetric: 52.5600 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 70/1000
2023-10-28 00:36:18.986 
Epoch 70/1000 
	 loss: 51.7957, MinusLogProbMetric: 51.7957, val_loss: 52.3824, val_MinusLogProbMetric: 52.3824

Epoch 70: val_loss improved from 52.53008 to 52.38239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 51.7957 - MinusLogProbMetric: 51.7957 - val_loss: 52.3824 - val_MinusLogProbMetric: 52.3824 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 71/1000
2023-10-28 00:37:01.621 
Epoch 71/1000 
	 loss: 51.5496, MinusLogProbMetric: 51.5496, val_loss: 52.1113, val_MinusLogProbMetric: 52.1113

Epoch 71: val_loss improved from 52.38239 to 52.11126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 51.5496 - MinusLogProbMetric: 51.5496 - val_loss: 52.1113 - val_MinusLogProbMetric: 52.1113 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 72/1000
2023-10-28 00:37:45.271 
Epoch 72/1000 
	 loss: 51.8514, MinusLogProbMetric: 51.8514, val_loss: 52.3067, val_MinusLogProbMetric: 52.3067

Epoch 72: val_loss did not improve from 52.11126
196/196 - 43s - loss: 51.8514 - MinusLogProbMetric: 51.8514 - val_loss: 52.3067 - val_MinusLogProbMetric: 52.3067 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 73/1000
2023-10-28 00:38:28.235 
Epoch 73/1000 
	 loss: 51.5925, MinusLogProbMetric: 51.5925, val_loss: 53.1547, val_MinusLogProbMetric: 53.1547

Epoch 73: val_loss did not improve from 52.11126
196/196 - 43s - loss: 51.5925 - MinusLogProbMetric: 51.5925 - val_loss: 53.1547 - val_MinusLogProbMetric: 53.1547 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 74/1000
2023-10-28 00:39:11.980 
Epoch 74/1000 
	 loss: 51.5527, MinusLogProbMetric: 51.5527, val_loss: 52.5150, val_MinusLogProbMetric: 52.5150

Epoch 74: val_loss did not improve from 52.11126
196/196 - 44s - loss: 51.5527 - MinusLogProbMetric: 51.5527 - val_loss: 52.5150 - val_MinusLogProbMetric: 52.5150 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 75/1000
2023-10-28 00:39:53.831 
Epoch 75/1000 
	 loss: 51.3969, MinusLogProbMetric: 51.3969, val_loss: 52.6142, val_MinusLogProbMetric: 52.6142

Epoch 75: val_loss did not improve from 52.11126
196/196 - 42s - loss: 51.3969 - MinusLogProbMetric: 51.3969 - val_loss: 52.6142 - val_MinusLogProbMetric: 52.6142 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 76/1000
2023-10-28 00:40:36.997 
Epoch 76/1000 
	 loss: 51.3147, MinusLogProbMetric: 51.3147, val_loss: 51.9811, val_MinusLogProbMetric: 51.9811

Epoch 76: val_loss improved from 52.11126 to 51.98114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 51.3147 - MinusLogProbMetric: 51.3147 - val_loss: 51.9811 - val_MinusLogProbMetric: 51.9811 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 77/1000
2023-10-28 00:41:20.807 
Epoch 77/1000 
	 loss: 51.6521, MinusLogProbMetric: 51.6521, val_loss: 52.1143, val_MinusLogProbMetric: 52.1143

Epoch 77: val_loss did not improve from 51.98114
196/196 - 43s - loss: 51.6521 - MinusLogProbMetric: 51.6521 - val_loss: 52.1143 - val_MinusLogProbMetric: 52.1143 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 78/1000
2023-10-28 00:42:04.114 
Epoch 78/1000 
	 loss: 51.1676, MinusLogProbMetric: 51.1676, val_loss: 52.2673, val_MinusLogProbMetric: 52.2673

Epoch 78: val_loss did not improve from 51.98114
196/196 - 43s - loss: 51.1676 - MinusLogProbMetric: 51.1676 - val_loss: 52.2673 - val_MinusLogProbMetric: 52.2673 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 79/1000
2023-10-28 00:42:47.037 
Epoch 79/1000 
	 loss: 51.2048, MinusLogProbMetric: 51.2048, val_loss: 53.4156, val_MinusLogProbMetric: 53.4156

Epoch 79: val_loss did not improve from 51.98114
196/196 - 43s - loss: 51.2048 - MinusLogProbMetric: 51.2048 - val_loss: 53.4156 - val_MinusLogProbMetric: 53.4156 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 80/1000
2023-10-28 00:43:30.223 
Epoch 80/1000 
	 loss: 51.1040, MinusLogProbMetric: 51.1040, val_loss: 51.8639, val_MinusLogProbMetric: 51.8639

Epoch 80: val_loss improved from 51.98114 to 51.86389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 51.1040 - MinusLogProbMetric: 51.1040 - val_loss: 51.8639 - val_MinusLogProbMetric: 51.8639 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 81/1000
2023-10-28 00:44:14.563 
Epoch 81/1000 
	 loss: 51.1368, MinusLogProbMetric: 51.1368, val_loss: 51.7458, val_MinusLogProbMetric: 51.7458

Epoch 81: val_loss improved from 51.86389 to 51.74580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 51.1368 - MinusLogProbMetric: 51.1368 - val_loss: 51.7458 - val_MinusLogProbMetric: 51.7458 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 82/1000
2023-10-28 00:44:57.998 
Epoch 82/1000 
	 loss: 50.9683, MinusLogProbMetric: 50.9683, val_loss: 51.6603, val_MinusLogProbMetric: 51.6603

Epoch 82: val_loss improved from 51.74580 to 51.66026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 50.9683 - MinusLogProbMetric: 50.9683 - val_loss: 51.6603 - val_MinusLogProbMetric: 51.6603 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 83/1000
2023-10-28 00:45:42.349 
Epoch 83/1000 
	 loss: 50.9667, MinusLogProbMetric: 50.9667, val_loss: 51.5643, val_MinusLogProbMetric: 51.5643

Epoch 83: val_loss improved from 51.66026 to 51.56431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 50.9667 - MinusLogProbMetric: 50.9667 - val_loss: 51.5643 - val_MinusLogProbMetric: 51.5643 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 84/1000
2023-10-28 00:46:25.790 
Epoch 84/1000 
	 loss: 51.0050, MinusLogProbMetric: 51.0050, val_loss: 51.5925, val_MinusLogProbMetric: 51.5925

Epoch 84: val_loss did not improve from 51.56431
196/196 - 43s - loss: 51.0050 - MinusLogProbMetric: 51.0050 - val_loss: 51.5925 - val_MinusLogProbMetric: 51.5925 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 85/1000
2023-10-28 00:47:08.188 
Epoch 85/1000 
	 loss: 50.9600, MinusLogProbMetric: 50.9600, val_loss: 51.6778, val_MinusLogProbMetric: 51.6778

Epoch 85: val_loss did not improve from 51.56431
196/196 - 42s - loss: 50.9600 - MinusLogProbMetric: 50.9600 - val_loss: 51.6778 - val_MinusLogProbMetric: 51.6778 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 86/1000
2023-10-28 00:47:51.250 
Epoch 86/1000 
	 loss: 50.8510, MinusLogProbMetric: 50.8510, val_loss: 51.7876, val_MinusLogProbMetric: 51.7876

Epoch 86: val_loss did not improve from 51.56431
196/196 - 43s - loss: 50.8510 - MinusLogProbMetric: 50.8510 - val_loss: 51.7876 - val_MinusLogProbMetric: 51.7876 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 87/1000
2023-10-28 00:48:34.483 
Epoch 87/1000 
	 loss: 50.6979, MinusLogProbMetric: 50.6979, val_loss: 52.8427, val_MinusLogProbMetric: 52.8427

Epoch 87: val_loss did not improve from 51.56431
196/196 - 43s - loss: 50.6979 - MinusLogProbMetric: 50.6979 - val_loss: 52.8427 - val_MinusLogProbMetric: 52.8427 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 88/1000
2023-10-28 00:49:18.139 
Epoch 88/1000 
	 loss: 50.8452, MinusLogProbMetric: 50.8452, val_loss: 51.8287, val_MinusLogProbMetric: 51.8287

Epoch 88: val_loss did not improve from 51.56431
196/196 - 44s - loss: 50.8452 - MinusLogProbMetric: 50.8452 - val_loss: 51.8287 - val_MinusLogProbMetric: 51.8287 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 89/1000
2023-10-28 00:49:59.846 
Epoch 89/1000 
	 loss: 50.7652, MinusLogProbMetric: 50.7652, val_loss: 51.5265, val_MinusLogProbMetric: 51.5265

Epoch 89: val_loss improved from 51.56431 to 51.52650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 50.7652 - MinusLogProbMetric: 50.7652 - val_loss: 51.5265 - val_MinusLogProbMetric: 51.5265 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 90/1000
2023-10-28 00:50:43.433 
Epoch 90/1000 
	 loss: 50.7184, MinusLogProbMetric: 50.7184, val_loss: 51.6632, val_MinusLogProbMetric: 51.6632

Epoch 90: val_loss did not improve from 51.52650
196/196 - 43s - loss: 50.7184 - MinusLogProbMetric: 50.7184 - val_loss: 51.6632 - val_MinusLogProbMetric: 51.6632 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 91/1000
2023-10-28 00:51:26.357 
Epoch 91/1000 
	 loss: 50.5852, MinusLogProbMetric: 50.5852, val_loss: 52.1555, val_MinusLogProbMetric: 52.1555

Epoch 91: val_loss did not improve from 51.52650
196/196 - 43s - loss: 50.5852 - MinusLogProbMetric: 50.5852 - val_loss: 52.1555 - val_MinusLogProbMetric: 52.1555 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 92/1000
2023-10-28 00:52:09.066 
Epoch 92/1000 
	 loss: 50.9902, MinusLogProbMetric: 50.9902, val_loss: 51.0423, val_MinusLogProbMetric: 51.0423

Epoch 92: val_loss improved from 51.52650 to 51.04235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 50.9902 - MinusLogProbMetric: 50.9902 - val_loss: 51.0423 - val_MinusLogProbMetric: 51.0423 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 93/1000
2023-10-28 00:52:51.923 
Epoch 93/1000 
	 loss: 50.5548, MinusLogProbMetric: 50.5548, val_loss: 51.6709, val_MinusLogProbMetric: 51.6709

Epoch 93: val_loss did not improve from 51.04235
196/196 - 42s - loss: 50.5548 - MinusLogProbMetric: 50.5548 - val_loss: 51.6709 - val_MinusLogProbMetric: 51.6709 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 94/1000
2023-10-28 00:53:35.696 
Epoch 94/1000 
	 loss: 50.4433, MinusLogProbMetric: 50.4433, val_loss: 51.5742, val_MinusLogProbMetric: 51.5742

Epoch 94: val_loss did not improve from 51.04235
196/196 - 44s - loss: 50.4433 - MinusLogProbMetric: 50.4433 - val_loss: 51.5742 - val_MinusLogProbMetric: 51.5742 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 95/1000
2023-10-28 00:54:18.970 
Epoch 95/1000 
	 loss: 50.4095, MinusLogProbMetric: 50.4095, val_loss: 51.5704, val_MinusLogProbMetric: 51.5704

Epoch 95: val_loss did not improve from 51.04235
196/196 - 43s - loss: 50.4095 - MinusLogProbMetric: 50.4095 - val_loss: 51.5704 - val_MinusLogProbMetric: 51.5704 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 96/1000
2023-10-28 00:55:01.141 
Epoch 96/1000 
	 loss: 50.4269, MinusLogProbMetric: 50.4269, val_loss: 51.2841, val_MinusLogProbMetric: 51.2841

Epoch 96: val_loss did not improve from 51.04235
196/196 - 42s - loss: 50.4269 - MinusLogProbMetric: 50.4269 - val_loss: 51.2841 - val_MinusLogProbMetric: 51.2841 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 97/1000
2023-10-28 00:55:44.002 
Epoch 97/1000 
	 loss: 50.2473, MinusLogProbMetric: 50.2473, val_loss: 51.0040, val_MinusLogProbMetric: 51.0040

Epoch 97: val_loss improved from 51.04235 to 51.00402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 50.2473 - MinusLogProbMetric: 50.2473 - val_loss: 51.0040 - val_MinusLogProbMetric: 51.0040 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 98/1000
2023-10-28 00:56:28.240 
Epoch 98/1000 
	 loss: 50.4833, MinusLogProbMetric: 50.4833, val_loss: 51.1870, val_MinusLogProbMetric: 51.1870

Epoch 98: val_loss did not improve from 51.00402
196/196 - 43s - loss: 50.4833 - MinusLogProbMetric: 50.4833 - val_loss: 51.1870 - val_MinusLogProbMetric: 51.1870 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 99/1000
2023-10-28 00:57:10.877 
Epoch 99/1000 
	 loss: 50.4911, MinusLogProbMetric: 50.4911, val_loss: 51.3740, val_MinusLogProbMetric: 51.3740

Epoch 99: val_loss did not improve from 51.00402
196/196 - 43s - loss: 50.4911 - MinusLogProbMetric: 50.4911 - val_loss: 51.3740 - val_MinusLogProbMetric: 51.3740 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 100/1000
2023-10-28 00:57:53.728 
Epoch 100/1000 
	 loss: 50.4054, MinusLogProbMetric: 50.4054, val_loss: 51.4307, val_MinusLogProbMetric: 51.4307

Epoch 100: val_loss did not improve from 51.00402
196/196 - 43s - loss: 50.4054 - MinusLogProbMetric: 50.4054 - val_loss: 51.4307 - val_MinusLogProbMetric: 51.4307 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 101/1000
2023-10-28 00:58:31.223 
Epoch 101/1000 
	 loss: 50.3406, MinusLogProbMetric: 50.3406, val_loss: 51.2000, val_MinusLogProbMetric: 51.2000

Epoch 101: val_loss did not improve from 51.00402
196/196 - 37s - loss: 50.3406 - MinusLogProbMetric: 50.3406 - val_loss: 51.2000 - val_MinusLogProbMetric: 51.2000 - lr: 1.2346e-05 - 37s/epoch - 191ms/step
Epoch 102/1000
2023-10-28 00:59:05.643 
Epoch 102/1000 
	 loss: 50.1163, MinusLogProbMetric: 50.1163, val_loss: 50.8904, val_MinusLogProbMetric: 50.8904

Epoch 102: val_loss improved from 51.00402 to 50.89041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 50.1163 - MinusLogProbMetric: 50.1163 - val_loss: 50.8904 - val_MinusLogProbMetric: 50.8904 - lr: 1.2346e-05 - 35s/epoch - 179ms/step
Epoch 103/1000
2023-10-28 00:59:40.937 
Epoch 103/1000 
	 loss: 50.0543, MinusLogProbMetric: 50.0543, val_loss: 51.3332, val_MinusLogProbMetric: 51.3332

Epoch 103: val_loss did not improve from 50.89041
196/196 - 35s - loss: 50.0543 - MinusLogProbMetric: 50.0543 - val_loss: 51.3332 - val_MinusLogProbMetric: 51.3332 - lr: 1.2346e-05 - 35s/epoch - 177ms/step
Epoch 104/1000
2023-10-28 01:00:17.244 
Epoch 104/1000 
	 loss: 50.0942, MinusLogProbMetric: 50.0942, val_loss: 50.9071, val_MinusLogProbMetric: 50.9071

Epoch 104: val_loss did not improve from 50.89041
196/196 - 36s - loss: 50.0942 - MinusLogProbMetric: 50.0942 - val_loss: 50.9071 - val_MinusLogProbMetric: 50.9071 - lr: 1.2346e-05 - 36s/epoch - 185ms/step
Epoch 105/1000
2023-10-28 01:00:53.877 
Epoch 105/1000 
	 loss: 51.7601, MinusLogProbMetric: 51.7601, val_loss: 70.4801, val_MinusLogProbMetric: 70.4801

Epoch 105: val_loss did not improve from 50.89041
196/196 - 37s - loss: 51.7601 - MinusLogProbMetric: 51.7601 - val_loss: 70.4801 - val_MinusLogProbMetric: 70.4801 - lr: 1.2346e-05 - 37s/epoch - 187ms/step
Epoch 106/1000
2023-10-28 01:01:28.199 
Epoch 106/1000 
	 loss: 52.0170, MinusLogProbMetric: 52.0170, val_loss: 51.3637, val_MinusLogProbMetric: 51.3637

Epoch 106: val_loss did not improve from 50.89041
196/196 - 34s - loss: 52.0170 - MinusLogProbMetric: 52.0170 - val_loss: 51.3637 - val_MinusLogProbMetric: 51.3637 - lr: 1.2346e-05 - 34s/epoch - 175ms/step
Epoch 107/1000
2023-10-28 01:02:02.166 
Epoch 107/1000 
	 loss: 50.2717, MinusLogProbMetric: 50.2717, val_loss: 50.9041, val_MinusLogProbMetric: 50.9041

Epoch 107: val_loss did not improve from 50.89041
196/196 - 34s - loss: 50.2717 - MinusLogProbMetric: 50.2717 - val_loss: 50.9041 - val_MinusLogProbMetric: 50.9041 - lr: 1.2346e-05 - 34s/epoch - 173ms/step
Epoch 108/1000
2023-10-28 01:02:37.840 
Epoch 108/1000 
	 loss: 50.0916, MinusLogProbMetric: 50.0916, val_loss: 50.9535, val_MinusLogProbMetric: 50.9535

Epoch 108: val_loss did not improve from 50.89041
196/196 - 36s - loss: 50.0916 - MinusLogProbMetric: 50.0916 - val_loss: 50.9535 - val_MinusLogProbMetric: 50.9535 - lr: 1.2346e-05 - 36s/epoch - 182ms/step
Epoch 109/1000
2023-10-28 01:03:16.342 
Epoch 109/1000 
	 loss: 49.9618, MinusLogProbMetric: 49.9618, val_loss: 50.9644, val_MinusLogProbMetric: 50.9644

Epoch 109: val_loss did not improve from 50.89041
196/196 - 38s - loss: 49.9618 - MinusLogProbMetric: 49.9618 - val_loss: 50.9644 - val_MinusLogProbMetric: 50.9644 - lr: 1.2346e-05 - 38s/epoch - 196ms/step
Epoch 110/1000
2023-10-28 01:03:50.018 
Epoch 110/1000 
	 loss: 49.9019, MinusLogProbMetric: 49.9019, val_loss: 50.6055, val_MinusLogProbMetric: 50.6055

Epoch 110: val_loss improved from 50.89041 to 50.60549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 34s - loss: 49.9019 - MinusLogProbMetric: 49.9019 - val_loss: 50.6055 - val_MinusLogProbMetric: 50.6055 - lr: 1.2346e-05 - 34s/epoch - 175ms/step
Epoch 111/1000
2023-10-28 01:04:24.516 
Epoch 111/1000 
	 loss: 50.2709, MinusLogProbMetric: 50.2709, val_loss: 50.7104, val_MinusLogProbMetric: 50.7104

Epoch 111: val_loss did not improve from 50.60549
196/196 - 34s - loss: 50.2709 - MinusLogProbMetric: 50.2709 - val_loss: 50.7104 - val_MinusLogProbMetric: 50.7104 - lr: 1.2346e-05 - 34s/epoch - 173ms/step
Epoch 112/1000
2023-10-28 01:04:59.967 
Epoch 112/1000 
	 loss: 49.7033, MinusLogProbMetric: 49.7033, val_loss: 50.8178, val_MinusLogProbMetric: 50.8178

Epoch 112: val_loss did not improve from 50.60549
196/196 - 35s - loss: 49.7033 - MinusLogProbMetric: 49.7033 - val_loss: 50.8178 - val_MinusLogProbMetric: 50.8178 - lr: 1.2346e-05 - 35s/epoch - 181ms/step
Epoch 113/1000
2023-10-28 01:05:38.175 
Epoch 113/1000 
	 loss: 49.6419, MinusLogProbMetric: 49.6419, val_loss: 50.5034, val_MinusLogProbMetric: 50.5034

Epoch 113: val_loss improved from 50.60549 to 50.50343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 39s - loss: 49.6419 - MinusLogProbMetric: 49.6419 - val_loss: 50.5034 - val_MinusLogProbMetric: 50.5034 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 114/1000
2023-10-28 01:06:15.415 
Epoch 114/1000 
	 loss: 49.6287, MinusLogProbMetric: 49.6287, val_loss: 50.4346, val_MinusLogProbMetric: 50.4346

Epoch 114: val_loss improved from 50.50343 to 50.43464, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 37s - loss: 49.6287 - MinusLogProbMetric: 49.6287 - val_loss: 50.4346 - val_MinusLogProbMetric: 50.4346 - lr: 1.2346e-05 - 37s/epoch - 189ms/step
Epoch 115/1000
2023-10-28 01:06:49.523 
Epoch 115/1000 
	 loss: 49.6648, MinusLogProbMetric: 49.6648, val_loss: 50.4673, val_MinusLogProbMetric: 50.4673

Epoch 115: val_loss did not improve from 50.43464
196/196 - 33s - loss: 49.6648 - MinusLogProbMetric: 49.6648 - val_loss: 50.4673 - val_MinusLogProbMetric: 50.4673 - lr: 1.2346e-05 - 33s/epoch - 171ms/step
Epoch 116/1000
2023-10-28 01:07:22.704 
Epoch 116/1000 
	 loss: 49.5268, MinusLogProbMetric: 49.5268, val_loss: 50.8076, val_MinusLogProbMetric: 50.8076

Epoch 116: val_loss did not improve from 50.43464
196/196 - 33s - loss: 49.5268 - MinusLogProbMetric: 49.5268 - val_loss: 50.8076 - val_MinusLogProbMetric: 50.8076 - lr: 1.2346e-05 - 33s/epoch - 169ms/step
Epoch 117/1000
2023-10-28 01:08:00.961 
Epoch 117/1000 
	 loss: 49.7149, MinusLogProbMetric: 49.7149, val_loss: 51.1346, val_MinusLogProbMetric: 51.1346

Epoch 117: val_loss did not improve from 50.43464
196/196 - 38s - loss: 49.7149 - MinusLogProbMetric: 49.7149 - val_loss: 51.1346 - val_MinusLogProbMetric: 51.1346 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 118/1000
2023-10-28 01:08:38.316 
Epoch 118/1000 
	 loss: 49.3389, MinusLogProbMetric: 49.3389, val_loss: 50.2569, val_MinusLogProbMetric: 50.2569

Epoch 118: val_loss improved from 50.43464 to 50.25690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 49.3389 - MinusLogProbMetric: 49.3389 - val_loss: 50.2569 - val_MinusLogProbMetric: 50.2569 - lr: 1.2346e-05 - 38s/epoch - 194ms/step
Epoch 119/1000
2023-10-28 01:09:12.385 
Epoch 119/1000 
	 loss: 49.2512, MinusLogProbMetric: 49.2512, val_loss: 50.0268, val_MinusLogProbMetric: 50.0268

Epoch 119: val_loss improved from 50.25690 to 50.02679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 34s - loss: 49.2512 - MinusLogProbMetric: 49.2512 - val_loss: 50.0268 - val_MinusLogProbMetric: 50.0268 - lr: 1.2346e-05 - 34s/epoch - 174ms/step
Epoch 120/1000
2023-10-28 01:09:46.853 
Epoch 120/1000 
	 loss: 49.3559, MinusLogProbMetric: 49.3559, val_loss: 50.1611, val_MinusLogProbMetric: 50.1611

Epoch 120: val_loss did not improve from 50.02679
196/196 - 34s - loss: 49.3559 - MinusLogProbMetric: 49.3559 - val_loss: 50.1611 - val_MinusLogProbMetric: 50.1611 - lr: 1.2346e-05 - 34s/epoch - 173ms/step
Epoch 121/1000
2023-10-28 01:10:21.304 
Epoch 121/1000 
	 loss: 49.2874, MinusLogProbMetric: 49.2874, val_loss: 50.5528, val_MinusLogProbMetric: 50.5528

Epoch 121: val_loss did not improve from 50.02679
196/196 - 34s - loss: 49.2874 - MinusLogProbMetric: 49.2874 - val_loss: 50.5528 - val_MinusLogProbMetric: 50.5528 - lr: 1.2346e-05 - 34s/epoch - 176ms/step
Epoch 122/1000
2023-10-28 01:10:59.876 
Epoch 122/1000 
	 loss: 49.2392, MinusLogProbMetric: 49.2392, val_loss: 50.1263, val_MinusLogProbMetric: 50.1263

Epoch 122: val_loss did not improve from 50.02679
196/196 - 39s - loss: 49.2392 - MinusLogProbMetric: 49.2392 - val_loss: 50.1263 - val_MinusLogProbMetric: 50.1263 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 123/1000
2023-10-28 01:11:33.716 
Epoch 123/1000 
	 loss: 49.1424, MinusLogProbMetric: 49.1424, val_loss: 50.8552, val_MinusLogProbMetric: 50.8552

Epoch 123: val_loss did not improve from 50.02679
196/196 - 34s - loss: 49.1424 - MinusLogProbMetric: 49.1424 - val_loss: 50.8552 - val_MinusLogProbMetric: 50.8552 - lr: 1.2346e-05 - 34s/epoch - 173ms/step
Epoch 124/1000
2023-10-28 01:12:07.790 
Epoch 124/1000 
	 loss: 49.2144, MinusLogProbMetric: 49.2144, val_loss: 49.8792, val_MinusLogProbMetric: 49.8792

Epoch 124: val_loss improved from 50.02679 to 49.87924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 49.2144 - MinusLogProbMetric: 49.2144 - val_loss: 49.8792 - val_MinusLogProbMetric: 49.8792 - lr: 1.2346e-05 - 35s/epoch - 177ms/step
Epoch 125/1000
2023-10-28 01:12:43.133 
Epoch 125/1000 
	 loss: 49.2727, MinusLogProbMetric: 49.2727, val_loss: 50.0188, val_MinusLogProbMetric: 50.0188

Epoch 125: val_loss did not improve from 49.87924
196/196 - 35s - loss: 49.2727 - MinusLogProbMetric: 49.2727 - val_loss: 50.0188 - val_MinusLogProbMetric: 50.0188 - lr: 1.2346e-05 - 35s/epoch - 177ms/step
Epoch 126/1000
2023-10-28 01:13:20.513 
Epoch 126/1000 
	 loss: 49.1928, MinusLogProbMetric: 49.1928, val_loss: 51.1440, val_MinusLogProbMetric: 51.1440

Epoch 126: val_loss did not improve from 49.87924
196/196 - 37s - loss: 49.1928 - MinusLogProbMetric: 49.1928 - val_loss: 51.1440 - val_MinusLogProbMetric: 51.1440 - lr: 1.2346e-05 - 37s/epoch - 191ms/step
Epoch 127/1000
2023-10-28 01:13:57.425 
Epoch 127/1000 
	 loss: 49.3018, MinusLogProbMetric: 49.3018, val_loss: 50.9213, val_MinusLogProbMetric: 50.9213

Epoch 127: val_loss did not improve from 49.87924
196/196 - 37s - loss: 49.3018 - MinusLogProbMetric: 49.3018 - val_loss: 50.9213 - val_MinusLogProbMetric: 50.9213 - lr: 1.2346e-05 - 37s/epoch - 188ms/step
Epoch 128/1000
2023-10-28 01:14:30.964 
Epoch 128/1000 
	 loss: 49.1979, MinusLogProbMetric: 49.1979, val_loss: 49.8000, val_MinusLogProbMetric: 49.8000

Epoch 128: val_loss improved from 49.87924 to 49.79996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 34s - loss: 49.1979 - MinusLogProbMetric: 49.1979 - val_loss: 49.8000 - val_MinusLogProbMetric: 49.8000 - lr: 1.2346e-05 - 34s/epoch - 174ms/step
Epoch 129/1000
2023-10-28 01:15:05.015 
Epoch 129/1000 
	 loss: 48.9113, MinusLogProbMetric: 48.9113, val_loss: 50.0812, val_MinusLogProbMetric: 50.0812

Epoch 129: val_loss did not improve from 49.79996
196/196 - 33s - loss: 48.9113 - MinusLogProbMetric: 48.9113 - val_loss: 50.0812 - val_MinusLogProbMetric: 50.0812 - lr: 1.2346e-05 - 33s/epoch - 171ms/step
Epoch 130/1000
2023-10-28 01:15:41.701 
Epoch 130/1000 
	 loss: 49.0163, MinusLogProbMetric: 49.0163, val_loss: 50.8460, val_MinusLogProbMetric: 50.8460

Epoch 130: val_loss did not improve from 49.79996
196/196 - 37s - loss: 49.0163 - MinusLogProbMetric: 49.0163 - val_loss: 50.8460 - val_MinusLogProbMetric: 50.8460 - lr: 1.2346e-05 - 37s/epoch - 187ms/step
Epoch 131/1000
2023-10-28 01:16:18.959 
Epoch 131/1000 
	 loss: 49.0761, MinusLogProbMetric: 49.0761, val_loss: 50.0023, val_MinusLogProbMetric: 50.0023

Epoch 131: val_loss did not improve from 49.79996
196/196 - 37s - loss: 49.0761 - MinusLogProbMetric: 49.0761 - val_loss: 50.0023 - val_MinusLogProbMetric: 50.0023 - lr: 1.2346e-05 - 37s/epoch - 190ms/step
Epoch 132/1000
2023-10-28 01:16:52.525 
Epoch 132/1000 
	 loss: 49.4908, MinusLogProbMetric: 49.4908, val_loss: 49.8155, val_MinusLogProbMetric: 49.8155

Epoch 132: val_loss did not improve from 49.79996
196/196 - 34s - loss: 49.4908 - MinusLogProbMetric: 49.4908 - val_loss: 49.8155 - val_MinusLogProbMetric: 49.8155 - lr: 1.2346e-05 - 34s/epoch - 171ms/step
Epoch 133/1000
2023-10-28 01:17:25.755 
Epoch 133/1000 
	 loss: 49.4892, MinusLogProbMetric: 49.4892, val_loss: 49.9810, val_MinusLogProbMetric: 49.9810

Epoch 133: val_loss did not improve from 49.79996
196/196 - 33s - loss: 49.4892 - MinusLogProbMetric: 49.4892 - val_loss: 49.9810 - val_MinusLogProbMetric: 49.9810 - lr: 1.2346e-05 - 33s/epoch - 170ms/step
Epoch 134/1000
2023-10-28 01:17:59.133 
Epoch 134/1000 
	 loss: 48.7710, MinusLogProbMetric: 48.7710, val_loss: 50.8416, val_MinusLogProbMetric: 50.8416

Epoch 134: val_loss did not improve from 49.79996
196/196 - 33s - loss: 48.7710 - MinusLogProbMetric: 48.7710 - val_loss: 50.8416 - val_MinusLogProbMetric: 50.8416 - lr: 1.2346e-05 - 33s/epoch - 170ms/step
Epoch 135/1000
2023-10-28 01:18:33.912 
Epoch 135/1000 
	 loss: 48.9882, MinusLogProbMetric: 48.9882, val_loss: 49.6730, val_MinusLogProbMetric: 49.6730

Epoch 135: val_loss improved from 49.79996 to 49.67299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 48.9882 - MinusLogProbMetric: 48.9882 - val_loss: 49.6730 - val_MinusLogProbMetric: 49.6730 - lr: 1.2346e-05 - 35s/epoch - 181ms/step
Epoch 136/1000
2023-10-28 01:19:11.378 
Epoch 136/1000 
	 loss: 48.9515, MinusLogProbMetric: 48.9515, val_loss: 50.2796, val_MinusLogProbMetric: 50.2796

Epoch 136: val_loss did not improve from 49.67299
196/196 - 37s - loss: 48.9515 - MinusLogProbMetric: 48.9515 - val_loss: 50.2796 - val_MinusLogProbMetric: 50.2796 - lr: 1.2346e-05 - 37s/epoch - 188ms/step
Epoch 137/1000
2023-10-28 01:19:43.830 
Epoch 137/1000 
	 loss: 49.0143, MinusLogProbMetric: 49.0143, val_loss: 50.5731, val_MinusLogProbMetric: 50.5731

Epoch 137: val_loss did not improve from 49.67299
196/196 - 32s - loss: 49.0143 - MinusLogProbMetric: 49.0143 - val_loss: 50.5731 - val_MinusLogProbMetric: 50.5731 - lr: 1.2346e-05 - 32s/epoch - 166ms/step
Epoch 138/1000
2023-10-28 01:20:16.366 
Epoch 138/1000 
	 loss: 48.7562, MinusLogProbMetric: 48.7562, val_loss: 49.6426, val_MinusLogProbMetric: 49.6426

Epoch 138: val_loss improved from 49.67299 to 49.64259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 33s - loss: 48.7562 - MinusLogProbMetric: 48.7562 - val_loss: 49.6426 - val_MinusLogProbMetric: 49.6426 - lr: 1.2346e-05 - 33s/epoch - 169ms/step
Epoch 139/1000
2023-10-28 01:20:52.717 
Epoch 139/1000 
	 loss: 48.8968, MinusLogProbMetric: 48.8968, val_loss: 49.8879, val_MinusLogProbMetric: 49.8879

Epoch 139: val_loss did not improve from 49.64259
196/196 - 36s - loss: 48.8968 - MinusLogProbMetric: 48.8968 - val_loss: 49.8879 - val_MinusLogProbMetric: 49.8879 - lr: 1.2346e-05 - 36s/epoch - 182ms/step
Epoch 140/1000
2023-10-28 01:21:28.815 
Epoch 140/1000 
	 loss: 48.6601, MinusLogProbMetric: 48.6601, val_loss: 49.6783, val_MinusLogProbMetric: 49.6783

Epoch 140: val_loss did not improve from 49.64259
196/196 - 36s - loss: 48.6601 - MinusLogProbMetric: 48.6601 - val_loss: 49.6783 - val_MinusLogProbMetric: 49.6783 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 141/1000
2023-10-28 01:22:02.628 
Epoch 141/1000 
	 loss: 48.8293, MinusLogProbMetric: 48.8293, val_loss: 49.4347, val_MinusLogProbMetric: 49.4347

Epoch 141: val_loss improved from 49.64259 to 49.43470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 34s - loss: 48.8293 - MinusLogProbMetric: 48.8293 - val_loss: 49.4347 - val_MinusLogProbMetric: 49.4347 - lr: 1.2346e-05 - 34s/epoch - 176ms/step
Epoch 142/1000
2023-10-28 01:22:35.843 
Epoch 142/1000 
	 loss: 48.7269, MinusLogProbMetric: 48.7269, val_loss: 49.6409, val_MinusLogProbMetric: 49.6409

Epoch 142: val_loss did not improve from 49.43470
196/196 - 33s - loss: 48.7269 - MinusLogProbMetric: 48.7269 - val_loss: 49.6409 - val_MinusLogProbMetric: 49.6409 - lr: 1.2346e-05 - 33s/epoch - 166ms/step
Epoch 143/1000
2023-10-28 01:23:07.992 
Epoch 143/1000 
	 loss: 48.6814, MinusLogProbMetric: 48.6814, val_loss: 49.4899, val_MinusLogProbMetric: 49.4899

Epoch 143: val_loss did not improve from 49.43470
196/196 - 32s - loss: 48.6814 - MinusLogProbMetric: 48.6814 - val_loss: 49.4899 - val_MinusLogProbMetric: 49.4899 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 144/1000
2023-10-28 01:23:42.397 
Epoch 144/1000 
	 loss: 48.7030, MinusLogProbMetric: 48.7030, val_loss: 49.5814, val_MinusLogProbMetric: 49.5814

Epoch 144: val_loss did not improve from 49.43470
196/196 - 34s - loss: 48.7030 - MinusLogProbMetric: 48.7030 - val_loss: 49.5814 - val_MinusLogProbMetric: 49.5814 - lr: 1.2346e-05 - 34s/epoch - 176ms/step
Epoch 145/1000
2023-10-28 01:24:19.998 
Epoch 145/1000 
	 loss: 48.5308, MinusLogProbMetric: 48.5308, val_loss: 49.9116, val_MinusLogProbMetric: 49.9116

Epoch 145: val_loss did not improve from 49.43470
196/196 - 38s - loss: 48.5308 - MinusLogProbMetric: 48.5308 - val_loss: 49.9116 - val_MinusLogProbMetric: 49.9116 - lr: 1.2346e-05 - 38s/epoch - 192ms/step
Epoch 146/1000
2023-10-28 01:24:53.559 
Epoch 146/1000 
	 loss: 48.5095, MinusLogProbMetric: 48.5095, val_loss: 49.4331, val_MinusLogProbMetric: 49.4331

Epoch 146: val_loss improved from 49.43470 to 49.43314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 34s - loss: 48.5095 - MinusLogProbMetric: 48.5095 - val_loss: 49.4331 - val_MinusLogProbMetric: 49.4331 - lr: 1.2346e-05 - 34s/epoch - 174ms/step
Epoch 147/1000
2023-10-28 01:25:26.728 
Epoch 147/1000 
	 loss: 48.6220, MinusLogProbMetric: 48.6220, val_loss: 49.6728, val_MinusLogProbMetric: 49.6728

Epoch 147: val_loss did not improve from 49.43314
196/196 - 33s - loss: 48.6220 - MinusLogProbMetric: 48.6220 - val_loss: 49.6728 - val_MinusLogProbMetric: 49.6728 - lr: 1.2346e-05 - 33s/epoch - 166ms/step
Epoch 148/1000
2023-10-28 01:25:59.651 
Epoch 148/1000 
	 loss: 48.6529, MinusLogProbMetric: 48.6529, val_loss: 49.1032, val_MinusLogProbMetric: 49.1032

Epoch 148: val_loss improved from 49.43314 to 49.10319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 34s - loss: 48.6529 - MinusLogProbMetric: 48.6529 - val_loss: 49.1032 - val_MinusLogProbMetric: 49.1032 - lr: 1.2346e-05 - 34s/epoch - 171ms/step
Epoch 149/1000
2023-10-28 01:26:35.256 
Epoch 149/1000 
	 loss: 48.4810, MinusLogProbMetric: 48.4810, val_loss: 49.1517, val_MinusLogProbMetric: 49.1517

Epoch 149: val_loss did not improve from 49.10319
196/196 - 35s - loss: 48.4810 - MinusLogProbMetric: 48.4810 - val_loss: 49.1517 - val_MinusLogProbMetric: 49.1517 - lr: 1.2346e-05 - 35s/epoch - 178ms/step
Epoch 150/1000
2023-10-28 01:27:14.277 
Epoch 150/1000 
	 loss: 48.5002, MinusLogProbMetric: 48.5002, val_loss: 49.7840, val_MinusLogProbMetric: 49.7840

Epoch 150: val_loss did not improve from 49.10319
196/196 - 39s - loss: 48.5002 - MinusLogProbMetric: 48.5002 - val_loss: 49.7840 - val_MinusLogProbMetric: 49.7840 - lr: 1.2346e-05 - 39s/epoch - 199ms/step
Epoch 151/1000
2023-10-28 01:27:45.747 
Epoch 151/1000 
	 loss: 48.4516, MinusLogProbMetric: 48.4516, val_loss: 49.1038, val_MinusLogProbMetric: 49.1038

Epoch 151: val_loss did not improve from 49.10319
196/196 - 31s - loss: 48.4516 - MinusLogProbMetric: 48.4516 - val_loss: 49.1038 - val_MinusLogProbMetric: 49.1038 - lr: 1.2346e-05 - 31s/epoch - 161ms/step
Epoch 152/1000
2023-10-28 01:28:17.444 
Epoch 152/1000 
	 loss: 48.3491, MinusLogProbMetric: 48.3491, val_loss: 49.0398, val_MinusLogProbMetric: 49.0398

Epoch 152: val_loss improved from 49.10319 to 49.03984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 32s - loss: 48.3491 - MinusLogProbMetric: 48.3491 - val_loss: 49.0398 - val_MinusLogProbMetric: 49.0398 - lr: 1.2346e-05 - 32s/epoch - 165ms/step
Epoch 153/1000
2023-10-28 01:28:50.117 
Epoch 153/1000 
	 loss: 48.6311, MinusLogProbMetric: 48.6311, val_loss: 49.5029, val_MinusLogProbMetric: 49.5029

Epoch 153: val_loss did not improve from 49.03984
196/196 - 32s - loss: 48.6311 - MinusLogProbMetric: 48.6311 - val_loss: 49.5029 - val_MinusLogProbMetric: 49.5029 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 154/1000
2023-10-28 01:29:26.868 
Epoch 154/1000 
	 loss: 48.2866, MinusLogProbMetric: 48.2866, val_loss: 49.3194, val_MinusLogProbMetric: 49.3194

Epoch 154: val_loss did not improve from 49.03984
196/196 - 37s - loss: 48.2866 - MinusLogProbMetric: 48.2866 - val_loss: 49.3194 - val_MinusLogProbMetric: 49.3194 - lr: 1.2346e-05 - 37s/epoch - 187ms/step
Epoch 155/1000
2023-10-28 01:30:00.974 
Epoch 155/1000 
	 loss: 48.3108, MinusLogProbMetric: 48.3108, val_loss: 50.3982, val_MinusLogProbMetric: 50.3982

Epoch 155: val_loss did not improve from 49.03984
196/196 - 34s - loss: 48.3108 - MinusLogProbMetric: 48.3108 - val_loss: 50.3982 - val_MinusLogProbMetric: 50.3982 - lr: 1.2346e-05 - 34s/epoch - 174ms/step
Epoch 156/1000
2023-10-28 01:30:32.853 
Epoch 156/1000 
	 loss: 49.1799, MinusLogProbMetric: 49.1799, val_loss: 48.9178, val_MinusLogProbMetric: 48.9178

Epoch 156: val_loss improved from 49.03984 to 48.91777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 32s - loss: 49.1799 - MinusLogProbMetric: 49.1799 - val_loss: 48.9178 - val_MinusLogProbMetric: 48.9178 - lr: 1.2346e-05 - 32s/epoch - 166ms/step
Epoch 157/1000
2023-10-28 01:31:04.906 
Epoch 157/1000 
	 loss: 48.3187, MinusLogProbMetric: 48.3187, val_loss: 50.3478, val_MinusLogProbMetric: 50.3478

Epoch 157: val_loss did not improve from 48.91777
196/196 - 31s - loss: 48.3187 - MinusLogProbMetric: 48.3187 - val_loss: 50.3478 - val_MinusLogProbMetric: 50.3478 - lr: 1.2346e-05 - 31s/epoch - 160ms/step
Epoch 158/1000
2023-10-28 01:31:37.033 
Epoch 158/1000 
	 loss: 48.2825, MinusLogProbMetric: 48.2825, val_loss: 50.5191, val_MinusLogProbMetric: 50.5191

Epoch 158: val_loss did not improve from 48.91777
196/196 - 32s - loss: 48.2825 - MinusLogProbMetric: 48.2825 - val_loss: 50.5191 - val_MinusLogProbMetric: 50.5191 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 159/1000
2023-10-28 01:32:10.952 
Epoch 159/1000 
	 loss: 48.2347, MinusLogProbMetric: 48.2347, val_loss: 49.0784, val_MinusLogProbMetric: 49.0784

Epoch 159: val_loss did not improve from 48.91777
196/196 - 34s - loss: 48.2347 - MinusLogProbMetric: 48.2347 - val_loss: 49.0784 - val_MinusLogProbMetric: 49.0784 - lr: 1.2346e-05 - 34s/epoch - 173ms/step
Epoch 160/1000
2023-10-28 01:32:45.211 
Epoch 160/1000 
	 loss: 48.1844, MinusLogProbMetric: 48.1844, val_loss: 49.0844, val_MinusLogProbMetric: 49.0844

Epoch 160: val_loss did not improve from 48.91777
196/196 - 34s - loss: 48.1844 - MinusLogProbMetric: 48.1844 - val_loss: 49.0844 - val_MinusLogProbMetric: 49.0844 - lr: 1.2346e-05 - 34s/epoch - 175ms/step
Epoch 161/1000
2023-10-28 01:33:17.439 
Epoch 161/1000 
	 loss: 48.1280, MinusLogProbMetric: 48.1280, val_loss: 49.5179, val_MinusLogProbMetric: 49.5179

Epoch 161: val_loss did not improve from 48.91777
196/196 - 32s - loss: 48.1280 - MinusLogProbMetric: 48.1280 - val_loss: 49.5179 - val_MinusLogProbMetric: 49.5179 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 162/1000
2023-10-28 01:33:49.089 
Epoch 162/1000 
	 loss: 48.2741, MinusLogProbMetric: 48.2741, val_loss: 49.1796, val_MinusLogProbMetric: 49.1796

Epoch 162: val_loss did not improve from 48.91777
196/196 - 32s - loss: 48.2741 - MinusLogProbMetric: 48.2741 - val_loss: 49.1796 - val_MinusLogProbMetric: 49.1796 - lr: 1.2346e-05 - 32s/epoch - 161ms/step
Epoch 163/1000
2023-10-28 01:34:21.487 
Epoch 163/1000 
	 loss: 48.2890, MinusLogProbMetric: 48.2890, val_loss: 49.4252, val_MinusLogProbMetric: 49.4252

Epoch 163: val_loss did not improve from 48.91777
196/196 - 32s - loss: 48.2890 - MinusLogProbMetric: 48.2890 - val_loss: 49.4252 - val_MinusLogProbMetric: 49.4252 - lr: 1.2346e-05 - 32s/epoch - 165ms/step
Epoch 164/1000
2023-10-28 01:34:55.705 
Epoch 164/1000 
	 loss: 48.2214, MinusLogProbMetric: 48.2214, val_loss: 50.2753, val_MinusLogProbMetric: 50.2753

Epoch 164: val_loss did not improve from 48.91777
196/196 - 34s - loss: 48.2214 - MinusLogProbMetric: 48.2214 - val_loss: 50.2753 - val_MinusLogProbMetric: 50.2753 - lr: 1.2346e-05 - 34s/epoch - 175ms/step
Epoch 165/1000
2023-10-28 01:35:28.841 
Epoch 165/1000 
	 loss: 48.2209, MinusLogProbMetric: 48.2209, val_loss: 49.0927, val_MinusLogProbMetric: 49.0927

Epoch 165: val_loss did not improve from 48.91777
196/196 - 33s - loss: 48.2209 - MinusLogProbMetric: 48.2209 - val_loss: 49.0927 - val_MinusLogProbMetric: 49.0927 - lr: 1.2346e-05 - 33s/epoch - 169ms/step
Epoch 166/1000
2023-10-28 01:36:00.974 
Epoch 166/1000 
	 loss: 48.2480, MinusLogProbMetric: 48.2480, val_loss: 49.3039, val_MinusLogProbMetric: 49.3039

Epoch 166: val_loss did not improve from 48.91777
196/196 - 32s - loss: 48.2480 - MinusLogProbMetric: 48.2480 - val_loss: 49.3039 - val_MinusLogProbMetric: 49.3039 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 167/1000
2023-10-28 01:36:32.280 
Epoch 167/1000 
	 loss: 48.1192, MinusLogProbMetric: 48.1192, val_loss: 50.0990, val_MinusLogProbMetric: 50.0990

Epoch 167: val_loss did not improve from 48.91777
196/196 - 31s - loss: 48.1192 - MinusLogProbMetric: 48.1192 - val_loss: 50.0990 - val_MinusLogProbMetric: 50.0990 - lr: 1.2346e-05 - 31s/epoch - 160ms/step
Epoch 168/1000
2023-10-28 01:37:04.186 
Epoch 168/1000 
	 loss: 48.0291, MinusLogProbMetric: 48.0291, val_loss: 49.2523, val_MinusLogProbMetric: 49.2523

Epoch 168: val_loss did not improve from 48.91777
196/196 - 32s - loss: 48.0291 - MinusLogProbMetric: 48.0291 - val_loss: 49.2523 - val_MinusLogProbMetric: 49.2523 - lr: 1.2346e-05 - 32s/epoch - 163ms/step
Epoch 169/1000
2023-10-28 01:37:38.151 
Epoch 169/1000 
	 loss: 48.9920, MinusLogProbMetric: 48.9920, val_loss: 48.8105, val_MinusLogProbMetric: 48.8105

Epoch 169: val_loss improved from 48.91777 to 48.81049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 48.9920 - MinusLogProbMetric: 48.9920 - val_loss: 48.8105 - val_MinusLogProbMetric: 48.8105 - lr: 1.2346e-05 - 35s/epoch - 176ms/step
Epoch 170/1000
2023-10-28 01:38:12.261 
Epoch 170/1000 
	 loss: 48.0093, MinusLogProbMetric: 48.0093, val_loss: 49.0828, val_MinusLogProbMetric: 49.0828

Epoch 170: val_loss did not improve from 48.81049
196/196 - 34s - loss: 48.0093 - MinusLogProbMetric: 48.0093 - val_loss: 49.0828 - val_MinusLogProbMetric: 49.0828 - lr: 1.2346e-05 - 34s/epoch - 171ms/step
Epoch 171/1000
2023-10-28 01:38:44.122 
Epoch 171/1000 
	 loss: 47.8562, MinusLogProbMetric: 47.8562, val_loss: 48.7781, val_MinusLogProbMetric: 48.7781

Epoch 171: val_loss improved from 48.81049 to 48.77807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 32s - loss: 47.8562 - MinusLogProbMetric: 47.8562 - val_loss: 48.7781 - val_MinusLogProbMetric: 48.7781 - lr: 1.2346e-05 - 32s/epoch - 166ms/step
Epoch 172/1000
2023-10-28 01:39:16.141 
Epoch 172/1000 
	 loss: 48.0654, MinusLogProbMetric: 48.0654, val_loss: 49.7116, val_MinusLogProbMetric: 49.7116

Epoch 172: val_loss did not improve from 48.77807
196/196 - 31s - loss: 48.0654 - MinusLogProbMetric: 48.0654 - val_loss: 49.7116 - val_MinusLogProbMetric: 49.7116 - lr: 1.2346e-05 - 31s/epoch - 160ms/step
Epoch 173/1000
2023-10-28 01:39:50.210 
Epoch 173/1000 
	 loss: 47.9631, MinusLogProbMetric: 47.9631, val_loss: 49.2320, val_MinusLogProbMetric: 49.2320

Epoch 173: val_loss did not improve from 48.77807
196/196 - 34s - loss: 47.9631 - MinusLogProbMetric: 47.9631 - val_loss: 49.2320 - val_MinusLogProbMetric: 49.2320 - lr: 1.2346e-05 - 34s/epoch - 174ms/step
Epoch 174/1000
2023-10-28 01:40:27.944 
Epoch 174/1000 
	 loss: 48.0558, MinusLogProbMetric: 48.0558, val_loss: 48.5764, val_MinusLogProbMetric: 48.5764

Epoch 174: val_loss improved from 48.77807 to 48.57642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 38s - loss: 48.0558 - MinusLogProbMetric: 48.0558 - val_loss: 48.5764 - val_MinusLogProbMetric: 48.5764 - lr: 1.2346e-05 - 38s/epoch - 196ms/step
Epoch 175/1000
2023-10-28 01:41:03.865 
Epoch 175/1000 
	 loss: 47.9470, MinusLogProbMetric: 47.9470, val_loss: 51.1035, val_MinusLogProbMetric: 51.1035

Epoch 175: val_loss did not improve from 48.57642
196/196 - 35s - loss: 47.9470 - MinusLogProbMetric: 47.9470 - val_loss: 51.1035 - val_MinusLogProbMetric: 51.1035 - lr: 1.2346e-05 - 35s/epoch - 180ms/step
Epoch 176/1000
2023-10-28 01:41:35.503 
Epoch 176/1000 
	 loss: 48.0800, MinusLogProbMetric: 48.0800, val_loss: 48.7128, val_MinusLogProbMetric: 48.7128

Epoch 176: val_loss did not improve from 48.57642
196/196 - 32s - loss: 48.0800 - MinusLogProbMetric: 48.0800 - val_loss: 48.7128 - val_MinusLogProbMetric: 48.7128 - lr: 1.2346e-05 - 32s/epoch - 161ms/step
Epoch 177/1000
2023-10-28 01:42:07.104 
Epoch 177/1000 
	 loss: 48.1033, MinusLogProbMetric: 48.1033, val_loss: 48.7037, val_MinusLogProbMetric: 48.7037

Epoch 177: val_loss did not improve from 48.57642
196/196 - 32s - loss: 48.1033 - MinusLogProbMetric: 48.1033 - val_loss: 48.7037 - val_MinusLogProbMetric: 48.7037 - lr: 1.2346e-05 - 32s/epoch - 161ms/step
Epoch 178/1000
2023-10-28 01:42:40.437 
Epoch 178/1000 
	 loss: 47.7922, MinusLogProbMetric: 47.7922, val_loss: 49.1219, val_MinusLogProbMetric: 49.1219

Epoch 178: val_loss did not improve from 48.57642
196/196 - 33s - loss: 47.7922 - MinusLogProbMetric: 47.7922 - val_loss: 49.1219 - val_MinusLogProbMetric: 49.1219 - lr: 1.2346e-05 - 33s/epoch - 170ms/step
Epoch 179/1000
2023-10-28 01:43:18.570 
Epoch 179/1000 
	 loss: 47.7985, MinusLogProbMetric: 47.7985, val_loss: 48.6106, val_MinusLogProbMetric: 48.6106

Epoch 179: val_loss did not improve from 48.57642
196/196 - 38s - loss: 47.7985 - MinusLogProbMetric: 47.7985 - val_loss: 48.6106 - val_MinusLogProbMetric: 48.6106 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 180/1000
2023-10-28 01:43:53.888 
Epoch 180/1000 
	 loss: 47.6267, MinusLogProbMetric: 47.6267, val_loss: 48.9579, val_MinusLogProbMetric: 48.9579

Epoch 180: val_loss did not improve from 48.57642
196/196 - 35s - loss: 47.6267 - MinusLogProbMetric: 47.6267 - val_loss: 48.9579 - val_MinusLogProbMetric: 48.9579 - lr: 1.2346e-05 - 35s/epoch - 180ms/step
Epoch 181/1000
2023-10-28 01:44:25.210 
Epoch 181/1000 
	 loss: 47.6224, MinusLogProbMetric: 47.6224, val_loss: 49.0533, val_MinusLogProbMetric: 49.0533

Epoch 181: val_loss did not improve from 48.57642
196/196 - 31s - loss: 47.6224 - MinusLogProbMetric: 47.6224 - val_loss: 49.0533 - val_MinusLogProbMetric: 49.0533 - lr: 1.2346e-05 - 31s/epoch - 160ms/step
Epoch 182/1000
2023-10-28 01:44:56.705 
Epoch 182/1000 
	 loss: 47.5850, MinusLogProbMetric: 47.5850, val_loss: 48.5745, val_MinusLogProbMetric: 48.5745

Epoch 182: val_loss improved from 48.57642 to 48.57447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 32s - loss: 47.5850 - MinusLogProbMetric: 47.5850 - val_loss: 48.5745 - val_MinusLogProbMetric: 48.5745 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 183/1000
2023-10-28 01:45:31.759 
Epoch 183/1000 
	 loss: 48.1706, MinusLogProbMetric: 48.1706, val_loss: 48.8786, val_MinusLogProbMetric: 48.8786

Epoch 183: val_loss did not improve from 48.57447
196/196 - 34s - loss: 48.1706 - MinusLogProbMetric: 48.1706 - val_loss: 48.8786 - val_MinusLogProbMetric: 48.8786 - lr: 1.2346e-05 - 34s/epoch - 176ms/step
Epoch 184/1000
2023-10-28 01:46:06.513 
Epoch 184/1000 
	 loss: 47.5977, MinusLogProbMetric: 47.5977, val_loss: 48.6803, val_MinusLogProbMetric: 48.6803

Epoch 184: val_loss did not improve from 48.57447
196/196 - 35s - loss: 47.5977 - MinusLogProbMetric: 47.5977 - val_loss: 48.6803 - val_MinusLogProbMetric: 48.6803 - lr: 1.2346e-05 - 35s/epoch - 177ms/step
Epoch 185/1000
2023-10-28 01:46:39.631 
Epoch 185/1000 
	 loss: 47.5927, MinusLogProbMetric: 47.5927, val_loss: 48.7421, val_MinusLogProbMetric: 48.7421

Epoch 185: val_loss did not improve from 48.57447
196/196 - 33s - loss: 47.5927 - MinusLogProbMetric: 47.5927 - val_loss: 48.7421 - val_MinusLogProbMetric: 48.7421 - lr: 1.2346e-05 - 33s/epoch - 169ms/step
Epoch 186/1000
2023-10-28 01:47:11.737 
Epoch 186/1000 
	 loss: 47.6978, MinusLogProbMetric: 47.6978, val_loss: 48.7107, val_MinusLogProbMetric: 48.7107

Epoch 186: val_loss did not improve from 48.57447
196/196 - 32s - loss: 47.6978 - MinusLogProbMetric: 47.6978 - val_loss: 48.7107 - val_MinusLogProbMetric: 48.7107 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 187/1000
2023-10-28 01:47:43.645 
Epoch 187/1000 
	 loss: 48.3609, MinusLogProbMetric: 48.3609, val_loss: 51.8046, val_MinusLogProbMetric: 51.8046

Epoch 187: val_loss did not improve from 48.57447
196/196 - 32s - loss: 48.3609 - MinusLogProbMetric: 48.3609 - val_loss: 51.8046 - val_MinusLogProbMetric: 51.8046 - lr: 1.2346e-05 - 32s/epoch - 163ms/step
Epoch 188/1000
2023-10-28 01:48:17.201 
Epoch 188/1000 
	 loss: 51.5837, MinusLogProbMetric: 51.5837, val_loss: 51.9277, val_MinusLogProbMetric: 51.9277

Epoch 188: val_loss did not improve from 48.57447
196/196 - 34s - loss: 51.5837 - MinusLogProbMetric: 51.5837 - val_loss: 51.9277 - val_MinusLogProbMetric: 51.9277 - lr: 1.2346e-05 - 34s/epoch - 171ms/step
Epoch 189/1000
2023-10-28 01:48:54.207 
Epoch 189/1000 
	 loss: 48.8130, MinusLogProbMetric: 48.8130, val_loss: 49.8713, val_MinusLogProbMetric: 49.8713

Epoch 189: val_loss did not improve from 48.57447
196/196 - 37s - loss: 48.8130 - MinusLogProbMetric: 48.8130 - val_loss: 49.8713 - val_MinusLogProbMetric: 49.8713 - lr: 1.2346e-05 - 37s/epoch - 189ms/step
Epoch 190/1000
2023-10-28 01:49:28.992 
Epoch 190/1000 
	 loss: 48.0051, MinusLogProbMetric: 48.0051, val_loss: 48.6537, val_MinusLogProbMetric: 48.6537

Epoch 190: val_loss did not improve from 48.57447
196/196 - 35s - loss: 48.0051 - MinusLogProbMetric: 48.0051 - val_loss: 48.6537 - val_MinusLogProbMetric: 48.6537 - lr: 1.2346e-05 - 35s/epoch - 177ms/step
Epoch 191/1000
2023-10-28 01:50:01.101 
Epoch 191/1000 
	 loss: 47.6673, MinusLogProbMetric: 47.6673, val_loss: 49.0068, val_MinusLogProbMetric: 49.0068

Epoch 191: val_loss did not improve from 48.57447
196/196 - 32s - loss: 47.6673 - MinusLogProbMetric: 47.6673 - val_loss: 49.0068 - val_MinusLogProbMetric: 49.0068 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 192/1000
2023-10-28 01:50:33.457 
Epoch 192/1000 
	 loss: 48.1094, MinusLogProbMetric: 48.1094, val_loss: 48.3365, val_MinusLogProbMetric: 48.3365

Epoch 192: val_loss improved from 48.57447 to 48.33654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 33s - loss: 48.1094 - MinusLogProbMetric: 48.1094 - val_loss: 48.3365 - val_MinusLogProbMetric: 48.3365 - lr: 1.2346e-05 - 33s/epoch - 168ms/step
Epoch 193/1000
2023-10-28 01:51:08.472 
Epoch 193/1000 
	 loss: 47.4535, MinusLogProbMetric: 47.4535, val_loss: 48.2067, val_MinusLogProbMetric: 48.2067

Epoch 193: val_loss improved from 48.33654 to 48.20667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 35s - loss: 47.4535 - MinusLogProbMetric: 47.4535 - val_loss: 48.2067 - val_MinusLogProbMetric: 48.2067 - lr: 1.2346e-05 - 35s/epoch - 179ms/step
Epoch 194/1000
2023-10-28 01:51:46.367 
Epoch 194/1000 
	 loss: 47.5565, MinusLogProbMetric: 47.5565, val_loss: 48.3519, val_MinusLogProbMetric: 48.3519

Epoch 194: val_loss did not improve from 48.20667
196/196 - 37s - loss: 47.5565 - MinusLogProbMetric: 47.5565 - val_loss: 48.3519 - val_MinusLogProbMetric: 48.3519 - lr: 1.2346e-05 - 37s/epoch - 190ms/step
Epoch 195/1000
2023-10-28 01:52:19.181 
Epoch 195/1000 
	 loss: 47.5058, MinusLogProbMetric: 47.5058, val_loss: 48.5686, val_MinusLogProbMetric: 48.5686

Epoch 195: val_loss did not improve from 48.20667
196/196 - 33s - loss: 47.5058 - MinusLogProbMetric: 47.5058 - val_loss: 48.5686 - val_MinusLogProbMetric: 48.5686 - lr: 1.2346e-05 - 33s/epoch - 167ms/step
Epoch 196/1000
2023-10-28 01:52:51.344 
Epoch 196/1000 
	 loss: 47.4274, MinusLogProbMetric: 47.4274, val_loss: 48.3950, val_MinusLogProbMetric: 48.3950

Epoch 196: val_loss did not improve from 48.20667
196/196 - 32s - loss: 47.4274 - MinusLogProbMetric: 47.4274 - val_loss: 48.3950 - val_MinusLogProbMetric: 48.3950 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 197/1000
2023-10-28 01:53:22.916 
Epoch 197/1000 
	 loss: 47.5319, MinusLogProbMetric: 47.5319, val_loss: 48.9860, val_MinusLogProbMetric: 48.9860

Epoch 197: val_loss did not improve from 48.20667
196/196 - 32s - loss: 47.5319 - MinusLogProbMetric: 47.5319 - val_loss: 48.9860 - val_MinusLogProbMetric: 48.9860 - lr: 1.2346e-05 - 32s/epoch - 161ms/step
Epoch 198/1000
2023-10-28 01:53:58.181 
Epoch 198/1000 
	 loss: 47.3508, MinusLogProbMetric: 47.3508, val_loss: 48.4575, val_MinusLogProbMetric: 48.4575

Epoch 198: val_loss did not improve from 48.20667
196/196 - 35s - loss: 47.3508 - MinusLogProbMetric: 47.3508 - val_loss: 48.4575 - val_MinusLogProbMetric: 48.4575 - lr: 1.2346e-05 - 35s/epoch - 180ms/step
Epoch 199/1000
2023-10-28 01:54:34.238 
Epoch 199/1000 
	 loss: 47.2203, MinusLogProbMetric: 47.2203, val_loss: 48.7110, val_MinusLogProbMetric: 48.7110

Epoch 199: val_loss did not improve from 48.20667
196/196 - 36s - loss: 47.2203 - MinusLogProbMetric: 47.2203 - val_loss: 48.7110 - val_MinusLogProbMetric: 48.7110 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 200/1000
2023-10-28 01:55:06.895 
Epoch 200/1000 
	 loss: 47.3931, MinusLogProbMetric: 47.3931, val_loss: 48.5678, val_MinusLogProbMetric: 48.5678

Epoch 200: val_loss did not improve from 48.20667
196/196 - 33s - loss: 47.3931 - MinusLogProbMetric: 47.3931 - val_loss: 48.5678 - val_MinusLogProbMetric: 48.5678 - lr: 1.2346e-05 - 33s/epoch - 167ms/step
Epoch 201/1000
2023-10-28 01:55:39.296 
Epoch 201/1000 
	 loss: 47.3848, MinusLogProbMetric: 47.3848, val_loss: 48.2465, val_MinusLogProbMetric: 48.2465

Epoch 201: val_loss did not improve from 48.20667
196/196 - 32s - loss: 47.3848 - MinusLogProbMetric: 47.3848 - val_loss: 48.2465 - val_MinusLogProbMetric: 48.2465 - lr: 1.2346e-05 - 32s/epoch - 165ms/step
Epoch 202/1000
2023-10-28 01:56:11.308 
Epoch 202/1000 
	 loss: 47.4943, MinusLogProbMetric: 47.4943, val_loss: 48.3821, val_MinusLogProbMetric: 48.3821

Epoch 202: val_loss did not improve from 48.20667
196/196 - 32s - loss: 47.4943 - MinusLogProbMetric: 47.4943 - val_loss: 48.3821 - val_MinusLogProbMetric: 48.3821 - lr: 1.2346e-05 - 32s/epoch - 163ms/step
Epoch 203/1000
2023-10-28 01:56:45.913 
Epoch 203/1000 
	 loss: 47.3450, MinusLogProbMetric: 47.3450, val_loss: 48.5813, val_MinusLogProbMetric: 48.5813

Epoch 203: val_loss did not improve from 48.20667
196/196 - 35s - loss: 47.3450 - MinusLogProbMetric: 47.3450 - val_loss: 48.5813 - val_MinusLogProbMetric: 48.5813 - lr: 1.2346e-05 - 35s/epoch - 177ms/step
Epoch 204/1000
2023-10-28 01:57:21.299 
Epoch 204/1000 
	 loss: 47.2532, MinusLogProbMetric: 47.2532, val_loss: 47.9767, val_MinusLogProbMetric: 47.9767

Epoch 204: val_loss improved from 48.20667 to 47.97667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 47.2532 - MinusLogProbMetric: 47.2532 - val_loss: 47.9767 - val_MinusLogProbMetric: 47.9767 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 205/1000
2023-10-28 01:57:54.000 
Epoch 205/1000 
	 loss: 47.1989, MinusLogProbMetric: 47.1989, val_loss: 48.0525, val_MinusLogProbMetric: 48.0525

Epoch 205: val_loss did not improve from 47.97667
196/196 - 32s - loss: 47.1989 - MinusLogProbMetric: 47.1989 - val_loss: 48.0525 - val_MinusLogProbMetric: 48.0525 - lr: 1.2346e-05 - 32s/epoch - 164ms/step
Epoch 206/1000
2023-10-28 01:58:28.850 
Epoch 206/1000 
	 loss: 48.7256, MinusLogProbMetric: 48.7256, val_loss: 51.6976, val_MinusLogProbMetric: 51.6976

Epoch 206: val_loss did not improve from 47.97667
196/196 - 35s - loss: 48.7256 - MinusLogProbMetric: 48.7256 - val_loss: 51.6976 - val_MinusLogProbMetric: 51.6976 - lr: 1.2346e-05 - 35s/epoch - 178ms/step
Epoch 207/1000
2023-10-28 01:59:01.938 
Epoch 207/1000 
	 loss: 47.5653, MinusLogProbMetric: 47.5653, val_loss: 48.4914, val_MinusLogProbMetric: 48.4914

Epoch 207: val_loss did not improve from 47.97667
196/196 - 33s - loss: 47.5653 - MinusLogProbMetric: 47.5653 - val_loss: 48.4914 - val_MinusLogProbMetric: 48.4914 - lr: 1.2346e-05 - 33s/epoch - 169ms/step
Epoch 208/1000
2023-10-28 01:59:35.777 
Epoch 208/1000 
	 loss: 47.6320, MinusLogProbMetric: 47.6320, val_loss: 48.5199, val_MinusLogProbMetric: 48.5199

Epoch 208: val_loss did not improve from 47.97667
196/196 - 34s - loss: 47.6320 - MinusLogProbMetric: 47.6320 - val_loss: 48.5199 - val_MinusLogProbMetric: 48.5199 - lr: 1.2346e-05 - 34s/epoch - 173ms/step
Epoch 209/1000
2023-10-28 02:00:09.898 
Epoch 209/1000 
	 loss: 47.0343, MinusLogProbMetric: 47.0343, val_loss: 48.3852, val_MinusLogProbMetric: 48.3852

Epoch 209: val_loss did not improve from 47.97667
196/196 - 34s - loss: 47.0343 - MinusLogProbMetric: 47.0343 - val_loss: 48.3852 - val_MinusLogProbMetric: 48.3852 - lr: 1.2346e-05 - 34s/epoch - 174ms/step
Epoch 210/1000
2023-10-28 02:00:44.456 
Epoch 210/1000 
	 loss: 47.0613, MinusLogProbMetric: 47.0613, val_loss: 48.5982, val_MinusLogProbMetric: 48.5982

Epoch 210: val_loss did not improve from 47.97667
196/196 - 35s - loss: 47.0613 - MinusLogProbMetric: 47.0613 - val_loss: 48.5982 - val_MinusLogProbMetric: 48.5982 - lr: 1.2346e-05 - 35s/epoch - 176ms/step
Epoch 211/1000
2023-10-28 02:01:19.004 
Epoch 211/1000 
	 loss: 47.3012, MinusLogProbMetric: 47.3012, val_loss: 48.2834, val_MinusLogProbMetric: 48.2834

Epoch 211: val_loss did not improve from 47.97667
196/196 - 35s - loss: 47.3012 - MinusLogProbMetric: 47.3012 - val_loss: 48.2834 - val_MinusLogProbMetric: 48.2834 - lr: 1.2346e-05 - 35s/epoch - 176ms/step
Epoch 212/1000
2023-10-28 02:01:54.976 
Epoch 212/1000 
	 loss: 47.2135, MinusLogProbMetric: 47.2135, val_loss: 48.3331, val_MinusLogProbMetric: 48.3331

Epoch 212: val_loss did not improve from 47.97667
196/196 - 36s - loss: 47.2135 - MinusLogProbMetric: 47.2135 - val_loss: 48.3331 - val_MinusLogProbMetric: 48.3331 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 213/1000
2023-10-28 02:02:30.090 
Epoch 213/1000 
	 loss: 47.0570, MinusLogProbMetric: 47.0570, val_loss: 47.9540, val_MinusLogProbMetric: 47.9540

Epoch 213: val_loss improved from 47.97667 to 47.95402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 36s - loss: 47.0570 - MinusLogProbMetric: 47.0570 - val_loss: 47.9540 - val_MinusLogProbMetric: 47.9540 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 214/1000
2023-10-28 02:03:11.502 
Epoch 214/1000 
	 loss: 47.0233, MinusLogProbMetric: 47.0233, val_loss: 48.3219, val_MinusLogProbMetric: 48.3219

Epoch 214: val_loss did not improve from 47.95402
196/196 - 40s - loss: 47.0233 - MinusLogProbMetric: 47.0233 - val_loss: 48.3219 - val_MinusLogProbMetric: 48.3219 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 215/1000
2023-10-28 02:03:53.148 
Epoch 215/1000 
	 loss: 47.0636, MinusLogProbMetric: 47.0636, val_loss: 48.7592, val_MinusLogProbMetric: 48.7592

Epoch 215: val_loss did not improve from 47.95402
196/196 - 42s - loss: 47.0636 - MinusLogProbMetric: 47.0636 - val_loss: 48.7592 - val_MinusLogProbMetric: 48.7592 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 216/1000
2023-10-28 02:04:33.933 
Epoch 216/1000 
	 loss: 47.1848, MinusLogProbMetric: 47.1848, val_loss: 48.3422, val_MinusLogProbMetric: 48.3422

Epoch 216: val_loss did not improve from 47.95402
196/196 - 41s - loss: 47.1848 - MinusLogProbMetric: 47.1848 - val_loss: 48.3422 - val_MinusLogProbMetric: 48.3422 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 217/1000
2023-10-28 02:05:14.351 
Epoch 217/1000 
	 loss: 47.1083, MinusLogProbMetric: 47.1083, val_loss: 48.7747, val_MinusLogProbMetric: 48.7747

Epoch 217: val_loss did not improve from 47.95402
196/196 - 40s - loss: 47.1083 - MinusLogProbMetric: 47.1083 - val_loss: 48.7747 - val_MinusLogProbMetric: 48.7747 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 218/1000
2023-10-28 02:05:56.174 
Epoch 218/1000 
	 loss: 49.5557, MinusLogProbMetric: 49.5557, val_loss: 48.8312, val_MinusLogProbMetric: 48.8312

Epoch 218: val_loss did not improve from 47.95402
196/196 - 42s - loss: 49.5557 - MinusLogProbMetric: 49.5557 - val_loss: 48.8312 - val_MinusLogProbMetric: 48.8312 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 219/1000
2023-10-28 02:06:37.708 
Epoch 219/1000 
	 loss: 47.4678, MinusLogProbMetric: 47.4678, val_loss: 48.1606, val_MinusLogProbMetric: 48.1606

Epoch 219: val_loss did not improve from 47.95402
196/196 - 42s - loss: 47.4678 - MinusLogProbMetric: 47.4678 - val_loss: 48.1606 - val_MinusLogProbMetric: 48.1606 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 220/1000
2023-10-28 02:07:18.068 
Epoch 220/1000 
	 loss: 46.9530, MinusLogProbMetric: 46.9530, val_loss: 48.2302, val_MinusLogProbMetric: 48.2302

Epoch 220: val_loss did not improve from 47.95402
196/196 - 40s - loss: 46.9530 - MinusLogProbMetric: 46.9530 - val_loss: 48.2302 - val_MinusLogProbMetric: 48.2302 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 221/1000
2023-10-28 02:07:58.367 
Epoch 221/1000 
	 loss: 46.8969, MinusLogProbMetric: 46.8969, val_loss: 47.6989, val_MinusLogProbMetric: 47.6989

Epoch 221: val_loss improved from 47.95402 to 47.69893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 46.8969 - MinusLogProbMetric: 46.8969 - val_loss: 47.6989 - val_MinusLogProbMetric: 47.6989 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 222/1000
2023-10-28 02:08:39.937 
Epoch 222/1000 
	 loss: 47.0266, MinusLogProbMetric: 47.0266, val_loss: 48.3330, val_MinusLogProbMetric: 48.3330

Epoch 222: val_loss did not improve from 47.69893
196/196 - 41s - loss: 47.0266 - MinusLogProbMetric: 47.0266 - val_loss: 48.3330 - val_MinusLogProbMetric: 48.3330 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 223/1000
2023-10-28 02:09:21.417 
Epoch 223/1000 
	 loss: 47.0046, MinusLogProbMetric: 47.0046, val_loss: 47.7629, val_MinusLogProbMetric: 47.7629

Epoch 223: val_loss did not improve from 47.69893
196/196 - 41s - loss: 47.0046 - MinusLogProbMetric: 47.0046 - val_loss: 47.7629 - val_MinusLogProbMetric: 47.7629 - lr: 1.2346e-05 - 41s/epoch - 212ms/step
Epoch 224/1000
2023-10-28 02:10:02.577 
Epoch 224/1000 
	 loss: 46.9300, MinusLogProbMetric: 46.9300, val_loss: 47.8345, val_MinusLogProbMetric: 47.8345

Epoch 224: val_loss did not improve from 47.69893
196/196 - 41s - loss: 46.9300 - MinusLogProbMetric: 46.9300 - val_loss: 47.8345 - val_MinusLogProbMetric: 47.8345 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 225/1000
2023-10-28 02:10:44.861 
Epoch 225/1000 
	 loss: 46.8389, MinusLogProbMetric: 46.8389, val_loss: 48.3903, val_MinusLogProbMetric: 48.3903

Epoch 225: val_loss did not improve from 47.69893
196/196 - 42s - loss: 46.8389 - MinusLogProbMetric: 46.8389 - val_loss: 48.3903 - val_MinusLogProbMetric: 48.3903 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 226/1000
2023-10-28 02:11:26.921 
Epoch 226/1000 
	 loss: 47.0047, MinusLogProbMetric: 47.0047, val_loss: 48.0027, val_MinusLogProbMetric: 48.0027

Epoch 226: val_loss did not improve from 47.69893
196/196 - 42s - loss: 47.0047 - MinusLogProbMetric: 47.0047 - val_loss: 48.0027 - val_MinusLogProbMetric: 48.0027 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 227/1000
2023-10-28 02:12:09.195 
Epoch 227/1000 
	 loss: 46.7360, MinusLogProbMetric: 46.7360, val_loss: 47.8102, val_MinusLogProbMetric: 47.8102

Epoch 227: val_loss did not improve from 47.69893
196/196 - 42s - loss: 46.7360 - MinusLogProbMetric: 46.7360 - val_loss: 47.8102 - val_MinusLogProbMetric: 47.8102 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 228/1000
2023-10-28 02:12:50.906 
Epoch 228/1000 
	 loss: 46.8574, MinusLogProbMetric: 46.8574, val_loss: 48.1291, val_MinusLogProbMetric: 48.1291

Epoch 228: val_loss did not improve from 47.69893
196/196 - 42s - loss: 46.8574 - MinusLogProbMetric: 46.8574 - val_loss: 48.1291 - val_MinusLogProbMetric: 48.1291 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 229/1000
2023-10-28 02:13:31.964 
Epoch 229/1000 
	 loss: 46.7405, MinusLogProbMetric: 46.7405, val_loss: 47.6226, val_MinusLogProbMetric: 47.6226

Epoch 229: val_loss improved from 47.69893 to 47.62263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 46.7405 - MinusLogProbMetric: 46.7405 - val_loss: 47.6226 - val_MinusLogProbMetric: 47.6226 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 230/1000
2023-10-28 02:14:13.554 
Epoch 230/1000 
	 loss: 46.9384, MinusLogProbMetric: 46.9384, val_loss: 48.9783, val_MinusLogProbMetric: 48.9783

Epoch 230: val_loss did not improve from 47.62263
196/196 - 41s - loss: 46.9384 - MinusLogProbMetric: 46.9384 - val_loss: 48.9783 - val_MinusLogProbMetric: 48.9783 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 231/1000
2023-10-28 02:14:56.075 
Epoch 231/1000 
	 loss: 46.7389, MinusLogProbMetric: 46.7389, val_loss: 47.6990, val_MinusLogProbMetric: 47.6990

Epoch 231: val_loss did not improve from 47.62263
196/196 - 43s - loss: 46.7389 - MinusLogProbMetric: 46.7389 - val_loss: 47.6990 - val_MinusLogProbMetric: 47.6990 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 232/1000
2023-10-28 02:15:37.417 
Epoch 232/1000 
	 loss: 46.6686, MinusLogProbMetric: 46.6686, val_loss: 47.6983, val_MinusLogProbMetric: 47.6983

Epoch 232: val_loss did not improve from 47.62263
196/196 - 41s - loss: 46.6686 - MinusLogProbMetric: 46.6686 - val_loss: 47.6983 - val_MinusLogProbMetric: 47.6983 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 233/1000
2023-10-28 02:16:17.791 
Epoch 233/1000 
	 loss: 46.8358, MinusLogProbMetric: 46.8358, val_loss: 47.8881, val_MinusLogProbMetric: 47.8881

Epoch 233: val_loss did not improve from 47.62263
196/196 - 40s - loss: 46.8358 - MinusLogProbMetric: 46.8358 - val_loss: 47.8881 - val_MinusLogProbMetric: 47.8881 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 234/1000
2023-10-28 02:17:00.956 
Epoch 234/1000 
	 loss: 47.1738, MinusLogProbMetric: 47.1738, val_loss: 47.7546, val_MinusLogProbMetric: 47.7546

Epoch 234: val_loss did not improve from 47.62263
196/196 - 43s - loss: 47.1738 - MinusLogProbMetric: 47.1738 - val_loss: 47.7546 - val_MinusLogProbMetric: 47.7546 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 235/1000
2023-10-28 02:17:43.184 
Epoch 235/1000 
	 loss: 46.9707, MinusLogProbMetric: 46.9707, val_loss: 48.7873, val_MinusLogProbMetric: 48.7873

Epoch 235: val_loss did not improve from 47.62263
196/196 - 42s - loss: 46.9707 - MinusLogProbMetric: 46.9707 - val_loss: 48.7873 - val_MinusLogProbMetric: 48.7873 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 236/1000
2023-10-28 02:18:25.205 
Epoch 236/1000 
	 loss: 46.8643, MinusLogProbMetric: 46.8643, val_loss: 48.0228, val_MinusLogProbMetric: 48.0228

Epoch 236: val_loss did not improve from 47.62263
196/196 - 42s - loss: 46.8643 - MinusLogProbMetric: 46.8643 - val_loss: 48.0228 - val_MinusLogProbMetric: 48.0228 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 237/1000
2023-10-28 02:19:06.045 
Epoch 237/1000 
	 loss: 46.9665, MinusLogProbMetric: 46.9665, val_loss: 48.4177, val_MinusLogProbMetric: 48.4177

Epoch 237: val_loss did not improve from 47.62263
196/196 - 41s - loss: 46.9665 - MinusLogProbMetric: 46.9665 - val_loss: 48.4177 - val_MinusLogProbMetric: 48.4177 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 238/1000
2023-10-28 02:19:45.373 
Epoch 238/1000 
	 loss: 46.7075, MinusLogProbMetric: 46.7075, val_loss: 48.5692, val_MinusLogProbMetric: 48.5692

Epoch 238: val_loss did not improve from 47.62263
196/196 - 39s - loss: 46.7075 - MinusLogProbMetric: 46.7075 - val_loss: 48.5692 - val_MinusLogProbMetric: 48.5692 - lr: 1.2346e-05 - 39s/epoch - 201ms/step
Epoch 239/1000
2023-10-28 02:20:27.649 
Epoch 239/1000 
	 loss: 46.9104, MinusLogProbMetric: 46.9104, val_loss: 48.4515, val_MinusLogProbMetric: 48.4515

Epoch 239: val_loss did not improve from 47.62263
196/196 - 42s - loss: 46.9104 - MinusLogProbMetric: 46.9104 - val_loss: 48.4515 - val_MinusLogProbMetric: 48.4515 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 240/1000
2023-10-28 02:21:09.009 
Epoch 240/1000 
	 loss: 46.7106, MinusLogProbMetric: 46.7106, val_loss: 47.6821, val_MinusLogProbMetric: 47.6821

Epoch 240: val_loss did not improve from 47.62263
196/196 - 41s - loss: 46.7106 - MinusLogProbMetric: 46.7106 - val_loss: 47.6821 - val_MinusLogProbMetric: 47.6821 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 241/1000
2023-10-28 02:21:50.514 
Epoch 241/1000 
	 loss: 46.5620, MinusLogProbMetric: 46.5620, val_loss: 47.5543, val_MinusLogProbMetric: 47.5543

Epoch 241: val_loss improved from 47.62263 to 47.55431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 46.5620 - MinusLogProbMetric: 46.5620 - val_loss: 47.5543 - val_MinusLogProbMetric: 47.5543 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 242/1000
2023-10-28 02:22:33.621 
Epoch 242/1000 
	 loss: 46.8040, MinusLogProbMetric: 46.8040, val_loss: 49.3233, val_MinusLogProbMetric: 49.3233

Epoch 242: val_loss did not improve from 47.55431
196/196 - 42s - loss: 46.8040 - MinusLogProbMetric: 46.8040 - val_loss: 49.3233 - val_MinusLogProbMetric: 49.3233 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 243/1000
2023-10-28 02:23:11.474 
Epoch 243/1000 
	 loss: 46.7962, MinusLogProbMetric: 46.7962, val_loss: 47.9964, val_MinusLogProbMetric: 47.9964

Epoch 243: val_loss did not improve from 47.55431
196/196 - 38s - loss: 46.7962 - MinusLogProbMetric: 46.7962 - val_loss: 47.9964 - val_MinusLogProbMetric: 47.9964 - lr: 1.2346e-05 - 38s/epoch - 193ms/step
Epoch 244/1000
2023-10-28 02:23:49.470 
Epoch 244/1000 
	 loss: 46.5853, MinusLogProbMetric: 46.5853, val_loss: 47.4356, val_MinusLogProbMetric: 47.4356

Epoch 244: val_loss improved from 47.55431 to 47.43559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 39s - loss: 46.5853 - MinusLogProbMetric: 46.5853 - val_loss: 47.4356 - val_MinusLogProbMetric: 47.4356 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 245/1000
2023-10-28 02:24:25.243 
Epoch 245/1000 
	 loss: 46.5687, MinusLogProbMetric: 46.5687, val_loss: 49.3387, val_MinusLogProbMetric: 49.3387

Epoch 245: val_loss did not improve from 47.43559
196/196 - 35s - loss: 46.5687 - MinusLogProbMetric: 46.5687 - val_loss: 49.3387 - val_MinusLogProbMetric: 49.3387 - lr: 1.2346e-05 - 35s/epoch - 179ms/step
Epoch 246/1000
2023-10-28 02:25:06.522 
Epoch 246/1000 
	 loss: 46.8458, MinusLogProbMetric: 46.8458, val_loss: 47.3048, val_MinusLogProbMetric: 47.3048

Epoch 246: val_loss improved from 47.43559 to 47.30480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 46.8458 - MinusLogProbMetric: 46.8458 - val_loss: 47.3048 - val_MinusLogProbMetric: 47.3048 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 247/1000
2023-10-28 02:25:48.761 
Epoch 247/1000 
	 loss: 46.4573, MinusLogProbMetric: 46.4573, val_loss: 47.6522, val_MinusLogProbMetric: 47.6522

Epoch 247: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.4573 - MinusLogProbMetric: 46.4573 - val_loss: 47.6522 - val_MinusLogProbMetric: 47.6522 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 248/1000
2023-10-28 02:26:30.965 
Epoch 248/1000 
	 loss: 46.8409, MinusLogProbMetric: 46.8409, val_loss: 47.7980, val_MinusLogProbMetric: 47.7980

Epoch 248: val_loss did not improve from 47.30480
196/196 - 42s - loss: 46.8409 - MinusLogProbMetric: 46.8409 - val_loss: 47.7980 - val_MinusLogProbMetric: 47.7980 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 249/1000
2023-10-28 02:27:12.093 
Epoch 249/1000 
	 loss: 46.6333, MinusLogProbMetric: 46.6333, val_loss: 50.2294, val_MinusLogProbMetric: 50.2294

Epoch 249: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.6333 - MinusLogProbMetric: 46.6333 - val_loss: 50.2294 - val_MinusLogProbMetric: 50.2294 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 250/1000
2023-10-28 02:27:51.708 
Epoch 250/1000 
	 loss: 46.5091, MinusLogProbMetric: 46.5091, val_loss: 47.8177, val_MinusLogProbMetric: 47.8177

Epoch 250: val_loss did not improve from 47.30480
196/196 - 40s - loss: 46.5091 - MinusLogProbMetric: 46.5091 - val_loss: 47.8177 - val_MinusLogProbMetric: 47.8177 - lr: 1.2346e-05 - 40s/epoch - 202ms/step
Epoch 251/1000
2023-10-28 02:28:32.935 
Epoch 251/1000 
	 loss: 46.6178, MinusLogProbMetric: 46.6178, val_loss: 47.6306, val_MinusLogProbMetric: 47.6306

Epoch 251: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.6178 - MinusLogProbMetric: 46.6178 - val_loss: 47.6306 - val_MinusLogProbMetric: 47.6306 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 252/1000
2023-10-28 02:29:14.764 
Epoch 252/1000 
	 loss: 47.2302, MinusLogProbMetric: 47.2302, val_loss: 47.3435, val_MinusLogProbMetric: 47.3435

Epoch 252: val_loss did not improve from 47.30480
196/196 - 42s - loss: 47.2302 - MinusLogProbMetric: 47.2302 - val_loss: 47.3435 - val_MinusLogProbMetric: 47.3435 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 253/1000
2023-10-28 02:29:56.612 
Epoch 253/1000 
	 loss: 46.3525, MinusLogProbMetric: 46.3525, val_loss: 47.4842, val_MinusLogProbMetric: 47.4842

Epoch 253: val_loss did not improve from 47.30480
196/196 - 42s - loss: 46.3525 - MinusLogProbMetric: 46.3525 - val_loss: 47.4842 - val_MinusLogProbMetric: 47.4842 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 254/1000
2023-10-28 02:30:37.708 
Epoch 254/1000 
	 loss: 46.4416, MinusLogProbMetric: 46.4416, val_loss: 47.6562, val_MinusLogProbMetric: 47.6562

Epoch 254: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.4416 - MinusLogProbMetric: 46.4416 - val_loss: 47.6562 - val_MinusLogProbMetric: 47.6562 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 255/1000
2023-10-28 02:31:18.940 
Epoch 255/1000 
	 loss: 46.7011, MinusLogProbMetric: 46.7011, val_loss: 47.5392, val_MinusLogProbMetric: 47.5392

Epoch 255: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.7011 - MinusLogProbMetric: 46.7011 - val_loss: 47.5392 - val_MinusLogProbMetric: 47.5392 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 256/1000
2023-10-28 02:31:59.601 
Epoch 256/1000 
	 loss: 46.4400, MinusLogProbMetric: 46.4400, val_loss: 48.7827, val_MinusLogProbMetric: 48.7827

Epoch 256: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.4400 - MinusLogProbMetric: 46.4400 - val_loss: 48.7827 - val_MinusLogProbMetric: 48.7827 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 257/1000
2023-10-28 02:32:40.854 
Epoch 257/1000 
	 loss: 46.3853, MinusLogProbMetric: 46.3853, val_loss: 47.3607, val_MinusLogProbMetric: 47.3607

Epoch 257: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.3853 - MinusLogProbMetric: 46.3853 - val_loss: 47.3607 - val_MinusLogProbMetric: 47.3607 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 258/1000
2023-10-28 02:33:21.753 
Epoch 258/1000 
	 loss: 46.7553, MinusLogProbMetric: 46.7553, val_loss: 47.6102, val_MinusLogProbMetric: 47.6102

Epoch 258: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.7553 - MinusLogProbMetric: 46.7553 - val_loss: 47.6102 - val_MinusLogProbMetric: 47.6102 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 259/1000
2023-10-28 02:34:03.046 
Epoch 259/1000 
	 loss: 46.3733, MinusLogProbMetric: 46.3733, val_loss: 47.8705, val_MinusLogProbMetric: 47.8705

Epoch 259: val_loss did not improve from 47.30480
196/196 - 41s - loss: 46.3733 - MinusLogProbMetric: 46.3733 - val_loss: 47.8705 - val_MinusLogProbMetric: 47.8705 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 260/1000
2023-10-28 02:34:42.920 
Epoch 260/1000 
	 loss: 46.4667, MinusLogProbMetric: 46.4667, val_loss: 47.0408, val_MinusLogProbMetric: 47.0408

Epoch 260: val_loss improved from 47.30480 to 47.04076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 46.4667 - MinusLogProbMetric: 46.4667 - val_loss: 47.0408 - val_MinusLogProbMetric: 47.0408 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 261/1000
2023-10-28 02:35:24.846 
Epoch 261/1000 
	 loss: 46.3309, MinusLogProbMetric: 46.3309, val_loss: 47.5668, val_MinusLogProbMetric: 47.5668

Epoch 261: val_loss did not improve from 47.04076
196/196 - 41s - loss: 46.3309 - MinusLogProbMetric: 46.3309 - val_loss: 47.5668 - val_MinusLogProbMetric: 47.5668 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 262/1000
2023-10-28 02:36:06.455 
Epoch 262/1000 
	 loss: 46.5157, MinusLogProbMetric: 46.5157, val_loss: 47.9081, val_MinusLogProbMetric: 47.9081

Epoch 262: val_loss did not improve from 47.04076
196/196 - 42s - loss: 46.5157 - MinusLogProbMetric: 46.5157 - val_loss: 47.9081 - val_MinusLogProbMetric: 47.9081 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 263/1000
2023-10-28 02:36:47.687 
Epoch 263/1000 
	 loss: 46.9505, MinusLogProbMetric: 46.9505, val_loss: 49.2298, val_MinusLogProbMetric: 49.2298

Epoch 263: val_loss did not improve from 47.04076
196/196 - 41s - loss: 46.9505 - MinusLogProbMetric: 46.9505 - val_loss: 49.2298 - val_MinusLogProbMetric: 49.2298 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 264/1000
2023-10-28 02:37:29.954 
Epoch 264/1000 
	 loss: 46.3669, MinusLogProbMetric: 46.3669, val_loss: 47.3043, val_MinusLogProbMetric: 47.3043

Epoch 264: val_loss did not improve from 47.04076
196/196 - 42s - loss: 46.3669 - MinusLogProbMetric: 46.3669 - val_loss: 47.3043 - val_MinusLogProbMetric: 47.3043 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 265/1000
2023-10-28 02:38:10.108 
Epoch 265/1000 
	 loss: 46.2781, MinusLogProbMetric: 46.2781, val_loss: 47.8153, val_MinusLogProbMetric: 47.8153

Epoch 265: val_loss did not improve from 47.04076
196/196 - 40s - loss: 46.2781 - MinusLogProbMetric: 46.2781 - val_loss: 47.8153 - val_MinusLogProbMetric: 47.8153 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 266/1000
2023-10-28 02:38:52.353 
Epoch 266/1000 
	 loss: 46.5345, MinusLogProbMetric: 46.5345, val_loss: 47.5290, val_MinusLogProbMetric: 47.5290

Epoch 266: val_loss did not improve from 47.04076
196/196 - 42s - loss: 46.5345 - MinusLogProbMetric: 46.5345 - val_loss: 47.5290 - val_MinusLogProbMetric: 47.5290 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 267/1000
2023-10-28 02:39:33.741 
Epoch 267/1000 
	 loss: 46.8948, MinusLogProbMetric: 46.8948, val_loss: 47.2761, val_MinusLogProbMetric: 47.2761

Epoch 267: val_loss did not improve from 47.04076
196/196 - 41s - loss: 46.8948 - MinusLogProbMetric: 46.8948 - val_loss: 47.2761 - val_MinusLogProbMetric: 47.2761 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 268/1000
2023-10-28 02:40:14.321 
Epoch 268/1000 
	 loss: 46.5742, MinusLogProbMetric: 46.5742, val_loss: 47.3792, val_MinusLogProbMetric: 47.3792

Epoch 268: val_loss did not improve from 47.04076
196/196 - 41s - loss: 46.5742 - MinusLogProbMetric: 46.5742 - val_loss: 47.3792 - val_MinusLogProbMetric: 47.3792 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 269/1000
2023-10-28 02:40:55.537 
Epoch 269/1000 
	 loss: 46.6208, MinusLogProbMetric: 46.6208, val_loss: 47.3415, val_MinusLogProbMetric: 47.3415

Epoch 269: val_loss did not improve from 47.04076
196/196 - 41s - loss: 46.6208 - MinusLogProbMetric: 46.6208 - val_loss: 47.3415 - val_MinusLogProbMetric: 47.3415 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 270/1000
2023-10-28 02:41:36.045 
Epoch 270/1000 
	 loss: 46.2219, MinusLogProbMetric: 46.2219, val_loss: 47.0232, val_MinusLogProbMetric: 47.0232

Epoch 270: val_loss improved from 47.04076 to 47.02315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 46.2219 - MinusLogProbMetric: 46.2219 - val_loss: 47.0232 - val_MinusLogProbMetric: 47.0232 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 271/1000
2023-10-28 02:42:16.802 
Epoch 271/1000 
	 loss: 46.2644, MinusLogProbMetric: 46.2644, val_loss: 48.0791, val_MinusLogProbMetric: 48.0791

Epoch 271: val_loss did not improve from 47.02315
196/196 - 40s - loss: 46.2644 - MinusLogProbMetric: 46.2644 - val_loss: 48.0791 - val_MinusLogProbMetric: 48.0791 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 272/1000
2023-10-28 02:42:56.627 
Epoch 272/1000 
	 loss: 47.2910, MinusLogProbMetric: 47.2910, val_loss: 47.1452, val_MinusLogProbMetric: 47.1452

Epoch 272: val_loss did not improve from 47.02315
196/196 - 40s - loss: 47.2910 - MinusLogProbMetric: 47.2910 - val_loss: 47.1452 - val_MinusLogProbMetric: 47.1452 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 273/1000
2023-10-28 02:43:37.439 
Epoch 273/1000 
	 loss: 46.5398, MinusLogProbMetric: 46.5398, val_loss: 47.1815, val_MinusLogProbMetric: 47.1815

Epoch 273: val_loss did not improve from 47.02315
196/196 - 41s - loss: 46.5398 - MinusLogProbMetric: 46.5398 - val_loss: 47.1815 - val_MinusLogProbMetric: 47.1815 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 274/1000
2023-10-28 02:44:18.288 
Epoch 274/1000 
	 loss: 46.5240, MinusLogProbMetric: 46.5240, val_loss: 51.6790, val_MinusLogProbMetric: 51.6790

Epoch 274: val_loss did not improve from 47.02315
196/196 - 41s - loss: 46.5240 - MinusLogProbMetric: 46.5240 - val_loss: 51.6790 - val_MinusLogProbMetric: 51.6790 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 275/1000
2023-10-28 02:44:58.714 
Epoch 275/1000 
	 loss: 46.2905, MinusLogProbMetric: 46.2905, val_loss: 47.3724, val_MinusLogProbMetric: 47.3724

Epoch 275: val_loss did not improve from 47.02315
196/196 - 40s - loss: 46.2905 - MinusLogProbMetric: 46.2905 - val_loss: 47.3724 - val_MinusLogProbMetric: 47.3724 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 276/1000
2023-10-28 02:45:39.796 
Epoch 276/1000 
	 loss: 46.2851, MinusLogProbMetric: 46.2851, val_loss: 47.3014, val_MinusLogProbMetric: 47.3014

Epoch 276: val_loss did not improve from 47.02315
196/196 - 41s - loss: 46.2851 - MinusLogProbMetric: 46.2851 - val_loss: 47.3014 - val_MinusLogProbMetric: 47.3014 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 277/1000
2023-10-28 02:46:21.100 
Epoch 277/1000 
	 loss: 46.4089, MinusLogProbMetric: 46.4089, val_loss: 47.2182, val_MinusLogProbMetric: 47.2182

Epoch 277: val_loss did not improve from 47.02315
196/196 - 41s - loss: 46.4089 - MinusLogProbMetric: 46.4089 - val_loss: 47.2182 - val_MinusLogProbMetric: 47.2182 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 278/1000
2023-10-28 02:47:02.504 
Epoch 278/1000 
	 loss: 46.3393, MinusLogProbMetric: 46.3393, val_loss: 47.4041, val_MinusLogProbMetric: 47.4041

Epoch 278: val_loss did not improve from 47.02315
196/196 - 41s - loss: 46.3393 - MinusLogProbMetric: 46.3393 - val_loss: 47.4041 - val_MinusLogProbMetric: 47.4041 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 279/1000
2023-10-28 02:47:44.443 
Epoch 279/1000 
	 loss: 46.0752, MinusLogProbMetric: 46.0752, val_loss: 47.5247, val_MinusLogProbMetric: 47.5247

Epoch 279: val_loss did not improve from 47.02315
196/196 - 42s - loss: 46.0752 - MinusLogProbMetric: 46.0752 - val_loss: 47.5247 - val_MinusLogProbMetric: 47.5247 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 280/1000
2023-10-28 02:48:27.569 
Epoch 280/1000 
	 loss: 47.1729, MinusLogProbMetric: 47.1729, val_loss: 51.0726, val_MinusLogProbMetric: 51.0726

Epoch 280: val_loss did not improve from 47.02315
196/196 - 43s - loss: 47.1729 - MinusLogProbMetric: 47.1729 - val_loss: 51.0726 - val_MinusLogProbMetric: 51.0726 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 281/1000
2023-10-28 02:49:10.164 
Epoch 281/1000 
	 loss: 46.4020, MinusLogProbMetric: 46.4020, val_loss: 47.0164, val_MinusLogProbMetric: 47.0164

Epoch 281: val_loss improved from 47.02315 to 47.01638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 46.4020 - MinusLogProbMetric: 46.4020 - val_loss: 47.0164 - val_MinusLogProbMetric: 47.0164 - lr: 1.2346e-05 - 43s/epoch - 222ms/step
Epoch 282/1000
2023-10-28 02:49:53.439 
Epoch 282/1000 
	 loss: 46.0157, MinusLogProbMetric: 46.0157, val_loss: 46.9493, val_MinusLogProbMetric: 46.9493

Epoch 282: val_loss improved from 47.01638 to 46.94933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 46.0157 - MinusLogProbMetric: 46.0157 - val_loss: 46.9493 - val_MinusLogProbMetric: 46.9493 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 283/1000
2023-10-28 02:50:35.532 
Epoch 283/1000 
	 loss: 46.1998, MinusLogProbMetric: 46.1998, val_loss: 48.3606, val_MinusLogProbMetric: 48.3606

Epoch 283: val_loss did not improve from 46.94933
196/196 - 41s - loss: 46.1998 - MinusLogProbMetric: 46.1998 - val_loss: 48.3606 - val_MinusLogProbMetric: 48.3606 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 284/1000
2023-10-28 02:51:16.753 
Epoch 284/1000 
	 loss: 47.2915, MinusLogProbMetric: 47.2915, val_loss: 47.1460, val_MinusLogProbMetric: 47.1460

Epoch 284: val_loss did not improve from 46.94933
196/196 - 41s - loss: 47.2915 - MinusLogProbMetric: 47.2915 - val_loss: 47.1460 - val_MinusLogProbMetric: 47.1460 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 285/1000
2023-10-28 02:51:58.253 
Epoch 285/1000 
	 loss: 46.0331, MinusLogProbMetric: 46.0331, val_loss: 46.8589, val_MinusLogProbMetric: 46.8589

Epoch 285: val_loss improved from 46.94933 to 46.85886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 46.0331 - MinusLogProbMetric: 46.0331 - val_loss: 46.8589 - val_MinusLogProbMetric: 46.8589 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 286/1000
2023-10-28 02:52:42.130 
Epoch 286/1000 
	 loss: 45.9197, MinusLogProbMetric: 45.9197, val_loss: 46.8162, val_MinusLogProbMetric: 46.8162

Epoch 286: val_loss improved from 46.85886 to 46.81615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 44s - loss: 45.9197 - MinusLogProbMetric: 45.9197 - val_loss: 46.8162 - val_MinusLogProbMetric: 46.8162 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 287/1000
2023-10-28 02:53:25.296 
Epoch 287/1000 
	 loss: 46.2064, MinusLogProbMetric: 46.2064, val_loss: 46.8670, val_MinusLogProbMetric: 46.8670

Epoch 287: val_loss did not improve from 46.81615
196/196 - 42s - loss: 46.2064 - MinusLogProbMetric: 46.2064 - val_loss: 46.8670 - val_MinusLogProbMetric: 46.8670 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 288/1000
2023-10-28 02:54:07.639 
Epoch 288/1000 
	 loss: 46.0388, MinusLogProbMetric: 46.0388, val_loss: 47.4686, val_MinusLogProbMetric: 47.4686

Epoch 288: val_loss did not improve from 46.81615
196/196 - 42s - loss: 46.0388 - MinusLogProbMetric: 46.0388 - val_loss: 47.4686 - val_MinusLogProbMetric: 47.4686 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 289/1000
2023-10-28 02:54:49.143 
Epoch 289/1000 
	 loss: 46.4277, MinusLogProbMetric: 46.4277, val_loss: 48.0632, val_MinusLogProbMetric: 48.0632

Epoch 289: val_loss did not improve from 46.81615
196/196 - 42s - loss: 46.4277 - MinusLogProbMetric: 46.4277 - val_loss: 48.0632 - val_MinusLogProbMetric: 48.0632 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 290/1000
2023-10-28 02:55:30.205 
Epoch 290/1000 
	 loss: 46.4989, MinusLogProbMetric: 46.4989, val_loss: 46.7882, val_MinusLogProbMetric: 46.7882

Epoch 290: val_loss improved from 46.81615 to 46.78822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 46.4989 - MinusLogProbMetric: 46.4989 - val_loss: 46.7882 - val_MinusLogProbMetric: 46.7882 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 291/1000
2023-10-28 02:56:13.131 
Epoch 291/1000 
	 loss: 45.8481, MinusLogProbMetric: 45.8481, val_loss: 47.0230, val_MinusLogProbMetric: 47.0230

Epoch 291: val_loss did not improve from 46.78822
196/196 - 42s - loss: 45.8481 - MinusLogProbMetric: 45.8481 - val_loss: 47.0230 - val_MinusLogProbMetric: 47.0230 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 292/1000
2023-10-28 02:56:53.456 
Epoch 292/1000 
	 loss: 46.0678, MinusLogProbMetric: 46.0678, val_loss: 47.1542, val_MinusLogProbMetric: 47.1542

Epoch 292: val_loss did not improve from 46.78822
196/196 - 40s - loss: 46.0678 - MinusLogProbMetric: 46.0678 - val_loss: 47.1542 - val_MinusLogProbMetric: 47.1542 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 293/1000
2023-10-28 02:57:35.075 
Epoch 293/1000 
	 loss: 46.6351, MinusLogProbMetric: 46.6351, val_loss: 47.2177, val_MinusLogProbMetric: 47.2177

Epoch 293: val_loss did not improve from 46.78822
196/196 - 42s - loss: 46.6351 - MinusLogProbMetric: 46.6351 - val_loss: 47.2177 - val_MinusLogProbMetric: 47.2177 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 294/1000
2023-10-28 02:58:15.194 
Epoch 294/1000 
	 loss: 46.0574, MinusLogProbMetric: 46.0574, val_loss: 48.1718, val_MinusLogProbMetric: 48.1718

Epoch 294: val_loss did not improve from 46.78822
196/196 - 40s - loss: 46.0574 - MinusLogProbMetric: 46.0574 - val_loss: 48.1718 - val_MinusLogProbMetric: 48.1718 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 295/1000
2023-10-28 02:58:56.785 
Epoch 295/1000 
	 loss: 45.9087, MinusLogProbMetric: 45.9087, val_loss: 46.7978, val_MinusLogProbMetric: 46.7978

Epoch 295: val_loss did not improve from 46.78822
196/196 - 42s - loss: 45.9087 - MinusLogProbMetric: 45.9087 - val_loss: 46.7978 - val_MinusLogProbMetric: 46.7978 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 296/1000
2023-10-28 02:59:37.804 
Epoch 296/1000 
	 loss: 48.3220, MinusLogProbMetric: 48.3220, val_loss: 47.0727, val_MinusLogProbMetric: 47.0727

Epoch 296: val_loss did not improve from 46.78822
196/196 - 41s - loss: 48.3220 - MinusLogProbMetric: 48.3220 - val_loss: 47.0727 - val_MinusLogProbMetric: 47.0727 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 297/1000
2023-10-28 03:00:18.498 
Epoch 297/1000 
	 loss: 46.0683, MinusLogProbMetric: 46.0683, val_loss: 46.9833, val_MinusLogProbMetric: 46.9833

Epoch 297: val_loss did not improve from 46.78822
196/196 - 41s - loss: 46.0683 - MinusLogProbMetric: 46.0683 - val_loss: 46.9833 - val_MinusLogProbMetric: 46.9833 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 298/1000
2023-10-28 03:00:59.464 
Epoch 298/1000 
	 loss: 45.9991, MinusLogProbMetric: 45.9991, val_loss: 46.7315, val_MinusLogProbMetric: 46.7315

Epoch 298: val_loss improved from 46.78822 to 46.73154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 45.9991 - MinusLogProbMetric: 45.9991 - val_loss: 46.7315 - val_MinusLogProbMetric: 46.7315 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 299/1000
2023-10-28 03:01:41.228 
Epoch 299/1000 
	 loss: 46.0072, MinusLogProbMetric: 46.0072, val_loss: 47.8081, val_MinusLogProbMetric: 47.8081

Epoch 299: val_loss did not improve from 46.73154
196/196 - 41s - loss: 46.0072 - MinusLogProbMetric: 46.0072 - val_loss: 47.8081 - val_MinusLogProbMetric: 47.8081 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 300/1000
2023-10-28 03:02:23.856 
Epoch 300/1000 
	 loss: 46.1785, MinusLogProbMetric: 46.1785, val_loss: 48.9699, val_MinusLogProbMetric: 48.9699

Epoch 300: val_loss did not improve from 46.73154
196/196 - 43s - loss: 46.1785 - MinusLogProbMetric: 46.1785 - val_loss: 48.9699 - val_MinusLogProbMetric: 48.9699 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 301/1000
2023-10-28 03:03:05.750 
Epoch 301/1000 
	 loss: 45.8527, MinusLogProbMetric: 45.8527, val_loss: 46.7441, val_MinusLogProbMetric: 46.7441

Epoch 301: val_loss did not improve from 46.73154
196/196 - 42s - loss: 45.8527 - MinusLogProbMetric: 45.8527 - val_loss: 46.7441 - val_MinusLogProbMetric: 46.7441 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 302/1000
2023-10-28 03:03:47.805 
Epoch 302/1000 
	 loss: 45.9503, MinusLogProbMetric: 45.9503, val_loss: 47.4452, val_MinusLogProbMetric: 47.4452

Epoch 302: val_loss did not improve from 46.73154
196/196 - 42s - loss: 45.9503 - MinusLogProbMetric: 45.9503 - val_loss: 47.4452 - val_MinusLogProbMetric: 47.4452 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 303/1000
2023-10-28 03:04:29.789 
Epoch 303/1000 
	 loss: 45.8473, MinusLogProbMetric: 45.8473, val_loss: 47.0178, val_MinusLogProbMetric: 47.0178

Epoch 303: val_loss did not improve from 46.73154
196/196 - 42s - loss: 45.8473 - MinusLogProbMetric: 45.8473 - val_loss: 47.0178 - val_MinusLogProbMetric: 47.0178 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 304/1000
2023-10-28 03:05:12.330 
Epoch 304/1000 
	 loss: 46.1556, MinusLogProbMetric: 46.1556, val_loss: 46.9551, val_MinusLogProbMetric: 46.9551

Epoch 304: val_loss did not improve from 46.73154
196/196 - 43s - loss: 46.1556 - MinusLogProbMetric: 46.1556 - val_loss: 46.9551 - val_MinusLogProbMetric: 46.9551 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 305/1000
2023-10-28 03:05:54.273 
Epoch 305/1000 
	 loss: 46.0842, MinusLogProbMetric: 46.0842, val_loss: 47.0255, val_MinusLogProbMetric: 47.0255

Epoch 305: val_loss did not improve from 46.73154
196/196 - 42s - loss: 46.0842 - MinusLogProbMetric: 46.0842 - val_loss: 47.0255 - val_MinusLogProbMetric: 47.0255 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 306/1000
2023-10-28 03:06:35.093 
Epoch 306/1000 
	 loss: 45.9414, MinusLogProbMetric: 45.9414, val_loss: 47.9305, val_MinusLogProbMetric: 47.9305

Epoch 306: val_loss did not improve from 46.73154
196/196 - 41s - loss: 45.9414 - MinusLogProbMetric: 45.9414 - val_loss: 47.9305 - val_MinusLogProbMetric: 47.9305 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 307/1000
2023-10-28 03:07:16.484 
Epoch 307/1000 
	 loss: 45.8429, MinusLogProbMetric: 45.8429, val_loss: 47.1368, val_MinusLogProbMetric: 47.1368

Epoch 307: val_loss did not improve from 46.73154
196/196 - 41s - loss: 45.8429 - MinusLogProbMetric: 45.8429 - val_loss: 47.1368 - val_MinusLogProbMetric: 47.1368 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 308/1000
2023-10-28 03:07:57.785 
Epoch 308/1000 
	 loss: 45.9720, MinusLogProbMetric: 45.9720, val_loss: 47.5720, val_MinusLogProbMetric: 47.5720

Epoch 308: val_loss did not improve from 46.73154
196/196 - 41s - loss: 45.9720 - MinusLogProbMetric: 45.9720 - val_loss: 47.5720 - val_MinusLogProbMetric: 47.5720 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 309/1000
2023-10-28 03:08:39.134 
Epoch 309/1000 
	 loss: 47.4346, MinusLogProbMetric: 47.4346, val_loss: 47.5095, val_MinusLogProbMetric: 47.5095

Epoch 309: val_loss did not improve from 46.73154
196/196 - 41s - loss: 47.4346 - MinusLogProbMetric: 47.4346 - val_loss: 47.5095 - val_MinusLogProbMetric: 47.5095 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 310/1000
2023-10-28 03:09:20.114 
Epoch 310/1000 
	 loss: 46.7294, MinusLogProbMetric: 46.7294, val_loss: 46.9619, val_MinusLogProbMetric: 46.9619

Epoch 310: val_loss did not improve from 46.73154
196/196 - 41s - loss: 46.7294 - MinusLogProbMetric: 46.7294 - val_loss: 46.9619 - val_MinusLogProbMetric: 46.9619 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 311/1000
2023-10-28 03:10:01.722 
Epoch 311/1000 
	 loss: 45.7551, MinusLogProbMetric: 45.7551, val_loss: 47.1471, val_MinusLogProbMetric: 47.1471

Epoch 311: val_loss did not improve from 46.73154
196/196 - 42s - loss: 45.7551 - MinusLogProbMetric: 45.7551 - val_loss: 47.1471 - val_MinusLogProbMetric: 47.1471 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 312/1000
2023-10-28 03:10:42.865 
Epoch 312/1000 
	 loss: 45.8834, MinusLogProbMetric: 45.8834, val_loss: 47.2030, val_MinusLogProbMetric: 47.2030

Epoch 312: val_loss did not improve from 46.73154
196/196 - 41s - loss: 45.8834 - MinusLogProbMetric: 45.8834 - val_loss: 47.2030 - val_MinusLogProbMetric: 47.2030 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 313/1000
2023-10-28 03:11:24.354 
Epoch 313/1000 
	 loss: 46.8046, MinusLogProbMetric: 46.8046, val_loss: 46.8467, val_MinusLogProbMetric: 46.8467

Epoch 313: val_loss did not improve from 46.73154
196/196 - 41s - loss: 46.8046 - MinusLogProbMetric: 46.8046 - val_loss: 46.8467 - val_MinusLogProbMetric: 46.8467 - lr: 1.2346e-05 - 41s/epoch - 212ms/step
Epoch 314/1000
2023-10-28 03:12:05.428 
Epoch 314/1000 
	 loss: 45.7383, MinusLogProbMetric: 45.7383, val_loss: 47.0032, val_MinusLogProbMetric: 47.0032

Epoch 314: val_loss did not improve from 46.73154
196/196 - 41s - loss: 45.7383 - MinusLogProbMetric: 45.7383 - val_loss: 47.0032 - val_MinusLogProbMetric: 47.0032 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 315/1000
2023-10-28 03:12:46.353 
Epoch 315/1000 
	 loss: 45.6023, MinusLogProbMetric: 45.6023, val_loss: 47.0950, val_MinusLogProbMetric: 47.0950

Epoch 315: val_loss did not improve from 46.73154
196/196 - 41s - loss: 45.6023 - MinusLogProbMetric: 45.6023 - val_loss: 47.0950 - val_MinusLogProbMetric: 47.0950 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 316/1000
2023-10-28 03:13:27.214 
Epoch 316/1000 
	 loss: 46.1584, MinusLogProbMetric: 46.1584, val_loss: 46.9844, val_MinusLogProbMetric: 46.9844

Epoch 316: val_loss did not improve from 46.73154
196/196 - 41s - loss: 46.1584 - MinusLogProbMetric: 46.1584 - val_loss: 46.9844 - val_MinusLogProbMetric: 46.9844 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 317/1000
2023-10-28 03:14:08.133 
Epoch 317/1000 
	 loss: 46.1906, MinusLogProbMetric: 46.1906, val_loss: 46.7879, val_MinusLogProbMetric: 46.7879

Epoch 317: val_loss did not improve from 46.73154
196/196 - 41s - loss: 46.1906 - MinusLogProbMetric: 46.1906 - val_loss: 46.7879 - val_MinusLogProbMetric: 46.7879 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 318/1000
2023-10-28 03:14:48.984 
Epoch 318/1000 
	 loss: 45.9657, MinusLogProbMetric: 45.9657, val_loss: 47.3889, val_MinusLogProbMetric: 47.3889

Epoch 318: val_loss did not improve from 46.73154
196/196 - 41s - loss: 45.9657 - MinusLogProbMetric: 45.9657 - val_loss: 47.3889 - val_MinusLogProbMetric: 47.3889 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 319/1000
2023-10-28 03:15:29.709 
Epoch 319/1000 
	 loss: 45.7458, MinusLogProbMetric: 45.7458, val_loss: 46.5622, val_MinusLogProbMetric: 46.5622

Epoch 319: val_loss improved from 46.73154 to 46.56223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 45.7458 - MinusLogProbMetric: 45.7458 - val_loss: 46.5622 - val_MinusLogProbMetric: 46.5622 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 320/1000
2023-10-28 03:16:10.973 
Epoch 320/1000 
	 loss: 45.8108, MinusLogProbMetric: 45.8108, val_loss: 47.2331, val_MinusLogProbMetric: 47.2331

Epoch 320: val_loss did not improve from 46.56223
196/196 - 40s - loss: 45.8108 - MinusLogProbMetric: 45.8108 - val_loss: 47.2331 - val_MinusLogProbMetric: 47.2331 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 321/1000
2023-10-28 03:16:52.398 
Epoch 321/1000 
	 loss: 45.7997, MinusLogProbMetric: 45.7997, val_loss: 46.8093, val_MinusLogProbMetric: 46.8093

Epoch 321: val_loss did not improve from 46.56223
196/196 - 41s - loss: 45.7997 - MinusLogProbMetric: 45.7997 - val_loss: 46.8093 - val_MinusLogProbMetric: 46.8093 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 322/1000
2023-10-28 03:17:32.277 
Epoch 322/1000 
	 loss: 45.6517, MinusLogProbMetric: 45.6517, val_loss: 46.6552, val_MinusLogProbMetric: 46.6552

Epoch 322: val_loss did not improve from 46.56223
196/196 - 40s - loss: 45.6517 - MinusLogProbMetric: 45.6517 - val_loss: 46.6552 - val_MinusLogProbMetric: 46.6552 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 323/1000
2023-10-28 03:18:10.963 
Epoch 323/1000 
	 loss: 45.6795, MinusLogProbMetric: 45.6795, val_loss: 47.2214, val_MinusLogProbMetric: 47.2214

Epoch 323: val_loss did not improve from 46.56223
196/196 - 39s - loss: 45.6795 - MinusLogProbMetric: 45.6795 - val_loss: 47.2214 - val_MinusLogProbMetric: 47.2214 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 324/1000
2023-10-28 03:18:50.764 
Epoch 324/1000 
	 loss: 46.0278, MinusLogProbMetric: 46.0278, val_loss: 47.2089, val_MinusLogProbMetric: 47.2089

Epoch 324: val_loss did not improve from 46.56223
196/196 - 40s - loss: 46.0278 - MinusLogProbMetric: 46.0278 - val_loss: 47.2089 - val_MinusLogProbMetric: 47.2089 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 325/1000
2023-10-28 03:19:30.960 
Epoch 325/1000 
	 loss: 45.5268, MinusLogProbMetric: 45.5268, val_loss: 46.8615, val_MinusLogProbMetric: 46.8615

Epoch 325: val_loss did not improve from 46.56223
196/196 - 40s - loss: 45.5268 - MinusLogProbMetric: 45.5268 - val_loss: 46.8615 - val_MinusLogProbMetric: 46.8615 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 326/1000
2023-10-28 03:20:12.818 
Epoch 326/1000 
	 loss: 45.6900, MinusLogProbMetric: 45.6900, val_loss: 48.5531, val_MinusLogProbMetric: 48.5531

Epoch 326: val_loss did not improve from 46.56223
196/196 - 42s - loss: 45.6900 - MinusLogProbMetric: 45.6900 - val_loss: 48.5531 - val_MinusLogProbMetric: 48.5531 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 327/1000
2023-10-28 03:20:54.540 
Epoch 327/1000 
	 loss: 45.6197, MinusLogProbMetric: 45.6197, val_loss: 47.3058, val_MinusLogProbMetric: 47.3058

Epoch 327: val_loss did not improve from 46.56223
196/196 - 42s - loss: 45.6197 - MinusLogProbMetric: 45.6197 - val_loss: 47.3058 - val_MinusLogProbMetric: 47.3058 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 328/1000
2023-10-28 03:21:35.291 
Epoch 328/1000 
	 loss: 45.9698, MinusLogProbMetric: 45.9698, val_loss: 46.9131, val_MinusLogProbMetric: 46.9131

Epoch 328: val_loss did not improve from 46.56223
196/196 - 41s - loss: 45.9698 - MinusLogProbMetric: 45.9698 - val_loss: 46.9131 - val_MinusLogProbMetric: 46.9131 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 329/1000
2023-10-28 03:22:15.831 
Epoch 329/1000 
	 loss: 45.7125, MinusLogProbMetric: 45.7125, val_loss: 47.1874, val_MinusLogProbMetric: 47.1874

Epoch 329: val_loss did not improve from 46.56223
196/196 - 41s - loss: 45.7125 - MinusLogProbMetric: 45.7125 - val_loss: 47.1874 - val_MinusLogProbMetric: 47.1874 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 330/1000
2023-10-28 03:22:56.776 
Epoch 330/1000 
	 loss: 46.1683, MinusLogProbMetric: 46.1683, val_loss: 46.8426, val_MinusLogProbMetric: 46.8426

Epoch 330: val_loss did not improve from 46.56223
196/196 - 41s - loss: 46.1683 - MinusLogProbMetric: 46.1683 - val_loss: 46.8426 - val_MinusLogProbMetric: 46.8426 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 331/1000
2023-10-28 03:23:36.792 
Epoch 331/1000 
	 loss: 45.7266, MinusLogProbMetric: 45.7266, val_loss: 47.6990, val_MinusLogProbMetric: 47.6990

Epoch 331: val_loss did not improve from 46.56223
196/196 - 40s - loss: 45.7266 - MinusLogProbMetric: 45.7266 - val_loss: 47.6990 - val_MinusLogProbMetric: 47.6990 - lr: 1.2346e-05 - 40s/epoch - 204ms/step
Epoch 332/1000
2023-10-28 03:24:17.449 
Epoch 332/1000 
	 loss: 46.1015, MinusLogProbMetric: 46.1015, val_loss: 52.1301, val_MinusLogProbMetric: 52.1301

Epoch 332: val_loss did not improve from 46.56223
196/196 - 41s - loss: 46.1015 - MinusLogProbMetric: 46.1015 - val_loss: 52.1301 - val_MinusLogProbMetric: 52.1301 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 333/1000
2023-10-28 03:24:58.152 
Epoch 333/1000 
	 loss: 46.6283, MinusLogProbMetric: 46.6283, val_loss: 47.0766, val_MinusLogProbMetric: 47.0766

Epoch 333: val_loss did not improve from 46.56223
196/196 - 41s - loss: 46.6283 - MinusLogProbMetric: 46.6283 - val_loss: 47.0766 - val_MinusLogProbMetric: 47.0766 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 334/1000
2023-10-28 03:25:39.598 
Epoch 334/1000 
	 loss: 45.4880, MinusLogProbMetric: 45.4880, val_loss: 46.4130, val_MinusLogProbMetric: 46.4130

Epoch 334: val_loss improved from 46.56223 to 46.41303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 45.4880 - MinusLogProbMetric: 45.4880 - val_loss: 46.4130 - val_MinusLogProbMetric: 46.4130 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 335/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:26:21.387 
Epoch 335/1000 
	 loss: 46.0824, MinusLogProbMetric: 46.0824, val_loss: 46.5513, val_MinusLogProbMetric: 46.5512

Epoch 335: val_loss did not improve from 46.41303
196/196 - 41s - loss: 46.0824 - MinusLogProbMetric: 46.0824 - val_loss: 46.5513 - val_MinusLogProbMetric: 46.5512 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 336/1000
2023-10-28 03:27:02.286 
Epoch 336/1000 
	 loss: 45.3966, MinusLogProbMetric: 45.3966, val_loss: 47.0134, val_MinusLogProbMetric: 47.0134

Epoch 336: val_loss did not improve from 46.41303
196/196 - 41s - loss: 45.3966 - MinusLogProbMetric: 45.3966 - val_loss: 47.0134 - val_MinusLogProbMetric: 47.0134 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 337/1000
2023-10-28 03:27:44.392 
Epoch 337/1000 
	 loss: 46.0522, MinusLogProbMetric: 46.0522, val_loss: 46.7200, val_MinusLogProbMetric: 46.7200

Epoch 337: val_loss did not improve from 46.41303
196/196 - 42s - loss: 46.0522 - MinusLogProbMetric: 46.0522 - val_loss: 46.7200 - val_MinusLogProbMetric: 46.7200 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 338/1000
2023-10-28 03:28:26.064 
Epoch 338/1000 
	 loss: 45.6395, MinusLogProbMetric: 45.6395, val_loss: 46.6499, val_MinusLogProbMetric: 46.6499

Epoch 338: val_loss did not improve from 46.41303
196/196 - 42s - loss: 45.6395 - MinusLogProbMetric: 45.6395 - val_loss: 46.6499 - val_MinusLogProbMetric: 46.6499 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 339/1000
2023-10-28 03:29:07.143 
Epoch 339/1000 
	 loss: 47.0412, MinusLogProbMetric: 47.0412, val_loss: 48.0347, val_MinusLogProbMetric: 48.0347

Epoch 339: val_loss did not improve from 46.41303
196/196 - 41s - loss: 47.0412 - MinusLogProbMetric: 47.0412 - val_loss: 48.0347 - val_MinusLogProbMetric: 48.0347 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 340/1000
2023-10-28 03:29:48.113 
Epoch 340/1000 
	 loss: 45.7161, MinusLogProbMetric: 45.7161, val_loss: 46.3847, val_MinusLogProbMetric: 46.3847

Epoch 340: val_loss improved from 46.41303 to 46.38475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 45.7161 - MinusLogProbMetric: 45.7161 - val_loss: 46.3847 - val_MinusLogProbMetric: 46.3847 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 341/1000
2023-10-28 03:30:29.688 
Epoch 341/1000 
	 loss: 45.6246, MinusLogProbMetric: 45.6246, val_loss: 46.7311, val_MinusLogProbMetric: 46.7311

Epoch 341: val_loss did not improve from 46.38475
196/196 - 41s - loss: 45.6246 - MinusLogProbMetric: 45.6246 - val_loss: 46.7311 - val_MinusLogProbMetric: 46.7311 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 342/1000
2023-10-28 03:31:10.499 
Epoch 342/1000 
	 loss: 45.5483, MinusLogProbMetric: 45.5483, val_loss: 46.2179, val_MinusLogProbMetric: 46.2179

Epoch 342: val_loss improved from 46.38475 to 46.21793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 45.5483 - MinusLogProbMetric: 45.5483 - val_loss: 46.2179 - val_MinusLogProbMetric: 46.2179 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 343/1000
2023-10-28 03:31:51.385 
Epoch 343/1000 
	 loss: 45.6368, MinusLogProbMetric: 45.6368, val_loss: 47.7011, val_MinusLogProbMetric: 47.7011

Epoch 343: val_loss did not improve from 46.21793
196/196 - 40s - loss: 45.6368 - MinusLogProbMetric: 45.6368 - val_loss: 47.7011 - val_MinusLogProbMetric: 47.7011 - lr: 1.2346e-05 - 40s/epoch - 204ms/step
Epoch 344/1000
2023-10-28 03:32:32.406 
Epoch 344/1000 
	 loss: 45.6945, MinusLogProbMetric: 45.6945, val_loss: 46.9612, val_MinusLogProbMetric: 46.9612

Epoch 344: val_loss did not improve from 46.21793
196/196 - 41s - loss: 45.6945 - MinusLogProbMetric: 45.6945 - val_loss: 46.9612 - val_MinusLogProbMetric: 46.9612 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 345/1000
2023-10-28 03:33:14.554 
Epoch 345/1000 
	 loss: 45.4877, MinusLogProbMetric: 45.4877, val_loss: 46.5077, val_MinusLogProbMetric: 46.5077

Epoch 345: val_loss did not improve from 46.21793
196/196 - 42s - loss: 45.4877 - MinusLogProbMetric: 45.4877 - val_loss: 46.5077 - val_MinusLogProbMetric: 46.5077 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 346/1000
2023-10-28 03:33:56.041 
Epoch 346/1000 
	 loss: 45.7068, MinusLogProbMetric: 45.7068, val_loss: 46.8603, val_MinusLogProbMetric: 46.8603

Epoch 346: val_loss did not improve from 46.21793
196/196 - 41s - loss: 45.7068 - MinusLogProbMetric: 45.7068 - val_loss: 46.8603 - val_MinusLogProbMetric: 46.8603 - lr: 1.2346e-05 - 41s/epoch - 212ms/step
Epoch 347/1000
2023-10-28 03:34:38.227 
Epoch 347/1000 
	 loss: 45.6193, MinusLogProbMetric: 45.6193, val_loss: 46.5539, val_MinusLogProbMetric: 46.5539

Epoch 347: val_loss did not improve from 46.21793
196/196 - 42s - loss: 45.6193 - MinusLogProbMetric: 45.6193 - val_loss: 46.5539 - val_MinusLogProbMetric: 46.5539 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 348/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:35:20.725 
Epoch 348/1000 
	 loss: 45.7820, MinusLogProbMetric: 45.7820, val_loss: 48.0783, val_MinusLogProbMetric: 48.0782

Epoch 348: val_loss did not improve from 46.21793
196/196 - 42s - loss: 45.7820 - MinusLogProbMetric: 45.7820 - val_loss: 48.0783 - val_MinusLogProbMetric: 48.0782 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 349/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:36:02.890 
Epoch 349/1000 
	 loss: 45.7715, MinusLogProbMetric: 45.7715, val_loss: 47.1233, val_MinusLogProbMetric: 47.1233

Epoch 349: val_loss did not improve from 46.21793
196/196 - 42s - loss: 45.7715 - MinusLogProbMetric: 45.7715 - val_loss: 47.1233 - val_MinusLogProbMetric: 47.1233 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 350/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:36:43.757 
Epoch 350/1000 
	 loss: 45.6102, MinusLogProbMetric: 45.6102, val_loss: 46.5804, val_MinusLogProbMetric: 46.5804

Epoch 350: val_loss did not improve from 46.21793
196/196 - 41s - loss: 45.6102 - MinusLogProbMetric: 45.6102 - val_loss: 46.5804 - val_MinusLogProbMetric: 46.5804 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 351/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:37:26.363 
Epoch 351/1000 
	 loss: 45.8329, MinusLogProbMetric: 45.8329, val_loss: 46.1652, val_MinusLogProbMetric: 46.1652

Epoch 351: val_loss improved from 46.21793 to 46.16518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 45.8329 - MinusLogProbMetric: 45.8329 - val_loss: 46.1652 - val_MinusLogProbMetric: 46.1652 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 352/1000
2023-10-28 03:38:08.070 
Epoch 352/1000 
	 loss: 45.6264, MinusLogProbMetric: 45.6264, val_loss: 46.5365, val_MinusLogProbMetric: 46.5365

Epoch 352: val_loss did not improve from 46.16518
196/196 - 41s - loss: 45.6264 - MinusLogProbMetric: 45.6264 - val_loss: 46.5365 - val_MinusLogProbMetric: 46.5365 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 353/1000
2023-10-28 03:38:49.332 
Epoch 353/1000 
	 loss: 45.4617, MinusLogProbMetric: 45.4617, val_loss: 46.6809, val_MinusLogProbMetric: 46.6809

Epoch 353: val_loss did not improve from 46.16518
196/196 - 41s - loss: 45.4617 - MinusLogProbMetric: 45.4617 - val_loss: 46.6809 - val_MinusLogProbMetric: 46.6809 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 354/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 102: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 03:39:12.352 
Epoch 354/1000 
	 loss: nan, MinusLogProbMetric: 50.9567, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 354: val_loss did not improve from 46.16518
196/196 - 23s - loss: nan - MinusLogProbMetric: 50.9567 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 23s/epoch - 117ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 428.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_428/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_428
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_482"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_483 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7f3e017f4610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35d616b4f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35d616b4f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f39001717e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f374071d7e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f374071efb0>, <keras.callbacks.ModelCheckpoint object at 0x7f374071fa00>, <keras.callbacks.EarlyStopping object at 0x7f374071f8e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f374071de70>, <keras.callbacks.TerminateOnNaN object at 0x7f374071df60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 428/720 with hyperparameters:
timestamp = 2023-10-28 03:39:19.271670
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
2023-10-28 03:41:05.013 
Epoch 1/1000 
	 loss: 47.1470, MinusLogProbMetric: 47.1470, val_loss: 46.1979, val_MinusLogProbMetric: 46.1979

Epoch 1: val_loss improved from inf to 46.19790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 106s - loss: 47.1470 - MinusLogProbMetric: 47.1470 - val_loss: 46.1979 - val_MinusLogProbMetric: 46.1979 - lr: 4.1152e-06 - 106s/epoch - 543ms/step
Epoch 2/1000
2023-10-28 03:41:46.580 
Epoch 2/1000 
	 loss: 44.8349, MinusLogProbMetric: 44.8349, val_loss: 46.0147, val_MinusLogProbMetric: 46.0147

Epoch 2: val_loss improved from 46.19790 to 46.01472, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 44.8349 - MinusLogProbMetric: 44.8349 - val_loss: 46.0147 - val_MinusLogProbMetric: 46.0147 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 3/1000
2023-10-28 03:42:27.878 
Epoch 3/1000 
	 loss: 44.7984, MinusLogProbMetric: 44.7984, val_loss: 46.2127, val_MinusLogProbMetric: 46.2127

Epoch 3: val_loss did not improve from 46.01472
196/196 - 40s - loss: 44.7984 - MinusLogProbMetric: 44.7984 - val_loss: 46.2127 - val_MinusLogProbMetric: 46.2127 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 4/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:43:07.617 
Epoch 4/1000 
	 loss: 44.8657, MinusLogProbMetric: 44.8657, val_loss: 46.1768, val_MinusLogProbMetric: 46.1767

Epoch 4: val_loss did not improve from 46.01472
196/196 - 40s - loss: 44.8657 - MinusLogProbMetric: 44.8657 - val_loss: 46.1768 - val_MinusLogProbMetric: 46.1767 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:43:47.967 
Epoch 5/1000 
	 loss: 44.8015, MinusLogProbMetric: 44.8015, val_loss: 46.4759, val_MinusLogProbMetric: 46.4759

Epoch 5: val_loss did not improve from 46.01472
196/196 - 40s - loss: 44.8015 - MinusLogProbMetric: 44.8015 - val_loss: 46.4759 - val_MinusLogProbMetric: 46.4759 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 6/1000
2023-10-28 03:44:28.415 
Epoch 6/1000 
	 loss: 44.9409, MinusLogProbMetric: 44.9409, val_loss: 46.2888, val_MinusLogProbMetric: 46.2888

Epoch 6: val_loss did not improve from 46.01472
196/196 - 40s - loss: 44.9409 - MinusLogProbMetric: 44.9409 - val_loss: 46.2888 - val_MinusLogProbMetric: 46.2888 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:45:09.503 
Epoch 7/1000 
	 loss: 44.7953, MinusLogProbMetric: 44.7953, val_loss: 46.0367, val_MinusLogProbMetric: 46.0366

Epoch 7: val_loss did not improve from 46.01472
196/196 - 41s - loss: 44.7953 - MinusLogProbMetric: 44.7953 - val_loss: 46.0367 - val_MinusLogProbMetric: 46.0366 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 8/1000
2023-10-28 03:45:49.343 
Epoch 8/1000 
	 loss: 44.7985, MinusLogProbMetric: 44.7985, val_loss: 46.5656, val_MinusLogProbMetric: 46.5656

Epoch 8: val_loss did not improve from 46.01472
196/196 - 40s - loss: 44.7985 - MinusLogProbMetric: 44.7985 - val_loss: 46.5656 - val_MinusLogProbMetric: 46.5656 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 9/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:46:30.688 
Epoch 9/1000 
	 loss: 44.8939, MinusLogProbMetric: 44.8939, val_loss: 46.0876, val_MinusLogProbMetric: 46.0875

Epoch 9: val_loss did not improve from 46.01472
196/196 - 41s - loss: 44.8939 - MinusLogProbMetric: 44.8939 - val_loss: 46.0876 - val_MinusLogProbMetric: 46.0875 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 10/1000
2023-10-28 03:47:11.750 
Epoch 10/1000 
	 loss: 44.7951, MinusLogProbMetric: 44.7951, val_loss: 46.0385, val_MinusLogProbMetric: 46.0385

Epoch 10: val_loss did not improve from 46.01472
196/196 - 41s - loss: 44.7951 - MinusLogProbMetric: 44.7951 - val_loss: 46.0385 - val_MinusLogProbMetric: 46.0385 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 11/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:47:51.004 
Epoch 11/1000 
	 loss: 44.9371, MinusLogProbMetric: 44.9371, val_loss: 46.1703, val_MinusLogProbMetric: 46.1702

Epoch 11: val_loss did not improve from 46.01472
196/196 - 39s - loss: 44.9371 - MinusLogProbMetric: 44.9371 - val_loss: 46.1703 - val_MinusLogProbMetric: 46.1702 - lr: 4.1152e-06 - 39s/epoch - 200ms/step
Epoch 12/1000
2023-10-28 03:48:31.635 
Epoch 12/1000 
	 loss: 44.7487, MinusLogProbMetric: 44.7487, val_loss: 46.2586, val_MinusLogProbMetric: 46.2586

Epoch 12: val_loss did not improve from 46.01472
196/196 - 41s - loss: 44.7487 - MinusLogProbMetric: 44.7487 - val_loss: 46.2586 - val_MinusLogProbMetric: 46.2586 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 13/1000
2023-10-28 03:49:12.280 
Epoch 13/1000 
	 loss: 44.8127, MinusLogProbMetric: 44.8127, val_loss: 46.8494, val_MinusLogProbMetric: 46.8494

Epoch 13: val_loss did not improve from 46.01472
196/196 - 41s - loss: 44.8127 - MinusLogProbMetric: 44.8127 - val_loss: 46.8494 - val_MinusLogProbMetric: 46.8494 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 14/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:49:52.888 
Epoch 14/1000 
	 loss: 44.8317, MinusLogProbMetric: 44.8317, val_loss: 46.5912, val_MinusLogProbMetric: 46.5912

Epoch 14: val_loss did not improve from 46.01472
196/196 - 41s - loss: 44.8317 - MinusLogProbMetric: 44.8317 - val_loss: 46.5912 - val_MinusLogProbMetric: 46.5912 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 15/1000
2023-10-28 03:50:32.857 
Epoch 15/1000 
	 loss: 44.8093, MinusLogProbMetric: 44.8093, val_loss: 45.9772, val_MinusLogProbMetric: 45.9772

Epoch 15: val_loss improved from 46.01472 to 45.97719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 44.8093 - MinusLogProbMetric: 44.8093 - val_loss: 45.9772 - val_MinusLogProbMetric: 45.9772 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 16/1000
2023-10-28 03:51:13.898 
Epoch 16/1000 
	 loss: 44.8997, MinusLogProbMetric: 44.8997, val_loss: 46.2068, val_MinusLogProbMetric: 46.2068

Epoch 16: val_loss did not improve from 45.97719
196/196 - 40s - loss: 44.8997 - MinusLogProbMetric: 44.8997 - val_loss: 46.2068 - val_MinusLogProbMetric: 46.2068 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 17/1000
2023-10-28 03:51:53.611 
Epoch 17/1000 
	 loss: 44.7782, MinusLogProbMetric: 44.7782, val_loss: 46.0527, val_MinusLogProbMetric: 46.0527

Epoch 17: val_loss did not improve from 45.97719
196/196 - 40s - loss: 44.7782 - MinusLogProbMetric: 44.7782 - val_loss: 46.0527 - val_MinusLogProbMetric: 46.0527 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 18/1000
2023-10-28 03:52:33.634 
Epoch 18/1000 
	 loss: 44.8633, MinusLogProbMetric: 44.8633, val_loss: 46.0385, val_MinusLogProbMetric: 46.0385

Epoch 18: val_loss did not improve from 45.97719
196/196 - 40s - loss: 44.8633 - MinusLogProbMetric: 44.8633 - val_loss: 46.0385 - val_MinusLogProbMetric: 46.0385 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 19/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:53:13.751 
Epoch 19/1000 
	 loss: 44.7177, MinusLogProbMetric: 44.7177, val_loss: 46.2193, val_MinusLogProbMetric: 46.2193

Epoch 19: val_loss did not improve from 45.97719
196/196 - 40s - loss: 44.7177 - MinusLogProbMetric: 44.7177 - val_loss: 46.2193 - val_MinusLogProbMetric: 46.2193 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 20/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:53:53.694 
Epoch 20/1000 
	 loss: 44.7082, MinusLogProbMetric: 44.7082, val_loss: 46.1094, val_MinusLogProbMetric: 46.1094

Epoch 20: val_loss did not improve from 45.97719
196/196 - 40s - loss: 44.7082 - MinusLogProbMetric: 44.7082 - val_loss: 46.1094 - val_MinusLogProbMetric: 46.1094 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 21/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:54:34.486 
Epoch 21/1000 
	 loss: 44.7627, MinusLogProbMetric: 44.7627, val_loss: 46.1551, val_MinusLogProbMetric: 46.1551

Epoch 21: val_loss did not improve from 45.97719
196/196 - 41s - loss: 44.7627 - MinusLogProbMetric: 44.7627 - val_loss: 46.1551 - val_MinusLogProbMetric: 46.1551 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 22/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:55:15.674 
Epoch 22/1000 
	 loss: 44.7257, MinusLogProbMetric: 44.7257, val_loss: 46.0320, val_MinusLogProbMetric: 46.0320

Epoch 22: val_loss did not improve from 45.97719
196/196 - 41s - loss: 44.7257 - MinusLogProbMetric: 44.7257 - val_loss: 46.0320 - val_MinusLogProbMetric: 46.0320 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 23/1000
2023-10-28 03:55:57.726 
Epoch 23/1000 
	 loss: 44.6940, MinusLogProbMetric: 44.6940, val_loss: 46.0297, val_MinusLogProbMetric: 46.0297

Epoch 23: val_loss did not improve from 45.97719
196/196 - 42s - loss: 44.6940 - MinusLogProbMetric: 44.6940 - val_loss: 46.0297 - val_MinusLogProbMetric: 46.0297 - lr: 4.1152e-06 - 42s/epoch - 215ms/step
Epoch 24/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:56:37.635 
Epoch 24/1000 
	 loss: 44.7281, MinusLogProbMetric: 44.7281, val_loss: 46.4639, val_MinusLogProbMetric: 46.4639

Epoch 24: val_loss did not improve from 45.97719
196/196 - 40s - loss: 44.7281 - MinusLogProbMetric: 44.7281 - val_loss: 46.4639 - val_MinusLogProbMetric: 46.4639 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 25/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:57:18.248 
Epoch 25/1000 
	 loss: 44.9717, MinusLogProbMetric: 44.9717, val_loss: 46.3821, val_MinusLogProbMetric: 46.3821

Epoch 25: val_loss did not improve from 45.97719
196/196 - 41s - loss: 44.9717 - MinusLogProbMetric: 44.9717 - val_loss: 46.3821 - val_MinusLogProbMetric: 46.3821 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 26/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:57:59.234 
Epoch 26/1000 
	 loss: 44.6569, MinusLogProbMetric: 44.6569, val_loss: 45.7810, val_MinusLogProbMetric: 45.7810

Epoch 26: val_loss improved from 45.97719 to 45.78103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 44.6569 - MinusLogProbMetric: 44.6569 - val_loss: 45.7810 - val_MinusLogProbMetric: 45.7810 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 27/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:58:41.435 
Epoch 27/1000 
	 loss: 44.6782, MinusLogProbMetric: 44.6782, val_loss: 45.9269, val_MinusLogProbMetric: 45.9269

Epoch 27: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.6782 - MinusLogProbMetric: 44.6782 - val_loss: 45.9269 - val_MinusLogProbMetric: 45.9269 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 28/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 03:59:23.342 
Epoch 28/1000 
	 loss: 44.6330, MinusLogProbMetric: 44.6330, val_loss: 45.9926, val_MinusLogProbMetric: 45.9926

Epoch 28: val_loss did not improve from 45.78103
196/196 - 42s - loss: 44.6330 - MinusLogProbMetric: 44.6330 - val_loss: 45.9926 - val_MinusLogProbMetric: 45.9926 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 29/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:00:03.895 
Epoch 29/1000 
	 loss: 44.6685, MinusLogProbMetric: 44.6685, val_loss: 45.8506, val_MinusLogProbMetric: 45.8506

Epoch 29: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.6685 - MinusLogProbMetric: 44.6685 - val_loss: 45.8506 - val_MinusLogProbMetric: 45.8506 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 30/1000
2023-10-28 04:00:45.191 
Epoch 30/1000 
	 loss: 44.6377, MinusLogProbMetric: 44.6377, val_loss: 45.9804, val_MinusLogProbMetric: 45.9804

Epoch 30: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.6377 - MinusLogProbMetric: 44.6377 - val_loss: 45.9804 - val_MinusLogProbMetric: 45.9804 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 31/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:01:25.792 
Epoch 31/1000 
	 loss: 44.7113, MinusLogProbMetric: 44.7113, val_loss: 46.1209, val_MinusLogProbMetric: 46.1208

Epoch 31: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.7113 - MinusLogProbMetric: 44.7113 - val_loss: 46.1209 - val_MinusLogProbMetric: 46.1208 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 32/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:02:06.450 
Epoch 32/1000 
	 loss: 44.9901, MinusLogProbMetric: 44.9901, val_loss: 45.9187, val_MinusLogProbMetric: 45.9187

Epoch 32: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.9901 - MinusLogProbMetric: 44.9901 - val_loss: 45.9187 - val_MinusLogProbMetric: 45.9187 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 33/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:02:46.683 
Epoch 33/1000 
	 loss: 44.5804, MinusLogProbMetric: 44.5804, val_loss: 46.0628, val_MinusLogProbMetric: 46.0628

Epoch 33: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.5804 - MinusLogProbMetric: 44.5804 - val_loss: 46.0628 - val_MinusLogProbMetric: 46.0628 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 34/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:03:27.068 
Epoch 34/1000 
	 loss: 44.6153, MinusLogProbMetric: 44.6153, val_loss: 45.8247, val_MinusLogProbMetric: 45.8247

Epoch 34: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.6153 - MinusLogProbMetric: 44.6153 - val_loss: 45.8247 - val_MinusLogProbMetric: 45.8247 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 35/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:04:07.261 
Epoch 35/1000 
	 loss: 44.5965, MinusLogProbMetric: 44.5965, val_loss: 46.2898, val_MinusLogProbMetric: 46.2898

Epoch 35: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.5965 - MinusLogProbMetric: 44.5965 - val_loss: 46.2898 - val_MinusLogProbMetric: 46.2898 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 36/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:04:47.683 
Epoch 36/1000 
	 loss: 44.7860, MinusLogProbMetric: 44.7860, val_loss: 46.3185, val_MinusLogProbMetric: 46.3184

Epoch 36: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.7860 - MinusLogProbMetric: 44.7860 - val_loss: 46.3185 - val_MinusLogProbMetric: 46.3184 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 37/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:05:29.068 
Epoch 37/1000 
	 loss: 44.5958, MinusLogProbMetric: 44.5958, val_loss: 46.2323, val_MinusLogProbMetric: 46.2322

Epoch 37: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.5958 - MinusLogProbMetric: 44.5958 - val_loss: 46.2323 - val_MinusLogProbMetric: 46.2322 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 38/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:06:09.783 
Epoch 38/1000 
	 loss: 44.6269, MinusLogProbMetric: 44.6269, val_loss: 46.2436, val_MinusLogProbMetric: 46.2435

Epoch 38: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.6269 - MinusLogProbMetric: 44.6269 - val_loss: 46.2436 - val_MinusLogProbMetric: 46.2435 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 39/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:06:50.528 
Epoch 39/1000 
	 loss: 44.6272, MinusLogProbMetric: 44.6272, val_loss: 46.1585, val_MinusLogProbMetric: 46.1585

Epoch 39: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.6272 - MinusLogProbMetric: 44.6272 - val_loss: 46.1585 - val_MinusLogProbMetric: 46.1585 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 40/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:07:30.627 
Epoch 40/1000 
	 loss: 44.5984, MinusLogProbMetric: 44.5984, val_loss: 46.0222, val_MinusLogProbMetric: 46.0222

Epoch 40: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.5984 - MinusLogProbMetric: 44.5984 - val_loss: 46.0222 - val_MinusLogProbMetric: 46.0222 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 41/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:08:10.895 
Epoch 41/1000 
	 loss: 44.6112, MinusLogProbMetric: 44.6112, val_loss: 46.6967, val_MinusLogProbMetric: 46.6967

Epoch 41: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.6112 - MinusLogProbMetric: 44.6112 - val_loss: 46.6967 - val_MinusLogProbMetric: 46.6967 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 42/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:08:50.825 
Epoch 42/1000 
	 loss: 44.6304, MinusLogProbMetric: 44.6304, val_loss: 46.0604, val_MinusLogProbMetric: 46.0603

Epoch 42: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.6304 - MinusLogProbMetric: 44.6304 - val_loss: 46.0604 - val_MinusLogProbMetric: 46.0603 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 43/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:09:31.256 
Epoch 43/1000 
	 loss: 44.5669, MinusLogProbMetric: 44.5669, val_loss: 45.7947, val_MinusLogProbMetric: 45.7947

Epoch 43: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.5669 - MinusLogProbMetric: 44.5669 - val_loss: 45.7947 - val_MinusLogProbMetric: 45.7947 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 44/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:10:11.938 
Epoch 44/1000 
	 loss: 44.5422, MinusLogProbMetric: 44.5422, val_loss: 46.2616, val_MinusLogProbMetric: 46.2616

Epoch 44: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.5422 - MinusLogProbMetric: 44.5422 - val_loss: 46.2616 - val_MinusLogProbMetric: 46.2616 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 45/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:10:52.788 
Epoch 45/1000 
	 loss: 44.5999, MinusLogProbMetric: 44.5999, val_loss: 45.9261, val_MinusLogProbMetric: 45.9261

Epoch 45: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.5999 - MinusLogProbMetric: 44.5999 - val_loss: 45.9261 - val_MinusLogProbMetric: 45.9261 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 46/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:11:33.886 
Epoch 46/1000 
	 loss: 44.6002, MinusLogProbMetric: 44.6002, val_loss: 46.0387, val_MinusLogProbMetric: 46.0387

Epoch 46: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.6002 - MinusLogProbMetric: 44.6002 - val_loss: 46.0387 - val_MinusLogProbMetric: 46.0387 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 47/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:12:14.013 
Epoch 47/1000 
	 loss: 44.5800, MinusLogProbMetric: 44.5800, val_loss: 46.1667, val_MinusLogProbMetric: 46.1667

Epoch 47: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.5800 - MinusLogProbMetric: 44.5800 - val_loss: 46.1667 - val_MinusLogProbMetric: 46.1667 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 48/1000
2023-10-28 04:12:54.692 
Epoch 48/1000 
	 loss: 44.6253, MinusLogProbMetric: 44.6253, val_loss: 45.8738, val_MinusLogProbMetric: 45.8738

Epoch 48: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.6253 - MinusLogProbMetric: 44.6253 - val_loss: 45.8738 - val_MinusLogProbMetric: 45.8738 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 49/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:13:35.151 
Epoch 49/1000 
	 loss: 44.5217, MinusLogProbMetric: 44.5217, val_loss: 45.9690, val_MinusLogProbMetric: 45.9690

Epoch 49: val_loss did not improve from 45.78103
196/196 - 40s - loss: 44.5217 - MinusLogProbMetric: 44.5217 - val_loss: 45.9690 - val_MinusLogProbMetric: 45.9690 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 50/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:14:16.553 
Epoch 50/1000 
	 loss: 44.5414, MinusLogProbMetric: 44.5414, val_loss: 45.7988, val_MinusLogProbMetric: 45.7988

Epoch 50: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.5414 - MinusLogProbMetric: 44.5414 - val_loss: 45.7988 - val_MinusLogProbMetric: 45.7988 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 51/1000
2023-10-28 04:14:57.474 
Epoch 51/1000 
	 loss: 44.5123, MinusLogProbMetric: 44.5123, val_loss: 46.0848, val_MinusLogProbMetric: 46.0848

Epoch 51: val_loss did not improve from 45.78103
196/196 - 41s - loss: 44.5123 - MinusLogProbMetric: 44.5123 - val_loss: 46.0848 - val_MinusLogProbMetric: 46.0848 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 52/1000
2023-10-28 04:15:38.846 
Epoch 52/1000 
	 loss: 44.5132, MinusLogProbMetric: 44.5132, val_loss: 45.7451, val_MinusLogProbMetric: 45.7451

Epoch 52: val_loss improved from 45.78103 to 45.74508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 44.5132 - MinusLogProbMetric: 44.5132 - val_loss: 45.7451 - val_MinusLogProbMetric: 45.7451 - lr: 4.1152e-06 - 42s/epoch - 215ms/step
Epoch 53/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:16:18.013 
Epoch 53/1000 
	 loss: 44.4671, MinusLogProbMetric: 44.4671, val_loss: 46.1180, val_MinusLogProbMetric: 46.1179

Epoch 53: val_loss did not improve from 45.74508
196/196 - 38s - loss: 44.4671 - MinusLogProbMetric: 44.4671 - val_loss: 46.1180 - val_MinusLogProbMetric: 46.1179 - lr: 4.1152e-06 - 38s/epoch - 196ms/step
Epoch 54/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:16:56.310 
Epoch 54/1000 
	 loss: 44.5493, MinusLogProbMetric: 44.5493, val_loss: 46.3665, val_MinusLogProbMetric: 46.3664

Epoch 54: val_loss did not improve from 45.74508
196/196 - 38s - loss: 44.5493 - MinusLogProbMetric: 44.5493 - val_loss: 46.3665 - val_MinusLogProbMetric: 46.3664 - lr: 4.1152e-06 - 38s/epoch - 195ms/step
Epoch 55/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:17:36.844 
Epoch 55/1000 
	 loss: 44.5554, MinusLogProbMetric: 44.5554, val_loss: 46.0699, val_MinusLogProbMetric: 46.0699

Epoch 55: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.5554 - MinusLogProbMetric: 44.5554 - val_loss: 46.0699 - val_MinusLogProbMetric: 46.0699 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 56/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:18:17.412 
Epoch 56/1000 
	 loss: 44.7062, MinusLogProbMetric: 44.7062, val_loss: 45.8577, val_MinusLogProbMetric: 45.8577

Epoch 56: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.7062 - MinusLogProbMetric: 44.7062 - val_loss: 45.8577 - val_MinusLogProbMetric: 45.8577 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 57/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:18:58.284 
Epoch 57/1000 
	 loss: 44.4985, MinusLogProbMetric: 44.4985, val_loss: 46.0767, val_MinusLogProbMetric: 46.0766

Epoch 57: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.4985 - MinusLogProbMetric: 44.4985 - val_loss: 46.0767 - val_MinusLogProbMetric: 46.0766 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 58/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:19:38.480 
Epoch 58/1000 
	 loss: 44.5629, MinusLogProbMetric: 44.5629, val_loss: 46.5906, val_MinusLogProbMetric: 46.5906

Epoch 58: val_loss did not improve from 45.74508
196/196 - 40s - loss: 44.5629 - MinusLogProbMetric: 44.5629 - val_loss: 46.5906 - val_MinusLogProbMetric: 46.5906 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 59/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:20:19.063 
Epoch 59/1000 
	 loss: 44.5743, MinusLogProbMetric: 44.5743, val_loss: 46.0496, val_MinusLogProbMetric: 46.0496

Epoch 59: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.5743 - MinusLogProbMetric: 44.5743 - val_loss: 46.0496 - val_MinusLogProbMetric: 46.0496 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 60/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:21:00.127 
Epoch 60/1000 
	 loss: 44.5935, MinusLogProbMetric: 44.5935, val_loss: 46.0396, val_MinusLogProbMetric: 46.0395

Epoch 60: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.5935 - MinusLogProbMetric: 44.5935 - val_loss: 46.0396 - val_MinusLogProbMetric: 46.0395 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 61/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:21:41.784 
Epoch 61/1000 
	 loss: 44.4092, MinusLogProbMetric: 44.4092, val_loss: 46.0370, val_MinusLogProbMetric: 46.0370

Epoch 61: val_loss did not improve from 45.74508
196/196 - 42s - loss: 44.4092 - MinusLogProbMetric: 44.4092 - val_loss: 46.0370 - val_MinusLogProbMetric: 46.0370 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 62/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:22:22.862 
Epoch 62/1000 
	 loss: 44.4332, MinusLogProbMetric: 44.4332, val_loss: 45.7621, val_MinusLogProbMetric: 45.7621

Epoch 62: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.4332 - MinusLogProbMetric: 44.4332 - val_loss: 45.7621 - val_MinusLogProbMetric: 45.7621 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 63/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:23:02.813 
Epoch 63/1000 
	 loss: 44.4307, MinusLogProbMetric: 44.4307, val_loss: 46.1860, val_MinusLogProbMetric: 46.1860

Epoch 63: val_loss did not improve from 45.74508
196/196 - 40s - loss: 44.4307 - MinusLogProbMetric: 44.4307 - val_loss: 46.1860 - val_MinusLogProbMetric: 46.1860 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 64/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:23:43.154 
Epoch 64/1000 
	 loss: 44.5464, MinusLogProbMetric: 44.5464, val_loss: 46.1721, val_MinusLogProbMetric: 46.1721

Epoch 64: val_loss did not improve from 45.74508
196/196 - 40s - loss: 44.5464 - MinusLogProbMetric: 44.5464 - val_loss: 46.1721 - val_MinusLogProbMetric: 46.1721 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 65/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:24:23.397 
Epoch 65/1000 
	 loss: 44.4625, MinusLogProbMetric: 44.4625, val_loss: 46.0658, val_MinusLogProbMetric: 46.0658

Epoch 65: val_loss did not improve from 45.74508
196/196 - 40s - loss: 44.4625 - MinusLogProbMetric: 44.4625 - val_loss: 46.0658 - val_MinusLogProbMetric: 46.0658 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 66/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:25:04.922 
Epoch 66/1000 
	 loss: 44.4556, MinusLogProbMetric: 44.4556, val_loss: 46.2149, val_MinusLogProbMetric: 46.2149

Epoch 66: val_loss did not improve from 45.74508
196/196 - 42s - loss: 44.4556 - MinusLogProbMetric: 44.4556 - val_loss: 46.2149 - val_MinusLogProbMetric: 46.2149 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 67/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:25:46.174 
Epoch 67/1000 
	 loss: 44.4489, MinusLogProbMetric: 44.4489, val_loss: 46.2556, val_MinusLogProbMetric: 46.2556

Epoch 67: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.4489 - MinusLogProbMetric: 44.4489 - val_loss: 46.2556 - val_MinusLogProbMetric: 46.2556 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 68/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:26:26.703 
Epoch 68/1000 
	 loss: 44.5611, MinusLogProbMetric: 44.5611, val_loss: 45.8497, val_MinusLogProbMetric: 45.8497

Epoch 68: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.5611 - MinusLogProbMetric: 44.5611 - val_loss: 45.8497 - val_MinusLogProbMetric: 45.8497 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 69/1000
2023-10-28 04:27:06.972 
Epoch 69/1000 
	 loss: 44.4171, MinusLogProbMetric: 44.4171, val_loss: 45.7825, val_MinusLogProbMetric: 45.7825

Epoch 69: val_loss did not improve from 45.74508
196/196 - 40s - loss: 44.4171 - MinusLogProbMetric: 44.4171 - val_loss: 45.7825 - val_MinusLogProbMetric: 45.7825 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 70/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:27:48.858 
Epoch 70/1000 
	 loss: 44.4226, MinusLogProbMetric: 44.4226, val_loss: 45.9154, val_MinusLogProbMetric: 45.9153

Epoch 70: val_loss did not improve from 45.74508
196/196 - 42s - loss: 44.4226 - MinusLogProbMetric: 44.4226 - val_loss: 45.9154 - val_MinusLogProbMetric: 45.9153 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 71/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:28:29.690 
Epoch 71/1000 
	 loss: 44.3826, MinusLogProbMetric: 44.3826, val_loss: 46.0528, val_MinusLogProbMetric: 46.0528

Epoch 71: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.3826 - MinusLogProbMetric: 44.3826 - val_loss: 46.0528 - val_MinusLogProbMetric: 46.0528 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 72/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:29:08.937 
Epoch 72/1000 
	 loss: 44.5444, MinusLogProbMetric: 44.5444, val_loss: 45.8579, val_MinusLogProbMetric: 45.8579

Epoch 72: val_loss did not improve from 45.74508
196/196 - 39s - loss: 44.5444 - MinusLogProbMetric: 44.5444 - val_loss: 45.8579 - val_MinusLogProbMetric: 45.8579 - lr: 4.1152e-06 - 39s/epoch - 200ms/step
Epoch 73/1000
2023-10-28 04:29:48.298 
Epoch 73/1000 
	 loss: 44.3636, MinusLogProbMetric: 44.3636, val_loss: 45.9600, val_MinusLogProbMetric: 45.9600

Epoch 73: val_loss did not improve from 45.74508
196/196 - 39s - loss: 44.3636 - MinusLogProbMetric: 44.3636 - val_loss: 45.9600 - val_MinusLogProbMetric: 45.9600 - lr: 4.1152e-06 - 39s/epoch - 201ms/step
Epoch 74/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:30:29.715 
Epoch 74/1000 
	 loss: 44.3713, MinusLogProbMetric: 44.3713, val_loss: 46.3560, val_MinusLogProbMetric: 46.3560

Epoch 74: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.3713 - MinusLogProbMetric: 44.3713 - val_loss: 46.3560 - val_MinusLogProbMetric: 46.3560 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 75/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:31:10.803 
Epoch 75/1000 
	 loss: 44.4665, MinusLogProbMetric: 44.4665, val_loss: 45.8433, val_MinusLogProbMetric: 45.8432

Epoch 75: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.4665 - MinusLogProbMetric: 44.4665 - val_loss: 45.8433 - val_MinusLogProbMetric: 45.8432 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 76/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:31:51.790 
Epoch 76/1000 
	 loss: 44.3785, MinusLogProbMetric: 44.3785, val_loss: 45.9670, val_MinusLogProbMetric: 45.9670

Epoch 76: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.3785 - MinusLogProbMetric: 44.3785 - val_loss: 45.9670 - val_MinusLogProbMetric: 45.9670 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 77/1000
2023-10-28 04:32:32.444 
Epoch 77/1000 
	 loss: 44.3690, MinusLogProbMetric: 44.3690, val_loss: 45.7658, val_MinusLogProbMetric: 45.7658

Epoch 77: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.3690 - MinusLogProbMetric: 44.3690 - val_loss: 45.7658 - val_MinusLogProbMetric: 45.7658 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 78/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:33:13.254 
Epoch 78/1000 
	 loss: 44.4673, MinusLogProbMetric: 44.4673, val_loss: 45.8678, val_MinusLogProbMetric: 45.8678

Epoch 78: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.4673 - MinusLogProbMetric: 44.4673 - val_loss: 45.8678 - val_MinusLogProbMetric: 45.8678 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 79/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:33:54.234 
Epoch 79/1000 
	 loss: 44.5301, MinusLogProbMetric: 44.5301, val_loss: 45.7685, val_MinusLogProbMetric: 45.7685

Epoch 79: val_loss did not improve from 45.74508
196/196 - 41s - loss: 44.5301 - MinusLogProbMetric: 44.5301 - val_loss: 45.7685 - val_MinusLogProbMetric: 45.7685 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 80/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:34:34.654 
Epoch 80/1000 
	 loss: 44.6180, MinusLogProbMetric: 44.6180, val_loss: 45.6185, val_MinusLogProbMetric: 45.6185

Epoch 80: val_loss improved from 45.74508 to 45.61848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 44.6180 - MinusLogProbMetric: 44.6180 - val_loss: 45.6185 - val_MinusLogProbMetric: 45.6185 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 81/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:35:15.606 
Epoch 81/1000 
	 loss: 44.3566, MinusLogProbMetric: 44.3566, val_loss: 45.8752, val_MinusLogProbMetric: 45.8751

Epoch 81: val_loss did not improve from 45.61848
196/196 - 40s - loss: 44.3566 - MinusLogProbMetric: 44.3566 - val_loss: 45.8752 - val_MinusLogProbMetric: 45.8751 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 82/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:35:55.430 
Epoch 82/1000 
	 loss: 44.3650, MinusLogProbMetric: 44.3650, val_loss: 45.8046, val_MinusLogProbMetric: 45.8045

Epoch 82: val_loss did not improve from 45.61848
196/196 - 40s - loss: 44.3650 - MinusLogProbMetric: 44.3650 - val_loss: 45.8046 - val_MinusLogProbMetric: 45.8045 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 83/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:36:35.838 
Epoch 83/1000 
	 loss: 44.3112, MinusLogProbMetric: 44.3112, val_loss: 46.3303, val_MinusLogProbMetric: 46.3302

Epoch 83: val_loss did not improve from 45.61848
196/196 - 40s - loss: 44.3112 - MinusLogProbMetric: 44.3112 - val_loss: 46.3303 - val_MinusLogProbMetric: 46.3302 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 84/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:37:16.499 
Epoch 84/1000 
	 loss: 44.5154, MinusLogProbMetric: 44.5154, val_loss: 45.8116, val_MinusLogProbMetric: 45.8115

Epoch 84: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.5154 - MinusLogProbMetric: 44.5154 - val_loss: 45.8116 - val_MinusLogProbMetric: 45.8115 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 85/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:37:58.160 
Epoch 85/1000 
	 loss: 44.3466, MinusLogProbMetric: 44.3466, val_loss: 45.8240, val_MinusLogProbMetric: 45.8240

Epoch 85: val_loss did not improve from 45.61848
196/196 - 42s - loss: 44.3466 - MinusLogProbMetric: 44.3466 - val_loss: 45.8240 - val_MinusLogProbMetric: 45.8240 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 86/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:38:40.099 
Epoch 86/1000 
	 loss: 44.6031, MinusLogProbMetric: 44.6031, val_loss: 46.7287, val_MinusLogProbMetric: 46.7287

Epoch 86: val_loss did not improve from 45.61848
196/196 - 42s - loss: 44.6031 - MinusLogProbMetric: 44.6031 - val_loss: 46.7287 - val_MinusLogProbMetric: 46.7287 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 87/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:39:21.744 
Epoch 87/1000 
	 loss: 44.4160, MinusLogProbMetric: 44.4160, val_loss: 45.8440, val_MinusLogProbMetric: 45.8439

Epoch 87: val_loss did not improve from 45.61848
196/196 - 42s - loss: 44.4160 - MinusLogProbMetric: 44.4160 - val_loss: 45.8440 - val_MinusLogProbMetric: 45.8439 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 88/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:40:03.327 
Epoch 88/1000 
	 loss: 44.3361, MinusLogProbMetric: 44.3361, val_loss: 45.8782, val_MinusLogProbMetric: 45.8781

Epoch 88: val_loss did not improve from 45.61848
196/196 - 42s - loss: 44.3361 - MinusLogProbMetric: 44.3361 - val_loss: 45.8782 - val_MinusLogProbMetric: 45.8781 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 89/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:40:44.976 
Epoch 89/1000 
	 loss: 44.3069, MinusLogProbMetric: 44.3069, val_loss: 45.9379, val_MinusLogProbMetric: 45.9379

Epoch 89: val_loss did not improve from 45.61848
196/196 - 42s - loss: 44.3069 - MinusLogProbMetric: 44.3069 - val_loss: 45.9379 - val_MinusLogProbMetric: 45.9379 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 90/1000
2023-10-28 04:41:25.776 
Epoch 90/1000 
	 loss: 44.5681, MinusLogProbMetric: 44.5681, val_loss: 46.1281, val_MinusLogProbMetric: 46.1281

Epoch 90: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.5681 - MinusLogProbMetric: 44.5681 - val_loss: 46.1281 - val_MinusLogProbMetric: 46.1281 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 91/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:42:06.767 
Epoch 91/1000 
	 loss: 44.2983, MinusLogProbMetric: 44.2983, val_loss: 45.6991, val_MinusLogProbMetric: 45.6991

Epoch 91: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.2983 - MinusLogProbMetric: 44.2983 - val_loss: 45.6991 - val_MinusLogProbMetric: 45.6991 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 92/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:42:47.986 
Epoch 92/1000 
	 loss: 44.2505, MinusLogProbMetric: 44.2505, val_loss: 45.7233, val_MinusLogProbMetric: 45.7232

Epoch 92: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.2505 - MinusLogProbMetric: 44.2505 - val_loss: 45.7233 - val_MinusLogProbMetric: 45.7232 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 93/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:43:29.364 
Epoch 93/1000 
	 loss: 44.3343, MinusLogProbMetric: 44.3343, val_loss: 46.0840, val_MinusLogProbMetric: 46.0839

Epoch 93: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.3343 - MinusLogProbMetric: 44.3343 - val_loss: 46.0840 - val_MinusLogProbMetric: 46.0839 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 94/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:44:10.914 
Epoch 94/1000 
	 loss: 47.2582, MinusLogProbMetric: 47.2582, val_loss: 47.5215, val_MinusLogProbMetric: 47.5214

Epoch 94: val_loss did not improve from 45.61848
196/196 - 42s - loss: 47.2582 - MinusLogProbMetric: 47.2582 - val_loss: 47.5215 - val_MinusLogProbMetric: 47.5214 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 95/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:44:51.909 
Epoch 95/1000 
	 loss: 45.6065, MinusLogProbMetric: 45.6065, val_loss: 46.8396, val_MinusLogProbMetric: 46.8396

Epoch 95: val_loss did not improve from 45.61848
196/196 - 41s - loss: 45.6065 - MinusLogProbMetric: 45.6065 - val_loss: 46.8396 - val_MinusLogProbMetric: 46.8396 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 96/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:45:32.838 
Epoch 96/1000 
	 loss: 45.6493, MinusLogProbMetric: 45.6493, val_loss: 46.7826, val_MinusLogProbMetric: 46.7826

Epoch 96: val_loss did not improve from 45.61848
196/196 - 41s - loss: 45.6493 - MinusLogProbMetric: 45.6493 - val_loss: 46.7826 - val_MinusLogProbMetric: 46.7826 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 97/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:46:14.218 
Epoch 97/1000 
	 loss: 45.3234, MinusLogProbMetric: 45.3234, val_loss: 46.5578, val_MinusLogProbMetric: 46.5578

Epoch 97: val_loss did not improve from 45.61848
196/196 - 41s - loss: 45.3234 - MinusLogProbMetric: 45.3234 - val_loss: 46.5578 - val_MinusLogProbMetric: 46.5578 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 98/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:46:54.820 
Epoch 98/1000 
	 loss: 45.2410, MinusLogProbMetric: 45.2410, val_loss: 46.6123, val_MinusLogProbMetric: 46.6123

Epoch 98: val_loss did not improve from 45.61848
196/196 - 41s - loss: 45.2410 - MinusLogProbMetric: 45.2410 - val_loss: 46.6123 - val_MinusLogProbMetric: 46.6123 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 99/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:47:35.505 
Epoch 99/1000 
	 loss: 45.0747, MinusLogProbMetric: 45.0747, val_loss: 46.5630, val_MinusLogProbMetric: 46.5629

Epoch 99: val_loss did not improve from 45.61848
196/196 - 41s - loss: 45.0747 - MinusLogProbMetric: 45.0747 - val_loss: 46.5630 - val_MinusLogProbMetric: 46.5629 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 100/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:48:16.063 
Epoch 100/1000 
	 loss: 45.1648, MinusLogProbMetric: 45.1648, val_loss: 46.4220, val_MinusLogProbMetric: 46.4219

Epoch 100: val_loss did not improve from 45.61848
196/196 - 41s - loss: 45.1648 - MinusLogProbMetric: 45.1648 - val_loss: 46.4220 - val_MinusLogProbMetric: 46.4219 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 101/1000
2023-10-28 04:48:56.991 
Epoch 101/1000 
	 loss: 44.8572, MinusLogProbMetric: 44.8572, val_loss: 46.3824, val_MinusLogProbMetric: 46.3824

Epoch 101: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.8572 - MinusLogProbMetric: 44.8572 - val_loss: 46.3824 - val_MinusLogProbMetric: 46.3824 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 102/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:49:37.596 
Epoch 102/1000 
	 loss: 44.7594, MinusLogProbMetric: 44.7594, val_loss: 46.3846, val_MinusLogProbMetric: 46.3846

Epoch 102: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.7594 - MinusLogProbMetric: 44.7594 - val_loss: 46.3846 - val_MinusLogProbMetric: 46.3846 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 103/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:50:17.953 
Epoch 103/1000 
	 loss: 44.6382, MinusLogProbMetric: 44.6382, val_loss: 45.9322, val_MinusLogProbMetric: 45.9322

Epoch 103: val_loss did not improve from 45.61848
196/196 - 40s - loss: 44.6382 - MinusLogProbMetric: 44.6382 - val_loss: 45.9322 - val_MinusLogProbMetric: 45.9322 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 104/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:50:59.562 
Epoch 104/1000 
	 loss: 44.5396, MinusLogProbMetric: 44.5396, val_loss: 45.8613, val_MinusLogProbMetric: 45.8612

Epoch 104: val_loss did not improve from 45.61848
196/196 - 42s - loss: 44.5396 - MinusLogProbMetric: 44.5396 - val_loss: 45.8613 - val_MinusLogProbMetric: 45.8612 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 105/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:51:41.505 
Epoch 105/1000 
	 loss: 44.4793, MinusLogProbMetric: 44.4793, val_loss: 45.7853, val_MinusLogProbMetric: 45.7853

Epoch 105: val_loss did not improve from 45.61848
196/196 - 42s - loss: 44.4793 - MinusLogProbMetric: 44.4793 - val_loss: 45.7853 - val_MinusLogProbMetric: 45.7853 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 106/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:52:23.680 
Epoch 106/1000 
	 loss: 44.4565, MinusLogProbMetric: 44.4565, val_loss: 45.9477, val_MinusLogProbMetric: 45.9476

Epoch 106: val_loss did not improve from 45.61848
196/196 - 42s - loss: 44.4565 - MinusLogProbMetric: 44.4565 - val_loss: 45.9477 - val_MinusLogProbMetric: 45.9476 - lr: 4.1152e-06 - 42s/epoch - 215ms/step
Epoch 107/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:53:04.952 
Epoch 107/1000 
	 loss: 44.4986, MinusLogProbMetric: 44.4986, val_loss: 46.0569, val_MinusLogProbMetric: 46.0569

Epoch 107: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.4986 - MinusLogProbMetric: 44.4986 - val_loss: 46.0569 - val_MinusLogProbMetric: 46.0569 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 108/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:53:45.388 
Epoch 108/1000 
	 loss: 44.4093, MinusLogProbMetric: 44.4093, val_loss: 46.5774, val_MinusLogProbMetric: 46.5774

Epoch 108: val_loss did not improve from 45.61848
196/196 - 40s - loss: 44.4093 - MinusLogProbMetric: 44.4093 - val_loss: 46.5774 - val_MinusLogProbMetric: 46.5774 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 109/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:54:25.718 
Epoch 109/1000 
	 loss: 44.3551, MinusLogProbMetric: 44.3551, val_loss: 45.9639, val_MinusLogProbMetric: 45.9639

Epoch 109: val_loss did not improve from 45.61848
196/196 - 40s - loss: 44.3551 - MinusLogProbMetric: 44.3551 - val_loss: 45.9639 - val_MinusLogProbMetric: 45.9639 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 110/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:55:06.708 
Epoch 110/1000 
	 loss: 44.4077, MinusLogProbMetric: 44.4077, val_loss: 45.8242, val_MinusLogProbMetric: 45.8241

Epoch 110: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.4077 - MinusLogProbMetric: 44.4077 - val_loss: 45.8242 - val_MinusLogProbMetric: 45.8241 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 111/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:55:47.387 
Epoch 111/1000 
	 loss: 44.3496, MinusLogProbMetric: 44.3496, val_loss: 45.7662, val_MinusLogProbMetric: 45.7662

Epoch 111: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.3496 - MinusLogProbMetric: 44.3496 - val_loss: 45.7662 - val_MinusLogProbMetric: 45.7662 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 112/1000
2023-10-28 04:56:28.708 
Epoch 112/1000 
	 loss: 44.3709, MinusLogProbMetric: 44.3709, val_loss: 46.0250, val_MinusLogProbMetric: 46.0250

Epoch 112: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.3709 - MinusLogProbMetric: 44.3709 - val_loss: 46.0250 - val_MinusLogProbMetric: 46.0250 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 113/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:57:08.820 
Epoch 113/1000 
	 loss: 44.4191, MinusLogProbMetric: 44.4191, val_loss: 45.7988, val_MinusLogProbMetric: 45.7988

Epoch 113: val_loss did not improve from 45.61848
196/196 - 40s - loss: 44.4191 - MinusLogProbMetric: 44.4191 - val_loss: 45.7988 - val_MinusLogProbMetric: 45.7988 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 114/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:57:48.272 
Epoch 114/1000 
	 loss: 44.3181, MinusLogProbMetric: 44.3181, val_loss: 45.7663, val_MinusLogProbMetric: 45.7663

Epoch 114: val_loss did not improve from 45.61848
196/196 - 39s - loss: 44.3181 - MinusLogProbMetric: 44.3181 - val_loss: 45.7663 - val_MinusLogProbMetric: 45.7663 - lr: 4.1152e-06 - 39s/epoch - 201ms/step
Epoch 115/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:58:28.694 
Epoch 115/1000 
	 loss: 44.3172, MinusLogProbMetric: 44.3172, val_loss: 47.0756, val_MinusLogProbMetric: 47.0756

Epoch 115: val_loss did not improve from 45.61848
196/196 - 40s - loss: 44.3172 - MinusLogProbMetric: 44.3172 - val_loss: 47.0756 - val_MinusLogProbMetric: 47.0756 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 116/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:59:10.074 
Epoch 116/1000 
	 loss: 44.4545, MinusLogProbMetric: 44.4545, val_loss: 45.7262, val_MinusLogProbMetric: 45.7262

Epoch 116: val_loss did not improve from 45.61848
196/196 - 41s - loss: 44.4545 - MinusLogProbMetric: 44.4545 - val_loss: 45.7262 - val_MinusLogProbMetric: 45.7262 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 117/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 04:59:50.453 
Epoch 117/1000 
	 loss: 44.2407, MinusLogProbMetric: 44.2407, val_loss: 45.5956, val_MinusLogProbMetric: 45.5956

Epoch 117: val_loss improved from 45.61848 to 45.59560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 44.2407 - MinusLogProbMetric: 44.2407 - val_loss: 45.5956 - val_MinusLogProbMetric: 45.5956 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 118/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:00:31.982 
Epoch 118/1000 
	 loss: 44.2907, MinusLogProbMetric: 44.2907, val_loss: 45.8016, val_MinusLogProbMetric: 45.8016

Epoch 118: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.2907 - MinusLogProbMetric: 44.2907 - val_loss: 45.8016 - val_MinusLogProbMetric: 45.8016 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 119/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:01:12.928 
Epoch 119/1000 
	 loss: 44.2487, MinusLogProbMetric: 44.2487, val_loss: 46.1764, val_MinusLogProbMetric: 46.1764

Epoch 119: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.2487 - MinusLogProbMetric: 44.2487 - val_loss: 46.1764 - val_MinusLogProbMetric: 46.1764 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 120/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:01:54.342 
Epoch 120/1000 
	 loss: 44.2492, MinusLogProbMetric: 44.2492, val_loss: 45.7798, val_MinusLogProbMetric: 45.7798

Epoch 120: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.2492 - MinusLogProbMetric: 44.2492 - val_loss: 45.7798 - val_MinusLogProbMetric: 45.7798 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 121/1000
2023-10-28 05:02:35.291 
Epoch 121/1000 
	 loss: 44.2452, MinusLogProbMetric: 44.2452, val_loss: 45.7333, val_MinusLogProbMetric: 45.7333

Epoch 121: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.2452 - MinusLogProbMetric: 44.2452 - val_loss: 45.7333 - val_MinusLogProbMetric: 45.7333 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 122/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:03:15.901 
Epoch 122/1000 
	 loss: 44.2579, MinusLogProbMetric: 44.2579, val_loss: 46.0473, val_MinusLogProbMetric: 46.0473

Epoch 122: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.2579 - MinusLogProbMetric: 44.2579 - val_loss: 46.0473 - val_MinusLogProbMetric: 46.0473 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 123/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:03:56.332 
Epoch 123/1000 
	 loss: 44.1959, MinusLogProbMetric: 44.1959, val_loss: 45.5958, val_MinusLogProbMetric: 45.5958

Epoch 123: val_loss did not improve from 45.59560
196/196 - 40s - loss: 44.1959 - MinusLogProbMetric: 44.1959 - val_loss: 45.5958 - val_MinusLogProbMetric: 45.5958 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 124/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:04:36.728 
Epoch 124/1000 
	 loss: 44.1928, MinusLogProbMetric: 44.1928, val_loss: 46.0978, val_MinusLogProbMetric: 46.0977

Epoch 124: val_loss did not improve from 45.59560
196/196 - 40s - loss: 44.1928 - MinusLogProbMetric: 44.1928 - val_loss: 46.0978 - val_MinusLogProbMetric: 46.0977 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 125/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:05:16.882 
Epoch 125/1000 
	 loss: 44.1803, MinusLogProbMetric: 44.1803, val_loss: 46.1129, val_MinusLogProbMetric: 46.1128

Epoch 125: val_loss did not improve from 45.59560
196/196 - 40s - loss: 44.1803 - MinusLogProbMetric: 44.1803 - val_loss: 46.1129 - val_MinusLogProbMetric: 46.1128 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 126/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:05:57.238 
Epoch 126/1000 
	 loss: 44.2061, MinusLogProbMetric: 44.2061, val_loss: 45.7070, val_MinusLogProbMetric: 45.7070

Epoch 126: val_loss did not improve from 45.59560
196/196 - 40s - loss: 44.2061 - MinusLogProbMetric: 44.2061 - val_loss: 45.7070 - val_MinusLogProbMetric: 45.7070 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 127/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:06:38.476 
Epoch 127/1000 
	 loss: 44.1900, MinusLogProbMetric: 44.1900, val_loss: 45.6473, val_MinusLogProbMetric: 45.6473

Epoch 127: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.1900 - MinusLogProbMetric: 44.1900 - val_loss: 45.6473 - val_MinusLogProbMetric: 45.6473 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 128/1000
2023-10-28 05:07:18.746 
Epoch 128/1000 
	 loss: 44.1804, MinusLogProbMetric: 44.1804, val_loss: 46.2756, val_MinusLogProbMetric: 46.2756

Epoch 128: val_loss did not improve from 45.59560
196/196 - 40s - loss: 44.1804 - MinusLogProbMetric: 44.1804 - val_loss: 46.2756 - val_MinusLogProbMetric: 46.2756 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 129/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:07:59.733 
Epoch 129/1000 
	 loss: 44.3479, MinusLogProbMetric: 44.3479, val_loss: 46.0368, val_MinusLogProbMetric: 46.0368

Epoch 129: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.3479 - MinusLogProbMetric: 44.3479 - val_loss: 46.0368 - val_MinusLogProbMetric: 46.0368 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 130/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:08:40.419 
Epoch 130/1000 
	 loss: 44.2414, MinusLogProbMetric: 44.2414, val_loss: 45.8044, val_MinusLogProbMetric: 45.8044

Epoch 130: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.2414 - MinusLogProbMetric: 44.2414 - val_loss: 45.8044 - val_MinusLogProbMetric: 45.8044 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 131/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:09:20.383 
Epoch 131/1000 
	 loss: 44.1379, MinusLogProbMetric: 44.1379, val_loss: 45.9161, val_MinusLogProbMetric: 45.9161

Epoch 131: val_loss did not improve from 45.59560
196/196 - 40s - loss: 44.1379 - MinusLogProbMetric: 44.1379 - val_loss: 45.9161 - val_MinusLogProbMetric: 45.9161 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 132/1000
2023-10-28 05:10:00.771 
Epoch 132/1000 
	 loss: 44.3329, MinusLogProbMetric: 44.3329, val_loss: 45.6479, val_MinusLogProbMetric: 45.6479

Epoch 132: val_loss did not improve from 45.59560
196/196 - 40s - loss: 44.3329 - MinusLogProbMetric: 44.3329 - val_loss: 45.6479 - val_MinusLogProbMetric: 45.6479 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 133/1000
2023-10-28 05:10:40.534 
Epoch 133/1000 
	 loss: 44.2167, MinusLogProbMetric: 44.2167, val_loss: 45.7344, val_MinusLogProbMetric: 45.7344

Epoch 133: val_loss did not improve from 45.59560
196/196 - 40s - loss: 44.2167 - MinusLogProbMetric: 44.2167 - val_loss: 45.7344 - val_MinusLogProbMetric: 45.7344 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 134/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:11:21.590 
Epoch 134/1000 
	 loss: 44.1655, MinusLogProbMetric: 44.1655, val_loss: 45.9268, val_MinusLogProbMetric: 45.9268

Epoch 134: val_loss did not improve from 45.59560
196/196 - 41s - loss: 44.1655 - MinusLogProbMetric: 44.1655 - val_loss: 45.9268 - val_MinusLogProbMetric: 45.9268 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 135/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:12:03.283 
Epoch 135/1000 
	 loss: 44.2319, MinusLogProbMetric: 44.2319, val_loss: 45.5550, val_MinusLogProbMetric: 45.5550

Epoch 135: val_loss improved from 45.59560 to 45.55499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 44.2319 - MinusLogProbMetric: 44.2319 - val_loss: 45.5550 - val_MinusLogProbMetric: 45.5550 - lr: 4.1152e-06 - 42s/epoch - 217ms/step
Epoch 136/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:12:45.853 
Epoch 136/1000 
	 loss: 44.1906, MinusLogProbMetric: 44.1906, val_loss: 45.5338, val_MinusLogProbMetric: 45.5338

Epoch 136: val_loss improved from 45.55499 to 45.53378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 44.1906 - MinusLogProbMetric: 44.1906 - val_loss: 45.5338 - val_MinusLogProbMetric: 45.5338 - lr: 4.1152e-06 - 43s/epoch - 217ms/step
Epoch 137/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:13:28.456 
Epoch 137/1000 
	 loss: 44.1263, MinusLogProbMetric: 44.1263, val_loss: 46.1457, val_MinusLogProbMetric: 46.1457

Epoch 137: val_loss did not improve from 45.53378
196/196 - 42s - loss: 44.1263 - MinusLogProbMetric: 44.1263 - val_loss: 46.1457 - val_MinusLogProbMetric: 46.1457 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 138/1000
2023-10-28 05:14:10.061 
Epoch 138/1000 
	 loss: 44.1642, MinusLogProbMetric: 44.1642, val_loss: 45.6063, val_MinusLogProbMetric: 45.6063

Epoch 138: val_loss did not improve from 45.53378
196/196 - 42s - loss: 44.1642 - MinusLogProbMetric: 44.1642 - val_loss: 45.6063 - val_MinusLogProbMetric: 45.6063 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 139/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:14:51.945 
Epoch 139/1000 
	 loss: 44.2365, MinusLogProbMetric: 44.2365, val_loss: 45.5167, val_MinusLogProbMetric: 45.5167

Epoch 139: val_loss improved from 45.53378 to 45.51670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 43s - loss: 44.2365 - MinusLogProbMetric: 44.2365 - val_loss: 45.5167 - val_MinusLogProbMetric: 45.5167 - lr: 4.1152e-06 - 43s/epoch - 218ms/step
Epoch 140/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:15:33.839 
Epoch 140/1000 
	 loss: 44.1800, MinusLogProbMetric: 44.1800, val_loss: 45.5043, val_MinusLogProbMetric: 45.5043

Epoch 140: val_loss improved from 45.51670 to 45.50428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 44.1800 - MinusLogProbMetric: 44.1800 - val_loss: 45.5043 - val_MinusLogProbMetric: 45.5043 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 141/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:16:15.706 
Epoch 141/1000 
	 loss: 44.1183, MinusLogProbMetric: 44.1183, val_loss: 46.1342, val_MinusLogProbMetric: 46.1341

Epoch 141: val_loss did not improve from 45.50428
196/196 - 41s - loss: 44.1183 - MinusLogProbMetric: 44.1183 - val_loss: 46.1342 - val_MinusLogProbMetric: 46.1341 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 142/1000
2023-10-28 05:16:56.478 
Epoch 142/1000 
	 loss: 44.1125, MinusLogProbMetric: 44.1125, val_loss: 45.5000, val_MinusLogProbMetric: 45.5000

Epoch 142: val_loss improved from 45.50428 to 45.50003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 44.1125 - MinusLogProbMetric: 44.1125 - val_loss: 45.5000 - val_MinusLogProbMetric: 45.5000 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 143/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:17:38.417 
Epoch 143/1000 
	 loss: 44.1478, MinusLogProbMetric: 44.1478, val_loss: 45.9743, val_MinusLogProbMetric: 45.9742

Epoch 143: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.1478 - MinusLogProbMetric: 44.1478 - val_loss: 45.9743 - val_MinusLogProbMetric: 45.9742 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 144/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:18:19.540 
Epoch 144/1000 
	 loss: 44.2470, MinusLogProbMetric: 44.2470, val_loss: 46.1931, val_MinusLogProbMetric: 46.1931

Epoch 144: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.2470 - MinusLogProbMetric: 44.2470 - val_loss: 46.1931 - val_MinusLogProbMetric: 46.1931 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 145/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:18:59.648 
Epoch 145/1000 
	 loss: 44.0914, MinusLogProbMetric: 44.0914, val_loss: 45.7691, val_MinusLogProbMetric: 45.7691

Epoch 145: val_loss did not improve from 45.50003
196/196 - 40s - loss: 44.0914 - MinusLogProbMetric: 44.0914 - val_loss: 45.7691 - val_MinusLogProbMetric: 45.7691 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 146/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:19:40.547 
Epoch 146/1000 
	 loss: 44.1266, MinusLogProbMetric: 44.1266, val_loss: 45.6336, val_MinusLogProbMetric: 45.6335

Epoch 146: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.1266 - MinusLogProbMetric: 44.1266 - val_loss: 45.6336 - val_MinusLogProbMetric: 45.6335 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 147/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:20:22.291 
Epoch 147/1000 
	 loss: 44.1867, MinusLogProbMetric: 44.1867, val_loss: 45.8562, val_MinusLogProbMetric: 45.8562

Epoch 147: val_loss did not improve from 45.50003
196/196 - 42s - loss: 44.1867 - MinusLogProbMetric: 44.1867 - val_loss: 45.8562 - val_MinusLogProbMetric: 45.8562 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 148/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:21:03.787 
Epoch 148/1000 
	 loss: 44.2204, MinusLogProbMetric: 44.2204, val_loss: 45.7017, val_MinusLogProbMetric: 45.7017

Epoch 148: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.2204 - MinusLogProbMetric: 44.2204 - val_loss: 45.7017 - val_MinusLogProbMetric: 45.7017 - lr: 4.1152e-06 - 41s/epoch - 212ms/step
Epoch 149/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:21:44.283 
Epoch 149/1000 
	 loss: 44.0920, MinusLogProbMetric: 44.0920, val_loss: 45.7847, val_MinusLogProbMetric: 45.7847

Epoch 149: val_loss did not improve from 45.50003
196/196 - 40s - loss: 44.0920 - MinusLogProbMetric: 44.0920 - val_loss: 45.7847 - val_MinusLogProbMetric: 45.7847 - lr: 4.1152e-06 - 40s/epoch - 207ms/step
Epoch 150/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:22:24.815 
Epoch 150/1000 
	 loss: 44.1322, MinusLogProbMetric: 44.1322, val_loss: 45.9729, val_MinusLogProbMetric: 45.9729

Epoch 150: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.1322 - MinusLogProbMetric: 44.1322 - val_loss: 45.9729 - val_MinusLogProbMetric: 45.9729 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 151/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:23:05.556 
Epoch 151/1000 
	 loss: 44.0498, MinusLogProbMetric: 44.0498, val_loss: 45.6906, val_MinusLogProbMetric: 45.6906

Epoch 151: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.0498 - MinusLogProbMetric: 44.0498 - val_loss: 45.6906 - val_MinusLogProbMetric: 45.6906 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 152/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:23:45.791 
Epoch 152/1000 
	 loss: 44.1176, MinusLogProbMetric: 44.1176, val_loss: 45.5712, val_MinusLogProbMetric: 45.5711

Epoch 152: val_loss did not improve from 45.50003
196/196 - 40s - loss: 44.1176 - MinusLogProbMetric: 44.1176 - val_loss: 45.5712 - val_MinusLogProbMetric: 45.5711 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 153/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:24:26.297 
Epoch 153/1000 
	 loss: 44.0327, MinusLogProbMetric: 44.0327, val_loss: 45.8219, val_MinusLogProbMetric: 45.8218

Epoch 153: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.0327 - MinusLogProbMetric: 44.0327 - val_loss: 45.8219 - val_MinusLogProbMetric: 45.8218 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 154/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:25:07.104 
Epoch 154/1000 
	 loss: 44.0733, MinusLogProbMetric: 44.0733, val_loss: 45.7306, val_MinusLogProbMetric: 45.7306

Epoch 154: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.0733 - MinusLogProbMetric: 44.0733 - val_loss: 45.7306 - val_MinusLogProbMetric: 45.7306 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 155/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:25:47.895 
Epoch 155/1000 
	 loss: 44.0650, MinusLogProbMetric: 44.0650, val_loss: 45.8432, val_MinusLogProbMetric: 45.8432

Epoch 155: val_loss did not improve from 45.50003
196/196 - 41s - loss: 44.0650 - MinusLogProbMetric: 44.0650 - val_loss: 45.8432 - val_MinusLogProbMetric: 45.8432 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 156/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:26:28.522 
Epoch 156/1000 
	 loss: 44.0106, MinusLogProbMetric: 44.0106, val_loss: 45.4325, val_MinusLogProbMetric: 45.4325

Epoch 156: val_loss improved from 45.50003 to 45.43254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 44.0106 - MinusLogProbMetric: 44.0106 - val_loss: 45.4325 - val_MinusLogProbMetric: 45.4325 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 157/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:27:10.836 
Epoch 157/1000 
	 loss: 44.2179, MinusLogProbMetric: 44.2179, val_loss: 45.9017, val_MinusLogProbMetric: 45.9017

Epoch 157: val_loss did not improve from 45.43254
196/196 - 42s - loss: 44.2179 - MinusLogProbMetric: 44.2179 - val_loss: 45.9017 - val_MinusLogProbMetric: 45.9017 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 158/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:27:52.459 
Epoch 158/1000 
	 loss: 43.9913, MinusLogProbMetric: 43.9913, val_loss: 45.7700, val_MinusLogProbMetric: 45.7700

Epoch 158: val_loss did not improve from 45.43254
196/196 - 42s - loss: 43.9913 - MinusLogProbMetric: 43.9913 - val_loss: 45.7700 - val_MinusLogProbMetric: 45.7700 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 159/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:28:33.963 
Epoch 159/1000 
	 loss: 43.9914, MinusLogProbMetric: 43.9914, val_loss: 45.4927, val_MinusLogProbMetric: 45.4927

Epoch 159: val_loss did not improve from 45.43254
196/196 - 41s - loss: 43.9914 - MinusLogProbMetric: 43.9914 - val_loss: 45.4927 - val_MinusLogProbMetric: 45.4927 - lr: 4.1152e-06 - 41s/epoch - 212ms/step
Epoch 160/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:29:15.861 
Epoch 160/1000 
	 loss: 44.0545, MinusLogProbMetric: 44.0545, val_loss: 45.4327, val_MinusLogProbMetric: 45.4327

Epoch 160: val_loss did not improve from 45.43254
196/196 - 42s - loss: 44.0545 - MinusLogProbMetric: 44.0545 - val_loss: 45.4327 - val_MinusLogProbMetric: 45.4327 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 161/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:29:56.515 
Epoch 161/1000 
	 loss: 44.0798, MinusLogProbMetric: 44.0798, val_loss: 45.6947, val_MinusLogProbMetric: 45.6946

Epoch 161: val_loss did not improve from 45.43254
196/196 - 41s - loss: 44.0798 - MinusLogProbMetric: 44.0798 - val_loss: 45.6947 - val_MinusLogProbMetric: 45.6946 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 162/1000
2023-10-28 05:30:36.496 
Epoch 162/1000 
	 loss: 44.0402, MinusLogProbMetric: 44.0402, val_loss: 46.0801, val_MinusLogProbMetric: 46.0801

Epoch 162: val_loss did not improve from 45.43254
196/196 - 40s - loss: 44.0402 - MinusLogProbMetric: 44.0402 - val_loss: 46.0801 - val_MinusLogProbMetric: 46.0801 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 163/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:31:17.494 
Epoch 163/1000 
	 loss: 44.1248, MinusLogProbMetric: 44.1248, val_loss: 46.1233, val_MinusLogProbMetric: 46.1233

Epoch 163: val_loss did not improve from 45.43254
196/196 - 41s - loss: 44.1248 - MinusLogProbMetric: 44.1248 - val_loss: 46.1233 - val_MinusLogProbMetric: 46.1233 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 164/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:31:59.523 
Epoch 164/1000 
	 loss: 44.0705, MinusLogProbMetric: 44.0705, val_loss: 45.9356, val_MinusLogProbMetric: 45.9356

Epoch 164: val_loss did not improve from 45.43254
196/196 - 42s - loss: 44.0705 - MinusLogProbMetric: 44.0705 - val_loss: 45.9356 - val_MinusLogProbMetric: 45.9356 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 165/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:32:39.953 
Epoch 165/1000 
	 loss: 44.1347, MinusLogProbMetric: 44.1347, val_loss: 45.6214, val_MinusLogProbMetric: 45.6214

Epoch 165: val_loss did not improve from 45.43254
196/196 - 40s - loss: 44.1347 - MinusLogProbMetric: 44.1347 - val_loss: 45.6214 - val_MinusLogProbMetric: 45.6214 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 166/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:33:20.508 
Epoch 166/1000 
	 loss: 43.9931, MinusLogProbMetric: 43.9931, val_loss: 45.7792, val_MinusLogProbMetric: 45.7792

Epoch 166: val_loss did not improve from 45.43254
196/196 - 41s - loss: 43.9931 - MinusLogProbMetric: 43.9931 - val_loss: 45.7792 - val_MinusLogProbMetric: 45.7792 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 167/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:34:00.661 
Epoch 167/1000 
	 loss: 44.0463, MinusLogProbMetric: 44.0463, val_loss: 45.5213, val_MinusLogProbMetric: 45.5213

Epoch 167: val_loss did not improve from 45.43254
196/196 - 40s - loss: 44.0463 - MinusLogProbMetric: 44.0463 - val_loss: 45.5213 - val_MinusLogProbMetric: 45.5213 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 168/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:34:41.086 
Epoch 168/1000 
	 loss: 44.6136, MinusLogProbMetric: 44.6136, val_loss: 45.4307, val_MinusLogProbMetric: 45.4307

Epoch 168: val_loss improved from 45.43254 to 45.43071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 44.6136 - MinusLogProbMetric: 44.6136 - val_loss: 45.4307 - val_MinusLogProbMetric: 45.4307 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 169/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:35:21.057 
Epoch 169/1000 
	 loss: 44.0169, MinusLogProbMetric: 44.0169, val_loss: 45.3916, val_MinusLogProbMetric: 45.3916

Epoch 169: val_loss improved from 45.43071 to 45.39164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 40s - loss: 44.0169 - MinusLogProbMetric: 44.0169 - val_loss: 45.3916 - val_MinusLogProbMetric: 45.3916 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 170/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:35:59.407 
Epoch 170/1000 
	 loss: 44.0613, MinusLogProbMetric: 44.0613, val_loss: 45.6482, val_MinusLogProbMetric: 45.6482

Epoch 170: val_loss did not improve from 45.39164
196/196 - 38s - loss: 44.0613 - MinusLogProbMetric: 44.0613 - val_loss: 45.6482 - val_MinusLogProbMetric: 45.6482 - lr: 4.1152e-06 - 38s/epoch - 192ms/step
Epoch 171/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:36:39.665 
Epoch 171/1000 
	 loss: 44.0888, MinusLogProbMetric: 44.0888, val_loss: 45.3970, val_MinusLogProbMetric: 45.3970

Epoch 171: val_loss did not improve from 45.39164
196/196 - 40s - loss: 44.0888 - MinusLogProbMetric: 44.0888 - val_loss: 45.3970 - val_MinusLogProbMetric: 45.3970 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 172/1000
2023-10-28 05:37:20.094 
Epoch 172/1000 
	 loss: 43.9479, MinusLogProbMetric: 43.9479, val_loss: 45.6427, val_MinusLogProbMetric: 45.6427

Epoch 172: val_loss did not improve from 45.39164
196/196 - 40s - loss: 43.9479 - MinusLogProbMetric: 43.9479 - val_loss: 45.6427 - val_MinusLogProbMetric: 45.6427 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 173/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:38:01.330 
Epoch 173/1000 
	 loss: 43.9415, MinusLogProbMetric: 43.9415, val_loss: 46.0645, val_MinusLogProbMetric: 46.0645

Epoch 173: val_loss did not improve from 45.39164
196/196 - 41s - loss: 43.9415 - MinusLogProbMetric: 43.9415 - val_loss: 46.0645 - val_MinusLogProbMetric: 46.0645 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 174/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:38:41.586 
Epoch 174/1000 
	 loss: 43.8836, MinusLogProbMetric: 43.8836, val_loss: 45.4499, val_MinusLogProbMetric: 45.4499

Epoch 174: val_loss did not improve from 45.39164
196/196 - 40s - loss: 43.8836 - MinusLogProbMetric: 43.8836 - val_loss: 45.4499 - val_MinusLogProbMetric: 45.4499 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 175/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:39:23.192 
Epoch 175/1000 
	 loss: 43.9728, MinusLogProbMetric: 43.9728, val_loss: 45.7525, val_MinusLogProbMetric: 45.7525

Epoch 175: val_loss did not improve from 45.39164
196/196 - 42s - loss: 43.9728 - MinusLogProbMetric: 43.9728 - val_loss: 45.7525 - val_MinusLogProbMetric: 45.7525 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 176/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:40:05.673 
Epoch 176/1000 
	 loss: 44.0267, MinusLogProbMetric: 44.0267, val_loss: 45.7233, val_MinusLogProbMetric: 45.7233

Epoch 176: val_loss did not improve from 45.39164
196/196 - 42s - loss: 44.0267 - MinusLogProbMetric: 44.0267 - val_loss: 45.7233 - val_MinusLogProbMetric: 45.7233 - lr: 4.1152e-06 - 42s/epoch - 217ms/step
Epoch 177/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:40:47.311 
Epoch 177/1000 
	 loss: 43.9507, MinusLogProbMetric: 43.9507, val_loss: 45.5434, val_MinusLogProbMetric: 45.5434

Epoch 177: val_loss did not improve from 45.39164
196/196 - 42s - loss: 43.9507 - MinusLogProbMetric: 43.9507 - val_loss: 45.5434 - val_MinusLogProbMetric: 45.5434 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 178/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:41:27.892 
Epoch 178/1000 
	 loss: 43.9033, MinusLogProbMetric: 43.9033, val_loss: 45.8856, val_MinusLogProbMetric: 45.8856

Epoch 178: val_loss did not improve from 45.39164
196/196 - 41s - loss: 43.9033 - MinusLogProbMetric: 43.9033 - val_loss: 45.8856 - val_MinusLogProbMetric: 45.8856 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 179/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:42:08.594 
Epoch 179/1000 
	 loss: 43.9732, MinusLogProbMetric: 43.9732, val_loss: 46.0075, val_MinusLogProbMetric: 46.0075

Epoch 179: val_loss did not improve from 45.39164
196/196 - 41s - loss: 43.9732 - MinusLogProbMetric: 43.9732 - val_loss: 46.0075 - val_MinusLogProbMetric: 46.0075 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 180/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:42:49.918 
Epoch 180/1000 
	 loss: 44.2388, MinusLogProbMetric: 44.2388, val_loss: 46.0492, val_MinusLogProbMetric: 46.0492

Epoch 180: val_loss did not improve from 45.39164
196/196 - 41s - loss: 44.2388 - MinusLogProbMetric: 44.2388 - val_loss: 46.0492 - val_MinusLogProbMetric: 46.0492 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 181/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:43:30.430 
Epoch 181/1000 
	 loss: 44.0573, MinusLogProbMetric: 44.0573, val_loss: 45.9260, val_MinusLogProbMetric: 45.9261

Epoch 181: val_loss did not improve from 45.39164
196/196 - 41s - loss: 44.0573 - MinusLogProbMetric: 44.0573 - val_loss: 45.9260 - val_MinusLogProbMetric: 45.9261 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 182/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:44:10.935 
Epoch 182/1000 
	 loss: 43.8974, MinusLogProbMetric: 43.8974, val_loss: 45.5944, val_MinusLogProbMetric: 45.5944

Epoch 182: val_loss did not improve from 45.39164
196/196 - 41s - loss: 43.8974 - MinusLogProbMetric: 43.8974 - val_loss: 45.5944 - val_MinusLogProbMetric: 45.5944 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 183/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:44:50.896 
Epoch 183/1000 
	 loss: 43.9113, MinusLogProbMetric: 43.9113, val_loss: 46.1046, val_MinusLogProbMetric: 46.1045

Epoch 183: val_loss did not improve from 45.39164
196/196 - 40s - loss: 43.9113 - MinusLogProbMetric: 43.9113 - val_loss: 46.1046 - val_MinusLogProbMetric: 46.1045 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 184/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:45:31.769 
Epoch 184/1000 
	 loss: 43.9832, MinusLogProbMetric: 43.9832, val_loss: 45.6952, val_MinusLogProbMetric: 45.6952

Epoch 184: val_loss did not improve from 45.39164
196/196 - 41s - loss: 43.9832 - MinusLogProbMetric: 43.9832 - val_loss: 45.6952 - val_MinusLogProbMetric: 45.6952 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 185/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:46:11.556 
Epoch 185/1000 
	 loss: 43.9492, MinusLogProbMetric: 43.9492, val_loss: 45.4973, val_MinusLogProbMetric: 45.4973

Epoch 185: val_loss did not improve from 45.39164
196/196 - 40s - loss: 43.9492 - MinusLogProbMetric: 43.9492 - val_loss: 45.4973 - val_MinusLogProbMetric: 45.4973 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 186/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:46:52.106 
Epoch 186/1000 
	 loss: 44.0094, MinusLogProbMetric: 44.0094, val_loss: 47.0497, val_MinusLogProbMetric: 47.0497

Epoch 186: val_loss did not improve from 45.39164
196/196 - 41s - loss: 44.0094 - MinusLogProbMetric: 44.0094 - val_loss: 47.0497 - val_MinusLogProbMetric: 47.0497 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 187/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:47:32.085 
Epoch 187/1000 
	 loss: 44.2350, MinusLogProbMetric: 44.2350, val_loss: 45.5501, val_MinusLogProbMetric: 45.5501

Epoch 187: val_loss did not improve from 45.39164
196/196 - 40s - loss: 44.2350 - MinusLogProbMetric: 44.2350 - val_loss: 45.5501 - val_MinusLogProbMetric: 45.5501 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 188/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:48:11.271 
Epoch 188/1000 
	 loss: 44.0339, MinusLogProbMetric: 44.0339, val_loss: 45.3327, val_MinusLogProbMetric: 45.3327

Epoch 188: val_loss improved from 45.39164 to 45.33270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 40s - loss: 44.0339 - MinusLogProbMetric: 44.0339 - val_loss: 45.3327 - val_MinusLogProbMetric: 45.3327 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 189/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:48:51.398 
Epoch 189/1000 
	 loss: 43.8463, MinusLogProbMetric: 43.8463, val_loss: 45.4969, val_MinusLogProbMetric: 45.4968

Epoch 189: val_loss did not improve from 45.33270
196/196 - 39s - loss: 43.8463 - MinusLogProbMetric: 43.8463 - val_loss: 45.4969 - val_MinusLogProbMetric: 45.4968 - lr: 4.1152e-06 - 39s/epoch - 201ms/step
Epoch 190/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:49:31.525 
Epoch 190/1000 
	 loss: 43.9241, MinusLogProbMetric: 43.9241, val_loss: 45.7041, val_MinusLogProbMetric: 45.7041

Epoch 190: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.9241 - MinusLogProbMetric: 43.9241 - val_loss: 45.7041 - val_MinusLogProbMetric: 45.7041 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 191/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:50:10.876 
Epoch 191/1000 
	 loss: 43.8852, MinusLogProbMetric: 43.8852, val_loss: 45.3898, val_MinusLogProbMetric: 45.3897

Epoch 191: val_loss did not improve from 45.33270
196/196 - 39s - loss: 43.8852 - MinusLogProbMetric: 43.8852 - val_loss: 45.3898 - val_MinusLogProbMetric: 45.3897 - lr: 4.1152e-06 - 39s/epoch - 201ms/step
Epoch 192/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:50:51.535 
Epoch 192/1000 
	 loss: 43.9941, MinusLogProbMetric: 43.9941, val_loss: 45.7079, val_MinusLogProbMetric: 45.7079

Epoch 192: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.9941 - MinusLogProbMetric: 43.9941 - val_loss: 45.7079 - val_MinusLogProbMetric: 45.7079 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 193/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:51:33.164 
Epoch 193/1000 
	 loss: 44.0228, MinusLogProbMetric: 44.0228, val_loss: 45.9006, val_MinusLogProbMetric: 45.9006

Epoch 193: val_loss did not improve from 45.33270
196/196 - 42s - loss: 44.0228 - MinusLogProbMetric: 44.0228 - val_loss: 45.9006 - val_MinusLogProbMetric: 45.9006 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 194/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:52:13.975 
Epoch 194/1000 
	 loss: 43.8560, MinusLogProbMetric: 43.8560, val_loss: 45.8834, val_MinusLogProbMetric: 45.8834

Epoch 194: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8560 - MinusLogProbMetric: 43.8560 - val_loss: 45.8834 - val_MinusLogProbMetric: 45.8834 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 195/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:52:54.826 
Epoch 195/1000 
	 loss: 44.0235, MinusLogProbMetric: 44.0235, val_loss: 45.7246, val_MinusLogProbMetric: 45.7246

Epoch 195: val_loss did not improve from 45.33270
196/196 - 41s - loss: 44.0235 - MinusLogProbMetric: 44.0235 - val_loss: 45.7246 - val_MinusLogProbMetric: 45.7246 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 196/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:53:36.141 
Epoch 196/1000 
	 loss: 43.8691, MinusLogProbMetric: 43.8691, val_loss: 45.5059, val_MinusLogProbMetric: 45.5059

Epoch 196: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8691 - MinusLogProbMetric: 43.8691 - val_loss: 45.5059 - val_MinusLogProbMetric: 45.5059 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 197/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:54:17.730 
Epoch 197/1000 
	 loss: 43.8409, MinusLogProbMetric: 43.8409, val_loss: 46.1193, val_MinusLogProbMetric: 46.1192

Epoch 197: val_loss did not improve from 45.33270
196/196 - 42s - loss: 43.8409 - MinusLogProbMetric: 43.8409 - val_loss: 46.1193 - val_MinusLogProbMetric: 46.1192 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 198/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:54:59.110 
Epoch 198/1000 
	 loss: 43.8516, MinusLogProbMetric: 43.8516, val_loss: 45.3774, val_MinusLogProbMetric: 45.3773

Epoch 198: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8516 - MinusLogProbMetric: 43.8516 - val_loss: 45.3774 - val_MinusLogProbMetric: 45.3773 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 199/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:55:40.758 
Epoch 199/1000 
	 loss: 43.8516, MinusLogProbMetric: 43.8516, val_loss: 45.9764, val_MinusLogProbMetric: 45.9764

Epoch 199: val_loss did not improve from 45.33270
196/196 - 42s - loss: 43.8516 - MinusLogProbMetric: 43.8516 - val_loss: 45.9764 - val_MinusLogProbMetric: 45.9764 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 200/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:56:21.706 
Epoch 200/1000 
	 loss: 43.9030, MinusLogProbMetric: 43.9030, val_loss: 45.9919, val_MinusLogProbMetric: 45.9919

Epoch 200: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.9030 - MinusLogProbMetric: 43.9030 - val_loss: 45.9919 - val_MinusLogProbMetric: 45.9919 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 201/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:57:02.438 
Epoch 201/1000 
	 loss: 43.8542, MinusLogProbMetric: 43.8542, val_loss: 46.0776, val_MinusLogProbMetric: 46.0776

Epoch 201: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8542 - MinusLogProbMetric: 43.8542 - val_loss: 46.0776 - val_MinusLogProbMetric: 46.0776 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 202/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:57:42.796 
Epoch 202/1000 
	 loss: 43.8481, MinusLogProbMetric: 43.8481, val_loss: 45.6506, val_MinusLogProbMetric: 45.6506

Epoch 202: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.8481 - MinusLogProbMetric: 43.8481 - val_loss: 45.6506 - val_MinusLogProbMetric: 45.6506 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 203/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:58:23.173 
Epoch 203/1000 
	 loss: 43.8469, MinusLogProbMetric: 43.8469, val_loss: 45.8884, val_MinusLogProbMetric: 45.8884

Epoch 203: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.8469 - MinusLogProbMetric: 43.8469 - val_loss: 45.8884 - val_MinusLogProbMetric: 45.8884 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 204/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:59:03.507 
Epoch 204/1000 
	 loss: 43.8608, MinusLogProbMetric: 43.8608, val_loss: 45.5033, val_MinusLogProbMetric: 45.5033

Epoch 204: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.8608 - MinusLogProbMetric: 43.8608 - val_loss: 45.5033 - val_MinusLogProbMetric: 45.5033 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 205/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 05:59:43.906 
Epoch 205/1000 
	 loss: 43.9381, MinusLogProbMetric: 43.9381, val_loss: 45.5067, val_MinusLogProbMetric: 45.5067

Epoch 205: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.9381 - MinusLogProbMetric: 43.9381 - val_loss: 45.5067 - val_MinusLogProbMetric: 45.5067 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 206/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:00:24.417 
Epoch 206/1000 
	 loss: 43.7744, MinusLogProbMetric: 43.7744, val_loss: 45.5772, val_MinusLogProbMetric: 45.5772

Epoch 206: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.7744 - MinusLogProbMetric: 43.7744 - val_loss: 45.5772 - val_MinusLogProbMetric: 45.5772 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 207/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:01:05.329 
Epoch 207/1000 
	 loss: 43.8645, MinusLogProbMetric: 43.8645, val_loss: 45.4925, val_MinusLogProbMetric: 45.4924

Epoch 207: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8645 - MinusLogProbMetric: 43.8645 - val_loss: 45.4925 - val_MinusLogProbMetric: 45.4924 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 208/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:01:46.063 
Epoch 208/1000 
	 loss: 43.8633, MinusLogProbMetric: 43.8633, val_loss: 45.4048, val_MinusLogProbMetric: 45.4048

Epoch 208: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8633 - MinusLogProbMetric: 43.8633 - val_loss: 45.4048 - val_MinusLogProbMetric: 45.4048 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 209/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:02:26.384 
Epoch 209/1000 
	 loss: 43.8358, MinusLogProbMetric: 43.8358, val_loss: 45.7772, val_MinusLogProbMetric: 45.7771

Epoch 209: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.8358 - MinusLogProbMetric: 43.8358 - val_loss: 45.7772 - val_MinusLogProbMetric: 45.7771 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 210/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:03:06.888 
Epoch 210/1000 
	 loss: 43.9519, MinusLogProbMetric: 43.9519, val_loss: 46.1224, val_MinusLogProbMetric: 46.1224

Epoch 210: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.9519 - MinusLogProbMetric: 43.9519 - val_loss: 46.1224 - val_MinusLogProbMetric: 46.1224 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 211/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:03:47.686 
Epoch 211/1000 
	 loss: 43.8407, MinusLogProbMetric: 43.8407, val_loss: 45.9834, val_MinusLogProbMetric: 45.9834

Epoch 211: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8407 - MinusLogProbMetric: 43.8407 - val_loss: 45.9834 - val_MinusLogProbMetric: 45.9834 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 212/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:04:29.253 
Epoch 212/1000 
	 loss: 43.8024, MinusLogProbMetric: 43.8024, val_loss: 45.8067, val_MinusLogProbMetric: 45.8067

Epoch 212: val_loss did not improve from 45.33270
196/196 - 42s - loss: 43.8024 - MinusLogProbMetric: 43.8024 - val_loss: 45.8067 - val_MinusLogProbMetric: 45.8067 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 213/1000
2023-10-28 06:05:10.496 
Epoch 213/1000 
	 loss: 43.8474, MinusLogProbMetric: 43.8474, val_loss: 45.5568, val_MinusLogProbMetric: 45.5568

Epoch 213: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8474 - MinusLogProbMetric: 43.8474 - val_loss: 45.5568 - val_MinusLogProbMetric: 45.5568 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 214/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:05:51.486 
Epoch 214/1000 
	 loss: 43.8835, MinusLogProbMetric: 43.8835, val_loss: 45.6254, val_MinusLogProbMetric: 45.6254

Epoch 214: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8835 - MinusLogProbMetric: 43.8835 - val_loss: 45.6254 - val_MinusLogProbMetric: 45.6254 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 215/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:06:32.283 
Epoch 215/1000 
	 loss: 43.8212, MinusLogProbMetric: 43.8212, val_loss: 45.5254, val_MinusLogProbMetric: 45.5254

Epoch 215: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8212 - MinusLogProbMetric: 43.8212 - val_loss: 45.5254 - val_MinusLogProbMetric: 45.5254 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 216/1000
2023-10-28 06:07:12.549 
Epoch 216/1000 
	 loss: 43.7544, MinusLogProbMetric: 43.7544, val_loss: 45.6996, val_MinusLogProbMetric: 45.6996

Epoch 216: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.7544 - MinusLogProbMetric: 43.7544 - val_loss: 45.6996 - val_MinusLogProbMetric: 45.6996 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 217/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:07:53.018 
Epoch 217/1000 
	 loss: 43.7835, MinusLogProbMetric: 43.7835, val_loss: 45.6283, val_MinusLogProbMetric: 45.6282

Epoch 217: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.7835 - MinusLogProbMetric: 43.7835 - val_loss: 45.6283 - val_MinusLogProbMetric: 45.6282 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 218/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:08:32.537 
Epoch 218/1000 
	 loss: 43.8384, MinusLogProbMetric: 43.8384, val_loss: 45.5580, val_MinusLogProbMetric: 45.5579

Epoch 218: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.8384 - MinusLogProbMetric: 43.8384 - val_loss: 45.5580 - val_MinusLogProbMetric: 45.5579 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 219/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:09:12.209 
Epoch 219/1000 
	 loss: 43.7875, MinusLogProbMetric: 43.7875, val_loss: 45.7861, val_MinusLogProbMetric: 45.7860

Epoch 219: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.7875 - MinusLogProbMetric: 43.7875 - val_loss: 45.7861 - val_MinusLogProbMetric: 45.7860 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 220/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:09:52.500 
Epoch 220/1000 
	 loss: 43.8921, MinusLogProbMetric: 43.8921, val_loss: 45.5858, val_MinusLogProbMetric: 45.5858

Epoch 220: val_loss did not improve from 45.33270
196/196 - 40s - loss: 43.8921 - MinusLogProbMetric: 43.8921 - val_loss: 45.5858 - val_MinusLogProbMetric: 45.5858 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 221/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:10:33.493 
Epoch 221/1000 
	 loss: 43.8977, MinusLogProbMetric: 43.8977, val_loss: 45.3903, val_MinusLogProbMetric: 45.3902

Epoch 221: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.8977 - MinusLogProbMetric: 43.8977 - val_loss: 45.3903 - val_MinusLogProbMetric: 45.3902 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 222/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:11:15.095 
Epoch 222/1000 
	 loss: 44.2801, MinusLogProbMetric: 44.2801, val_loss: 45.5944, val_MinusLogProbMetric: 45.5944

Epoch 222: val_loss did not improve from 45.33270
196/196 - 42s - loss: 44.2801 - MinusLogProbMetric: 44.2801 - val_loss: 45.5944 - val_MinusLogProbMetric: 45.5944 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 223/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:11:57.222 
Epoch 223/1000 
	 loss: 43.8225, MinusLogProbMetric: 43.8225, val_loss: 45.4209, val_MinusLogProbMetric: 45.4209

Epoch 223: val_loss did not improve from 45.33270
196/196 - 42s - loss: 43.8225 - MinusLogProbMetric: 43.8225 - val_loss: 45.4209 - val_MinusLogProbMetric: 45.4209 - lr: 4.1152e-06 - 42s/epoch - 215ms/step
Epoch 224/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:12:38.783 
Epoch 224/1000 
	 loss: 43.7909, MinusLogProbMetric: 43.7909, val_loss: 45.6458, val_MinusLogProbMetric: 45.6458

Epoch 224: val_loss did not improve from 45.33270
196/196 - 42s - loss: 43.7909 - MinusLogProbMetric: 43.7909 - val_loss: 45.6458 - val_MinusLogProbMetric: 45.6458 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 225/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:13:20.192 
Epoch 225/1000 
	 loss: 43.9888, MinusLogProbMetric: 43.9888, val_loss: 46.0054, val_MinusLogProbMetric: 46.0053

Epoch 225: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.9888 - MinusLogProbMetric: 43.9888 - val_loss: 46.0054 - val_MinusLogProbMetric: 46.0053 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 226/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:14:00.949 
Epoch 226/1000 
	 loss: 43.7412, MinusLogProbMetric: 43.7412, val_loss: 45.7486, val_MinusLogProbMetric: 45.7486

Epoch 226: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.7412 - MinusLogProbMetric: 43.7412 - val_loss: 45.7486 - val_MinusLogProbMetric: 45.7486 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 227/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:14:41.587 
Epoch 227/1000 
	 loss: 43.7812, MinusLogProbMetric: 43.7812, val_loss: 45.6956, val_MinusLogProbMetric: 45.6955

Epoch 227: val_loss did not improve from 45.33270
196/196 - 41s - loss: 43.7812 - MinusLogProbMetric: 43.7812 - val_loss: 45.6956 - val_MinusLogProbMetric: 45.6955 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 228/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:15:21.996 
Epoch 228/1000 
	 loss: 43.7692, MinusLogProbMetric: 43.7692, val_loss: 45.2908, val_MinusLogProbMetric: 45.2908

Epoch 228: val_loss improved from 45.33270 to 45.29083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 43.7692 - MinusLogProbMetric: 43.7692 - val_loss: 45.2908 - val_MinusLogProbMetric: 45.2908 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 229/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:16:02.706 
Epoch 229/1000 
	 loss: 43.7520, MinusLogProbMetric: 43.7520, val_loss: 45.4034, val_MinusLogProbMetric: 45.4033

Epoch 229: val_loss did not improve from 45.29083
196/196 - 40s - loss: 43.7520 - MinusLogProbMetric: 43.7520 - val_loss: 45.4034 - val_MinusLogProbMetric: 45.4033 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 230/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:16:41.940 
Epoch 230/1000 
	 loss: 43.7771, MinusLogProbMetric: 43.7771, val_loss: 45.3152, val_MinusLogProbMetric: 45.3152

Epoch 230: val_loss did not improve from 45.29083
196/196 - 39s - loss: 43.7771 - MinusLogProbMetric: 43.7771 - val_loss: 45.3152 - val_MinusLogProbMetric: 45.3152 - lr: 4.1152e-06 - 39s/epoch - 200ms/step
Epoch 231/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:17:22.694 
Epoch 231/1000 
	 loss: 43.9080, MinusLogProbMetric: 43.9080, val_loss: 45.3393, val_MinusLogProbMetric: 45.3393

Epoch 231: val_loss did not improve from 45.29083
196/196 - 41s - loss: 43.9080 - MinusLogProbMetric: 43.9080 - val_loss: 45.3393 - val_MinusLogProbMetric: 45.3393 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 232/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:18:03.368 
Epoch 232/1000 
	 loss: 43.7344, MinusLogProbMetric: 43.7344, val_loss: 45.8769, val_MinusLogProbMetric: 45.8769

Epoch 232: val_loss did not improve from 45.29083
196/196 - 41s - loss: 43.7344 - MinusLogProbMetric: 43.7344 - val_loss: 45.8769 - val_MinusLogProbMetric: 45.8769 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 233/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:18:43.970 
Epoch 233/1000 
	 loss: 43.6930, MinusLogProbMetric: 43.6930, val_loss: 45.8830, val_MinusLogProbMetric: 45.8830

Epoch 233: val_loss did not improve from 45.29083
196/196 - 41s - loss: 43.6930 - MinusLogProbMetric: 43.6930 - val_loss: 45.8830 - val_MinusLogProbMetric: 45.8830 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 234/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:19:25.300 
Epoch 234/1000 
	 loss: 44.1214, MinusLogProbMetric: 44.1214, val_loss: 45.3536, val_MinusLogProbMetric: 45.3536

Epoch 234: val_loss did not improve from 45.29083
196/196 - 41s - loss: 44.1214 - MinusLogProbMetric: 44.1214 - val_loss: 45.3536 - val_MinusLogProbMetric: 45.3536 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 235/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:20:07.295 
Epoch 235/1000 
	 loss: 43.9492, MinusLogProbMetric: 43.9492, val_loss: 45.3994, val_MinusLogProbMetric: 45.3993

Epoch 235: val_loss did not improve from 45.29083
196/196 - 42s - loss: 43.9492 - MinusLogProbMetric: 43.9492 - val_loss: 45.3994 - val_MinusLogProbMetric: 45.3993 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 236/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:20:48.796 
Epoch 236/1000 
	 loss: 43.6849, MinusLogProbMetric: 43.6849, val_loss: 45.5812, val_MinusLogProbMetric: 45.5812

Epoch 236: val_loss did not improve from 45.29083
196/196 - 41s - loss: 43.6849 - MinusLogProbMetric: 43.6849 - val_loss: 45.5812 - val_MinusLogProbMetric: 45.5812 - lr: 4.1152e-06 - 41s/epoch - 212ms/step
Epoch 237/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:21:29.558 
Epoch 237/1000 
	 loss: 43.8573, MinusLogProbMetric: 43.8573, val_loss: 45.2237, val_MinusLogProbMetric: 45.2237

Epoch 237: val_loss improved from 45.29083 to 45.22374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 43.8573 - MinusLogProbMetric: 43.8573 - val_loss: 45.2237 - val_MinusLogProbMetric: 45.2237 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 238/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:22:11.204 
Epoch 238/1000 
	 loss: 43.7417, MinusLogProbMetric: 43.7417, val_loss: 45.3438, val_MinusLogProbMetric: 45.3437

Epoch 238: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.7417 - MinusLogProbMetric: 43.7417 - val_loss: 45.3438 - val_MinusLogProbMetric: 45.3437 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 239/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:22:52.857 
Epoch 239/1000 
	 loss: 43.7054, MinusLogProbMetric: 43.7054, val_loss: 45.4115, val_MinusLogProbMetric: 45.4114

Epoch 239: val_loss did not improve from 45.22374
196/196 - 42s - loss: 43.7054 - MinusLogProbMetric: 43.7054 - val_loss: 45.4115 - val_MinusLogProbMetric: 45.4114 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 240/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:23:31.673 
Epoch 240/1000 
	 loss: 43.8080, MinusLogProbMetric: 43.8080, val_loss: 45.3265, val_MinusLogProbMetric: 45.3265

Epoch 240: val_loss did not improve from 45.22374
196/196 - 39s - loss: 43.8080 - MinusLogProbMetric: 43.8080 - val_loss: 45.3265 - val_MinusLogProbMetric: 45.3265 - lr: 4.1152e-06 - 39s/epoch - 198ms/step
Epoch 241/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:24:12.291 
Epoch 241/1000 
	 loss: 43.8450, MinusLogProbMetric: 43.8450, val_loss: 45.7275, val_MinusLogProbMetric: 45.7274

Epoch 241: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.8450 - MinusLogProbMetric: 43.8450 - val_loss: 45.7275 - val_MinusLogProbMetric: 45.7274 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 242/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:24:52.994 
Epoch 242/1000 
	 loss: 43.7353, MinusLogProbMetric: 43.7353, val_loss: 45.3002, val_MinusLogProbMetric: 45.3002

Epoch 242: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.7353 - MinusLogProbMetric: 43.7353 - val_loss: 45.3002 - val_MinusLogProbMetric: 45.3002 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 243/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:25:33.457 
Epoch 243/1000 
	 loss: 43.7279, MinusLogProbMetric: 43.7279, val_loss: 45.7011, val_MinusLogProbMetric: 45.7011

Epoch 243: val_loss did not improve from 45.22374
196/196 - 40s - loss: 43.7279 - MinusLogProbMetric: 43.7279 - val_loss: 45.7011 - val_MinusLogProbMetric: 45.7011 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 244/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:26:14.950 
Epoch 244/1000 
	 loss: 43.7443, MinusLogProbMetric: 43.7443, val_loss: 45.2995, val_MinusLogProbMetric: 45.2994

Epoch 244: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.7443 - MinusLogProbMetric: 43.7443 - val_loss: 45.2995 - val_MinusLogProbMetric: 45.2994 - lr: 4.1152e-06 - 41s/epoch - 212ms/step
Epoch 245/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:26:55.590 
Epoch 245/1000 
	 loss: 43.6544, MinusLogProbMetric: 43.6544, val_loss: 45.5062, val_MinusLogProbMetric: 45.5062

Epoch 245: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.6544 - MinusLogProbMetric: 43.6544 - val_loss: 45.5062 - val_MinusLogProbMetric: 45.5062 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 246/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:27:37.005 
Epoch 246/1000 
	 loss: 43.8344, MinusLogProbMetric: 43.8344, val_loss: 45.4798, val_MinusLogProbMetric: 45.4798

Epoch 246: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.8344 - MinusLogProbMetric: 43.8344 - val_loss: 45.4798 - val_MinusLogProbMetric: 45.4798 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 247/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:28:19.020 
Epoch 247/1000 
	 loss: 43.6937, MinusLogProbMetric: 43.6937, val_loss: 45.6129, val_MinusLogProbMetric: 45.6129

Epoch 247: val_loss did not improve from 45.22374
196/196 - 42s - loss: 43.6937 - MinusLogProbMetric: 43.6937 - val_loss: 45.6129 - val_MinusLogProbMetric: 45.6129 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 248/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:29:00.299 
Epoch 248/1000 
	 loss: 43.6720, MinusLogProbMetric: 43.6720, val_loss: 45.4558, val_MinusLogProbMetric: 45.4558

Epoch 248: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.6720 - MinusLogProbMetric: 43.6720 - val_loss: 45.4558 - val_MinusLogProbMetric: 45.4558 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 249/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:29:40.978 
Epoch 249/1000 
	 loss: 43.8957, MinusLogProbMetric: 43.8957, val_loss: 45.5141, val_MinusLogProbMetric: 45.5141

Epoch 249: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.8957 - MinusLogProbMetric: 43.8957 - val_loss: 45.5141 - val_MinusLogProbMetric: 45.5141 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 250/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:30:21.469 
Epoch 250/1000 
	 loss: 43.7334, MinusLogProbMetric: 43.7334, val_loss: 45.2580, val_MinusLogProbMetric: 45.2580

Epoch 250: val_loss did not improve from 45.22374
196/196 - 40s - loss: 43.7334 - MinusLogProbMetric: 43.7334 - val_loss: 45.2580 - val_MinusLogProbMetric: 45.2580 - lr: 4.1152e-06 - 40s/epoch - 207ms/step
Epoch 251/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:31:02.284 
Epoch 251/1000 
	 loss: 43.6637, MinusLogProbMetric: 43.6637, val_loss: 45.2676, val_MinusLogProbMetric: 45.2676

Epoch 251: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.6637 - MinusLogProbMetric: 43.6637 - val_loss: 45.2676 - val_MinusLogProbMetric: 45.2676 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 252/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:31:43.666 
Epoch 252/1000 
	 loss: 43.6561, MinusLogProbMetric: 43.6561, val_loss: 45.3774, val_MinusLogProbMetric: 45.3774

Epoch 252: val_loss did not improve from 45.22374
196/196 - 41s - loss: 43.6561 - MinusLogProbMetric: 43.6561 - val_loss: 45.3774 - val_MinusLogProbMetric: 45.3774 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 253/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:32:24.005 
Epoch 253/1000 
	 loss: 43.6108, MinusLogProbMetric: 43.6108, val_loss: 45.2293, val_MinusLogProbMetric: 45.2292

Epoch 253: val_loss did not improve from 45.22374
196/196 - 40s - loss: 43.6108 - MinusLogProbMetric: 43.6108 - val_loss: 45.2293 - val_MinusLogProbMetric: 45.2292 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 254/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:33:04.730 
Epoch 254/1000 
	 loss: 43.7742, MinusLogProbMetric: 43.7742, val_loss: 45.1162, val_MinusLogProbMetric: 45.1161

Epoch 254: val_loss improved from 45.22374 to 45.11617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 43.7742 - MinusLogProbMetric: 43.7742 - val_loss: 45.1162 - val_MinusLogProbMetric: 45.1161 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 255/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:33:44.496 
Epoch 255/1000 
	 loss: 43.6601, MinusLogProbMetric: 43.6601, val_loss: 45.1095, val_MinusLogProbMetric: 45.1095

Epoch 255: val_loss improved from 45.11617 to 45.10952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 40s - loss: 43.6601 - MinusLogProbMetric: 43.6601 - val_loss: 45.1095 - val_MinusLogProbMetric: 45.1095 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 256/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:34:24.767 
Epoch 256/1000 
	 loss: 43.9445, MinusLogProbMetric: 43.9445, val_loss: 46.6239, val_MinusLogProbMetric: 46.6239

Epoch 256: val_loss did not improve from 45.10952
196/196 - 39s - loss: 43.9445 - MinusLogProbMetric: 43.9445 - val_loss: 46.6239 - val_MinusLogProbMetric: 46.6239 - lr: 4.1152e-06 - 39s/epoch - 201ms/step
Epoch 257/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:35:05.519 
Epoch 257/1000 
	 loss: 43.8438, MinusLogProbMetric: 43.8438, val_loss: 45.2512, val_MinusLogProbMetric: 45.2512

Epoch 257: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.8438 - MinusLogProbMetric: 43.8438 - val_loss: 45.2512 - val_MinusLogProbMetric: 45.2512 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 258/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:35:45.748 
Epoch 258/1000 
	 loss: 43.6431, MinusLogProbMetric: 43.6431, val_loss: 45.7862, val_MinusLogProbMetric: 45.7862

Epoch 258: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.6431 - MinusLogProbMetric: 43.6431 - val_loss: 45.7862 - val_MinusLogProbMetric: 45.7862 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 259/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:36:26.239 
Epoch 259/1000 
	 loss: 43.6897, MinusLogProbMetric: 43.6897, val_loss: 45.4405, val_MinusLogProbMetric: 45.4405

Epoch 259: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.6897 - MinusLogProbMetric: 43.6897 - val_loss: 45.4405 - val_MinusLogProbMetric: 45.4405 - lr: 4.1152e-06 - 40s/epoch - 207ms/step
Epoch 260/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:37:06.647 
Epoch 260/1000 
	 loss: 43.7060, MinusLogProbMetric: 43.7060, val_loss: 45.3105, val_MinusLogProbMetric: 45.3104

Epoch 260: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.7060 - MinusLogProbMetric: 43.7060 - val_loss: 45.3105 - val_MinusLogProbMetric: 45.3104 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 261/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:37:47.164 
Epoch 261/1000 
	 loss: 43.6842, MinusLogProbMetric: 43.6842, val_loss: 45.5988, val_MinusLogProbMetric: 45.5988

Epoch 261: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6842 - MinusLogProbMetric: 43.6842 - val_loss: 45.5988 - val_MinusLogProbMetric: 45.5988 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 262/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:38:27.873 
Epoch 262/1000 
	 loss: 43.6490, MinusLogProbMetric: 43.6490, val_loss: 45.3671, val_MinusLogProbMetric: 45.3670

Epoch 262: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6490 - MinusLogProbMetric: 43.6490 - val_loss: 45.3671 - val_MinusLogProbMetric: 45.3670 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 263/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:39:08.661 
Epoch 263/1000 
	 loss: 43.6577, MinusLogProbMetric: 43.6577, val_loss: 45.5556, val_MinusLogProbMetric: 45.5556

Epoch 263: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6577 - MinusLogProbMetric: 43.6577 - val_loss: 45.5556 - val_MinusLogProbMetric: 45.5556 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 264/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:39:49.962 
Epoch 264/1000 
	 loss: 43.6168, MinusLogProbMetric: 43.6168, val_loss: 45.4160, val_MinusLogProbMetric: 45.4160

Epoch 264: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6168 - MinusLogProbMetric: 43.6168 - val_loss: 45.4160 - val_MinusLogProbMetric: 45.4160 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 265/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:40:31.377 
Epoch 265/1000 
	 loss: 43.6681, MinusLogProbMetric: 43.6681, val_loss: 45.5387, val_MinusLogProbMetric: 45.5387

Epoch 265: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6681 - MinusLogProbMetric: 43.6681 - val_loss: 45.5387 - val_MinusLogProbMetric: 45.5387 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 266/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:41:12.696 
Epoch 266/1000 
	 loss: 43.6368, MinusLogProbMetric: 43.6368, val_loss: 45.4221, val_MinusLogProbMetric: 45.4221

Epoch 266: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6368 - MinusLogProbMetric: 43.6368 - val_loss: 45.4221 - val_MinusLogProbMetric: 45.4221 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 267/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:41:53.145 
Epoch 267/1000 
	 loss: 43.6739, MinusLogProbMetric: 43.6739, val_loss: 45.7341, val_MinusLogProbMetric: 45.7341

Epoch 267: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.6739 - MinusLogProbMetric: 43.6739 - val_loss: 45.7341 - val_MinusLogProbMetric: 45.7341 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 268/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:42:32.713 
Epoch 268/1000 
	 loss: 43.6384, MinusLogProbMetric: 43.6384, val_loss: 45.7176, val_MinusLogProbMetric: 45.7175

Epoch 268: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.6384 - MinusLogProbMetric: 43.6384 - val_loss: 45.7176 - val_MinusLogProbMetric: 45.7175 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 269/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:43:13.451 
Epoch 269/1000 
	 loss: 43.5967, MinusLogProbMetric: 43.5967, val_loss: 45.6423, val_MinusLogProbMetric: 45.6423

Epoch 269: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.5967 - MinusLogProbMetric: 43.5967 - val_loss: 45.6423 - val_MinusLogProbMetric: 45.6423 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 270/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:43:54.561 
Epoch 270/1000 
	 loss: 43.6247, MinusLogProbMetric: 43.6247, val_loss: 45.5558, val_MinusLogProbMetric: 45.5557

Epoch 270: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6247 - MinusLogProbMetric: 43.6247 - val_loss: 45.5558 - val_MinusLogProbMetric: 45.5557 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 271/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:44:35.821 
Epoch 271/1000 
	 loss: 43.6768, MinusLogProbMetric: 43.6768, val_loss: 45.5244, val_MinusLogProbMetric: 45.5244

Epoch 271: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6768 - MinusLogProbMetric: 43.6768 - val_loss: 45.5244 - val_MinusLogProbMetric: 45.5244 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 272/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:45:16.425 
Epoch 272/1000 
	 loss: 43.6366, MinusLogProbMetric: 43.6366, val_loss: 45.7234, val_MinusLogProbMetric: 45.7234

Epoch 272: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6366 - MinusLogProbMetric: 43.6366 - val_loss: 45.7234 - val_MinusLogProbMetric: 45.7234 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 273/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:45:57.051 
Epoch 273/1000 
	 loss: 43.7399, MinusLogProbMetric: 43.7399, val_loss: 45.8502, val_MinusLogProbMetric: 45.8502

Epoch 273: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.7399 - MinusLogProbMetric: 43.7399 - val_loss: 45.8502 - val_MinusLogProbMetric: 45.8502 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 274/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:46:38.192 
Epoch 274/1000 
	 loss: 43.6533, MinusLogProbMetric: 43.6533, val_loss: 45.2955, val_MinusLogProbMetric: 45.2955

Epoch 274: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6533 - MinusLogProbMetric: 43.6533 - val_loss: 45.2955 - val_MinusLogProbMetric: 45.2955 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 275/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:47:19.150 
Epoch 275/1000 
	 loss: 43.6253, MinusLogProbMetric: 43.6253, val_loss: 45.5415, val_MinusLogProbMetric: 45.5415

Epoch 275: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6253 - MinusLogProbMetric: 43.6253 - val_loss: 45.5415 - val_MinusLogProbMetric: 45.5415 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 276/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:47:58.710 
Epoch 276/1000 
	 loss: 43.6838, MinusLogProbMetric: 43.6838, val_loss: 45.3873, val_MinusLogProbMetric: 45.3873

Epoch 276: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.6838 - MinusLogProbMetric: 43.6838 - val_loss: 45.3873 - val_MinusLogProbMetric: 45.3873 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 277/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:48:38.732 
Epoch 277/1000 
	 loss: 43.5775, MinusLogProbMetric: 43.5775, val_loss: 45.6369, val_MinusLogProbMetric: 45.6369

Epoch 277: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.5775 - MinusLogProbMetric: 43.5775 - val_loss: 45.6369 - val_MinusLogProbMetric: 45.6369 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 278/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:49:19.148 
Epoch 278/1000 
	 loss: 43.7336, MinusLogProbMetric: 43.7336, val_loss: 45.9135, val_MinusLogProbMetric: 45.9134

Epoch 278: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.7336 - MinusLogProbMetric: 43.7336 - val_loss: 45.9135 - val_MinusLogProbMetric: 45.9134 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 279/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:50:00.078 
Epoch 279/1000 
	 loss: 43.7920, MinusLogProbMetric: 43.7920, val_loss: 45.2338, val_MinusLogProbMetric: 45.2337

Epoch 279: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.7920 - MinusLogProbMetric: 43.7920 - val_loss: 45.2338 - val_MinusLogProbMetric: 45.2337 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 280/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:50:41.209 
Epoch 280/1000 
	 loss: 43.6794, MinusLogProbMetric: 43.6794, val_loss: 45.1245, val_MinusLogProbMetric: 45.1245

Epoch 280: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.6794 - MinusLogProbMetric: 43.6794 - val_loss: 45.1245 - val_MinusLogProbMetric: 45.1245 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 281/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:51:21.491 
Epoch 281/1000 
	 loss: 43.7798, MinusLogProbMetric: 43.7798, val_loss: 45.1322, val_MinusLogProbMetric: 45.1322

Epoch 281: val_loss did not improve from 45.10952
196/196 - 40s - loss: 43.7798 - MinusLogProbMetric: 43.7798 - val_loss: 45.1322 - val_MinusLogProbMetric: 45.1322 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 282/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:52:02.889 
Epoch 282/1000 
	 loss: 43.5690, MinusLogProbMetric: 43.5690, val_loss: 45.4510, val_MinusLogProbMetric: 45.4510

Epoch 282: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.5690 - MinusLogProbMetric: 43.5690 - val_loss: 45.4510 - val_MinusLogProbMetric: 45.4510 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 283/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:52:43.597 
Epoch 283/1000 
	 loss: 43.5505, MinusLogProbMetric: 43.5505, val_loss: 45.4831, val_MinusLogProbMetric: 45.4831

Epoch 283: val_loss did not improve from 45.10952
196/196 - 41s - loss: 43.5505 - MinusLogProbMetric: 43.5505 - val_loss: 45.4831 - val_MinusLogProbMetric: 45.4831 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 284/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:53:25.076 
Epoch 284/1000 
	 loss: 43.5253, MinusLogProbMetric: 43.5253, val_loss: 45.0999, val_MinusLogProbMetric: 45.0999

Epoch 284: val_loss improved from 45.10952 to 45.09990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 43.5253 - MinusLogProbMetric: 43.5253 - val_loss: 45.0999 - val_MinusLogProbMetric: 45.0999 - lr: 4.1152e-06 - 42s/epoch - 216ms/step
Epoch 285/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:54:06.951 
Epoch 285/1000 
	 loss: 43.6682, MinusLogProbMetric: 43.6682, val_loss: 45.1159, val_MinusLogProbMetric: 45.1159

Epoch 285: val_loss did not improve from 45.09990
196/196 - 41s - loss: 43.6682 - MinusLogProbMetric: 43.6682 - val_loss: 45.1159 - val_MinusLogProbMetric: 45.1159 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 286/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:54:45.683 
Epoch 286/1000 
	 loss: 43.6623, MinusLogProbMetric: 43.6623, val_loss: 45.2710, val_MinusLogProbMetric: 45.2710

Epoch 286: val_loss did not improve from 45.09990
196/196 - 39s - loss: 43.6623 - MinusLogProbMetric: 43.6623 - val_loss: 45.2710 - val_MinusLogProbMetric: 45.2710 - lr: 4.1152e-06 - 39s/epoch - 198ms/step
Epoch 287/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:55:24.055 
Epoch 287/1000 
	 loss: 43.6021, MinusLogProbMetric: 43.6021, val_loss: 45.3740, val_MinusLogProbMetric: 45.3740

Epoch 287: val_loss did not improve from 45.09990
196/196 - 38s - loss: 43.6021 - MinusLogProbMetric: 43.6021 - val_loss: 45.3740 - val_MinusLogProbMetric: 45.3740 - lr: 4.1152e-06 - 38s/epoch - 196ms/step
Epoch 288/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:56:03.983 
Epoch 288/1000 
	 loss: 43.5685, MinusLogProbMetric: 43.5685, val_loss: 45.3540, val_MinusLogProbMetric: 45.3540

Epoch 288: val_loss did not improve from 45.09990
196/196 - 40s - loss: 43.5685 - MinusLogProbMetric: 43.5685 - val_loss: 45.3540 - val_MinusLogProbMetric: 45.3540 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 289/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:56:43.487 
Epoch 289/1000 
	 loss: 43.5761, MinusLogProbMetric: 43.5761, val_loss: 45.3484, val_MinusLogProbMetric: 45.3484

Epoch 289: val_loss did not improve from 45.09990
196/196 - 40s - loss: 43.5761 - MinusLogProbMetric: 43.5761 - val_loss: 45.3484 - val_MinusLogProbMetric: 45.3484 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 290/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:57:24.497 
Epoch 290/1000 
	 loss: 43.5789, MinusLogProbMetric: 43.5789, val_loss: 45.0491, val_MinusLogProbMetric: 45.0490

Epoch 290: val_loss improved from 45.09990 to 45.04906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 43.5789 - MinusLogProbMetric: 43.5789 - val_loss: 45.0491 - val_MinusLogProbMetric: 45.0490 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 291/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:58:05.824 
Epoch 291/1000 
	 loss: 43.6109, MinusLogProbMetric: 43.6109, val_loss: 46.5360, val_MinusLogProbMetric: 46.5361

Epoch 291: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.6109 - MinusLogProbMetric: 43.6109 - val_loss: 46.5360 - val_MinusLogProbMetric: 46.5361 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 292/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:58:46.770 
Epoch 292/1000 
	 loss: 43.6470, MinusLogProbMetric: 43.6470, val_loss: 45.2251, val_MinusLogProbMetric: 45.2251

Epoch 292: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.6470 - MinusLogProbMetric: 43.6470 - val_loss: 45.2251 - val_MinusLogProbMetric: 45.2251 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 293/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 06:59:29.147 
Epoch 293/1000 
	 loss: 43.5269, MinusLogProbMetric: 43.5269, val_loss: 45.7098, val_MinusLogProbMetric: 45.7098

Epoch 293: val_loss did not improve from 45.04906
196/196 - 42s - loss: 43.5269 - MinusLogProbMetric: 43.5269 - val_loss: 45.7098 - val_MinusLogProbMetric: 45.7098 - lr: 4.1152e-06 - 42s/epoch - 216ms/step
Epoch 294/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:00:10.164 
Epoch 294/1000 
	 loss: 43.5671, MinusLogProbMetric: 43.5671, val_loss: 45.1638, val_MinusLogProbMetric: 45.1638

Epoch 294: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.5671 - MinusLogProbMetric: 43.5671 - val_loss: 45.1638 - val_MinusLogProbMetric: 45.1638 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 295/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:00:50.432 
Epoch 295/1000 
	 loss: 43.4963, MinusLogProbMetric: 43.4963, val_loss: 45.4434, val_MinusLogProbMetric: 45.4434

Epoch 295: val_loss did not improve from 45.04906
196/196 - 40s - loss: 43.4963 - MinusLogProbMetric: 43.4963 - val_loss: 45.4434 - val_MinusLogProbMetric: 45.4434 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 296/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:01:30.006 
Epoch 296/1000 
	 loss: 43.5279, MinusLogProbMetric: 43.5279, val_loss: 45.3022, val_MinusLogProbMetric: 45.3021

Epoch 296: val_loss did not improve from 45.04906
196/196 - 40s - loss: 43.5279 - MinusLogProbMetric: 43.5279 - val_loss: 45.3022 - val_MinusLogProbMetric: 45.3021 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 297/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:02:10.749 
Epoch 297/1000 
	 loss: 43.5396, MinusLogProbMetric: 43.5396, val_loss: 45.5156, val_MinusLogProbMetric: 45.5156

Epoch 297: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.5396 - MinusLogProbMetric: 43.5396 - val_loss: 45.5156 - val_MinusLogProbMetric: 45.5156 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 298/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:02:51.478 
Epoch 298/1000 
	 loss: 43.7986, MinusLogProbMetric: 43.7986, val_loss: 45.8312, val_MinusLogProbMetric: 45.8311

Epoch 298: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.7986 - MinusLogProbMetric: 43.7986 - val_loss: 45.8312 - val_MinusLogProbMetric: 45.8311 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 299/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:03:32.216 
Epoch 299/1000 
	 loss: 43.5452, MinusLogProbMetric: 43.5452, val_loss: 45.2771, val_MinusLogProbMetric: 45.2771

Epoch 299: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.5452 - MinusLogProbMetric: 43.5452 - val_loss: 45.2771 - val_MinusLogProbMetric: 45.2771 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 300/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:04:13.286 
Epoch 300/1000 
	 loss: 43.5426, MinusLogProbMetric: 43.5426, val_loss: 45.9695, val_MinusLogProbMetric: 45.9695

Epoch 300: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.5426 - MinusLogProbMetric: 43.5426 - val_loss: 45.9695 - val_MinusLogProbMetric: 45.9695 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 301/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:04:53.647 
Epoch 301/1000 
	 loss: 43.5715, MinusLogProbMetric: 43.5715, val_loss: 45.2863, val_MinusLogProbMetric: 45.2863

Epoch 301: val_loss did not improve from 45.04906
196/196 - 40s - loss: 43.5715 - MinusLogProbMetric: 43.5715 - val_loss: 45.2863 - val_MinusLogProbMetric: 45.2863 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 302/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:05:34.534 
Epoch 302/1000 
	 loss: 43.5254, MinusLogProbMetric: 43.5254, val_loss: 45.2687, val_MinusLogProbMetric: 45.2687

Epoch 302: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.5254 - MinusLogProbMetric: 43.5254 - val_loss: 45.2687 - val_MinusLogProbMetric: 45.2687 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 303/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:06:15.464 
Epoch 303/1000 
	 loss: 43.4430, MinusLogProbMetric: 43.4430, val_loss: 45.5420, val_MinusLogProbMetric: 45.5420

Epoch 303: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.4430 - MinusLogProbMetric: 43.4430 - val_loss: 45.5420 - val_MinusLogProbMetric: 45.5420 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 304/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:06:56.007 
Epoch 304/1000 
	 loss: 43.4665, MinusLogProbMetric: 43.4665, val_loss: 45.1913, val_MinusLogProbMetric: 45.1913

Epoch 304: val_loss did not improve from 45.04906
196/196 - 41s - loss: 43.4665 - MinusLogProbMetric: 43.4665 - val_loss: 45.1913 - val_MinusLogProbMetric: 45.1913 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 305/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:07:36.722 
Epoch 305/1000 
	 loss: 43.6820, MinusLogProbMetric: 43.6820, val_loss: 44.9355, val_MinusLogProbMetric: 44.9355

Epoch 305: val_loss improved from 45.04906 to 44.93554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 43.6820 - MinusLogProbMetric: 43.6820 - val_loss: 44.9355 - val_MinusLogProbMetric: 44.9355 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 306/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:08:17.535 
Epoch 306/1000 
	 loss: 43.5861, MinusLogProbMetric: 43.5861, val_loss: 45.5601, val_MinusLogProbMetric: 45.5601

Epoch 306: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.5861 - MinusLogProbMetric: 43.5861 - val_loss: 45.5601 - val_MinusLogProbMetric: 45.5601 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 307/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:08:57.768 
Epoch 307/1000 
	 loss: 43.5107, MinusLogProbMetric: 43.5107, val_loss: 44.9973, val_MinusLogProbMetric: 44.9973

Epoch 307: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.5107 - MinusLogProbMetric: 43.5107 - val_loss: 44.9973 - val_MinusLogProbMetric: 44.9973 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 308/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:09:38.712 
Epoch 308/1000 
	 loss: 43.8971, MinusLogProbMetric: 43.8971, val_loss: 45.9175, val_MinusLogProbMetric: 45.9175

Epoch 308: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.8971 - MinusLogProbMetric: 43.8971 - val_loss: 45.9175 - val_MinusLogProbMetric: 45.9175 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 309/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:10:19.037 
Epoch 309/1000 
	 loss: 43.5631, MinusLogProbMetric: 43.5631, val_loss: 46.1138, val_MinusLogProbMetric: 46.1138

Epoch 309: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.5631 - MinusLogProbMetric: 43.5631 - val_loss: 46.1138 - val_MinusLogProbMetric: 46.1138 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 310/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:10:59.465 
Epoch 310/1000 
	 loss: 43.7190, MinusLogProbMetric: 43.7190, val_loss: 45.2584, val_MinusLogProbMetric: 45.2584

Epoch 310: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.7190 - MinusLogProbMetric: 43.7190 - val_loss: 45.2584 - val_MinusLogProbMetric: 45.2584 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 311/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:11:38.613 
Epoch 311/1000 
	 loss: 43.6304, MinusLogProbMetric: 43.6304, val_loss: 45.0335, val_MinusLogProbMetric: 45.0335

Epoch 311: val_loss did not improve from 44.93554
196/196 - 39s - loss: 43.6304 - MinusLogProbMetric: 43.6304 - val_loss: 45.0335 - val_MinusLogProbMetric: 45.0335 - lr: 4.1152e-06 - 39s/epoch - 200ms/step
Epoch 312/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:12:19.465 
Epoch 312/1000 
	 loss: 43.4709, MinusLogProbMetric: 43.4709, val_loss: 44.9838, val_MinusLogProbMetric: 44.9838

Epoch 312: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4709 - MinusLogProbMetric: 43.4709 - val_loss: 44.9838 - val_MinusLogProbMetric: 44.9838 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 313/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:12:59.812 
Epoch 313/1000 
	 loss: 43.5312, MinusLogProbMetric: 43.5312, val_loss: 45.6194, val_MinusLogProbMetric: 45.6194

Epoch 313: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.5312 - MinusLogProbMetric: 43.5312 - val_loss: 45.6194 - val_MinusLogProbMetric: 45.6194 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 314/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:13:40.202 
Epoch 314/1000 
	 loss: 43.5322, MinusLogProbMetric: 43.5322, val_loss: 45.5226, val_MinusLogProbMetric: 45.5226

Epoch 314: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.5322 - MinusLogProbMetric: 43.5322 - val_loss: 45.5226 - val_MinusLogProbMetric: 45.5226 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 315/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:14:21.368 
Epoch 315/1000 
	 loss: 43.5776, MinusLogProbMetric: 43.5776, val_loss: 45.1115, val_MinusLogProbMetric: 45.1116

Epoch 315: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.5776 - MinusLogProbMetric: 43.5776 - val_loss: 45.1115 - val_MinusLogProbMetric: 45.1116 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 316/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:15:02.116 
Epoch 316/1000 
	 loss: 43.5213, MinusLogProbMetric: 43.5213, val_loss: 46.1091, val_MinusLogProbMetric: 46.1091

Epoch 316: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.5213 - MinusLogProbMetric: 43.5213 - val_loss: 46.1091 - val_MinusLogProbMetric: 46.1091 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 317/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:15:42.976 
Epoch 317/1000 
	 loss: 43.4272, MinusLogProbMetric: 43.4272, val_loss: 45.4603, val_MinusLogProbMetric: 45.4602

Epoch 317: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4272 - MinusLogProbMetric: 43.4272 - val_loss: 45.4603 - val_MinusLogProbMetric: 45.4602 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 318/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:16:23.985 
Epoch 318/1000 
	 loss: 43.4454, MinusLogProbMetric: 43.4454, val_loss: 45.2366, val_MinusLogProbMetric: 45.2366

Epoch 318: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4454 - MinusLogProbMetric: 43.4454 - val_loss: 45.2366 - val_MinusLogProbMetric: 45.2366 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 319/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:17:05.057 
Epoch 319/1000 
	 loss: 43.6055, MinusLogProbMetric: 43.6055, val_loss: 45.3876, val_MinusLogProbMetric: 45.3876

Epoch 319: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.6055 - MinusLogProbMetric: 43.6055 - val_loss: 45.3876 - val_MinusLogProbMetric: 45.3876 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 320/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:17:45.340 
Epoch 320/1000 
	 loss: 43.5342, MinusLogProbMetric: 43.5342, val_loss: 46.0113, val_MinusLogProbMetric: 46.0112

Epoch 320: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.5342 - MinusLogProbMetric: 43.5342 - val_loss: 46.0113 - val_MinusLogProbMetric: 46.0112 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 321/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:18:25.756 
Epoch 321/1000 
	 loss: 43.4686, MinusLogProbMetric: 43.4686, val_loss: 45.2728, val_MinusLogProbMetric: 45.2727

Epoch 321: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.4686 - MinusLogProbMetric: 43.4686 - val_loss: 45.2728 - val_MinusLogProbMetric: 45.2727 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 322/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:19:05.842 
Epoch 322/1000 
	 loss: 43.5070, MinusLogProbMetric: 43.5070, val_loss: 45.0133, val_MinusLogProbMetric: 45.0133

Epoch 322: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.5070 - MinusLogProbMetric: 43.5070 - val_loss: 45.0133 - val_MinusLogProbMetric: 45.0133 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 323/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:19:47.629 
Epoch 323/1000 
	 loss: 43.4982, MinusLogProbMetric: 43.4982, val_loss: 45.3704, val_MinusLogProbMetric: 45.3703

Epoch 323: val_loss did not improve from 44.93554
196/196 - 42s - loss: 43.4982 - MinusLogProbMetric: 43.4982 - val_loss: 45.3704 - val_MinusLogProbMetric: 45.3703 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 324/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:20:28.524 
Epoch 324/1000 
	 loss: 43.3959, MinusLogProbMetric: 43.3959, val_loss: 45.4347, val_MinusLogProbMetric: 45.4346

Epoch 324: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.3959 - MinusLogProbMetric: 43.3959 - val_loss: 45.4347 - val_MinusLogProbMetric: 45.4346 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 325/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:21:10.052 
Epoch 325/1000 
	 loss: 43.4148, MinusLogProbMetric: 43.4148, val_loss: 45.2152, val_MinusLogProbMetric: 45.2151

Epoch 325: val_loss did not improve from 44.93554
196/196 - 42s - loss: 43.4148 - MinusLogProbMetric: 43.4148 - val_loss: 45.2152 - val_MinusLogProbMetric: 45.2151 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 326/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:21:51.309 
Epoch 326/1000 
	 loss: 43.4235, MinusLogProbMetric: 43.4235, val_loss: 45.4104, val_MinusLogProbMetric: 45.4104

Epoch 326: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4235 - MinusLogProbMetric: 43.4235 - val_loss: 45.4104 - val_MinusLogProbMetric: 45.4104 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 327/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:22:33.290 
Epoch 327/1000 
	 loss: 43.4059, MinusLogProbMetric: 43.4059, val_loss: 45.6100, val_MinusLogProbMetric: 45.6100

Epoch 327: val_loss did not improve from 44.93554
196/196 - 42s - loss: 43.4059 - MinusLogProbMetric: 43.4059 - val_loss: 45.6100 - val_MinusLogProbMetric: 45.6100 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 328/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:23:14.705 
Epoch 328/1000 
	 loss: 43.6513, MinusLogProbMetric: 43.6513, val_loss: 45.1103, val_MinusLogProbMetric: 45.1103

Epoch 328: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.6513 - MinusLogProbMetric: 43.6513 - val_loss: 45.1103 - val_MinusLogProbMetric: 45.1103 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 329/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:23:55.663 
Epoch 329/1000 
	 loss: 43.4451, MinusLogProbMetric: 43.4451, val_loss: 45.0616, val_MinusLogProbMetric: 45.0616

Epoch 329: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4451 - MinusLogProbMetric: 43.4451 - val_loss: 45.0616 - val_MinusLogProbMetric: 45.0616 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 330/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:24:36.279 
Epoch 330/1000 
	 loss: 43.4172, MinusLogProbMetric: 43.4172, val_loss: 45.6205, val_MinusLogProbMetric: 45.6205

Epoch 330: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4172 - MinusLogProbMetric: 43.4172 - val_loss: 45.6205 - val_MinusLogProbMetric: 45.6205 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 331/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:25:15.394 
Epoch 331/1000 
	 loss: 43.5092, MinusLogProbMetric: 43.5092, val_loss: 45.1927, val_MinusLogProbMetric: 45.1927

Epoch 331: val_loss did not improve from 44.93554
196/196 - 39s - loss: 43.5092 - MinusLogProbMetric: 43.5092 - val_loss: 45.1927 - val_MinusLogProbMetric: 45.1927 - lr: 4.1152e-06 - 39s/epoch - 200ms/step
Epoch 332/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:25:54.923 
Epoch 332/1000 
	 loss: 43.4135, MinusLogProbMetric: 43.4135, val_loss: 45.2579, val_MinusLogProbMetric: 45.2578

Epoch 332: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.4135 - MinusLogProbMetric: 43.4135 - val_loss: 45.2579 - val_MinusLogProbMetric: 45.2578 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 333/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:26:34.149 
Epoch 333/1000 
	 loss: 43.5512, MinusLogProbMetric: 43.5512, val_loss: 45.8369, val_MinusLogProbMetric: 45.8369

Epoch 333: val_loss did not improve from 44.93554
196/196 - 39s - loss: 43.5512 - MinusLogProbMetric: 43.5512 - val_loss: 45.8369 - val_MinusLogProbMetric: 45.8369 - lr: 4.1152e-06 - 39s/epoch - 200ms/step
Epoch 334/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:27:13.995 
Epoch 334/1000 
	 loss: 43.4340, MinusLogProbMetric: 43.4340, val_loss: 45.0745, val_MinusLogProbMetric: 45.0744

Epoch 334: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.4340 - MinusLogProbMetric: 43.4340 - val_loss: 45.0745 - val_MinusLogProbMetric: 45.0744 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 335/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:27:54.453 
Epoch 335/1000 
	 loss: 43.3810, MinusLogProbMetric: 43.3810, val_loss: 44.9984, val_MinusLogProbMetric: 44.9983

Epoch 335: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.3810 - MinusLogProbMetric: 43.3810 - val_loss: 44.9984 - val_MinusLogProbMetric: 44.9983 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 336/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:28:34.786 
Epoch 336/1000 
	 loss: 43.4232, MinusLogProbMetric: 43.4232, val_loss: 45.1786, val_MinusLogProbMetric: 45.1785

Epoch 336: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.4232 - MinusLogProbMetric: 43.4232 - val_loss: 45.1786 - val_MinusLogProbMetric: 45.1785 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 337/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:29:15.059 
Epoch 337/1000 
	 loss: 43.5609, MinusLogProbMetric: 43.5609, val_loss: 45.3851, val_MinusLogProbMetric: 45.3851

Epoch 337: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.5609 - MinusLogProbMetric: 43.5609 - val_loss: 45.3851 - val_MinusLogProbMetric: 45.3851 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 338/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:29:55.362 
Epoch 338/1000 
	 loss: 43.4941, MinusLogProbMetric: 43.4941, val_loss: 45.2364, val_MinusLogProbMetric: 45.2364

Epoch 338: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.4941 - MinusLogProbMetric: 43.4941 - val_loss: 45.2364 - val_MinusLogProbMetric: 45.2364 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 339/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:30:35.952 
Epoch 339/1000 
	 loss: 43.4459, MinusLogProbMetric: 43.4459, val_loss: 45.3503, val_MinusLogProbMetric: 45.3503

Epoch 339: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4459 - MinusLogProbMetric: 43.4459 - val_loss: 45.3503 - val_MinusLogProbMetric: 45.3503 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 340/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:31:14.999 
Epoch 340/1000 
	 loss: 43.4890, MinusLogProbMetric: 43.4890, val_loss: 45.2387, val_MinusLogProbMetric: 45.2387

Epoch 340: val_loss did not improve from 44.93554
196/196 - 39s - loss: 43.4890 - MinusLogProbMetric: 43.4890 - val_loss: 45.2387 - val_MinusLogProbMetric: 45.2387 - lr: 4.1152e-06 - 39s/epoch - 199ms/step
Epoch 341/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:31:56.150 
Epoch 341/1000 
	 loss: 43.4770, MinusLogProbMetric: 43.4770, val_loss: 45.4039, val_MinusLogProbMetric: 45.4039

Epoch 341: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4770 - MinusLogProbMetric: 43.4770 - val_loss: 45.4039 - val_MinusLogProbMetric: 45.4039 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 342/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:32:37.709 
Epoch 342/1000 
	 loss: 43.4144, MinusLogProbMetric: 43.4144, val_loss: 45.1140, val_MinusLogProbMetric: 45.1140

Epoch 342: val_loss did not improve from 44.93554
196/196 - 42s - loss: 43.4144 - MinusLogProbMetric: 43.4144 - val_loss: 45.1140 - val_MinusLogProbMetric: 45.1140 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 343/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:33:19.298 
Epoch 343/1000 
	 loss: 43.3764, MinusLogProbMetric: 43.3764, val_loss: 45.1129, val_MinusLogProbMetric: 45.1128

Epoch 343: val_loss did not improve from 44.93554
196/196 - 42s - loss: 43.3764 - MinusLogProbMetric: 43.3764 - val_loss: 45.1129 - val_MinusLogProbMetric: 45.1128 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 344/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:34:00.343 
Epoch 344/1000 
	 loss: 43.5320, MinusLogProbMetric: 43.5320, val_loss: 45.3938, val_MinusLogProbMetric: 45.3938

Epoch 344: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.5320 - MinusLogProbMetric: 43.5320 - val_loss: 45.3938 - val_MinusLogProbMetric: 45.3938 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 345/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:34:40.251 
Epoch 345/1000 
	 loss: 43.4671, MinusLogProbMetric: 43.4671, val_loss: 45.5855, val_MinusLogProbMetric: 45.5854

Epoch 345: val_loss did not improve from 44.93554
196/196 - 40s - loss: 43.4671 - MinusLogProbMetric: 43.4671 - val_loss: 45.5855 - val_MinusLogProbMetric: 45.5854 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 346/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:35:21.470 
Epoch 346/1000 
	 loss: 43.4322, MinusLogProbMetric: 43.4322, val_loss: 44.9564, val_MinusLogProbMetric: 44.9564

Epoch 346: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4322 - MinusLogProbMetric: 43.4322 - val_loss: 44.9564 - val_MinusLogProbMetric: 44.9564 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 347/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:36:03.448 
Epoch 347/1000 
	 loss: 43.4120, MinusLogProbMetric: 43.4120, val_loss: 45.2193, val_MinusLogProbMetric: 45.2193

Epoch 347: val_loss did not improve from 44.93554
196/196 - 42s - loss: 43.4120 - MinusLogProbMetric: 43.4120 - val_loss: 45.2193 - val_MinusLogProbMetric: 45.2193 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 348/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:36:44.121 
Epoch 348/1000 
	 loss: 43.4142, MinusLogProbMetric: 43.4142, val_loss: 45.5119, val_MinusLogProbMetric: 45.5119

Epoch 348: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.4142 - MinusLogProbMetric: 43.4142 - val_loss: 45.5119 - val_MinusLogProbMetric: 45.5119 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 349/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:37:25.051 
Epoch 349/1000 
	 loss: 43.3739, MinusLogProbMetric: 43.3739, val_loss: 45.5776, val_MinusLogProbMetric: 45.5776

Epoch 349: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.3739 - MinusLogProbMetric: 43.3739 - val_loss: 45.5776 - val_MinusLogProbMetric: 45.5776 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 350/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:38:06.103 
Epoch 350/1000 
	 loss: 43.3508, MinusLogProbMetric: 43.3508, val_loss: 45.3122, val_MinusLogProbMetric: 45.3122

Epoch 350: val_loss did not improve from 44.93554
196/196 - 41s - loss: 43.3508 - MinusLogProbMetric: 43.3508 - val_loss: 45.3122 - val_MinusLogProbMetric: 45.3122 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 351/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:38:46.440 
Epoch 351/1000 
	 loss: 43.5013, MinusLogProbMetric: 43.5013, val_loss: 44.8682, val_MinusLogProbMetric: 44.8682

Epoch 351: val_loss improved from 44.93554 to 44.86818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 43.5013 - MinusLogProbMetric: 43.5013 - val_loss: 44.8682 - val_MinusLogProbMetric: 44.8682 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 352/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:39:27.396 
Epoch 352/1000 
	 loss: 43.5595, MinusLogProbMetric: 43.5595, val_loss: 45.3912, val_MinusLogProbMetric: 45.3912

Epoch 352: val_loss did not improve from 44.86818
196/196 - 40s - loss: 43.5595 - MinusLogProbMetric: 43.5595 - val_loss: 45.3912 - val_MinusLogProbMetric: 45.3912 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 353/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:40:07.771 
Epoch 353/1000 
	 loss: 43.3595, MinusLogProbMetric: 43.3595, val_loss: 45.0164, val_MinusLogProbMetric: 45.0164

Epoch 353: val_loss did not improve from 44.86818
196/196 - 40s - loss: 43.3595 - MinusLogProbMetric: 43.3595 - val_loss: 45.0164 - val_MinusLogProbMetric: 45.0164 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 354/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:40:49.002 
Epoch 354/1000 
	 loss: 43.3619, MinusLogProbMetric: 43.3619, val_loss: 45.5054, val_MinusLogProbMetric: 45.5054

Epoch 354: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.3619 - MinusLogProbMetric: 43.3619 - val_loss: 45.5054 - val_MinusLogProbMetric: 45.5054 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 355/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:41:29.365 
Epoch 355/1000 
	 loss: 43.5207, MinusLogProbMetric: 43.5207, val_loss: 45.5013, val_MinusLogProbMetric: 45.5013

Epoch 355: val_loss did not improve from 44.86818
196/196 - 40s - loss: 43.5207 - MinusLogProbMetric: 43.5207 - val_loss: 45.5013 - val_MinusLogProbMetric: 45.5013 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 356/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:42:09.945 
Epoch 356/1000 
	 loss: 43.4058, MinusLogProbMetric: 43.4058, val_loss: 45.2552, val_MinusLogProbMetric: 45.2552

Epoch 356: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.4058 - MinusLogProbMetric: 43.4058 - val_loss: 45.2552 - val_MinusLogProbMetric: 45.2552 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 357/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:42:50.759 
Epoch 357/1000 
	 loss: 43.4421, MinusLogProbMetric: 43.4421, val_loss: 45.6109, val_MinusLogProbMetric: 45.6108

Epoch 357: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.4421 - MinusLogProbMetric: 43.4421 - val_loss: 45.6109 - val_MinusLogProbMetric: 45.6108 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 358/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:43:31.873 
Epoch 358/1000 
	 loss: 43.3739, MinusLogProbMetric: 43.3739, val_loss: 45.4898, val_MinusLogProbMetric: 45.4898

Epoch 358: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.3739 - MinusLogProbMetric: 43.3739 - val_loss: 45.4898 - val_MinusLogProbMetric: 45.4898 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 359/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:44:12.752 
Epoch 359/1000 
	 loss: 43.3236, MinusLogProbMetric: 43.3236, val_loss: 45.2844, val_MinusLogProbMetric: 45.2844

Epoch 359: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.3236 - MinusLogProbMetric: 43.3236 - val_loss: 45.2844 - val_MinusLogProbMetric: 45.2844 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 360/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:44:53.906 
Epoch 360/1000 
	 loss: 43.3147, MinusLogProbMetric: 43.3147, val_loss: 45.6624, val_MinusLogProbMetric: 45.6624

Epoch 360: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.3147 - MinusLogProbMetric: 43.3147 - val_loss: 45.6624 - val_MinusLogProbMetric: 45.6624 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 361/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:45:34.747 
Epoch 361/1000 
	 loss: 43.4438, MinusLogProbMetric: 43.4438, val_loss: 45.0477, val_MinusLogProbMetric: 45.0477

Epoch 361: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.4438 - MinusLogProbMetric: 43.4438 - val_loss: 45.0477 - val_MinusLogProbMetric: 45.0477 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 362/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:46:15.711 
Epoch 362/1000 
	 loss: 43.4539, MinusLogProbMetric: 43.4539, val_loss: 45.4519, val_MinusLogProbMetric: 45.4519

Epoch 362: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.4539 - MinusLogProbMetric: 43.4539 - val_loss: 45.4519 - val_MinusLogProbMetric: 45.4519 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 363/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:46:56.457 
Epoch 363/1000 
	 loss: 43.4691, MinusLogProbMetric: 43.4691, val_loss: 44.9971, val_MinusLogProbMetric: 44.9971

Epoch 363: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.4691 - MinusLogProbMetric: 43.4691 - val_loss: 44.9971 - val_MinusLogProbMetric: 44.9971 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 364/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:47:37.038 
Epoch 364/1000 
	 loss: 43.5816, MinusLogProbMetric: 43.5816, val_loss: 45.3031, val_MinusLogProbMetric: 45.3030

Epoch 364: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.5816 - MinusLogProbMetric: 43.5816 - val_loss: 45.3031 - val_MinusLogProbMetric: 45.3030 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 365/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:48:16.908 
Epoch 365/1000 
	 loss: 43.2936, MinusLogProbMetric: 43.2936, val_loss: 45.5014, val_MinusLogProbMetric: 45.5014

Epoch 365: val_loss did not improve from 44.86818
196/196 - 40s - loss: 43.2936 - MinusLogProbMetric: 43.2936 - val_loss: 45.5014 - val_MinusLogProbMetric: 45.5014 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 366/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:48:57.598 
Epoch 366/1000 
	 loss: 43.3502, MinusLogProbMetric: 43.3502, val_loss: 44.9773, val_MinusLogProbMetric: 44.9773

Epoch 366: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.3502 - MinusLogProbMetric: 43.3502 - val_loss: 44.9773 - val_MinusLogProbMetric: 44.9773 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 367/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:49:38.987 
Epoch 367/1000 
	 loss: 43.2885, MinusLogProbMetric: 43.2885, val_loss: 45.1209, val_MinusLogProbMetric: 45.1209

Epoch 367: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.2885 - MinusLogProbMetric: 43.2885 - val_loss: 45.1209 - val_MinusLogProbMetric: 45.1209 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 368/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:50:19.909 
Epoch 368/1000 
	 loss: 43.4188, MinusLogProbMetric: 43.4188, val_loss: 45.1499, val_MinusLogProbMetric: 45.1499

Epoch 368: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.4188 - MinusLogProbMetric: 43.4188 - val_loss: 45.1499 - val_MinusLogProbMetric: 45.1499 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 369/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:51:00.763 
Epoch 369/1000 
	 loss: 43.4341, MinusLogProbMetric: 43.4341, val_loss: 45.4694, val_MinusLogProbMetric: 45.4694

Epoch 369: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.4341 - MinusLogProbMetric: 43.4341 - val_loss: 45.4694 - val_MinusLogProbMetric: 45.4694 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 370/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:51:41.893 
Epoch 370/1000 
	 loss: 43.3803, MinusLogProbMetric: 43.3803, val_loss: 45.1666, val_MinusLogProbMetric: 45.1665

Epoch 370: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.3803 - MinusLogProbMetric: 43.3803 - val_loss: 45.1666 - val_MinusLogProbMetric: 45.1665 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 371/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:52:23.224 
Epoch 371/1000 
	 loss: 43.2642, MinusLogProbMetric: 43.2642, val_loss: 45.5967, val_MinusLogProbMetric: 45.5967

Epoch 371: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.2642 - MinusLogProbMetric: 43.2642 - val_loss: 45.5967 - val_MinusLogProbMetric: 45.5967 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 372/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:53:04.616 
Epoch 372/1000 
	 loss: 43.2794, MinusLogProbMetric: 43.2794, val_loss: 45.4348, val_MinusLogProbMetric: 45.4347

Epoch 372: val_loss did not improve from 44.86818
196/196 - 41s - loss: 43.2794 - MinusLogProbMetric: 43.2794 - val_loss: 45.4348 - val_MinusLogProbMetric: 45.4347 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 373/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:53:45.385 
Epoch 373/1000 
	 loss: 43.3528, MinusLogProbMetric: 43.3528, val_loss: 44.8656, val_MinusLogProbMetric: 44.8657

Epoch 373: val_loss improved from 44.86818 to 44.86564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 42s - loss: 43.3528 - MinusLogProbMetric: 43.3528 - val_loss: 44.8656 - val_MinusLogProbMetric: 44.8657 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 374/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:54:26.467 
Epoch 374/1000 
	 loss: 43.2989, MinusLogProbMetric: 43.2989, val_loss: 45.9201, val_MinusLogProbMetric: 45.9200

Epoch 374: val_loss did not improve from 44.86564
196/196 - 40s - loss: 43.2989 - MinusLogProbMetric: 43.2989 - val_loss: 45.9201 - val_MinusLogProbMetric: 45.9200 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 375/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:55:07.153 
Epoch 375/1000 
	 loss: 43.3497, MinusLogProbMetric: 43.3497, val_loss: 45.1169, val_MinusLogProbMetric: 45.1169

Epoch 375: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.3497 - MinusLogProbMetric: 43.3497 - val_loss: 45.1169 - val_MinusLogProbMetric: 45.1169 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 376/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:55:48.483 
Epoch 376/1000 
	 loss: 43.3739, MinusLogProbMetric: 43.3739, val_loss: 45.3808, val_MinusLogProbMetric: 45.3807

Epoch 376: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.3739 - MinusLogProbMetric: 43.3739 - val_loss: 45.3808 - val_MinusLogProbMetric: 45.3807 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 377/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:56:30.137 
Epoch 377/1000 
	 loss: 43.2776, MinusLogProbMetric: 43.2776, val_loss: 45.0689, val_MinusLogProbMetric: 45.0688

Epoch 377: val_loss did not improve from 44.86564
196/196 - 42s - loss: 43.2776 - MinusLogProbMetric: 43.2776 - val_loss: 45.0689 - val_MinusLogProbMetric: 45.0688 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 378/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:57:10.790 
Epoch 378/1000 
	 loss: 43.2871, MinusLogProbMetric: 43.2871, val_loss: 45.3358, val_MinusLogProbMetric: 45.3358

Epoch 378: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.2871 - MinusLogProbMetric: 43.2871 - val_loss: 45.3358 - val_MinusLogProbMetric: 45.3358 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 379/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:57:50.998 
Epoch 379/1000 
	 loss: 43.4849, MinusLogProbMetric: 43.4849, val_loss: 45.4600, val_MinusLogProbMetric: 45.4599

Epoch 379: val_loss did not improve from 44.86564
196/196 - 40s - loss: 43.4849 - MinusLogProbMetric: 43.4849 - val_loss: 45.4600 - val_MinusLogProbMetric: 45.4599 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 380/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:58:31.424 
Epoch 380/1000 
	 loss: 43.3949, MinusLogProbMetric: 43.3949, val_loss: 45.5100, val_MinusLogProbMetric: 45.5099

Epoch 380: val_loss did not improve from 44.86564
196/196 - 40s - loss: 43.3949 - MinusLogProbMetric: 43.3949 - val_loss: 45.5100 - val_MinusLogProbMetric: 45.5099 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 381/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:59:12.992 
Epoch 381/1000 
	 loss: 43.3910, MinusLogProbMetric: 43.3910, val_loss: 45.2063, val_MinusLogProbMetric: 45.2063

Epoch 381: val_loss did not improve from 44.86564
196/196 - 42s - loss: 43.3910 - MinusLogProbMetric: 43.3910 - val_loss: 45.2063 - val_MinusLogProbMetric: 45.2063 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 382/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 07:59:54.313 
Epoch 382/1000 
	 loss: 43.2817, MinusLogProbMetric: 43.2817, val_loss: 45.2770, val_MinusLogProbMetric: 45.2769

Epoch 382: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.2817 - MinusLogProbMetric: 43.2817 - val_loss: 45.2770 - val_MinusLogProbMetric: 45.2769 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 383/1000
2023-10-28 08:00:35.574 
Epoch 383/1000 
	 loss: 43.3677, MinusLogProbMetric: 43.3677, val_loss: 45.0746, val_MinusLogProbMetric: 45.0746

Epoch 383: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.3677 - MinusLogProbMetric: 43.3677 - val_loss: 45.0746 - val_MinusLogProbMetric: 45.0746 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 384/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:01:16.536 
Epoch 384/1000 
	 loss: 43.3113, MinusLogProbMetric: 43.3113, val_loss: 45.2071, val_MinusLogProbMetric: 45.2071

Epoch 384: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.3113 - MinusLogProbMetric: 43.3113 - val_loss: 45.2071 - val_MinusLogProbMetric: 45.2071 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 385/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:01:57.077 
Epoch 385/1000 
	 loss: 46.6617, MinusLogProbMetric: 46.6617, val_loss: 47.6401, val_MinusLogProbMetric: 47.6401

Epoch 385: val_loss did not improve from 44.86564
196/196 - 41s - loss: 46.6617 - MinusLogProbMetric: 46.6617 - val_loss: 47.6401 - val_MinusLogProbMetric: 47.6401 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 386/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:02:37.429 
Epoch 386/1000 
	 loss: 45.1858, MinusLogProbMetric: 45.1858, val_loss: 46.3114, val_MinusLogProbMetric: 46.3114

Epoch 386: val_loss did not improve from 44.86564
196/196 - 40s - loss: 45.1858 - MinusLogProbMetric: 45.1858 - val_loss: 46.3114 - val_MinusLogProbMetric: 46.3114 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 387/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:03:17.668 
Epoch 387/1000 
	 loss: 44.6699, MinusLogProbMetric: 44.6699, val_loss: 46.0133, val_MinusLogProbMetric: 46.0134

Epoch 387: val_loss did not improve from 44.86564
196/196 - 40s - loss: 44.6699 - MinusLogProbMetric: 44.6699 - val_loss: 46.0133 - val_MinusLogProbMetric: 46.0134 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 388/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:03:58.579 
Epoch 388/1000 
	 loss: 44.3400, MinusLogProbMetric: 44.3400, val_loss: 45.8049, val_MinusLogProbMetric: 45.8049

Epoch 388: val_loss did not improve from 44.86564
196/196 - 41s - loss: 44.3400 - MinusLogProbMetric: 44.3400 - val_loss: 45.8049 - val_MinusLogProbMetric: 45.8049 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 389/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:04:39.372 
Epoch 389/1000 
	 loss: 43.6112, MinusLogProbMetric: 43.6112, val_loss: 45.7705, val_MinusLogProbMetric: 45.7705

Epoch 389: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.6112 - MinusLogProbMetric: 43.6112 - val_loss: 45.7705 - val_MinusLogProbMetric: 45.7705 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 390/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:05:19.867 
Epoch 390/1000 
	 loss: 43.3717, MinusLogProbMetric: 43.3717, val_loss: 45.3162, val_MinusLogProbMetric: 45.3162

Epoch 390: val_loss did not improve from 44.86564
196/196 - 40s - loss: 43.3717 - MinusLogProbMetric: 43.3717 - val_loss: 45.3162 - val_MinusLogProbMetric: 45.3162 - lr: 4.1152e-06 - 40s/epoch - 207ms/step
Epoch 391/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:06:00.557 
Epoch 391/1000 
	 loss: 43.3836, MinusLogProbMetric: 43.3836, val_loss: 45.6817, val_MinusLogProbMetric: 45.6817

Epoch 391: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.3836 - MinusLogProbMetric: 43.3836 - val_loss: 45.6817 - val_MinusLogProbMetric: 45.6817 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 392/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:06:41.595 
Epoch 392/1000 
	 loss: 43.3216, MinusLogProbMetric: 43.3216, val_loss: 45.2215, val_MinusLogProbMetric: 45.2215

Epoch 392: val_loss did not improve from 44.86564
196/196 - 41s - loss: 43.3216 - MinusLogProbMetric: 43.3216 - val_loss: 45.2215 - val_MinusLogProbMetric: 45.2215 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 393/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:07:20.877 
Epoch 393/1000 
	 loss: 43.2167, MinusLogProbMetric: 43.2167, val_loss: 45.5430, val_MinusLogProbMetric: 45.5430

Epoch 393: val_loss did not improve from 44.86564
196/196 - 39s - loss: 43.2167 - MinusLogProbMetric: 43.2167 - val_loss: 45.5430 - val_MinusLogProbMetric: 45.5430 - lr: 4.1152e-06 - 39s/epoch - 200ms/step
Epoch 394/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:08:00.874 
Epoch 394/1000 
	 loss: 43.2641, MinusLogProbMetric: 43.2641, val_loss: 44.9542, val_MinusLogProbMetric: 44.9542

Epoch 394: val_loss did not improve from 44.86564
196/196 - 40s - loss: 43.2641 - MinusLogProbMetric: 43.2641 - val_loss: 44.9542 - val_MinusLogProbMetric: 44.9542 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 395/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:08:40.887 
Epoch 395/1000 
	 loss: 43.3472, MinusLogProbMetric: 43.3472, val_loss: 44.7389, val_MinusLogProbMetric: 44.7389

Epoch 395: val_loss improved from 44.86564 to 44.73890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 43.3472 - MinusLogProbMetric: 43.3472 - val_loss: 44.7389 - val_MinusLogProbMetric: 44.7389 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 396/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:09:21.773 
Epoch 396/1000 
	 loss: 43.2204, MinusLogProbMetric: 43.2204, val_loss: 44.8242, val_MinusLogProbMetric: 44.8242

Epoch 396: val_loss did not improve from 44.73890
196/196 - 40s - loss: 43.2204 - MinusLogProbMetric: 43.2204 - val_loss: 44.8242 - val_MinusLogProbMetric: 44.8242 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 397/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:10:02.091 
Epoch 397/1000 
	 loss: 43.2260, MinusLogProbMetric: 43.2260, val_loss: 44.8401, val_MinusLogProbMetric: 44.8401

Epoch 397: val_loss did not improve from 44.73890
196/196 - 40s - loss: 43.2260 - MinusLogProbMetric: 43.2260 - val_loss: 44.8401 - val_MinusLogProbMetric: 44.8401 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 398/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:10:42.648 
Epoch 398/1000 
	 loss: 43.2768, MinusLogProbMetric: 43.2768, val_loss: 45.8775, val_MinusLogProbMetric: 45.8774

Epoch 398: val_loss did not improve from 44.73890
196/196 - 41s - loss: 43.2768 - MinusLogProbMetric: 43.2768 - val_loss: 45.8775 - val_MinusLogProbMetric: 45.8774 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 399/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:11:23.262 
Epoch 399/1000 
	 loss: 43.2720, MinusLogProbMetric: 43.2720, val_loss: 44.5833, val_MinusLogProbMetric: 44.5833

Epoch 399: val_loss improved from 44.73890 to 44.58332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_428/weights/best_weights.h5
196/196 - 41s - loss: 43.2720 - MinusLogProbMetric: 43.2720 - val_loss: 44.5833 - val_MinusLogProbMetric: 44.5833 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 400/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:12:04.311 
Epoch 400/1000 
	 loss: 43.1908, MinusLogProbMetric: 43.1908, val_loss: 45.6775, val_MinusLogProbMetric: 45.6775

Epoch 400: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.1908 - MinusLogProbMetric: 43.1908 - val_loss: 45.6775 - val_MinusLogProbMetric: 45.6775 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 401/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:12:44.364 
Epoch 401/1000 
	 loss: 43.2270, MinusLogProbMetric: 43.2270, val_loss: 45.3721, val_MinusLogProbMetric: 45.3721

Epoch 401: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2270 - MinusLogProbMetric: 43.2270 - val_loss: 45.3721 - val_MinusLogProbMetric: 45.3721 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 402/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:13:24.713 
Epoch 402/1000 
	 loss: 43.2871, MinusLogProbMetric: 43.2871, val_loss: 45.1962, val_MinusLogProbMetric: 45.1962

Epoch 402: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2871 - MinusLogProbMetric: 43.2871 - val_loss: 45.1962 - val_MinusLogProbMetric: 45.1962 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 403/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:14:05.207 
Epoch 403/1000 
	 loss: 43.2875, MinusLogProbMetric: 43.2875, val_loss: 44.6515, val_MinusLogProbMetric: 44.6515

Epoch 403: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2875 - MinusLogProbMetric: 43.2875 - val_loss: 44.6515 - val_MinusLogProbMetric: 44.6515 - lr: 4.1152e-06 - 40s/epoch - 207ms/step
Epoch 404/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:14:45.355 
Epoch 404/1000 
	 loss: 43.2084, MinusLogProbMetric: 43.2084, val_loss: 45.2762, val_MinusLogProbMetric: 45.2762

Epoch 404: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2084 - MinusLogProbMetric: 43.2084 - val_loss: 45.2762 - val_MinusLogProbMetric: 45.2762 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 405/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:15:25.387 
Epoch 405/1000 
	 loss: 43.2170, MinusLogProbMetric: 43.2170, val_loss: 45.1355, val_MinusLogProbMetric: 45.1355

Epoch 405: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2170 - MinusLogProbMetric: 43.2170 - val_loss: 45.1355 - val_MinusLogProbMetric: 45.1355 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 406/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:16:06.117 
Epoch 406/1000 
	 loss: 43.2203, MinusLogProbMetric: 43.2203, val_loss: 44.8529, val_MinusLogProbMetric: 44.8529

Epoch 406: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.2203 - MinusLogProbMetric: 43.2203 - val_loss: 44.8529 - val_MinusLogProbMetric: 44.8529 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 407/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:16:46.646 
Epoch 407/1000 
	 loss: 43.2781, MinusLogProbMetric: 43.2781, val_loss: 45.6696, val_MinusLogProbMetric: 45.6696

Epoch 407: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.2781 - MinusLogProbMetric: 43.2781 - val_loss: 45.6696 - val_MinusLogProbMetric: 45.6696 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 408/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:17:26.751 
Epoch 408/1000 
	 loss: 43.2142, MinusLogProbMetric: 43.2142, val_loss: 45.0156, val_MinusLogProbMetric: 45.0156

Epoch 408: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2142 - MinusLogProbMetric: 43.2142 - val_loss: 45.0156 - val_MinusLogProbMetric: 45.0156 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 409/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:18:06.757 
Epoch 409/1000 
	 loss: 43.3421, MinusLogProbMetric: 43.3421, val_loss: 45.1413, val_MinusLogProbMetric: 45.1413

Epoch 409: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.3421 - MinusLogProbMetric: 43.3421 - val_loss: 45.1413 - val_MinusLogProbMetric: 45.1413 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 410/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:18:46.878 
Epoch 410/1000 
	 loss: 43.1728, MinusLogProbMetric: 43.1728, val_loss: 45.3673, val_MinusLogProbMetric: 45.3673

Epoch 410: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.1728 - MinusLogProbMetric: 43.1728 - val_loss: 45.3673 - val_MinusLogProbMetric: 45.3673 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 411/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:19:27.085 
Epoch 411/1000 
	 loss: 43.2426, MinusLogProbMetric: 43.2426, val_loss: 44.9017, val_MinusLogProbMetric: 44.9017

Epoch 411: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2426 - MinusLogProbMetric: 43.2426 - val_loss: 44.9017 - val_MinusLogProbMetric: 44.9017 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 412/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:20:08.275 
Epoch 412/1000 
	 loss: 43.3594, MinusLogProbMetric: 43.3594, val_loss: 45.2891, val_MinusLogProbMetric: 45.2891

Epoch 412: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.3594 - MinusLogProbMetric: 43.3594 - val_loss: 45.2891 - val_MinusLogProbMetric: 45.2891 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 413/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:20:49.029 
Epoch 413/1000 
	 loss: 43.2299, MinusLogProbMetric: 43.2299, val_loss: 45.0122, val_MinusLogProbMetric: 45.0122

Epoch 413: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.2299 - MinusLogProbMetric: 43.2299 - val_loss: 45.0122 - val_MinusLogProbMetric: 45.0122 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 414/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:21:28.576 
Epoch 414/1000 
	 loss: 43.3338, MinusLogProbMetric: 43.3338, val_loss: 45.5949, val_MinusLogProbMetric: 45.5949

Epoch 414: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.3338 - MinusLogProbMetric: 43.3338 - val_loss: 45.5949 - val_MinusLogProbMetric: 45.5949 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 415/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:22:08.454 
Epoch 415/1000 
	 loss: 43.2552, MinusLogProbMetric: 43.2552, val_loss: 45.0306, val_MinusLogProbMetric: 45.0305

Epoch 415: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2552 - MinusLogProbMetric: 43.2552 - val_loss: 45.0306 - val_MinusLogProbMetric: 45.0305 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 416/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:22:49.954 
Epoch 416/1000 
	 loss: 43.2500, MinusLogProbMetric: 43.2500, val_loss: 45.8765, val_MinusLogProbMetric: 45.8764

Epoch 416: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.2500 - MinusLogProbMetric: 43.2500 - val_loss: 45.8765 - val_MinusLogProbMetric: 45.8764 - lr: 4.1152e-06 - 41s/epoch - 212ms/step
Epoch 417/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:23:31.299 
Epoch 417/1000 
	 loss: 43.1642, MinusLogProbMetric: 43.1642, val_loss: 44.9863, val_MinusLogProbMetric: 44.9863

Epoch 417: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1642 - MinusLogProbMetric: 43.1642 - val_loss: 44.9863 - val_MinusLogProbMetric: 44.9863 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 418/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:24:12.861 
Epoch 418/1000 
	 loss: 43.2185, MinusLogProbMetric: 43.2185, val_loss: 45.1505, val_MinusLogProbMetric: 45.1505

Epoch 418: val_loss did not improve from 44.58332
196/196 - 42s - loss: 43.2185 - MinusLogProbMetric: 43.2185 - val_loss: 45.1505 - val_MinusLogProbMetric: 45.1505 - lr: 4.1152e-06 - 42s/epoch - 212ms/step
Epoch 419/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:24:53.993 
Epoch 419/1000 
	 loss: 43.2124, MinusLogProbMetric: 43.2124, val_loss: 45.0659, val_MinusLogProbMetric: 45.0659

Epoch 419: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.2124 - MinusLogProbMetric: 43.2124 - val_loss: 45.0659 - val_MinusLogProbMetric: 45.0659 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 420/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:25:34.728 
Epoch 420/1000 
	 loss: 43.2006, MinusLogProbMetric: 43.2006, val_loss: 44.8619, val_MinusLogProbMetric: 44.8619

Epoch 420: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.2006 - MinusLogProbMetric: 43.2006 - val_loss: 44.8619 - val_MinusLogProbMetric: 44.8619 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 421/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:26:13.993 
Epoch 421/1000 
	 loss: 43.1751, MinusLogProbMetric: 43.1751, val_loss: 45.5470, val_MinusLogProbMetric: 45.5470

Epoch 421: val_loss did not improve from 44.58332
196/196 - 39s - loss: 43.1751 - MinusLogProbMetric: 43.1751 - val_loss: 45.5470 - val_MinusLogProbMetric: 45.5470 - lr: 4.1152e-06 - 39s/epoch - 200ms/step
Epoch 422/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:26:53.592 
Epoch 422/1000 
	 loss: 43.2378, MinusLogProbMetric: 43.2378, val_loss: 45.2680, val_MinusLogProbMetric: 45.2680

Epoch 422: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2378 - MinusLogProbMetric: 43.2378 - val_loss: 45.2680 - val_MinusLogProbMetric: 45.2680 - lr: 4.1152e-06 - 40s/epoch - 202ms/step
Epoch 423/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:27:33.811 
Epoch 423/1000 
	 loss: 43.1395, MinusLogProbMetric: 43.1395, val_loss: 45.5101, val_MinusLogProbMetric: 45.5101

Epoch 423: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.1395 - MinusLogProbMetric: 43.1395 - val_loss: 45.5101 - val_MinusLogProbMetric: 45.5101 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 424/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:28:14.560 
Epoch 424/1000 
	 loss: 43.3719, MinusLogProbMetric: 43.3719, val_loss: 45.0315, val_MinusLogProbMetric: 45.0315

Epoch 424: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.3719 - MinusLogProbMetric: 43.3719 - val_loss: 45.0315 - val_MinusLogProbMetric: 45.0315 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 425/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:28:54.394 
Epoch 425/1000 
	 loss: 43.1832, MinusLogProbMetric: 43.1832, val_loss: 45.2715, val_MinusLogProbMetric: 45.2715

Epoch 425: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.1832 - MinusLogProbMetric: 43.1832 - val_loss: 45.2715 - val_MinusLogProbMetric: 45.2715 - lr: 4.1152e-06 - 40s/epoch - 203ms/step
Epoch 426/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:29:35.157 
Epoch 426/1000 
	 loss: 43.1666, MinusLogProbMetric: 43.1666, val_loss: 45.4626, val_MinusLogProbMetric: 45.4626

Epoch 426: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1666 - MinusLogProbMetric: 43.1666 - val_loss: 45.4626 - val_MinusLogProbMetric: 45.4626 - lr: 4.1152e-06 - 41s/epoch - 208ms/step
Epoch 427/1000
2023-10-28 08:30:16.341 
Epoch 427/1000 
	 loss: 43.1876, MinusLogProbMetric: 43.1876, val_loss: 44.9125, val_MinusLogProbMetric: 44.9125

Epoch 427: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1876 - MinusLogProbMetric: 43.1876 - val_loss: 44.9125 - val_MinusLogProbMetric: 44.9125 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 428/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:30:57.332 
Epoch 428/1000 
	 loss: 43.3361, MinusLogProbMetric: 43.3361, val_loss: 44.7075, val_MinusLogProbMetric: 44.7075

Epoch 428: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.3361 - MinusLogProbMetric: 43.3361 - val_loss: 44.7075 - val_MinusLogProbMetric: 44.7075 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 429/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:31:39.168 
Epoch 429/1000 
	 loss: 43.1345, MinusLogProbMetric: 43.1345, val_loss: 45.0225, val_MinusLogProbMetric: 45.0225

Epoch 429: val_loss did not improve from 44.58332
196/196 - 42s - loss: 43.1345 - MinusLogProbMetric: 43.1345 - val_loss: 45.0225 - val_MinusLogProbMetric: 45.0225 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 430/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:32:20.247 
Epoch 430/1000 
	 loss: 43.3488, MinusLogProbMetric: 43.3488, val_loss: 45.6555, val_MinusLogProbMetric: 45.6555

Epoch 430: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.3488 - MinusLogProbMetric: 43.3488 - val_loss: 45.6555 - val_MinusLogProbMetric: 45.6555 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 431/1000
2023-10-28 08:33:01.136 
Epoch 431/1000 
	 loss: 43.1133, MinusLogProbMetric: 43.1133, val_loss: 44.8648, val_MinusLogProbMetric: 44.8648

Epoch 431: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1133 - MinusLogProbMetric: 43.1133 - val_loss: 44.8648 - val_MinusLogProbMetric: 44.8648 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 432/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:33:42.080 
Epoch 432/1000 
	 loss: 43.1638, MinusLogProbMetric: 43.1638, val_loss: 45.0287, val_MinusLogProbMetric: 45.0287

Epoch 432: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1638 - MinusLogProbMetric: 43.1638 - val_loss: 45.0287 - val_MinusLogProbMetric: 45.0287 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 433/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:34:23.570 
Epoch 433/1000 
	 loss: 43.1886, MinusLogProbMetric: 43.1886, val_loss: 45.1287, val_MinusLogProbMetric: 45.1287

Epoch 433: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1886 - MinusLogProbMetric: 43.1886 - val_loss: 45.1287 - val_MinusLogProbMetric: 45.1287 - lr: 4.1152e-06 - 41s/epoch - 212ms/step
Epoch 434/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:35:04.929 
Epoch 434/1000 
	 loss: 43.1877, MinusLogProbMetric: 43.1877, val_loss: 45.0939, val_MinusLogProbMetric: 45.0939

Epoch 434: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1877 - MinusLogProbMetric: 43.1877 - val_loss: 45.0939 - val_MinusLogProbMetric: 45.0939 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 435/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:35:46.052 
Epoch 435/1000 
	 loss: 43.1178, MinusLogProbMetric: 43.1178, val_loss: 44.9100, val_MinusLogProbMetric: 44.9100

Epoch 435: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1178 - MinusLogProbMetric: 43.1178 - val_loss: 44.9100 - val_MinusLogProbMetric: 44.9100 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 436/1000
2023-10-28 08:36:27.441 
Epoch 436/1000 
	 loss: 43.1759, MinusLogProbMetric: 43.1759, val_loss: 44.8719, val_MinusLogProbMetric: 44.8719

Epoch 436: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1759 - MinusLogProbMetric: 43.1759 - val_loss: 44.8719 - val_MinusLogProbMetric: 44.8719 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 437/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:37:09.365 
Epoch 437/1000 
	 loss: 43.1519, MinusLogProbMetric: 43.1519, val_loss: 45.2110, val_MinusLogProbMetric: 45.2110

Epoch 437: val_loss did not improve from 44.58332
196/196 - 42s - loss: 43.1519 - MinusLogProbMetric: 43.1519 - val_loss: 45.2110 - val_MinusLogProbMetric: 45.2110 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 438/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:37:49.981 
Epoch 438/1000 
	 loss: 43.1512, MinusLogProbMetric: 43.1512, val_loss: 44.6384, val_MinusLogProbMetric: 44.6384

Epoch 438: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1512 - MinusLogProbMetric: 43.1512 - val_loss: 44.6384 - val_MinusLogProbMetric: 44.6384 - lr: 4.1152e-06 - 41s/epoch - 207ms/step
Epoch 439/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:38:31.989 
Epoch 439/1000 
	 loss: 43.3451, MinusLogProbMetric: 43.3451, val_loss: 44.9107, val_MinusLogProbMetric: 44.9107

Epoch 439: val_loss did not improve from 44.58332
196/196 - 42s - loss: 43.3451 - MinusLogProbMetric: 43.3451 - val_loss: 44.9107 - val_MinusLogProbMetric: 44.9107 - lr: 4.1152e-06 - 42s/epoch - 214ms/step
Epoch 440/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:39:12.969 
Epoch 440/1000 
	 loss: 43.2413, MinusLogProbMetric: 43.2413, val_loss: 44.9867, val_MinusLogProbMetric: 44.9866

Epoch 440: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.2413 - MinusLogProbMetric: 43.2413 - val_loss: 44.9867 - val_MinusLogProbMetric: 44.9866 - lr: 4.1152e-06 - 41s/epoch - 209ms/step
Epoch 441/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:39:54.157 
Epoch 441/1000 
	 loss: 43.0952, MinusLogProbMetric: 43.0952, val_loss: 45.3200, val_MinusLogProbMetric: 45.3199

Epoch 441: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.0952 - MinusLogProbMetric: 43.0952 - val_loss: 45.3200 - val_MinusLogProbMetric: 45.3199 - lr: 4.1152e-06 - 41s/epoch - 210ms/step
Epoch 442/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:40:35.865 
Epoch 442/1000 
	 loss: 43.1547, MinusLogProbMetric: 43.1547, val_loss: 45.2197, val_MinusLogProbMetric: 45.2197

Epoch 442: val_loss did not improve from 44.58332
196/196 - 42s - loss: 43.1547 - MinusLogProbMetric: 43.1547 - val_loss: 45.2197 - val_MinusLogProbMetric: 45.2197 - lr: 4.1152e-06 - 42s/epoch - 213ms/step
Epoch 443/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:41:16.308 
Epoch 443/1000 
	 loss: 43.2883, MinusLogProbMetric: 43.2883, val_loss: 45.2169, val_MinusLogProbMetric: 45.2169

Epoch 443: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.2883 - MinusLogProbMetric: 43.2883 - val_loss: 45.2169 - val_MinusLogProbMetric: 45.2169 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 444/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:41:57.652 
Epoch 444/1000 
	 loss: 43.1188, MinusLogProbMetric: 43.1188, val_loss: 44.7275, val_MinusLogProbMetric: 44.7274

Epoch 444: val_loss did not improve from 44.58332
196/196 - 41s - loss: 43.1188 - MinusLogProbMetric: 43.1188 - val_loss: 44.7275 - val_MinusLogProbMetric: 44.7274 - lr: 4.1152e-06 - 41s/epoch - 211ms/step
Epoch 445/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:42:38.121 
Epoch 445/1000 
	 loss: 43.1415, MinusLogProbMetric: 43.1415, val_loss: 45.2339, val_MinusLogProbMetric: 45.2339

Epoch 445: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.1415 - MinusLogProbMetric: 43.1415 - val_loss: 45.2339 - val_MinusLogProbMetric: 45.2339 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 446/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:43:18.286 
Epoch 446/1000 
	 loss: 43.1443, MinusLogProbMetric: 43.1443, val_loss: 45.3789, val_MinusLogProbMetric: 45.3789

Epoch 446: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.1443 - MinusLogProbMetric: 43.1443 - val_loss: 45.3789 - val_MinusLogProbMetric: 45.3789 - lr: 4.1152e-06 - 40s/epoch - 205ms/step
Epoch 447/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:43:58.363 
Epoch 447/1000 
	 loss: 43.1234, MinusLogProbMetric: 43.1234, val_loss: 45.1400, val_MinusLogProbMetric: 45.1400

Epoch 447: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.1234 - MinusLogProbMetric: 43.1234 - val_loss: 45.1400 - val_MinusLogProbMetric: 45.1400 - lr: 4.1152e-06 - 40s/epoch - 204ms/step
Epoch 448/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:44:38.701 
Epoch 448/1000 
	 loss: 43.1138, MinusLogProbMetric: 43.1138, val_loss: 45.3478, val_MinusLogProbMetric: 45.3478

Epoch 448: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.1138 - MinusLogProbMetric: 43.1138 - val_loss: 45.3478 - val_MinusLogProbMetric: 45.3478 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 449/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:45:19.001 
Epoch 449/1000 
	 loss: 43.0786, MinusLogProbMetric: 43.0786, val_loss: 44.5874, val_MinusLogProbMetric: 44.5874

Epoch 449: val_loss did not improve from 44.58332
196/196 - 40s - loss: 43.0786 - MinusLogProbMetric: 43.0786 - val_loss: 44.5874 - val_MinusLogProbMetric: 44.5874 - lr: 4.1152e-06 - 40s/epoch - 206ms/step
Epoch 450/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:45:59.195 
Epoch 450/1000 
	 loss: 42.9287, MinusLogProbMetric: 42.9287, val_loss: 44.8519, val_MinusLogProbMetric: 44.8519

Epoch 450: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9287 - MinusLogProbMetric: 42.9287 - val_loss: 44.8519 - val_MinusLogProbMetric: 44.8519 - lr: 2.0576e-06 - 40s/epoch - 205ms/step
Epoch 451/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:46:39.106 
Epoch 451/1000 
	 loss: 42.9308, MinusLogProbMetric: 42.9308, val_loss: 45.0733, val_MinusLogProbMetric: 45.0732

Epoch 451: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9308 - MinusLogProbMetric: 42.9308 - val_loss: 45.0733 - val_MinusLogProbMetric: 45.0732 - lr: 2.0576e-06 - 40s/epoch - 204ms/step
Epoch 452/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:47:19.583 
Epoch 452/1000 
	 loss: 42.9173, MinusLogProbMetric: 42.9173, val_loss: 45.1324, val_MinusLogProbMetric: 45.1323

Epoch 452: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9173 - MinusLogProbMetric: 42.9173 - val_loss: 45.1324 - val_MinusLogProbMetric: 45.1323 - lr: 2.0576e-06 - 40s/epoch - 207ms/step
Epoch 453/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:48:00.008 
Epoch 453/1000 
	 loss: 42.9163, MinusLogProbMetric: 42.9163, val_loss: 45.2731, val_MinusLogProbMetric: 45.2731

Epoch 453: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9163 - MinusLogProbMetric: 42.9163 - val_loss: 45.2731 - val_MinusLogProbMetric: 45.2731 - lr: 2.0576e-06 - 40s/epoch - 206ms/step
Epoch 454/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:48:40.403 
Epoch 454/1000 
	 loss: 42.9034, MinusLogProbMetric: 42.9034, val_loss: 44.8801, val_MinusLogProbMetric: 44.8801

Epoch 454: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9034 - MinusLogProbMetric: 42.9034 - val_loss: 44.8801 - val_MinusLogProbMetric: 44.8801 - lr: 2.0576e-06 - 40s/epoch - 206ms/step
Epoch 455/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:49:21.062 
Epoch 455/1000 
	 loss: 42.8921, MinusLogProbMetric: 42.8921, val_loss: 45.1166, val_MinusLogProbMetric: 45.1166

Epoch 455: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8921 - MinusLogProbMetric: 42.8921 - val_loss: 45.1166 - val_MinusLogProbMetric: 45.1166 - lr: 2.0576e-06 - 41s/epoch - 207ms/step
Epoch 456/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:50:02.664 
Epoch 456/1000 
	 loss: 42.8905, MinusLogProbMetric: 42.8905, val_loss: 45.1142, val_MinusLogProbMetric: 45.1142

Epoch 456: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8905 - MinusLogProbMetric: 42.8905 - val_loss: 45.1142 - val_MinusLogProbMetric: 45.1142 - lr: 2.0576e-06 - 42s/epoch - 212ms/step
Epoch 457/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:50:43.406 
Epoch 457/1000 
	 loss: 42.8984, MinusLogProbMetric: 42.8984, val_loss: 44.7439, val_MinusLogProbMetric: 44.7439

Epoch 457: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8984 - MinusLogProbMetric: 42.8984 - val_loss: 44.7439 - val_MinusLogProbMetric: 44.7439 - lr: 2.0576e-06 - 41s/epoch - 208ms/step
Epoch 458/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:51:23.631 
Epoch 458/1000 
	 loss: 42.9045, MinusLogProbMetric: 42.9045, val_loss: 44.9792, val_MinusLogProbMetric: 44.9792

Epoch 458: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9045 - MinusLogProbMetric: 42.9045 - val_loss: 44.9792 - val_MinusLogProbMetric: 44.9792 - lr: 2.0576e-06 - 40s/epoch - 205ms/step
Epoch 459/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:52:02.952 
Epoch 459/1000 
	 loss: 42.8999, MinusLogProbMetric: 42.8999, val_loss: 45.0558, val_MinusLogProbMetric: 45.0557

Epoch 459: val_loss did not improve from 44.58332
196/196 - 39s - loss: 42.8999 - MinusLogProbMetric: 42.8999 - val_loss: 45.0558 - val_MinusLogProbMetric: 45.0557 - lr: 2.0576e-06 - 39s/epoch - 201ms/step
Epoch 460/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:52:41.345 
Epoch 460/1000 
	 loss: 42.8896, MinusLogProbMetric: 42.8896, val_loss: 45.3276, val_MinusLogProbMetric: 45.3275

Epoch 460: val_loss did not improve from 44.58332
196/196 - 38s - loss: 42.8896 - MinusLogProbMetric: 42.8896 - val_loss: 45.3276 - val_MinusLogProbMetric: 45.3275 - lr: 2.0576e-06 - 38s/epoch - 196ms/step
Epoch 461/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:53:19.068 
Epoch 461/1000 
	 loss: 42.8958, MinusLogProbMetric: 42.8958, val_loss: 45.0242, val_MinusLogProbMetric: 45.0242

Epoch 461: val_loss did not improve from 44.58332
196/196 - 38s - loss: 42.8958 - MinusLogProbMetric: 42.8958 - val_loss: 45.0242 - val_MinusLogProbMetric: 45.0242 - lr: 2.0576e-06 - 38s/epoch - 192ms/step
Epoch 462/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:53:59.012 
Epoch 462/1000 
	 loss: 42.9111, MinusLogProbMetric: 42.9111, val_loss: 44.8691, val_MinusLogProbMetric: 44.8691

Epoch 462: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9111 - MinusLogProbMetric: 42.9111 - val_loss: 44.8691 - val_MinusLogProbMetric: 44.8691 - lr: 2.0576e-06 - 40s/epoch - 204ms/step
Epoch 463/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:54:39.204 
Epoch 463/1000 
	 loss: 42.9042, MinusLogProbMetric: 42.9042, val_loss: 45.1712, val_MinusLogProbMetric: 45.1712

Epoch 463: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9042 - MinusLogProbMetric: 42.9042 - val_loss: 45.1712 - val_MinusLogProbMetric: 45.1712 - lr: 2.0576e-06 - 40s/epoch - 205ms/step
Epoch 464/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:55:19.435 
Epoch 464/1000 
	 loss: 42.9027, MinusLogProbMetric: 42.9027, val_loss: 45.2101, val_MinusLogProbMetric: 45.2100

Epoch 464: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9027 - MinusLogProbMetric: 42.9027 - val_loss: 45.2101 - val_MinusLogProbMetric: 45.2100 - lr: 2.0576e-06 - 40s/epoch - 205ms/step
Epoch 465/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:56:00.017 
Epoch 465/1000 
	 loss: 42.9010, MinusLogProbMetric: 42.9010, val_loss: 45.3469, val_MinusLogProbMetric: 45.3469

Epoch 465: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.9010 - MinusLogProbMetric: 42.9010 - val_loss: 45.3469 - val_MinusLogProbMetric: 45.3469 - lr: 2.0576e-06 - 41s/epoch - 207ms/step
Epoch 466/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:56:40.970 
Epoch 466/1000 
	 loss: 42.8874, MinusLogProbMetric: 42.8874, val_loss: 45.0153, val_MinusLogProbMetric: 45.0152

Epoch 466: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8874 - MinusLogProbMetric: 42.8874 - val_loss: 45.0153 - val_MinusLogProbMetric: 45.0152 - lr: 2.0576e-06 - 41s/epoch - 209ms/step
Epoch 467/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:57:21.300 
Epoch 467/1000 
	 loss: 42.9033, MinusLogProbMetric: 42.9033, val_loss: 44.8277, val_MinusLogProbMetric: 44.8277

Epoch 467: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9033 - MinusLogProbMetric: 42.9033 - val_loss: 44.8277 - val_MinusLogProbMetric: 44.8277 - lr: 2.0576e-06 - 40s/epoch - 206ms/step
Epoch 468/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:58:01.303 
Epoch 468/1000 
	 loss: 42.8863, MinusLogProbMetric: 42.8863, val_loss: 45.0605, val_MinusLogProbMetric: 45.0605

Epoch 468: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.8863 - MinusLogProbMetric: 42.8863 - val_loss: 45.0605 - val_MinusLogProbMetric: 45.0605 - lr: 2.0576e-06 - 40s/epoch - 204ms/step
Epoch 469/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:58:41.241 
Epoch 469/1000 
	 loss: 42.8940, MinusLogProbMetric: 42.8940, val_loss: 44.9685, val_MinusLogProbMetric: 44.9685

Epoch 469: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.8940 - MinusLogProbMetric: 42.8940 - val_loss: 44.9685 - val_MinusLogProbMetric: 44.9685 - lr: 2.0576e-06 - 40s/epoch - 204ms/step
Epoch 470/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 08:59:21.362 
Epoch 470/1000 
	 loss: 42.8979, MinusLogProbMetric: 42.8979, val_loss: 45.0852, val_MinusLogProbMetric: 45.0852

Epoch 470: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.8979 - MinusLogProbMetric: 42.8979 - val_loss: 45.0852 - val_MinusLogProbMetric: 45.0852 - lr: 2.0576e-06 - 40s/epoch - 205ms/step
Epoch 471/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:00:02.039 
Epoch 471/1000 
	 loss: 42.8788, MinusLogProbMetric: 42.8788, val_loss: 45.1694, val_MinusLogProbMetric: 45.1694

Epoch 471: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8788 - MinusLogProbMetric: 42.8788 - val_loss: 45.1694 - val_MinusLogProbMetric: 45.1694 - lr: 2.0576e-06 - 41s/epoch - 208ms/step
Epoch 472/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:00:42.896 
Epoch 472/1000 
	 loss: 42.8948, MinusLogProbMetric: 42.8948, val_loss: 45.0231, val_MinusLogProbMetric: 45.0231

Epoch 472: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8948 - MinusLogProbMetric: 42.8948 - val_loss: 45.0231 - val_MinusLogProbMetric: 45.0231 - lr: 2.0576e-06 - 41s/epoch - 208ms/step
Epoch 473/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:01:22.960 
Epoch 473/1000 
	 loss: 42.8852, MinusLogProbMetric: 42.8852, val_loss: 44.8718, val_MinusLogProbMetric: 44.8718

Epoch 473: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.8852 - MinusLogProbMetric: 42.8852 - val_loss: 44.8718 - val_MinusLogProbMetric: 44.8718 - lr: 2.0576e-06 - 40s/epoch - 204ms/step
Epoch 474/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:02:03.414 
Epoch 474/1000 
	 loss: 42.8906, MinusLogProbMetric: 42.8906, val_loss: 45.2401, val_MinusLogProbMetric: 45.2400

Epoch 474: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.8906 - MinusLogProbMetric: 42.8906 - val_loss: 45.2401 - val_MinusLogProbMetric: 45.2400 - lr: 2.0576e-06 - 40s/epoch - 206ms/step
Epoch 475/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:02:44.159 
Epoch 475/1000 
	 loss: 42.8966, MinusLogProbMetric: 42.8966, val_loss: 45.2171, val_MinusLogProbMetric: 45.2170

Epoch 475: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8966 - MinusLogProbMetric: 42.8966 - val_loss: 45.2171 - val_MinusLogProbMetric: 45.2170 - lr: 2.0576e-06 - 41s/epoch - 208ms/step
Epoch 476/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:03:24.081 
Epoch 476/1000 
	 loss: 42.9232, MinusLogProbMetric: 42.9232, val_loss: 44.9809, val_MinusLogProbMetric: 44.9809

Epoch 476: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.9232 - MinusLogProbMetric: 42.9232 - val_loss: 44.9809 - val_MinusLogProbMetric: 44.9809 - lr: 2.0576e-06 - 40s/epoch - 204ms/step
Epoch 477/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:04:05.020 
Epoch 477/1000 
	 loss: 42.9114, MinusLogProbMetric: 42.9114, val_loss: 44.9657, val_MinusLogProbMetric: 44.9656

Epoch 477: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.9114 - MinusLogProbMetric: 42.9114 - val_loss: 44.9657 - val_MinusLogProbMetric: 44.9656 - lr: 2.0576e-06 - 41s/epoch - 209ms/step
Epoch 478/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:04:45.855 
Epoch 478/1000 
	 loss: 42.8870, MinusLogProbMetric: 42.8870, val_loss: 44.7688, val_MinusLogProbMetric: 44.7688

Epoch 478: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8870 - MinusLogProbMetric: 42.8870 - val_loss: 44.7688 - val_MinusLogProbMetric: 44.7688 - lr: 2.0576e-06 - 41s/epoch - 208ms/step
Epoch 479/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:05:26.503 
Epoch 479/1000 
	 loss: 42.8858, MinusLogProbMetric: 42.8858, val_loss: 45.0616, val_MinusLogProbMetric: 45.0615

Epoch 479: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8858 - MinusLogProbMetric: 42.8858 - val_loss: 45.0616 - val_MinusLogProbMetric: 45.0615 - lr: 2.0576e-06 - 41s/epoch - 207ms/step
Epoch 480/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:06:07.403 
Epoch 480/1000 
	 loss: 42.8680, MinusLogProbMetric: 42.8680, val_loss: 45.1842, val_MinusLogProbMetric: 45.1842

Epoch 480: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8680 - MinusLogProbMetric: 42.8680 - val_loss: 45.1842 - val_MinusLogProbMetric: 45.1842 - lr: 2.0576e-06 - 41s/epoch - 209ms/step
Epoch 481/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:06:48.614 
Epoch 481/1000 
	 loss: 42.8993, MinusLogProbMetric: 42.8993, val_loss: 45.1568, val_MinusLogProbMetric: 45.1568

Epoch 481: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8993 - MinusLogProbMetric: 42.8993 - val_loss: 45.1568 - val_MinusLogProbMetric: 45.1568 - lr: 2.0576e-06 - 41s/epoch - 210ms/step
Epoch 482/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:07:30.051 
Epoch 482/1000 
	 loss: 42.8739, MinusLogProbMetric: 42.8739, val_loss: 45.4333, val_MinusLogProbMetric: 45.4333

Epoch 482: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8739 - MinusLogProbMetric: 42.8739 - val_loss: 45.4333 - val_MinusLogProbMetric: 45.4333 - lr: 2.0576e-06 - 41s/epoch - 211ms/step
Epoch 483/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:08:10.117 
Epoch 483/1000 
	 loss: 42.8926, MinusLogProbMetric: 42.8926, val_loss: 45.1616, val_MinusLogProbMetric: 45.1616

Epoch 483: val_loss did not improve from 44.58332
196/196 - 40s - loss: 42.8926 - MinusLogProbMetric: 42.8926 - val_loss: 45.1616 - val_MinusLogProbMetric: 45.1616 - lr: 2.0576e-06 - 40s/epoch - 204ms/step
Epoch 484/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:08:51.333 
Epoch 484/1000 
	 loss: 42.8979, MinusLogProbMetric: 42.8979, val_loss: 44.9341, val_MinusLogProbMetric: 44.9340

Epoch 484: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8979 - MinusLogProbMetric: 42.8979 - val_loss: 44.9341 - val_MinusLogProbMetric: 44.9340 - lr: 2.0576e-06 - 41s/epoch - 210ms/step
Epoch 485/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:09:32.816 
Epoch 485/1000 
	 loss: 42.9004, MinusLogProbMetric: 42.9004, val_loss: 45.0554, val_MinusLogProbMetric: 45.0553

Epoch 485: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.9004 - MinusLogProbMetric: 42.9004 - val_loss: 45.0554 - val_MinusLogProbMetric: 45.0553 - lr: 2.0576e-06 - 41s/epoch - 212ms/step
Epoch 486/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:10:14.006 
Epoch 486/1000 
	 loss: 42.8833, MinusLogProbMetric: 42.8833, val_loss: 45.0402, val_MinusLogProbMetric: 45.0401

Epoch 486: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8833 - MinusLogProbMetric: 42.8833 - val_loss: 45.0402 - val_MinusLogProbMetric: 45.0401 - lr: 2.0576e-06 - 41s/epoch - 210ms/step
Epoch 487/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:10:55.479 
Epoch 487/1000 
	 loss: 42.8828, MinusLogProbMetric: 42.8828, val_loss: 45.4221, val_MinusLogProbMetric: 45.4221

Epoch 487: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8828 - MinusLogProbMetric: 42.8828 - val_loss: 45.4221 - val_MinusLogProbMetric: 45.4221 - lr: 2.0576e-06 - 41s/epoch - 212ms/step
Epoch 488/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:11:37.472 
Epoch 488/1000 
	 loss: 42.8637, MinusLogProbMetric: 42.8637, val_loss: 44.7740, val_MinusLogProbMetric: 44.7740

Epoch 488: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8637 - MinusLogProbMetric: 42.8637 - val_loss: 44.7740 - val_MinusLogProbMetric: 44.7740 - lr: 2.0576e-06 - 42s/epoch - 214ms/step
Epoch 489/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:12:18.840 
Epoch 489/1000 
	 loss: 42.8575, MinusLogProbMetric: 42.8575, val_loss: 45.1491, val_MinusLogProbMetric: 45.1491

Epoch 489: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8575 - MinusLogProbMetric: 42.8575 - val_loss: 45.1491 - val_MinusLogProbMetric: 45.1491 - lr: 2.0576e-06 - 41s/epoch - 211ms/step
Epoch 490/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:13:01.168 
Epoch 490/1000 
	 loss: 42.8819, MinusLogProbMetric: 42.8819, val_loss: 44.9006, val_MinusLogProbMetric: 44.9006

Epoch 490: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8819 - MinusLogProbMetric: 42.8819 - val_loss: 44.9006 - val_MinusLogProbMetric: 44.9006 - lr: 2.0576e-06 - 42s/epoch - 216ms/step
Epoch 491/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:13:43.100 
Epoch 491/1000 
	 loss: 42.8732, MinusLogProbMetric: 42.8732, val_loss: 45.1121, val_MinusLogProbMetric: 45.1121

Epoch 491: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8732 - MinusLogProbMetric: 42.8732 - val_loss: 45.1121 - val_MinusLogProbMetric: 45.1121 - lr: 2.0576e-06 - 42s/epoch - 214ms/step
Epoch 492/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:14:24.724 
Epoch 492/1000 
	 loss: 42.8586, MinusLogProbMetric: 42.8586, val_loss: 45.6134, val_MinusLogProbMetric: 45.6133

Epoch 492: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8586 - MinusLogProbMetric: 42.8586 - val_loss: 45.6134 - val_MinusLogProbMetric: 45.6133 - lr: 2.0576e-06 - 42s/epoch - 212ms/step
Epoch 493/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:15:06.344 
Epoch 493/1000 
	 loss: 42.8921, MinusLogProbMetric: 42.8921, val_loss: 45.2021, val_MinusLogProbMetric: 45.2020

Epoch 493: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8921 - MinusLogProbMetric: 42.8921 - val_loss: 45.2021 - val_MinusLogProbMetric: 45.2020 - lr: 2.0576e-06 - 42s/epoch - 212ms/step
Epoch 494/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:15:46.998 
Epoch 494/1000 
	 loss: 42.8684, MinusLogProbMetric: 42.8684, val_loss: 45.0466, val_MinusLogProbMetric: 45.0466

Epoch 494: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8684 - MinusLogProbMetric: 42.8684 - val_loss: 45.0466 - val_MinusLogProbMetric: 45.0466 - lr: 2.0576e-06 - 41s/epoch - 207ms/step
Epoch 495/1000
2023-10-28 09:16:28.372 
Epoch 495/1000 
	 loss: 42.8638, MinusLogProbMetric: 42.8638, val_loss: 45.1491, val_MinusLogProbMetric: 45.1491

Epoch 495: val_loss did not improve from 44.58332
196/196 - 41s - loss: 42.8638 - MinusLogProbMetric: 42.8638 - val_loss: 45.1491 - val_MinusLogProbMetric: 45.1491 - lr: 2.0576e-06 - 41s/epoch - 211ms/step
Epoch 496/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:17:10.292 
Epoch 496/1000 
	 loss: 42.8790, MinusLogProbMetric: 42.8790, val_loss: 45.1788, val_MinusLogProbMetric: 45.1788

Epoch 496: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8790 - MinusLogProbMetric: 42.8790 - val_loss: 45.1788 - val_MinusLogProbMetric: 45.1788 - lr: 2.0576e-06 - 42s/epoch - 214ms/step
Epoch 497/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:17:51.949 
Epoch 497/1000 
	 loss: 42.8717, MinusLogProbMetric: 42.8717, val_loss: 45.2927, val_MinusLogProbMetric: 45.2927

Epoch 497: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8717 - MinusLogProbMetric: 42.8717 - val_loss: 45.2927 - val_MinusLogProbMetric: 45.2927 - lr: 2.0576e-06 - 42s/epoch - 213ms/step
Epoch 498/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:18:33.914 
Epoch 498/1000 
	 loss: 42.8728, MinusLogProbMetric: 42.8728, val_loss: 45.0190, val_MinusLogProbMetric: 45.0189

Epoch 498: val_loss did not improve from 44.58332
196/196 - 42s - loss: 42.8728 - MinusLogProbMetric: 42.8728 - val_loss: 45.0190 - val_MinusLogProbMetric: 45.0189 - lr: 2.0576e-06 - 42s/epoch - 214ms/step
Epoch 499/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-28 09:19:14.934 
Epoch 499/1000 
	 loss: 42.8944, MinusLogProbMetric: 42.8944, val_loss: 44.9345, val_MinusLogProbMetric: 44.9345

Epoch 499: val_loss did not improve from 44.58332
Restoring model weights from the end of the best epoch: 399.
196/196 - 41s - loss: 42.8944 - MinusLogProbMetric: 42.8944 - val_loss: 44.9345 - val_MinusLogProbMetric: 44.9345 - lr: 2.0576e-06 - 41s/epoch - 211ms/step
Epoch 499: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 440.
Model trained in 20396.04 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.89 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.14 s.
===========
Run 428/720 done in 40733.87 s.
===========

Directory ../../results/CsplineN_new/run_429/ already exists.
Skipping it.
===========
Run 429/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_430/ already exists.
Skipping it.
===========
Run 430/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_431/ already exists.
Skipping it.
===========
Run 431/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_432/ already exists.
Skipping it.
===========
Run 432/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_433/ already exists.
Skipping it.
===========
Run 433/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_434/ already exists.
Skipping it.
===========
Run 434/720 already exists. Skipping it.
===========

===========
Generating train data for run 435.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_488"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_489 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7f3e2472a500>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36b9af1c90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36b9af1c90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f357cc91fc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3df0ff6b90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3df0ff62f0>, <keras.callbacks.ModelCheckpoint object at 0x7f3df0ff4400>, <keras.callbacks.EarlyStopping object at 0x7f3df0ff6410>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3df0ff4ac0>, <keras.callbacks.TerminateOnNaN object at 0x7f3df0ff7e50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:19:21.264680
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:20:24.588 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 63s/epoch - 323ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 435.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_494"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_495 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7f34bdc5ff40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34adf51630>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34adf51630>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f334195b2b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34adc7be50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f34adcd0400>, <keras.callbacks.ModelCheckpoint object at 0x7f34adcd04c0>, <keras.callbacks.EarlyStopping object at 0x7f34adcd0730>, <keras.callbacks.ReduceLROnPlateau object at 0x7f34adcd0760>, <keras.callbacks.TerminateOnNaN object at 0x7f34adcd03a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:20:29.836520
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:21:40.651 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 71s/epoch - 361ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 435.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_500"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_501 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7f355da751e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3515433f70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3515433f70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f355d0cebc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f359d8e1ab0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f359d8e2020>, <keras.callbacks.ModelCheckpoint object at 0x7f359d8e20e0>, <keras.callbacks.EarlyStopping object at 0x7f359d8e2350>, <keras.callbacks.ReduceLROnPlateau object at 0x7f359d8e2380>, <keras.callbacks.TerminateOnNaN object at 0x7f359d8e1fc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:21:45.989391
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:22:46.843 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 61s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 61s/epoch - 310ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 435.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_506"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_507 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7f3514702d40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d47369db0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d47369db0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d47263f70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d46e1bd60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d46ec4310>, <keras.callbacks.ModelCheckpoint object at 0x7f3d46ec43d0>, <keras.callbacks.EarlyStopping object at 0x7f3d46ec4640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d46ec4670>, <keras.callbacks.TerminateOnNaN object at 0x7f3d46ec42b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:22:51.965992
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:23:55.714 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 64s/epoch - 325ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 435.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_512"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_513 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_62 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_62/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_62'")
self.model: <keras.engine.functional.Functional object at 0x7f33420d1450>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f334267e3b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f334267e3b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f350d6f28f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34cc379450>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f34cc3799c0>, <keras.callbacks.ModelCheckpoint object at 0x7f34cc379a80>, <keras.callbacks.EarlyStopping object at 0x7f34cc379cf0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f34cc379d20>, <keras.callbacks.TerminateOnNaN object at 0x7f34cc379960>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:23:59.053799
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:25:12.222 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 73s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 73s/epoch - 374ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 435.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_518"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_519 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_63 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_63/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_63'")
self.model: <keras.engine.functional.Functional object at 0x7f354c5b05e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f334069ae30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f334069ae30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f355d0ce3b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35d8a23e80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35d8ab8430>, <keras.callbacks.ModelCheckpoint object at 0x7f35d8ab84f0>, <keras.callbacks.EarlyStopping object at 0x7f35d8ab8760>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35d8ab8790>, <keras.callbacks.TerminateOnNaN object at 0x7f35d8ab83d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:25:17.848068
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:26:20.472 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 63s/epoch - 319ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 435.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_524"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_525 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_64 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_64/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_64'")
self.model: <keras.engine.functional.Functional object at 0x7f35d8ac9180>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35d8aea8f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35d8aea8f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f351556e710>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36b84114b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36b8411a20>, <keras.callbacks.ModelCheckpoint object at 0x7f36b8411ae0>, <keras.callbacks.EarlyStopping object at 0x7f36b8411d50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36b8411d80>, <keras.callbacks.TerminateOnNaN object at 0x7f36b84119c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:26:25.839702
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:27:29.071 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 63s/epoch - 323ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 435.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_530"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_531 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_65 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_65/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_65'")
self.model: <keras.engine.functional.Functional object at 0x7f333386f490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3350799360>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3350799360>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f33314a4d00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3332c76a10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3332c76f80>, <keras.callbacks.ModelCheckpoint object at 0x7f3332c77040>, <keras.callbacks.EarlyStopping object at 0x7f3332c772b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3332c772e0>, <keras.callbacks.TerminateOnNaN object at 0x7f3332c76f20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:27:34.253164
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:28:45.532 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 71s/epoch - 364ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 435.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_536"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_537 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_66 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_66/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_66'")
self.model: <keras.engine.functional.Functional object at 0x7f3671d78e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f373833a560>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f373833a560>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36e8eb35b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34bd497d90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f34bd497700>, <keras.callbacks.ModelCheckpoint object at 0x7f34bd495300>, <keras.callbacks.EarlyStopping object at 0x7f34bd496e60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f34bd497ee0>, <keras.callbacks.TerminateOnNaN object at 0x7f34bd494580>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:28:51.550268
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:29:57.575 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 66s/epoch - 336ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 435.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_542"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_543 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_67 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_67/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_67'")
self.model: <keras.engine.functional.Functional object at 0x7f350d75b280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f363795f520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f363795f520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f34cc847640>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d8a657b80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d8a618130>, <keras.callbacks.ModelCheckpoint object at 0x7f3d8a6181f0>, <keras.callbacks.EarlyStopping object at 0x7f3d8a618460>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d8a618490>, <keras.callbacks.TerminateOnNaN object at 0x7f3d8a6180d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:30:02.689774
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:31:05.472 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 63s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 63s/epoch - 320ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 435.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_435/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_435
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_548"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_549 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_68 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_68/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_68'")
self.model: <keras.engine.functional.Functional object at 0x7f331a1caa40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f333a11ff40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f333a11ff40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d8a65aaa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f333b969180>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_435/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f333b9696f0>, <keras.callbacks.ModelCheckpoint object at 0x7f333b9697b0>, <keras.callbacks.EarlyStopping object at 0x7f333b969a20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f333b969a50>, <keras.callbacks.TerminateOnNaN object at 0x7f333b969690>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_435/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 435/720 with hyperparameters:
timestamp = 2023-10-28 09:31:10.426899
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:32:24.396 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10571.7490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 74s - loss: nan - MinusLogProbMetric: 10571.7490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 74s/epoch - 378ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 435/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 436.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_436/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_436/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_436/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_436
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_554"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_555 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_69 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_69/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_69'")
self.model: <keras.engine.functional.Functional object at 0x7f3310aa4130>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3595059de0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3595059de0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d4fedb5e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f363788c550>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f363788cac0>, <keras.callbacks.ModelCheckpoint object at 0x7f363788cb80>, <keras.callbacks.EarlyStopping object at 0x7f363788d450>, <keras.callbacks.ReduceLROnPlateau object at 0x7f363788c3a0>, <keras.callbacks.TerminateOnNaN object at 0x7f363788c190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_436/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 436/720 with hyperparameters:
timestamp = 2023-10-28 09:32:29.683801
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 19: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 09:33:35.231 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7718.8354, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 7718.8354 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 66s/epoch - 335ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 436.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_436/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_436/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_436/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_436
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_560"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_561 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_70 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_70/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_70'")
self.model: <keras.engine.functional.Functional object at 0x7f36c87d7dc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f333a2e35b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f333a2e35b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3505eb0be0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34bd286560>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f34bd286ad0>, <keras.callbacks.ModelCheckpoint object at 0x7f34bd286b90>, <keras.callbacks.EarlyStopping object at 0x7f34bd286e00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f34bd286e30>, <keras.callbacks.TerminateOnNaN object at 0x7f34bd286a70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_436/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 436/720 with hyperparameters:
timestamp = 2023-10-28 09:33:40.476673
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
2023-10-28 09:35:19.137 
Epoch 1/1000 
	 loss: 1891.3301, MinusLogProbMetric: 1891.3301, val_loss: 554.1653, val_MinusLogProbMetric: 554.1653

Epoch 1: val_loss improved from inf to 554.16528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 99s - loss: 1891.3301 - MinusLogProbMetric: 1891.3301 - val_loss: 554.1653 - val_MinusLogProbMetric: 554.1653 - lr: 3.3333e-04 - 99s/epoch - 507ms/step
Epoch 2/1000
2023-10-28 09:36:00.753 
Epoch 2/1000 
	 loss: 491.2850, MinusLogProbMetric: 491.2850, val_loss: 374.7531, val_MinusLogProbMetric: 374.7531

Epoch 2: val_loss improved from 554.16528 to 374.75311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 491.2850 - MinusLogProbMetric: 491.2850 - val_loss: 374.7531 - val_MinusLogProbMetric: 374.7531 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 3/1000
2023-10-28 09:36:42.668 
Epoch 3/1000 
	 loss: 326.3085, MinusLogProbMetric: 326.3085, val_loss: 459.8635, val_MinusLogProbMetric: 459.8635

Epoch 3: val_loss did not improve from 374.75311
196/196 - 41s - loss: 326.3085 - MinusLogProbMetric: 326.3085 - val_loss: 459.8635 - val_MinusLogProbMetric: 459.8635 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 4/1000
2023-10-28 09:37:23.868 
Epoch 4/1000 
	 loss: 297.6917, MinusLogProbMetric: 297.6917, val_loss: 237.8353, val_MinusLogProbMetric: 237.8353

Epoch 4: val_loss improved from 374.75311 to 237.83527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 297.6917 - MinusLogProbMetric: 297.6917 - val_loss: 237.8353 - val_MinusLogProbMetric: 237.8353 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 5/1000
2023-10-28 09:38:04.452 
Epoch 5/1000 
	 loss: 207.4626, MinusLogProbMetric: 207.4626, val_loss: 171.5061, val_MinusLogProbMetric: 171.5061

Epoch 5: val_loss improved from 237.83527 to 171.50612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 40s - loss: 207.4626 - MinusLogProbMetric: 207.4626 - val_loss: 171.5061 - val_MinusLogProbMetric: 171.5061 - lr: 3.3333e-04 - 40s/epoch - 207ms/step
Epoch 6/1000
2023-10-28 09:38:44.195 
Epoch 6/1000 
	 loss: 169.9602, MinusLogProbMetric: 169.9602, val_loss: 238.2357, val_MinusLogProbMetric: 238.2357

Epoch 6: val_loss did not improve from 171.50612
196/196 - 39s - loss: 169.9602 - MinusLogProbMetric: 169.9602 - val_loss: 238.2357 - val_MinusLogProbMetric: 238.2357 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 7/1000
2023-10-28 09:39:24.057 
Epoch 7/1000 
	 loss: 152.5940, MinusLogProbMetric: 152.5940, val_loss: 134.9850, val_MinusLogProbMetric: 134.9850

Epoch 7: val_loss improved from 171.50612 to 134.98499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 152.5940 - MinusLogProbMetric: 152.5940 - val_loss: 134.9850 - val_MinusLogProbMetric: 134.9850 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 8/1000
2023-10-28 09:40:04.627 
Epoch 8/1000 
	 loss: 129.4828, MinusLogProbMetric: 129.4828, val_loss: 121.8751, val_MinusLogProbMetric: 121.8751

Epoch 8: val_loss improved from 134.98499 to 121.87508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 129.4828 - MinusLogProbMetric: 129.4828 - val_loss: 121.8751 - val_MinusLogProbMetric: 121.8751 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 9/1000
2023-10-28 09:40:45.125 
Epoch 9/1000 
	 loss: 117.1806, MinusLogProbMetric: 117.1806, val_loss: 116.6235, val_MinusLogProbMetric: 116.6235

Epoch 9: val_loss improved from 121.87508 to 116.62354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 40s - loss: 117.1806 - MinusLogProbMetric: 117.1806 - val_loss: 116.6235 - val_MinusLogProbMetric: 116.6235 - lr: 3.3333e-04 - 40s/epoch - 207ms/step
Epoch 10/1000
2023-10-28 09:41:25.590 
Epoch 10/1000 
	 loss: 108.9390, MinusLogProbMetric: 108.9390, val_loss: 105.8973, val_MinusLogProbMetric: 105.8973

Epoch 10: val_loss improved from 116.62354 to 105.89729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 40s - loss: 108.9390 - MinusLogProbMetric: 108.9390 - val_loss: 105.8973 - val_MinusLogProbMetric: 105.8973 - lr: 3.3333e-04 - 40s/epoch - 207ms/step
Epoch 11/1000
2023-10-28 09:42:05.874 
Epoch 11/1000 
	 loss: 102.0305, MinusLogProbMetric: 102.0305, val_loss: 99.3081, val_MinusLogProbMetric: 99.3081

Epoch 11: val_loss improved from 105.89729 to 99.30807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 40s - loss: 102.0305 - MinusLogProbMetric: 102.0305 - val_loss: 99.3081 - val_MinusLogProbMetric: 99.3081 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 12/1000
2023-10-28 09:42:47.036 
Epoch 12/1000 
	 loss: 96.5397, MinusLogProbMetric: 96.5397, val_loss: 96.5131, val_MinusLogProbMetric: 96.5131

Epoch 12: val_loss improved from 99.30807 to 96.51312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 96.5397 - MinusLogProbMetric: 96.5397 - val_loss: 96.5131 - val_MinusLogProbMetric: 96.5131 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 13/1000
2023-10-28 09:43:28.281 
Epoch 13/1000 
	 loss: 91.9544, MinusLogProbMetric: 91.9544, val_loss: 91.5546, val_MinusLogProbMetric: 91.5546

Epoch 13: val_loss improved from 96.51312 to 91.55457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 91.9544 - MinusLogProbMetric: 91.9544 - val_loss: 91.5546 - val_MinusLogProbMetric: 91.5546 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 14/1000
2023-10-28 09:44:09.382 
Epoch 14/1000 
	 loss: 87.5713, MinusLogProbMetric: 87.5713, val_loss: 88.5383, val_MinusLogProbMetric: 88.5383

Epoch 14: val_loss improved from 91.55457 to 88.53826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 87.5713 - MinusLogProbMetric: 87.5713 - val_loss: 88.5383 - val_MinusLogProbMetric: 88.5383 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 15/1000
2023-10-28 09:44:50.768 
Epoch 15/1000 
	 loss: 84.4496, MinusLogProbMetric: 84.4496, val_loss: 83.5503, val_MinusLogProbMetric: 83.5503

Epoch 15: val_loss improved from 88.53826 to 83.55028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 84.4496 - MinusLogProbMetric: 84.4496 - val_loss: 83.5503 - val_MinusLogProbMetric: 83.5503 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 16/1000
2023-10-28 09:45:32.429 
Epoch 16/1000 
	 loss: 81.1566, MinusLogProbMetric: 81.1566, val_loss: 79.5536, val_MinusLogProbMetric: 79.5536

Epoch 16: val_loss improved from 83.55028 to 79.55359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 81.1566 - MinusLogProbMetric: 81.1566 - val_loss: 79.5536 - val_MinusLogProbMetric: 79.5536 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 17/1000
2023-10-28 09:46:14.468 
Epoch 17/1000 
	 loss: 78.6069, MinusLogProbMetric: 78.6069, val_loss: 78.9960, val_MinusLogProbMetric: 78.9960

Epoch 17: val_loss improved from 79.55359 to 78.99603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 78.6069 - MinusLogProbMetric: 78.6069 - val_loss: 78.9960 - val_MinusLogProbMetric: 78.9960 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 18/1000
2023-10-28 09:46:56.810 
Epoch 18/1000 
	 loss: 76.6671, MinusLogProbMetric: 76.6671, val_loss: 75.8882, val_MinusLogProbMetric: 75.8882

Epoch 18: val_loss improved from 78.99603 to 75.88819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 76.6671 - MinusLogProbMetric: 76.6671 - val_loss: 75.8882 - val_MinusLogProbMetric: 75.8882 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 19/1000
2023-10-28 09:47:38.675 
Epoch 19/1000 
	 loss: 74.0902, MinusLogProbMetric: 74.0902, val_loss: 75.3312, val_MinusLogProbMetric: 75.3312

Epoch 19: val_loss improved from 75.88819 to 75.33117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 74.0902 - MinusLogProbMetric: 74.0902 - val_loss: 75.3312 - val_MinusLogProbMetric: 75.3312 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 20/1000
2023-10-28 09:48:20.919 
Epoch 20/1000 
	 loss: 72.9074, MinusLogProbMetric: 72.9074, val_loss: 71.2078, val_MinusLogProbMetric: 71.2078

Epoch 20: val_loss improved from 75.33117 to 71.20783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 72.9074 - MinusLogProbMetric: 72.9074 - val_loss: 71.2078 - val_MinusLogProbMetric: 71.2078 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 21/1000
2023-10-28 09:49:02.762 
Epoch 21/1000 
	 loss: 70.9446, MinusLogProbMetric: 70.9446, val_loss: 70.9018, val_MinusLogProbMetric: 70.9018

Epoch 21: val_loss improved from 71.20783 to 70.90182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 70.9446 - MinusLogProbMetric: 70.9446 - val_loss: 70.9018 - val_MinusLogProbMetric: 70.9018 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 22/1000
2023-10-28 09:49:44.184 
Epoch 22/1000 
	 loss: 69.5617, MinusLogProbMetric: 69.5617, val_loss: 72.0500, val_MinusLogProbMetric: 72.0500

Epoch 22: val_loss did not improve from 70.90182
196/196 - 41s - loss: 69.5617 - MinusLogProbMetric: 69.5617 - val_loss: 72.0500 - val_MinusLogProbMetric: 72.0500 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 23/1000
2023-10-28 09:50:25.485 
Epoch 23/1000 
	 loss: 68.5061, MinusLogProbMetric: 68.5061, val_loss: 68.0959, val_MinusLogProbMetric: 68.0959

Epoch 23: val_loss improved from 70.90182 to 68.09586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 68.5061 - MinusLogProbMetric: 68.5061 - val_loss: 68.0959 - val_MinusLogProbMetric: 68.0959 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 24/1000
2023-10-28 09:51:07.375 
Epoch 24/1000 
	 loss: 68.4680, MinusLogProbMetric: 68.4680, val_loss: 72.9370, val_MinusLogProbMetric: 72.9370

Epoch 24: val_loss did not improve from 68.09586
196/196 - 41s - loss: 68.4680 - MinusLogProbMetric: 68.4680 - val_loss: 72.9370 - val_MinusLogProbMetric: 72.9370 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 25/1000
2023-10-28 09:51:48.043 
Epoch 25/1000 
	 loss: 67.0215, MinusLogProbMetric: 67.0215, val_loss: 65.7858, val_MinusLogProbMetric: 65.7858

Epoch 25: val_loss improved from 68.09586 to 65.78584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 67.0215 - MinusLogProbMetric: 67.0215 - val_loss: 65.7858 - val_MinusLogProbMetric: 65.7858 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 26/1000
2023-10-28 09:52:28.268 
Epoch 26/1000 
	 loss: 66.0182, MinusLogProbMetric: 66.0182, val_loss: 64.8037, val_MinusLogProbMetric: 64.8037

Epoch 26: val_loss improved from 65.78584 to 64.80366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 40s - loss: 66.0182 - MinusLogProbMetric: 66.0182 - val_loss: 64.8037 - val_MinusLogProbMetric: 64.8037 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 27/1000
2023-10-28 09:53:10.077 
Epoch 27/1000 
	 loss: 66.0136, MinusLogProbMetric: 66.0136, val_loss: 64.6407, val_MinusLogProbMetric: 64.6407

Epoch 27: val_loss improved from 64.80366 to 64.64071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 66.0136 - MinusLogProbMetric: 66.0136 - val_loss: 64.6407 - val_MinusLogProbMetric: 64.6407 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 28/1000
2023-10-28 09:53:50.023 
Epoch 28/1000 
	 loss: 63.8254, MinusLogProbMetric: 63.8254, val_loss: 63.4449, val_MinusLogProbMetric: 63.4449

Epoch 28: val_loss improved from 64.64071 to 63.44492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 40s - loss: 63.8254 - MinusLogProbMetric: 63.8254 - val_loss: 63.4449 - val_MinusLogProbMetric: 63.4449 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 29/1000
2023-10-28 09:54:31.740 
Epoch 29/1000 
	 loss: 62.8703, MinusLogProbMetric: 62.8703, val_loss: 63.3204, val_MinusLogProbMetric: 63.3204

Epoch 29: val_loss improved from 63.44492 to 63.32040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 62.8703 - MinusLogProbMetric: 62.8703 - val_loss: 63.3204 - val_MinusLogProbMetric: 63.3204 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 30/1000
2023-10-28 09:55:12.518 
Epoch 30/1000 
	 loss: 62.9043, MinusLogProbMetric: 62.9043, val_loss: 61.9043, val_MinusLogProbMetric: 61.9043

Epoch 30: val_loss improved from 63.32040 to 61.90431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 62.9043 - MinusLogProbMetric: 62.9043 - val_loss: 61.9043 - val_MinusLogProbMetric: 61.9043 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 31/1000
2023-10-28 09:55:53.272 
Epoch 31/1000 
	 loss: 61.8994, MinusLogProbMetric: 61.8994, val_loss: 63.5962, val_MinusLogProbMetric: 63.5962

Epoch 31: val_loss did not improve from 61.90431
196/196 - 40s - loss: 61.8994 - MinusLogProbMetric: 61.8994 - val_loss: 63.5962 - val_MinusLogProbMetric: 63.5962 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 32/1000
2023-10-28 09:56:33.097 
Epoch 32/1000 
	 loss: 61.6960, MinusLogProbMetric: 61.6960, val_loss: 61.4244, val_MinusLogProbMetric: 61.4244

Epoch 32: val_loss improved from 61.90431 to 61.42439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 61.6960 - MinusLogProbMetric: 61.6960 - val_loss: 61.4244 - val_MinusLogProbMetric: 61.4244 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 33/1000
2023-10-28 09:57:13.767 
Epoch 33/1000 
	 loss: 60.8635, MinusLogProbMetric: 60.8635, val_loss: 63.6704, val_MinusLogProbMetric: 63.6704

Epoch 33: val_loss did not improve from 61.42439
196/196 - 40s - loss: 60.8635 - MinusLogProbMetric: 60.8635 - val_loss: 63.6704 - val_MinusLogProbMetric: 63.6704 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 34/1000
2023-10-28 09:57:53.827 
Epoch 34/1000 
	 loss: 60.1909, MinusLogProbMetric: 60.1909, val_loss: 59.2492, val_MinusLogProbMetric: 59.2492

Epoch 34: val_loss improved from 61.42439 to 59.24924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 60.1909 - MinusLogProbMetric: 60.1909 - val_loss: 59.2492 - val_MinusLogProbMetric: 59.2492 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 35/1000
2023-10-28 09:58:33.864 
Epoch 35/1000 
	 loss: 60.1636, MinusLogProbMetric: 60.1636, val_loss: 60.2409, val_MinusLogProbMetric: 60.2409

Epoch 35: val_loss did not improve from 59.24924
196/196 - 39s - loss: 60.1636 - MinusLogProbMetric: 60.1636 - val_loss: 60.2409 - val_MinusLogProbMetric: 60.2409 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 36/1000
2023-10-28 09:59:13.861 
Epoch 36/1000 
	 loss: 59.6977, MinusLogProbMetric: 59.6977, val_loss: 59.7888, val_MinusLogProbMetric: 59.7888

Epoch 36: val_loss did not improve from 59.24924
196/196 - 40s - loss: 59.6977 - MinusLogProbMetric: 59.6977 - val_loss: 59.7888 - val_MinusLogProbMetric: 59.7888 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 37/1000
2023-10-28 09:59:53.851 
Epoch 37/1000 
	 loss: 60.1034, MinusLogProbMetric: 60.1034, val_loss: 66.2053, val_MinusLogProbMetric: 66.2053

Epoch 37: val_loss did not improve from 59.24924
196/196 - 40s - loss: 60.1034 - MinusLogProbMetric: 60.1034 - val_loss: 66.2053 - val_MinusLogProbMetric: 66.2053 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 38/1000
2023-10-28 10:00:33.780 
Epoch 38/1000 
	 loss: 59.1706, MinusLogProbMetric: 59.1706, val_loss: 58.1363, val_MinusLogProbMetric: 58.1363

Epoch 38: val_loss improved from 59.24924 to 58.13632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 59.1706 - MinusLogProbMetric: 59.1706 - val_loss: 58.1363 - val_MinusLogProbMetric: 58.1363 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 39/1000
2023-10-28 10:01:15.261 
Epoch 39/1000 
	 loss: 58.4959, MinusLogProbMetric: 58.4959, val_loss: 62.6197, val_MinusLogProbMetric: 62.6197

Epoch 39: val_loss did not improve from 58.13632
196/196 - 41s - loss: 58.4959 - MinusLogProbMetric: 58.4959 - val_loss: 62.6197 - val_MinusLogProbMetric: 62.6197 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 40/1000
2023-10-28 10:01:55.609 
Epoch 40/1000 
	 loss: 58.4171, MinusLogProbMetric: 58.4171, val_loss: 63.6335, val_MinusLogProbMetric: 63.6335

Epoch 40: val_loss did not improve from 58.13632
196/196 - 40s - loss: 58.4171 - MinusLogProbMetric: 58.4171 - val_loss: 63.6335 - val_MinusLogProbMetric: 63.6335 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 41/1000
2023-10-28 10:02:35.544 
Epoch 41/1000 
	 loss: 63.3035, MinusLogProbMetric: 63.3035, val_loss: 57.7088, val_MinusLogProbMetric: 57.7088

Epoch 41: val_loss improved from 58.13632 to 57.70884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 63.3035 - MinusLogProbMetric: 63.3035 - val_loss: 57.7088 - val_MinusLogProbMetric: 57.7088 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 42/1000
2023-10-28 10:03:15.923 
Epoch 42/1000 
	 loss: 59.4057, MinusLogProbMetric: 59.4057, val_loss: 57.7874, val_MinusLogProbMetric: 57.7874

Epoch 42: val_loss did not improve from 57.70884
196/196 - 40s - loss: 59.4057 - MinusLogProbMetric: 59.4057 - val_loss: 57.7874 - val_MinusLogProbMetric: 57.7874 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 43/1000
2023-10-28 10:03:55.936 
Epoch 43/1000 
	 loss: 56.3452, MinusLogProbMetric: 56.3452, val_loss: 56.6125, val_MinusLogProbMetric: 56.6125

Epoch 43: val_loss improved from 57.70884 to 56.61255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 56.3452 - MinusLogProbMetric: 56.3452 - val_loss: 56.6125 - val_MinusLogProbMetric: 56.6125 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 44/1000
2023-10-28 10:04:37.001 
Epoch 44/1000 
	 loss: 56.4804, MinusLogProbMetric: 56.4804, val_loss: 57.4592, val_MinusLogProbMetric: 57.4592

Epoch 44: val_loss did not improve from 56.61255
196/196 - 40s - loss: 56.4804 - MinusLogProbMetric: 56.4804 - val_loss: 57.4592 - val_MinusLogProbMetric: 57.4592 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 45/1000
2023-10-28 10:05:17.493 
Epoch 45/1000 
	 loss: 67.0871, MinusLogProbMetric: 67.0871, val_loss: 72.7564, val_MinusLogProbMetric: 72.7564

Epoch 45: val_loss did not improve from 56.61255
196/196 - 40s - loss: 67.0871 - MinusLogProbMetric: 67.0871 - val_loss: 72.7564 - val_MinusLogProbMetric: 72.7564 - lr: 3.3333e-04 - 40s/epoch - 207ms/step
Epoch 46/1000
2023-10-28 10:05:57.839 
Epoch 46/1000 
	 loss: 58.9903, MinusLogProbMetric: 58.9903, val_loss: 59.3896, val_MinusLogProbMetric: 59.3896

Epoch 46: val_loss did not improve from 56.61255
196/196 - 40s - loss: 58.9903 - MinusLogProbMetric: 58.9903 - val_loss: 59.3896 - val_MinusLogProbMetric: 59.3896 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 47/1000
2023-10-28 10:06:38.232 
Epoch 47/1000 
	 loss: 56.4802, MinusLogProbMetric: 56.4802, val_loss: 56.4538, val_MinusLogProbMetric: 56.4538

Epoch 47: val_loss improved from 56.61255 to 56.45384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 56.4802 - MinusLogProbMetric: 56.4802 - val_loss: 56.4538 - val_MinusLogProbMetric: 56.4538 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 48/1000
2023-10-28 10:07:18.042 
Epoch 48/1000 
	 loss: 55.9143, MinusLogProbMetric: 55.9143, val_loss: 55.2408, val_MinusLogProbMetric: 55.2408

Epoch 48: val_loss improved from 56.45384 to 55.24084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 40s - loss: 55.9143 - MinusLogProbMetric: 55.9143 - val_loss: 55.2408 - val_MinusLogProbMetric: 55.2408 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 49/1000
2023-10-28 10:07:58.760 
Epoch 49/1000 
	 loss: 55.5564, MinusLogProbMetric: 55.5564, val_loss: 54.9793, val_MinusLogProbMetric: 54.9793

Epoch 49: val_loss improved from 55.24084 to 54.97935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 55.5564 - MinusLogProbMetric: 55.5564 - val_loss: 54.9793 - val_MinusLogProbMetric: 54.9793 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 50/1000
2023-10-28 10:08:39.384 
Epoch 50/1000 
	 loss: 57.2485, MinusLogProbMetric: 57.2485, val_loss: 58.7088, val_MinusLogProbMetric: 58.7088

Epoch 50: val_loss did not improve from 54.97935
196/196 - 40s - loss: 57.2485 - MinusLogProbMetric: 57.2485 - val_loss: 58.7088 - val_MinusLogProbMetric: 58.7088 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 51/1000
2023-10-28 10:09:19.404 
Epoch 51/1000 
	 loss: 55.7709, MinusLogProbMetric: 55.7709, val_loss: 56.2881, val_MinusLogProbMetric: 56.2881

Epoch 51: val_loss did not improve from 54.97935
196/196 - 40s - loss: 55.7709 - MinusLogProbMetric: 55.7709 - val_loss: 56.2881 - val_MinusLogProbMetric: 56.2881 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 52/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 136: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 10:09:48.329 
Epoch 52/1000 
	 loss: nan, MinusLogProbMetric: 65.9748, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 52: val_loss did not improve from 54.97935
196/196 - 29s - loss: nan - MinusLogProbMetric: 65.9748 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 29s/epoch - 148ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 436.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_436/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_436/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_436/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_436
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_566"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_567 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_71 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_71/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_71'")
self.model: <keras.engine.functional.Functional object at 0x7f331be29360>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3331d1b4f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3331d1b4f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3332afba00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3318183ac0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3318183fd0>, <keras.callbacks.ModelCheckpoint object at 0x7f33180fc130>, <keras.callbacks.EarlyStopping object at 0x7f33180fc3a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f33180fc3d0>, <keras.callbacks.TerminateOnNaN object at 0x7f33180fc040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 436/720 with hyperparameters:
timestamp = 2023-10-28 10:09:53.619154
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
2023-10-28 10:11:45.990 
Epoch 1/1000 
	 loss: 96.6266, MinusLogProbMetric: 96.6266, val_loss: 58.6682, val_MinusLogProbMetric: 58.6682

Epoch 1: val_loss improved from inf to 58.66819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 113s - loss: 96.6266 - MinusLogProbMetric: 96.6266 - val_loss: 58.6682 - val_MinusLogProbMetric: 58.6682 - lr: 1.1111e-04 - 113s/epoch - 577ms/step
Epoch 2/1000
2023-10-28 10:12:26.961 
Epoch 2/1000 
	 loss: 56.2474, MinusLogProbMetric: 56.2474, val_loss: 55.6623, val_MinusLogProbMetric: 55.6623

Epoch 2: val_loss improved from 58.66819 to 55.66235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 56.2474 - MinusLogProbMetric: 56.2474 - val_loss: 55.6623 - val_MinusLogProbMetric: 55.6623 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 3/1000
2023-10-28 10:13:07.956 
Epoch 3/1000 
	 loss: 60.3684, MinusLogProbMetric: 60.3684, val_loss: 54.6743, val_MinusLogProbMetric: 54.6743

Epoch 3: val_loss improved from 55.66235 to 54.67430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 60.3684 - MinusLogProbMetric: 60.3684 - val_loss: 54.6743 - val_MinusLogProbMetric: 54.6743 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 4/1000
2023-10-28 10:13:49.341 
Epoch 4/1000 
	 loss: 54.9599, MinusLogProbMetric: 54.9599, val_loss: 56.6956, val_MinusLogProbMetric: 56.6956

Epoch 4: val_loss did not improve from 54.67430
196/196 - 41s - loss: 54.9599 - MinusLogProbMetric: 54.9599 - val_loss: 56.6956 - val_MinusLogProbMetric: 56.6956 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 5/1000
2023-10-28 10:14:30.159 
Epoch 5/1000 
	 loss: 95.8175, MinusLogProbMetric: 95.8175, val_loss: 73.6608, val_MinusLogProbMetric: 73.6608

Epoch 5: val_loss did not improve from 54.67430
196/196 - 41s - loss: 95.8175 - MinusLogProbMetric: 95.8175 - val_loss: 73.6608 - val_MinusLogProbMetric: 73.6608 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 6/1000
2023-10-28 10:15:11.179 
Epoch 6/1000 
	 loss: 65.6549, MinusLogProbMetric: 65.6549, val_loss: 61.0719, val_MinusLogProbMetric: 61.0719

Epoch 6: val_loss did not improve from 54.67430
196/196 - 41s - loss: 65.6549 - MinusLogProbMetric: 65.6549 - val_loss: 61.0719 - val_MinusLogProbMetric: 61.0719 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 7/1000
2023-10-28 10:15:52.561 
Epoch 7/1000 
	 loss: 59.2066, MinusLogProbMetric: 59.2066, val_loss: 59.6107, val_MinusLogProbMetric: 59.6107

Epoch 7: val_loss did not improve from 54.67430
196/196 - 41s - loss: 59.2066 - MinusLogProbMetric: 59.2066 - val_loss: 59.6107 - val_MinusLogProbMetric: 59.6107 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 8/1000
2023-10-28 10:16:33.652 
Epoch 8/1000 
	 loss: 57.2893, MinusLogProbMetric: 57.2893, val_loss: 57.2654, val_MinusLogProbMetric: 57.2654

Epoch 8: val_loss did not improve from 54.67430
196/196 - 41s - loss: 57.2893 - MinusLogProbMetric: 57.2893 - val_loss: 57.2654 - val_MinusLogProbMetric: 57.2654 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 9/1000
2023-10-28 10:17:13.001 
Epoch 9/1000 
	 loss: 55.9808, MinusLogProbMetric: 55.9808, val_loss: 55.5640, val_MinusLogProbMetric: 55.5640

Epoch 9: val_loss did not improve from 54.67430
196/196 - 39s - loss: 55.9808 - MinusLogProbMetric: 55.9808 - val_loss: 55.5640 - val_MinusLogProbMetric: 55.5640 - lr: 1.1111e-04 - 39s/epoch - 201ms/step
Epoch 10/1000
2023-10-28 10:17:54.925 
Epoch 10/1000 
	 loss: 55.2367, MinusLogProbMetric: 55.2367, val_loss: 55.5405, val_MinusLogProbMetric: 55.5405

Epoch 10: val_loss did not improve from 54.67430
196/196 - 42s - loss: 55.2367 - MinusLogProbMetric: 55.2367 - val_loss: 55.5405 - val_MinusLogProbMetric: 55.5405 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 11/1000
2023-10-28 10:18:36.002 
Epoch 11/1000 
	 loss: 54.6543, MinusLogProbMetric: 54.6543, val_loss: 55.5988, val_MinusLogProbMetric: 55.5988

Epoch 11: val_loss did not improve from 54.67430
196/196 - 41s - loss: 54.6543 - MinusLogProbMetric: 54.6543 - val_loss: 55.5988 - val_MinusLogProbMetric: 55.5988 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 12/1000
2023-10-28 10:19:17.033 
Epoch 12/1000 
	 loss: 73.3888, MinusLogProbMetric: 73.3888, val_loss: 57.3771, val_MinusLogProbMetric: 57.3771

Epoch 12: val_loss did not improve from 54.67430
196/196 - 41s - loss: 73.3888 - MinusLogProbMetric: 73.3888 - val_loss: 57.3771 - val_MinusLogProbMetric: 57.3771 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 13/1000
2023-10-28 10:19:57.701 
Epoch 13/1000 
	 loss: 55.6608, MinusLogProbMetric: 55.6608, val_loss: 55.9086, val_MinusLogProbMetric: 55.9086

Epoch 13: val_loss did not improve from 54.67430
196/196 - 41s - loss: 55.6608 - MinusLogProbMetric: 55.6608 - val_loss: 55.9086 - val_MinusLogProbMetric: 55.9086 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 14/1000
2023-10-28 10:20:38.699 
Epoch 14/1000 
	 loss: 54.2032, MinusLogProbMetric: 54.2032, val_loss: 53.7281, val_MinusLogProbMetric: 53.7281

Epoch 14: val_loss improved from 54.67430 to 53.72805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 54.2032 - MinusLogProbMetric: 54.2032 - val_loss: 53.7281 - val_MinusLogProbMetric: 53.7281 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 15/1000
2023-10-28 10:21:20.685 
Epoch 15/1000 
	 loss: 56.3232, MinusLogProbMetric: 56.3232, val_loss: 54.1454, val_MinusLogProbMetric: 54.1454

Epoch 15: val_loss did not improve from 53.72805
196/196 - 41s - loss: 56.3232 - MinusLogProbMetric: 56.3232 - val_loss: 54.1454 - val_MinusLogProbMetric: 54.1454 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 16/1000
2023-10-28 10:22:02.106 
Epoch 16/1000 
	 loss: 53.2267, MinusLogProbMetric: 53.2267, val_loss: 53.3282, val_MinusLogProbMetric: 53.3282

Epoch 16: val_loss improved from 53.72805 to 53.32823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 53.2267 - MinusLogProbMetric: 53.2267 - val_loss: 53.3282 - val_MinusLogProbMetric: 53.3282 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 17/1000
2023-10-28 10:22:42.712 
Epoch 17/1000 
	 loss: 52.6734, MinusLogProbMetric: 52.6734, val_loss: 54.3025, val_MinusLogProbMetric: 54.3025

Epoch 17: val_loss did not improve from 53.32823
196/196 - 40s - loss: 52.6734 - MinusLogProbMetric: 52.6734 - val_loss: 54.3025 - val_MinusLogProbMetric: 54.3025 - lr: 1.1111e-04 - 40s/epoch - 203ms/step
Epoch 18/1000
2023-10-28 10:23:24.045 
Epoch 18/1000 
	 loss: 51.8857, MinusLogProbMetric: 51.8857, val_loss: 53.7547, val_MinusLogProbMetric: 53.7547

Epoch 18: val_loss did not improve from 53.32823
196/196 - 41s - loss: 51.8857 - MinusLogProbMetric: 51.8857 - val_loss: 53.7547 - val_MinusLogProbMetric: 53.7547 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 19/1000
2023-10-28 10:24:06.098 
Epoch 19/1000 
	 loss: 52.1894, MinusLogProbMetric: 52.1894, val_loss: 53.0480, val_MinusLogProbMetric: 53.0480

Epoch 19: val_loss improved from 53.32823 to 53.04799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 43s - loss: 52.1894 - MinusLogProbMetric: 52.1894 - val_loss: 53.0480 - val_MinusLogProbMetric: 53.0480 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 20/1000
2023-10-28 10:24:47.687 
Epoch 20/1000 
	 loss: 51.8263, MinusLogProbMetric: 51.8263, val_loss: 53.4564, val_MinusLogProbMetric: 53.4564

Epoch 20: val_loss did not improve from 53.04799
196/196 - 41s - loss: 51.8263 - MinusLogProbMetric: 51.8263 - val_loss: 53.4564 - val_MinusLogProbMetric: 53.4564 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 21/1000
2023-10-28 10:25:28.980 
Epoch 21/1000 
	 loss: 52.1773, MinusLogProbMetric: 52.1773, val_loss: 51.6107, val_MinusLogProbMetric: 51.6107

Epoch 21: val_loss improved from 53.04799 to 51.61069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 52.1773 - MinusLogProbMetric: 52.1773 - val_loss: 51.6107 - val_MinusLogProbMetric: 51.6107 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 22/1000
2023-10-28 10:26:11.027 
Epoch 22/1000 
	 loss: 51.5252, MinusLogProbMetric: 51.5252, val_loss: 54.1738, val_MinusLogProbMetric: 54.1738

Epoch 22: val_loss did not improve from 51.61069
196/196 - 41s - loss: 51.5252 - MinusLogProbMetric: 51.5252 - val_loss: 54.1738 - val_MinusLogProbMetric: 54.1738 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 23/1000
2023-10-28 10:26:53.223 
Epoch 23/1000 
	 loss: 52.2440, MinusLogProbMetric: 52.2440, val_loss: 51.4231, val_MinusLogProbMetric: 51.4231

Epoch 23: val_loss improved from 51.61069 to 51.42310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 43s - loss: 52.2440 - MinusLogProbMetric: 52.2440 - val_loss: 51.4231 - val_MinusLogProbMetric: 51.4231 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 24/1000
2023-10-28 10:27:34.462 
Epoch 24/1000 
	 loss: 50.6427, MinusLogProbMetric: 50.6427, val_loss: 50.9601, val_MinusLogProbMetric: 50.9601

Epoch 24: val_loss improved from 51.42310 to 50.96008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 50.6427 - MinusLogProbMetric: 50.6427 - val_loss: 50.9601 - val_MinusLogProbMetric: 50.9601 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 25/1000
2023-10-28 10:28:16.399 
Epoch 25/1000 
	 loss: 50.7862, MinusLogProbMetric: 50.7862, val_loss: 50.2970, val_MinusLogProbMetric: 50.2970

Epoch 25: val_loss improved from 50.96008 to 50.29704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 50.7862 - MinusLogProbMetric: 50.7862 - val_loss: 50.2970 - val_MinusLogProbMetric: 50.2970 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 26/1000
2023-10-28 10:28:58.664 
Epoch 26/1000 
	 loss: 50.3784, MinusLogProbMetric: 50.3784, val_loss: 50.7142, val_MinusLogProbMetric: 50.7142

Epoch 26: val_loss did not improve from 50.29704
196/196 - 42s - loss: 50.3784 - MinusLogProbMetric: 50.3784 - val_loss: 50.7142 - val_MinusLogProbMetric: 50.7142 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 27/1000
2023-10-28 10:29:39.225 
Epoch 27/1000 
	 loss: 52.2666, MinusLogProbMetric: 52.2666, val_loss: 49.9148, val_MinusLogProbMetric: 49.9148

Epoch 27: val_loss improved from 50.29704 to 49.91482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 52.2666 - MinusLogProbMetric: 52.2666 - val_loss: 49.9148 - val_MinusLogProbMetric: 49.9148 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 28/1000
2023-10-28 10:30:20.329 
Epoch 28/1000 
	 loss: 50.1805, MinusLogProbMetric: 50.1805, val_loss: 54.0966, val_MinusLogProbMetric: 54.0966

Epoch 28: val_loss did not improve from 49.91482
196/196 - 40s - loss: 50.1805 - MinusLogProbMetric: 50.1805 - val_loss: 54.0966 - val_MinusLogProbMetric: 54.0966 - lr: 1.1111e-04 - 40s/epoch - 205ms/step
Epoch 29/1000
2023-10-28 10:31:01.988 
Epoch 29/1000 
	 loss: 50.4431, MinusLogProbMetric: 50.4431, val_loss: 49.0870, val_MinusLogProbMetric: 49.0870

Epoch 29: val_loss improved from 49.91482 to 49.08699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 50.4431 - MinusLogProbMetric: 50.4431 - val_loss: 49.0870 - val_MinusLogProbMetric: 49.0870 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 30/1000
2023-10-28 10:31:44.224 
Epoch 30/1000 
	 loss: 50.8502, MinusLogProbMetric: 50.8502, val_loss: 49.2594, val_MinusLogProbMetric: 49.2594

Epoch 30: val_loss did not improve from 49.08699
196/196 - 41s - loss: 50.8502 - MinusLogProbMetric: 50.8502 - val_loss: 49.2594 - val_MinusLogProbMetric: 49.2594 - lr: 1.1111e-04 - 41s/epoch - 212ms/step
Epoch 31/1000
2023-10-28 10:32:25.724 
Epoch 31/1000 
	 loss: 50.2724, MinusLogProbMetric: 50.2724, val_loss: 49.3978, val_MinusLogProbMetric: 49.3978

Epoch 31: val_loss did not improve from 49.08699
196/196 - 41s - loss: 50.2724 - MinusLogProbMetric: 50.2724 - val_loss: 49.3978 - val_MinusLogProbMetric: 49.3978 - lr: 1.1111e-04 - 41s/epoch - 212ms/step
Epoch 32/1000
2023-10-28 10:33:07.100 
Epoch 32/1000 
	 loss: 49.9501, MinusLogProbMetric: 49.9501, val_loss: 49.2618, val_MinusLogProbMetric: 49.2618

Epoch 32: val_loss did not improve from 49.08699
196/196 - 41s - loss: 49.9501 - MinusLogProbMetric: 49.9501 - val_loss: 49.2618 - val_MinusLogProbMetric: 49.2618 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 33/1000
2023-10-28 10:33:48.955 
Epoch 33/1000 
	 loss: 51.5213, MinusLogProbMetric: 51.5213, val_loss: 49.6382, val_MinusLogProbMetric: 49.6382

Epoch 33: val_loss did not improve from 49.08699
196/196 - 42s - loss: 51.5213 - MinusLogProbMetric: 51.5213 - val_loss: 49.6382 - val_MinusLogProbMetric: 49.6382 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 34/1000
2023-10-28 10:34:30.441 
Epoch 34/1000 
	 loss: 48.8456, MinusLogProbMetric: 48.8456, val_loss: 55.2112, val_MinusLogProbMetric: 55.2112

Epoch 34: val_loss did not improve from 49.08699
196/196 - 41s - loss: 48.8456 - MinusLogProbMetric: 48.8456 - val_loss: 55.2112 - val_MinusLogProbMetric: 55.2112 - lr: 1.1111e-04 - 41s/epoch - 212ms/step
Epoch 35/1000
2023-10-28 10:35:13.447 
Epoch 35/1000 
	 loss: 49.4718, MinusLogProbMetric: 49.4718, val_loss: 49.5294, val_MinusLogProbMetric: 49.5294

Epoch 35: val_loss did not improve from 49.08699
196/196 - 43s - loss: 49.4718 - MinusLogProbMetric: 49.4718 - val_loss: 49.5294 - val_MinusLogProbMetric: 49.5294 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 36/1000
2023-10-28 10:35:54.874 
Epoch 36/1000 
	 loss: 50.1276, MinusLogProbMetric: 50.1276, val_loss: 49.2015, val_MinusLogProbMetric: 49.2015

Epoch 36: val_loss did not improve from 49.08699
196/196 - 41s - loss: 50.1276 - MinusLogProbMetric: 50.1276 - val_loss: 49.2015 - val_MinusLogProbMetric: 49.2015 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 37/1000
2023-10-28 10:36:36.478 
Epoch 37/1000 
	 loss: 49.2206, MinusLogProbMetric: 49.2206, val_loss: 48.3538, val_MinusLogProbMetric: 48.3538

Epoch 37: val_loss improved from 49.08699 to 48.35382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 49.2206 - MinusLogProbMetric: 49.2206 - val_loss: 48.3538 - val_MinusLogProbMetric: 48.3538 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 38/1000
2023-10-28 10:37:19.461 
Epoch 38/1000 
	 loss: 49.0434, MinusLogProbMetric: 49.0434, val_loss: 48.3137, val_MinusLogProbMetric: 48.3137

Epoch 38: val_loss improved from 48.35382 to 48.31375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 43s - loss: 49.0434 - MinusLogProbMetric: 49.0434 - val_loss: 48.3137 - val_MinusLogProbMetric: 48.3137 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 39/1000
2023-10-28 10:38:01.331 
Epoch 39/1000 
	 loss: 49.9279, MinusLogProbMetric: 49.9279, val_loss: 48.5538, val_MinusLogProbMetric: 48.5538

Epoch 39: val_loss did not improve from 48.31375
196/196 - 41s - loss: 49.9279 - MinusLogProbMetric: 49.9279 - val_loss: 48.5538 - val_MinusLogProbMetric: 48.5538 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 40/1000
2023-10-28 10:38:42.414 
Epoch 40/1000 
	 loss: 49.5742, MinusLogProbMetric: 49.5742, val_loss: 48.6474, val_MinusLogProbMetric: 48.6474

Epoch 40: val_loss did not improve from 48.31375
196/196 - 41s - loss: 49.5742 - MinusLogProbMetric: 49.5742 - val_loss: 48.6474 - val_MinusLogProbMetric: 48.6474 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 41/1000
2023-10-28 10:39:23.149 
Epoch 41/1000 
	 loss: 48.7133, MinusLogProbMetric: 48.7133, val_loss: 48.3419, val_MinusLogProbMetric: 48.3419

Epoch 41: val_loss did not improve from 48.31375
196/196 - 41s - loss: 48.7133 - MinusLogProbMetric: 48.7133 - val_loss: 48.3419 - val_MinusLogProbMetric: 48.3419 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 42/1000
2023-10-28 10:40:03.152 
Epoch 42/1000 
	 loss: 48.5853, MinusLogProbMetric: 48.5853, val_loss: 48.7343, val_MinusLogProbMetric: 48.7343

Epoch 42: val_loss did not improve from 48.31375
196/196 - 40s - loss: 48.5853 - MinusLogProbMetric: 48.5853 - val_loss: 48.7343 - val_MinusLogProbMetric: 48.7343 - lr: 1.1111e-04 - 40s/epoch - 204ms/step
Epoch 43/1000
2023-10-28 10:40:41.699 
Epoch 43/1000 
	 loss: 48.5049, MinusLogProbMetric: 48.5049, val_loss: 48.0082, val_MinusLogProbMetric: 48.0082

Epoch 43: val_loss improved from 48.31375 to 48.00817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 39s - loss: 48.5049 - MinusLogProbMetric: 48.5049 - val_loss: 48.0082 - val_MinusLogProbMetric: 48.0082 - lr: 1.1111e-04 - 39s/epoch - 201ms/step
Epoch 44/1000
2023-10-28 10:41:23.004 
Epoch 44/1000 
	 loss: 48.5211, MinusLogProbMetric: 48.5211, val_loss: 48.1142, val_MinusLogProbMetric: 48.1142

Epoch 44: val_loss did not improve from 48.00817
196/196 - 41s - loss: 48.5211 - MinusLogProbMetric: 48.5211 - val_loss: 48.1142 - val_MinusLogProbMetric: 48.1142 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 45/1000
2023-10-28 10:42:03.534 
Epoch 45/1000 
	 loss: 48.7437, MinusLogProbMetric: 48.7437, val_loss: 48.5484, val_MinusLogProbMetric: 48.5484

Epoch 45: val_loss did not improve from 48.00817
196/196 - 41s - loss: 48.7437 - MinusLogProbMetric: 48.7437 - val_loss: 48.5484 - val_MinusLogProbMetric: 48.5484 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 46/1000
2023-10-28 10:42:44.243 
Epoch 46/1000 
	 loss: 48.4320, MinusLogProbMetric: 48.4320, val_loss: 47.2500, val_MinusLogProbMetric: 47.2500

Epoch 46: val_loss improved from 48.00817 to 47.25005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 48.4320 - MinusLogProbMetric: 48.4320 - val_loss: 47.2500 - val_MinusLogProbMetric: 47.2500 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 47/1000
2023-10-28 10:43:25.684 
Epoch 47/1000 
	 loss: 48.4240, MinusLogProbMetric: 48.4240, val_loss: 49.5683, val_MinusLogProbMetric: 49.5683

Epoch 47: val_loss did not improve from 47.25005
196/196 - 41s - loss: 48.4240 - MinusLogProbMetric: 48.4240 - val_loss: 49.5683 - val_MinusLogProbMetric: 49.5683 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 48/1000
2023-10-28 10:44:07.689 
Epoch 48/1000 
	 loss: 48.6643, MinusLogProbMetric: 48.6643, val_loss: 49.4304, val_MinusLogProbMetric: 49.4304

Epoch 48: val_loss did not improve from 47.25005
196/196 - 42s - loss: 48.6643 - MinusLogProbMetric: 48.6643 - val_loss: 49.4304 - val_MinusLogProbMetric: 49.4304 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 49/1000
2023-10-28 10:44:49.346 
Epoch 49/1000 
	 loss: 54.5661, MinusLogProbMetric: 54.5661, val_loss: 48.5687, val_MinusLogProbMetric: 48.5687

Epoch 49: val_loss did not improve from 47.25005
196/196 - 42s - loss: 54.5661 - MinusLogProbMetric: 54.5661 - val_loss: 48.5687 - val_MinusLogProbMetric: 48.5687 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 50/1000
2023-10-28 10:45:30.698 
Epoch 50/1000 
	 loss: 47.7460, MinusLogProbMetric: 47.7460, val_loss: 47.9774, val_MinusLogProbMetric: 47.9774

Epoch 50: val_loss did not improve from 47.25005
196/196 - 41s - loss: 47.7460 - MinusLogProbMetric: 47.7460 - val_loss: 47.9774 - val_MinusLogProbMetric: 47.9774 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 51/1000
2023-10-28 10:46:12.286 
Epoch 51/1000 
	 loss: 48.6946, MinusLogProbMetric: 48.6946, val_loss: 47.6330, val_MinusLogProbMetric: 47.6330

Epoch 51: val_loss did not improve from 47.25005
196/196 - 42s - loss: 48.6946 - MinusLogProbMetric: 48.6946 - val_loss: 47.6330 - val_MinusLogProbMetric: 47.6330 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 52/1000
2023-10-28 10:46:53.608 
Epoch 52/1000 
	 loss: 47.7058, MinusLogProbMetric: 47.7058, val_loss: 53.4395, val_MinusLogProbMetric: 53.4395

Epoch 52: val_loss did not improve from 47.25005
196/196 - 41s - loss: 47.7058 - MinusLogProbMetric: 47.7058 - val_loss: 53.4395 - val_MinusLogProbMetric: 53.4395 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 53/1000
2023-10-28 10:47:34.244 
Epoch 53/1000 
	 loss: 47.9056, MinusLogProbMetric: 47.9056, val_loss: 47.7012, val_MinusLogProbMetric: 47.7012

Epoch 53: val_loss did not improve from 47.25005
196/196 - 41s - loss: 47.9056 - MinusLogProbMetric: 47.9056 - val_loss: 47.7012 - val_MinusLogProbMetric: 47.7012 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 54/1000
2023-10-28 10:48:15.565 
Epoch 54/1000 
	 loss: 47.8696, MinusLogProbMetric: 47.8696, val_loss: 48.3879, val_MinusLogProbMetric: 48.3879

Epoch 54: val_loss did not improve from 47.25005
196/196 - 41s - loss: 47.8696 - MinusLogProbMetric: 47.8696 - val_loss: 48.3879 - val_MinusLogProbMetric: 48.3879 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 55/1000
2023-10-28 10:48:54.907 
Epoch 55/1000 
	 loss: 47.3027, MinusLogProbMetric: 47.3027, val_loss: 47.9018, val_MinusLogProbMetric: 47.9018

Epoch 55: val_loss did not improve from 47.25005
196/196 - 39s - loss: 47.3027 - MinusLogProbMetric: 47.3027 - val_loss: 47.9018 - val_MinusLogProbMetric: 47.9018 - lr: 1.1111e-04 - 39s/epoch - 201ms/step
Epoch 56/1000
2023-10-28 10:49:33.733 
Epoch 56/1000 
	 loss: 47.4327, MinusLogProbMetric: 47.4327, val_loss: 47.9673, val_MinusLogProbMetric: 47.9673

Epoch 56: val_loss did not improve from 47.25005
196/196 - 39s - loss: 47.4327 - MinusLogProbMetric: 47.4327 - val_loss: 47.9673 - val_MinusLogProbMetric: 47.9673 - lr: 1.1111e-04 - 39s/epoch - 198ms/step
Epoch 57/1000
2023-10-28 10:50:13.999 
Epoch 57/1000 
	 loss: 47.7377, MinusLogProbMetric: 47.7377, val_loss: 47.8002, val_MinusLogProbMetric: 47.8002

Epoch 57: val_loss did not improve from 47.25005
196/196 - 40s - loss: 47.7377 - MinusLogProbMetric: 47.7377 - val_loss: 47.8002 - val_MinusLogProbMetric: 47.8002 - lr: 1.1111e-04 - 40s/epoch - 205ms/step
Epoch 58/1000
2023-10-28 10:50:54.320 
Epoch 58/1000 
	 loss: 47.7883, MinusLogProbMetric: 47.7883, val_loss: 48.4531, val_MinusLogProbMetric: 48.4531

Epoch 58: val_loss did not improve from 47.25005
196/196 - 40s - loss: 47.7883 - MinusLogProbMetric: 47.7883 - val_loss: 48.4531 - val_MinusLogProbMetric: 48.4531 - lr: 1.1111e-04 - 40s/epoch - 206ms/step
Epoch 59/1000
2023-10-28 10:51:35.230 
Epoch 59/1000 
	 loss: 47.0698, MinusLogProbMetric: 47.0698, val_loss: 48.6485, val_MinusLogProbMetric: 48.6485

Epoch 59: val_loss did not improve from 47.25005
196/196 - 41s - loss: 47.0698 - MinusLogProbMetric: 47.0698 - val_loss: 48.6485 - val_MinusLogProbMetric: 48.6485 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 60/1000
2023-10-28 10:52:16.034 
Epoch 60/1000 
	 loss: 46.9331, MinusLogProbMetric: 46.9331, val_loss: 47.9031, val_MinusLogProbMetric: 47.9031

Epoch 60: val_loss did not improve from 47.25005
196/196 - 41s - loss: 46.9331 - MinusLogProbMetric: 46.9331 - val_loss: 47.9031 - val_MinusLogProbMetric: 47.9031 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 61/1000
2023-10-28 10:52:56.815 
Epoch 61/1000 
	 loss: 47.3564, MinusLogProbMetric: 47.3564, val_loss: 48.9059, val_MinusLogProbMetric: 48.9059

Epoch 61: val_loss did not improve from 47.25005
196/196 - 41s - loss: 47.3564 - MinusLogProbMetric: 47.3564 - val_loss: 48.9059 - val_MinusLogProbMetric: 48.9059 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 62/1000
2023-10-28 10:53:38.683 
Epoch 62/1000 
	 loss: 47.1585, MinusLogProbMetric: 47.1585, val_loss: 59.1611, val_MinusLogProbMetric: 59.1611

Epoch 62: val_loss did not improve from 47.25005
196/196 - 42s - loss: 47.1585 - MinusLogProbMetric: 47.1585 - val_loss: 59.1611 - val_MinusLogProbMetric: 59.1611 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 63/1000
2023-10-28 10:54:21.411 
Epoch 63/1000 
	 loss: 48.2587, MinusLogProbMetric: 48.2587, val_loss: 50.6593, val_MinusLogProbMetric: 50.6593

Epoch 63: val_loss did not improve from 47.25005
196/196 - 43s - loss: 48.2587 - MinusLogProbMetric: 48.2587 - val_loss: 50.6593 - val_MinusLogProbMetric: 50.6593 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 64/1000
2023-10-28 10:55:02.661 
Epoch 64/1000 
	 loss: 47.5088, MinusLogProbMetric: 47.5088, val_loss: 49.7398, val_MinusLogProbMetric: 49.7398

Epoch 64: val_loss did not improve from 47.25005
196/196 - 41s - loss: 47.5088 - MinusLogProbMetric: 47.5088 - val_loss: 49.7398 - val_MinusLogProbMetric: 49.7398 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 65/1000
2023-10-28 10:55:44.719 
Epoch 65/1000 
	 loss: 46.9997, MinusLogProbMetric: 46.9997, val_loss: 48.6868, val_MinusLogProbMetric: 48.6868

Epoch 65: val_loss did not improve from 47.25005
196/196 - 42s - loss: 46.9997 - MinusLogProbMetric: 46.9997 - val_loss: 48.6868 - val_MinusLogProbMetric: 48.6868 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 66/1000
2023-10-28 10:56:27.081 
Epoch 66/1000 
	 loss: 47.3773, MinusLogProbMetric: 47.3773, val_loss: 50.0343, val_MinusLogProbMetric: 50.0343

Epoch 66: val_loss did not improve from 47.25005
196/196 - 42s - loss: 47.3773 - MinusLogProbMetric: 47.3773 - val_loss: 50.0343 - val_MinusLogProbMetric: 50.0343 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 67/1000
2023-10-28 10:57:08.318 
Epoch 67/1000 
	 loss: 47.8428, MinusLogProbMetric: 47.8428, val_loss: 46.9830, val_MinusLogProbMetric: 46.9830

Epoch 67: val_loss improved from 47.25005 to 46.98299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 47.8428 - MinusLogProbMetric: 47.8428 - val_loss: 46.9830 - val_MinusLogProbMetric: 46.9830 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 68/1000
2023-10-28 10:57:51.197 
Epoch 68/1000 
	 loss: 46.5530, MinusLogProbMetric: 46.5530, val_loss: 48.1209, val_MinusLogProbMetric: 48.1209

Epoch 68: val_loss did not improve from 46.98299
196/196 - 42s - loss: 46.5530 - MinusLogProbMetric: 46.5530 - val_loss: 48.1209 - val_MinusLogProbMetric: 48.1209 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 69/1000
2023-10-28 10:58:32.721 
Epoch 69/1000 
	 loss: 47.0335, MinusLogProbMetric: 47.0335, val_loss: 47.2408, val_MinusLogProbMetric: 47.2408

Epoch 69: val_loss did not improve from 46.98299
196/196 - 42s - loss: 47.0335 - MinusLogProbMetric: 47.0335 - val_loss: 47.2408 - val_MinusLogProbMetric: 47.2408 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 70/1000
2023-10-28 10:59:03.899 
Epoch 70/1000 
	 loss: 46.8865, MinusLogProbMetric: 46.8865, val_loss: 54.6926, val_MinusLogProbMetric: 54.6926

Epoch 70: val_loss did not improve from 46.98299
196/196 - 31s - loss: 46.8865 - MinusLogProbMetric: 46.8865 - val_loss: 54.6926 - val_MinusLogProbMetric: 54.6926 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 71/1000
2023-10-28 10:59:35.993 
Epoch 71/1000 
	 loss: 46.8768, MinusLogProbMetric: 46.8768, val_loss: 47.8009, val_MinusLogProbMetric: 47.8009

Epoch 71: val_loss did not improve from 46.98299
196/196 - 32s - loss: 46.8768 - MinusLogProbMetric: 46.8768 - val_loss: 47.8009 - val_MinusLogProbMetric: 47.8009 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 72/1000
2023-10-28 11:00:09.570 
Epoch 72/1000 
	 loss: 46.7050, MinusLogProbMetric: 46.7050, val_loss: 46.9511, val_MinusLogProbMetric: 46.9511

Epoch 72: val_loss improved from 46.98299 to 46.95109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 35s - loss: 46.7050 - MinusLogProbMetric: 46.7050 - val_loss: 46.9511 - val_MinusLogProbMetric: 46.9511 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 73/1000
2023-10-28 11:00:49.142 
Epoch 73/1000 
	 loss: 46.3067, MinusLogProbMetric: 46.3067, val_loss: 46.8008, val_MinusLogProbMetric: 46.8008

Epoch 73: val_loss improved from 46.95109 to 46.80081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 39s - loss: 46.3067 - MinusLogProbMetric: 46.3067 - val_loss: 46.8008 - val_MinusLogProbMetric: 46.8008 - lr: 1.1111e-04 - 39s/epoch - 201ms/step
Epoch 74/1000
2023-10-28 11:01:32.609 
Epoch 74/1000 
	 loss: 46.7361, MinusLogProbMetric: 46.7361, val_loss: 46.8944, val_MinusLogProbMetric: 46.8944

Epoch 74: val_loss did not improve from 46.80081
196/196 - 43s - loss: 46.7361 - MinusLogProbMetric: 46.7361 - val_loss: 46.8944 - val_MinusLogProbMetric: 46.8944 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 75/1000
2023-10-28 11:02:15.090 
Epoch 75/1000 
	 loss: 46.9056, MinusLogProbMetric: 46.9056, val_loss: 49.8058, val_MinusLogProbMetric: 49.8058

Epoch 75: val_loss did not improve from 46.80081
196/196 - 42s - loss: 46.9056 - MinusLogProbMetric: 46.9056 - val_loss: 49.8058 - val_MinusLogProbMetric: 49.8058 - lr: 1.1111e-04 - 42s/epoch - 217ms/step
Epoch 76/1000
2023-10-28 11:02:56.502 
Epoch 76/1000 
	 loss: 47.3275, MinusLogProbMetric: 47.3275, val_loss: 46.9872, val_MinusLogProbMetric: 46.9872

Epoch 76: val_loss did not improve from 46.80081
196/196 - 41s - loss: 47.3275 - MinusLogProbMetric: 47.3275 - val_loss: 46.9872 - val_MinusLogProbMetric: 46.9872 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 77/1000
2023-10-28 11:03:37.949 
Epoch 77/1000 
	 loss: 46.3363, MinusLogProbMetric: 46.3363, val_loss: 45.9526, val_MinusLogProbMetric: 45.9526

Epoch 77: val_loss improved from 46.80081 to 45.95258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 43s - loss: 46.3363 - MinusLogProbMetric: 46.3363 - val_loss: 45.9526 - val_MinusLogProbMetric: 45.9526 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 78/1000
2023-10-28 11:04:20.459 
Epoch 78/1000 
	 loss: 46.5723, MinusLogProbMetric: 46.5723, val_loss: 46.3611, val_MinusLogProbMetric: 46.3611

Epoch 78: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.5723 - MinusLogProbMetric: 46.5723 - val_loss: 46.3611 - val_MinusLogProbMetric: 46.3611 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 79/1000
2023-10-28 11:05:02.439 
Epoch 79/1000 
	 loss: 45.8868, MinusLogProbMetric: 45.8868, val_loss: 47.0192, val_MinusLogProbMetric: 47.0192

Epoch 79: val_loss did not improve from 45.95258
196/196 - 42s - loss: 45.8868 - MinusLogProbMetric: 45.8868 - val_loss: 47.0192 - val_MinusLogProbMetric: 47.0192 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 80/1000
2023-10-28 11:05:43.413 
Epoch 80/1000 
	 loss: 46.2328, MinusLogProbMetric: 46.2328, val_loss: 47.2716, val_MinusLogProbMetric: 47.2716

Epoch 80: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.2328 - MinusLogProbMetric: 46.2328 - val_loss: 47.2716 - val_MinusLogProbMetric: 47.2716 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 81/1000
2023-10-28 11:06:24.505 
Epoch 81/1000 
	 loss: 47.1648, MinusLogProbMetric: 47.1648, val_loss: 46.8419, val_MinusLogProbMetric: 46.8419

Epoch 81: val_loss did not improve from 45.95258
196/196 - 41s - loss: 47.1648 - MinusLogProbMetric: 47.1648 - val_loss: 46.8419 - val_MinusLogProbMetric: 46.8419 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 82/1000
2023-10-28 11:07:06.780 
Epoch 82/1000 
	 loss: 46.3268, MinusLogProbMetric: 46.3268, val_loss: 46.6176, val_MinusLogProbMetric: 46.6176

Epoch 82: val_loss did not improve from 45.95258
196/196 - 42s - loss: 46.3268 - MinusLogProbMetric: 46.3268 - val_loss: 46.6176 - val_MinusLogProbMetric: 46.6176 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 83/1000
2023-10-28 11:07:48.226 
Epoch 83/1000 
	 loss: 46.2508, MinusLogProbMetric: 46.2508, val_loss: 47.6507, val_MinusLogProbMetric: 47.6507

Epoch 83: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.2508 - MinusLogProbMetric: 46.2508 - val_loss: 47.6507 - val_MinusLogProbMetric: 47.6507 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 84/1000
2023-10-28 11:08:29.096 
Epoch 84/1000 
	 loss: 46.5360, MinusLogProbMetric: 46.5360, val_loss: 47.2451, val_MinusLogProbMetric: 47.2451

Epoch 84: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.5360 - MinusLogProbMetric: 46.5360 - val_loss: 47.2451 - val_MinusLogProbMetric: 47.2451 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 85/1000
2023-10-28 11:09:10.494 
Epoch 85/1000 
	 loss: 46.0338, MinusLogProbMetric: 46.0338, val_loss: 46.6115, val_MinusLogProbMetric: 46.6115

Epoch 85: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.0338 - MinusLogProbMetric: 46.0338 - val_loss: 46.6115 - val_MinusLogProbMetric: 46.6115 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 86/1000
2023-10-28 11:09:51.074 
Epoch 86/1000 
	 loss: 46.1606, MinusLogProbMetric: 46.1606, val_loss: 45.9603, val_MinusLogProbMetric: 45.9603

Epoch 86: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.1606 - MinusLogProbMetric: 46.1606 - val_loss: 45.9603 - val_MinusLogProbMetric: 45.9603 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 87/1000
2023-10-28 11:10:32.392 
Epoch 87/1000 
	 loss: 45.6414, MinusLogProbMetric: 45.6414, val_loss: 46.9077, val_MinusLogProbMetric: 46.9077

Epoch 87: val_loss did not improve from 45.95258
196/196 - 41s - loss: 45.6414 - MinusLogProbMetric: 45.6414 - val_loss: 46.9077 - val_MinusLogProbMetric: 46.9077 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 88/1000
2023-10-28 11:11:13.306 
Epoch 88/1000 
	 loss: 46.0785, MinusLogProbMetric: 46.0785, val_loss: 46.0352, val_MinusLogProbMetric: 46.0352

Epoch 88: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.0785 - MinusLogProbMetric: 46.0785 - val_loss: 46.0352 - val_MinusLogProbMetric: 46.0352 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 89/1000
2023-10-28 11:11:54.696 
Epoch 89/1000 
	 loss: 46.1352, MinusLogProbMetric: 46.1352, val_loss: 49.2963, val_MinusLogProbMetric: 49.2963

Epoch 89: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.1352 - MinusLogProbMetric: 46.1352 - val_loss: 49.2963 - val_MinusLogProbMetric: 49.2963 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 90/1000
2023-10-28 11:12:35.406 
Epoch 90/1000 
	 loss: 46.1801, MinusLogProbMetric: 46.1801, val_loss: 46.2439, val_MinusLogProbMetric: 46.2439

Epoch 90: val_loss did not improve from 45.95258
196/196 - 41s - loss: 46.1801 - MinusLogProbMetric: 46.1801 - val_loss: 46.2439 - val_MinusLogProbMetric: 46.2439 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 91/1000
2023-10-28 11:13:15.980 
Epoch 91/1000 
	 loss: 45.5209, MinusLogProbMetric: 45.5209, val_loss: 45.5651, val_MinusLogProbMetric: 45.5651

Epoch 91: val_loss improved from 45.95258 to 45.56511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 45.5209 - MinusLogProbMetric: 45.5209 - val_loss: 45.5651 - val_MinusLogProbMetric: 45.5651 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 92/1000
2023-10-28 11:13:57.119 
Epoch 92/1000 
	 loss: 46.4292, MinusLogProbMetric: 46.4292, val_loss: 46.4829, val_MinusLogProbMetric: 46.4829

Epoch 92: val_loss did not improve from 45.56511
196/196 - 40s - loss: 46.4292 - MinusLogProbMetric: 46.4292 - val_loss: 46.4829 - val_MinusLogProbMetric: 46.4829 - lr: 1.1111e-04 - 40s/epoch - 206ms/step
Epoch 93/1000
2023-10-28 11:14:37.853 
Epoch 93/1000 
	 loss: 47.2099, MinusLogProbMetric: 47.2099, val_loss: 47.4051, val_MinusLogProbMetric: 47.4051

Epoch 93: val_loss did not improve from 45.56511
196/196 - 41s - loss: 47.2099 - MinusLogProbMetric: 47.2099 - val_loss: 47.4051 - val_MinusLogProbMetric: 47.4051 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 94/1000
2023-10-28 11:15:19.446 
Epoch 94/1000 
	 loss: 45.2470, MinusLogProbMetric: 45.2470, val_loss: 46.9408, val_MinusLogProbMetric: 46.9408

Epoch 94: val_loss did not improve from 45.56511
196/196 - 42s - loss: 45.2470 - MinusLogProbMetric: 45.2470 - val_loss: 46.9408 - val_MinusLogProbMetric: 46.9408 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 95/1000
2023-10-28 11:15:59.478 
Epoch 95/1000 
	 loss: 45.6472, MinusLogProbMetric: 45.6472, val_loss: 46.5188, val_MinusLogProbMetric: 46.5188

Epoch 95: val_loss did not improve from 45.56511
196/196 - 40s - loss: 45.6472 - MinusLogProbMetric: 45.6472 - val_loss: 46.5188 - val_MinusLogProbMetric: 46.5188 - lr: 1.1111e-04 - 40s/epoch - 204ms/step
Epoch 96/1000
2023-10-28 11:16:40.179 
Epoch 96/1000 
	 loss: 45.4946, MinusLogProbMetric: 45.4946, val_loss: 46.0454, val_MinusLogProbMetric: 46.0454

Epoch 96: val_loss did not improve from 45.56511
196/196 - 41s - loss: 45.4946 - MinusLogProbMetric: 45.4946 - val_loss: 46.0454 - val_MinusLogProbMetric: 46.0454 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 97/1000
2023-10-28 11:17:20.408 
Epoch 97/1000 
	 loss: 45.6362, MinusLogProbMetric: 45.6362, val_loss: 45.4844, val_MinusLogProbMetric: 45.4844

Epoch 97: val_loss improved from 45.56511 to 45.48436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 45.6362 - MinusLogProbMetric: 45.6362 - val_loss: 45.4844 - val_MinusLogProbMetric: 45.4844 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 98/1000
2023-10-28 11:18:02.326 
Epoch 98/1000 
	 loss: 45.5472, MinusLogProbMetric: 45.5472, val_loss: 45.3470, val_MinusLogProbMetric: 45.3470

Epoch 98: val_loss improved from 45.48436 to 45.34702, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 45.5472 - MinusLogProbMetric: 45.5472 - val_loss: 45.3470 - val_MinusLogProbMetric: 45.3470 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 99/1000
2023-10-28 11:18:42.859 
Epoch 99/1000 
	 loss: 45.7532, MinusLogProbMetric: 45.7532, val_loss: 45.7228, val_MinusLogProbMetric: 45.7228

Epoch 99: val_loss did not improve from 45.34702
196/196 - 40s - loss: 45.7532 - MinusLogProbMetric: 45.7532 - val_loss: 45.7228 - val_MinusLogProbMetric: 45.7228 - lr: 1.1111e-04 - 40s/epoch - 203ms/step
Epoch 100/1000
2023-10-28 11:19:22.798 
Epoch 100/1000 
	 loss: 45.4428, MinusLogProbMetric: 45.4428, val_loss: 49.9370, val_MinusLogProbMetric: 49.9370

Epoch 100: val_loss did not improve from 45.34702
196/196 - 40s - loss: 45.4428 - MinusLogProbMetric: 45.4428 - val_loss: 49.9370 - val_MinusLogProbMetric: 49.9370 - lr: 1.1111e-04 - 40s/epoch - 204ms/step
Epoch 101/1000
2023-10-28 11:20:02.103 
Epoch 101/1000 
	 loss: 45.4965, MinusLogProbMetric: 45.4965, val_loss: 46.1762, val_MinusLogProbMetric: 46.1762

Epoch 101: val_loss did not improve from 45.34702
196/196 - 39s - loss: 45.4965 - MinusLogProbMetric: 45.4965 - val_loss: 46.1762 - val_MinusLogProbMetric: 46.1762 - lr: 1.1111e-04 - 39s/epoch - 201ms/step
Epoch 102/1000
2023-10-28 11:20:44.340 
Epoch 102/1000 
	 loss: 46.8578, MinusLogProbMetric: 46.8578, val_loss: 45.4659, val_MinusLogProbMetric: 45.4659

Epoch 102: val_loss did not improve from 45.34702
196/196 - 42s - loss: 46.8578 - MinusLogProbMetric: 46.8578 - val_loss: 45.4659 - val_MinusLogProbMetric: 45.4659 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 103/1000
2023-10-28 11:21:26.001 
Epoch 103/1000 
	 loss: 45.1901, MinusLogProbMetric: 45.1901, val_loss: 47.4180, val_MinusLogProbMetric: 47.4180

Epoch 103: val_loss did not improve from 45.34702
196/196 - 42s - loss: 45.1901 - MinusLogProbMetric: 45.1901 - val_loss: 47.4180 - val_MinusLogProbMetric: 47.4180 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 104/1000
2023-10-28 11:22:06.579 
Epoch 104/1000 
	 loss: 45.9224, MinusLogProbMetric: 45.9224, val_loss: 46.2829, val_MinusLogProbMetric: 46.2829

Epoch 104: val_loss did not improve from 45.34702
196/196 - 41s - loss: 45.9224 - MinusLogProbMetric: 45.9224 - val_loss: 46.2829 - val_MinusLogProbMetric: 46.2829 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 105/1000
2023-10-28 11:22:46.082 
Epoch 105/1000 
	 loss: 44.9243, MinusLogProbMetric: 44.9243, val_loss: 45.4897, val_MinusLogProbMetric: 45.4897

Epoch 105: val_loss did not improve from 45.34702
196/196 - 39s - loss: 44.9243 - MinusLogProbMetric: 44.9243 - val_loss: 45.4897 - val_MinusLogProbMetric: 45.4897 - lr: 1.1111e-04 - 39s/epoch - 202ms/step
Epoch 106/1000
2023-10-28 11:23:27.136 
Epoch 106/1000 
	 loss: 45.5513, MinusLogProbMetric: 45.5513, val_loss: 46.0340, val_MinusLogProbMetric: 46.0340

Epoch 106: val_loss did not improve from 45.34702
196/196 - 41s - loss: 45.5513 - MinusLogProbMetric: 45.5513 - val_loss: 46.0340 - val_MinusLogProbMetric: 46.0340 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 107/1000
2023-10-28 11:24:07.646 
Epoch 107/1000 
	 loss: 45.3527, MinusLogProbMetric: 45.3527, val_loss: 46.1346, val_MinusLogProbMetric: 46.1346

Epoch 107: val_loss did not improve from 45.34702
196/196 - 41s - loss: 45.3527 - MinusLogProbMetric: 45.3527 - val_loss: 46.1346 - val_MinusLogProbMetric: 46.1346 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 108/1000
2023-10-28 11:24:48.692 
Epoch 108/1000 
	 loss: 45.1181, MinusLogProbMetric: 45.1181, val_loss: 46.2598, val_MinusLogProbMetric: 46.2598

Epoch 108: val_loss did not improve from 45.34702
196/196 - 41s - loss: 45.1181 - MinusLogProbMetric: 45.1181 - val_loss: 46.2598 - val_MinusLogProbMetric: 46.2598 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 109/1000
2023-10-28 11:25:29.337 
Epoch 109/1000 
	 loss: 45.9072, MinusLogProbMetric: 45.9072, val_loss: 45.9494, val_MinusLogProbMetric: 45.9494

Epoch 109: val_loss did not improve from 45.34702
196/196 - 41s - loss: 45.9072 - MinusLogProbMetric: 45.9072 - val_loss: 45.9494 - val_MinusLogProbMetric: 45.9494 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 110/1000
2023-10-28 11:26:10.771 
Epoch 110/1000 
	 loss: 44.9074, MinusLogProbMetric: 44.9074, val_loss: 45.2382, val_MinusLogProbMetric: 45.2382

Epoch 110: val_loss improved from 45.34702 to 45.23819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 44.9074 - MinusLogProbMetric: 44.9074 - val_loss: 45.2382 - val_MinusLogProbMetric: 45.2382 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 111/1000
2023-10-28 11:26:51.712 
Epoch 111/1000 
	 loss: 45.9126, MinusLogProbMetric: 45.9126, val_loss: 46.9869, val_MinusLogProbMetric: 46.9869

Epoch 111: val_loss did not improve from 45.23819
196/196 - 40s - loss: 45.9126 - MinusLogProbMetric: 45.9126 - val_loss: 46.9869 - val_MinusLogProbMetric: 46.9869 - lr: 1.1111e-04 - 40s/epoch - 205ms/step
Epoch 112/1000
2023-10-28 11:27:32.427 
Epoch 112/1000 
	 loss: 44.7069, MinusLogProbMetric: 44.7069, val_loss: 45.6583, val_MinusLogProbMetric: 45.6583

Epoch 112: val_loss did not improve from 45.23819
196/196 - 41s - loss: 44.7069 - MinusLogProbMetric: 44.7069 - val_loss: 45.6583 - val_MinusLogProbMetric: 45.6583 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 113/1000
2023-10-28 11:28:13.279 
Epoch 113/1000 
	 loss: 45.0636, MinusLogProbMetric: 45.0636, val_loss: 45.7071, val_MinusLogProbMetric: 45.7071

Epoch 113: val_loss did not improve from 45.23819
196/196 - 41s - loss: 45.0636 - MinusLogProbMetric: 45.0636 - val_loss: 45.7071 - val_MinusLogProbMetric: 45.7071 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 114/1000
2023-10-28 11:28:52.882 
Epoch 114/1000 
	 loss: 45.0479, MinusLogProbMetric: 45.0479, val_loss: 45.8029, val_MinusLogProbMetric: 45.8029

Epoch 114: val_loss did not improve from 45.23819
196/196 - 40s - loss: 45.0479 - MinusLogProbMetric: 45.0479 - val_loss: 45.8029 - val_MinusLogProbMetric: 45.8029 - lr: 1.1111e-04 - 40s/epoch - 202ms/step
Epoch 115/1000
2023-10-28 11:29:32.562 
Epoch 115/1000 
	 loss: 44.9395, MinusLogProbMetric: 44.9395, val_loss: 45.4189, val_MinusLogProbMetric: 45.4189

Epoch 115: val_loss did not improve from 45.23819
196/196 - 40s - loss: 44.9395 - MinusLogProbMetric: 44.9395 - val_loss: 45.4189 - val_MinusLogProbMetric: 45.4189 - lr: 1.1111e-04 - 40s/epoch - 202ms/step
Epoch 116/1000
2023-10-28 11:30:10.942 
Epoch 116/1000 
	 loss: 44.8701, MinusLogProbMetric: 44.8701, val_loss: 46.4659, val_MinusLogProbMetric: 46.4659

Epoch 116: val_loss did not improve from 45.23819
196/196 - 38s - loss: 44.8701 - MinusLogProbMetric: 44.8701 - val_loss: 46.4659 - val_MinusLogProbMetric: 46.4659 - lr: 1.1111e-04 - 38s/epoch - 196ms/step
Epoch 117/1000
2023-10-28 11:30:49.264 
Epoch 117/1000 
	 loss: 45.7276, MinusLogProbMetric: 45.7276, val_loss: 46.5500, val_MinusLogProbMetric: 46.5500

Epoch 117: val_loss did not improve from 45.23819
196/196 - 38s - loss: 45.7276 - MinusLogProbMetric: 45.7276 - val_loss: 46.5500 - val_MinusLogProbMetric: 46.5500 - lr: 1.1111e-04 - 38s/epoch - 196ms/step
Epoch 118/1000
2023-10-28 11:31:27.883 
Epoch 118/1000 
	 loss: 44.8166, MinusLogProbMetric: 44.8166, val_loss: 45.6054, val_MinusLogProbMetric: 45.6054

Epoch 118: val_loss did not improve from 45.23819
196/196 - 39s - loss: 44.8166 - MinusLogProbMetric: 44.8166 - val_loss: 45.6054 - val_MinusLogProbMetric: 45.6054 - lr: 1.1111e-04 - 39s/epoch - 197ms/step
Epoch 119/1000
2023-10-28 11:32:08.584 
Epoch 119/1000 
	 loss: 45.0880, MinusLogProbMetric: 45.0880, val_loss: 45.5946, val_MinusLogProbMetric: 45.5946

Epoch 119: val_loss did not improve from 45.23819
196/196 - 41s - loss: 45.0880 - MinusLogProbMetric: 45.0880 - val_loss: 45.5946 - val_MinusLogProbMetric: 45.5946 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 120/1000
2023-10-28 11:32:49.273 
Epoch 120/1000 
	 loss: 45.7060, MinusLogProbMetric: 45.7060, val_loss: 46.0035, val_MinusLogProbMetric: 46.0035

Epoch 120: val_loss did not improve from 45.23819
196/196 - 41s - loss: 45.7060 - MinusLogProbMetric: 45.7060 - val_loss: 46.0035 - val_MinusLogProbMetric: 46.0035 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 121/1000
2023-10-28 11:33:29.840 
Epoch 121/1000 
	 loss: 45.2752, MinusLogProbMetric: 45.2752, val_loss: 46.1066, val_MinusLogProbMetric: 46.1066

Epoch 121: val_loss did not improve from 45.23819
196/196 - 41s - loss: 45.2752 - MinusLogProbMetric: 45.2752 - val_loss: 46.1066 - val_MinusLogProbMetric: 46.1066 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 122/1000
2023-10-28 11:34:10.464 
Epoch 122/1000 
	 loss: 44.6031, MinusLogProbMetric: 44.6031, val_loss: 46.4830, val_MinusLogProbMetric: 46.4830

Epoch 122: val_loss did not improve from 45.23819
196/196 - 41s - loss: 44.6031 - MinusLogProbMetric: 44.6031 - val_loss: 46.4830 - val_MinusLogProbMetric: 46.4830 - lr: 1.1111e-04 - 41s/epoch - 207ms/step
Epoch 123/1000
2023-10-28 11:34:51.364 
Epoch 123/1000 
	 loss: 44.8434, MinusLogProbMetric: 44.8434, val_loss: 52.8073, val_MinusLogProbMetric: 52.8073

Epoch 123: val_loss did not improve from 45.23819
196/196 - 41s - loss: 44.8434 - MinusLogProbMetric: 44.8434 - val_loss: 52.8073 - val_MinusLogProbMetric: 52.8073 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 124/1000
2023-10-28 11:35:32.291 
Epoch 124/1000 
	 loss: 45.0844, MinusLogProbMetric: 45.0844, val_loss: 47.2325, val_MinusLogProbMetric: 47.2325

Epoch 124: val_loss did not improve from 45.23819
196/196 - 41s - loss: 45.0844 - MinusLogProbMetric: 45.0844 - val_loss: 47.2325 - val_MinusLogProbMetric: 47.2325 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 125/1000
2023-10-28 11:36:13.851 
Epoch 125/1000 
	 loss: 45.3440, MinusLogProbMetric: 45.3440, val_loss: 46.8626, val_MinusLogProbMetric: 46.8626

Epoch 125: val_loss did not improve from 45.23819
196/196 - 42s - loss: 45.3440 - MinusLogProbMetric: 45.3440 - val_loss: 46.8626 - val_MinusLogProbMetric: 46.8626 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 126/1000
2023-10-28 11:36:48.071 
Epoch 126/1000 
	 loss: 45.0650, MinusLogProbMetric: 45.0650, val_loss: 45.4862, val_MinusLogProbMetric: 45.4862

Epoch 126: val_loss did not improve from 45.23819
196/196 - 34s - loss: 45.0650 - MinusLogProbMetric: 45.0650 - val_loss: 45.4862 - val_MinusLogProbMetric: 45.4862 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 127/1000
2023-10-28 11:37:23.119 
Epoch 127/1000 
	 loss: 44.4793, MinusLogProbMetric: 44.4793, val_loss: 45.1948, val_MinusLogProbMetric: 45.1948

Epoch 127: val_loss improved from 45.23819 to 45.19477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 36s - loss: 44.4793 - MinusLogProbMetric: 44.4793 - val_loss: 45.1948 - val_MinusLogProbMetric: 45.1948 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 128/1000
2023-10-28 11:38:03.037 
Epoch 128/1000 
	 loss: 45.8349, MinusLogProbMetric: 45.8349, val_loss: 45.9831, val_MinusLogProbMetric: 45.9831

Epoch 128: val_loss did not improve from 45.19477
196/196 - 39s - loss: 45.8349 - MinusLogProbMetric: 45.8349 - val_loss: 45.9831 - val_MinusLogProbMetric: 45.9831 - lr: 1.1111e-04 - 39s/epoch - 200ms/step
Epoch 129/1000
2023-10-28 11:38:38.575 
Epoch 129/1000 
	 loss: 44.7397, MinusLogProbMetric: 44.7397, val_loss: 45.6530, val_MinusLogProbMetric: 45.6530

Epoch 129: val_loss did not improve from 45.19477
196/196 - 36s - loss: 44.7397 - MinusLogProbMetric: 44.7397 - val_loss: 45.6530 - val_MinusLogProbMetric: 45.6530 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 130/1000
2023-10-28 11:39:13.262 
Epoch 130/1000 
	 loss: 44.8597, MinusLogProbMetric: 44.8597, val_loss: 44.6877, val_MinusLogProbMetric: 44.6877

Epoch 130: val_loss improved from 45.19477 to 44.68769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 35s - loss: 44.8597 - MinusLogProbMetric: 44.8597 - val_loss: 44.6877 - val_MinusLogProbMetric: 44.6877 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 131/1000
2023-10-28 11:39:50.801 
Epoch 131/1000 
	 loss: 44.3147, MinusLogProbMetric: 44.3147, val_loss: 45.4347, val_MinusLogProbMetric: 45.4347

Epoch 131: val_loss did not improve from 44.68769
196/196 - 37s - loss: 44.3147 - MinusLogProbMetric: 44.3147 - val_loss: 45.4347 - val_MinusLogProbMetric: 45.4347 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 132/1000
2023-10-28 11:40:27.089 
Epoch 132/1000 
	 loss: 44.9799, MinusLogProbMetric: 44.9799, val_loss: 48.7915, val_MinusLogProbMetric: 48.7915

Epoch 132: val_loss did not improve from 44.68769
196/196 - 36s - loss: 44.9799 - MinusLogProbMetric: 44.9799 - val_loss: 48.7915 - val_MinusLogProbMetric: 48.7915 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 133/1000
2023-10-28 11:41:02.071 
Epoch 133/1000 
	 loss: 44.6140, MinusLogProbMetric: 44.6140, val_loss: 45.2547, val_MinusLogProbMetric: 45.2547

Epoch 133: val_loss did not improve from 44.68769
196/196 - 35s - loss: 44.6140 - MinusLogProbMetric: 44.6140 - val_loss: 45.2547 - val_MinusLogProbMetric: 45.2547 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 134/1000
2023-10-28 11:41:38.114 
Epoch 134/1000 
	 loss: 44.5952, MinusLogProbMetric: 44.5952, val_loss: 44.8419, val_MinusLogProbMetric: 44.8419

Epoch 134: val_loss did not improve from 44.68769
196/196 - 36s - loss: 44.5952 - MinusLogProbMetric: 44.5952 - val_loss: 44.8419 - val_MinusLogProbMetric: 44.8419 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 135/1000
2023-10-28 11:42:15.709 
Epoch 135/1000 
	 loss: 45.1061, MinusLogProbMetric: 45.1061, val_loss: 44.9352, val_MinusLogProbMetric: 44.9352

Epoch 135: val_loss did not improve from 44.68769
196/196 - 38s - loss: 45.1061 - MinusLogProbMetric: 45.1061 - val_loss: 44.9352 - val_MinusLogProbMetric: 44.9352 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 136/1000
2023-10-28 11:42:47.714 
Epoch 136/1000 
	 loss: 44.5346, MinusLogProbMetric: 44.5346, val_loss: 44.3285, val_MinusLogProbMetric: 44.3285

Epoch 136: val_loss improved from 44.68769 to 44.32850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 33s - loss: 44.5346 - MinusLogProbMetric: 44.5346 - val_loss: 44.3285 - val_MinusLogProbMetric: 44.3285 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 137/1000
2023-10-28 11:43:23.815 
Epoch 137/1000 
	 loss: 44.3434, MinusLogProbMetric: 44.3434, val_loss: 44.7787, val_MinusLogProbMetric: 44.7787

Epoch 137: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.3434 - MinusLogProbMetric: 44.3434 - val_loss: 44.7787 - val_MinusLogProbMetric: 44.7787 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 138/1000
2023-10-28 11:44:01.520 
Epoch 138/1000 
	 loss: 44.2684, MinusLogProbMetric: 44.2684, val_loss: 44.9217, val_MinusLogProbMetric: 44.9217

Epoch 138: val_loss did not improve from 44.32850
196/196 - 38s - loss: 44.2684 - MinusLogProbMetric: 44.2684 - val_loss: 44.9217 - val_MinusLogProbMetric: 44.9217 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 139/1000
2023-10-28 11:44:36.120 
Epoch 139/1000 
	 loss: 44.8174, MinusLogProbMetric: 44.8174, val_loss: 45.4472, val_MinusLogProbMetric: 45.4472

Epoch 139: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.8174 - MinusLogProbMetric: 44.8174 - val_loss: 45.4472 - val_MinusLogProbMetric: 45.4472 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 140/1000
2023-10-28 11:45:11.100 
Epoch 140/1000 
	 loss: 44.2187, MinusLogProbMetric: 44.2187, val_loss: 45.0238, val_MinusLogProbMetric: 45.0238

Epoch 140: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.2187 - MinusLogProbMetric: 44.2187 - val_loss: 45.0238 - val_MinusLogProbMetric: 45.0238 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 141/1000
2023-10-28 11:45:49.665 
Epoch 141/1000 
	 loss: 44.5875, MinusLogProbMetric: 44.5875, val_loss: 45.1079, val_MinusLogProbMetric: 45.1079

Epoch 141: val_loss did not improve from 44.32850
196/196 - 39s - loss: 44.5875 - MinusLogProbMetric: 44.5875 - val_loss: 45.1079 - val_MinusLogProbMetric: 45.1079 - lr: 1.1111e-04 - 39s/epoch - 197ms/step
Epoch 142/1000
2023-10-28 11:46:25.139 
Epoch 142/1000 
	 loss: 44.3686, MinusLogProbMetric: 44.3686, val_loss: 45.2041, val_MinusLogProbMetric: 45.2041

Epoch 142: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.3686 - MinusLogProbMetric: 44.3686 - val_loss: 45.2041 - val_MinusLogProbMetric: 45.2041 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 143/1000
2023-10-28 11:46:59.534 
Epoch 143/1000 
	 loss: 44.3430, MinusLogProbMetric: 44.3430, val_loss: 45.3735, val_MinusLogProbMetric: 45.3735

Epoch 143: val_loss did not improve from 44.32850
196/196 - 34s - loss: 44.3430 - MinusLogProbMetric: 44.3430 - val_loss: 45.3735 - val_MinusLogProbMetric: 45.3735 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 144/1000
2023-10-28 11:47:36.221 
Epoch 144/1000 
	 loss: 44.2917, MinusLogProbMetric: 44.2917, val_loss: 44.6499, val_MinusLogProbMetric: 44.6499

Epoch 144: val_loss did not improve from 44.32850
196/196 - 37s - loss: 44.2917 - MinusLogProbMetric: 44.2917 - val_loss: 44.6499 - val_MinusLogProbMetric: 44.6499 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 145/1000
2023-10-28 11:48:11.851 
Epoch 145/1000 
	 loss: 44.7348, MinusLogProbMetric: 44.7348, val_loss: 46.7807, val_MinusLogProbMetric: 46.7807

Epoch 145: val_loss did not improve from 44.32850
196/196 - 36s - loss: 44.7348 - MinusLogProbMetric: 44.7348 - val_loss: 46.7807 - val_MinusLogProbMetric: 46.7807 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 146/1000
2023-10-28 11:48:46.406 
Epoch 146/1000 
	 loss: 44.4483, MinusLogProbMetric: 44.4483, val_loss: 44.5507, val_MinusLogProbMetric: 44.5507

Epoch 146: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.4483 - MinusLogProbMetric: 44.4483 - val_loss: 44.5507 - val_MinusLogProbMetric: 44.5507 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 147/1000
2023-10-28 11:49:22.690 
Epoch 147/1000 
	 loss: 44.5790, MinusLogProbMetric: 44.5790, val_loss: 58.5314, val_MinusLogProbMetric: 58.5314

Epoch 147: val_loss did not improve from 44.32850
196/196 - 36s - loss: 44.5790 - MinusLogProbMetric: 44.5790 - val_loss: 58.5314 - val_MinusLogProbMetric: 58.5314 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 148/1000
2023-10-28 11:50:00.378 
Epoch 148/1000 
	 loss: 45.1397, MinusLogProbMetric: 45.1397, val_loss: 45.2121, val_MinusLogProbMetric: 45.2121

Epoch 148: val_loss did not improve from 44.32850
196/196 - 38s - loss: 45.1397 - MinusLogProbMetric: 45.1397 - val_loss: 45.2121 - val_MinusLogProbMetric: 45.2121 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 149/1000
2023-10-28 11:50:35.115 
Epoch 149/1000 
	 loss: 44.8333, MinusLogProbMetric: 44.8333, val_loss: 44.7939, val_MinusLogProbMetric: 44.7939

Epoch 149: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.8333 - MinusLogProbMetric: 44.8333 - val_loss: 44.7939 - val_MinusLogProbMetric: 44.7939 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 150/1000
2023-10-28 11:51:10.168 
Epoch 150/1000 
	 loss: 44.3779, MinusLogProbMetric: 44.3779, val_loss: 45.1756, val_MinusLogProbMetric: 45.1756

Epoch 150: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.3779 - MinusLogProbMetric: 44.3779 - val_loss: 45.1756 - val_MinusLogProbMetric: 45.1756 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 151/1000
2023-10-28 11:51:47.860 
Epoch 151/1000 
	 loss: 44.0243, MinusLogProbMetric: 44.0243, val_loss: 45.1960, val_MinusLogProbMetric: 45.1960

Epoch 151: val_loss did not improve from 44.32850
196/196 - 38s - loss: 44.0243 - MinusLogProbMetric: 44.0243 - val_loss: 45.1960 - val_MinusLogProbMetric: 45.1960 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 152/1000
2023-10-28 11:52:22.775 
Epoch 152/1000 
	 loss: 44.1570, MinusLogProbMetric: 44.1570, val_loss: 44.6351, val_MinusLogProbMetric: 44.6351

Epoch 152: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.1570 - MinusLogProbMetric: 44.1570 - val_loss: 44.6351 - val_MinusLogProbMetric: 44.6351 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 153/1000
2023-10-28 11:52:57.190 
Epoch 153/1000 
	 loss: 44.6121, MinusLogProbMetric: 44.6121, val_loss: 44.9173, val_MinusLogProbMetric: 44.9173

Epoch 153: val_loss did not improve from 44.32850
196/196 - 34s - loss: 44.6121 - MinusLogProbMetric: 44.6121 - val_loss: 44.9173 - val_MinusLogProbMetric: 44.9173 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 154/1000
2023-10-28 11:53:35.142 
Epoch 154/1000 
	 loss: 43.9870, MinusLogProbMetric: 43.9870, val_loss: 45.1539, val_MinusLogProbMetric: 45.1539

Epoch 154: val_loss did not improve from 44.32850
196/196 - 38s - loss: 43.9870 - MinusLogProbMetric: 43.9870 - val_loss: 45.1539 - val_MinusLogProbMetric: 45.1539 - lr: 1.1111e-04 - 38s/epoch - 194ms/step
Epoch 155/1000
2023-10-28 11:54:10.482 
Epoch 155/1000 
	 loss: 44.2998, MinusLogProbMetric: 44.2998, val_loss: 45.1208, val_MinusLogProbMetric: 45.1208

Epoch 155: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.2998 - MinusLogProbMetric: 44.2998 - val_loss: 45.1208 - val_MinusLogProbMetric: 45.1208 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 156/1000
2023-10-28 11:54:43.671 
Epoch 156/1000 
	 loss: 44.3499, MinusLogProbMetric: 44.3499, val_loss: 44.9576, val_MinusLogProbMetric: 44.9576

Epoch 156: val_loss did not improve from 44.32850
196/196 - 33s - loss: 44.3499 - MinusLogProbMetric: 44.3499 - val_loss: 44.9576 - val_MinusLogProbMetric: 44.9576 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 157/1000
2023-10-28 11:55:19.129 
Epoch 157/1000 
	 loss: 45.0201, MinusLogProbMetric: 45.0201, val_loss: 45.3538, val_MinusLogProbMetric: 45.3538

Epoch 157: val_loss did not improve from 44.32850
196/196 - 35s - loss: 45.0201 - MinusLogProbMetric: 45.0201 - val_loss: 45.3538 - val_MinusLogProbMetric: 45.3538 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 158/1000
2023-10-28 11:55:56.939 
Epoch 158/1000 
	 loss: 44.2361, MinusLogProbMetric: 44.2361, val_loss: 47.0752, val_MinusLogProbMetric: 47.0752

Epoch 158: val_loss did not improve from 44.32850
196/196 - 38s - loss: 44.2361 - MinusLogProbMetric: 44.2361 - val_loss: 47.0752 - val_MinusLogProbMetric: 47.0752 - lr: 1.1111e-04 - 38s/epoch - 193ms/step
Epoch 159/1000
2023-10-28 11:56:31.365 
Epoch 159/1000 
	 loss: 44.0040, MinusLogProbMetric: 44.0040, val_loss: 45.4447, val_MinusLogProbMetric: 45.4447

Epoch 159: val_loss did not improve from 44.32850
196/196 - 34s - loss: 44.0040 - MinusLogProbMetric: 44.0040 - val_loss: 45.4447 - val_MinusLogProbMetric: 45.4447 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 160/1000
2023-10-28 11:57:07.819 
Epoch 160/1000 
	 loss: 43.9448, MinusLogProbMetric: 43.9448, val_loss: 45.1852, val_MinusLogProbMetric: 45.1852

Epoch 160: val_loss did not improve from 44.32850
196/196 - 36s - loss: 43.9448 - MinusLogProbMetric: 43.9448 - val_loss: 45.1852 - val_MinusLogProbMetric: 45.1852 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 161/1000
2023-10-28 11:57:46.364 
Epoch 161/1000 
	 loss: 44.5098, MinusLogProbMetric: 44.5098, val_loss: 44.9448, val_MinusLogProbMetric: 44.9448

Epoch 161: val_loss did not improve from 44.32850
196/196 - 39s - loss: 44.5098 - MinusLogProbMetric: 44.5098 - val_loss: 44.9448 - val_MinusLogProbMetric: 44.9448 - lr: 1.1111e-04 - 39s/epoch - 197ms/step
Epoch 162/1000
2023-10-28 11:58:20.977 
Epoch 162/1000 
	 loss: 44.1869, MinusLogProbMetric: 44.1869, val_loss: 44.5665, val_MinusLogProbMetric: 44.5665

Epoch 162: val_loss did not improve from 44.32850
196/196 - 35s - loss: 44.1869 - MinusLogProbMetric: 44.1869 - val_loss: 44.5665 - val_MinusLogProbMetric: 44.5665 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 163/1000
2023-10-28 11:58:56.512 
Epoch 163/1000 
	 loss: 44.3417, MinusLogProbMetric: 44.3417, val_loss: 45.7287, val_MinusLogProbMetric: 45.7287

Epoch 163: val_loss did not improve from 44.32850
196/196 - 36s - loss: 44.3417 - MinusLogProbMetric: 44.3417 - val_loss: 45.7287 - val_MinusLogProbMetric: 45.7287 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 164/1000
2023-10-28 11:59:35.586 
Epoch 164/1000 
	 loss: 44.4038, MinusLogProbMetric: 44.4038, val_loss: 46.1085, val_MinusLogProbMetric: 46.1085

Epoch 164: val_loss did not improve from 44.32850
196/196 - 39s - loss: 44.4038 - MinusLogProbMetric: 44.4038 - val_loss: 46.1085 - val_MinusLogProbMetric: 46.1085 - lr: 1.1111e-04 - 39s/epoch - 199ms/step
Epoch 165/1000
2023-10-28 12:00:08.581 
Epoch 165/1000 
	 loss: 43.9127, MinusLogProbMetric: 43.9127, val_loss: 47.7172, val_MinusLogProbMetric: 47.7172

Epoch 165: val_loss did not improve from 44.32850
196/196 - 33s - loss: 43.9127 - MinusLogProbMetric: 43.9127 - val_loss: 47.7172 - val_MinusLogProbMetric: 47.7172 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 166/1000
2023-10-28 12:00:42.595 
Epoch 166/1000 
	 loss: 44.1224, MinusLogProbMetric: 44.1224, val_loss: 44.4668, val_MinusLogProbMetric: 44.4668

Epoch 166: val_loss did not improve from 44.32850
196/196 - 34s - loss: 44.1224 - MinusLogProbMetric: 44.1224 - val_loss: 44.4668 - val_MinusLogProbMetric: 44.4668 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 167/1000
2023-10-28 12:01:17.789 
Epoch 167/1000 
	 loss: 43.9694, MinusLogProbMetric: 43.9694, val_loss: 44.5228, val_MinusLogProbMetric: 44.5228

Epoch 167: val_loss did not improve from 44.32850
196/196 - 35s - loss: 43.9694 - MinusLogProbMetric: 43.9694 - val_loss: 44.5228 - val_MinusLogProbMetric: 44.5228 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 168/1000
2023-10-28 12:01:53.731 
Epoch 168/1000 
	 loss: 43.8017, MinusLogProbMetric: 43.8017, val_loss: 44.4078, val_MinusLogProbMetric: 44.4078

Epoch 168: val_loss did not improve from 44.32850
196/196 - 36s - loss: 43.8017 - MinusLogProbMetric: 43.8017 - val_loss: 44.4078 - val_MinusLogProbMetric: 44.4078 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 169/1000
2023-10-28 12:02:28.217 
Epoch 169/1000 
	 loss: 44.5664, MinusLogProbMetric: 44.5664, val_loss: 44.1698, val_MinusLogProbMetric: 44.1698

Epoch 169: val_loss improved from 44.32850 to 44.16983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 36s - loss: 44.5664 - MinusLogProbMetric: 44.5664 - val_loss: 44.1698 - val_MinusLogProbMetric: 44.1698 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 170/1000
2023-10-28 12:03:06.917 
Epoch 170/1000 
	 loss: 43.8218, MinusLogProbMetric: 43.8218, val_loss: 45.8135, val_MinusLogProbMetric: 45.8135

Epoch 170: val_loss did not improve from 44.16983
196/196 - 37s - loss: 43.8218 - MinusLogProbMetric: 43.8218 - val_loss: 45.8135 - val_MinusLogProbMetric: 45.8135 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 171/1000
2023-10-28 12:03:44.383 
Epoch 171/1000 
	 loss: 43.7467, MinusLogProbMetric: 43.7467, val_loss: 44.7155, val_MinusLogProbMetric: 44.7155

Epoch 171: val_loss did not improve from 44.16983
196/196 - 37s - loss: 43.7467 - MinusLogProbMetric: 43.7467 - val_loss: 44.7155 - val_MinusLogProbMetric: 44.7155 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 172/1000
2023-10-28 12:04:18.657 
Epoch 172/1000 
	 loss: 44.2474, MinusLogProbMetric: 44.2474, val_loss: 44.4110, val_MinusLogProbMetric: 44.4110

Epoch 172: val_loss did not improve from 44.16983
196/196 - 34s - loss: 44.2474 - MinusLogProbMetric: 44.2474 - val_loss: 44.4110 - val_MinusLogProbMetric: 44.4110 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 173/1000
2023-10-28 12:04:54.365 
Epoch 173/1000 
	 loss: 43.7573, MinusLogProbMetric: 43.7573, val_loss: 45.0994, val_MinusLogProbMetric: 45.0994

Epoch 173: val_loss did not improve from 44.16983
196/196 - 36s - loss: 43.7573 - MinusLogProbMetric: 43.7573 - val_loss: 45.0994 - val_MinusLogProbMetric: 45.0994 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 174/1000
2023-10-28 12:05:33.122 
Epoch 174/1000 
	 loss: 43.8036, MinusLogProbMetric: 43.8036, val_loss: 44.6468, val_MinusLogProbMetric: 44.6468

Epoch 174: val_loss did not improve from 44.16983
196/196 - 39s - loss: 43.8036 - MinusLogProbMetric: 43.8036 - val_loss: 44.6468 - val_MinusLogProbMetric: 44.6468 - lr: 1.1111e-04 - 39s/epoch - 198ms/step
Epoch 175/1000
2023-10-28 12:06:07.594 
Epoch 175/1000 
	 loss: 44.2017, MinusLogProbMetric: 44.2017, val_loss: 44.3137, val_MinusLogProbMetric: 44.3137

Epoch 175: val_loss did not improve from 44.16983
196/196 - 34s - loss: 44.2017 - MinusLogProbMetric: 44.2017 - val_loss: 44.3137 - val_MinusLogProbMetric: 44.3137 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 176/1000
2023-10-28 12:06:42.163 
Epoch 176/1000 
	 loss: 43.8853, MinusLogProbMetric: 43.8853, val_loss: 44.0550, val_MinusLogProbMetric: 44.0550

Epoch 176: val_loss improved from 44.16983 to 44.05499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 35s - loss: 43.8853 - MinusLogProbMetric: 43.8853 - val_loss: 44.0550 - val_MinusLogProbMetric: 44.0550 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 177/1000
2023-10-28 12:07:22.480 
Epoch 177/1000 
	 loss: 43.6945, MinusLogProbMetric: 43.6945, val_loss: 44.5999, val_MinusLogProbMetric: 44.5999

Epoch 177: val_loss did not improve from 44.05499
196/196 - 40s - loss: 43.6945 - MinusLogProbMetric: 43.6945 - val_loss: 44.5999 - val_MinusLogProbMetric: 44.5999 - lr: 1.1111e-04 - 40s/epoch - 202ms/step
Epoch 178/1000
2023-10-28 12:07:57.801 
Epoch 178/1000 
	 loss: 44.0923, MinusLogProbMetric: 44.0923, val_loss: 44.3524, val_MinusLogProbMetric: 44.3524

Epoch 178: val_loss did not improve from 44.05499
196/196 - 35s - loss: 44.0923 - MinusLogProbMetric: 44.0923 - val_loss: 44.3524 - val_MinusLogProbMetric: 44.3524 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 179/1000
2023-10-28 12:08:32.554 
Epoch 179/1000 
	 loss: 43.9439, MinusLogProbMetric: 43.9439, val_loss: 44.4600, val_MinusLogProbMetric: 44.4600

Epoch 179: val_loss did not improve from 44.05499
196/196 - 35s - loss: 43.9439 - MinusLogProbMetric: 43.9439 - val_loss: 44.4600 - val_MinusLogProbMetric: 44.4600 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 180/1000
2023-10-28 12:09:08.047 
Epoch 180/1000 
	 loss: 43.7086, MinusLogProbMetric: 43.7086, val_loss: 43.8568, val_MinusLogProbMetric: 43.8568

Epoch 180: val_loss improved from 44.05499 to 43.85677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 36s - loss: 43.7086 - MinusLogProbMetric: 43.7086 - val_loss: 43.8568 - val_MinusLogProbMetric: 43.8568 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 181/1000
2023-10-28 12:09:44.847 
Epoch 181/1000 
	 loss: 44.4682, MinusLogProbMetric: 44.4682, val_loss: 46.1737, val_MinusLogProbMetric: 46.1737

Epoch 181: val_loss did not improve from 43.85677
196/196 - 36s - loss: 44.4682 - MinusLogProbMetric: 44.4682 - val_loss: 46.1737 - val_MinusLogProbMetric: 46.1737 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 182/1000
2023-10-28 12:10:19.384 
Epoch 182/1000 
	 loss: 43.4609, MinusLogProbMetric: 43.4609, val_loss: 48.4519, val_MinusLogProbMetric: 48.4519

Epoch 182: val_loss did not improve from 43.85677
196/196 - 35s - loss: 43.4609 - MinusLogProbMetric: 43.4609 - val_loss: 48.4519 - val_MinusLogProbMetric: 48.4519 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 183/1000
2023-10-28 12:10:55.125 
Epoch 183/1000 
	 loss: 43.9416, MinusLogProbMetric: 43.9416, val_loss: 43.8720, val_MinusLogProbMetric: 43.8720

Epoch 183: val_loss did not improve from 43.85677
196/196 - 36s - loss: 43.9416 - MinusLogProbMetric: 43.9416 - val_loss: 43.8720 - val_MinusLogProbMetric: 43.8720 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 184/1000
2023-10-28 12:11:30.809 
Epoch 184/1000 
	 loss: 43.5460, MinusLogProbMetric: 43.5460, val_loss: 44.2362, val_MinusLogProbMetric: 44.2362

Epoch 184: val_loss did not improve from 43.85677
196/196 - 36s - loss: 43.5460 - MinusLogProbMetric: 43.5460 - val_loss: 44.2362 - val_MinusLogProbMetric: 44.2362 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 185/1000
2023-10-28 12:12:05.395 
Epoch 185/1000 
	 loss: 43.7901, MinusLogProbMetric: 43.7901, val_loss: 46.0455, val_MinusLogProbMetric: 46.0455

Epoch 185: val_loss did not improve from 43.85677
196/196 - 35s - loss: 43.7901 - MinusLogProbMetric: 43.7901 - val_loss: 46.0455 - val_MinusLogProbMetric: 46.0455 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 186/1000
2023-10-28 12:12:41.056 
Epoch 186/1000 
	 loss: 43.6642, MinusLogProbMetric: 43.6642, val_loss: 47.6227, val_MinusLogProbMetric: 47.6227

Epoch 186: val_loss did not improve from 43.85677
196/196 - 36s - loss: 43.6642 - MinusLogProbMetric: 43.6642 - val_loss: 47.6227 - val_MinusLogProbMetric: 47.6227 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 187/1000
2023-10-28 12:13:18.262 
Epoch 187/1000 
	 loss: 44.1823, MinusLogProbMetric: 44.1823, val_loss: 44.6429, val_MinusLogProbMetric: 44.6429

Epoch 187: val_loss did not improve from 43.85677
196/196 - 37s - loss: 44.1823 - MinusLogProbMetric: 44.1823 - val_loss: 44.6429 - val_MinusLogProbMetric: 44.6429 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 188/1000
2023-10-28 12:13:53.047 
Epoch 188/1000 
	 loss: 43.6628, MinusLogProbMetric: 43.6628, val_loss: 45.4277, val_MinusLogProbMetric: 45.4277

Epoch 188: val_loss did not improve from 43.85677
196/196 - 35s - loss: 43.6628 - MinusLogProbMetric: 43.6628 - val_loss: 45.4277 - val_MinusLogProbMetric: 45.4277 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 189/1000
2023-10-28 12:14:27.976 
Epoch 189/1000 
	 loss: 44.1612, MinusLogProbMetric: 44.1612, val_loss: 44.1630, val_MinusLogProbMetric: 44.1630

Epoch 189: val_loss did not improve from 43.85677
196/196 - 35s - loss: 44.1612 - MinusLogProbMetric: 44.1612 - val_loss: 44.1630 - val_MinusLogProbMetric: 44.1630 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 190/1000
2023-10-28 12:15:04.544 
Epoch 190/1000 
	 loss: 43.4279, MinusLogProbMetric: 43.4279, val_loss: 44.5129, val_MinusLogProbMetric: 44.5129

Epoch 190: val_loss did not improve from 43.85677
196/196 - 37s - loss: 43.4279 - MinusLogProbMetric: 43.4279 - val_loss: 44.5129 - val_MinusLogProbMetric: 44.5129 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 191/1000
2023-10-28 12:15:39.411 
Epoch 191/1000 
	 loss: 43.5193, MinusLogProbMetric: 43.5193, val_loss: 44.0376, val_MinusLogProbMetric: 44.0376

Epoch 191: val_loss did not improve from 43.85677
196/196 - 35s - loss: 43.5193 - MinusLogProbMetric: 43.5193 - val_loss: 44.0376 - val_MinusLogProbMetric: 44.0376 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 192/1000
2023-10-28 12:16:13.073 
Epoch 192/1000 
	 loss: 44.4878, MinusLogProbMetric: 44.4878, val_loss: 44.5431, val_MinusLogProbMetric: 44.5431

Epoch 192: val_loss did not improve from 43.85677
196/196 - 34s - loss: 44.4878 - MinusLogProbMetric: 44.4878 - val_loss: 44.5431 - val_MinusLogProbMetric: 44.5431 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 193/1000
2023-10-28 12:16:49.904 
Epoch 193/1000 
	 loss: 43.3125, MinusLogProbMetric: 43.3125, val_loss: 46.6486, val_MinusLogProbMetric: 46.6486

Epoch 193: val_loss did not improve from 43.85677
196/196 - 37s - loss: 43.3125 - MinusLogProbMetric: 43.3125 - val_loss: 46.6486 - val_MinusLogProbMetric: 46.6486 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 194/1000
2023-10-28 12:17:27.395 
Epoch 194/1000 
	 loss: 43.6529, MinusLogProbMetric: 43.6529, val_loss: 44.4498, val_MinusLogProbMetric: 44.4498

Epoch 194: val_loss did not improve from 43.85677
196/196 - 37s - loss: 43.6529 - MinusLogProbMetric: 43.6529 - val_loss: 44.4498 - val_MinusLogProbMetric: 44.4498 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 195/1000
2023-10-28 12:18:02.239 
Epoch 195/1000 
	 loss: 43.6750, MinusLogProbMetric: 43.6750, val_loss: 44.4409, val_MinusLogProbMetric: 44.4409

Epoch 195: val_loss did not improve from 43.85677
196/196 - 35s - loss: 43.6750 - MinusLogProbMetric: 43.6750 - val_loss: 44.4409 - val_MinusLogProbMetric: 44.4409 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 196/1000
2023-10-28 12:18:37.807 
Epoch 196/1000 
	 loss: 43.5478, MinusLogProbMetric: 43.5478, val_loss: 44.7785, val_MinusLogProbMetric: 44.7785

Epoch 196: val_loss did not improve from 43.85677
196/196 - 36s - loss: 43.5478 - MinusLogProbMetric: 43.5478 - val_loss: 44.7785 - val_MinusLogProbMetric: 44.7785 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 197/1000
2023-10-28 12:19:14.839 
Epoch 197/1000 
	 loss: 43.8717, MinusLogProbMetric: 43.8717, val_loss: 44.7663, val_MinusLogProbMetric: 44.7663

Epoch 197: val_loss did not improve from 43.85677
196/196 - 37s - loss: 43.8717 - MinusLogProbMetric: 43.8717 - val_loss: 44.7663 - val_MinusLogProbMetric: 44.7663 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 198/1000
2023-10-28 12:19:48.533 
Epoch 198/1000 
	 loss: 43.7217, MinusLogProbMetric: 43.7217, val_loss: 43.8164, val_MinusLogProbMetric: 43.8164

Epoch 198: val_loss improved from 43.85677 to 43.81637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 35s - loss: 43.7217 - MinusLogProbMetric: 43.7217 - val_loss: 43.8164 - val_MinusLogProbMetric: 43.8164 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 199/1000
2023-10-28 12:20:24.029 
Epoch 199/1000 
	 loss: 43.3746, MinusLogProbMetric: 43.3746, val_loss: 44.0271, val_MinusLogProbMetric: 44.0271

Epoch 199: val_loss did not improve from 43.81637
196/196 - 35s - loss: 43.3746 - MinusLogProbMetric: 43.3746 - val_loss: 44.0271 - val_MinusLogProbMetric: 44.0271 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 200/1000
2023-10-28 12:20:59.780 
Epoch 200/1000 
	 loss: 43.5886, MinusLogProbMetric: 43.5886, val_loss: 44.3363, val_MinusLogProbMetric: 44.3363

Epoch 200: val_loss did not improve from 43.81637
196/196 - 36s - loss: 43.5886 - MinusLogProbMetric: 43.5886 - val_loss: 44.3363 - val_MinusLogProbMetric: 44.3363 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 201/1000
2023-10-28 12:21:37.665 
Epoch 201/1000 
	 loss: 43.4817, MinusLogProbMetric: 43.4817, val_loss: 44.4533, val_MinusLogProbMetric: 44.4533

Epoch 201: val_loss did not improve from 43.81637
196/196 - 38s - loss: 43.4817 - MinusLogProbMetric: 43.4817 - val_loss: 44.4533 - val_MinusLogProbMetric: 44.4533 - lr: 1.1111e-04 - 38s/epoch - 193ms/step
Epoch 202/1000
2023-10-28 12:22:12.704 
Epoch 202/1000 
	 loss: 43.7734, MinusLogProbMetric: 43.7734, val_loss: 44.6251, val_MinusLogProbMetric: 44.6251

Epoch 202: val_loss did not improve from 43.81637
196/196 - 35s - loss: 43.7734 - MinusLogProbMetric: 43.7734 - val_loss: 44.6251 - val_MinusLogProbMetric: 44.6251 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 203/1000
2023-10-28 12:22:46.999 
Epoch 203/1000 
	 loss: 43.8618, MinusLogProbMetric: 43.8618, val_loss: 44.3390, val_MinusLogProbMetric: 44.3390

Epoch 203: val_loss did not improve from 43.81637
196/196 - 34s - loss: 43.8618 - MinusLogProbMetric: 43.8618 - val_loss: 44.3390 - val_MinusLogProbMetric: 44.3390 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 204/1000
2023-10-28 12:23:21.422 
Epoch 204/1000 
	 loss: 43.2485, MinusLogProbMetric: 43.2485, val_loss: 48.3485, val_MinusLogProbMetric: 48.3485

Epoch 204: val_loss did not improve from 43.81637
196/196 - 34s - loss: 43.2485 - MinusLogProbMetric: 43.2485 - val_loss: 48.3485 - val_MinusLogProbMetric: 48.3485 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 205/1000
2023-10-28 12:23:59.728 
Epoch 205/1000 
	 loss: 43.5606, MinusLogProbMetric: 43.5606, val_loss: 44.8554, val_MinusLogProbMetric: 44.8554

Epoch 205: val_loss did not improve from 43.81637
196/196 - 38s - loss: 43.5606 - MinusLogProbMetric: 43.5606 - val_loss: 44.8554 - val_MinusLogProbMetric: 44.8554 - lr: 1.1111e-04 - 38s/epoch - 195ms/step
Epoch 206/1000
2023-10-28 12:24:35.578 
Epoch 206/1000 
	 loss: 43.4697, MinusLogProbMetric: 43.4697, val_loss: 44.1878, val_MinusLogProbMetric: 44.1878

Epoch 206: val_loss did not improve from 43.81637
196/196 - 36s - loss: 43.4697 - MinusLogProbMetric: 43.4697 - val_loss: 44.1878 - val_MinusLogProbMetric: 44.1878 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 207/1000
2023-10-28 12:25:09.641 
Epoch 207/1000 
	 loss: 43.4399, MinusLogProbMetric: 43.4399, val_loss: 44.2880, val_MinusLogProbMetric: 44.2880

Epoch 207: val_loss did not improve from 43.81637
196/196 - 34s - loss: 43.4399 - MinusLogProbMetric: 43.4399 - val_loss: 44.2880 - val_MinusLogProbMetric: 44.2880 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 208/1000
2023-10-28 12:25:44.204 
Epoch 208/1000 
	 loss: 43.3853, MinusLogProbMetric: 43.3853, val_loss: 44.9061, val_MinusLogProbMetric: 44.9061

Epoch 208: val_loss did not improve from 43.81637
196/196 - 35s - loss: 43.3853 - MinusLogProbMetric: 43.3853 - val_loss: 44.9061 - val_MinusLogProbMetric: 44.9061 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 209/1000
2023-10-28 12:26:21.189 
Epoch 209/1000 
	 loss: 44.3402, MinusLogProbMetric: 44.3402, val_loss: 44.3884, val_MinusLogProbMetric: 44.3884

Epoch 209: val_loss did not improve from 43.81637
196/196 - 37s - loss: 44.3402 - MinusLogProbMetric: 44.3402 - val_loss: 44.3884 - val_MinusLogProbMetric: 44.3884 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 210/1000
2023-10-28 12:26:58.145 
Epoch 210/1000 
	 loss: 43.3392, MinusLogProbMetric: 43.3392, val_loss: 43.9690, val_MinusLogProbMetric: 43.9690

Epoch 210: val_loss did not improve from 43.81637
196/196 - 37s - loss: 43.3392 - MinusLogProbMetric: 43.3392 - val_loss: 43.9690 - val_MinusLogProbMetric: 43.9690 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 211/1000
2023-10-28 12:27:31.397 
Epoch 211/1000 
	 loss: 43.2138, MinusLogProbMetric: 43.2138, val_loss: 49.7884, val_MinusLogProbMetric: 49.7884

Epoch 211: val_loss did not improve from 43.81637
196/196 - 33s - loss: 43.2138 - MinusLogProbMetric: 43.2138 - val_loss: 49.7884 - val_MinusLogProbMetric: 49.7884 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 212/1000
2023-10-28 12:28:05.770 
Epoch 212/1000 
	 loss: 43.7459, MinusLogProbMetric: 43.7459, val_loss: 44.0734, val_MinusLogProbMetric: 44.0734

Epoch 212: val_loss did not improve from 43.81637
196/196 - 34s - loss: 43.7459 - MinusLogProbMetric: 43.7459 - val_loss: 44.0734 - val_MinusLogProbMetric: 44.0734 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 213/1000
2023-10-28 12:28:40.598 
Epoch 213/1000 
	 loss: 43.3355, MinusLogProbMetric: 43.3355, val_loss: 44.2217, val_MinusLogProbMetric: 44.2217

Epoch 213: val_loss did not improve from 43.81637
196/196 - 35s - loss: 43.3355 - MinusLogProbMetric: 43.3355 - val_loss: 44.2217 - val_MinusLogProbMetric: 44.2217 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 214/1000
2023-10-28 12:29:18.324 
Epoch 214/1000 
	 loss: 43.4745, MinusLogProbMetric: 43.4745, val_loss: 44.4561, val_MinusLogProbMetric: 44.4561

Epoch 214: val_loss did not improve from 43.81637
196/196 - 38s - loss: 43.4745 - MinusLogProbMetric: 43.4745 - val_loss: 44.4561 - val_MinusLogProbMetric: 44.4561 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 215/1000
2023-10-28 12:29:54.270 
Epoch 215/1000 
	 loss: 43.2971, MinusLogProbMetric: 43.2971, val_loss: 43.8594, val_MinusLogProbMetric: 43.8594

Epoch 215: val_loss did not improve from 43.81637
196/196 - 36s - loss: 43.2971 - MinusLogProbMetric: 43.2971 - val_loss: 43.8594 - val_MinusLogProbMetric: 43.8594 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 216/1000
2023-10-28 12:30:28.581 
Epoch 216/1000 
	 loss: 43.9603, MinusLogProbMetric: 43.9603, val_loss: 45.1440, val_MinusLogProbMetric: 45.1440

Epoch 216: val_loss did not improve from 43.81637
196/196 - 34s - loss: 43.9603 - MinusLogProbMetric: 43.9603 - val_loss: 45.1440 - val_MinusLogProbMetric: 45.1440 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 217/1000
2023-10-28 12:31:03.461 
Epoch 217/1000 
	 loss: 43.3881, MinusLogProbMetric: 43.3881, val_loss: 44.3006, val_MinusLogProbMetric: 44.3006

Epoch 217: val_loss did not improve from 43.81637
196/196 - 35s - loss: 43.3881 - MinusLogProbMetric: 43.3881 - val_loss: 44.3006 - val_MinusLogProbMetric: 44.3006 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 218/1000
2023-10-28 12:31:41.446 
Epoch 218/1000 
	 loss: 43.0719, MinusLogProbMetric: 43.0719, val_loss: 44.3171, val_MinusLogProbMetric: 44.3171

Epoch 218: val_loss did not improve from 43.81637
196/196 - 38s - loss: 43.0719 - MinusLogProbMetric: 43.0719 - val_loss: 44.3171 - val_MinusLogProbMetric: 44.3171 - lr: 1.1111e-04 - 38s/epoch - 194ms/step
Epoch 219/1000
2023-10-28 12:32:17.445 
Epoch 219/1000 
	 loss: 43.1469, MinusLogProbMetric: 43.1469, val_loss: 43.4744, val_MinusLogProbMetric: 43.4744

Epoch 219: val_loss improved from 43.81637 to 43.47440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 37s - loss: 43.1469 - MinusLogProbMetric: 43.1469 - val_loss: 43.4744 - val_MinusLogProbMetric: 43.4744 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 220/1000
2023-10-28 12:32:52.746 
Epoch 220/1000 
	 loss: 43.3266, MinusLogProbMetric: 43.3266, val_loss: 44.0128, val_MinusLogProbMetric: 44.0128

Epoch 220: val_loss did not improve from 43.47440
196/196 - 35s - loss: 43.3266 - MinusLogProbMetric: 43.3266 - val_loss: 44.0128 - val_MinusLogProbMetric: 44.0128 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 221/1000
2023-10-28 12:33:27.267 
Epoch 221/1000 
	 loss: 43.4422, MinusLogProbMetric: 43.4422, val_loss: 44.5638, val_MinusLogProbMetric: 44.5638

Epoch 221: val_loss did not improve from 43.47440
196/196 - 35s - loss: 43.4422 - MinusLogProbMetric: 43.4422 - val_loss: 44.5638 - val_MinusLogProbMetric: 44.5638 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 222/1000
2023-10-28 12:34:05.613 
Epoch 222/1000 
	 loss: 43.3034, MinusLogProbMetric: 43.3034, val_loss: 43.8255, val_MinusLogProbMetric: 43.8255

Epoch 222: val_loss did not improve from 43.47440
196/196 - 38s - loss: 43.3034 - MinusLogProbMetric: 43.3034 - val_loss: 43.8255 - val_MinusLogProbMetric: 43.8255 - lr: 1.1111e-04 - 38s/epoch - 196ms/step
Epoch 223/1000
2023-10-28 12:34:42.138 
Epoch 223/1000 
	 loss: 43.2698, MinusLogProbMetric: 43.2698, val_loss: 45.1782, val_MinusLogProbMetric: 45.1782

Epoch 223: val_loss did not improve from 43.47440
196/196 - 37s - loss: 43.2698 - MinusLogProbMetric: 43.2698 - val_loss: 45.1782 - val_MinusLogProbMetric: 45.1782 - lr: 1.1111e-04 - 37s/epoch - 186ms/step
Epoch 224/1000
2023-10-28 12:35:16.694 
Epoch 224/1000 
	 loss: 43.2708, MinusLogProbMetric: 43.2708, val_loss: 44.6057, val_MinusLogProbMetric: 44.6057

Epoch 224: val_loss did not improve from 43.47440
196/196 - 35s - loss: 43.2708 - MinusLogProbMetric: 43.2708 - val_loss: 44.6057 - val_MinusLogProbMetric: 44.6057 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 225/1000
2023-10-28 12:35:50.980 
Epoch 225/1000 
	 loss: 43.1297, MinusLogProbMetric: 43.1297, val_loss: 44.1293, val_MinusLogProbMetric: 44.1293

Epoch 225: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.1297 - MinusLogProbMetric: 43.1297 - val_loss: 44.1293 - val_MinusLogProbMetric: 44.1293 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 226/1000
2023-10-28 12:36:26.905 
Epoch 226/1000 
	 loss: 43.1126, MinusLogProbMetric: 43.1126, val_loss: 43.7281, val_MinusLogProbMetric: 43.7281

Epoch 226: val_loss did not improve from 43.47440
196/196 - 36s - loss: 43.1126 - MinusLogProbMetric: 43.1126 - val_loss: 43.7281 - val_MinusLogProbMetric: 43.7281 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 227/1000
2023-10-28 12:37:06.045 
Epoch 227/1000 
	 loss: 43.1583, MinusLogProbMetric: 43.1583, val_loss: 44.3384, val_MinusLogProbMetric: 44.3384

Epoch 227: val_loss did not improve from 43.47440
196/196 - 39s - loss: 43.1583 - MinusLogProbMetric: 43.1583 - val_loss: 44.3384 - val_MinusLogProbMetric: 44.3384 - lr: 1.1111e-04 - 39s/epoch - 200ms/step
Epoch 228/1000
2023-10-28 12:37:40.618 
Epoch 228/1000 
	 loss: 43.2102, MinusLogProbMetric: 43.2102, val_loss: 44.1353, val_MinusLogProbMetric: 44.1353

Epoch 228: val_loss did not improve from 43.47440
196/196 - 35s - loss: 43.2102 - MinusLogProbMetric: 43.2102 - val_loss: 44.1353 - val_MinusLogProbMetric: 44.1353 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 229/1000
2023-10-28 12:38:14.707 
Epoch 229/1000 
	 loss: 43.5052, MinusLogProbMetric: 43.5052, val_loss: 47.4370, val_MinusLogProbMetric: 47.4370

Epoch 229: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.5052 - MinusLogProbMetric: 43.5052 - val_loss: 47.4370 - val_MinusLogProbMetric: 47.4370 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 230/1000
2023-10-28 12:38:48.980 
Epoch 230/1000 
	 loss: 43.3582, MinusLogProbMetric: 43.3582, val_loss: 43.6439, val_MinusLogProbMetric: 43.6439

Epoch 230: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.3582 - MinusLogProbMetric: 43.3582 - val_loss: 43.6439 - val_MinusLogProbMetric: 43.6439 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 231/1000
2023-10-28 12:39:26.738 
Epoch 231/1000 
	 loss: 43.1939, MinusLogProbMetric: 43.1939, val_loss: 45.2556, val_MinusLogProbMetric: 45.2556

Epoch 231: val_loss did not improve from 43.47440
196/196 - 38s - loss: 43.1939 - MinusLogProbMetric: 43.1939 - val_loss: 45.2556 - val_MinusLogProbMetric: 45.2556 - lr: 1.1111e-04 - 38s/epoch - 193ms/step
Epoch 232/1000
2023-10-28 12:40:04.088 
Epoch 232/1000 
	 loss: 43.0966, MinusLogProbMetric: 43.0966, val_loss: 44.5754, val_MinusLogProbMetric: 44.5754

Epoch 232: val_loss did not improve from 43.47440
196/196 - 37s - loss: 43.0966 - MinusLogProbMetric: 43.0966 - val_loss: 44.5754 - val_MinusLogProbMetric: 44.5754 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 233/1000
2023-10-28 12:40:38.472 
Epoch 233/1000 
	 loss: 43.0776, MinusLogProbMetric: 43.0776, val_loss: 43.7854, val_MinusLogProbMetric: 43.7854

Epoch 233: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.0776 - MinusLogProbMetric: 43.0776 - val_loss: 43.7854 - val_MinusLogProbMetric: 43.7854 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 234/1000
2023-10-28 12:41:13.276 
Epoch 234/1000 
	 loss: 43.1768, MinusLogProbMetric: 43.1768, val_loss: 44.6780, val_MinusLogProbMetric: 44.6780

Epoch 234: val_loss did not improve from 43.47440
196/196 - 35s - loss: 43.1768 - MinusLogProbMetric: 43.1768 - val_loss: 44.6780 - val_MinusLogProbMetric: 44.6780 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 235/1000
2023-10-28 12:41:50.789 
Epoch 235/1000 
	 loss: 43.1923, MinusLogProbMetric: 43.1923, val_loss: 49.6460, val_MinusLogProbMetric: 49.6460

Epoch 235: val_loss did not improve from 43.47440
196/196 - 38s - loss: 43.1923 - MinusLogProbMetric: 43.1923 - val_loss: 49.6460 - val_MinusLogProbMetric: 49.6460 - lr: 1.1111e-04 - 38s/epoch - 191ms/step
Epoch 236/1000
2023-10-28 12:42:27.432 
Epoch 236/1000 
	 loss: 43.2455, MinusLogProbMetric: 43.2455, val_loss: 43.4791, val_MinusLogProbMetric: 43.4791

Epoch 236: val_loss did not improve from 43.47440
196/196 - 37s - loss: 43.2455 - MinusLogProbMetric: 43.2455 - val_loss: 43.4791 - val_MinusLogProbMetric: 43.4791 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 237/1000
2023-10-28 12:43:02.027 
Epoch 237/1000 
	 loss: 42.9675, MinusLogProbMetric: 42.9675, val_loss: 43.6539, val_MinusLogProbMetric: 43.6539

Epoch 237: val_loss did not improve from 43.47440
196/196 - 35s - loss: 42.9675 - MinusLogProbMetric: 42.9675 - val_loss: 43.6539 - val_MinusLogProbMetric: 43.6539 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 238/1000
2023-10-28 12:43:36.415 
Epoch 238/1000 
	 loss: 43.6090, MinusLogProbMetric: 43.6090, val_loss: 44.0245, val_MinusLogProbMetric: 44.0245

Epoch 238: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.6090 - MinusLogProbMetric: 43.6090 - val_loss: 44.0245 - val_MinusLogProbMetric: 44.0245 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 239/1000
2023-10-28 12:44:13.117 
Epoch 239/1000 
	 loss: 42.9230, MinusLogProbMetric: 42.9230, val_loss: 43.9724, val_MinusLogProbMetric: 43.9724

Epoch 239: val_loss did not improve from 43.47440
196/196 - 37s - loss: 42.9230 - MinusLogProbMetric: 42.9230 - val_loss: 43.9724 - val_MinusLogProbMetric: 43.9724 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 240/1000
2023-10-28 12:44:51.116 
Epoch 240/1000 
	 loss: 43.0893, MinusLogProbMetric: 43.0893, val_loss: 44.1013, val_MinusLogProbMetric: 44.1013

Epoch 240: val_loss did not improve from 43.47440
196/196 - 38s - loss: 43.0893 - MinusLogProbMetric: 43.0893 - val_loss: 44.1013 - val_MinusLogProbMetric: 44.1013 - lr: 1.1111e-04 - 38s/epoch - 194ms/step
Epoch 241/1000
2023-10-28 12:45:25.815 
Epoch 241/1000 
	 loss: 43.2322, MinusLogProbMetric: 43.2322, val_loss: 44.3571, val_MinusLogProbMetric: 44.3571

Epoch 241: val_loss did not improve from 43.47440
196/196 - 35s - loss: 43.2322 - MinusLogProbMetric: 43.2322 - val_loss: 44.3571 - val_MinusLogProbMetric: 44.3571 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 242/1000
2023-10-28 12:46:00.094 
Epoch 242/1000 
	 loss: 42.8727, MinusLogProbMetric: 42.8727, val_loss: 44.5106, val_MinusLogProbMetric: 44.5106

Epoch 242: val_loss did not improve from 43.47440
196/196 - 34s - loss: 42.8727 - MinusLogProbMetric: 42.8727 - val_loss: 44.5106 - val_MinusLogProbMetric: 44.5106 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 243/1000
2023-10-28 12:46:34.105 
Epoch 243/1000 
	 loss: 43.0906, MinusLogProbMetric: 43.0906, val_loss: 44.7416, val_MinusLogProbMetric: 44.7416

Epoch 243: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.0906 - MinusLogProbMetric: 43.0906 - val_loss: 44.7416 - val_MinusLogProbMetric: 44.7416 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 244/1000
2023-10-28 12:47:13.051 
Epoch 244/1000 
	 loss: 42.9744, MinusLogProbMetric: 42.9744, val_loss: 43.4942, val_MinusLogProbMetric: 43.4942

Epoch 244: val_loss did not improve from 43.47440
196/196 - 39s - loss: 42.9744 - MinusLogProbMetric: 42.9744 - val_loss: 43.4942 - val_MinusLogProbMetric: 43.4942 - lr: 1.1111e-04 - 39s/epoch - 199ms/step
Epoch 245/1000
2023-10-28 12:47:49.254 
Epoch 245/1000 
	 loss: 43.5645, MinusLogProbMetric: 43.5645, val_loss: 45.4956, val_MinusLogProbMetric: 45.4956

Epoch 245: val_loss did not improve from 43.47440
196/196 - 36s - loss: 43.5645 - MinusLogProbMetric: 43.5645 - val_loss: 45.4956 - val_MinusLogProbMetric: 45.4956 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 246/1000
2023-10-28 12:48:23.683 
Epoch 246/1000 
	 loss: 43.1312, MinusLogProbMetric: 43.1312, val_loss: 45.8651, val_MinusLogProbMetric: 45.8651

Epoch 246: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.1312 - MinusLogProbMetric: 43.1312 - val_loss: 45.8651 - val_MinusLogProbMetric: 45.8651 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 247/1000
2023-10-28 12:48:58.669 
Epoch 247/1000 
	 loss: 42.8804, MinusLogProbMetric: 42.8804, val_loss: 44.8526, val_MinusLogProbMetric: 44.8526

Epoch 247: val_loss did not improve from 43.47440
196/196 - 35s - loss: 42.8804 - MinusLogProbMetric: 42.8804 - val_loss: 44.8526 - val_MinusLogProbMetric: 44.8526 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 248/1000
2023-10-28 12:49:37.289 
Epoch 248/1000 
	 loss: 43.4875, MinusLogProbMetric: 43.4875, val_loss: 43.5682, val_MinusLogProbMetric: 43.5682

Epoch 248: val_loss did not improve from 43.47440
196/196 - 39s - loss: 43.4875 - MinusLogProbMetric: 43.4875 - val_loss: 43.5682 - val_MinusLogProbMetric: 43.5682 - lr: 1.1111e-04 - 39s/epoch - 197ms/step
Epoch 249/1000
2023-10-28 12:50:13.762 
Epoch 249/1000 
	 loss: 43.1108, MinusLogProbMetric: 43.1108, val_loss: 43.9702, val_MinusLogProbMetric: 43.9702

Epoch 249: val_loss did not improve from 43.47440
196/196 - 36s - loss: 43.1108 - MinusLogProbMetric: 43.1108 - val_loss: 43.9702 - val_MinusLogProbMetric: 43.9702 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 250/1000
2023-10-28 12:50:48.540 
Epoch 250/1000 
	 loss: 42.7819, MinusLogProbMetric: 42.7819, val_loss: 44.5733, val_MinusLogProbMetric: 44.5733

Epoch 250: val_loss did not improve from 43.47440
196/196 - 35s - loss: 42.7819 - MinusLogProbMetric: 42.7819 - val_loss: 44.5733 - val_MinusLogProbMetric: 44.5733 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 251/1000
2023-10-28 12:51:22.882 
Epoch 251/1000 
	 loss: 43.6743, MinusLogProbMetric: 43.6743, val_loss: 44.3646, val_MinusLogProbMetric: 44.3646

Epoch 251: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.6743 - MinusLogProbMetric: 43.6743 - val_loss: 44.3646 - val_MinusLogProbMetric: 44.3646 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 252/1000
2023-10-28 12:52:01.485 
Epoch 252/1000 
	 loss: 42.7986, MinusLogProbMetric: 42.7986, val_loss: 44.5002, val_MinusLogProbMetric: 44.5002

Epoch 252: val_loss did not improve from 43.47440
196/196 - 39s - loss: 42.7986 - MinusLogProbMetric: 42.7986 - val_loss: 44.5002 - val_MinusLogProbMetric: 44.5002 - lr: 1.1111e-04 - 39s/epoch - 197ms/step
Epoch 253/1000
2023-10-28 12:52:39.212 
Epoch 253/1000 
	 loss: 42.9236, MinusLogProbMetric: 42.9236, val_loss: 45.0853, val_MinusLogProbMetric: 45.0853

Epoch 253: val_loss did not improve from 43.47440
196/196 - 38s - loss: 42.9236 - MinusLogProbMetric: 42.9236 - val_loss: 45.0853 - val_MinusLogProbMetric: 45.0853 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 254/1000
2023-10-28 12:53:13.250 
Epoch 254/1000 
	 loss: 42.8214, MinusLogProbMetric: 42.8214, val_loss: 44.5150, val_MinusLogProbMetric: 44.5150

Epoch 254: val_loss did not improve from 43.47440
196/196 - 34s - loss: 42.8214 - MinusLogProbMetric: 42.8214 - val_loss: 44.5150 - val_MinusLogProbMetric: 44.5150 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 255/1000
2023-10-28 12:53:47.296 
Epoch 255/1000 
	 loss: 43.1572, MinusLogProbMetric: 43.1572, val_loss: 44.8082, val_MinusLogProbMetric: 44.8082

Epoch 255: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.1572 - MinusLogProbMetric: 43.1572 - val_loss: 44.8082 - val_MinusLogProbMetric: 44.8082 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 256/1000
2023-10-28 12:54:22.219 
Epoch 256/1000 
	 loss: 43.2870, MinusLogProbMetric: 43.2870, val_loss: 43.9453, val_MinusLogProbMetric: 43.9453

Epoch 256: val_loss did not improve from 43.47440
196/196 - 35s - loss: 43.2870 - MinusLogProbMetric: 43.2870 - val_loss: 43.9453 - val_MinusLogProbMetric: 43.9453 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 257/1000
2023-10-28 12:54:59.978 
Epoch 257/1000 
	 loss: 42.7151, MinusLogProbMetric: 42.7151, val_loss: 44.6590, val_MinusLogProbMetric: 44.6590

Epoch 257: val_loss did not improve from 43.47440
196/196 - 38s - loss: 42.7151 - MinusLogProbMetric: 42.7151 - val_loss: 44.6590 - val_MinusLogProbMetric: 44.6590 - lr: 1.1111e-04 - 38s/epoch - 193ms/step
Epoch 258/1000
2023-10-28 12:55:34.222 
Epoch 258/1000 
	 loss: 43.4129, MinusLogProbMetric: 43.4129, val_loss: 44.1414, val_MinusLogProbMetric: 44.1414

Epoch 258: val_loss did not improve from 43.47440
196/196 - 34s - loss: 43.4129 - MinusLogProbMetric: 43.4129 - val_loss: 44.1414 - val_MinusLogProbMetric: 44.1414 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 259/1000
2023-10-28 12:56:08.578 
Epoch 259/1000 
	 loss: 42.6479, MinusLogProbMetric: 42.6479, val_loss: 43.8195, val_MinusLogProbMetric: 43.8195

Epoch 259: val_loss did not improve from 43.47440
196/196 - 34s - loss: 42.6479 - MinusLogProbMetric: 42.6479 - val_loss: 43.8195 - val_MinusLogProbMetric: 43.8195 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 260/1000
2023-10-28 12:56:43.286 
Epoch 260/1000 
	 loss: 43.2519, MinusLogProbMetric: 43.2519, val_loss: 43.9958, val_MinusLogProbMetric: 43.9958

Epoch 260: val_loss did not improve from 43.47440
196/196 - 35s - loss: 43.2519 - MinusLogProbMetric: 43.2519 - val_loss: 43.9958 - val_MinusLogProbMetric: 43.9958 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 261/1000
2023-10-28 12:57:20.843 
Epoch 261/1000 
	 loss: 42.7960, MinusLogProbMetric: 42.7960, val_loss: 44.4125, val_MinusLogProbMetric: 44.4125

Epoch 261: val_loss did not improve from 43.47440
196/196 - 38s - loss: 42.7960 - MinusLogProbMetric: 42.7960 - val_loss: 44.4125 - val_MinusLogProbMetric: 44.4125 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 262/1000
2023-10-28 12:57:55.225 
Epoch 262/1000 
	 loss: 42.8794, MinusLogProbMetric: 42.8794, val_loss: 43.4185, val_MinusLogProbMetric: 43.4185

Epoch 262: val_loss improved from 43.47440 to 43.41851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 35s - loss: 42.8794 - MinusLogProbMetric: 42.8794 - val_loss: 43.4185 - val_MinusLogProbMetric: 43.4185 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 263/1000
2023-10-28 12:58:30.424 
Epoch 263/1000 
	 loss: 42.9091, MinusLogProbMetric: 42.9091, val_loss: 43.8871, val_MinusLogProbMetric: 43.8871

Epoch 263: val_loss did not improve from 43.41851
196/196 - 34s - loss: 42.9091 - MinusLogProbMetric: 42.9091 - val_loss: 43.8871 - val_MinusLogProbMetric: 43.8871 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 264/1000
2023-10-28 12:59:05.491 
Epoch 264/1000 
	 loss: 43.0447, MinusLogProbMetric: 43.0447, val_loss: 43.5634, val_MinusLogProbMetric: 43.5634

Epoch 264: val_loss did not improve from 43.41851
196/196 - 35s - loss: 43.0447 - MinusLogProbMetric: 43.0447 - val_loss: 43.5634 - val_MinusLogProbMetric: 43.5634 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 265/1000
2023-10-28 12:59:42.911 
Epoch 265/1000 
	 loss: 42.6145, MinusLogProbMetric: 42.6145, val_loss: 43.4929, val_MinusLogProbMetric: 43.4929

Epoch 265: val_loss did not improve from 43.41851
196/196 - 37s - loss: 42.6145 - MinusLogProbMetric: 42.6145 - val_loss: 43.4929 - val_MinusLogProbMetric: 43.4929 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 266/1000
2023-10-28 13:00:19.184 
Epoch 266/1000 
	 loss: 42.9587, MinusLogProbMetric: 42.9587, val_loss: 43.6416, val_MinusLogProbMetric: 43.6416

Epoch 266: val_loss did not improve from 43.41851
196/196 - 36s - loss: 42.9587 - MinusLogProbMetric: 42.9587 - val_loss: 43.6416 - val_MinusLogProbMetric: 43.6416 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 267/1000
2023-10-28 13:00:53.684 
Epoch 267/1000 
	 loss: 42.8439, MinusLogProbMetric: 42.8439, val_loss: 43.9385, val_MinusLogProbMetric: 43.9385

Epoch 267: val_loss did not improve from 43.41851
196/196 - 34s - loss: 42.8439 - MinusLogProbMetric: 42.8439 - val_loss: 43.9385 - val_MinusLogProbMetric: 43.9385 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 268/1000
2023-10-28 13:01:28.483 
Epoch 268/1000 
	 loss: 43.1030, MinusLogProbMetric: 43.1030, val_loss: 45.5620, val_MinusLogProbMetric: 45.5620

Epoch 268: val_loss did not improve from 43.41851
196/196 - 35s - loss: 43.1030 - MinusLogProbMetric: 43.1030 - val_loss: 45.5620 - val_MinusLogProbMetric: 45.5620 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 269/1000
2023-10-28 13:02:04.338 
Epoch 269/1000 
	 loss: 42.7458, MinusLogProbMetric: 42.7458, val_loss: 44.6342, val_MinusLogProbMetric: 44.6342

Epoch 269: val_loss did not improve from 43.41851
196/196 - 36s - loss: 42.7458 - MinusLogProbMetric: 42.7458 - val_loss: 44.6342 - val_MinusLogProbMetric: 44.6342 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 270/1000
2023-10-28 13:02:38.221 
Epoch 270/1000 
	 loss: 42.7568, MinusLogProbMetric: 42.7568, val_loss: 43.6356, val_MinusLogProbMetric: 43.6356

Epoch 270: val_loss did not improve from 43.41851
196/196 - 34s - loss: 42.7568 - MinusLogProbMetric: 42.7568 - val_loss: 43.6356 - val_MinusLogProbMetric: 43.6356 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 271/1000
2023-10-28 13:03:10.580 
Epoch 271/1000 
	 loss: 42.7076, MinusLogProbMetric: 42.7076, val_loss: 43.6431, val_MinusLogProbMetric: 43.6431

Epoch 271: val_loss did not improve from 43.41851
196/196 - 32s - loss: 42.7076 - MinusLogProbMetric: 42.7076 - val_loss: 43.6431 - val_MinusLogProbMetric: 43.6431 - lr: 1.1111e-04 - 32s/epoch - 165ms/step
Epoch 272/1000
2023-10-28 13:03:44.287 
Epoch 272/1000 
	 loss: 42.8239, MinusLogProbMetric: 42.8239, val_loss: 44.1599, val_MinusLogProbMetric: 44.1599

Epoch 272: val_loss did not improve from 43.41851
196/196 - 34s - loss: 42.8239 - MinusLogProbMetric: 42.8239 - val_loss: 44.1599 - val_MinusLogProbMetric: 44.1599 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 273/1000
2023-10-28 13:04:20.606 
Epoch 273/1000 
	 loss: 42.9523, MinusLogProbMetric: 42.9523, val_loss: 44.6055, val_MinusLogProbMetric: 44.6055

Epoch 273: val_loss did not improve from 43.41851
196/196 - 36s - loss: 42.9523 - MinusLogProbMetric: 42.9523 - val_loss: 44.6055 - val_MinusLogProbMetric: 44.6055 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 274/1000
2023-10-28 13:04:57.637 
Epoch 274/1000 
	 loss: 42.9268, MinusLogProbMetric: 42.9268, val_loss: 44.6520, val_MinusLogProbMetric: 44.6520

Epoch 274: val_loss did not improve from 43.41851
196/196 - 37s - loss: 42.9268 - MinusLogProbMetric: 42.9268 - val_loss: 44.6520 - val_MinusLogProbMetric: 44.6520 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 275/1000
2023-10-28 13:05:32.427 
Epoch 275/1000 
	 loss: 42.7679, MinusLogProbMetric: 42.7679, val_loss: 43.8708, val_MinusLogProbMetric: 43.8708

Epoch 275: val_loss did not improve from 43.41851
196/196 - 35s - loss: 42.7679 - MinusLogProbMetric: 42.7679 - val_loss: 43.8708 - val_MinusLogProbMetric: 43.8708 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 276/1000
2023-10-28 13:06:06.082 
Epoch 276/1000 
	 loss: 42.6876, MinusLogProbMetric: 42.6876, val_loss: 43.8292, val_MinusLogProbMetric: 43.8292

Epoch 276: val_loss did not improve from 43.41851
196/196 - 34s - loss: 42.6876 - MinusLogProbMetric: 42.6876 - val_loss: 43.8292 - val_MinusLogProbMetric: 43.8292 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 277/1000
2023-10-28 13:06:38.041 
Epoch 277/1000 
	 loss: 42.6006, MinusLogProbMetric: 42.6006, val_loss: 43.3406, val_MinusLogProbMetric: 43.3406

Epoch 277: val_loss improved from 43.41851 to 43.34062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 33s - loss: 42.6006 - MinusLogProbMetric: 42.6006 - val_loss: 43.3406 - val_MinusLogProbMetric: 43.3406 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 278/1000
2023-10-28 13:07:14.221 
Epoch 278/1000 
	 loss: 43.1082, MinusLogProbMetric: 43.1082, val_loss: 43.7906, val_MinusLogProbMetric: 43.7906

Epoch 278: val_loss did not improve from 43.34062
196/196 - 35s - loss: 43.1082 - MinusLogProbMetric: 43.1082 - val_loss: 43.7906 - val_MinusLogProbMetric: 43.7906 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 279/1000
2023-10-28 13:07:47.644 
Epoch 279/1000 
	 loss: 42.5272, MinusLogProbMetric: 42.5272, val_loss: 44.1939, val_MinusLogProbMetric: 44.1939

Epoch 279: val_loss did not improve from 43.34062
196/196 - 33s - loss: 42.5272 - MinusLogProbMetric: 42.5272 - val_loss: 44.1939 - val_MinusLogProbMetric: 44.1939 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 280/1000
2023-10-28 13:08:20.476 
Epoch 280/1000 
	 loss: 43.4549, MinusLogProbMetric: 43.4549, val_loss: 43.4618, val_MinusLogProbMetric: 43.4618

Epoch 280: val_loss did not improve from 43.34062
196/196 - 33s - loss: 43.4549 - MinusLogProbMetric: 43.4549 - val_loss: 43.4618 - val_MinusLogProbMetric: 43.4618 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 281/1000
2023-10-28 13:08:53.853 
Epoch 281/1000 
	 loss: 42.6278, MinusLogProbMetric: 42.6278, val_loss: 45.1937, val_MinusLogProbMetric: 45.1937

Epoch 281: val_loss did not improve from 43.34062
196/196 - 33s - loss: 42.6278 - MinusLogProbMetric: 42.6278 - val_loss: 45.1937 - val_MinusLogProbMetric: 45.1937 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 282/1000
2023-10-28 13:09:31.799 
Epoch 282/1000 
	 loss: 42.7758, MinusLogProbMetric: 42.7758, val_loss: 44.1105, val_MinusLogProbMetric: 44.1105

Epoch 282: val_loss did not improve from 43.34062
196/196 - 38s - loss: 42.7758 - MinusLogProbMetric: 42.7758 - val_loss: 44.1105 - val_MinusLogProbMetric: 44.1105 - lr: 1.1111e-04 - 38s/epoch - 194ms/step
Epoch 283/1000
2023-10-28 13:10:07.304 
Epoch 283/1000 
	 loss: 42.7168, MinusLogProbMetric: 42.7168, val_loss: 44.1283, val_MinusLogProbMetric: 44.1283

Epoch 283: val_loss did not improve from 43.34062
196/196 - 36s - loss: 42.7168 - MinusLogProbMetric: 42.7168 - val_loss: 44.1283 - val_MinusLogProbMetric: 44.1283 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 284/1000
2023-10-28 13:10:38.707 
Epoch 284/1000 
	 loss: 42.8103, MinusLogProbMetric: 42.8103, val_loss: 43.6728, val_MinusLogProbMetric: 43.6728

Epoch 284: val_loss did not improve from 43.34062
196/196 - 31s - loss: 42.8103 - MinusLogProbMetric: 42.8103 - val_loss: 43.6728 - val_MinusLogProbMetric: 43.6728 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 285/1000
2023-10-28 13:11:11.671 
Epoch 285/1000 
	 loss: 42.5197, MinusLogProbMetric: 42.5197, val_loss: 44.4566, val_MinusLogProbMetric: 44.4566

Epoch 285: val_loss did not improve from 43.34062
196/196 - 33s - loss: 42.5197 - MinusLogProbMetric: 42.5197 - val_loss: 44.4566 - val_MinusLogProbMetric: 44.4566 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 286/1000
2023-10-28 13:11:43.492 
Epoch 286/1000 
	 loss: 42.9658, MinusLogProbMetric: 42.9658, val_loss: 44.6867, val_MinusLogProbMetric: 44.6867

Epoch 286: val_loss did not improve from 43.34062
196/196 - 32s - loss: 42.9658 - MinusLogProbMetric: 42.9658 - val_loss: 44.6867 - val_MinusLogProbMetric: 44.6867 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 287/1000
2023-10-28 13:12:17.578 
Epoch 287/1000 
	 loss: 42.5450, MinusLogProbMetric: 42.5450, val_loss: 43.6245, val_MinusLogProbMetric: 43.6245

Epoch 287: val_loss did not improve from 43.34062
196/196 - 34s - loss: 42.5450 - MinusLogProbMetric: 42.5450 - val_loss: 43.6245 - val_MinusLogProbMetric: 43.6245 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 288/1000
2023-10-28 13:12:49.458 
Epoch 288/1000 
	 loss: 42.4866, MinusLogProbMetric: 42.4866, val_loss: 43.3559, val_MinusLogProbMetric: 43.3559

Epoch 288: val_loss did not improve from 43.34062
196/196 - 32s - loss: 42.4866 - MinusLogProbMetric: 42.4866 - val_loss: 43.3559 - val_MinusLogProbMetric: 43.3559 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 289/1000
2023-10-28 13:13:21.269 
Epoch 289/1000 
	 loss: 42.4998, MinusLogProbMetric: 42.4998, val_loss: 45.0340, val_MinusLogProbMetric: 45.0340

Epoch 289: val_loss did not improve from 43.34062
196/196 - 32s - loss: 42.4998 - MinusLogProbMetric: 42.4998 - val_loss: 45.0340 - val_MinusLogProbMetric: 45.0340 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 290/1000
2023-10-28 13:13:53.211 
Epoch 290/1000 
	 loss: 42.8518, MinusLogProbMetric: 42.8518, val_loss: 44.6853, val_MinusLogProbMetric: 44.6853

Epoch 290: val_loss did not improve from 43.34062
196/196 - 32s - loss: 42.8518 - MinusLogProbMetric: 42.8518 - val_loss: 44.6853 - val_MinusLogProbMetric: 44.6853 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 291/1000
2023-10-28 13:14:29.850 
Epoch 291/1000 
	 loss: 42.7128, MinusLogProbMetric: 42.7128, val_loss: 44.1872, val_MinusLogProbMetric: 44.1872

Epoch 291: val_loss did not improve from 43.34062
196/196 - 37s - loss: 42.7128 - MinusLogProbMetric: 42.7128 - val_loss: 44.1872 - val_MinusLogProbMetric: 44.1872 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 292/1000
2023-10-28 13:15:02.724 
Epoch 292/1000 
	 loss: 42.6630, MinusLogProbMetric: 42.6630, val_loss: 43.4198, val_MinusLogProbMetric: 43.4198

Epoch 292: val_loss did not improve from 43.34062
196/196 - 33s - loss: 42.6630 - MinusLogProbMetric: 42.6630 - val_loss: 43.4198 - val_MinusLogProbMetric: 43.4198 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 293/1000
2023-10-28 13:15:34.763 
Epoch 293/1000 
	 loss: 42.8839, MinusLogProbMetric: 42.8839, val_loss: 43.5282, val_MinusLogProbMetric: 43.5282

Epoch 293: val_loss did not improve from 43.34062
196/196 - 32s - loss: 42.8839 - MinusLogProbMetric: 42.8839 - val_loss: 43.5282 - val_MinusLogProbMetric: 43.5282 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 294/1000
2023-10-28 13:16:09.667 
Epoch 294/1000 
	 loss: 42.6484, MinusLogProbMetric: 42.6484, val_loss: 44.4956, val_MinusLogProbMetric: 44.4956

Epoch 294: val_loss did not improve from 43.34062
196/196 - 35s - loss: 42.6484 - MinusLogProbMetric: 42.6484 - val_loss: 44.4956 - val_MinusLogProbMetric: 44.4956 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 295/1000
2023-10-28 13:16:41.129 
Epoch 295/1000 
	 loss: 42.3989, MinusLogProbMetric: 42.3989, val_loss: 44.1481, val_MinusLogProbMetric: 44.1481

Epoch 295: val_loss did not improve from 43.34062
196/196 - 31s - loss: 42.3989 - MinusLogProbMetric: 42.3989 - val_loss: 44.1481 - val_MinusLogProbMetric: 44.1481 - lr: 1.1111e-04 - 31s/epoch - 161ms/step
Epoch 296/1000
2023-10-28 13:17:14.557 
Epoch 296/1000 
	 loss: 42.8287, MinusLogProbMetric: 42.8287, val_loss: 44.0186, val_MinusLogProbMetric: 44.0186

Epoch 296: val_loss did not improve from 43.34062
196/196 - 33s - loss: 42.8287 - MinusLogProbMetric: 42.8287 - val_loss: 44.0186 - val_MinusLogProbMetric: 44.0186 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 297/1000
2023-10-28 13:17:46.048 
Epoch 297/1000 
	 loss: 42.4387, MinusLogProbMetric: 42.4387, val_loss: 44.6749, val_MinusLogProbMetric: 44.6749

Epoch 297: val_loss did not improve from 43.34062
196/196 - 31s - loss: 42.4387 - MinusLogProbMetric: 42.4387 - val_loss: 44.6749 - val_MinusLogProbMetric: 44.6749 - lr: 1.1111e-04 - 31s/epoch - 161ms/step
Epoch 298/1000
2023-10-28 13:18:17.588 
Epoch 298/1000 
	 loss: 42.6048, MinusLogProbMetric: 42.6048, val_loss: 43.8454, val_MinusLogProbMetric: 43.8454

Epoch 298: val_loss did not improve from 43.34062
196/196 - 32s - loss: 42.6048 - MinusLogProbMetric: 42.6048 - val_loss: 43.8454 - val_MinusLogProbMetric: 43.8454 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 299/1000
2023-10-28 13:18:51.109 
Epoch 299/1000 
	 loss: 42.5155, MinusLogProbMetric: 42.5155, val_loss: 44.2816, val_MinusLogProbMetric: 44.2816

Epoch 299: val_loss did not improve from 43.34062
196/196 - 34s - loss: 42.5155 - MinusLogProbMetric: 42.5155 - val_loss: 44.2816 - val_MinusLogProbMetric: 44.2816 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 300/1000
2023-10-28 13:19:24.293 
Epoch 300/1000 
	 loss: 42.3809, MinusLogProbMetric: 42.3809, val_loss: 44.1909, val_MinusLogProbMetric: 44.1909

Epoch 300: val_loss did not improve from 43.34062
196/196 - 33s - loss: 42.3809 - MinusLogProbMetric: 42.3809 - val_loss: 44.1909 - val_MinusLogProbMetric: 44.1909 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 301/1000
2023-10-28 13:19:59.082 
Epoch 301/1000 
	 loss: 42.4959, MinusLogProbMetric: 42.4959, val_loss: 43.1123, val_MinusLogProbMetric: 43.1123

Epoch 301: val_loss improved from 43.34062 to 43.11226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 36s - loss: 42.4959 - MinusLogProbMetric: 42.4959 - val_loss: 43.1123 - val_MinusLogProbMetric: 43.1123 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 302/1000
2023-10-28 13:20:35.222 
Epoch 302/1000 
	 loss: 42.5202, MinusLogProbMetric: 42.5202, val_loss: 45.2515, val_MinusLogProbMetric: 45.2515

Epoch 302: val_loss did not improve from 43.11226
196/196 - 35s - loss: 42.5202 - MinusLogProbMetric: 42.5202 - val_loss: 45.2515 - val_MinusLogProbMetric: 45.2515 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 303/1000
2023-10-28 13:21:10.412 
Epoch 303/1000 
	 loss: 42.4577, MinusLogProbMetric: 42.4577, val_loss: 43.4917, val_MinusLogProbMetric: 43.4917

Epoch 303: val_loss did not improve from 43.11226
196/196 - 35s - loss: 42.4577 - MinusLogProbMetric: 42.4577 - val_loss: 43.4917 - val_MinusLogProbMetric: 43.4917 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 304/1000
2023-10-28 13:21:50.301 
Epoch 304/1000 
	 loss: 42.5061, MinusLogProbMetric: 42.5061, val_loss: 43.8143, val_MinusLogProbMetric: 43.8143

Epoch 304: val_loss did not improve from 43.11226
196/196 - 40s - loss: 42.5061 - MinusLogProbMetric: 42.5061 - val_loss: 43.8143 - val_MinusLogProbMetric: 43.8143 - lr: 1.1111e-04 - 40s/epoch - 203ms/step
Epoch 305/1000
2023-10-28 13:22:32.036 
Epoch 305/1000 
	 loss: 42.4046, MinusLogProbMetric: 42.4046, val_loss: 44.2693, val_MinusLogProbMetric: 44.2693

Epoch 305: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.4046 - MinusLogProbMetric: 42.4046 - val_loss: 44.2693 - val_MinusLogProbMetric: 44.2693 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 306/1000
2023-10-28 13:23:13.380 
Epoch 306/1000 
	 loss: 42.4688, MinusLogProbMetric: 42.4688, val_loss: 43.5461, val_MinusLogProbMetric: 43.5461

Epoch 306: val_loss did not improve from 43.11226
196/196 - 41s - loss: 42.4688 - MinusLogProbMetric: 42.4688 - val_loss: 43.5461 - val_MinusLogProbMetric: 43.5461 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 307/1000
2023-10-28 13:23:55.245 
Epoch 307/1000 
	 loss: 42.3898, MinusLogProbMetric: 42.3898, val_loss: 43.3274, val_MinusLogProbMetric: 43.3274

Epoch 307: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.3898 - MinusLogProbMetric: 42.3898 - val_loss: 43.3274 - val_MinusLogProbMetric: 43.3274 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 308/1000
2023-10-28 13:24:38.126 
Epoch 308/1000 
	 loss: 42.5743, MinusLogProbMetric: 42.5743, val_loss: 43.6582, val_MinusLogProbMetric: 43.6582

Epoch 308: val_loss did not improve from 43.11226
196/196 - 43s - loss: 42.5743 - MinusLogProbMetric: 42.5743 - val_loss: 43.6582 - val_MinusLogProbMetric: 43.6582 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 309/1000
2023-10-28 13:25:20.575 
Epoch 309/1000 
	 loss: 42.3281, MinusLogProbMetric: 42.3281, val_loss: 44.1112, val_MinusLogProbMetric: 44.1112

Epoch 309: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.3281 - MinusLogProbMetric: 42.3281 - val_loss: 44.1112 - val_MinusLogProbMetric: 44.1112 - lr: 1.1111e-04 - 42s/epoch - 217ms/step
Epoch 310/1000
2023-10-28 13:26:02.687 
Epoch 310/1000 
	 loss: 43.0184, MinusLogProbMetric: 43.0184, val_loss: 43.5989, val_MinusLogProbMetric: 43.5989

Epoch 310: val_loss did not improve from 43.11226
196/196 - 42s - loss: 43.0184 - MinusLogProbMetric: 43.0184 - val_loss: 43.5989 - val_MinusLogProbMetric: 43.5989 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 311/1000
2023-10-28 13:26:45.249 
Epoch 311/1000 
	 loss: 42.3642, MinusLogProbMetric: 42.3642, val_loss: 45.1265, val_MinusLogProbMetric: 45.1265

Epoch 311: val_loss did not improve from 43.11226
196/196 - 43s - loss: 42.3642 - MinusLogProbMetric: 42.3642 - val_loss: 45.1265 - val_MinusLogProbMetric: 45.1265 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 312/1000
2023-10-28 13:27:27.493 
Epoch 312/1000 
	 loss: 42.5785, MinusLogProbMetric: 42.5785, val_loss: 44.0300, val_MinusLogProbMetric: 44.0300

Epoch 312: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.5785 - MinusLogProbMetric: 42.5785 - val_loss: 44.0300 - val_MinusLogProbMetric: 44.0300 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 313/1000
2023-10-28 13:28:10.002 
Epoch 313/1000 
	 loss: 43.0186, MinusLogProbMetric: 43.0186, val_loss: 44.6206, val_MinusLogProbMetric: 44.6206

Epoch 313: val_loss did not improve from 43.11226
196/196 - 43s - loss: 43.0186 - MinusLogProbMetric: 43.0186 - val_loss: 44.6206 - val_MinusLogProbMetric: 44.6206 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 314/1000
2023-10-28 13:28:52.038 
Epoch 314/1000 
	 loss: 42.3544, MinusLogProbMetric: 42.3544, val_loss: 44.1109, val_MinusLogProbMetric: 44.1109

Epoch 314: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.3544 - MinusLogProbMetric: 42.3544 - val_loss: 44.1109 - val_MinusLogProbMetric: 44.1109 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 315/1000
2023-10-28 13:29:34.411 
Epoch 315/1000 
	 loss: 42.3226, MinusLogProbMetric: 42.3226, val_loss: 44.5772, val_MinusLogProbMetric: 44.5772

Epoch 315: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.3226 - MinusLogProbMetric: 42.3226 - val_loss: 44.5772 - val_MinusLogProbMetric: 44.5772 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 316/1000
2023-10-28 13:30:16.449 
Epoch 316/1000 
	 loss: 42.4237, MinusLogProbMetric: 42.4237, val_loss: 43.6345, val_MinusLogProbMetric: 43.6345

Epoch 316: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.4237 - MinusLogProbMetric: 42.4237 - val_loss: 43.6345 - val_MinusLogProbMetric: 43.6345 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 317/1000
2023-10-28 13:30:58.775 
Epoch 317/1000 
	 loss: 42.5235, MinusLogProbMetric: 42.5235, val_loss: 44.1916, val_MinusLogProbMetric: 44.1916

Epoch 317: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.5235 - MinusLogProbMetric: 42.5235 - val_loss: 44.1916 - val_MinusLogProbMetric: 44.1916 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 318/1000
2023-10-28 13:31:41.474 
Epoch 318/1000 
	 loss: 42.4426, MinusLogProbMetric: 42.4426, val_loss: 44.7722, val_MinusLogProbMetric: 44.7722

Epoch 318: val_loss did not improve from 43.11226
196/196 - 43s - loss: 42.4426 - MinusLogProbMetric: 42.4426 - val_loss: 44.7722 - val_MinusLogProbMetric: 44.7722 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 319/1000
2023-10-28 13:32:24.395 
Epoch 319/1000 
	 loss: 42.4968, MinusLogProbMetric: 42.4968, val_loss: 43.4822, val_MinusLogProbMetric: 43.4822

Epoch 319: val_loss did not improve from 43.11226
196/196 - 43s - loss: 42.4968 - MinusLogProbMetric: 42.4968 - val_loss: 43.4822 - val_MinusLogProbMetric: 43.4822 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 320/1000
2023-10-28 13:33:06.667 
Epoch 320/1000 
	 loss: 42.8665, MinusLogProbMetric: 42.8665, val_loss: 44.9257, val_MinusLogProbMetric: 44.9257

Epoch 320: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.8665 - MinusLogProbMetric: 42.8665 - val_loss: 44.9257 - val_MinusLogProbMetric: 44.9257 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 321/1000
2023-10-28 13:33:48.622 
Epoch 321/1000 
	 loss: 42.4199, MinusLogProbMetric: 42.4199, val_loss: 44.1908, val_MinusLogProbMetric: 44.1908

Epoch 321: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.4199 - MinusLogProbMetric: 42.4199 - val_loss: 44.1908 - val_MinusLogProbMetric: 44.1908 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 322/1000
2023-10-28 13:34:31.391 
Epoch 322/1000 
	 loss: 42.3943, MinusLogProbMetric: 42.3943, val_loss: 43.7032, val_MinusLogProbMetric: 43.7032

Epoch 322: val_loss did not improve from 43.11226
196/196 - 43s - loss: 42.3943 - MinusLogProbMetric: 42.3943 - val_loss: 43.7032 - val_MinusLogProbMetric: 43.7032 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 323/1000
2023-10-28 13:35:14.218 
Epoch 323/1000 
	 loss: 42.3925, MinusLogProbMetric: 42.3925, val_loss: 43.1698, val_MinusLogProbMetric: 43.1698

Epoch 323: val_loss did not improve from 43.11226
196/196 - 43s - loss: 42.3925 - MinusLogProbMetric: 42.3925 - val_loss: 43.1698 - val_MinusLogProbMetric: 43.1698 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 324/1000
2023-10-28 13:35:56.967 
Epoch 324/1000 
	 loss: 42.1049, MinusLogProbMetric: 42.1049, val_loss: 44.4063, val_MinusLogProbMetric: 44.4063

Epoch 324: val_loss did not improve from 43.11226
196/196 - 43s - loss: 42.1049 - MinusLogProbMetric: 42.1049 - val_loss: 44.4063 - val_MinusLogProbMetric: 44.4063 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 325/1000
2023-10-28 13:36:39.234 
Epoch 325/1000 
	 loss: 42.5002, MinusLogProbMetric: 42.5002, val_loss: 43.8191, val_MinusLogProbMetric: 43.8191

Epoch 325: val_loss did not improve from 43.11226
196/196 - 42s - loss: 42.5002 - MinusLogProbMetric: 42.5002 - val_loss: 43.8191 - val_MinusLogProbMetric: 43.8191 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 326/1000
2023-10-28 13:37:21.456 
Epoch 326/1000 
	 loss: 42.4205, MinusLogProbMetric: 42.4205, val_loss: 43.0181, val_MinusLogProbMetric: 43.0181

Epoch 326: val_loss improved from 43.11226 to 43.01814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 43s - loss: 42.4205 - MinusLogProbMetric: 42.4205 - val_loss: 43.0181 - val_MinusLogProbMetric: 43.0181 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 327/1000
2023-10-28 13:38:04.399 
Epoch 327/1000 
	 loss: 42.7956, MinusLogProbMetric: 42.7956, val_loss: 43.2779, val_MinusLogProbMetric: 43.2779

Epoch 327: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.7956 - MinusLogProbMetric: 42.7956 - val_loss: 43.2779 - val_MinusLogProbMetric: 43.2779 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 328/1000
2023-10-28 13:38:46.601 
Epoch 328/1000 
	 loss: 42.1067, MinusLogProbMetric: 42.1067, val_loss: 45.0041, val_MinusLogProbMetric: 45.0041

Epoch 328: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.1067 - MinusLogProbMetric: 42.1067 - val_loss: 45.0041 - val_MinusLogProbMetric: 45.0041 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 329/1000
2023-10-28 13:39:28.927 
Epoch 329/1000 
	 loss: 42.3514, MinusLogProbMetric: 42.3514, val_loss: 44.1650, val_MinusLogProbMetric: 44.1650

Epoch 329: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.3514 - MinusLogProbMetric: 42.3514 - val_loss: 44.1650 - val_MinusLogProbMetric: 44.1650 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 330/1000
2023-10-28 13:40:11.474 
Epoch 330/1000 
	 loss: 42.3201, MinusLogProbMetric: 42.3201, val_loss: 43.1114, val_MinusLogProbMetric: 43.1114

Epoch 330: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.3201 - MinusLogProbMetric: 42.3201 - val_loss: 43.1114 - val_MinusLogProbMetric: 43.1114 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 331/1000
2023-10-28 13:40:53.092 
Epoch 331/1000 
	 loss: 42.3752, MinusLogProbMetric: 42.3752, val_loss: 43.4115, val_MinusLogProbMetric: 43.4115

Epoch 331: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.3752 - MinusLogProbMetric: 42.3752 - val_loss: 43.4115 - val_MinusLogProbMetric: 43.4115 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 332/1000
2023-10-28 13:41:34.656 
Epoch 332/1000 
	 loss: 42.7790, MinusLogProbMetric: 42.7790, val_loss: 43.5035, val_MinusLogProbMetric: 43.5035

Epoch 332: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.7790 - MinusLogProbMetric: 42.7790 - val_loss: 43.5035 - val_MinusLogProbMetric: 43.5035 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 333/1000
2023-10-28 13:42:17.836 
Epoch 333/1000 
	 loss: 42.4766, MinusLogProbMetric: 42.4766, val_loss: 44.0355, val_MinusLogProbMetric: 44.0355

Epoch 333: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.4766 - MinusLogProbMetric: 42.4766 - val_loss: 44.0355 - val_MinusLogProbMetric: 44.0355 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 334/1000
2023-10-28 13:43:00.577 
Epoch 334/1000 
	 loss: 42.1157, MinusLogProbMetric: 42.1157, val_loss: 43.2946, val_MinusLogProbMetric: 43.2946

Epoch 334: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.1157 - MinusLogProbMetric: 42.1157 - val_loss: 43.2946 - val_MinusLogProbMetric: 43.2946 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 335/1000
2023-10-28 13:43:42.338 
Epoch 335/1000 
	 loss: 42.3381, MinusLogProbMetric: 42.3381, val_loss: 43.4404, val_MinusLogProbMetric: 43.4404

Epoch 335: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.3381 - MinusLogProbMetric: 42.3381 - val_loss: 43.4404 - val_MinusLogProbMetric: 43.4404 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 336/1000
2023-10-28 13:44:24.859 
Epoch 336/1000 
	 loss: 43.2549, MinusLogProbMetric: 43.2549, val_loss: 43.8589, val_MinusLogProbMetric: 43.8589

Epoch 336: val_loss did not improve from 43.01814
196/196 - 43s - loss: 43.2549 - MinusLogProbMetric: 43.2549 - val_loss: 43.8589 - val_MinusLogProbMetric: 43.8589 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 337/1000
2023-10-28 13:45:07.059 
Epoch 337/1000 
	 loss: 42.3501, MinusLogProbMetric: 42.3501, val_loss: 43.5780, val_MinusLogProbMetric: 43.5780

Epoch 337: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.3501 - MinusLogProbMetric: 42.3501 - val_loss: 43.5780 - val_MinusLogProbMetric: 43.5780 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 338/1000
2023-10-28 13:45:49.236 
Epoch 338/1000 
	 loss: 41.9256, MinusLogProbMetric: 41.9256, val_loss: 43.0364, val_MinusLogProbMetric: 43.0364

Epoch 338: val_loss did not improve from 43.01814
196/196 - 42s - loss: 41.9256 - MinusLogProbMetric: 41.9256 - val_loss: 43.0364 - val_MinusLogProbMetric: 43.0364 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 339/1000
2023-10-28 13:46:30.961 
Epoch 339/1000 
	 loss: 42.2634, MinusLogProbMetric: 42.2634, val_loss: 43.5600, val_MinusLogProbMetric: 43.5600

Epoch 339: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.2634 - MinusLogProbMetric: 42.2634 - val_loss: 43.5600 - val_MinusLogProbMetric: 43.5600 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 340/1000
2023-10-28 13:47:12.719 
Epoch 340/1000 
	 loss: 42.1002, MinusLogProbMetric: 42.1002, val_loss: 43.2948, val_MinusLogProbMetric: 43.2948

Epoch 340: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.1002 - MinusLogProbMetric: 42.1002 - val_loss: 43.2948 - val_MinusLogProbMetric: 43.2948 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 341/1000
2023-10-28 13:47:55.296 
Epoch 341/1000 
	 loss: 42.6594, MinusLogProbMetric: 42.6594, val_loss: 43.3243, val_MinusLogProbMetric: 43.3243

Epoch 341: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.6594 - MinusLogProbMetric: 42.6594 - val_loss: 43.3243 - val_MinusLogProbMetric: 43.3243 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 342/1000
2023-10-28 13:48:37.523 
Epoch 342/1000 
	 loss: 42.1780, MinusLogProbMetric: 42.1780, val_loss: 43.2495, val_MinusLogProbMetric: 43.2495

Epoch 342: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.1780 - MinusLogProbMetric: 42.1780 - val_loss: 43.2495 - val_MinusLogProbMetric: 43.2495 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 343/1000
2023-10-28 13:49:20.033 
Epoch 343/1000 
	 loss: 42.1146, MinusLogProbMetric: 42.1146, val_loss: 44.0619, val_MinusLogProbMetric: 44.0619

Epoch 343: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.1146 - MinusLogProbMetric: 42.1146 - val_loss: 44.0619 - val_MinusLogProbMetric: 44.0619 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 344/1000
2023-10-28 13:50:03.142 
Epoch 344/1000 
	 loss: 42.3517, MinusLogProbMetric: 42.3517, val_loss: 44.4041, val_MinusLogProbMetric: 44.4041

Epoch 344: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.3517 - MinusLogProbMetric: 42.3517 - val_loss: 44.4041 - val_MinusLogProbMetric: 44.4041 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 345/1000
2023-10-28 13:50:45.183 
Epoch 345/1000 
	 loss: 42.1749, MinusLogProbMetric: 42.1749, val_loss: 44.1112, val_MinusLogProbMetric: 44.1112

Epoch 345: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.1749 - MinusLogProbMetric: 42.1749 - val_loss: 44.1112 - val_MinusLogProbMetric: 44.1112 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 346/1000
2023-10-28 13:51:26.821 
Epoch 346/1000 
	 loss: 42.1833, MinusLogProbMetric: 42.1833, val_loss: 43.5400, val_MinusLogProbMetric: 43.5400

Epoch 346: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.1833 - MinusLogProbMetric: 42.1833 - val_loss: 43.5400 - val_MinusLogProbMetric: 43.5400 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 347/1000
2023-10-28 13:52:09.044 
Epoch 347/1000 
	 loss: 42.0238, MinusLogProbMetric: 42.0238, val_loss: 43.4266, val_MinusLogProbMetric: 43.4266

Epoch 347: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.0238 - MinusLogProbMetric: 42.0238 - val_loss: 43.4266 - val_MinusLogProbMetric: 43.4266 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 348/1000
2023-10-28 13:52:51.887 
Epoch 348/1000 
	 loss: 42.4424, MinusLogProbMetric: 42.4424, val_loss: 45.7529, val_MinusLogProbMetric: 45.7529

Epoch 348: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.4424 - MinusLogProbMetric: 42.4424 - val_loss: 45.7529 - val_MinusLogProbMetric: 45.7529 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 349/1000
2023-10-28 13:53:34.934 
Epoch 349/1000 
	 loss: 42.1360, MinusLogProbMetric: 42.1360, val_loss: 43.6349, val_MinusLogProbMetric: 43.6349

Epoch 349: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.1360 - MinusLogProbMetric: 42.1360 - val_loss: 43.6349 - val_MinusLogProbMetric: 43.6349 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 350/1000
2023-10-28 13:54:18.069 
Epoch 350/1000 
	 loss: 41.9538, MinusLogProbMetric: 41.9538, val_loss: 43.1663, val_MinusLogProbMetric: 43.1663

Epoch 350: val_loss did not improve from 43.01814
196/196 - 43s - loss: 41.9538 - MinusLogProbMetric: 41.9538 - val_loss: 43.1663 - val_MinusLogProbMetric: 43.1663 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 351/1000
2023-10-28 13:55:01.004 
Epoch 351/1000 
	 loss: 42.0723, MinusLogProbMetric: 42.0723, val_loss: 43.7367, val_MinusLogProbMetric: 43.7367

Epoch 351: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.0723 - MinusLogProbMetric: 42.0723 - val_loss: 43.7367 - val_MinusLogProbMetric: 43.7367 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 352/1000
2023-10-28 13:55:43.652 
Epoch 352/1000 
	 loss: 42.3512, MinusLogProbMetric: 42.3512, val_loss: 43.6739, val_MinusLogProbMetric: 43.6739

Epoch 352: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.3512 - MinusLogProbMetric: 42.3512 - val_loss: 43.6739 - val_MinusLogProbMetric: 43.6739 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 353/1000
2023-10-28 13:56:25.953 
Epoch 353/1000 
	 loss: 42.2113, MinusLogProbMetric: 42.2113, val_loss: 43.3435, val_MinusLogProbMetric: 43.3435

Epoch 353: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.2113 - MinusLogProbMetric: 42.2113 - val_loss: 43.3435 - val_MinusLogProbMetric: 43.3435 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 354/1000
2023-10-28 13:57:08.204 
Epoch 354/1000 
	 loss: 42.1439, MinusLogProbMetric: 42.1439, val_loss: 43.3304, val_MinusLogProbMetric: 43.3304

Epoch 354: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.1439 - MinusLogProbMetric: 42.1439 - val_loss: 43.3304 - val_MinusLogProbMetric: 43.3304 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 355/1000
2023-10-28 13:57:50.661 
Epoch 355/1000 
	 loss: 42.2201, MinusLogProbMetric: 42.2201, val_loss: 43.3879, val_MinusLogProbMetric: 43.3879

Epoch 355: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.2201 - MinusLogProbMetric: 42.2201 - val_loss: 43.3879 - val_MinusLogProbMetric: 43.3879 - lr: 1.1111e-04 - 42s/epoch - 217ms/step
Epoch 356/1000
2023-10-28 13:58:32.876 
Epoch 356/1000 
	 loss: 41.9705, MinusLogProbMetric: 41.9705, val_loss: 43.3197, val_MinusLogProbMetric: 43.3197

Epoch 356: val_loss did not improve from 43.01814
196/196 - 42s - loss: 41.9705 - MinusLogProbMetric: 41.9705 - val_loss: 43.3197 - val_MinusLogProbMetric: 43.3197 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 357/1000
2023-10-28 13:59:15.246 
Epoch 357/1000 
	 loss: 42.0315, MinusLogProbMetric: 42.0315, val_loss: 43.4420, val_MinusLogProbMetric: 43.4420

Epoch 357: val_loss did not improve from 43.01814
196/196 - 42s - loss: 42.0315 - MinusLogProbMetric: 42.0315 - val_loss: 43.4420 - val_MinusLogProbMetric: 43.4420 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 358/1000
2023-10-28 13:59:57.773 
Epoch 358/1000 
	 loss: 42.1824, MinusLogProbMetric: 42.1824, val_loss: 44.1251, val_MinusLogProbMetric: 44.1251

Epoch 358: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.1824 - MinusLogProbMetric: 42.1824 - val_loss: 44.1251 - val_MinusLogProbMetric: 44.1251 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 359/1000
2023-10-28 14:00:40.280 
Epoch 359/1000 
	 loss: 41.9754, MinusLogProbMetric: 41.9754, val_loss: 43.4540, val_MinusLogProbMetric: 43.4540

Epoch 359: val_loss did not improve from 43.01814
196/196 - 43s - loss: 41.9754 - MinusLogProbMetric: 41.9754 - val_loss: 43.4540 - val_MinusLogProbMetric: 43.4540 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 360/1000
2023-10-28 14:01:23.171 
Epoch 360/1000 
	 loss: 41.9212, MinusLogProbMetric: 41.9212, val_loss: 43.5024, val_MinusLogProbMetric: 43.5024

Epoch 360: val_loss did not improve from 43.01814
196/196 - 43s - loss: 41.9212 - MinusLogProbMetric: 41.9212 - val_loss: 43.5024 - val_MinusLogProbMetric: 43.5024 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 361/1000
2023-10-28 14:02:05.450 
Epoch 361/1000 
	 loss: 41.8682, MinusLogProbMetric: 41.8682, val_loss: 43.5002, val_MinusLogProbMetric: 43.5002

Epoch 361: val_loss did not improve from 43.01814
196/196 - 42s - loss: 41.8682 - MinusLogProbMetric: 41.8682 - val_loss: 43.5002 - val_MinusLogProbMetric: 43.5002 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 362/1000
2023-10-28 14:02:48.609 
Epoch 362/1000 
	 loss: 42.3034, MinusLogProbMetric: 42.3034, val_loss: 43.7767, val_MinusLogProbMetric: 43.7767

Epoch 362: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.3034 - MinusLogProbMetric: 42.3034 - val_loss: 43.7767 - val_MinusLogProbMetric: 43.7767 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 363/1000
2023-10-28 14:03:32.287 
Epoch 363/1000 
	 loss: 42.0587, MinusLogProbMetric: 42.0587, val_loss: 44.6069, val_MinusLogProbMetric: 44.6069

Epoch 363: val_loss did not improve from 43.01814
196/196 - 44s - loss: 42.0587 - MinusLogProbMetric: 42.0587 - val_loss: 44.6069 - val_MinusLogProbMetric: 44.6069 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 364/1000
2023-10-28 14:04:15.197 
Epoch 364/1000 
	 loss: 42.2888, MinusLogProbMetric: 42.2888, val_loss: 43.5215, val_MinusLogProbMetric: 43.5215

Epoch 364: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.2888 - MinusLogProbMetric: 42.2888 - val_loss: 43.5215 - val_MinusLogProbMetric: 43.5215 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 365/1000
2023-10-28 14:04:58.066 
Epoch 365/1000 
	 loss: 42.4503, MinusLogProbMetric: 42.4503, val_loss: 45.5658, val_MinusLogProbMetric: 45.5658

Epoch 365: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.4503 - MinusLogProbMetric: 42.4503 - val_loss: 45.5658 - val_MinusLogProbMetric: 45.5658 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 366/1000
2023-10-28 14:05:41.214 
Epoch 366/1000 
	 loss: 41.9825, MinusLogProbMetric: 41.9825, val_loss: 43.7630, val_MinusLogProbMetric: 43.7630

Epoch 366: val_loss did not improve from 43.01814
196/196 - 43s - loss: 41.9825 - MinusLogProbMetric: 41.9825 - val_loss: 43.7630 - val_MinusLogProbMetric: 43.7630 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 367/1000
2023-10-28 14:06:23.227 
Epoch 367/1000 
	 loss: 41.9279, MinusLogProbMetric: 41.9279, val_loss: 43.1868, val_MinusLogProbMetric: 43.1868

Epoch 367: val_loss did not improve from 43.01814
196/196 - 42s - loss: 41.9279 - MinusLogProbMetric: 41.9279 - val_loss: 43.1868 - val_MinusLogProbMetric: 43.1868 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 368/1000
2023-10-28 14:07:06.151 
Epoch 368/1000 
	 loss: 41.9177, MinusLogProbMetric: 41.9177, val_loss: 43.9257, val_MinusLogProbMetric: 43.9257

Epoch 368: val_loss did not improve from 43.01814
196/196 - 43s - loss: 41.9177 - MinusLogProbMetric: 41.9177 - val_loss: 43.9257 - val_MinusLogProbMetric: 43.9257 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 369/1000
2023-10-28 14:07:48.934 
Epoch 369/1000 
	 loss: 42.3252, MinusLogProbMetric: 42.3252, val_loss: 43.0906, val_MinusLogProbMetric: 43.0906

Epoch 369: val_loss did not improve from 43.01814
196/196 - 43s - loss: 42.3252 - MinusLogProbMetric: 42.3252 - val_loss: 43.0906 - val_MinusLogProbMetric: 43.0906 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 370/1000
2023-10-28 14:08:32.112 
Epoch 370/1000 
	 loss: 41.8635, MinusLogProbMetric: 41.8635, val_loss: 42.9494, val_MinusLogProbMetric: 42.9494

Epoch 370: val_loss improved from 43.01814 to 42.94943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 44s - loss: 41.8635 - MinusLogProbMetric: 41.8635 - val_loss: 42.9494 - val_MinusLogProbMetric: 42.9494 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 371/1000
2023-10-28 14:09:15.866 
Epoch 371/1000 
	 loss: 42.3812, MinusLogProbMetric: 42.3812, val_loss: 43.2367, val_MinusLogProbMetric: 43.2367

Epoch 371: val_loss did not improve from 42.94943
196/196 - 43s - loss: 42.3812 - MinusLogProbMetric: 42.3812 - val_loss: 43.2367 - val_MinusLogProbMetric: 43.2367 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 372/1000
2023-10-28 14:09:58.749 
Epoch 372/1000 
	 loss: 42.1527, MinusLogProbMetric: 42.1527, val_loss: 47.9231, val_MinusLogProbMetric: 47.9231

Epoch 372: val_loss did not improve from 42.94943
196/196 - 43s - loss: 42.1527 - MinusLogProbMetric: 42.1527 - val_loss: 47.9231 - val_MinusLogProbMetric: 47.9231 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 373/1000
2023-10-28 14:10:41.320 
Epoch 373/1000 
	 loss: 42.5279, MinusLogProbMetric: 42.5279, val_loss: 45.1213, val_MinusLogProbMetric: 45.1213

Epoch 373: val_loss did not improve from 42.94943
196/196 - 43s - loss: 42.5279 - MinusLogProbMetric: 42.5279 - val_loss: 45.1213 - val_MinusLogProbMetric: 45.1213 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 374/1000
2023-10-28 14:11:23.994 
Epoch 374/1000 
	 loss: 41.8267, MinusLogProbMetric: 41.8267, val_loss: 43.7700, val_MinusLogProbMetric: 43.7700

Epoch 374: val_loss did not improve from 42.94943
196/196 - 43s - loss: 41.8267 - MinusLogProbMetric: 41.8267 - val_loss: 43.7700 - val_MinusLogProbMetric: 43.7700 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 375/1000
2023-10-28 14:12:06.032 
Epoch 375/1000 
	 loss: 42.2540, MinusLogProbMetric: 42.2540, val_loss: 43.6501, val_MinusLogProbMetric: 43.6501

Epoch 375: val_loss did not improve from 42.94943
196/196 - 42s - loss: 42.2540 - MinusLogProbMetric: 42.2540 - val_loss: 43.6501 - val_MinusLogProbMetric: 43.6501 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 376/1000
2023-10-28 14:12:48.617 
Epoch 376/1000 
	 loss: 41.7681, MinusLogProbMetric: 41.7681, val_loss: 43.3975, val_MinusLogProbMetric: 43.3975

Epoch 376: val_loss did not improve from 42.94943
196/196 - 43s - loss: 41.7681 - MinusLogProbMetric: 41.7681 - val_loss: 43.3975 - val_MinusLogProbMetric: 43.3975 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 377/1000
2023-10-28 14:13:30.845 
Epoch 377/1000 
	 loss: 41.9098, MinusLogProbMetric: 41.9098, val_loss: 43.0304, val_MinusLogProbMetric: 43.0304

Epoch 377: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.9098 - MinusLogProbMetric: 41.9098 - val_loss: 43.0304 - val_MinusLogProbMetric: 43.0304 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 378/1000
2023-10-28 14:14:13.648 
Epoch 378/1000 
	 loss: 42.1133, MinusLogProbMetric: 42.1133, val_loss: 43.6971, val_MinusLogProbMetric: 43.6971

Epoch 378: val_loss did not improve from 42.94943
196/196 - 43s - loss: 42.1133 - MinusLogProbMetric: 42.1133 - val_loss: 43.6971 - val_MinusLogProbMetric: 43.6971 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 379/1000
2023-10-28 14:14:56.147 
Epoch 379/1000 
	 loss: 41.8114, MinusLogProbMetric: 41.8114, val_loss: 43.5062, val_MinusLogProbMetric: 43.5062

Epoch 379: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.8114 - MinusLogProbMetric: 41.8114 - val_loss: 43.5062 - val_MinusLogProbMetric: 43.5062 - lr: 1.1111e-04 - 42s/epoch - 217ms/step
Epoch 380/1000
2023-10-28 14:15:38.840 
Epoch 380/1000 
	 loss: 42.1347, MinusLogProbMetric: 42.1347, val_loss: 43.1553, val_MinusLogProbMetric: 43.1553

Epoch 380: val_loss did not improve from 42.94943
196/196 - 43s - loss: 42.1347 - MinusLogProbMetric: 42.1347 - val_loss: 43.1553 - val_MinusLogProbMetric: 43.1553 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 381/1000
2023-10-28 14:16:21.418 
Epoch 381/1000 
	 loss: 41.7817, MinusLogProbMetric: 41.7817, val_loss: 43.8181, val_MinusLogProbMetric: 43.8181

Epoch 381: val_loss did not improve from 42.94943
196/196 - 43s - loss: 41.7817 - MinusLogProbMetric: 41.7817 - val_loss: 43.8181 - val_MinusLogProbMetric: 43.8181 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 382/1000
2023-10-28 14:17:03.765 
Epoch 382/1000 
	 loss: 41.9333, MinusLogProbMetric: 41.9333, val_loss: 43.6987, val_MinusLogProbMetric: 43.6987

Epoch 382: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.9333 - MinusLogProbMetric: 41.9333 - val_loss: 43.6987 - val_MinusLogProbMetric: 43.6987 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 383/1000
2023-10-28 14:17:45.707 
Epoch 383/1000 
	 loss: 41.9129, MinusLogProbMetric: 41.9129, val_loss: 43.1997, val_MinusLogProbMetric: 43.1997

Epoch 383: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.9129 - MinusLogProbMetric: 41.9129 - val_loss: 43.1997 - val_MinusLogProbMetric: 43.1997 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 384/1000
2023-10-28 14:18:27.379 
Epoch 384/1000 
	 loss: 41.9686, MinusLogProbMetric: 41.9686, val_loss: 43.4640, val_MinusLogProbMetric: 43.4640

Epoch 384: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.9686 - MinusLogProbMetric: 41.9686 - val_loss: 43.4640 - val_MinusLogProbMetric: 43.4640 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 385/1000
2023-10-28 14:19:09.926 
Epoch 385/1000 
	 loss: 41.7080, MinusLogProbMetric: 41.7080, val_loss: 43.1599, val_MinusLogProbMetric: 43.1599

Epoch 385: val_loss did not improve from 42.94943
196/196 - 43s - loss: 41.7080 - MinusLogProbMetric: 41.7080 - val_loss: 43.1599 - val_MinusLogProbMetric: 43.1599 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 386/1000
2023-10-28 14:19:52.265 
Epoch 386/1000 
	 loss: 41.6959, MinusLogProbMetric: 41.6959, val_loss: 43.9492, val_MinusLogProbMetric: 43.9492

Epoch 386: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.6959 - MinusLogProbMetric: 41.6959 - val_loss: 43.9492 - val_MinusLogProbMetric: 43.9492 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 387/1000
2023-10-28 14:20:34.258 
Epoch 387/1000 
	 loss: 42.4821, MinusLogProbMetric: 42.4821, val_loss: 45.0578, val_MinusLogProbMetric: 45.0578

Epoch 387: val_loss did not improve from 42.94943
196/196 - 42s - loss: 42.4821 - MinusLogProbMetric: 42.4821 - val_loss: 45.0578 - val_MinusLogProbMetric: 45.0578 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 388/1000
2023-10-28 14:21:16.608 
Epoch 388/1000 
	 loss: 42.1299, MinusLogProbMetric: 42.1299, val_loss: 43.7820, val_MinusLogProbMetric: 43.7820

Epoch 388: val_loss did not improve from 42.94943
196/196 - 42s - loss: 42.1299 - MinusLogProbMetric: 42.1299 - val_loss: 43.7820 - val_MinusLogProbMetric: 43.7820 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 389/1000
2023-10-28 14:21:59.299 
Epoch 389/1000 
	 loss: 41.8435, MinusLogProbMetric: 41.8435, val_loss: 44.7051, val_MinusLogProbMetric: 44.7051

Epoch 389: val_loss did not improve from 42.94943
196/196 - 43s - loss: 41.8435 - MinusLogProbMetric: 41.8435 - val_loss: 44.7051 - val_MinusLogProbMetric: 44.7051 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 390/1000
2023-10-28 14:22:41.308 
Epoch 390/1000 
	 loss: 41.8737, MinusLogProbMetric: 41.8737, val_loss: 44.3601, val_MinusLogProbMetric: 44.3601

Epoch 390: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.8737 - MinusLogProbMetric: 41.8737 - val_loss: 44.3601 - val_MinusLogProbMetric: 44.3601 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 391/1000
2023-10-28 14:23:24.198 
Epoch 391/1000 
	 loss: 41.7526, MinusLogProbMetric: 41.7526, val_loss: 43.5843, val_MinusLogProbMetric: 43.5843

Epoch 391: val_loss did not improve from 42.94943
196/196 - 43s - loss: 41.7526 - MinusLogProbMetric: 41.7526 - val_loss: 43.5843 - val_MinusLogProbMetric: 43.5843 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 392/1000
2023-10-28 14:24:07.183 
Epoch 392/1000 
	 loss: 42.3019, MinusLogProbMetric: 42.3019, val_loss: 44.8183, val_MinusLogProbMetric: 44.8183

Epoch 392: val_loss did not improve from 42.94943
196/196 - 43s - loss: 42.3019 - MinusLogProbMetric: 42.3019 - val_loss: 44.8183 - val_MinusLogProbMetric: 44.8183 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 393/1000
2023-10-28 14:24:50.032 
Epoch 393/1000 
	 loss: 41.7743, MinusLogProbMetric: 41.7743, val_loss: 43.1442, val_MinusLogProbMetric: 43.1442

Epoch 393: val_loss did not improve from 42.94943
196/196 - 43s - loss: 41.7743 - MinusLogProbMetric: 41.7743 - val_loss: 43.1442 - val_MinusLogProbMetric: 43.1442 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 394/1000
2023-10-28 14:25:32.450 
Epoch 394/1000 
	 loss: 41.9255, MinusLogProbMetric: 41.9255, val_loss: 43.1020, val_MinusLogProbMetric: 43.1020

Epoch 394: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.9255 - MinusLogProbMetric: 41.9255 - val_loss: 43.1020 - val_MinusLogProbMetric: 43.1020 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 395/1000
2023-10-28 14:26:15.355 
Epoch 395/1000 
	 loss: 41.8469, MinusLogProbMetric: 41.8469, val_loss: 43.1343, val_MinusLogProbMetric: 43.1343

Epoch 395: val_loss did not improve from 42.94943
196/196 - 43s - loss: 41.8469 - MinusLogProbMetric: 41.8469 - val_loss: 43.1343 - val_MinusLogProbMetric: 43.1343 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 396/1000
2023-10-28 14:26:57.598 
Epoch 396/1000 
	 loss: 41.7622, MinusLogProbMetric: 41.7622, val_loss: 44.1662, val_MinusLogProbMetric: 44.1662

Epoch 396: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.7622 - MinusLogProbMetric: 41.7622 - val_loss: 44.1662 - val_MinusLogProbMetric: 44.1662 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 397/1000
2023-10-28 14:27:39.831 
Epoch 397/1000 
	 loss: 41.8361, MinusLogProbMetric: 41.8361, val_loss: 43.7496, val_MinusLogProbMetric: 43.7496

Epoch 397: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.8361 - MinusLogProbMetric: 41.8361 - val_loss: 43.7496 - val_MinusLogProbMetric: 43.7496 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 398/1000
2023-10-28 14:28:22.034 
Epoch 398/1000 
	 loss: 41.7971, MinusLogProbMetric: 41.7971, val_loss: 43.2548, val_MinusLogProbMetric: 43.2548

Epoch 398: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.7971 - MinusLogProbMetric: 41.7971 - val_loss: 43.2548 - val_MinusLogProbMetric: 43.2548 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 399/1000
2023-10-28 14:29:04.371 
Epoch 399/1000 
	 loss: 41.8252, MinusLogProbMetric: 41.8252, val_loss: 44.5175, val_MinusLogProbMetric: 44.5175

Epoch 399: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.8252 - MinusLogProbMetric: 41.8252 - val_loss: 44.5175 - val_MinusLogProbMetric: 44.5175 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 400/1000
2023-10-28 14:29:46.444 
Epoch 400/1000 
	 loss: 41.8668, MinusLogProbMetric: 41.8668, val_loss: 43.2539, val_MinusLogProbMetric: 43.2539

Epoch 400: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.8668 - MinusLogProbMetric: 41.8668 - val_loss: 43.2539 - val_MinusLogProbMetric: 43.2539 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 401/1000
2023-10-28 14:30:28.170 
Epoch 401/1000 
	 loss: 41.7658, MinusLogProbMetric: 41.7658, val_loss: 44.6628, val_MinusLogProbMetric: 44.6628

Epoch 401: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.7658 - MinusLogProbMetric: 41.7658 - val_loss: 44.6628 - val_MinusLogProbMetric: 44.6628 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 402/1000
2023-10-28 14:31:10.015 
Epoch 402/1000 
	 loss: 42.3604, MinusLogProbMetric: 42.3604, val_loss: 45.8015, val_MinusLogProbMetric: 45.8015

Epoch 402: val_loss did not improve from 42.94943
196/196 - 42s - loss: 42.3604 - MinusLogProbMetric: 42.3604 - val_loss: 45.8015 - val_MinusLogProbMetric: 45.8015 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 403/1000
2023-10-28 14:31:51.700 
Epoch 403/1000 
	 loss: 41.8235, MinusLogProbMetric: 41.8235, val_loss: 43.9725, val_MinusLogProbMetric: 43.9725

Epoch 403: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.8235 - MinusLogProbMetric: 41.8235 - val_loss: 43.9725 - val_MinusLogProbMetric: 43.9725 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 404/1000
2023-10-28 14:32:33.356 
Epoch 404/1000 
	 loss: 41.8481, MinusLogProbMetric: 41.8481, val_loss: 44.0083, val_MinusLogProbMetric: 44.0083

Epoch 404: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.8481 - MinusLogProbMetric: 41.8481 - val_loss: 44.0083 - val_MinusLogProbMetric: 44.0083 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 405/1000
2023-10-28 14:33:14.404 
Epoch 405/1000 
	 loss: 41.9166, MinusLogProbMetric: 41.9166, val_loss: 43.4231, val_MinusLogProbMetric: 43.4231

Epoch 405: val_loss did not improve from 42.94943
196/196 - 41s - loss: 41.9166 - MinusLogProbMetric: 41.9166 - val_loss: 43.4231 - val_MinusLogProbMetric: 43.4231 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 406/1000
2023-10-28 14:33:55.298 
Epoch 406/1000 
	 loss: 42.0192, MinusLogProbMetric: 42.0192, val_loss: 43.4312, val_MinusLogProbMetric: 43.4312

Epoch 406: val_loss did not improve from 42.94943
196/196 - 41s - loss: 42.0192 - MinusLogProbMetric: 42.0192 - val_loss: 43.4312 - val_MinusLogProbMetric: 43.4312 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 407/1000
2023-10-28 14:34:36.577 
Epoch 407/1000 
	 loss: 41.5936, MinusLogProbMetric: 41.5936, val_loss: 44.2861, val_MinusLogProbMetric: 44.2861

Epoch 407: val_loss did not improve from 42.94943
196/196 - 41s - loss: 41.5936 - MinusLogProbMetric: 41.5936 - val_loss: 44.2861 - val_MinusLogProbMetric: 44.2861 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 408/1000
2023-10-28 14:35:17.307 
Epoch 408/1000 
	 loss: 41.7481, MinusLogProbMetric: 41.7481, val_loss: 43.2269, val_MinusLogProbMetric: 43.2269

Epoch 408: val_loss did not improve from 42.94943
196/196 - 41s - loss: 41.7481 - MinusLogProbMetric: 41.7481 - val_loss: 43.2269 - val_MinusLogProbMetric: 43.2269 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 409/1000
2023-10-28 14:35:58.941 
Epoch 409/1000 
	 loss: 41.7772, MinusLogProbMetric: 41.7772, val_loss: 45.0954, val_MinusLogProbMetric: 45.0954

Epoch 409: val_loss did not improve from 42.94943
196/196 - 42s - loss: 41.7772 - MinusLogProbMetric: 41.7772 - val_loss: 45.0954 - val_MinusLogProbMetric: 45.0954 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 410/1000
2023-10-28 14:36:40.406 
Epoch 410/1000 
	 loss: 42.0732, MinusLogProbMetric: 42.0732, val_loss: 45.1930, val_MinusLogProbMetric: 45.1930

Epoch 410: val_loss did not improve from 42.94943
196/196 - 41s - loss: 42.0732 - MinusLogProbMetric: 42.0732 - val_loss: 45.1930 - val_MinusLogProbMetric: 45.1930 - lr: 1.1111e-04 - 41s/epoch - 212ms/step
Epoch 411/1000
2023-10-28 14:37:21.884 
Epoch 411/1000 
	 loss: 42.0281, MinusLogProbMetric: 42.0281, val_loss: 44.5739, val_MinusLogProbMetric: 44.5739

Epoch 411: val_loss did not improve from 42.94943
196/196 - 41s - loss: 42.0281 - MinusLogProbMetric: 42.0281 - val_loss: 44.5739 - val_MinusLogProbMetric: 44.5739 - lr: 1.1111e-04 - 41s/epoch - 212ms/step
Epoch 412/1000
2023-10-28 14:38:03.100 
Epoch 412/1000 
	 loss: 41.7718, MinusLogProbMetric: 41.7718, val_loss: 43.4536, val_MinusLogProbMetric: 43.4536

Epoch 412: val_loss did not improve from 42.94943
196/196 - 41s - loss: 41.7718 - MinusLogProbMetric: 41.7718 - val_loss: 43.4536 - val_MinusLogProbMetric: 43.4536 - lr: 1.1111e-04 - 41s/epoch - 210ms/step
Epoch 413/1000
2023-10-28 14:38:43.817 
Epoch 413/1000 
	 loss: 43.2072, MinusLogProbMetric: 43.2072, val_loss: 44.4407, val_MinusLogProbMetric: 44.4407

Epoch 413: val_loss did not improve from 42.94943
196/196 - 41s - loss: 43.2072 - MinusLogProbMetric: 43.2072 - val_loss: 44.4407 - val_MinusLogProbMetric: 44.4407 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 414/1000
2023-10-28 14:39:25.226 
Epoch 414/1000 
	 loss: 42.6014, MinusLogProbMetric: 42.6014, val_loss: 44.7159, val_MinusLogProbMetric: 44.7159

Epoch 414: val_loss did not improve from 42.94943
196/196 - 41s - loss: 42.6014 - MinusLogProbMetric: 42.6014 - val_loss: 44.7159 - val_MinusLogProbMetric: 44.7159 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 415/1000
2023-10-28 14:40:06.503 
Epoch 415/1000 
	 loss: 42.3359, MinusLogProbMetric: 42.3359, val_loss: 44.4662, val_MinusLogProbMetric: 44.4662

Epoch 415: val_loss did not improve from 42.94943
196/196 - 41s - loss: 42.3359 - MinusLogProbMetric: 42.3359 - val_loss: 44.4662 - val_MinusLogProbMetric: 44.4662 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 416/1000
2023-10-28 14:40:47.998 
Epoch 416/1000 
	 loss: 42.1629, MinusLogProbMetric: 42.1629, val_loss: 43.6442, val_MinusLogProbMetric: 43.6442

Epoch 416: val_loss did not improve from 42.94943
196/196 - 42s - loss: 42.1629 - MinusLogProbMetric: 42.1629 - val_loss: 43.6442 - val_MinusLogProbMetric: 43.6442 - lr: 1.1111e-04 - 42s/epoch - 212ms/step
Epoch 417/1000
2023-10-28 14:41:28.899 
Epoch 417/1000 
	 loss: 42.0776, MinusLogProbMetric: 42.0776, val_loss: 43.8002, val_MinusLogProbMetric: 43.8002

Epoch 417: val_loss did not improve from 42.94943
196/196 - 41s - loss: 42.0776 - MinusLogProbMetric: 42.0776 - val_loss: 43.8002 - val_MinusLogProbMetric: 43.8002 - lr: 1.1111e-04 - 41s/epoch - 209ms/step
Epoch 418/1000
2023-10-28 14:42:09.588 
Epoch 418/1000 
	 loss: 42.0536, MinusLogProbMetric: 42.0536, val_loss: 43.5238, val_MinusLogProbMetric: 43.5238

Epoch 418: val_loss did not improve from 42.94943
196/196 - 41s - loss: 42.0536 - MinusLogProbMetric: 42.0536 - val_loss: 43.5238 - val_MinusLogProbMetric: 43.5238 - lr: 1.1111e-04 - 41s/epoch - 208ms/step
Epoch 419/1000
2023-10-28 14:42:49.991 
Epoch 419/1000 
	 loss: 41.9927, MinusLogProbMetric: 41.9927, val_loss: 43.7166, val_MinusLogProbMetric: 43.7166

Epoch 419: val_loss did not improve from 42.94943
196/196 - 40s - loss: 41.9927 - MinusLogProbMetric: 41.9927 - val_loss: 43.7166 - val_MinusLogProbMetric: 43.7166 - lr: 1.1111e-04 - 40s/epoch - 206ms/step
Epoch 420/1000
2023-10-28 14:43:31.393 
Epoch 420/1000 
	 loss: 42.0705, MinusLogProbMetric: 42.0705, val_loss: 43.5463, val_MinusLogProbMetric: 43.5463

Epoch 420: val_loss did not improve from 42.94943
196/196 - 41s - loss: 42.0705 - MinusLogProbMetric: 42.0705 - val_loss: 43.5463 - val_MinusLogProbMetric: 43.5463 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 421/1000
2023-10-28 14:44:13.009 
Epoch 421/1000 
	 loss: 41.0940, MinusLogProbMetric: 41.0940, val_loss: 42.9307, val_MinusLogProbMetric: 42.9307

Epoch 421: val_loss improved from 42.94943 to 42.93068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 41.0940 - MinusLogProbMetric: 41.0940 - val_loss: 42.9307 - val_MinusLogProbMetric: 42.9307 - lr: 5.5556e-05 - 42s/epoch - 216ms/step
Epoch 422/1000
2023-10-28 14:44:54.476 
Epoch 422/1000 
	 loss: 40.9480, MinusLogProbMetric: 40.9480, val_loss: 42.8713, val_MinusLogProbMetric: 42.8713

Epoch 422: val_loss improved from 42.93068 to 42.87131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 40.9480 - MinusLogProbMetric: 40.9480 - val_loss: 42.8713 - val_MinusLogProbMetric: 42.8713 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 423/1000
2023-10-28 14:45:36.534 
Epoch 423/1000 
	 loss: 41.0278, MinusLogProbMetric: 41.0278, val_loss: 42.7739, val_MinusLogProbMetric: 42.7739

Epoch 423: val_loss improved from 42.87131 to 42.77387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 41.0278 - MinusLogProbMetric: 41.0278 - val_loss: 42.7739 - val_MinusLogProbMetric: 42.7739 - lr: 5.5556e-05 - 42s/epoch - 215ms/step
Epoch 424/1000
2023-10-28 14:46:19.725 
Epoch 424/1000 
	 loss: 40.9250, MinusLogProbMetric: 40.9250, val_loss: 42.7710, val_MinusLogProbMetric: 42.7710

Epoch 424: val_loss improved from 42.77387 to 42.77096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 43s - loss: 40.9250 - MinusLogProbMetric: 40.9250 - val_loss: 42.7710 - val_MinusLogProbMetric: 42.7710 - lr: 5.5556e-05 - 43s/epoch - 220ms/step
Epoch 425/1000
2023-10-28 14:47:01.669 
Epoch 425/1000 
	 loss: 40.9280, MinusLogProbMetric: 40.9280, val_loss: 42.9844, val_MinusLogProbMetric: 42.9844

Epoch 425: val_loss did not improve from 42.77096
196/196 - 41s - loss: 40.9280 - MinusLogProbMetric: 40.9280 - val_loss: 42.9844 - val_MinusLogProbMetric: 42.9844 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 426/1000
2023-10-28 14:47:42.461 
Epoch 426/1000 
	 loss: 40.9697, MinusLogProbMetric: 40.9697, val_loss: 42.8807, val_MinusLogProbMetric: 42.8807

Epoch 426: val_loss did not improve from 42.77096
196/196 - 41s - loss: 40.9697 - MinusLogProbMetric: 40.9697 - val_loss: 42.8807 - val_MinusLogProbMetric: 42.8807 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 427/1000
2023-10-28 14:48:23.880 
Epoch 427/1000 
	 loss: 40.9511, MinusLogProbMetric: 40.9511, val_loss: 42.7680, val_MinusLogProbMetric: 42.7680

Epoch 427: val_loss improved from 42.77096 to 42.76802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 40.9511 - MinusLogProbMetric: 40.9511 - val_loss: 42.7680 - val_MinusLogProbMetric: 42.7680 - lr: 5.5556e-05 - 42s/epoch - 215ms/step
Epoch 428/1000
2023-10-28 14:49:05.677 
Epoch 428/1000 
	 loss: 40.8626, MinusLogProbMetric: 40.8626, val_loss: 42.6130, val_MinusLogProbMetric: 42.6130

Epoch 428: val_loss improved from 42.76802 to 42.61303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 40.8626 - MinusLogProbMetric: 40.8626 - val_loss: 42.6130 - val_MinusLogProbMetric: 42.6130 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 429/1000
2023-10-28 14:49:47.368 
Epoch 429/1000 
	 loss: 40.9123, MinusLogProbMetric: 40.9123, val_loss: 43.0278, val_MinusLogProbMetric: 43.0278

Epoch 429: val_loss did not improve from 42.61303
196/196 - 41s - loss: 40.9123 - MinusLogProbMetric: 40.9123 - val_loss: 43.0278 - val_MinusLogProbMetric: 43.0278 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 430/1000
2023-10-28 14:50:28.622 
Epoch 430/1000 
	 loss: 40.8607, MinusLogProbMetric: 40.8607, val_loss: 42.6431, val_MinusLogProbMetric: 42.6431

Epoch 430: val_loss did not improve from 42.61303
196/196 - 41s - loss: 40.8607 - MinusLogProbMetric: 40.8607 - val_loss: 42.6431 - val_MinusLogProbMetric: 42.6431 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 431/1000
2023-10-28 14:51:09.659 
Epoch 431/1000 
	 loss: 40.8643, MinusLogProbMetric: 40.8643, val_loss: 42.9801, val_MinusLogProbMetric: 42.9801

Epoch 431: val_loss did not improve from 42.61303
196/196 - 41s - loss: 40.8643 - MinusLogProbMetric: 40.8643 - val_loss: 42.9801 - val_MinusLogProbMetric: 42.9801 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 432/1000
2023-10-28 14:51:50.460 
Epoch 432/1000 
	 loss: 40.8278, MinusLogProbMetric: 40.8278, val_loss: 42.5443, val_MinusLogProbMetric: 42.5443

Epoch 432: val_loss improved from 42.61303 to 42.54434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 40.8278 - MinusLogProbMetric: 40.8278 - val_loss: 42.5443 - val_MinusLogProbMetric: 42.5443 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 433/1000
2023-10-28 14:52:32.132 
Epoch 433/1000 
	 loss: 40.8610, MinusLogProbMetric: 40.8610, val_loss: 42.8572, val_MinusLogProbMetric: 42.8572

Epoch 433: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8610 - MinusLogProbMetric: 40.8610 - val_loss: 42.8572 - val_MinusLogProbMetric: 42.8572 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 434/1000
2023-10-28 14:53:12.964 
Epoch 434/1000 
	 loss: 40.8436, MinusLogProbMetric: 40.8436, val_loss: 42.6040, val_MinusLogProbMetric: 42.6040

Epoch 434: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8436 - MinusLogProbMetric: 40.8436 - val_loss: 42.6040 - val_MinusLogProbMetric: 42.6040 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 435/1000
2023-10-28 14:53:54.223 
Epoch 435/1000 
	 loss: 40.8055, MinusLogProbMetric: 40.8055, val_loss: 42.8073, val_MinusLogProbMetric: 42.8073

Epoch 435: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8055 - MinusLogProbMetric: 40.8055 - val_loss: 42.8073 - val_MinusLogProbMetric: 42.8073 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 436/1000
2023-10-28 14:54:35.341 
Epoch 436/1000 
	 loss: 40.8218, MinusLogProbMetric: 40.8218, val_loss: 42.5779, val_MinusLogProbMetric: 42.5779

Epoch 436: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8218 - MinusLogProbMetric: 40.8218 - val_loss: 42.5779 - val_MinusLogProbMetric: 42.5779 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 437/1000
2023-10-28 14:55:16.356 
Epoch 437/1000 
	 loss: 40.8383, MinusLogProbMetric: 40.8383, val_loss: 42.7258, val_MinusLogProbMetric: 42.7258

Epoch 437: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8383 - MinusLogProbMetric: 40.8383 - val_loss: 42.7258 - val_MinusLogProbMetric: 42.7258 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 438/1000
2023-10-28 14:55:57.435 
Epoch 438/1000 
	 loss: 40.8961, MinusLogProbMetric: 40.8961, val_loss: 42.6333, val_MinusLogProbMetric: 42.6333

Epoch 438: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8961 - MinusLogProbMetric: 40.8961 - val_loss: 42.6333 - val_MinusLogProbMetric: 42.6333 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 439/1000
2023-10-28 14:56:38.792 
Epoch 439/1000 
	 loss: 40.7528, MinusLogProbMetric: 40.7528, val_loss: 42.5708, val_MinusLogProbMetric: 42.5708

Epoch 439: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.7528 - MinusLogProbMetric: 40.7528 - val_loss: 42.5708 - val_MinusLogProbMetric: 42.5708 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 440/1000
2023-10-28 14:57:19.659 
Epoch 440/1000 
	 loss: 40.8013, MinusLogProbMetric: 40.8013, val_loss: 42.9159, val_MinusLogProbMetric: 42.9159

Epoch 440: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8013 - MinusLogProbMetric: 40.8013 - val_loss: 42.9159 - val_MinusLogProbMetric: 42.9159 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 441/1000
2023-10-28 14:58:00.835 
Epoch 441/1000 
	 loss: 40.8247, MinusLogProbMetric: 40.8247, val_loss: 42.9179, val_MinusLogProbMetric: 42.9179

Epoch 441: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8247 - MinusLogProbMetric: 40.8247 - val_loss: 42.9179 - val_MinusLogProbMetric: 42.9179 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 442/1000
2023-10-28 14:58:41.868 
Epoch 442/1000 
	 loss: 40.8070, MinusLogProbMetric: 40.8070, val_loss: 42.6999, val_MinusLogProbMetric: 42.6999

Epoch 442: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.8070 - MinusLogProbMetric: 40.8070 - val_loss: 42.6999 - val_MinusLogProbMetric: 42.6999 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 443/1000
2023-10-28 14:59:22.837 
Epoch 443/1000 
	 loss: 40.7649, MinusLogProbMetric: 40.7649, val_loss: 42.6501, val_MinusLogProbMetric: 42.6501

Epoch 443: val_loss did not improve from 42.54434
196/196 - 41s - loss: 40.7649 - MinusLogProbMetric: 40.7649 - val_loss: 42.6501 - val_MinusLogProbMetric: 42.6501 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 444/1000
2023-10-28 15:00:04.364 
Epoch 444/1000 
	 loss: 40.9331, MinusLogProbMetric: 40.9331, val_loss: 42.5151, val_MinusLogProbMetric: 42.5151

Epoch 444: val_loss improved from 42.54434 to 42.51511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 40.9331 - MinusLogProbMetric: 40.9331 - val_loss: 42.5151 - val_MinusLogProbMetric: 42.5151 - lr: 5.5556e-05 - 42s/epoch - 216ms/step
Epoch 445/1000
2023-10-28 15:00:46.358 
Epoch 445/1000 
	 loss: 40.7397, MinusLogProbMetric: 40.7397, val_loss: 42.6990, val_MinusLogProbMetric: 42.6990

Epoch 445: val_loss did not improve from 42.51511
196/196 - 41s - loss: 40.7397 - MinusLogProbMetric: 40.7397 - val_loss: 42.6990 - val_MinusLogProbMetric: 42.6990 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 446/1000
2023-10-28 15:01:27.195 
Epoch 446/1000 
	 loss: 40.7631, MinusLogProbMetric: 40.7631, val_loss: 42.6723, val_MinusLogProbMetric: 42.6723

Epoch 446: val_loss did not improve from 42.51511
196/196 - 41s - loss: 40.7631 - MinusLogProbMetric: 40.7631 - val_loss: 42.6723 - val_MinusLogProbMetric: 42.6723 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 447/1000
2023-10-28 15:02:06.109 
Epoch 447/1000 
	 loss: 40.7279, MinusLogProbMetric: 40.7279, val_loss: 42.3813, val_MinusLogProbMetric: 42.3813

Epoch 447: val_loss improved from 42.51511 to 42.38132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 40s - loss: 40.7279 - MinusLogProbMetric: 40.7279 - val_loss: 42.3813 - val_MinusLogProbMetric: 42.3813 - lr: 5.5556e-05 - 40s/epoch - 202ms/step
Epoch 448/1000
2023-10-28 15:02:45.595 
Epoch 448/1000 
	 loss: 40.7390, MinusLogProbMetric: 40.7390, val_loss: 42.9113, val_MinusLogProbMetric: 42.9113

Epoch 448: val_loss did not improve from 42.38132
196/196 - 39s - loss: 40.7390 - MinusLogProbMetric: 40.7390 - val_loss: 42.9113 - val_MinusLogProbMetric: 42.9113 - lr: 5.5556e-05 - 39s/epoch - 198ms/step
Epoch 449/1000
2023-10-28 15:03:26.219 
Epoch 449/1000 
	 loss: 40.8265, MinusLogProbMetric: 40.8265, val_loss: 42.7882, val_MinusLogProbMetric: 42.7882

Epoch 449: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.8265 - MinusLogProbMetric: 40.8265 - val_loss: 42.7882 - val_MinusLogProbMetric: 42.7882 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 450/1000
2023-10-28 15:04:05.878 
Epoch 450/1000 
	 loss: 40.7715, MinusLogProbMetric: 40.7715, val_loss: 43.8175, val_MinusLogProbMetric: 43.8175

Epoch 450: val_loss did not improve from 42.38132
196/196 - 40s - loss: 40.7715 - MinusLogProbMetric: 40.7715 - val_loss: 43.8175 - val_MinusLogProbMetric: 43.8175 - lr: 5.5556e-05 - 40s/epoch - 202ms/step
Epoch 451/1000
2023-10-28 15:04:44.779 
Epoch 451/1000 
	 loss: 40.8284, MinusLogProbMetric: 40.8284, val_loss: 42.7157, val_MinusLogProbMetric: 42.7157

Epoch 451: val_loss did not improve from 42.38132
196/196 - 39s - loss: 40.8284 - MinusLogProbMetric: 40.8284 - val_loss: 42.7157 - val_MinusLogProbMetric: 42.7157 - lr: 5.5556e-05 - 39s/epoch - 198ms/step
Epoch 452/1000
2023-10-28 15:05:23.805 
Epoch 452/1000 
	 loss: 40.6618, MinusLogProbMetric: 40.6618, val_loss: 42.5628, val_MinusLogProbMetric: 42.5628

Epoch 452: val_loss did not improve from 42.38132
196/196 - 39s - loss: 40.6618 - MinusLogProbMetric: 40.6618 - val_loss: 42.5628 - val_MinusLogProbMetric: 42.5628 - lr: 5.5556e-05 - 39s/epoch - 199ms/step
Epoch 453/1000
2023-10-28 15:06:03.007 
Epoch 453/1000 
	 loss: 40.6977, MinusLogProbMetric: 40.6977, val_loss: 42.7972, val_MinusLogProbMetric: 42.7972

Epoch 453: val_loss did not improve from 42.38132
196/196 - 39s - loss: 40.6977 - MinusLogProbMetric: 40.6977 - val_loss: 42.7972 - val_MinusLogProbMetric: 42.7972 - lr: 5.5556e-05 - 39s/epoch - 200ms/step
Epoch 454/1000
2023-10-28 15:06:41.573 
Epoch 454/1000 
	 loss: 40.7719, MinusLogProbMetric: 40.7719, val_loss: 42.6807, val_MinusLogProbMetric: 42.6807

Epoch 454: val_loss did not improve from 42.38132
196/196 - 39s - loss: 40.7719 - MinusLogProbMetric: 40.7719 - val_loss: 42.6807 - val_MinusLogProbMetric: 42.6807 - lr: 5.5556e-05 - 39s/epoch - 197ms/step
Epoch 455/1000
2023-10-28 15:07:19.905 
Epoch 455/1000 
	 loss: 40.7182, MinusLogProbMetric: 40.7182, val_loss: 42.3996, val_MinusLogProbMetric: 42.3996

Epoch 455: val_loss did not improve from 42.38132
196/196 - 38s - loss: 40.7182 - MinusLogProbMetric: 40.7182 - val_loss: 42.3996 - val_MinusLogProbMetric: 42.3996 - lr: 5.5556e-05 - 38s/epoch - 196ms/step
Epoch 456/1000
2023-10-28 15:07:59.126 
Epoch 456/1000 
	 loss: 40.7809, MinusLogProbMetric: 40.7809, val_loss: 43.1234, val_MinusLogProbMetric: 43.1234

Epoch 456: val_loss did not improve from 42.38132
196/196 - 39s - loss: 40.7809 - MinusLogProbMetric: 40.7809 - val_loss: 43.1234 - val_MinusLogProbMetric: 43.1234 - lr: 5.5556e-05 - 39s/epoch - 200ms/step
Epoch 457/1000
2023-10-28 15:08:39.602 
Epoch 457/1000 
	 loss: 40.7593, MinusLogProbMetric: 40.7593, val_loss: 43.0824, val_MinusLogProbMetric: 43.0824

Epoch 457: val_loss did not improve from 42.38132
196/196 - 40s - loss: 40.7593 - MinusLogProbMetric: 40.7593 - val_loss: 43.0824 - val_MinusLogProbMetric: 43.0824 - lr: 5.5556e-05 - 40s/epoch - 207ms/step
Epoch 458/1000
2023-10-28 15:09:20.302 
Epoch 458/1000 
	 loss: 40.6850, MinusLogProbMetric: 40.6850, val_loss: 42.8702, val_MinusLogProbMetric: 42.8702

Epoch 458: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6850 - MinusLogProbMetric: 40.6850 - val_loss: 42.8702 - val_MinusLogProbMetric: 42.8702 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 459/1000
2023-10-28 15:10:01.746 
Epoch 459/1000 
	 loss: 40.7814, MinusLogProbMetric: 40.7814, val_loss: 42.8896, val_MinusLogProbMetric: 42.8896

Epoch 459: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.7814 - MinusLogProbMetric: 40.7814 - val_loss: 42.8896 - val_MinusLogProbMetric: 42.8896 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 460/1000
2023-10-28 15:10:43.467 
Epoch 460/1000 
	 loss: 40.7359, MinusLogProbMetric: 40.7359, val_loss: 42.4934, val_MinusLogProbMetric: 42.4934

Epoch 460: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.7359 - MinusLogProbMetric: 40.7359 - val_loss: 42.4934 - val_MinusLogProbMetric: 42.4934 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 461/1000
2023-10-28 15:11:24.790 
Epoch 461/1000 
	 loss: 40.7520, MinusLogProbMetric: 40.7520, val_loss: 43.0883, val_MinusLogProbMetric: 43.0883

Epoch 461: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.7520 - MinusLogProbMetric: 40.7520 - val_loss: 43.0883 - val_MinusLogProbMetric: 43.0883 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 462/1000
2023-10-28 15:12:06.088 
Epoch 462/1000 
	 loss: 40.6877, MinusLogProbMetric: 40.6877, val_loss: 42.7225, val_MinusLogProbMetric: 42.7225

Epoch 462: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6877 - MinusLogProbMetric: 40.6877 - val_loss: 42.7225 - val_MinusLogProbMetric: 42.7225 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 463/1000
2023-10-28 15:12:46.865 
Epoch 463/1000 
	 loss: 40.6778, MinusLogProbMetric: 40.6778, val_loss: 42.5720, val_MinusLogProbMetric: 42.5720

Epoch 463: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6778 - MinusLogProbMetric: 40.6778 - val_loss: 42.5720 - val_MinusLogProbMetric: 42.5720 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 464/1000
2023-10-28 15:13:28.255 
Epoch 464/1000 
	 loss: 40.7103, MinusLogProbMetric: 40.7103, val_loss: 43.7904, val_MinusLogProbMetric: 43.7904

Epoch 464: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.7103 - MinusLogProbMetric: 40.7103 - val_loss: 43.7904 - val_MinusLogProbMetric: 43.7904 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 465/1000
2023-10-28 15:14:09.996 
Epoch 465/1000 
	 loss: 40.6774, MinusLogProbMetric: 40.6774, val_loss: 42.5431, val_MinusLogProbMetric: 42.5431

Epoch 465: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.6774 - MinusLogProbMetric: 40.6774 - val_loss: 42.5431 - val_MinusLogProbMetric: 42.5431 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 466/1000
2023-10-28 15:14:50.994 
Epoch 466/1000 
	 loss: 40.7076, MinusLogProbMetric: 40.7076, val_loss: 42.7289, val_MinusLogProbMetric: 42.7289

Epoch 466: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.7076 - MinusLogProbMetric: 40.7076 - val_loss: 42.7289 - val_MinusLogProbMetric: 42.7289 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 467/1000
2023-10-28 15:15:32.376 
Epoch 467/1000 
	 loss: 40.6581, MinusLogProbMetric: 40.6581, val_loss: 42.6764, val_MinusLogProbMetric: 42.6764

Epoch 467: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6581 - MinusLogProbMetric: 40.6581 - val_loss: 42.6764 - val_MinusLogProbMetric: 42.6764 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 468/1000
2023-10-28 15:16:12.348 
Epoch 468/1000 
	 loss: 40.6752, MinusLogProbMetric: 40.6752, val_loss: 42.9705, val_MinusLogProbMetric: 42.9705

Epoch 468: val_loss did not improve from 42.38132
196/196 - 40s - loss: 40.6752 - MinusLogProbMetric: 40.6752 - val_loss: 42.9705 - val_MinusLogProbMetric: 42.9705 - lr: 5.5556e-05 - 40s/epoch - 204ms/step
Epoch 469/1000
2023-10-28 15:16:51.441 
Epoch 469/1000 
	 loss: 40.7491, MinusLogProbMetric: 40.7491, val_loss: 42.8936, val_MinusLogProbMetric: 42.8936

Epoch 469: val_loss did not improve from 42.38132
196/196 - 39s - loss: 40.7491 - MinusLogProbMetric: 40.7491 - val_loss: 42.8936 - val_MinusLogProbMetric: 42.8936 - lr: 5.5556e-05 - 39s/epoch - 199ms/step
Epoch 470/1000
2023-10-28 15:17:30.291 
Epoch 470/1000 
	 loss: 40.6793, MinusLogProbMetric: 40.6793, val_loss: 42.5954, val_MinusLogProbMetric: 42.5954

Epoch 470: val_loss did not improve from 42.38132
196/196 - 39s - loss: 40.6793 - MinusLogProbMetric: 40.6793 - val_loss: 42.5954 - val_MinusLogProbMetric: 42.5954 - lr: 5.5556e-05 - 39s/epoch - 198ms/step
Epoch 471/1000
2023-10-28 15:18:10.880 
Epoch 471/1000 
	 loss: 40.7062, MinusLogProbMetric: 40.7062, val_loss: 42.8573, val_MinusLogProbMetric: 42.8573

Epoch 471: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.7062 - MinusLogProbMetric: 40.7062 - val_loss: 42.8573 - val_MinusLogProbMetric: 42.8573 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 472/1000
2023-10-28 15:18:52.554 
Epoch 472/1000 
	 loss: 40.7207, MinusLogProbMetric: 40.7207, val_loss: 43.3656, val_MinusLogProbMetric: 43.3656

Epoch 472: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.7207 - MinusLogProbMetric: 40.7207 - val_loss: 43.3656 - val_MinusLogProbMetric: 43.3656 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 473/1000
2023-10-28 15:19:34.212 
Epoch 473/1000 
	 loss: 40.6458, MinusLogProbMetric: 40.6458, val_loss: 42.4715, val_MinusLogProbMetric: 42.4715

Epoch 473: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.6458 - MinusLogProbMetric: 40.6458 - val_loss: 42.4715 - val_MinusLogProbMetric: 42.4715 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 474/1000
2023-10-28 15:20:15.525 
Epoch 474/1000 
	 loss: 40.6445, MinusLogProbMetric: 40.6445, val_loss: 42.4783, val_MinusLogProbMetric: 42.4783

Epoch 474: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6445 - MinusLogProbMetric: 40.6445 - val_loss: 42.4783 - val_MinusLogProbMetric: 42.4783 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 475/1000
2023-10-28 15:20:56.622 
Epoch 475/1000 
	 loss: 40.7675, MinusLogProbMetric: 40.7675, val_loss: 43.0337, val_MinusLogProbMetric: 43.0337

Epoch 475: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.7675 - MinusLogProbMetric: 40.7675 - val_loss: 43.0337 - val_MinusLogProbMetric: 43.0337 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 476/1000
2023-10-28 15:21:38.107 
Epoch 476/1000 
	 loss: 40.6236, MinusLogProbMetric: 40.6236, val_loss: 42.7425, val_MinusLogProbMetric: 42.7425

Epoch 476: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6236 - MinusLogProbMetric: 40.6236 - val_loss: 42.7425 - val_MinusLogProbMetric: 42.7425 - lr: 5.5556e-05 - 41s/epoch - 212ms/step
Epoch 477/1000
2023-10-28 15:22:19.831 
Epoch 477/1000 
	 loss: 40.6828, MinusLogProbMetric: 40.6828, val_loss: 42.5761, val_MinusLogProbMetric: 42.5761

Epoch 477: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.6828 - MinusLogProbMetric: 40.6828 - val_loss: 42.5761 - val_MinusLogProbMetric: 42.5761 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 478/1000
2023-10-28 15:23:01.372 
Epoch 478/1000 
	 loss: 40.6300, MinusLogProbMetric: 40.6300, val_loss: 42.7536, val_MinusLogProbMetric: 42.7536

Epoch 478: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.6300 - MinusLogProbMetric: 40.6300 - val_loss: 42.7536 - val_MinusLogProbMetric: 42.7536 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 479/1000
2023-10-28 15:23:43.860 
Epoch 479/1000 
	 loss: 40.6539, MinusLogProbMetric: 40.6539, val_loss: 43.3585, val_MinusLogProbMetric: 43.3585

Epoch 479: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.6539 - MinusLogProbMetric: 40.6539 - val_loss: 43.3585 - val_MinusLogProbMetric: 43.3585 - lr: 5.5556e-05 - 42s/epoch - 217ms/step
Epoch 480/1000
2023-10-28 15:24:25.403 
Epoch 480/1000 
	 loss: 40.6473, MinusLogProbMetric: 40.6473, val_loss: 43.1933, val_MinusLogProbMetric: 43.1933

Epoch 480: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.6473 - MinusLogProbMetric: 40.6473 - val_loss: 43.1933 - val_MinusLogProbMetric: 43.1933 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 481/1000
2023-10-28 15:25:06.061 
Epoch 481/1000 
	 loss: 40.6814, MinusLogProbMetric: 40.6814, val_loss: 42.7070, val_MinusLogProbMetric: 42.7070

Epoch 481: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6814 - MinusLogProbMetric: 40.6814 - val_loss: 42.7070 - val_MinusLogProbMetric: 42.7070 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 482/1000
2023-10-28 15:25:46.620 
Epoch 482/1000 
	 loss: 40.6405, MinusLogProbMetric: 40.6405, val_loss: 42.6014, val_MinusLogProbMetric: 42.6014

Epoch 482: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6405 - MinusLogProbMetric: 40.6405 - val_loss: 42.6014 - val_MinusLogProbMetric: 42.6014 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 483/1000
2023-10-28 15:26:27.016 
Epoch 483/1000 
	 loss: 40.7131, MinusLogProbMetric: 40.7131, val_loss: 42.5216, val_MinusLogProbMetric: 42.5216

Epoch 483: val_loss did not improve from 42.38132
196/196 - 40s - loss: 40.7131 - MinusLogProbMetric: 40.7131 - val_loss: 42.5216 - val_MinusLogProbMetric: 42.5216 - lr: 5.5556e-05 - 40s/epoch - 206ms/step
Epoch 484/1000
2023-10-28 15:27:07.935 
Epoch 484/1000 
	 loss: 40.5671, MinusLogProbMetric: 40.5671, val_loss: 42.4822, val_MinusLogProbMetric: 42.4822

Epoch 484: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.5671 - MinusLogProbMetric: 40.5671 - val_loss: 42.4822 - val_MinusLogProbMetric: 42.4822 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 485/1000
2023-10-28 15:27:48.764 
Epoch 485/1000 
	 loss: 40.6539, MinusLogProbMetric: 40.6539, val_loss: 43.6449, val_MinusLogProbMetric: 43.6449

Epoch 485: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6539 - MinusLogProbMetric: 40.6539 - val_loss: 43.6449 - val_MinusLogProbMetric: 43.6449 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 486/1000
2023-10-28 15:28:29.639 
Epoch 486/1000 
	 loss: 40.6564, MinusLogProbMetric: 40.6564, val_loss: 42.6759, val_MinusLogProbMetric: 42.6759

Epoch 486: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6564 - MinusLogProbMetric: 40.6564 - val_loss: 42.6759 - val_MinusLogProbMetric: 42.6759 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 487/1000
2023-10-28 15:29:10.417 
Epoch 487/1000 
	 loss: 40.6834, MinusLogProbMetric: 40.6834, val_loss: 42.5138, val_MinusLogProbMetric: 42.5138

Epoch 487: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6834 - MinusLogProbMetric: 40.6834 - val_loss: 42.5138 - val_MinusLogProbMetric: 42.5138 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 488/1000
2023-10-28 15:29:50.981 
Epoch 488/1000 
	 loss: 40.6748, MinusLogProbMetric: 40.6748, val_loss: 42.8891, val_MinusLogProbMetric: 42.8891

Epoch 488: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6748 - MinusLogProbMetric: 40.6748 - val_loss: 42.8891 - val_MinusLogProbMetric: 42.8891 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 489/1000
2023-10-28 15:30:31.684 
Epoch 489/1000 
	 loss: 40.5568, MinusLogProbMetric: 40.5568, val_loss: 42.5087, val_MinusLogProbMetric: 42.5087

Epoch 489: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.5568 - MinusLogProbMetric: 40.5568 - val_loss: 42.5087 - val_MinusLogProbMetric: 42.5087 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 490/1000
2023-10-28 15:31:12.842 
Epoch 490/1000 
	 loss: 40.6710, MinusLogProbMetric: 40.6710, val_loss: 43.0238, val_MinusLogProbMetric: 43.0238

Epoch 490: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.6710 - MinusLogProbMetric: 40.6710 - val_loss: 43.0238 - val_MinusLogProbMetric: 43.0238 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 491/1000
2023-10-28 15:31:53.206 
Epoch 491/1000 
	 loss: 40.5571, MinusLogProbMetric: 40.5571, val_loss: 42.6248, val_MinusLogProbMetric: 42.6248

Epoch 491: val_loss did not improve from 42.38132
196/196 - 40s - loss: 40.5571 - MinusLogProbMetric: 40.5571 - val_loss: 42.6248 - val_MinusLogProbMetric: 42.6248 - lr: 5.5556e-05 - 40s/epoch - 206ms/step
Epoch 492/1000
2023-10-28 15:32:34.936 
Epoch 492/1000 
	 loss: 40.5482, MinusLogProbMetric: 40.5482, val_loss: 43.0426, val_MinusLogProbMetric: 43.0426

Epoch 492: val_loss did not improve from 42.38132
196/196 - 42s - loss: 40.5482 - MinusLogProbMetric: 40.5482 - val_loss: 43.0426 - val_MinusLogProbMetric: 43.0426 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 493/1000
2023-10-28 15:33:16.283 
Epoch 493/1000 
	 loss: 40.5895, MinusLogProbMetric: 40.5895, val_loss: 42.7935, val_MinusLogProbMetric: 42.7935

Epoch 493: val_loss did not improve from 42.38132
196/196 - 41s - loss: 40.5895 - MinusLogProbMetric: 40.5895 - val_loss: 42.7935 - val_MinusLogProbMetric: 42.7935 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 494/1000
2023-10-28 15:33:57.149 
Epoch 494/1000 
	 loss: 40.5870, MinusLogProbMetric: 40.5870, val_loss: 42.3794, val_MinusLogProbMetric: 42.3794

Epoch 494: val_loss improved from 42.38132 to 42.37942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 40.5870 - MinusLogProbMetric: 40.5870 - val_loss: 42.3794 - val_MinusLogProbMetric: 42.3794 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 495/1000
2023-10-28 15:34:38.928 
Epoch 495/1000 
	 loss: 40.6478, MinusLogProbMetric: 40.6478, val_loss: 43.3190, val_MinusLogProbMetric: 43.3190

Epoch 495: val_loss did not improve from 42.37942
196/196 - 41s - loss: 40.6478 - MinusLogProbMetric: 40.6478 - val_loss: 43.3190 - val_MinusLogProbMetric: 43.3190 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 496/1000
2023-10-28 15:35:20.765 
Epoch 496/1000 
	 loss: 40.6211, MinusLogProbMetric: 40.6211, val_loss: 42.6388, val_MinusLogProbMetric: 42.6388

Epoch 496: val_loss did not improve from 42.37942
196/196 - 42s - loss: 40.6211 - MinusLogProbMetric: 40.6211 - val_loss: 42.6388 - val_MinusLogProbMetric: 42.6388 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 497/1000
2023-10-28 15:36:01.390 
Epoch 497/1000 
	 loss: 40.5824, MinusLogProbMetric: 40.5824, val_loss: 42.5556, val_MinusLogProbMetric: 42.5556

Epoch 497: val_loss did not improve from 42.37942
196/196 - 41s - loss: 40.5824 - MinusLogProbMetric: 40.5824 - val_loss: 42.5556 - val_MinusLogProbMetric: 42.5556 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 498/1000
2023-10-28 15:36:42.024 
Epoch 498/1000 
	 loss: 40.5864, MinusLogProbMetric: 40.5864, val_loss: 42.5528, val_MinusLogProbMetric: 42.5528

Epoch 498: val_loss did not improve from 42.37942
196/196 - 41s - loss: 40.5864 - MinusLogProbMetric: 40.5864 - val_loss: 42.5528 - val_MinusLogProbMetric: 42.5528 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 499/1000
2023-10-28 15:37:23.502 
Epoch 499/1000 
	 loss: 40.5618, MinusLogProbMetric: 40.5618, val_loss: 42.7517, val_MinusLogProbMetric: 42.7517

Epoch 499: val_loss did not improve from 42.37942
196/196 - 41s - loss: 40.5618 - MinusLogProbMetric: 40.5618 - val_loss: 42.7517 - val_MinusLogProbMetric: 42.7517 - lr: 5.5556e-05 - 41s/epoch - 212ms/step
Epoch 500/1000
2023-10-28 15:38:04.543 
Epoch 500/1000 
	 loss: 40.5843, MinusLogProbMetric: 40.5843, val_loss: 42.5878, val_MinusLogProbMetric: 42.5878

Epoch 500: val_loss did not improve from 42.37942
196/196 - 41s - loss: 40.5843 - MinusLogProbMetric: 40.5843 - val_loss: 42.5878 - val_MinusLogProbMetric: 42.5878 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 501/1000
2023-10-28 15:38:45.863 
Epoch 501/1000 
	 loss: 40.5186, MinusLogProbMetric: 40.5186, val_loss: 43.4978, val_MinusLogProbMetric: 43.4978

Epoch 501: val_loss did not improve from 42.37942
196/196 - 41s - loss: 40.5186 - MinusLogProbMetric: 40.5186 - val_loss: 43.4978 - val_MinusLogProbMetric: 43.4978 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 502/1000
2023-10-28 15:39:26.876 
Epoch 502/1000 
	 loss: 40.8985, MinusLogProbMetric: 40.8985, val_loss: 42.7706, val_MinusLogProbMetric: 42.7706

Epoch 502: val_loss did not improve from 42.37942
196/196 - 41s - loss: 40.8985 - MinusLogProbMetric: 40.8985 - val_loss: 42.7706 - val_MinusLogProbMetric: 42.7706 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 503/1000
2023-10-28 15:40:08.649 
Epoch 503/1000 
	 loss: 40.5080, MinusLogProbMetric: 40.5080, val_loss: 42.7990, val_MinusLogProbMetric: 42.7990

Epoch 503: val_loss did not improve from 42.37942
196/196 - 42s - loss: 40.5080 - MinusLogProbMetric: 40.5080 - val_loss: 42.7990 - val_MinusLogProbMetric: 42.7990 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 504/1000
2023-10-28 15:40:50.269 
Epoch 504/1000 
	 loss: 40.5715, MinusLogProbMetric: 40.5715, val_loss: 42.5112, val_MinusLogProbMetric: 42.5112

Epoch 504: val_loss did not improve from 42.37942
196/196 - 42s - loss: 40.5715 - MinusLogProbMetric: 40.5715 - val_loss: 42.5112 - val_MinusLogProbMetric: 42.5112 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 505/1000
2023-10-28 15:41:32.161 
Epoch 505/1000 
	 loss: 40.5567, MinusLogProbMetric: 40.5567, val_loss: 43.0580, val_MinusLogProbMetric: 43.0580

Epoch 505: val_loss did not improve from 42.37942
196/196 - 42s - loss: 40.5567 - MinusLogProbMetric: 40.5567 - val_loss: 43.0580 - val_MinusLogProbMetric: 43.0580 - lr: 5.5556e-05 - 42s/epoch - 214ms/step
Epoch 506/1000
2023-10-28 15:42:13.260 
Epoch 506/1000 
	 loss: 40.6118, MinusLogProbMetric: 40.6118, val_loss: 43.6751, val_MinusLogProbMetric: 43.6751

Epoch 506: val_loss did not improve from 42.37942
196/196 - 41s - loss: 40.6118 - MinusLogProbMetric: 40.6118 - val_loss: 43.6751 - val_MinusLogProbMetric: 43.6751 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 507/1000
2023-10-28 15:42:54.264 
Epoch 507/1000 
	 loss: 40.5711, MinusLogProbMetric: 40.5711, val_loss: 42.3218, val_MinusLogProbMetric: 42.3218

Epoch 507: val_loss improved from 42.37942 to 42.32185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 40.5711 - MinusLogProbMetric: 40.5711 - val_loss: 42.3218 - val_MinusLogProbMetric: 42.3218 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 508/1000
2023-10-28 15:43:35.602 
Epoch 508/1000 
	 loss: 40.5221, MinusLogProbMetric: 40.5221, val_loss: 43.0669, val_MinusLogProbMetric: 43.0669

Epoch 508: val_loss did not improve from 42.32185
196/196 - 41s - loss: 40.5221 - MinusLogProbMetric: 40.5221 - val_loss: 43.0669 - val_MinusLogProbMetric: 43.0669 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 509/1000
2023-10-28 15:44:17.333 
Epoch 509/1000 
	 loss: 40.5529, MinusLogProbMetric: 40.5529, val_loss: 42.3218, val_MinusLogProbMetric: 42.3218

Epoch 509: val_loss improved from 42.32185 to 42.32182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 40.5529 - MinusLogProbMetric: 40.5529 - val_loss: 42.3218 - val_MinusLogProbMetric: 42.3218 - lr: 5.5556e-05 - 42s/epoch - 216ms/step
Epoch 510/1000
2023-10-28 15:44:59.287 
Epoch 510/1000 
	 loss: 40.5342, MinusLogProbMetric: 40.5342, val_loss: 43.7895, val_MinusLogProbMetric: 43.7895

Epoch 510: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5342 - MinusLogProbMetric: 40.5342 - val_loss: 43.7895 - val_MinusLogProbMetric: 43.7895 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 511/1000
2023-10-28 15:45:40.652 
Epoch 511/1000 
	 loss: 40.5306, MinusLogProbMetric: 40.5306, val_loss: 42.6846, val_MinusLogProbMetric: 42.6846

Epoch 511: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5306 - MinusLogProbMetric: 40.5306 - val_loss: 42.6846 - val_MinusLogProbMetric: 42.6846 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 512/1000
2023-10-28 15:46:21.256 
Epoch 512/1000 
	 loss: 40.6275, MinusLogProbMetric: 40.6275, val_loss: 43.2208, val_MinusLogProbMetric: 43.2208

Epoch 512: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.6275 - MinusLogProbMetric: 40.6275 - val_loss: 43.2208 - val_MinusLogProbMetric: 43.2208 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 513/1000
2023-10-28 15:47:01.572 
Epoch 513/1000 
	 loss: 40.5313, MinusLogProbMetric: 40.5313, val_loss: 42.4762, val_MinusLogProbMetric: 42.4762

Epoch 513: val_loss did not improve from 42.32182
196/196 - 40s - loss: 40.5313 - MinusLogProbMetric: 40.5313 - val_loss: 42.4762 - val_MinusLogProbMetric: 42.4762 - lr: 5.5556e-05 - 40s/epoch - 206ms/step
Epoch 514/1000
2023-10-28 15:47:41.796 
Epoch 514/1000 
	 loss: 40.5951, MinusLogProbMetric: 40.5951, val_loss: 42.6165, val_MinusLogProbMetric: 42.6165

Epoch 514: val_loss did not improve from 42.32182
196/196 - 40s - loss: 40.5951 - MinusLogProbMetric: 40.5951 - val_loss: 42.6165 - val_MinusLogProbMetric: 42.6165 - lr: 5.5556e-05 - 40s/epoch - 205ms/step
Epoch 515/1000
2023-10-28 15:48:23.120 
Epoch 515/1000 
	 loss: 40.4785, MinusLogProbMetric: 40.4785, val_loss: 42.7752, val_MinusLogProbMetric: 42.7752

Epoch 515: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4785 - MinusLogProbMetric: 40.4785 - val_loss: 42.7752 - val_MinusLogProbMetric: 42.7752 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 516/1000
2023-10-28 15:49:04.757 
Epoch 516/1000 
	 loss: 40.4954, MinusLogProbMetric: 40.4954, val_loss: 42.5462, val_MinusLogProbMetric: 42.5462

Epoch 516: val_loss did not improve from 42.32182
196/196 - 42s - loss: 40.4954 - MinusLogProbMetric: 40.4954 - val_loss: 42.5462 - val_MinusLogProbMetric: 42.5462 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 517/1000
2023-10-28 15:49:46.024 
Epoch 517/1000 
	 loss: 40.6179, MinusLogProbMetric: 40.6179, val_loss: 42.4395, val_MinusLogProbMetric: 42.4395

Epoch 517: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.6179 - MinusLogProbMetric: 40.6179 - val_loss: 42.4395 - val_MinusLogProbMetric: 42.4395 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 518/1000
2023-10-28 15:50:27.687 
Epoch 518/1000 
	 loss: 40.5002, MinusLogProbMetric: 40.5002, val_loss: 42.9183, val_MinusLogProbMetric: 42.9183

Epoch 518: val_loss did not improve from 42.32182
196/196 - 42s - loss: 40.5002 - MinusLogProbMetric: 40.5002 - val_loss: 42.9183 - val_MinusLogProbMetric: 42.9183 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 519/1000
2023-10-28 15:51:08.886 
Epoch 519/1000 
	 loss: 40.4955, MinusLogProbMetric: 40.4955, val_loss: 42.5022, val_MinusLogProbMetric: 42.5022

Epoch 519: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4955 - MinusLogProbMetric: 40.4955 - val_loss: 42.5022 - val_MinusLogProbMetric: 42.5022 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 520/1000
2023-10-28 15:51:49.437 
Epoch 520/1000 
	 loss: 40.5165, MinusLogProbMetric: 40.5165, val_loss: 42.5989, val_MinusLogProbMetric: 42.5989

Epoch 520: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5165 - MinusLogProbMetric: 40.5165 - val_loss: 42.5989 - val_MinusLogProbMetric: 42.5989 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 521/1000
2023-10-28 15:52:29.987 
Epoch 521/1000 
	 loss: 40.5261, MinusLogProbMetric: 40.5261, val_loss: 42.5110, val_MinusLogProbMetric: 42.5110

Epoch 521: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5261 - MinusLogProbMetric: 40.5261 - val_loss: 42.5110 - val_MinusLogProbMetric: 42.5110 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 522/1000
2023-10-28 15:53:11.318 
Epoch 522/1000 
	 loss: 40.4851, MinusLogProbMetric: 40.4851, val_loss: 42.6791, val_MinusLogProbMetric: 42.6791

Epoch 522: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4851 - MinusLogProbMetric: 40.4851 - val_loss: 42.6791 - val_MinusLogProbMetric: 42.6791 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 523/1000
2023-10-28 15:53:52.705 
Epoch 523/1000 
	 loss: 40.5235, MinusLogProbMetric: 40.5235, val_loss: 42.9535, val_MinusLogProbMetric: 42.9535

Epoch 523: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5235 - MinusLogProbMetric: 40.5235 - val_loss: 42.9535 - val_MinusLogProbMetric: 42.9535 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 524/1000
2023-10-28 15:54:34.188 
Epoch 524/1000 
	 loss: 40.5644, MinusLogProbMetric: 40.5644, val_loss: 42.5575, val_MinusLogProbMetric: 42.5575

Epoch 524: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5644 - MinusLogProbMetric: 40.5644 - val_loss: 42.5575 - val_MinusLogProbMetric: 42.5575 - lr: 5.5556e-05 - 41s/epoch - 212ms/step
Epoch 525/1000
2023-10-28 15:55:15.758 
Epoch 525/1000 
	 loss: 40.5981, MinusLogProbMetric: 40.5981, val_loss: 42.5219, val_MinusLogProbMetric: 42.5219

Epoch 525: val_loss did not improve from 42.32182
196/196 - 42s - loss: 40.5981 - MinusLogProbMetric: 40.5981 - val_loss: 42.5219 - val_MinusLogProbMetric: 42.5219 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 526/1000
2023-10-28 15:55:56.889 
Epoch 526/1000 
	 loss: 40.4638, MinusLogProbMetric: 40.4638, val_loss: 43.1368, val_MinusLogProbMetric: 43.1368

Epoch 526: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4638 - MinusLogProbMetric: 40.4638 - val_loss: 43.1368 - val_MinusLogProbMetric: 43.1368 - lr: 5.5556e-05 - 41s/epoch - 210ms/step
Epoch 527/1000
2023-10-28 15:56:35.812 
Epoch 527/1000 
	 loss: 40.4654, MinusLogProbMetric: 40.4654, val_loss: 43.1878, val_MinusLogProbMetric: 43.1878

Epoch 527: val_loss did not improve from 42.32182
196/196 - 39s - loss: 40.4654 - MinusLogProbMetric: 40.4654 - val_loss: 43.1878 - val_MinusLogProbMetric: 43.1878 - lr: 5.5556e-05 - 39s/epoch - 199ms/step
Epoch 528/1000
2023-10-28 15:57:07.508 
Epoch 528/1000 
	 loss: 40.5743, MinusLogProbMetric: 40.5743, val_loss: 42.4607, val_MinusLogProbMetric: 42.4607

Epoch 528: val_loss did not improve from 42.32182
196/196 - 32s - loss: 40.5743 - MinusLogProbMetric: 40.5743 - val_loss: 42.4607 - val_MinusLogProbMetric: 42.4607 - lr: 5.5556e-05 - 32s/epoch - 162ms/step
Epoch 529/1000
2023-10-28 15:57:40.046 
Epoch 529/1000 
	 loss: 40.4809, MinusLogProbMetric: 40.4809, val_loss: 42.9430, val_MinusLogProbMetric: 42.9430

Epoch 529: val_loss did not improve from 42.32182
196/196 - 33s - loss: 40.4809 - MinusLogProbMetric: 40.4809 - val_loss: 42.9430 - val_MinusLogProbMetric: 42.9430 - lr: 5.5556e-05 - 33s/epoch - 166ms/step
Epoch 530/1000
2023-10-28 15:58:12.489 
Epoch 530/1000 
	 loss: 40.5012, MinusLogProbMetric: 40.5012, val_loss: 42.4852, val_MinusLogProbMetric: 42.4852

Epoch 530: val_loss did not improve from 42.32182
196/196 - 32s - loss: 40.5012 - MinusLogProbMetric: 40.5012 - val_loss: 42.4852 - val_MinusLogProbMetric: 42.4852 - lr: 5.5556e-05 - 32s/epoch - 166ms/step
Epoch 531/1000
2023-10-28 15:58:44.819 
Epoch 531/1000 
	 loss: 40.4472, MinusLogProbMetric: 40.4472, val_loss: 42.4752, val_MinusLogProbMetric: 42.4752

Epoch 531: val_loss did not improve from 42.32182
196/196 - 32s - loss: 40.4472 - MinusLogProbMetric: 40.4472 - val_loss: 42.4752 - val_MinusLogProbMetric: 42.4752 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 532/1000
2023-10-28 15:59:17.191 
Epoch 532/1000 
	 loss: 40.4497, MinusLogProbMetric: 40.4497, val_loss: 42.5637, val_MinusLogProbMetric: 42.5637

Epoch 532: val_loss did not improve from 42.32182
196/196 - 32s - loss: 40.4497 - MinusLogProbMetric: 40.4497 - val_loss: 42.5637 - val_MinusLogProbMetric: 42.5637 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 533/1000
2023-10-28 15:59:49.612 
Epoch 533/1000 
	 loss: 40.4994, MinusLogProbMetric: 40.4994, val_loss: 42.4998, val_MinusLogProbMetric: 42.4998

Epoch 533: val_loss did not improve from 42.32182
196/196 - 32s - loss: 40.4994 - MinusLogProbMetric: 40.4994 - val_loss: 42.4998 - val_MinusLogProbMetric: 42.4998 - lr: 5.5556e-05 - 32s/epoch - 165ms/step
Epoch 534/1000
2023-10-28 16:00:27.867 
Epoch 534/1000 
	 loss: 40.4939, MinusLogProbMetric: 40.4939, val_loss: 42.9551, val_MinusLogProbMetric: 42.9551

Epoch 534: val_loss did not improve from 42.32182
196/196 - 38s - loss: 40.4939 - MinusLogProbMetric: 40.4939 - val_loss: 42.9551 - val_MinusLogProbMetric: 42.9551 - lr: 5.5556e-05 - 38s/epoch - 195ms/step
Epoch 535/1000
2023-10-28 16:01:07.668 
Epoch 535/1000 
	 loss: 40.3981, MinusLogProbMetric: 40.3981, val_loss: 42.8422, val_MinusLogProbMetric: 42.8422

Epoch 535: val_loss did not improve from 42.32182
196/196 - 40s - loss: 40.3981 - MinusLogProbMetric: 40.3981 - val_loss: 42.8422 - val_MinusLogProbMetric: 42.8422 - lr: 5.5556e-05 - 40s/epoch - 203ms/step
Epoch 536/1000
2023-10-28 16:01:47.756 
Epoch 536/1000 
	 loss: 40.4652, MinusLogProbMetric: 40.4652, val_loss: 43.1155, val_MinusLogProbMetric: 43.1155

Epoch 536: val_loss did not improve from 42.32182
196/196 - 40s - loss: 40.4652 - MinusLogProbMetric: 40.4652 - val_loss: 43.1155 - val_MinusLogProbMetric: 43.1155 - lr: 5.5556e-05 - 40s/epoch - 205ms/step
Epoch 537/1000
2023-10-28 16:02:29.243 
Epoch 537/1000 
	 loss: 40.5084, MinusLogProbMetric: 40.5084, val_loss: 42.8873, val_MinusLogProbMetric: 42.8873

Epoch 537: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5084 - MinusLogProbMetric: 40.5084 - val_loss: 42.8873 - val_MinusLogProbMetric: 42.8873 - lr: 5.5556e-05 - 41s/epoch - 212ms/step
Epoch 538/1000
2023-10-28 16:03:09.772 
Epoch 538/1000 
	 loss: 40.4852, MinusLogProbMetric: 40.4852, val_loss: 42.6354, val_MinusLogProbMetric: 42.6354

Epoch 538: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4852 - MinusLogProbMetric: 40.4852 - val_loss: 42.6354 - val_MinusLogProbMetric: 42.6354 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 539/1000
2023-10-28 16:03:48.738 
Epoch 539/1000 
	 loss: 40.5376, MinusLogProbMetric: 40.5376, val_loss: 42.9049, val_MinusLogProbMetric: 42.9049

Epoch 539: val_loss did not improve from 42.32182
196/196 - 39s - loss: 40.5376 - MinusLogProbMetric: 40.5376 - val_loss: 42.9049 - val_MinusLogProbMetric: 42.9049 - lr: 5.5556e-05 - 39s/epoch - 199ms/step
Epoch 540/1000
2023-10-28 16:04:29.472 
Epoch 540/1000 
	 loss: 40.5179, MinusLogProbMetric: 40.5179, val_loss: 42.5531, val_MinusLogProbMetric: 42.5531

Epoch 540: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5179 - MinusLogProbMetric: 40.5179 - val_loss: 42.5531 - val_MinusLogProbMetric: 42.5531 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 541/1000
2023-10-28 16:05:10.394 
Epoch 541/1000 
	 loss: 40.3997, MinusLogProbMetric: 40.3997, val_loss: 42.5528, val_MinusLogProbMetric: 42.5528

Epoch 541: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.3997 - MinusLogProbMetric: 40.3997 - val_loss: 42.5528 - val_MinusLogProbMetric: 42.5528 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 542/1000
2023-10-28 16:05:51.717 
Epoch 542/1000 
	 loss: 40.4513, MinusLogProbMetric: 40.4513, val_loss: 42.6310, val_MinusLogProbMetric: 42.6310

Epoch 542: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4513 - MinusLogProbMetric: 40.4513 - val_loss: 42.6310 - val_MinusLogProbMetric: 42.6310 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 543/1000
2023-10-28 16:06:32.397 
Epoch 543/1000 
	 loss: 40.4642, MinusLogProbMetric: 40.4642, val_loss: 42.5061, val_MinusLogProbMetric: 42.5061

Epoch 543: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4642 - MinusLogProbMetric: 40.4642 - val_loss: 42.5061 - val_MinusLogProbMetric: 42.5061 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 544/1000
2023-10-28 16:07:12.902 
Epoch 544/1000 
	 loss: 40.4630, MinusLogProbMetric: 40.4630, val_loss: 42.3984, val_MinusLogProbMetric: 42.3984

Epoch 544: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4630 - MinusLogProbMetric: 40.4630 - val_loss: 42.3984 - val_MinusLogProbMetric: 42.3984 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 545/1000
2023-10-28 16:07:53.683 
Epoch 545/1000 
	 loss: 40.4325, MinusLogProbMetric: 40.4325, val_loss: 42.4607, val_MinusLogProbMetric: 42.4607

Epoch 545: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4325 - MinusLogProbMetric: 40.4325 - val_loss: 42.4607 - val_MinusLogProbMetric: 42.4607 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 546/1000
2023-10-28 16:08:34.242 
Epoch 546/1000 
	 loss: 40.4455, MinusLogProbMetric: 40.4455, val_loss: 42.6128, val_MinusLogProbMetric: 42.6128

Epoch 546: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4455 - MinusLogProbMetric: 40.4455 - val_loss: 42.6128 - val_MinusLogProbMetric: 42.6128 - lr: 5.5556e-05 - 41s/epoch - 207ms/step
Epoch 547/1000
2023-10-28 16:09:15.038 
Epoch 547/1000 
	 loss: 40.6581, MinusLogProbMetric: 40.6581, val_loss: 42.6090, val_MinusLogProbMetric: 42.6090

Epoch 547: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.6581 - MinusLogProbMetric: 40.6581 - val_loss: 42.6090 - val_MinusLogProbMetric: 42.6090 - lr: 5.5556e-05 - 41s/epoch - 208ms/step
Epoch 548/1000
2023-10-28 16:09:56.646 
Epoch 548/1000 
	 loss: 40.4435, MinusLogProbMetric: 40.4435, val_loss: 42.3699, val_MinusLogProbMetric: 42.3699

Epoch 548: val_loss did not improve from 42.32182
196/196 - 42s - loss: 40.4435 - MinusLogProbMetric: 40.4435 - val_loss: 42.3699 - val_MinusLogProbMetric: 42.3699 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 549/1000
2023-10-28 16:10:38.416 
Epoch 549/1000 
	 loss: 40.3784, MinusLogProbMetric: 40.3784, val_loss: 42.7992, val_MinusLogProbMetric: 42.7992

Epoch 549: val_loss did not improve from 42.32182
196/196 - 42s - loss: 40.3784 - MinusLogProbMetric: 40.3784 - val_loss: 42.7992 - val_MinusLogProbMetric: 42.7992 - lr: 5.5556e-05 - 42s/epoch - 213ms/step
Epoch 550/1000
2023-10-28 16:11:19.764 
Epoch 550/1000 
	 loss: 40.4259, MinusLogProbMetric: 40.4259, val_loss: 43.1225, val_MinusLogProbMetric: 43.1225

Epoch 550: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4259 - MinusLogProbMetric: 40.4259 - val_loss: 43.1225 - val_MinusLogProbMetric: 43.1225 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 551/1000
2023-10-28 16:12:00.805 
Epoch 551/1000 
	 loss: 40.4910, MinusLogProbMetric: 40.4910, val_loss: 42.5932, val_MinusLogProbMetric: 42.5932

Epoch 551: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4910 - MinusLogProbMetric: 40.4910 - val_loss: 42.5932 - val_MinusLogProbMetric: 42.5932 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 552/1000
2023-10-28 16:12:42.126 
Epoch 552/1000 
	 loss: 40.4283, MinusLogProbMetric: 40.4283, val_loss: 43.7756, val_MinusLogProbMetric: 43.7756

Epoch 552: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4283 - MinusLogProbMetric: 40.4283 - val_loss: 43.7756 - val_MinusLogProbMetric: 43.7756 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 553/1000
2023-10-28 16:13:23.397 
Epoch 553/1000 
	 loss: 40.5448, MinusLogProbMetric: 40.5448, val_loss: 42.6560, val_MinusLogProbMetric: 42.6560

Epoch 553: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.5448 - MinusLogProbMetric: 40.5448 - val_loss: 42.6560 - val_MinusLogProbMetric: 42.6560 - lr: 5.5556e-05 - 41s/epoch - 211ms/step
Epoch 554/1000
2023-10-28 16:14:04.392 
Epoch 554/1000 
	 loss: 40.4544, MinusLogProbMetric: 40.4544, val_loss: 43.2138, val_MinusLogProbMetric: 43.2138

Epoch 554: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.4544 - MinusLogProbMetric: 40.4544 - val_loss: 43.2138 - val_MinusLogProbMetric: 43.2138 - lr: 5.5556e-05 - 41s/epoch - 209ms/step
Epoch 555/1000
2023-10-28 16:14:44.170 
Epoch 555/1000 
	 loss: 40.3958, MinusLogProbMetric: 40.3958, val_loss: 42.4828, val_MinusLogProbMetric: 42.4828

Epoch 555: val_loss did not improve from 42.32182
196/196 - 40s - loss: 40.3958 - MinusLogProbMetric: 40.3958 - val_loss: 42.4828 - val_MinusLogProbMetric: 42.4828 - lr: 5.5556e-05 - 40s/epoch - 203ms/step
Epoch 556/1000
2023-10-28 16:15:23.890 
Epoch 556/1000 
	 loss: 40.4663, MinusLogProbMetric: 40.4663, val_loss: 42.4767, val_MinusLogProbMetric: 42.4767

Epoch 556: val_loss did not improve from 42.32182
196/196 - 40s - loss: 40.4663 - MinusLogProbMetric: 40.4663 - val_loss: 42.4767 - val_MinusLogProbMetric: 42.4767 - lr: 5.5556e-05 - 40s/epoch - 203ms/step
Epoch 557/1000
2023-10-28 16:16:05.426 
Epoch 557/1000 
	 loss: 40.4096, MinusLogProbMetric: 40.4096, val_loss: 42.9023, val_MinusLogProbMetric: 42.9023

Epoch 557: val_loss did not improve from 42.32182
196/196 - 42s - loss: 40.4096 - MinusLogProbMetric: 40.4096 - val_loss: 42.9023 - val_MinusLogProbMetric: 42.9023 - lr: 5.5556e-05 - 42s/epoch - 212ms/step
Epoch 558/1000
2023-10-28 16:16:46.069 
Epoch 558/1000 
	 loss: 40.0228, MinusLogProbMetric: 40.0228, val_loss: 42.4029, val_MinusLogProbMetric: 42.4029

Epoch 558: val_loss did not improve from 42.32182
196/196 - 41s - loss: 40.0228 - MinusLogProbMetric: 40.0228 - val_loss: 42.4029 - val_MinusLogProbMetric: 42.4029 - lr: 2.7778e-05 - 41s/epoch - 207ms/step
Epoch 559/1000
2023-10-28 16:17:26.439 
Epoch 559/1000 
	 loss: 40.0085, MinusLogProbMetric: 40.0085, val_loss: 42.1704, val_MinusLogProbMetric: 42.1704

Epoch 559: val_loss improved from 42.32182 to 42.17039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 41s - loss: 40.0085 - MinusLogProbMetric: 40.0085 - val_loss: 42.1704 - val_MinusLogProbMetric: 42.1704 - lr: 2.7778e-05 - 41s/epoch - 210ms/step
Epoch 560/1000
2023-10-28 16:18:08.314 
Epoch 560/1000 
	 loss: 40.0078, MinusLogProbMetric: 40.0078, val_loss: 42.2965, val_MinusLogProbMetric: 42.2965

Epoch 560: val_loss did not improve from 42.17039
196/196 - 41s - loss: 40.0078 - MinusLogProbMetric: 40.0078 - val_loss: 42.2965 - val_MinusLogProbMetric: 42.2965 - lr: 2.7778e-05 - 41s/epoch - 209ms/step
Epoch 561/1000
2023-10-28 16:18:49.576 
Epoch 561/1000 
	 loss: 40.0010, MinusLogProbMetric: 40.0010, val_loss: 42.2776, val_MinusLogProbMetric: 42.2776

Epoch 561: val_loss did not improve from 42.17039
196/196 - 41s - loss: 40.0010 - MinusLogProbMetric: 40.0010 - val_loss: 42.2776 - val_MinusLogProbMetric: 42.2776 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 562/1000
2023-10-28 16:19:29.094 
Epoch 562/1000 
	 loss: 40.0039, MinusLogProbMetric: 40.0039, val_loss: 42.2469, val_MinusLogProbMetric: 42.2469

Epoch 562: val_loss did not improve from 42.17039
196/196 - 40s - loss: 40.0039 - MinusLogProbMetric: 40.0039 - val_loss: 42.2469 - val_MinusLogProbMetric: 42.2469 - lr: 2.7778e-05 - 40s/epoch - 202ms/step
Epoch 563/1000
2023-10-28 16:20:11.147 
Epoch 563/1000 
	 loss: 40.0154, MinusLogProbMetric: 40.0154, val_loss: 42.2491, val_MinusLogProbMetric: 42.2491

Epoch 563: val_loss did not improve from 42.17039
196/196 - 42s - loss: 40.0154 - MinusLogProbMetric: 40.0154 - val_loss: 42.2491 - val_MinusLogProbMetric: 42.2491 - lr: 2.7778e-05 - 42s/epoch - 215ms/step
Epoch 564/1000
2023-10-28 16:20:51.260 
Epoch 564/1000 
	 loss: 40.0078, MinusLogProbMetric: 40.0078, val_loss: 42.1731, val_MinusLogProbMetric: 42.1731

Epoch 564: val_loss did not improve from 42.17039
196/196 - 40s - loss: 40.0078 - MinusLogProbMetric: 40.0078 - val_loss: 42.1731 - val_MinusLogProbMetric: 42.1731 - lr: 2.7778e-05 - 40s/epoch - 205ms/step
Epoch 565/1000
2023-10-28 16:21:31.907 
Epoch 565/1000 
	 loss: 40.0090, MinusLogProbMetric: 40.0090, val_loss: 42.3520, val_MinusLogProbMetric: 42.3520

Epoch 565: val_loss did not improve from 42.17039
196/196 - 41s - loss: 40.0090 - MinusLogProbMetric: 40.0090 - val_loss: 42.3520 - val_MinusLogProbMetric: 42.3520 - lr: 2.7778e-05 - 41s/epoch - 207ms/step
Epoch 566/1000
2023-10-28 16:22:12.426 
Epoch 566/1000 
	 loss: 40.0176, MinusLogProbMetric: 40.0176, val_loss: 42.2490, val_MinusLogProbMetric: 42.2490

Epoch 566: val_loss did not improve from 42.17039
196/196 - 41s - loss: 40.0176 - MinusLogProbMetric: 40.0176 - val_loss: 42.2490 - val_MinusLogProbMetric: 42.2490 - lr: 2.7778e-05 - 41s/epoch - 207ms/step
Epoch 567/1000
2023-10-28 16:22:53.181 
Epoch 567/1000 
	 loss: 39.9980, MinusLogProbMetric: 39.9980, val_loss: 42.2351, val_MinusLogProbMetric: 42.2351

Epoch 567: val_loss did not improve from 42.17039
196/196 - 41s - loss: 39.9980 - MinusLogProbMetric: 39.9980 - val_loss: 42.2351 - val_MinusLogProbMetric: 42.2351 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 568/1000
2023-10-28 16:23:34.114 
Epoch 568/1000 
	 loss: 40.0116, MinusLogProbMetric: 40.0116, val_loss: 42.2523, val_MinusLogProbMetric: 42.2523

Epoch 568: val_loss did not improve from 42.17039
196/196 - 41s - loss: 40.0116 - MinusLogProbMetric: 40.0116 - val_loss: 42.2523 - val_MinusLogProbMetric: 42.2523 - lr: 2.7778e-05 - 41s/epoch - 209ms/step
Epoch 569/1000
2023-10-28 16:24:15.449 
Epoch 569/1000 
	 loss: 40.0125, MinusLogProbMetric: 40.0125, val_loss: 42.5629, val_MinusLogProbMetric: 42.5629

Epoch 569: val_loss did not improve from 42.17039
196/196 - 41s - loss: 40.0125 - MinusLogProbMetric: 40.0125 - val_loss: 42.5629 - val_MinusLogProbMetric: 42.5629 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 570/1000
2023-10-28 16:24:56.377 
Epoch 570/1000 
	 loss: 39.9938, MinusLogProbMetric: 39.9938, val_loss: 42.1401, val_MinusLogProbMetric: 42.1401

Epoch 570: val_loss improved from 42.17039 to 42.14010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 39.9938 - MinusLogProbMetric: 39.9938 - val_loss: 42.1401 - val_MinusLogProbMetric: 42.1401 - lr: 2.7778e-05 - 42s/epoch - 213ms/step
Epoch 571/1000
2023-10-28 16:25:37.896 
Epoch 571/1000 
	 loss: 40.0155, MinusLogProbMetric: 40.0155, val_loss: 42.2175, val_MinusLogProbMetric: 42.2175

Epoch 571: val_loss did not improve from 42.14010
196/196 - 41s - loss: 40.0155 - MinusLogProbMetric: 40.0155 - val_loss: 42.2175 - val_MinusLogProbMetric: 42.2175 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 572/1000
2023-10-28 16:26:19.473 
Epoch 572/1000 
	 loss: 39.9960, MinusLogProbMetric: 39.9960, val_loss: 42.2798, val_MinusLogProbMetric: 42.2798

Epoch 572: val_loss did not improve from 42.14010
196/196 - 42s - loss: 39.9960 - MinusLogProbMetric: 39.9960 - val_loss: 42.2798 - val_MinusLogProbMetric: 42.2798 - lr: 2.7778e-05 - 42s/epoch - 212ms/step
Epoch 573/1000
2023-10-28 16:26:59.766 
Epoch 573/1000 
	 loss: 40.0177, MinusLogProbMetric: 40.0177, val_loss: 42.2482, val_MinusLogProbMetric: 42.2482

Epoch 573: val_loss did not improve from 42.14010
196/196 - 40s - loss: 40.0177 - MinusLogProbMetric: 40.0177 - val_loss: 42.2482 - val_MinusLogProbMetric: 42.2482 - lr: 2.7778e-05 - 40s/epoch - 206ms/step
Epoch 574/1000
2023-10-28 16:27:40.160 
Epoch 574/1000 
	 loss: 39.9863, MinusLogProbMetric: 39.9863, val_loss: 42.2480, val_MinusLogProbMetric: 42.2480

Epoch 574: val_loss did not improve from 42.14010
196/196 - 40s - loss: 39.9863 - MinusLogProbMetric: 39.9863 - val_loss: 42.2480 - val_MinusLogProbMetric: 42.2480 - lr: 2.7778e-05 - 40s/epoch - 206ms/step
Epoch 575/1000
2023-10-28 16:28:21.132 
Epoch 575/1000 
	 loss: 39.9973, MinusLogProbMetric: 39.9973, val_loss: 42.3998, val_MinusLogProbMetric: 42.3998

Epoch 575: val_loss did not improve from 42.14010
196/196 - 41s - loss: 39.9973 - MinusLogProbMetric: 39.9973 - val_loss: 42.3998 - val_MinusLogProbMetric: 42.3998 - lr: 2.7778e-05 - 41s/epoch - 209ms/step
Epoch 576/1000
2023-10-28 16:29:01.679 
Epoch 576/1000 
	 loss: 39.9889, MinusLogProbMetric: 39.9889, val_loss: 42.3094, val_MinusLogProbMetric: 42.3094

Epoch 576: val_loss did not improve from 42.14010
196/196 - 41s - loss: 39.9889 - MinusLogProbMetric: 39.9889 - val_loss: 42.3094 - val_MinusLogProbMetric: 42.3094 - lr: 2.7778e-05 - 41s/epoch - 207ms/step
Epoch 577/1000
2023-10-28 16:29:42.014 
Epoch 577/1000 
	 loss: 39.9999, MinusLogProbMetric: 39.9999, val_loss: 42.2646, val_MinusLogProbMetric: 42.2646

Epoch 577: val_loss did not improve from 42.14010
196/196 - 40s - loss: 39.9999 - MinusLogProbMetric: 39.9999 - val_loss: 42.2646 - val_MinusLogProbMetric: 42.2646 - lr: 2.7778e-05 - 40s/epoch - 206ms/step
Epoch 578/1000
2023-10-28 16:30:21.627 
Epoch 578/1000 
	 loss: 39.9874, MinusLogProbMetric: 39.9874, val_loss: 42.3418, val_MinusLogProbMetric: 42.3418

Epoch 578: val_loss did not improve from 42.14010
196/196 - 40s - loss: 39.9874 - MinusLogProbMetric: 39.9874 - val_loss: 42.3418 - val_MinusLogProbMetric: 42.3418 - lr: 2.7778e-05 - 40s/epoch - 202ms/step
Epoch 579/1000
2023-10-28 16:31:01.460 
Epoch 579/1000 
	 loss: 39.9741, MinusLogProbMetric: 39.9741, val_loss: 42.2407, val_MinusLogProbMetric: 42.2407

Epoch 579: val_loss did not improve from 42.14010
196/196 - 40s - loss: 39.9741 - MinusLogProbMetric: 39.9741 - val_loss: 42.2407 - val_MinusLogProbMetric: 42.2407 - lr: 2.7778e-05 - 40s/epoch - 203ms/step
Epoch 580/1000
2023-10-28 16:31:42.492 
Epoch 580/1000 
	 loss: 40.0299, MinusLogProbMetric: 40.0299, val_loss: 42.4021, val_MinusLogProbMetric: 42.4021

Epoch 580: val_loss did not improve from 42.14010
196/196 - 41s - loss: 40.0299 - MinusLogProbMetric: 40.0299 - val_loss: 42.4021 - val_MinusLogProbMetric: 42.4021 - lr: 2.7778e-05 - 41s/epoch - 209ms/step
Epoch 581/1000
2023-10-28 16:32:23.537 
Epoch 581/1000 
	 loss: 39.9815, MinusLogProbMetric: 39.9815, val_loss: 42.1123, val_MinusLogProbMetric: 42.1123

Epoch 581: val_loss improved from 42.14010 to 42.11228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_436/weights/best_weights.h5
196/196 - 42s - loss: 39.9815 - MinusLogProbMetric: 39.9815 - val_loss: 42.1123 - val_MinusLogProbMetric: 42.1123 - lr: 2.7778e-05 - 42s/epoch - 214ms/step
Epoch 582/1000
2023-10-28 16:33:05.096 
Epoch 582/1000 
	 loss: 39.9872, MinusLogProbMetric: 39.9872, val_loss: 42.3317, val_MinusLogProbMetric: 42.3317

Epoch 582: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9872 - MinusLogProbMetric: 39.9872 - val_loss: 42.3317 - val_MinusLogProbMetric: 42.3317 - lr: 2.7778e-05 - 41s/epoch - 207ms/step
Epoch 583/1000
2023-10-28 16:33:44.875 
Epoch 583/1000 
	 loss: 39.9739, MinusLogProbMetric: 39.9739, val_loss: 42.3406, val_MinusLogProbMetric: 42.3406

Epoch 583: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.9739 - MinusLogProbMetric: 39.9739 - val_loss: 42.3406 - val_MinusLogProbMetric: 42.3406 - lr: 2.7778e-05 - 40s/epoch - 203ms/step
Epoch 584/1000
2023-10-28 16:34:25.574 
Epoch 584/1000 
	 loss: 40.0081, MinusLogProbMetric: 40.0081, val_loss: 42.2582, val_MinusLogProbMetric: 42.2582

Epoch 584: val_loss did not improve from 42.11228
196/196 - 41s - loss: 40.0081 - MinusLogProbMetric: 40.0081 - val_loss: 42.2582 - val_MinusLogProbMetric: 42.2582 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 585/1000
2023-10-28 16:35:06.690 
Epoch 585/1000 
	 loss: 39.9816, MinusLogProbMetric: 39.9816, val_loss: 42.2970, val_MinusLogProbMetric: 42.2970

Epoch 585: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9816 - MinusLogProbMetric: 39.9816 - val_loss: 42.2970 - val_MinusLogProbMetric: 42.2970 - lr: 2.7778e-05 - 41s/epoch - 210ms/step
Epoch 586/1000
2023-10-28 16:35:47.383 
Epoch 586/1000 
	 loss: 39.9587, MinusLogProbMetric: 39.9587, val_loss: 42.2762, val_MinusLogProbMetric: 42.2762

Epoch 586: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9587 - MinusLogProbMetric: 39.9587 - val_loss: 42.2762 - val_MinusLogProbMetric: 42.2762 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 587/1000
2023-10-28 16:36:28.765 
Epoch 587/1000 
	 loss: 39.9581, MinusLogProbMetric: 39.9581, val_loss: 42.3206, val_MinusLogProbMetric: 42.3206

Epoch 587: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9581 - MinusLogProbMetric: 39.9581 - val_loss: 42.3206 - val_MinusLogProbMetric: 42.3206 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 588/1000
2023-10-28 16:37:10.043 
Epoch 588/1000 
	 loss: 39.9959, MinusLogProbMetric: 39.9959, val_loss: 42.3234, val_MinusLogProbMetric: 42.3234

Epoch 588: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9959 - MinusLogProbMetric: 39.9959 - val_loss: 42.3234 - val_MinusLogProbMetric: 42.3234 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 589/1000
2023-10-28 16:37:51.219 
Epoch 589/1000 
	 loss: 39.9644, MinusLogProbMetric: 39.9644, val_loss: 42.3677, val_MinusLogProbMetric: 42.3677

Epoch 589: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9644 - MinusLogProbMetric: 39.9644 - val_loss: 42.3677 - val_MinusLogProbMetric: 42.3677 - lr: 2.7778e-05 - 41s/epoch - 210ms/step
Epoch 590/1000
2023-10-28 16:38:31.223 
Epoch 590/1000 
	 loss: 39.9585, MinusLogProbMetric: 39.9585, val_loss: 42.2550, val_MinusLogProbMetric: 42.2550

Epoch 590: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.9585 - MinusLogProbMetric: 39.9585 - val_loss: 42.2550 - val_MinusLogProbMetric: 42.2550 - lr: 2.7778e-05 - 40s/epoch - 204ms/step
Epoch 591/1000
2023-10-28 16:39:11.654 
Epoch 591/1000 
	 loss: 39.9603, MinusLogProbMetric: 39.9603, val_loss: 42.3223, val_MinusLogProbMetric: 42.3223

Epoch 591: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.9603 - MinusLogProbMetric: 39.9603 - val_loss: 42.3223 - val_MinusLogProbMetric: 42.3223 - lr: 2.7778e-05 - 40s/epoch - 206ms/step
Epoch 592/1000
2023-10-28 16:39:50.644 
Epoch 592/1000 
	 loss: 39.9603, MinusLogProbMetric: 39.9603, val_loss: 42.2979, val_MinusLogProbMetric: 42.2979

Epoch 592: val_loss did not improve from 42.11228
196/196 - 39s - loss: 39.9603 - MinusLogProbMetric: 39.9603 - val_loss: 42.2979 - val_MinusLogProbMetric: 42.2979 - lr: 2.7778e-05 - 39s/epoch - 199ms/step
Epoch 593/1000
2023-10-28 16:40:31.328 
Epoch 593/1000 
	 loss: 39.9522, MinusLogProbMetric: 39.9522, val_loss: 42.4955, val_MinusLogProbMetric: 42.4955

Epoch 593: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9522 - MinusLogProbMetric: 39.9522 - val_loss: 42.4955 - val_MinusLogProbMetric: 42.4955 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 594/1000
2023-10-28 16:41:11.818 
Epoch 594/1000 
	 loss: 40.0037, MinusLogProbMetric: 40.0037, val_loss: 42.2626, val_MinusLogProbMetric: 42.2626

Epoch 594: val_loss did not improve from 42.11228
196/196 - 40s - loss: 40.0037 - MinusLogProbMetric: 40.0037 - val_loss: 42.2626 - val_MinusLogProbMetric: 42.2626 - lr: 2.7778e-05 - 40s/epoch - 207ms/step
Epoch 595/1000
2023-10-28 16:41:52.431 
Epoch 595/1000 
	 loss: 39.9591, MinusLogProbMetric: 39.9591, val_loss: 42.2614, val_MinusLogProbMetric: 42.2614

Epoch 595: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9591 - MinusLogProbMetric: 39.9591 - val_loss: 42.2614 - val_MinusLogProbMetric: 42.2614 - lr: 2.7778e-05 - 41s/epoch - 207ms/step
Epoch 596/1000
2023-10-28 16:42:33.785 
Epoch 596/1000 
	 loss: 40.0041, MinusLogProbMetric: 40.0041, val_loss: 42.2501, val_MinusLogProbMetric: 42.2501

Epoch 596: val_loss did not improve from 42.11228
196/196 - 41s - loss: 40.0041 - MinusLogProbMetric: 40.0041 - val_loss: 42.2501 - val_MinusLogProbMetric: 42.2501 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 597/1000
2023-10-28 16:43:15.160 
Epoch 597/1000 
	 loss: 39.9675, MinusLogProbMetric: 39.9675, val_loss: 42.4150, val_MinusLogProbMetric: 42.4150

Epoch 597: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9675 - MinusLogProbMetric: 39.9675 - val_loss: 42.4150 - val_MinusLogProbMetric: 42.4150 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 598/1000
2023-10-28 16:43:55.343 
Epoch 598/1000 
	 loss: 39.9743, MinusLogProbMetric: 39.9743, val_loss: 42.1982, val_MinusLogProbMetric: 42.1982

Epoch 598: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.9743 - MinusLogProbMetric: 39.9743 - val_loss: 42.1982 - val_MinusLogProbMetric: 42.1982 - lr: 2.7778e-05 - 40s/epoch - 205ms/step
Epoch 599/1000
2023-10-28 16:44:36.576 
Epoch 599/1000 
	 loss: 39.9577, MinusLogProbMetric: 39.9577, val_loss: 42.2332, val_MinusLogProbMetric: 42.2332

Epoch 599: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9577 - MinusLogProbMetric: 39.9577 - val_loss: 42.2332 - val_MinusLogProbMetric: 42.2332 - lr: 2.7778e-05 - 41s/epoch - 210ms/step
Epoch 600/1000
2023-10-28 16:45:17.282 
Epoch 600/1000 
	 loss: 39.9685, MinusLogProbMetric: 39.9685, val_loss: 42.3496, val_MinusLogProbMetric: 42.3496

Epoch 600: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9685 - MinusLogProbMetric: 39.9685 - val_loss: 42.3496 - val_MinusLogProbMetric: 42.3496 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 601/1000
2023-10-28 16:45:59.300 
Epoch 601/1000 
	 loss: 39.9589, MinusLogProbMetric: 39.9589, val_loss: 42.2638, val_MinusLogProbMetric: 42.2638

Epoch 601: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.9589 - MinusLogProbMetric: 39.9589 - val_loss: 42.2638 - val_MinusLogProbMetric: 42.2638 - lr: 2.7778e-05 - 42s/epoch - 214ms/step
Epoch 602/1000
2023-10-28 16:46:40.066 
Epoch 602/1000 
	 loss: 39.9598, MinusLogProbMetric: 39.9598, val_loss: 42.3484, val_MinusLogProbMetric: 42.3484

Epoch 602: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9598 - MinusLogProbMetric: 39.9598 - val_loss: 42.3484 - val_MinusLogProbMetric: 42.3484 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 603/1000
2023-10-28 16:47:20.284 
Epoch 603/1000 
	 loss: 39.9792, MinusLogProbMetric: 39.9792, val_loss: 42.4517, val_MinusLogProbMetric: 42.4517

Epoch 603: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.9792 - MinusLogProbMetric: 39.9792 - val_loss: 42.4517 - val_MinusLogProbMetric: 42.4517 - lr: 2.7778e-05 - 40s/epoch - 205ms/step
Epoch 604/1000
2023-10-28 16:48:01.493 
Epoch 604/1000 
	 loss: 39.9545, MinusLogProbMetric: 39.9545, val_loss: 42.2655, val_MinusLogProbMetric: 42.2655

Epoch 604: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9545 - MinusLogProbMetric: 39.9545 - val_loss: 42.2655 - val_MinusLogProbMetric: 42.2655 - lr: 2.7778e-05 - 41s/epoch - 210ms/step
Epoch 605/1000
2023-10-28 16:48:42.300 
Epoch 605/1000 
	 loss: 39.9426, MinusLogProbMetric: 39.9426, val_loss: 42.3604, val_MinusLogProbMetric: 42.3604

Epoch 605: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9426 - MinusLogProbMetric: 39.9426 - val_loss: 42.3604 - val_MinusLogProbMetric: 42.3604 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 606/1000
2023-10-28 16:49:23.221 
Epoch 606/1000 
	 loss: 39.9520, MinusLogProbMetric: 39.9520, val_loss: 42.2914, val_MinusLogProbMetric: 42.2914

Epoch 606: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9520 - MinusLogProbMetric: 39.9520 - val_loss: 42.2914 - val_MinusLogProbMetric: 42.2914 - lr: 2.7778e-05 - 41s/epoch - 209ms/step
Epoch 607/1000
2023-10-28 16:50:04.395 
Epoch 607/1000 
	 loss: 39.9850, MinusLogProbMetric: 39.9850, val_loss: 42.2506, val_MinusLogProbMetric: 42.2506

Epoch 607: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9850 - MinusLogProbMetric: 39.9850 - val_loss: 42.2506 - val_MinusLogProbMetric: 42.2506 - lr: 2.7778e-05 - 41s/epoch - 210ms/step
Epoch 608/1000
2023-10-28 16:50:45.222 
Epoch 608/1000 
	 loss: 39.9839, MinusLogProbMetric: 39.9839, val_loss: 42.4481, val_MinusLogProbMetric: 42.4481

Epoch 608: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9839 - MinusLogProbMetric: 39.9839 - val_loss: 42.4481 - val_MinusLogProbMetric: 42.4481 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 609/1000
2023-10-28 16:51:25.998 
Epoch 609/1000 
	 loss: 39.9332, MinusLogProbMetric: 39.9332, val_loss: 42.2209, val_MinusLogProbMetric: 42.2209

Epoch 609: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9332 - MinusLogProbMetric: 39.9332 - val_loss: 42.2209 - val_MinusLogProbMetric: 42.2209 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 610/1000
2023-10-28 16:52:07.850 
Epoch 610/1000 
	 loss: 39.9642, MinusLogProbMetric: 39.9642, val_loss: 42.1610, val_MinusLogProbMetric: 42.1610

Epoch 610: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.9642 - MinusLogProbMetric: 39.9642 - val_loss: 42.1610 - val_MinusLogProbMetric: 42.1610 - lr: 2.7778e-05 - 42s/epoch - 214ms/step
Epoch 611/1000
2023-10-28 16:52:48.434 
Epoch 611/1000 
	 loss: 39.9741, MinusLogProbMetric: 39.9741, val_loss: 42.3198, val_MinusLogProbMetric: 42.3198

Epoch 611: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9741 - MinusLogProbMetric: 39.9741 - val_loss: 42.3198 - val_MinusLogProbMetric: 42.3198 - lr: 2.7778e-05 - 41s/epoch - 207ms/step
Epoch 612/1000
2023-10-28 16:53:25.368 
Epoch 612/1000 
	 loss: 39.9708, MinusLogProbMetric: 39.9708, val_loss: 42.3956, val_MinusLogProbMetric: 42.3956

Epoch 612: val_loss did not improve from 42.11228
196/196 - 37s - loss: 39.9708 - MinusLogProbMetric: 39.9708 - val_loss: 42.3956 - val_MinusLogProbMetric: 42.3956 - lr: 2.7778e-05 - 37s/epoch - 188ms/step
Epoch 613/1000
2023-10-28 16:53:56.818 
Epoch 613/1000 
	 loss: 39.9553, MinusLogProbMetric: 39.9553, val_loss: 42.3030, val_MinusLogProbMetric: 42.3030

Epoch 613: val_loss did not improve from 42.11228
196/196 - 31s - loss: 39.9553 - MinusLogProbMetric: 39.9553 - val_loss: 42.3030 - val_MinusLogProbMetric: 42.3030 - lr: 2.7778e-05 - 31s/epoch - 160ms/step
Epoch 614/1000
2023-10-28 16:54:29.420 
Epoch 614/1000 
	 loss: 39.9636, MinusLogProbMetric: 39.9636, val_loss: 42.3659, val_MinusLogProbMetric: 42.3659

Epoch 614: val_loss did not improve from 42.11228
196/196 - 33s - loss: 39.9636 - MinusLogProbMetric: 39.9636 - val_loss: 42.3659 - val_MinusLogProbMetric: 42.3659 - lr: 2.7778e-05 - 33s/epoch - 166ms/step
Epoch 615/1000
2023-10-28 16:55:02.035 
Epoch 615/1000 
	 loss: 39.9534, MinusLogProbMetric: 39.9534, val_loss: 42.3643, val_MinusLogProbMetric: 42.3643

Epoch 615: val_loss did not improve from 42.11228
196/196 - 33s - loss: 39.9534 - MinusLogProbMetric: 39.9534 - val_loss: 42.3643 - val_MinusLogProbMetric: 42.3643 - lr: 2.7778e-05 - 33s/epoch - 166ms/step
Epoch 616/1000
2023-10-28 16:55:35.953 
Epoch 616/1000 
	 loss: 39.9436, MinusLogProbMetric: 39.9436, val_loss: 42.3992, val_MinusLogProbMetric: 42.3992

Epoch 616: val_loss did not improve from 42.11228
196/196 - 34s - loss: 39.9436 - MinusLogProbMetric: 39.9436 - val_loss: 42.3992 - val_MinusLogProbMetric: 42.3992 - lr: 2.7778e-05 - 34s/epoch - 173ms/step
Epoch 617/1000
2023-10-28 16:56:09.544 
Epoch 617/1000 
	 loss: 39.9298, MinusLogProbMetric: 39.9298, val_loss: 42.3548, val_MinusLogProbMetric: 42.3548

Epoch 617: val_loss did not improve from 42.11228
196/196 - 34s - loss: 39.9298 - MinusLogProbMetric: 39.9298 - val_loss: 42.3548 - val_MinusLogProbMetric: 42.3548 - lr: 2.7778e-05 - 34s/epoch - 171ms/step
Epoch 618/1000
2023-10-28 16:56:43.083 
Epoch 618/1000 
	 loss: 39.9744, MinusLogProbMetric: 39.9744, val_loss: 42.3249, val_MinusLogProbMetric: 42.3249

Epoch 618: val_loss did not improve from 42.11228
196/196 - 34s - loss: 39.9744 - MinusLogProbMetric: 39.9744 - val_loss: 42.3249 - val_MinusLogProbMetric: 42.3249 - lr: 2.7778e-05 - 34s/epoch - 171ms/step
Epoch 619/1000
2023-10-28 16:57:20.188 
Epoch 619/1000 
	 loss: 39.9461, MinusLogProbMetric: 39.9461, val_loss: 42.2722, val_MinusLogProbMetric: 42.2722

Epoch 619: val_loss did not improve from 42.11228
196/196 - 37s - loss: 39.9461 - MinusLogProbMetric: 39.9461 - val_loss: 42.2722 - val_MinusLogProbMetric: 42.2722 - lr: 2.7778e-05 - 37s/epoch - 189ms/step
Epoch 620/1000
2023-10-28 16:58:01.548 
Epoch 620/1000 
	 loss: 39.9433, MinusLogProbMetric: 39.9433, val_loss: 42.2828, val_MinusLogProbMetric: 42.2828

Epoch 620: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9433 - MinusLogProbMetric: 39.9433 - val_loss: 42.2828 - val_MinusLogProbMetric: 42.2828 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 621/1000
2023-10-28 16:58:42.935 
Epoch 621/1000 
	 loss: 39.9635, MinusLogProbMetric: 39.9635, val_loss: 42.3078, val_MinusLogProbMetric: 42.3078

Epoch 621: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9635 - MinusLogProbMetric: 39.9635 - val_loss: 42.3078 - val_MinusLogProbMetric: 42.3078 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 622/1000
2023-10-28 16:59:23.669 
Epoch 622/1000 
	 loss: 39.9344, MinusLogProbMetric: 39.9344, val_loss: 42.3210, val_MinusLogProbMetric: 42.3210

Epoch 622: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9344 - MinusLogProbMetric: 39.9344 - val_loss: 42.3210 - val_MinusLogProbMetric: 42.3210 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 623/1000
2023-10-28 17:00:02.778 
Epoch 623/1000 
	 loss: 39.9867, MinusLogProbMetric: 39.9867, val_loss: 42.3869, val_MinusLogProbMetric: 42.3869

Epoch 623: val_loss did not improve from 42.11228
196/196 - 39s - loss: 39.9867 - MinusLogProbMetric: 39.9867 - val_loss: 42.3869 - val_MinusLogProbMetric: 42.3869 - lr: 2.7778e-05 - 39s/epoch - 200ms/step
Epoch 624/1000
2023-10-28 17:00:44.281 
Epoch 624/1000 
	 loss: 39.9736, MinusLogProbMetric: 39.9736, val_loss: 42.2975, val_MinusLogProbMetric: 42.2975

Epoch 624: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.9736 - MinusLogProbMetric: 39.9736 - val_loss: 42.2975 - val_MinusLogProbMetric: 42.2975 - lr: 2.7778e-05 - 42s/epoch - 212ms/step
Epoch 625/1000
2023-10-28 17:01:24.987 
Epoch 625/1000 
	 loss: 39.9518, MinusLogProbMetric: 39.9518, val_loss: 42.4406, val_MinusLogProbMetric: 42.4406

Epoch 625: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9518 - MinusLogProbMetric: 39.9518 - val_loss: 42.4406 - val_MinusLogProbMetric: 42.4406 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 626/1000
2023-10-28 17:02:05.745 
Epoch 626/1000 
	 loss: 39.9440, MinusLogProbMetric: 39.9440, val_loss: 42.3727, val_MinusLogProbMetric: 42.3727

Epoch 626: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9440 - MinusLogProbMetric: 39.9440 - val_loss: 42.3727 - val_MinusLogProbMetric: 42.3727 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 627/1000
2023-10-28 17:02:46.555 
Epoch 627/1000 
	 loss: 39.9475, MinusLogProbMetric: 39.9475, val_loss: 42.3273, val_MinusLogProbMetric: 42.3273

Epoch 627: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9475 - MinusLogProbMetric: 39.9475 - val_loss: 42.3273 - val_MinusLogProbMetric: 42.3273 - lr: 2.7778e-05 - 41s/epoch - 208ms/step
Epoch 628/1000
2023-10-28 17:03:27.637 
Epoch 628/1000 
	 loss: 39.9638, MinusLogProbMetric: 39.9638, val_loss: 42.4122, val_MinusLogProbMetric: 42.4122

Epoch 628: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9638 - MinusLogProbMetric: 39.9638 - val_loss: 42.4122 - val_MinusLogProbMetric: 42.4122 - lr: 2.7778e-05 - 41s/epoch - 210ms/step
Epoch 629/1000
2023-10-28 17:04:08.193 
Epoch 629/1000 
	 loss: 39.9482, MinusLogProbMetric: 39.9482, val_loss: 42.4368, val_MinusLogProbMetric: 42.4368

Epoch 629: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9482 - MinusLogProbMetric: 39.9482 - val_loss: 42.4368 - val_MinusLogProbMetric: 42.4368 - lr: 2.7778e-05 - 41s/epoch - 207ms/step
Epoch 630/1000
2023-10-28 17:04:49.548 
Epoch 630/1000 
	 loss: 39.9576, MinusLogProbMetric: 39.9576, val_loss: 42.4554, val_MinusLogProbMetric: 42.4554

Epoch 630: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.9576 - MinusLogProbMetric: 39.9576 - val_loss: 42.4554 - val_MinusLogProbMetric: 42.4554 - lr: 2.7778e-05 - 41s/epoch - 211ms/step
Epoch 631/1000
2023-10-28 17:05:31.265 
Epoch 631/1000 
	 loss: 39.9383, MinusLogProbMetric: 39.9383, val_loss: 42.1730, val_MinusLogProbMetric: 42.1730

Epoch 631: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.9383 - MinusLogProbMetric: 39.9383 - val_loss: 42.1730 - val_MinusLogProbMetric: 42.1730 - lr: 2.7778e-05 - 42s/epoch - 213ms/step
Epoch 632/1000
2023-10-28 17:06:12.503 
Epoch 632/1000 
	 loss: 39.8080, MinusLogProbMetric: 39.8080, val_loss: 42.3033, val_MinusLogProbMetric: 42.3033

Epoch 632: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.8080 - MinusLogProbMetric: 39.8080 - val_loss: 42.3033 - val_MinusLogProbMetric: 42.3033 - lr: 1.3889e-05 - 41s/epoch - 210ms/step
Epoch 633/1000
2023-10-28 17:06:52.766 
Epoch 633/1000 
	 loss: 39.8063, MinusLogProbMetric: 39.8063, val_loss: 42.2221, val_MinusLogProbMetric: 42.2221

Epoch 633: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.8063 - MinusLogProbMetric: 39.8063 - val_loss: 42.2221 - val_MinusLogProbMetric: 42.2221 - lr: 1.3889e-05 - 40s/epoch - 205ms/step
Epoch 634/1000
2023-10-28 17:07:33.391 
Epoch 634/1000 
	 loss: 39.7967, MinusLogProbMetric: 39.7967, val_loss: 42.1867, val_MinusLogProbMetric: 42.1867

Epoch 634: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7967 - MinusLogProbMetric: 39.7967 - val_loss: 42.1867 - val_MinusLogProbMetric: 42.1867 - lr: 1.3889e-05 - 41s/epoch - 207ms/step
Epoch 635/1000
2023-10-28 17:08:14.422 
Epoch 635/1000 
	 loss: 39.8006, MinusLogProbMetric: 39.8006, val_loss: 42.2052, val_MinusLogProbMetric: 42.2052

Epoch 635: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.8006 - MinusLogProbMetric: 39.8006 - val_loss: 42.2052 - val_MinusLogProbMetric: 42.2052 - lr: 1.3889e-05 - 41s/epoch - 209ms/step
Epoch 636/1000
2023-10-28 17:08:55.484 
Epoch 636/1000 
	 loss: 39.8019, MinusLogProbMetric: 39.8019, val_loss: 42.1966, val_MinusLogProbMetric: 42.1966

Epoch 636: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.8019 - MinusLogProbMetric: 39.8019 - val_loss: 42.1966 - val_MinusLogProbMetric: 42.1966 - lr: 1.3889e-05 - 41s/epoch - 210ms/step
Epoch 637/1000
2023-10-28 17:09:34.737 
Epoch 637/1000 
	 loss: 39.7968, MinusLogProbMetric: 39.7968, val_loss: 42.2357, val_MinusLogProbMetric: 42.2357

Epoch 637: val_loss did not improve from 42.11228
196/196 - 39s - loss: 39.7968 - MinusLogProbMetric: 39.7968 - val_loss: 42.2357 - val_MinusLogProbMetric: 42.2357 - lr: 1.3889e-05 - 39s/epoch - 200ms/step
Epoch 638/1000
2023-10-28 17:10:14.062 
Epoch 638/1000 
	 loss: 39.8055, MinusLogProbMetric: 39.8055, val_loss: 42.2142, val_MinusLogProbMetric: 42.2142

Epoch 638: val_loss did not improve from 42.11228
196/196 - 39s - loss: 39.8055 - MinusLogProbMetric: 39.8055 - val_loss: 42.2142 - val_MinusLogProbMetric: 42.2142 - lr: 1.3889e-05 - 39s/epoch - 201ms/step
Epoch 639/1000
2023-10-28 17:10:55.636 
Epoch 639/1000 
	 loss: 39.8021, MinusLogProbMetric: 39.8021, val_loss: 42.2653, val_MinusLogProbMetric: 42.2653

Epoch 639: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.8021 - MinusLogProbMetric: 39.8021 - val_loss: 42.2653 - val_MinusLogProbMetric: 42.2653 - lr: 1.3889e-05 - 42s/epoch - 212ms/step
Epoch 640/1000
2023-10-28 17:11:37.213 
Epoch 640/1000 
	 loss: 39.7943, MinusLogProbMetric: 39.7943, val_loss: 42.2524, val_MinusLogProbMetric: 42.2524

Epoch 640: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.7943 - MinusLogProbMetric: 39.7943 - val_loss: 42.2524 - val_MinusLogProbMetric: 42.2524 - lr: 1.3889e-05 - 42s/epoch - 212ms/step
Epoch 641/1000
2023-10-28 17:12:18.925 
Epoch 641/1000 
	 loss: 39.7922, MinusLogProbMetric: 39.7922, val_loss: 42.1382, val_MinusLogProbMetric: 42.1382

Epoch 641: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.7922 - MinusLogProbMetric: 39.7922 - val_loss: 42.1382 - val_MinusLogProbMetric: 42.1382 - lr: 1.3889e-05 - 42s/epoch - 213ms/step
Epoch 642/1000
2023-10-28 17:13:00.498 
Epoch 642/1000 
	 loss: 39.7927, MinusLogProbMetric: 39.7927, val_loss: 42.1967, val_MinusLogProbMetric: 42.1967

Epoch 642: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.7927 - MinusLogProbMetric: 39.7927 - val_loss: 42.1967 - val_MinusLogProbMetric: 42.1967 - lr: 1.3889e-05 - 42s/epoch - 212ms/step
Epoch 643/1000
2023-10-28 17:13:40.990 
Epoch 643/1000 
	 loss: 39.8033, MinusLogProbMetric: 39.8033, val_loss: 42.1965, val_MinusLogProbMetric: 42.1965

Epoch 643: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.8033 - MinusLogProbMetric: 39.8033 - val_loss: 42.1965 - val_MinusLogProbMetric: 42.1965 - lr: 1.3889e-05 - 40s/epoch - 207ms/step
Epoch 644/1000
2023-10-28 17:14:21.934 
Epoch 644/1000 
	 loss: 39.7955, MinusLogProbMetric: 39.7955, val_loss: 42.2892, val_MinusLogProbMetric: 42.2892

Epoch 644: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7955 - MinusLogProbMetric: 39.7955 - val_loss: 42.2892 - val_MinusLogProbMetric: 42.2892 - lr: 1.3889e-05 - 41s/epoch - 209ms/step
Epoch 645/1000
2023-10-28 17:15:02.131 
Epoch 645/1000 
	 loss: 39.7897, MinusLogProbMetric: 39.7897, val_loss: 42.2288, val_MinusLogProbMetric: 42.2288

Epoch 645: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7897 - MinusLogProbMetric: 39.7897 - val_loss: 42.2288 - val_MinusLogProbMetric: 42.2288 - lr: 1.3889e-05 - 40s/epoch - 205ms/step
Epoch 646/1000
2023-10-28 17:15:33.654 
Epoch 646/1000 
	 loss: 39.7902, MinusLogProbMetric: 39.7902, val_loss: 42.2423, val_MinusLogProbMetric: 42.2423

Epoch 646: val_loss did not improve from 42.11228
196/196 - 32s - loss: 39.7902 - MinusLogProbMetric: 39.7902 - val_loss: 42.2423 - val_MinusLogProbMetric: 42.2423 - lr: 1.3889e-05 - 32s/epoch - 161ms/step
Epoch 647/1000
2023-10-28 17:16:05.543 
Epoch 647/1000 
	 loss: 39.7898, MinusLogProbMetric: 39.7898, val_loss: 42.2303, val_MinusLogProbMetric: 42.2303

Epoch 647: val_loss did not improve from 42.11228
196/196 - 32s - loss: 39.7898 - MinusLogProbMetric: 39.7898 - val_loss: 42.2303 - val_MinusLogProbMetric: 42.2303 - lr: 1.3889e-05 - 32s/epoch - 163ms/step
Epoch 648/1000
2023-10-28 17:16:37.210 
Epoch 648/1000 
	 loss: 39.7862, MinusLogProbMetric: 39.7862, val_loss: 42.2523, val_MinusLogProbMetric: 42.2523

Epoch 648: val_loss did not improve from 42.11228
196/196 - 32s - loss: 39.7862 - MinusLogProbMetric: 39.7862 - val_loss: 42.2523 - val_MinusLogProbMetric: 42.2523 - lr: 1.3889e-05 - 32s/epoch - 161ms/step
Epoch 649/1000
2023-10-28 17:17:08.630 
Epoch 649/1000 
	 loss: 39.7923, MinusLogProbMetric: 39.7923, val_loss: 42.2030, val_MinusLogProbMetric: 42.2030

Epoch 649: val_loss did not improve from 42.11228
196/196 - 31s - loss: 39.7923 - MinusLogProbMetric: 39.7923 - val_loss: 42.2030 - val_MinusLogProbMetric: 42.2030 - lr: 1.3889e-05 - 31s/epoch - 160ms/step
Epoch 650/1000
2023-10-28 17:17:41.813 
Epoch 650/1000 
	 loss: 39.7861, MinusLogProbMetric: 39.7861, val_loss: 42.2688, val_MinusLogProbMetric: 42.2688

Epoch 650: val_loss did not improve from 42.11228
196/196 - 33s - loss: 39.7861 - MinusLogProbMetric: 39.7861 - val_loss: 42.2688 - val_MinusLogProbMetric: 42.2688 - lr: 1.3889e-05 - 33s/epoch - 169ms/step
Epoch 651/1000
2023-10-28 17:18:14.856 
Epoch 651/1000 
	 loss: 39.7920, MinusLogProbMetric: 39.7920, val_loss: 42.2798, val_MinusLogProbMetric: 42.2798

Epoch 651: val_loss did not improve from 42.11228
196/196 - 33s - loss: 39.7920 - MinusLogProbMetric: 39.7920 - val_loss: 42.2798 - val_MinusLogProbMetric: 42.2798 - lr: 1.3889e-05 - 33s/epoch - 169ms/step
Epoch 652/1000
2023-10-28 17:18:55.838 
Epoch 652/1000 
	 loss: 39.7875, MinusLogProbMetric: 39.7875, val_loss: 42.1861, val_MinusLogProbMetric: 42.1861

Epoch 652: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7875 - MinusLogProbMetric: 39.7875 - val_loss: 42.1861 - val_MinusLogProbMetric: 42.1861 - lr: 1.3889e-05 - 41s/epoch - 209ms/step
Epoch 653/1000
2023-10-28 17:19:35.633 
Epoch 653/1000 
	 loss: 39.7886, MinusLogProbMetric: 39.7886, val_loss: 42.2383, val_MinusLogProbMetric: 42.2383

Epoch 653: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7886 - MinusLogProbMetric: 39.7886 - val_loss: 42.2383 - val_MinusLogProbMetric: 42.2383 - lr: 1.3889e-05 - 40s/epoch - 203ms/step
Epoch 654/1000
2023-10-28 17:20:16.099 
Epoch 654/1000 
	 loss: 39.7881, MinusLogProbMetric: 39.7881, val_loss: 42.2309, val_MinusLogProbMetric: 42.2309

Epoch 654: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7881 - MinusLogProbMetric: 39.7881 - val_loss: 42.2309 - val_MinusLogProbMetric: 42.2309 - lr: 1.3889e-05 - 40s/epoch - 206ms/step
Epoch 655/1000
2023-10-28 17:20:56.780 
Epoch 655/1000 
	 loss: 39.7944, MinusLogProbMetric: 39.7944, val_loss: 42.2561, val_MinusLogProbMetric: 42.2561

Epoch 655: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7944 - MinusLogProbMetric: 39.7944 - val_loss: 42.2561 - val_MinusLogProbMetric: 42.2561 - lr: 1.3889e-05 - 41s/epoch - 208ms/step
Epoch 656/1000
2023-10-28 17:21:36.725 
Epoch 656/1000 
	 loss: 39.7897, MinusLogProbMetric: 39.7897, val_loss: 42.2096, val_MinusLogProbMetric: 42.2096

Epoch 656: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7897 - MinusLogProbMetric: 39.7897 - val_loss: 42.2096 - val_MinusLogProbMetric: 42.2096 - lr: 1.3889e-05 - 40s/epoch - 204ms/step
Epoch 657/1000
2023-10-28 17:22:16.717 
Epoch 657/1000 
	 loss: 39.7848, MinusLogProbMetric: 39.7848, val_loss: 42.2146, val_MinusLogProbMetric: 42.2146

Epoch 657: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7848 - MinusLogProbMetric: 39.7848 - val_loss: 42.2146 - val_MinusLogProbMetric: 42.2146 - lr: 1.3889e-05 - 40s/epoch - 204ms/step
Epoch 658/1000
2023-10-28 17:22:48.938 
Epoch 658/1000 
	 loss: 39.7884, MinusLogProbMetric: 39.7884, val_loss: 42.2914, val_MinusLogProbMetric: 42.2914

Epoch 658: val_loss did not improve from 42.11228
196/196 - 32s - loss: 39.7884 - MinusLogProbMetric: 39.7884 - val_loss: 42.2914 - val_MinusLogProbMetric: 42.2914 - lr: 1.3889e-05 - 32s/epoch - 164ms/step
Epoch 659/1000
2023-10-28 17:23:20.936 
Epoch 659/1000 
	 loss: 39.7851, MinusLogProbMetric: 39.7851, val_loss: 42.2687, val_MinusLogProbMetric: 42.2687

Epoch 659: val_loss did not improve from 42.11228
196/196 - 32s - loss: 39.7851 - MinusLogProbMetric: 39.7851 - val_loss: 42.2687 - val_MinusLogProbMetric: 42.2687 - lr: 1.3889e-05 - 32s/epoch - 164ms/step
Epoch 660/1000
2023-10-28 17:23:53.364 
Epoch 660/1000 
	 loss: 39.7877, MinusLogProbMetric: 39.7877, val_loss: 42.1853, val_MinusLogProbMetric: 42.1853

Epoch 660: val_loss did not improve from 42.11228
196/196 - 32s - loss: 39.7877 - MinusLogProbMetric: 39.7877 - val_loss: 42.1853 - val_MinusLogProbMetric: 42.1853 - lr: 1.3889e-05 - 32s/epoch - 164ms/step
Epoch 661/1000
2023-10-28 17:24:27.522 
Epoch 661/1000 
	 loss: 39.7794, MinusLogProbMetric: 39.7794, val_loss: 42.2421, val_MinusLogProbMetric: 42.2421

Epoch 661: val_loss did not improve from 42.11228
196/196 - 34s - loss: 39.7794 - MinusLogProbMetric: 39.7794 - val_loss: 42.2421 - val_MinusLogProbMetric: 42.2421 - lr: 1.3889e-05 - 34s/epoch - 174ms/step
Epoch 662/1000
2023-10-28 17:25:01.860 
Epoch 662/1000 
	 loss: 39.7872, MinusLogProbMetric: 39.7872, val_loss: 42.1848, val_MinusLogProbMetric: 42.1848

Epoch 662: val_loss did not improve from 42.11228
196/196 - 34s - loss: 39.7872 - MinusLogProbMetric: 39.7872 - val_loss: 42.1848 - val_MinusLogProbMetric: 42.1848 - lr: 1.3889e-05 - 34s/epoch - 175ms/step
Epoch 663/1000
2023-10-28 17:25:38.744 
Epoch 663/1000 
	 loss: 39.7869, MinusLogProbMetric: 39.7869, val_loss: 42.2960, val_MinusLogProbMetric: 42.2960

Epoch 663: val_loss did not improve from 42.11228
196/196 - 37s - loss: 39.7869 - MinusLogProbMetric: 39.7869 - val_loss: 42.2960 - val_MinusLogProbMetric: 42.2960 - lr: 1.3889e-05 - 37s/epoch - 188ms/step
Epoch 664/1000
2023-10-28 17:26:19.919 
Epoch 664/1000 
	 loss: 39.7939, MinusLogProbMetric: 39.7939, val_loss: 42.1462, val_MinusLogProbMetric: 42.1462

Epoch 664: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7939 - MinusLogProbMetric: 39.7939 - val_loss: 42.1462 - val_MinusLogProbMetric: 42.1462 - lr: 1.3889e-05 - 41s/epoch - 210ms/step
Epoch 665/1000
2023-10-28 17:27:00.694 
Epoch 665/1000 
	 loss: 39.7777, MinusLogProbMetric: 39.7777, val_loss: 42.2658, val_MinusLogProbMetric: 42.2658

Epoch 665: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7777 - MinusLogProbMetric: 39.7777 - val_loss: 42.2658 - val_MinusLogProbMetric: 42.2658 - lr: 1.3889e-05 - 41s/epoch - 208ms/step
Epoch 666/1000
2023-10-28 17:27:40.916 
Epoch 666/1000 
	 loss: 39.7852, MinusLogProbMetric: 39.7852, val_loss: 42.2993, val_MinusLogProbMetric: 42.2993

Epoch 666: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7852 - MinusLogProbMetric: 39.7852 - val_loss: 42.2993 - val_MinusLogProbMetric: 42.2993 - lr: 1.3889e-05 - 40s/epoch - 205ms/step
Epoch 667/1000
2023-10-28 17:28:21.229 
Epoch 667/1000 
	 loss: 39.7722, MinusLogProbMetric: 39.7722, val_loss: 42.2883, val_MinusLogProbMetric: 42.2883

Epoch 667: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7722 - MinusLogProbMetric: 39.7722 - val_loss: 42.2883 - val_MinusLogProbMetric: 42.2883 - lr: 1.3889e-05 - 40s/epoch - 206ms/step
Epoch 668/1000
2023-10-28 17:29:02.243 
Epoch 668/1000 
	 loss: 39.7793, MinusLogProbMetric: 39.7793, val_loss: 42.1592, val_MinusLogProbMetric: 42.1592

Epoch 668: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7793 - MinusLogProbMetric: 39.7793 - val_loss: 42.1592 - val_MinusLogProbMetric: 42.1592 - lr: 1.3889e-05 - 41s/epoch - 209ms/step
Epoch 669/1000
2023-10-28 17:29:43.026 
Epoch 669/1000 
	 loss: 39.7832, MinusLogProbMetric: 39.7832, val_loss: 42.2621, val_MinusLogProbMetric: 42.2621

Epoch 669: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7832 - MinusLogProbMetric: 39.7832 - val_loss: 42.2621 - val_MinusLogProbMetric: 42.2621 - lr: 1.3889e-05 - 41s/epoch - 208ms/step
Epoch 670/1000
2023-10-28 17:30:24.689 
Epoch 670/1000 
	 loss: 39.7841, MinusLogProbMetric: 39.7841, val_loss: 42.2404, val_MinusLogProbMetric: 42.2404

Epoch 670: val_loss did not improve from 42.11228
196/196 - 42s - loss: 39.7841 - MinusLogProbMetric: 39.7841 - val_loss: 42.2404 - val_MinusLogProbMetric: 42.2404 - lr: 1.3889e-05 - 42s/epoch - 213ms/step
Epoch 671/1000
2023-10-28 17:31:05.454 
Epoch 671/1000 
	 loss: 39.7856, MinusLogProbMetric: 39.7856, val_loss: 42.1687, val_MinusLogProbMetric: 42.1687

Epoch 671: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7856 - MinusLogProbMetric: 39.7856 - val_loss: 42.1687 - val_MinusLogProbMetric: 42.1687 - lr: 1.3889e-05 - 41s/epoch - 208ms/step
Epoch 672/1000
2023-10-28 17:31:45.702 
Epoch 672/1000 
	 loss: 39.7734, MinusLogProbMetric: 39.7734, val_loss: 42.3450, val_MinusLogProbMetric: 42.3450

Epoch 672: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7734 - MinusLogProbMetric: 39.7734 - val_loss: 42.3450 - val_MinusLogProbMetric: 42.3450 - lr: 1.3889e-05 - 40s/epoch - 205ms/step
Epoch 673/1000
2023-10-28 17:32:26.363 
Epoch 673/1000 
	 loss: 39.7800, MinusLogProbMetric: 39.7800, val_loss: 42.1863, val_MinusLogProbMetric: 42.1863

Epoch 673: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7800 - MinusLogProbMetric: 39.7800 - val_loss: 42.1863 - val_MinusLogProbMetric: 42.1863 - lr: 1.3889e-05 - 41s/epoch - 207ms/step
Epoch 674/1000
2023-10-28 17:33:07.147 
Epoch 674/1000 
	 loss: 39.7807, MinusLogProbMetric: 39.7807, val_loss: 42.1726, val_MinusLogProbMetric: 42.1726

Epoch 674: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7807 - MinusLogProbMetric: 39.7807 - val_loss: 42.1726 - val_MinusLogProbMetric: 42.1726 - lr: 1.3889e-05 - 41s/epoch - 208ms/step
Epoch 675/1000
2023-10-28 17:33:47.237 
Epoch 675/1000 
	 loss: 39.7873, MinusLogProbMetric: 39.7873, val_loss: 42.3076, val_MinusLogProbMetric: 42.3076

Epoch 675: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7873 - MinusLogProbMetric: 39.7873 - val_loss: 42.3076 - val_MinusLogProbMetric: 42.3076 - lr: 1.3889e-05 - 40s/epoch - 205ms/step
Epoch 676/1000
2023-10-28 17:34:28.316 
Epoch 676/1000 
	 loss: 39.7685, MinusLogProbMetric: 39.7685, val_loss: 42.2803, val_MinusLogProbMetric: 42.2803

Epoch 676: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7685 - MinusLogProbMetric: 39.7685 - val_loss: 42.2803 - val_MinusLogProbMetric: 42.2803 - lr: 1.3889e-05 - 41s/epoch - 210ms/step
Epoch 677/1000
2023-10-28 17:35:09.277 
Epoch 677/1000 
	 loss: 39.7751, MinusLogProbMetric: 39.7751, val_loss: 42.2896, val_MinusLogProbMetric: 42.2896

Epoch 677: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7751 - MinusLogProbMetric: 39.7751 - val_loss: 42.2896 - val_MinusLogProbMetric: 42.2896 - lr: 1.3889e-05 - 41s/epoch - 209ms/step
Epoch 678/1000
2023-10-28 17:35:50.692 
Epoch 678/1000 
	 loss: 39.7774, MinusLogProbMetric: 39.7774, val_loss: 42.2942, val_MinusLogProbMetric: 42.2942

Epoch 678: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7774 - MinusLogProbMetric: 39.7774 - val_loss: 42.2942 - val_MinusLogProbMetric: 42.2942 - lr: 1.3889e-05 - 41s/epoch - 211ms/step
Epoch 679/1000
2023-10-28 17:36:31.228 
Epoch 679/1000 
	 loss: 39.7795, MinusLogProbMetric: 39.7795, val_loss: 42.2366, val_MinusLogProbMetric: 42.2366

Epoch 679: val_loss did not improve from 42.11228
196/196 - 41s - loss: 39.7795 - MinusLogProbMetric: 39.7795 - val_loss: 42.2366 - val_MinusLogProbMetric: 42.2366 - lr: 1.3889e-05 - 41s/epoch - 207ms/step
Epoch 680/1000
2023-10-28 17:37:11.709 
Epoch 680/1000 
	 loss: 39.7746, MinusLogProbMetric: 39.7746, val_loss: 42.2174, val_MinusLogProbMetric: 42.2174

Epoch 680: val_loss did not improve from 42.11228
196/196 - 40s - loss: 39.7746 - MinusLogProbMetric: 39.7746 - val_loss: 42.2174 - val_MinusLogProbMetric: 42.2174 - lr: 1.3889e-05 - 40s/epoch - 207ms/step
Epoch 681/1000
2023-10-28 17:37:53.594 
Epoch 681/1000 
	 loss: 39.7708, MinusLogProbMetric: 39.7708, val_loss: 42.2809, val_MinusLogProbMetric: 42.2809

Epoch 681: val_loss did not improve from 42.11228
Restoring model weights from the end of the best epoch: 581.
196/196 - 42s - loss: 39.7708 - MinusLogProbMetric: 39.7708 - val_loss: 42.2809 - val_MinusLogProbMetric: 42.2809 - lr: 1.3889e-05 - 42s/epoch - 216ms/step
Epoch 681: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 520.
Model trained in 26880.44 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.06 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.33 s.
===========
Run 436/720 done in 29130.65 s.
===========

Directory ../../results/CsplineN_new/run_437/ already exists.
Skipping it.
===========
Run 437/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_438/ already exists.
Skipping it.
===========
Run 438/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_439/ already exists.
Skipping it.
===========
Run 439/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_440/ already exists.
Skipping it.
===========
Run 440/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_441/ already exists.
Skipping it.
===========
Run 441/720 already exists. Skipping it.
===========

===========
Generating train data for run 442.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_442/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_442/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_442/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_442
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_572"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_573 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_72 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_72/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_72'")
self.model: <keras.engine.functional.Functional object at 0x7f350444d8d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d8aa8f940>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d8aa8f940>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f359d5ba710>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3589e988e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3589e99690>, <keras.callbacks.ModelCheckpoint object at 0x7f3589e99a80>, <keras.callbacks.EarlyStopping object at 0x7f3589e9b9d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3589e9b0d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3589e9b880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_442/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 442/720 with hyperparameters:
timestamp = 2023-10-28 17:37:58.690358
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 15: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 17:39:02.690 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7323.1519, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 7323.1519 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 64s/epoch - 326ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 442.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_442/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_442/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_442/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_442
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_578"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_579 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_73 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_73/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_73'")
self.model: <keras.engine.functional.Functional object at 0x7f3604388cd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36b8737220>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36b8737220>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3671014160>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36043ab7f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36043abd60>, <keras.callbacks.ModelCheckpoint object at 0x7f36043abe20>, <keras.callbacks.EarlyStopping object at 0x7f36043abfd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36043abd30>, <keras.callbacks.TerminateOnNaN object at 0x7f36043abf10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_442/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 442/720 with hyperparameters:
timestamp = 2023-10-28 17:39:05.480090
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
2023-10-28 17:40:32.612 
Epoch 1/1000 
	 loss: 1736.3773, MinusLogProbMetric: 1736.3773, val_loss: 457.7066, val_MinusLogProbMetric: 457.7066

Epoch 1: val_loss improved from inf to 457.70663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 88s - loss: 1736.3773 - MinusLogProbMetric: 1736.3773 - val_loss: 457.7066 - val_MinusLogProbMetric: 457.7066 - lr: 3.3333e-04 - 88s/epoch - 448ms/step
Epoch 2/1000
2023-10-28 17:41:07.147 
Epoch 2/1000 
	 loss: 364.8335, MinusLogProbMetric: 364.8335, val_loss: 294.0169, val_MinusLogProbMetric: 294.0169

Epoch 2: val_loss improved from 457.70663 to 294.01688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 364.8335 - MinusLogProbMetric: 364.8335 - val_loss: 294.0169 - val_MinusLogProbMetric: 294.0169 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 3/1000
2023-10-28 17:41:42.665 
Epoch 3/1000 
	 loss: 270.8607, MinusLogProbMetric: 270.8607, val_loss: 827.1605, val_MinusLogProbMetric: 827.1605

Epoch 3: val_loss did not improve from 294.01688
196/196 - 34s - loss: 270.8607 - MinusLogProbMetric: 270.8607 - val_loss: 827.1605 - val_MinusLogProbMetric: 827.1605 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 4/1000
2023-10-28 17:42:16.644 
Epoch 4/1000 
	 loss: 299.1818, MinusLogProbMetric: 299.1818, val_loss: 219.5555, val_MinusLogProbMetric: 219.5555

Epoch 4: val_loss improved from 294.01688 to 219.55550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 299.1818 - MinusLogProbMetric: 299.1818 - val_loss: 219.5555 - val_MinusLogProbMetric: 219.5555 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 5/1000
2023-10-28 17:42:51.333 
Epoch 5/1000 
	 loss: 199.6524, MinusLogProbMetric: 199.6524, val_loss: 216.0511, val_MinusLogProbMetric: 216.0511

Epoch 5: val_loss improved from 219.55550 to 216.05112, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 199.6524 - MinusLogProbMetric: 199.6524 - val_loss: 216.0511 - val_MinusLogProbMetric: 216.0511 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 6/1000
2023-10-28 17:43:26.634 
Epoch 6/1000 
	 loss: 191.3108, MinusLogProbMetric: 191.3108, val_loss: 162.6254, val_MinusLogProbMetric: 162.6254

Epoch 6: val_loss improved from 216.05112 to 162.62535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 191.3108 - MinusLogProbMetric: 191.3108 - val_loss: 162.6254 - val_MinusLogProbMetric: 162.6254 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 7/1000
2023-10-28 17:44:01.864 
Epoch 7/1000 
	 loss: 154.8276, MinusLogProbMetric: 154.8276, val_loss: 146.4758, val_MinusLogProbMetric: 146.4758

Epoch 7: val_loss improved from 162.62535 to 146.47580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 154.8276 - MinusLogProbMetric: 154.8276 - val_loss: 146.4758 - val_MinusLogProbMetric: 146.4758 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 8/1000
2023-10-28 17:44:36.411 
Epoch 8/1000 
	 loss: 140.6161, MinusLogProbMetric: 140.6161, val_loss: 135.5908, val_MinusLogProbMetric: 135.5908

Epoch 8: val_loss improved from 146.47580 to 135.59079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 140.6161 - MinusLogProbMetric: 140.6161 - val_loss: 135.5908 - val_MinusLogProbMetric: 135.5908 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 9/1000
2023-10-28 17:45:10.895 
Epoch 9/1000 
	 loss: 136.0129, MinusLogProbMetric: 136.0129, val_loss: 126.5361, val_MinusLogProbMetric: 126.5361

Epoch 9: val_loss improved from 135.59079 to 126.53609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 136.0129 - MinusLogProbMetric: 136.0129 - val_loss: 126.5361 - val_MinusLogProbMetric: 126.5361 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 10/1000
2023-10-28 17:45:45.732 
Epoch 10/1000 
	 loss: 122.2792, MinusLogProbMetric: 122.2792, val_loss: 119.3589, val_MinusLogProbMetric: 119.3589

Epoch 10: val_loss improved from 126.53609 to 119.35889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 122.2792 - MinusLogProbMetric: 122.2792 - val_loss: 119.3589 - val_MinusLogProbMetric: 119.3589 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 11/1000
2023-10-28 17:46:20.441 
Epoch 11/1000 
	 loss: 117.7043, MinusLogProbMetric: 117.7043, val_loss: 114.5166, val_MinusLogProbMetric: 114.5166

Epoch 11: val_loss improved from 119.35889 to 114.51663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 117.7043 - MinusLogProbMetric: 117.7043 - val_loss: 114.5166 - val_MinusLogProbMetric: 114.5166 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 12/1000
2023-10-28 17:46:56.021 
Epoch 12/1000 
	 loss: 111.2535, MinusLogProbMetric: 111.2535, val_loss: 109.3350, val_MinusLogProbMetric: 109.3350

Epoch 12: val_loss improved from 114.51663 to 109.33495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 36s - loss: 111.2535 - MinusLogProbMetric: 111.2535 - val_loss: 109.3350 - val_MinusLogProbMetric: 109.3350 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 13/1000
2023-10-28 17:47:31.357 
Epoch 13/1000 
	 loss: 104.4536, MinusLogProbMetric: 104.4536, val_loss: 101.7436, val_MinusLogProbMetric: 101.7436

Epoch 13: val_loss improved from 109.33495 to 101.74364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 104.4536 - MinusLogProbMetric: 104.4536 - val_loss: 101.7436 - val_MinusLogProbMetric: 101.7436 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 14/1000
2023-10-28 17:48:06.040 
Epoch 14/1000 
	 loss: 99.8015, MinusLogProbMetric: 99.8015, val_loss: 97.4622, val_MinusLogProbMetric: 97.4622

Epoch 14: val_loss improved from 101.74364 to 97.46223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 99.8015 - MinusLogProbMetric: 99.8015 - val_loss: 97.4622 - val_MinusLogProbMetric: 97.4622 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 15/1000
2023-10-28 17:48:41.181 
Epoch 15/1000 
	 loss: 95.7497, MinusLogProbMetric: 95.7497, val_loss: 94.4620, val_MinusLogProbMetric: 94.4620

Epoch 15: val_loss improved from 97.46223 to 94.46204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 95.7497 - MinusLogProbMetric: 95.7497 - val_loss: 94.4620 - val_MinusLogProbMetric: 94.4620 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 16/1000
2023-10-28 17:49:15.726 
Epoch 16/1000 
	 loss: 92.1406, MinusLogProbMetric: 92.1406, val_loss: 92.4479, val_MinusLogProbMetric: 92.4479

Epoch 16: val_loss improved from 94.46204 to 92.44788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 92.1406 - MinusLogProbMetric: 92.1406 - val_loss: 92.4479 - val_MinusLogProbMetric: 92.4479 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 17/1000
2023-10-28 17:49:49.649 
Epoch 17/1000 
	 loss: 89.6975, MinusLogProbMetric: 89.6975, val_loss: 88.1243, val_MinusLogProbMetric: 88.1243

Epoch 17: val_loss improved from 92.44788 to 88.12433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 89.6975 - MinusLogProbMetric: 89.6975 - val_loss: 88.1243 - val_MinusLogProbMetric: 88.1243 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 18/1000
2023-10-28 17:50:23.534 
Epoch 18/1000 
	 loss: 86.4896, MinusLogProbMetric: 86.4896, val_loss: 85.9658, val_MinusLogProbMetric: 85.9658

Epoch 18: val_loss improved from 88.12433 to 85.96578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 86.4896 - MinusLogProbMetric: 86.4896 - val_loss: 85.9658 - val_MinusLogProbMetric: 85.9658 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 19/1000
2023-10-28 17:50:58.039 
Epoch 19/1000 
	 loss: 84.1866, MinusLogProbMetric: 84.1866, val_loss: 83.8372, val_MinusLogProbMetric: 83.8372

Epoch 19: val_loss improved from 85.96578 to 83.83718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 84.1866 - MinusLogProbMetric: 84.1866 - val_loss: 83.8372 - val_MinusLogProbMetric: 83.8372 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 20/1000
2023-10-28 17:51:33.084 
Epoch 20/1000 
	 loss: 81.7481, MinusLogProbMetric: 81.7481, val_loss: 82.1071, val_MinusLogProbMetric: 82.1071

Epoch 20: val_loss improved from 83.83718 to 82.10713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 81.7481 - MinusLogProbMetric: 81.7481 - val_loss: 82.1071 - val_MinusLogProbMetric: 82.1071 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 21/1000
2023-10-28 17:52:08.355 
Epoch 21/1000 
	 loss: 80.3721, MinusLogProbMetric: 80.3721, val_loss: 78.9826, val_MinusLogProbMetric: 78.9826

Epoch 21: val_loss improved from 82.10713 to 78.98260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 80.3721 - MinusLogProbMetric: 80.3721 - val_loss: 78.9826 - val_MinusLogProbMetric: 78.9826 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 22/1000
2023-10-28 17:52:43.533 
Epoch 22/1000 
	 loss: 78.3877, MinusLogProbMetric: 78.3877, val_loss: 77.1544, val_MinusLogProbMetric: 77.1544

Epoch 22: val_loss improved from 78.98260 to 77.15445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 36s - loss: 78.3877 - MinusLogProbMetric: 78.3877 - val_loss: 77.1544 - val_MinusLogProbMetric: 77.1544 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 23/1000
2023-10-28 17:53:18.235 
Epoch 23/1000 
	 loss: 76.7451, MinusLogProbMetric: 76.7451, val_loss: 77.0341, val_MinusLogProbMetric: 77.0341

Epoch 23: val_loss improved from 77.15445 to 77.03410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 76.7451 - MinusLogProbMetric: 76.7451 - val_loss: 77.0341 - val_MinusLogProbMetric: 77.0341 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 24/1000
2023-10-28 17:53:52.791 
Epoch 24/1000 
	 loss: 75.3423, MinusLogProbMetric: 75.3423, val_loss: 77.5213, val_MinusLogProbMetric: 77.5213

Epoch 24: val_loss did not improve from 77.03410
196/196 - 34s - loss: 75.3423 - MinusLogProbMetric: 75.3423 - val_loss: 77.5213 - val_MinusLogProbMetric: 77.5213 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 25/1000
2023-10-28 17:54:26.910 
Epoch 25/1000 
	 loss: 73.8780, MinusLogProbMetric: 73.8780, val_loss: 72.5956, val_MinusLogProbMetric: 72.5956

Epoch 25: val_loss improved from 77.03410 to 72.59561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 73.8780 - MinusLogProbMetric: 73.8780 - val_loss: 72.5956 - val_MinusLogProbMetric: 72.5956 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 26/1000
2023-10-28 17:55:00.828 
Epoch 26/1000 
	 loss: 72.5277, MinusLogProbMetric: 72.5277, val_loss: 72.0452, val_MinusLogProbMetric: 72.0452

Epoch 26: val_loss improved from 72.59561 to 72.04524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 72.5277 - MinusLogProbMetric: 72.5277 - val_loss: 72.0452 - val_MinusLogProbMetric: 72.0452 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 27/1000
2023-10-28 17:55:34.943 
Epoch 27/1000 
	 loss: 71.0003, MinusLogProbMetric: 71.0003, val_loss: 71.0438, val_MinusLogProbMetric: 71.0438

Epoch 27: val_loss improved from 72.04524 to 71.04378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 71.0003 - MinusLogProbMetric: 71.0003 - val_loss: 71.0438 - val_MinusLogProbMetric: 71.0438 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 28/1000
2023-10-28 17:56:09.156 
Epoch 28/1000 
	 loss: 70.8731, MinusLogProbMetric: 70.8731, val_loss: 70.0908, val_MinusLogProbMetric: 70.0908

Epoch 28: val_loss improved from 71.04378 to 70.09077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 70.8731 - MinusLogProbMetric: 70.8731 - val_loss: 70.0908 - val_MinusLogProbMetric: 70.0908 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 29/1000
2023-10-28 17:56:43.864 
Epoch 29/1000 
	 loss: 69.4172, MinusLogProbMetric: 69.4172, val_loss: 68.4640, val_MinusLogProbMetric: 68.4640

Epoch 29: val_loss improved from 70.09077 to 68.46400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 69.4172 - MinusLogProbMetric: 69.4172 - val_loss: 68.4640 - val_MinusLogProbMetric: 68.4640 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 30/1000
2023-10-28 17:57:18.392 
Epoch 30/1000 
	 loss: 68.6786, MinusLogProbMetric: 68.6786, val_loss: 69.2447, val_MinusLogProbMetric: 69.2447

Epoch 30: val_loss did not improve from 68.46400
196/196 - 34s - loss: 68.6786 - MinusLogProbMetric: 68.6786 - val_loss: 69.2447 - val_MinusLogProbMetric: 69.2447 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 31/1000
2023-10-28 17:57:52.496 
Epoch 31/1000 
	 loss: 68.0563, MinusLogProbMetric: 68.0563, val_loss: 67.5261, val_MinusLogProbMetric: 67.5261

Epoch 31: val_loss improved from 68.46400 to 67.52613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 38s - loss: 68.0563 - MinusLogProbMetric: 68.0563 - val_loss: 67.5261 - val_MinusLogProbMetric: 67.5261 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 32/1000
2023-10-28 17:58:31.017 
Epoch 32/1000 
	 loss: 67.0420, MinusLogProbMetric: 67.0420, val_loss: 69.2243, val_MinusLogProbMetric: 69.2243

Epoch 32: val_loss did not improve from 67.52613
196/196 - 35s - loss: 67.0420 - MinusLogProbMetric: 67.0420 - val_loss: 69.2243 - val_MinusLogProbMetric: 69.2243 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 33/1000
2023-10-28 17:59:05.000 
Epoch 33/1000 
	 loss: 66.2238, MinusLogProbMetric: 66.2238, val_loss: 66.3526, val_MinusLogProbMetric: 66.3526

Epoch 33: val_loss improved from 67.52613 to 66.35263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 66.2238 - MinusLogProbMetric: 66.2238 - val_loss: 66.3526 - val_MinusLogProbMetric: 66.3526 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 34/1000
2023-10-28 17:59:39.681 
Epoch 34/1000 
	 loss: 66.2074, MinusLogProbMetric: 66.2074, val_loss: 65.8093, val_MinusLogProbMetric: 65.8093

Epoch 34: val_loss improved from 66.35263 to 65.80931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 36s - loss: 66.2074 - MinusLogProbMetric: 66.2074 - val_loss: 65.8093 - val_MinusLogProbMetric: 65.8093 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 35/1000
2023-10-28 18:00:15.559 
Epoch 35/1000 
	 loss: 65.5719, MinusLogProbMetric: 65.5719, val_loss: 65.4858, val_MinusLogProbMetric: 65.4858

Epoch 35: val_loss improved from 65.80931 to 65.48583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 36s - loss: 65.5719 - MinusLogProbMetric: 65.5719 - val_loss: 65.4858 - val_MinusLogProbMetric: 65.4858 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 36/1000
2023-10-28 18:00:51.975 
Epoch 36/1000 
	 loss: 65.1623, MinusLogProbMetric: 65.1623, val_loss: 64.4419, val_MinusLogProbMetric: 64.4419

Epoch 36: val_loss improved from 65.48583 to 64.44190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 65.1623 - MinusLogProbMetric: 65.1623 - val_loss: 64.4419 - val_MinusLogProbMetric: 64.4419 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 37/1000
2023-10-28 18:01:26.061 
Epoch 37/1000 
	 loss: 63.8900, MinusLogProbMetric: 63.8900, val_loss: 66.2342, val_MinusLogProbMetric: 66.2342

Epoch 37: val_loss did not improve from 64.44190
196/196 - 33s - loss: 63.8900 - MinusLogProbMetric: 63.8900 - val_loss: 66.2342 - val_MinusLogProbMetric: 66.2342 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 38/1000
2023-10-28 18:01:59.368 
Epoch 38/1000 
	 loss: 64.0440, MinusLogProbMetric: 64.0440, val_loss: 62.4762, val_MinusLogProbMetric: 62.4762

Epoch 38: val_loss improved from 64.44190 to 62.47617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 64.0440 - MinusLogProbMetric: 64.0440 - val_loss: 62.4762 - val_MinusLogProbMetric: 62.4762 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 39/1000
2023-10-28 18:02:34.638 
Epoch 39/1000 
	 loss: 107.9268, MinusLogProbMetric: 107.9268, val_loss: 81.1449, val_MinusLogProbMetric: 81.1449

Epoch 39: val_loss did not improve from 62.47617
196/196 - 34s - loss: 107.9268 - MinusLogProbMetric: 107.9268 - val_loss: 81.1449 - val_MinusLogProbMetric: 81.1449 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 40/1000
2023-10-28 18:03:08.975 
Epoch 40/1000 
	 loss: 73.5149, MinusLogProbMetric: 73.5149, val_loss: 68.9133, val_MinusLogProbMetric: 68.9133

Epoch 40: val_loss did not improve from 62.47617
196/196 - 34s - loss: 73.5149 - MinusLogProbMetric: 73.5149 - val_loss: 68.9133 - val_MinusLogProbMetric: 68.9133 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 41/1000
2023-10-28 18:03:43.496 
Epoch 41/1000 
	 loss: 68.2130, MinusLogProbMetric: 68.2130, val_loss: 67.1801, val_MinusLogProbMetric: 67.1801

Epoch 41: val_loss did not improve from 62.47617
196/196 - 35s - loss: 68.2130 - MinusLogProbMetric: 68.2130 - val_loss: 67.1801 - val_MinusLogProbMetric: 67.1801 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 42/1000
2023-10-28 18:04:16.733 
Epoch 42/1000 
	 loss: 65.7003, MinusLogProbMetric: 65.7003, val_loss: 65.9457, val_MinusLogProbMetric: 65.9457

Epoch 42: val_loss did not improve from 62.47617
196/196 - 33s - loss: 65.7003 - MinusLogProbMetric: 65.7003 - val_loss: 65.9457 - val_MinusLogProbMetric: 65.9457 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 43/1000
2023-10-28 18:04:49.977 
Epoch 43/1000 
	 loss: 64.4765, MinusLogProbMetric: 64.4765, val_loss: 64.1090, val_MinusLogProbMetric: 64.1090

Epoch 43: val_loss did not improve from 62.47617
196/196 - 33s - loss: 64.4765 - MinusLogProbMetric: 64.4765 - val_loss: 64.1090 - val_MinusLogProbMetric: 64.1090 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 44/1000
2023-10-28 18:05:23.761 
Epoch 44/1000 
	 loss: 63.4330, MinusLogProbMetric: 63.4330, val_loss: 64.5986, val_MinusLogProbMetric: 64.5986

Epoch 44: val_loss did not improve from 62.47617
196/196 - 34s - loss: 63.4330 - MinusLogProbMetric: 63.4330 - val_loss: 64.5986 - val_MinusLogProbMetric: 64.5986 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 45/1000
2023-10-28 18:05:57.652 
Epoch 45/1000 
	 loss: 64.5862, MinusLogProbMetric: 64.5862, val_loss: 65.3037, val_MinusLogProbMetric: 65.3037

Epoch 45: val_loss did not improve from 62.47617
196/196 - 34s - loss: 64.5862 - MinusLogProbMetric: 64.5862 - val_loss: 65.3037 - val_MinusLogProbMetric: 65.3037 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 46/1000
2023-10-28 18:06:31.942 
Epoch 46/1000 
	 loss: 62.7501, MinusLogProbMetric: 62.7501, val_loss: 68.3606, val_MinusLogProbMetric: 68.3606

Epoch 46: val_loss did not improve from 62.47617
196/196 - 34s - loss: 62.7501 - MinusLogProbMetric: 62.7501 - val_loss: 68.3606 - val_MinusLogProbMetric: 68.3606 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 47/1000
2023-10-28 18:07:06.206 
Epoch 47/1000 
	 loss: 61.5989, MinusLogProbMetric: 61.5989, val_loss: 61.3328, val_MinusLogProbMetric: 61.3328

Epoch 47: val_loss improved from 62.47617 to 61.33285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 61.5989 - MinusLogProbMetric: 61.5989 - val_loss: 61.3328 - val_MinusLogProbMetric: 61.3328 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 48/1000
2023-10-28 18:07:40.742 
Epoch 48/1000 
	 loss: 61.7162, MinusLogProbMetric: 61.7162, val_loss: 62.6298, val_MinusLogProbMetric: 62.6298

Epoch 48: val_loss did not improve from 61.33285
196/196 - 34s - loss: 61.7162 - MinusLogProbMetric: 61.7162 - val_loss: 62.6298 - val_MinusLogProbMetric: 62.6298 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 49/1000
2023-10-28 18:08:15.227 
Epoch 49/1000 
	 loss: 60.9167, MinusLogProbMetric: 60.9167, val_loss: 60.0208, val_MinusLogProbMetric: 60.0208

Epoch 49: val_loss improved from 61.33285 to 60.02076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 36s - loss: 60.9167 - MinusLogProbMetric: 60.9167 - val_loss: 60.0208 - val_MinusLogProbMetric: 60.0208 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 50/1000
2023-10-28 18:08:51.144 
Epoch 50/1000 
	 loss: 60.2768, MinusLogProbMetric: 60.2768, val_loss: 60.3201, val_MinusLogProbMetric: 60.3201

Epoch 50: val_loss did not improve from 60.02076
196/196 - 34s - loss: 60.2768 - MinusLogProbMetric: 60.2768 - val_loss: 60.3201 - val_MinusLogProbMetric: 60.3201 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 51/1000
2023-10-28 18:09:25.018 
Epoch 51/1000 
	 loss: 59.8601, MinusLogProbMetric: 59.8601, val_loss: 60.9157, val_MinusLogProbMetric: 60.9157

Epoch 51: val_loss did not improve from 60.02076
196/196 - 34s - loss: 59.8601 - MinusLogProbMetric: 59.8601 - val_loss: 60.9157 - val_MinusLogProbMetric: 60.9157 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 52/1000
2023-10-28 18:09:59.399 
Epoch 52/1000 
	 loss: 59.4315, MinusLogProbMetric: 59.4315, val_loss: 60.1239, val_MinusLogProbMetric: 60.1239

Epoch 52: val_loss did not improve from 60.02076
196/196 - 34s - loss: 59.4315 - MinusLogProbMetric: 59.4315 - val_loss: 60.1239 - val_MinusLogProbMetric: 60.1239 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 53/1000
2023-10-28 18:10:33.909 
Epoch 53/1000 
	 loss: 59.4014, MinusLogProbMetric: 59.4014, val_loss: 60.1685, val_MinusLogProbMetric: 60.1685

Epoch 53: val_loss did not improve from 60.02076
196/196 - 35s - loss: 59.4014 - MinusLogProbMetric: 59.4014 - val_loss: 60.1685 - val_MinusLogProbMetric: 60.1685 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 54/1000
2023-10-28 18:11:08.533 
Epoch 54/1000 
	 loss: 58.7291, MinusLogProbMetric: 58.7291, val_loss: 59.8448, val_MinusLogProbMetric: 59.8448

Epoch 54: val_loss improved from 60.02076 to 59.84476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 58.7291 - MinusLogProbMetric: 58.7291 - val_loss: 59.8448 - val_MinusLogProbMetric: 59.8448 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 55/1000
2023-10-28 18:11:43.295 
Epoch 55/1000 
	 loss: 58.5901, MinusLogProbMetric: 58.5901, val_loss: 58.6059, val_MinusLogProbMetric: 58.6059

Epoch 55: val_loss improved from 59.84476 to 58.60585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 36s - loss: 58.5901 - MinusLogProbMetric: 58.5901 - val_loss: 58.6059 - val_MinusLogProbMetric: 58.6059 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 56/1000
2023-10-28 18:12:19.432 
Epoch 56/1000 
	 loss: 58.0468, MinusLogProbMetric: 58.0468, val_loss: 57.5413, val_MinusLogProbMetric: 57.5413

Epoch 56: val_loss improved from 58.60585 to 57.54129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 37s - loss: 58.0468 - MinusLogProbMetric: 58.0468 - val_loss: 57.5413 - val_MinusLogProbMetric: 57.5413 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 57/1000
2023-10-28 18:12:56.075 
Epoch 57/1000 
	 loss: 57.6506, MinusLogProbMetric: 57.6506, val_loss: 58.4992, val_MinusLogProbMetric: 58.4992

Epoch 57: val_loss did not improve from 57.54129
196/196 - 33s - loss: 57.6506 - MinusLogProbMetric: 57.6506 - val_loss: 58.4992 - val_MinusLogProbMetric: 58.4992 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 58/1000
2023-10-28 18:13:30.260 
Epoch 58/1000 
	 loss: 57.5096, MinusLogProbMetric: 57.5096, val_loss: 57.5652, val_MinusLogProbMetric: 57.5652

Epoch 58: val_loss did not improve from 57.54129
196/196 - 34s - loss: 57.5096 - MinusLogProbMetric: 57.5096 - val_loss: 57.5652 - val_MinusLogProbMetric: 57.5652 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 59/1000
2023-10-28 18:14:03.652 
Epoch 59/1000 
	 loss: 57.1205, MinusLogProbMetric: 57.1205, val_loss: 59.0854, val_MinusLogProbMetric: 59.0854

Epoch 59: val_loss did not improve from 57.54129
196/196 - 33s - loss: 57.1205 - MinusLogProbMetric: 57.1205 - val_loss: 59.0854 - val_MinusLogProbMetric: 59.0854 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 60/1000
2023-10-28 18:14:36.473 
Epoch 60/1000 
	 loss: 56.4721, MinusLogProbMetric: 56.4721, val_loss: 56.5504, val_MinusLogProbMetric: 56.5504

Epoch 60: val_loss improved from 57.54129 to 56.55041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 56.4721 - MinusLogProbMetric: 56.4721 - val_loss: 56.5504 - val_MinusLogProbMetric: 56.5504 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 61/1000
2023-10-28 18:15:11.451 
Epoch 61/1000 
	 loss: 56.7197, MinusLogProbMetric: 56.7197, val_loss: 59.4833, val_MinusLogProbMetric: 59.4833

Epoch 61: val_loss did not improve from 56.55041
196/196 - 33s - loss: 56.7197 - MinusLogProbMetric: 56.7197 - val_loss: 59.4833 - val_MinusLogProbMetric: 59.4833 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 62/1000
2023-10-28 18:15:44.954 
Epoch 62/1000 
	 loss: 56.3229, MinusLogProbMetric: 56.3229, val_loss: 57.7360, val_MinusLogProbMetric: 57.7360

Epoch 62: val_loss did not improve from 56.55041
196/196 - 33s - loss: 56.3229 - MinusLogProbMetric: 56.3229 - val_loss: 57.7360 - val_MinusLogProbMetric: 57.7360 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 63/1000
2023-10-28 18:16:18.680 
Epoch 63/1000 
	 loss: 56.1605, MinusLogProbMetric: 56.1605, val_loss: 56.9579, val_MinusLogProbMetric: 56.9579

Epoch 63: val_loss did not improve from 56.55041
196/196 - 34s - loss: 56.1605 - MinusLogProbMetric: 56.1605 - val_loss: 56.9579 - val_MinusLogProbMetric: 56.9579 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 64/1000
2023-10-28 18:16:52.192 
Epoch 64/1000 
	 loss: 55.8016, MinusLogProbMetric: 55.8016, val_loss: 56.6526, val_MinusLogProbMetric: 56.6526

Epoch 64: val_loss did not improve from 56.55041
196/196 - 34s - loss: 55.8016 - MinusLogProbMetric: 55.8016 - val_loss: 56.6526 - val_MinusLogProbMetric: 56.6526 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 65/1000
2023-10-28 18:17:26.494 
Epoch 65/1000 
	 loss: 55.7511, MinusLogProbMetric: 55.7511, val_loss: 55.8468, val_MinusLogProbMetric: 55.8468

Epoch 65: val_loss improved from 56.55041 to 55.84683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 55.7511 - MinusLogProbMetric: 55.7511 - val_loss: 55.8468 - val_MinusLogProbMetric: 55.8468 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 66/1000
2023-10-28 18:18:01.604 
Epoch 66/1000 
	 loss: 55.7384, MinusLogProbMetric: 55.7384, val_loss: 56.5817, val_MinusLogProbMetric: 56.5817

Epoch 66: val_loss did not improve from 55.84683
196/196 - 34s - loss: 55.7384 - MinusLogProbMetric: 55.7384 - val_loss: 56.5817 - val_MinusLogProbMetric: 56.5817 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 67/1000
2023-10-28 18:18:35.581 
Epoch 67/1000 
	 loss: 55.5466, MinusLogProbMetric: 55.5466, val_loss: 56.2040, val_MinusLogProbMetric: 56.2040

Epoch 67: val_loss did not improve from 55.84683
196/196 - 34s - loss: 55.5466 - MinusLogProbMetric: 55.5466 - val_loss: 56.2040 - val_MinusLogProbMetric: 56.2040 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 68/1000
2023-10-28 18:19:09.788 
Epoch 68/1000 
	 loss: 55.4633, MinusLogProbMetric: 55.4633, val_loss: 54.7354, val_MinusLogProbMetric: 54.7354

Epoch 68: val_loss improved from 55.84683 to 54.73541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 36s - loss: 55.4633 - MinusLogProbMetric: 55.4633 - val_loss: 54.7354 - val_MinusLogProbMetric: 54.7354 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 69/1000
2023-10-28 18:19:45.771 
Epoch 69/1000 
	 loss: 54.8382, MinusLogProbMetric: 54.8382, val_loss: 55.1220, val_MinusLogProbMetric: 55.1220

Epoch 69: val_loss did not improve from 54.73541
196/196 - 34s - loss: 54.8382 - MinusLogProbMetric: 54.8382 - val_loss: 55.1220 - val_MinusLogProbMetric: 55.1220 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 70/1000
2023-10-28 18:20:19.815 
Epoch 70/1000 
	 loss: 54.8229, MinusLogProbMetric: 54.8229, val_loss: 60.8170, val_MinusLogProbMetric: 60.8170

Epoch 70: val_loss did not improve from 54.73541
196/196 - 34s - loss: 54.8229 - MinusLogProbMetric: 54.8229 - val_loss: 60.8170 - val_MinusLogProbMetric: 60.8170 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 71/1000
2023-10-28 18:20:53.157 
Epoch 71/1000 
	 loss: 55.3083, MinusLogProbMetric: 55.3083, val_loss: 53.8750, val_MinusLogProbMetric: 53.8750

Epoch 71: val_loss improved from 54.73541 to 53.87500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 55.3083 - MinusLogProbMetric: 55.3083 - val_loss: 53.8750 - val_MinusLogProbMetric: 53.8750 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 72/1000
2023-10-28 18:21:27.940 
Epoch 72/1000 
	 loss: 54.8657, MinusLogProbMetric: 54.8657, val_loss: 56.0531, val_MinusLogProbMetric: 56.0531

Epoch 72: val_loss did not improve from 53.87500
196/196 - 34s - loss: 54.8657 - MinusLogProbMetric: 54.8657 - val_loss: 56.0531 - val_MinusLogProbMetric: 56.0531 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 73/1000
2023-10-28 18:22:02.074 
Epoch 73/1000 
	 loss: 54.0458, MinusLogProbMetric: 54.0458, val_loss: 54.5342, val_MinusLogProbMetric: 54.5342

Epoch 73: val_loss did not improve from 53.87500
196/196 - 34s - loss: 54.0458 - MinusLogProbMetric: 54.0458 - val_loss: 54.5342 - val_MinusLogProbMetric: 54.5342 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 74/1000
2023-10-28 18:22:36.223 
Epoch 74/1000 
	 loss: 54.5107, MinusLogProbMetric: 54.5107, val_loss: 54.1895, val_MinusLogProbMetric: 54.1895

Epoch 74: val_loss did not improve from 53.87500
196/196 - 34s - loss: 54.5107 - MinusLogProbMetric: 54.5107 - val_loss: 54.1895 - val_MinusLogProbMetric: 54.1895 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 75/1000
2023-10-28 18:23:09.986 
Epoch 75/1000 
	 loss: 54.3528, MinusLogProbMetric: 54.3528, val_loss: 54.4545, val_MinusLogProbMetric: 54.4545

Epoch 75: val_loss did not improve from 53.87500
196/196 - 34s - loss: 54.3528 - MinusLogProbMetric: 54.3528 - val_loss: 54.4545 - val_MinusLogProbMetric: 54.4545 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 76/1000
2023-10-28 18:23:43.850 
Epoch 76/1000 
	 loss: 54.1918, MinusLogProbMetric: 54.1918, val_loss: 53.1729, val_MinusLogProbMetric: 53.1729

Epoch 76: val_loss improved from 53.87500 to 53.17288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 54.1918 - MinusLogProbMetric: 54.1918 - val_loss: 53.1729 - val_MinusLogProbMetric: 53.1729 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 77/1000
2023-10-28 18:24:18.217 
Epoch 77/1000 
	 loss: 53.7306, MinusLogProbMetric: 53.7306, val_loss: 55.6721, val_MinusLogProbMetric: 55.6721

Epoch 77: val_loss did not improve from 53.17288
196/196 - 33s - loss: 53.7306 - MinusLogProbMetric: 53.7306 - val_loss: 55.6721 - val_MinusLogProbMetric: 55.6721 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 78/1000
2023-10-28 18:24:52.692 
Epoch 78/1000 
	 loss: 53.7380, MinusLogProbMetric: 53.7380, val_loss: 53.8726, val_MinusLogProbMetric: 53.8726

Epoch 78: val_loss did not improve from 53.17288
196/196 - 34s - loss: 53.7380 - MinusLogProbMetric: 53.7380 - val_loss: 53.8726 - val_MinusLogProbMetric: 53.8726 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 79/1000
2023-10-28 18:25:26.418 
Epoch 79/1000 
	 loss: 56.8462, MinusLogProbMetric: 56.8462, val_loss: 55.2871, val_MinusLogProbMetric: 55.2871

Epoch 79: val_loss did not improve from 53.17288
196/196 - 34s - loss: 56.8462 - MinusLogProbMetric: 56.8462 - val_loss: 55.2871 - val_MinusLogProbMetric: 55.2871 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 80/1000
2023-10-28 18:26:00.055 
Epoch 80/1000 
	 loss: 53.3563, MinusLogProbMetric: 53.3563, val_loss: 52.8580, val_MinusLogProbMetric: 52.8580

Epoch 80: val_loss improved from 53.17288 to 52.85801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 53.3563 - MinusLogProbMetric: 53.3563 - val_loss: 52.8580 - val_MinusLogProbMetric: 52.8580 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 81/1000
2023-10-28 18:26:35.374 
Epoch 81/1000 
	 loss: 53.1906, MinusLogProbMetric: 53.1906, val_loss: 56.5583, val_MinusLogProbMetric: 56.5583

Epoch 81: val_loss did not improve from 52.85801
196/196 - 34s - loss: 53.1906 - MinusLogProbMetric: 53.1906 - val_loss: 56.5583 - val_MinusLogProbMetric: 56.5583 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 82/1000
2023-10-28 18:27:09.704 
Epoch 82/1000 
	 loss: 53.2697, MinusLogProbMetric: 53.2697, val_loss: 53.5070, val_MinusLogProbMetric: 53.5070

Epoch 82: val_loss did not improve from 52.85801
196/196 - 34s - loss: 53.2697 - MinusLogProbMetric: 53.2697 - val_loss: 53.5070 - val_MinusLogProbMetric: 53.5070 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 83/1000
2023-10-28 18:27:43.220 
Epoch 83/1000 
	 loss: 53.5866, MinusLogProbMetric: 53.5866, val_loss: 53.8735, val_MinusLogProbMetric: 53.8735

Epoch 83: val_loss did not improve from 52.85801
196/196 - 34s - loss: 53.5866 - MinusLogProbMetric: 53.5866 - val_loss: 53.8735 - val_MinusLogProbMetric: 53.8735 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 84/1000
2023-10-28 18:28:17.362 
Epoch 84/1000 
	 loss: 52.8536, MinusLogProbMetric: 52.8536, val_loss: 52.2557, val_MinusLogProbMetric: 52.2557

Epoch 84: val_loss improved from 52.85801 to 52.25570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 52.8536 - MinusLogProbMetric: 52.8536 - val_loss: 52.2557 - val_MinusLogProbMetric: 52.2557 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 85/1000
2023-10-28 18:28:51.801 
Epoch 85/1000 
	 loss: 53.8902, MinusLogProbMetric: 53.8902, val_loss: 52.9003, val_MinusLogProbMetric: 52.9003

Epoch 85: val_loss did not improve from 52.25570
196/196 - 33s - loss: 53.8902 - MinusLogProbMetric: 53.8902 - val_loss: 52.9003 - val_MinusLogProbMetric: 52.9003 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 86/1000
2023-10-28 18:29:26.014 
Epoch 86/1000 
	 loss: 53.0524, MinusLogProbMetric: 53.0524, val_loss: 53.2412, val_MinusLogProbMetric: 53.2412

Epoch 86: val_loss did not improve from 52.25570
196/196 - 34s - loss: 53.0524 - MinusLogProbMetric: 53.0524 - val_loss: 53.2412 - val_MinusLogProbMetric: 53.2412 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 87/1000
2023-10-28 18:30:00.041 
Epoch 87/1000 
	 loss: 53.1839, MinusLogProbMetric: 53.1839, val_loss: 53.8981, val_MinusLogProbMetric: 53.8981

Epoch 87: val_loss did not improve from 52.25570
196/196 - 34s - loss: 53.1839 - MinusLogProbMetric: 53.1839 - val_loss: 53.8981 - val_MinusLogProbMetric: 53.8981 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 88/1000
2023-10-28 18:30:33.549 
Epoch 88/1000 
	 loss: 52.6282, MinusLogProbMetric: 52.6282, val_loss: 57.5668, val_MinusLogProbMetric: 57.5668

Epoch 88: val_loss did not improve from 52.25570
196/196 - 34s - loss: 52.6282 - MinusLogProbMetric: 52.6282 - val_loss: 57.5668 - val_MinusLogProbMetric: 57.5668 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 89/1000
2023-10-28 18:31:07.204 
Epoch 89/1000 
	 loss: 53.0213, MinusLogProbMetric: 53.0213, val_loss: 52.2324, val_MinusLogProbMetric: 52.2324

Epoch 89: val_loss improved from 52.25570 to 52.23236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 53.0213 - MinusLogProbMetric: 53.0213 - val_loss: 52.2324 - val_MinusLogProbMetric: 52.2324 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 90/1000
2023-10-28 18:31:41.356 
Epoch 90/1000 
	 loss: 54.4652, MinusLogProbMetric: 54.4652, val_loss: 51.8423, val_MinusLogProbMetric: 51.8423

Epoch 90: val_loss improved from 52.23236 to 51.84234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 54.4652 - MinusLogProbMetric: 54.4652 - val_loss: 51.8423 - val_MinusLogProbMetric: 51.8423 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 91/1000
2023-10-28 18:32:15.751 
Epoch 91/1000 
	 loss: 52.2196, MinusLogProbMetric: 52.2196, val_loss: 53.4978, val_MinusLogProbMetric: 53.4978

Epoch 91: val_loss did not improve from 51.84234
196/196 - 33s - loss: 52.2196 - MinusLogProbMetric: 52.2196 - val_loss: 53.4978 - val_MinusLogProbMetric: 53.4978 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 92/1000
2023-10-28 18:32:50.454 
Epoch 92/1000 
	 loss: 52.4779, MinusLogProbMetric: 52.4779, val_loss: 53.0951, val_MinusLogProbMetric: 53.0951

Epoch 92: val_loss did not improve from 51.84234
196/196 - 35s - loss: 52.4779 - MinusLogProbMetric: 52.4779 - val_loss: 53.0951 - val_MinusLogProbMetric: 53.0951 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 93/1000
2023-10-28 18:33:24.689 
Epoch 93/1000 
	 loss: 52.1898, MinusLogProbMetric: 52.1898, val_loss: 54.2397, val_MinusLogProbMetric: 54.2397

Epoch 93: val_loss did not improve from 51.84234
196/196 - 34s - loss: 52.1898 - MinusLogProbMetric: 52.1898 - val_loss: 54.2397 - val_MinusLogProbMetric: 54.2397 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 94/1000
2023-10-28 18:33:58.795 
Epoch 94/1000 
	 loss: 52.1163, MinusLogProbMetric: 52.1163, val_loss: 52.3370, val_MinusLogProbMetric: 52.3370

Epoch 94: val_loss did not improve from 51.84234
196/196 - 34s - loss: 52.1163 - MinusLogProbMetric: 52.1163 - val_loss: 52.3370 - val_MinusLogProbMetric: 52.3370 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 95/1000
2023-10-28 18:34:33.014 
Epoch 95/1000 
	 loss: 52.6395, MinusLogProbMetric: 52.6395, val_loss: 52.2393, val_MinusLogProbMetric: 52.2393

Epoch 95: val_loss did not improve from 51.84234
196/196 - 34s - loss: 52.6395 - MinusLogProbMetric: 52.6395 - val_loss: 52.2393 - val_MinusLogProbMetric: 52.2393 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 96/1000
2023-10-28 18:35:07.185 
Epoch 96/1000 
	 loss: 51.9590, MinusLogProbMetric: 51.9590, val_loss: 51.7538, val_MinusLogProbMetric: 51.7538

Epoch 96: val_loss improved from 51.84234 to 51.75375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 51.9590 - MinusLogProbMetric: 51.9590 - val_loss: 51.7538 - val_MinusLogProbMetric: 51.7538 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 97/1000
2023-10-28 18:35:42.199 
Epoch 97/1000 
	 loss: 51.9109, MinusLogProbMetric: 51.9109, val_loss: 53.6729, val_MinusLogProbMetric: 53.6729

Epoch 97: val_loss did not improve from 51.75375
196/196 - 34s - loss: 51.9109 - MinusLogProbMetric: 51.9109 - val_loss: 53.6729 - val_MinusLogProbMetric: 53.6729 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 98/1000
2023-10-28 18:36:15.456 
Epoch 98/1000 
	 loss: 52.0268, MinusLogProbMetric: 52.0268, val_loss: 52.7522, val_MinusLogProbMetric: 52.7522

Epoch 98: val_loss did not improve from 51.75375
196/196 - 33s - loss: 52.0268 - MinusLogProbMetric: 52.0268 - val_loss: 52.7522 - val_MinusLogProbMetric: 52.7522 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 99/1000
2023-10-28 18:36:49.348 
Epoch 99/1000 
	 loss: 52.5379, MinusLogProbMetric: 52.5379, val_loss: 52.8618, val_MinusLogProbMetric: 52.8618

Epoch 99: val_loss did not improve from 51.75375
196/196 - 34s - loss: 52.5379 - MinusLogProbMetric: 52.5379 - val_loss: 52.8618 - val_MinusLogProbMetric: 52.8618 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 100/1000
2023-10-28 18:37:22.562 
Epoch 100/1000 
	 loss: 52.1528, MinusLogProbMetric: 52.1528, val_loss: 51.1808, val_MinusLogProbMetric: 51.1808

Epoch 100: val_loss improved from 51.75375 to 51.18083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 52.1528 - MinusLogProbMetric: 52.1528 - val_loss: 51.1808 - val_MinusLogProbMetric: 51.1808 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 101/1000
2023-10-28 18:37:56.899 
Epoch 101/1000 
	 loss: 51.6709, MinusLogProbMetric: 51.6709, val_loss: 51.2926, val_MinusLogProbMetric: 51.2926

Epoch 101: val_loss did not improve from 51.18083
196/196 - 34s - loss: 51.6709 - MinusLogProbMetric: 51.6709 - val_loss: 51.2926 - val_MinusLogProbMetric: 51.2926 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 102/1000
2023-10-28 18:38:30.531 
Epoch 102/1000 
	 loss: 51.9338, MinusLogProbMetric: 51.9338, val_loss: 53.2991, val_MinusLogProbMetric: 53.2991

Epoch 102: val_loss did not improve from 51.18083
196/196 - 34s - loss: 51.9338 - MinusLogProbMetric: 51.9338 - val_loss: 53.2991 - val_MinusLogProbMetric: 53.2991 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 103/1000
2023-10-28 18:39:04.862 
Epoch 103/1000 
	 loss: 52.0616, MinusLogProbMetric: 52.0616, val_loss: 51.2964, val_MinusLogProbMetric: 51.2964

Epoch 103: val_loss did not improve from 51.18083
196/196 - 34s - loss: 52.0616 - MinusLogProbMetric: 52.0616 - val_loss: 51.2964 - val_MinusLogProbMetric: 51.2964 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 104/1000
2023-10-28 18:39:38.046 
Epoch 104/1000 
	 loss: 51.8340, MinusLogProbMetric: 51.8340, val_loss: 52.5771, val_MinusLogProbMetric: 52.5771

Epoch 104: val_loss did not improve from 51.18083
196/196 - 33s - loss: 51.8340 - MinusLogProbMetric: 51.8340 - val_loss: 52.5771 - val_MinusLogProbMetric: 52.5771 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 105/1000
2023-10-28 18:40:12.322 
Epoch 105/1000 
	 loss: 51.3140, MinusLogProbMetric: 51.3140, val_loss: 53.3243, val_MinusLogProbMetric: 53.3243

Epoch 105: val_loss did not improve from 51.18083
196/196 - 34s - loss: 51.3140 - MinusLogProbMetric: 51.3140 - val_loss: 53.3243 - val_MinusLogProbMetric: 53.3243 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 106/1000
2023-10-28 18:40:46.117 
Epoch 106/1000 
	 loss: 51.6606, MinusLogProbMetric: 51.6606, val_loss: 51.0973, val_MinusLogProbMetric: 51.0973

Epoch 106: val_loss improved from 51.18083 to 51.09731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 51.6606 - MinusLogProbMetric: 51.6606 - val_loss: 51.0973 - val_MinusLogProbMetric: 51.0973 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 107/1000
2023-10-28 18:41:21.148 
Epoch 107/1000 
	 loss: 51.4274, MinusLogProbMetric: 51.4274, val_loss: 52.7584, val_MinusLogProbMetric: 52.7584

Epoch 107: val_loss did not improve from 51.09731
196/196 - 34s - loss: 51.4274 - MinusLogProbMetric: 51.4274 - val_loss: 52.7584 - val_MinusLogProbMetric: 52.7584 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 108/1000
2023-10-28 18:41:54.816 
Epoch 108/1000 
	 loss: 51.6732, MinusLogProbMetric: 51.6732, val_loss: 59.4798, val_MinusLogProbMetric: 59.4798

Epoch 108: val_loss did not improve from 51.09731
196/196 - 34s - loss: 51.6732 - MinusLogProbMetric: 51.6732 - val_loss: 59.4798 - val_MinusLogProbMetric: 59.4798 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 109/1000
2023-10-28 18:42:28.571 
Epoch 109/1000 
	 loss: 51.8417, MinusLogProbMetric: 51.8417, val_loss: 51.0787, val_MinusLogProbMetric: 51.0787

Epoch 109: val_loss improved from 51.09731 to 51.07870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 51.8417 - MinusLogProbMetric: 51.8417 - val_loss: 51.0787 - val_MinusLogProbMetric: 51.0787 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 110/1000
2023-10-28 18:43:03.173 
Epoch 110/1000 
	 loss: 51.4135, MinusLogProbMetric: 51.4135, val_loss: 52.0496, val_MinusLogProbMetric: 52.0496

Epoch 110: val_loss did not improve from 51.07870
196/196 - 34s - loss: 51.4135 - MinusLogProbMetric: 51.4135 - val_loss: 52.0496 - val_MinusLogProbMetric: 52.0496 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 111/1000
2023-10-28 18:43:37.105 
Epoch 111/1000 
	 loss: 50.8548, MinusLogProbMetric: 50.8548, val_loss: 51.9764, val_MinusLogProbMetric: 51.9764

Epoch 111: val_loss did not improve from 51.07870
196/196 - 34s - loss: 50.8548 - MinusLogProbMetric: 50.8548 - val_loss: 51.9764 - val_MinusLogProbMetric: 51.9764 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 112/1000
2023-10-28 18:44:10.943 
Epoch 112/1000 
	 loss: 51.5003, MinusLogProbMetric: 51.5003, val_loss: 52.3647, val_MinusLogProbMetric: 52.3647

Epoch 112: val_loss did not improve from 51.07870
196/196 - 34s - loss: 51.5003 - MinusLogProbMetric: 51.5003 - val_loss: 52.3647 - val_MinusLogProbMetric: 52.3647 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 113/1000
2023-10-28 18:44:44.558 
Epoch 113/1000 
	 loss: 51.5105, MinusLogProbMetric: 51.5105, val_loss: 50.5570, val_MinusLogProbMetric: 50.5570

Epoch 113: val_loss improved from 51.07870 to 50.55699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 51.5105 - MinusLogProbMetric: 51.5105 - val_loss: 50.5570 - val_MinusLogProbMetric: 50.5570 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 114/1000
2023-10-28 18:45:19.179 
Epoch 114/1000 
	 loss: 51.4688, MinusLogProbMetric: 51.4688, val_loss: 51.9696, val_MinusLogProbMetric: 51.9696

Epoch 114: val_loss did not improve from 50.55699
196/196 - 34s - loss: 51.4688 - MinusLogProbMetric: 51.4688 - val_loss: 51.9696 - val_MinusLogProbMetric: 51.9696 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 115/1000
2023-10-28 18:45:52.936 
Epoch 115/1000 
	 loss: 50.6749, MinusLogProbMetric: 50.6749, val_loss: 52.1322, val_MinusLogProbMetric: 52.1322

Epoch 115: val_loss did not improve from 50.55699
196/196 - 34s - loss: 50.6749 - MinusLogProbMetric: 50.6749 - val_loss: 52.1322 - val_MinusLogProbMetric: 52.1322 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 116/1000
2023-10-28 18:46:26.674 
Epoch 116/1000 
	 loss: 50.8743, MinusLogProbMetric: 50.8743, val_loss: 52.5743, val_MinusLogProbMetric: 52.5743

Epoch 116: val_loss did not improve from 50.55699
196/196 - 34s - loss: 50.8743 - MinusLogProbMetric: 50.8743 - val_loss: 52.5743 - val_MinusLogProbMetric: 52.5743 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 117/1000
2023-10-28 18:47:00.521 
Epoch 117/1000 
	 loss: 50.6806, MinusLogProbMetric: 50.6806, val_loss: 51.2299, val_MinusLogProbMetric: 51.2299

Epoch 117: val_loss did not improve from 50.55699
196/196 - 34s - loss: 50.6806 - MinusLogProbMetric: 50.6806 - val_loss: 51.2299 - val_MinusLogProbMetric: 51.2299 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 118/1000
2023-10-28 18:47:34.887 
Epoch 118/1000 
	 loss: 51.4041, MinusLogProbMetric: 51.4041, val_loss: 51.4191, val_MinusLogProbMetric: 51.4191

Epoch 118: val_loss did not improve from 50.55699
196/196 - 34s - loss: 51.4041 - MinusLogProbMetric: 51.4041 - val_loss: 51.4191 - val_MinusLogProbMetric: 51.4191 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 119/1000
2023-10-28 18:48:09.731 
Epoch 119/1000 
	 loss: 50.6259, MinusLogProbMetric: 50.6259, val_loss: 63.2617, val_MinusLogProbMetric: 63.2617

Epoch 119: val_loss did not improve from 50.55699
196/196 - 35s - loss: 50.6259 - MinusLogProbMetric: 50.6259 - val_loss: 63.2617 - val_MinusLogProbMetric: 63.2617 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 120/1000
2023-10-28 18:48:43.690 
Epoch 120/1000 
	 loss: 51.4512, MinusLogProbMetric: 51.4512, val_loss: 51.7012, val_MinusLogProbMetric: 51.7012

Epoch 120: val_loss did not improve from 50.55699
196/196 - 34s - loss: 51.4512 - MinusLogProbMetric: 51.4512 - val_loss: 51.7012 - val_MinusLogProbMetric: 51.7012 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 121/1000
2023-10-28 18:49:17.523 
Epoch 121/1000 
	 loss: 50.5442, MinusLogProbMetric: 50.5442, val_loss: 51.8996, val_MinusLogProbMetric: 51.8996

Epoch 121: val_loss did not improve from 50.55699
196/196 - 34s - loss: 50.5442 - MinusLogProbMetric: 50.5442 - val_loss: 51.8996 - val_MinusLogProbMetric: 51.8996 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 122/1000
2023-10-28 18:49:50.949 
Epoch 122/1000 
	 loss: 50.5835, MinusLogProbMetric: 50.5835, val_loss: 51.7372, val_MinusLogProbMetric: 51.7372

Epoch 122: val_loss did not improve from 50.55699
196/196 - 33s - loss: 50.5835 - MinusLogProbMetric: 50.5835 - val_loss: 51.7372 - val_MinusLogProbMetric: 51.7372 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 123/1000
2023-10-28 18:50:24.014 
Epoch 123/1000 
	 loss: 50.6879, MinusLogProbMetric: 50.6879, val_loss: 49.9007, val_MinusLogProbMetric: 49.9007

Epoch 123: val_loss improved from 50.55699 to 49.90075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 50.6879 - MinusLogProbMetric: 50.6879 - val_loss: 49.9007 - val_MinusLogProbMetric: 49.9007 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 124/1000
2023-10-28 18:50:58.034 
Epoch 124/1000 
	 loss: 63.4352, MinusLogProbMetric: 63.4352, val_loss: 54.5945, val_MinusLogProbMetric: 54.5945

Epoch 124: val_loss did not improve from 49.90075
196/196 - 33s - loss: 63.4352 - MinusLogProbMetric: 63.4352 - val_loss: 54.5945 - val_MinusLogProbMetric: 54.5945 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 125/1000
2023-10-28 18:51:31.828 
Epoch 125/1000 
	 loss: 52.6090, MinusLogProbMetric: 52.6090, val_loss: 52.7579, val_MinusLogProbMetric: 52.7579

Epoch 125: val_loss did not improve from 49.90075
196/196 - 34s - loss: 52.6090 - MinusLogProbMetric: 52.6090 - val_loss: 52.7579 - val_MinusLogProbMetric: 52.7579 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 126/1000
2023-10-28 18:52:05.391 
Epoch 126/1000 
	 loss: 52.4199, MinusLogProbMetric: 52.4199, val_loss: 52.4008, val_MinusLogProbMetric: 52.4008

Epoch 126: val_loss did not improve from 49.90075
196/196 - 34s - loss: 52.4199 - MinusLogProbMetric: 52.4199 - val_loss: 52.4008 - val_MinusLogProbMetric: 52.4008 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 127/1000
2023-10-28 18:52:39.229 
Epoch 127/1000 
	 loss: 51.8840, MinusLogProbMetric: 51.8840, val_loss: 52.1175, val_MinusLogProbMetric: 52.1175

Epoch 127: val_loss did not improve from 49.90075
196/196 - 34s - loss: 51.8840 - MinusLogProbMetric: 51.8840 - val_loss: 52.1175 - val_MinusLogProbMetric: 52.1175 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 128/1000
2023-10-28 18:53:13.368 
Epoch 128/1000 
	 loss: 51.5692, MinusLogProbMetric: 51.5692, val_loss: 51.2105, val_MinusLogProbMetric: 51.2105

Epoch 128: val_loss did not improve from 49.90075
196/196 - 34s - loss: 51.5692 - MinusLogProbMetric: 51.5692 - val_loss: 51.2105 - val_MinusLogProbMetric: 51.2105 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 129/1000
2023-10-28 18:53:47.734 
Epoch 129/1000 
	 loss: 52.0231, MinusLogProbMetric: 52.0231, val_loss: 51.5429, val_MinusLogProbMetric: 51.5429

Epoch 129: val_loss did not improve from 49.90075
196/196 - 34s - loss: 52.0231 - MinusLogProbMetric: 52.0231 - val_loss: 51.5429 - val_MinusLogProbMetric: 51.5429 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 130/1000
2023-10-28 18:54:22.064 
Epoch 130/1000 
	 loss: 51.4910, MinusLogProbMetric: 51.4910, val_loss: 51.8552, val_MinusLogProbMetric: 51.8552

Epoch 130: val_loss did not improve from 49.90075
196/196 - 34s - loss: 51.4910 - MinusLogProbMetric: 51.4910 - val_loss: 51.8552 - val_MinusLogProbMetric: 51.8552 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 131/1000
2023-10-28 18:54:55.846 
Epoch 131/1000 
	 loss: 51.2524, MinusLogProbMetric: 51.2524, val_loss: 52.4416, val_MinusLogProbMetric: 52.4416

Epoch 131: val_loss did not improve from 49.90075
196/196 - 34s - loss: 51.2524 - MinusLogProbMetric: 51.2524 - val_loss: 52.4416 - val_MinusLogProbMetric: 52.4416 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 132/1000
2023-10-28 18:55:29.214 
Epoch 132/1000 
	 loss: 51.2090, MinusLogProbMetric: 51.2090, val_loss: 52.0969, val_MinusLogProbMetric: 52.0969

Epoch 132: val_loss did not improve from 49.90075
196/196 - 33s - loss: 51.2090 - MinusLogProbMetric: 51.2090 - val_loss: 52.0969 - val_MinusLogProbMetric: 52.0969 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 133/1000
2023-10-28 18:56:02.944 
Epoch 133/1000 
	 loss: 52.2504, MinusLogProbMetric: 52.2504, val_loss: 51.0368, val_MinusLogProbMetric: 51.0368

Epoch 133: val_loss did not improve from 49.90075
196/196 - 34s - loss: 52.2504 - MinusLogProbMetric: 52.2504 - val_loss: 51.0368 - val_MinusLogProbMetric: 51.0368 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 134/1000
2023-10-28 18:56:37.101 
Epoch 134/1000 
	 loss: 50.3455, MinusLogProbMetric: 50.3455, val_loss: 52.1238, val_MinusLogProbMetric: 52.1238

Epoch 134: val_loss did not improve from 49.90075
196/196 - 34s - loss: 50.3455 - MinusLogProbMetric: 50.3455 - val_loss: 52.1238 - val_MinusLogProbMetric: 52.1238 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 135/1000
2023-10-28 18:57:11.261 
Epoch 135/1000 
	 loss: 50.4523, MinusLogProbMetric: 50.4523, val_loss: 50.5089, val_MinusLogProbMetric: 50.5089

Epoch 135: val_loss did not improve from 49.90075
196/196 - 34s - loss: 50.4523 - MinusLogProbMetric: 50.4523 - val_loss: 50.5089 - val_MinusLogProbMetric: 50.5089 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 136/1000
2023-10-28 18:57:45.196 
Epoch 136/1000 
	 loss: 50.5943, MinusLogProbMetric: 50.5943, val_loss: 52.3860, val_MinusLogProbMetric: 52.3860

Epoch 136: val_loss did not improve from 49.90075
196/196 - 34s - loss: 50.5943 - MinusLogProbMetric: 50.5943 - val_loss: 52.3860 - val_MinusLogProbMetric: 52.3860 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 137/1000
2023-10-28 18:58:18.540 
Epoch 137/1000 
	 loss: 51.7406, MinusLogProbMetric: 51.7406, val_loss: 56.7932, val_MinusLogProbMetric: 56.7932

Epoch 137: val_loss did not improve from 49.90075
196/196 - 33s - loss: 51.7406 - MinusLogProbMetric: 51.7406 - val_loss: 56.7932 - val_MinusLogProbMetric: 56.7932 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 138/1000
2023-10-28 18:58:52.609 
Epoch 138/1000 
	 loss: 51.5947, MinusLogProbMetric: 51.5947, val_loss: 51.4794, val_MinusLogProbMetric: 51.4794

Epoch 138: val_loss did not improve from 49.90075
196/196 - 34s - loss: 51.5947 - MinusLogProbMetric: 51.5947 - val_loss: 51.4794 - val_MinusLogProbMetric: 51.4794 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 139/1000
2023-10-28 18:59:26.019 
Epoch 139/1000 
	 loss: 50.3030, MinusLogProbMetric: 50.3030, val_loss: 52.5929, val_MinusLogProbMetric: 52.5929

Epoch 139: val_loss did not improve from 49.90075
196/196 - 33s - loss: 50.3030 - MinusLogProbMetric: 50.3030 - val_loss: 52.5929 - val_MinusLogProbMetric: 52.5929 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 140/1000
2023-10-28 18:59:59.369 
Epoch 140/1000 
	 loss: 51.2379, MinusLogProbMetric: 51.2379, val_loss: 58.2743, val_MinusLogProbMetric: 58.2743

Epoch 140: val_loss did not improve from 49.90075
196/196 - 33s - loss: 51.2379 - MinusLogProbMetric: 51.2379 - val_loss: 58.2743 - val_MinusLogProbMetric: 58.2743 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 141/1000
2023-10-28 19:00:33.019 
Epoch 141/1000 
	 loss: 51.2221, MinusLogProbMetric: 51.2221, val_loss: 49.7437, val_MinusLogProbMetric: 49.7437

Epoch 141: val_loss improved from 49.90075 to 49.74369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 51.2221 - MinusLogProbMetric: 51.2221 - val_loss: 49.7437 - val_MinusLogProbMetric: 49.7437 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 142/1000
2023-10-28 19:01:06.834 
Epoch 142/1000 
	 loss: 50.5524, MinusLogProbMetric: 50.5524, val_loss: 51.3107, val_MinusLogProbMetric: 51.3107

Epoch 142: val_loss did not improve from 49.74369
196/196 - 33s - loss: 50.5524 - MinusLogProbMetric: 50.5524 - val_loss: 51.3107 - val_MinusLogProbMetric: 51.3107 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 143/1000
2023-10-28 19:01:40.742 
Epoch 143/1000 
	 loss: 50.9322, MinusLogProbMetric: 50.9322, val_loss: 50.4386, val_MinusLogProbMetric: 50.4386

Epoch 143: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.9322 - MinusLogProbMetric: 50.9322 - val_loss: 50.4386 - val_MinusLogProbMetric: 50.4386 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 144/1000
2023-10-28 19:02:14.959 
Epoch 144/1000 
	 loss: 50.7648, MinusLogProbMetric: 50.7648, val_loss: 49.8266, val_MinusLogProbMetric: 49.8266

Epoch 144: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.7648 - MinusLogProbMetric: 50.7648 - val_loss: 49.8266 - val_MinusLogProbMetric: 49.8266 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 145/1000
2023-10-28 19:02:48.506 
Epoch 145/1000 
	 loss: 50.3903, MinusLogProbMetric: 50.3903, val_loss: 49.8144, val_MinusLogProbMetric: 49.8144

Epoch 145: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.3903 - MinusLogProbMetric: 50.3903 - val_loss: 49.8144 - val_MinusLogProbMetric: 49.8144 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 146/1000
2023-10-28 19:03:22.831 
Epoch 146/1000 
	 loss: 50.1955, MinusLogProbMetric: 50.1955, val_loss: 50.0959, val_MinusLogProbMetric: 50.0959

Epoch 146: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.1955 - MinusLogProbMetric: 50.1955 - val_loss: 50.0959 - val_MinusLogProbMetric: 50.0959 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 147/1000
2023-10-28 19:03:57.157 
Epoch 147/1000 
	 loss: 50.6484, MinusLogProbMetric: 50.6484, val_loss: 50.9737, val_MinusLogProbMetric: 50.9737

Epoch 147: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.6484 - MinusLogProbMetric: 50.6484 - val_loss: 50.9737 - val_MinusLogProbMetric: 50.9737 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 148/1000
2023-10-28 19:04:31.021 
Epoch 148/1000 
	 loss: 51.4320, MinusLogProbMetric: 51.4320, val_loss: 51.0512, val_MinusLogProbMetric: 51.0512

Epoch 148: val_loss did not improve from 49.74369
196/196 - 34s - loss: 51.4320 - MinusLogProbMetric: 51.4320 - val_loss: 51.0512 - val_MinusLogProbMetric: 51.0512 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 149/1000
2023-10-28 19:05:05.138 
Epoch 149/1000 
	 loss: 51.0188, MinusLogProbMetric: 51.0188, val_loss: 50.7964, val_MinusLogProbMetric: 50.7964

Epoch 149: val_loss did not improve from 49.74369
196/196 - 34s - loss: 51.0188 - MinusLogProbMetric: 51.0188 - val_loss: 50.7964 - val_MinusLogProbMetric: 50.7964 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 150/1000
2023-10-28 19:05:39.209 
Epoch 150/1000 
	 loss: 50.3989, MinusLogProbMetric: 50.3989, val_loss: 50.5419, val_MinusLogProbMetric: 50.5419

Epoch 150: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.3989 - MinusLogProbMetric: 50.3989 - val_loss: 50.5419 - val_MinusLogProbMetric: 50.5419 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 151/1000
2023-10-28 19:06:13.436 
Epoch 151/1000 
	 loss: 50.4249, MinusLogProbMetric: 50.4249, val_loss: 49.7965, val_MinusLogProbMetric: 49.7965

Epoch 151: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.4249 - MinusLogProbMetric: 50.4249 - val_loss: 49.7965 - val_MinusLogProbMetric: 49.7965 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 152/1000
2023-10-28 19:06:47.694 
Epoch 152/1000 
	 loss: 50.1728, MinusLogProbMetric: 50.1728, val_loss: 50.4755, val_MinusLogProbMetric: 50.4755

Epoch 152: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.1728 - MinusLogProbMetric: 50.1728 - val_loss: 50.4755 - val_MinusLogProbMetric: 50.4755 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 153/1000
2023-10-28 19:07:21.707 
Epoch 153/1000 
	 loss: 50.2938, MinusLogProbMetric: 50.2938, val_loss: 50.5930, val_MinusLogProbMetric: 50.5930

Epoch 153: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.2938 - MinusLogProbMetric: 50.2938 - val_loss: 50.5930 - val_MinusLogProbMetric: 50.5930 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 154/1000
2023-10-28 19:07:55.520 
Epoch 154/1000 
	 loss: 50.4428, MinusLogProbMetric: 50.4428, val_loss: 51.4196, val_MinusLogProbMetric: 51.4196

Epoch 154: val_loss did not improve from 49.74369
196/196 - 34s - loss: 50.4428 - MinusLogProbMetric: 50.4428 - val_loss: 51.4196 - val_MinusLogProbMetric: 51.4196 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 155/1000
2023-10-28 19:08:29.391 
Epoch 155/1000 
	 loss: 49.6294, MinusLogProbMetric: 49.6294, val_loss: 49.4157, val_MinusLogProbMetric: 49.4157

Epoch 155: val_loss improved from 49.74369 to 49.41570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 49.6294 - MinusLogProbMetric: 49.6294 - val_loss: 49.4157 - val_MinusLogProbMetric: 49.4157 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 156/1000
2023-10-28 19:09:03.847 
Epoch 156/1000 
	 loss: 49.5614, MinusLogProbMetric: 49.5614, val_loss: 49.9895, val_MinusLogProbMetric: 49.9895

Epoch 156: val_loss did not improve from 49.41570
196/196 - 34s - loss: 49.5614 - MinusLogProbMetric: 49.5614 - val_loss: 49.9895 - val_MinusLogProbMetric: 49.9895 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 157/1000
2023-10-28 19:09:37.657 
Epoch 157/1000 
	 loss: 49.4759, MinusLogProbMetric: 49.4759, val_loss: 49.4092, val_MinusLogProbMetric: 49.4092

Epoch 157: val_loss improved from 49.41570 to 49.40924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 49.4759 - MinusLogProbMetric: 49.4759 - val_loss: 49.4092 - val_MinusLogProbMetric: 49.4092 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 158/1000
2023-10-28 19:10:12.420 
Epoch 158/1000 
	 loss: 49.3896, MinusLogProbMetric: 49.3896, val_loss: 50.4341, val_MinusLogProbMetric: 50.4341

Epoch 158: val_loss did not improve from 49.40924
196/196 - 34s - loss: 49.3896 - MinusLogProbMetric: 49.3896 - val_loss: 50.4341 - val_MinusLogProbMetric: 50.4341 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 159/1000
2023-10-28 19:10:46.348 
Epoch 159/1000 
	 loss: 49.4873, MinusLogProbMetric: 49.4873, val_loss: 51.5483, val_MinusLogProbMetric: 51.5483

Epoch 159: val_loss did not improve from 49.40924
196/196 - 34s - loss: 49.4873 - MinusLogProbMetric: 49.4873 - val_loss: 51.5483 - val_MinusLogProbMetric: 51.5483 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 160/1000
2023-10-28 19:11:20.247 
Epoch 160/1000 
	 loss: 49.3524, MinusLogProbMetric: 49.3524, val_loss: 49.8587, val_MinusLogProbMetric: 49.8587

Epoch 160: val_loss did not improve from 49.40924
196/196 - 34s - loss: 49.3524 - MinusLogProbMetric: 49.3524 - val_loss: 49.8587 - val_MinusLogProbMetric: 49.8587 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 161/1000
2023-10-28 19:11:53.974 
Epoch 161/1000 
	 loss: 49.3828, MinusLogProbMetric: 49.3828, val_loss: 49.8921, val_MinusLogProbMetric: 49.8921

Epoch 161: val_loss did not improve from 49.40924
196/196 - 34s - loss: 49.3828 - MinusLogProbMetric: 49.3828 - val_loss: 49.8921 - val_MinusLogProbMetric: 49.8921 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 162/1000
2023-10-28 19:12:27.816 
Epoch 162/1000 
	 loss: 49.4244, MinusLogProbMetric: 49.4244, val_loss: 50.8206, val_MinusLogProbMetric: 50.8206

Epoch 162: val_loss did not improve from 49.40924
196/196 - 34s - loss: 49.4244 - MinusLogProbMetric: 49.4244 - val_loss: 50.8206 - val_MinusLogProbMetric: 50.8206 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 163/1000
2023-10-28 19:13:02.013 
Epoch 163/1000 
	 loss: 50.0396, MinusLogProbMetric: 50.0396, val_loss: 49.8800, val_MinusLogProbMetric: 49.8800

Epoch 163: val_loss did not improve from 49.40924
196/196 - 34s - loss: 50.0396 - MinusLogProbMetric: 50.0396 - val_loss: 49.8800 - val_MinusLogProbMetric: 49.8800 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 164/1000
2023-10-28 19:13:36.567 
Epoch 164/1000 
	 loss: 50.7628, MinusLogProbMetric: 50.7628, val_loss: 51.5401, val_MinusLogProbMetric: 51.5401

Epoch 164: val_loss did not improve from 49.40924
196/196 - 35s - loss: 50.7628 - MinusLogProbMetric: 50.7628 - val_loss: 51.5401 - val_MinusLogProbMetric: 51.5401 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 165/1000
2023-10-28 19:14:10.672 
Epoch 165/1000 
	 loss: 51.1116, MinusLogProbMetric: 51.1116, val_loss: 50.3581, val_MinusLogProbMetric: 50.3581

Epoch 165: val_loss did not improve from 49.40924
196/196 - 34s - loss: 51.1116 - MinusLogProbMetric: 51.1116 - val_loss: 50.3581 - val_MinusLogProbMetric: 50.3581 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 166/1000
2023-10-28 19:14:43.770 
Epoch 166/1000 
	 loss: 49.5900, MinusLogProbMetric: 49.5900, val_loss: 50.2034, val_MinusLogProbMetric: 50.2034

Epoch 166: val_loss did not improve from 49.40924
196/196 - 33s - loss: 49.5900 - MinusLogProbMetric: 49.5900 - val_loss: 50.2034 - val_MinusLogProbMetric: 50.2034 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 167/1000
2023-10-28 19:15:16.494 
Epoch 167/1000 
	 loss: 50.0228, MinusLogProbMetric: 50.0228, val_loss: 49.3422, val_MinusLogProbMetric: 49.3422

Epoch 167: val_loss improved from 49.40924 to 49.34221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 33s - loss: 50.0228 - MinusLogProbMetric: 50.0228 - val_loss: 49.3422 - val_MinusLogProbMetric: 49.3422 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 168/1000
2023-10-28 19:15:50.898 
Epoch 168/1000 
	 loss: 49.6446, MinusLogProbMetric: 49.6446, val_loss: 49.3684, val_MinusLogProbMetric: 49.3684

Epoch 168: val_loss did not improve from 49.34221
196/196 - 34s - loss: 49.6446 - MinusLogProbMetric: 49.6446 - val_loss: 49.3684 - val_MinusLogProbMetric: 49.3684 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 169/1000
2023-10-28 19:16:25.156 
Epoch 169/1000 
	 loss: 49.6502, MinusLogProbMetric: 49.6502, val_loss: 55.6615, val_MinusLogProbMetric: 55.6615

Epoch 169: val_loss did not improve from 49.34221
196/196 - 34s - loss: 49.6502 - MinusLogProbMetric: 49.6502 - val_loss: 55.6615 - val_MinusLogProbMetric: 55.6615 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 170/1000
2023-10-28 19:16:59.646 
Epoch 170/1000 
	 loss: 49.3235, MinusLogProbMetric: 49.3235, val_loss: 48.4082, val_MinusLogProbMetric: 48.4082

Epoch 170: val_loss improved from 49.34221 to 48.40822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 49.3235 - MinusLogProbMetric: 49.3235 - val_loss: 48.4082 - val_MinusLogProbMetric: 48.4082 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 171/1000
2023-10-28 19:17:32.402 
Epoch 171/1000 
	 loss: 49.2626, MinusLogProbMetric: 49.2626, val_loss: 49.0054, val_MinusLogProbMetric: 49.0054

Epoch 171: val_loss did not improve from 48.40822
196/196 - 32s - loss: 49.2626 - MinusLogProbMetric: 49.2626 - val_loss: 49.0054 - val_MinusLogProbMetric: 49.0054 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 172/1000
2023-10-28 19:18:05.677 
Epoch 172/1000 
	 loss: 53.4501, MinusLogProbMetric: 53.4501, val_loss: 54.2944, val_MinusLogProbMetric: 54.2944

Epoch 172: val_loss did not improve from 48.40822
196/196 - 33s - loss: 53.4501 - MinusLogProbMetric: 53.4501 - val_loss: 54.2944 - val_MinusLogProbMetric: 54.2944 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 173/1000
2023-10-28 19:18:39.099 
Epoch 173/1000 
	 loss: 49.7712, MinusLogProbMetric: 49.7712, val_loss: 49.2219, val_MinusLogProbMetric: 49.2219

Epoch 173: val_loss did not improve from 48.40822
196/196 - 33s - loss: 49.7712 - MinusLogProbMetric: 49.7712 - val_loss: 49.2219 - val_MinusLogProbMetric: 49.2219 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 174/1000
2023-10-28 19:19:13.316 
Epoch 174/1000 
	 loss: 48.5656, MinusLogProbMetric: 48.5656, val_loss: 49.0557, val_MinusLogProbMetric: 49.0557

Epoch 174: val_loss did not improve from 48.40822
196/196 - 34s - loss: 48.5656 - MinusLogProbMetric: 48.5656 - val_loss: 49.0557 - val_MinusLogProbMetric: 49.0557 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 175/1000
2023-10-28 19:19:47.070 
Epoch 175/1000 
	 loss: 49.1218, MinusLogProbMetric: 49.1218, val_loss: 50.4652, val_MinusLogProbMetric: 50.4652

Epoch 175: val_loss did not improve from 48.40822
196/196 - 34s - loss: 49.1218 - MinusLogProbMetric: 49.1218 - val_loss: 50.4652 - val_MinusLogProbMetric: 50.4652 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 176/1000
2023-10-28 19:20:20.689 
Epoch 176/1000 
	 loss: 48.6344, MinusLogProbMetric: 48.6344, val_loss: 48.7620, val_MinusLogProbMetric: 48.7620

Epoch 176: val_loss did not improve from 48.40822
196/196 - 34s - loss: 48.6344 - MinusLogProbMetric: 48.6344 - val_loss: 48.7620 - val_MinusLogProbMetric: 48.7620 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 177/1000
2023-10-28 19:20:54.788 
Epoch 177/1000 
	 loss: 48.4933, MinusLogProbMetric: 48.4933, val_loss: 48.1830, val_MinusLogProbMetric: 48.1830

Epoch 177: val_loss improved from 48.40822 to 48.18302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 48.4933 - MinusLogProbMetric: 48.4933 - val_loss: 48.1830 - val_MinusLogProbMetric: 48.1830 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 178/1000
2023-10-28 19:21:29.927 
Epoch 178/1000 
	 loss: 48.8051, MinusLogProbMetric: 48.8051, val_loss: 49.0508, val_MinusLogProbMetric: 49.0508

Epoch 178: val_loss did not improve from 48.18302
196/196 - 35s - loss: 48.8051 - MinusLogProbMetric: 48.8051 - val_loss: 49.0508 - val_MinusLogProbMetric: 49.0508 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 179/1000
2023-10-28 19:22:02.949 
Epoch 179/1000 
	 loss: 48.7326, MinusLogProbMetric: 48.7326, val_loss: 50.5512, val_MinusLogProbMetric: 50.5512

Epoch 179: val_loss did not improve from 48.18302
196/196 - 33s - loss: 48.7326 - MinusLogProbMetric: 48.7326 - val_loss: 50.5512 - val_MinusLogProbMetric: 50.5512 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 180/1000
2023-10-28 19:22:37.025 
Epoch 180/1000 
	 loss: 48.6095, MinusLogProbMetric: 48.6095, val_loss: 50.8172, val_MinusLogProbMetric: 50.8172

Epoch 180: val_loss did not improve from 48.18302
196/196 - 34s - loss: 48.6095 - MinusLogProbMetric: 48.6095 - val_loss: 50.8172 - val_MinusLogProbMetric: 50.8172 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 181/1000
2023-10-28 19:23:11.148 
Epoch 181/1000 
	 loss: 48.6951, MinusLogProbMetric: 48.6951, val_loss: 51.9262, val_MinusLogProbMetric: 51.9262

Epoch 181: val_loss did not improve from 48.18302
196/196 - 34s - loss: 48.6951 - MinusLogProbMetric: 48.6951 - val_loss: 51.9262 - val_MinusLogProbMetric: 51.9262 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 182/1000
2023-10-28 19:23:45.853 
Epoch 182/1000 
	 loss: 48.6899, MinusLogProbMetric: 48.6899, val_loss: 49.1701, val_MinusLogProbMetric: 49.1701

Epoch 182: val_loss did not improve from 48.18302
196/196 - 35s - loss: 48.6899 - MinusLogProbMetric: 48.6899 - val_loss: 49.1701 - val_MinusLogProbMetric: 49.1701 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 183/1000
2023-10-28 19:24:20.509 
Epoch 183/1000 
	 loss: 48.5986, MinusLogProbMetric: 48.5986, val_loss: 48.5833, val_MinusLogProbMetric: 48.5833

Epoch 183: val_loss did not improve from 48.18302
196/196 - 35s - loss: 48.5986 - MinusLogProbMetric: 48.5986 - val_loss: 48.5833 - val_MinusLogProbMetric: 48.5833 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 184/1000
2023-10-28 19:24:54.822 
Epoch 184/1000 
	 loss: 48.6714, MinusLogProbMetric: 48.6714, val_loss: 48.0424, val_MinusLogProbMetric: 48.0424

Epoch 184: val_loss improved from 48.18302 to 48.04237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 48.6714 - MinusLogProbMetric: 48.6714 - val_loss: 48.0424 - val_MinusLogProbMetric: 48.0424 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 185/1000
2023-10-28 19:25:29.828 
Epoch 185/1000 
	 loss: 48.9927, MinusLogProbMetric: 48.9927, val_loss: 48.9649, val_MinusLogProbMetric: 48.9649

Epoch 185: val_loss did not improve from 48.04237
196/196 - 34s - loss: 48.9927 - MinusLogProbMetric: 48.9927 - val_loss: 48.9649 - val_MinusLogProbMetric: 48.9649 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 186/1000
2023-10-28 19:26:04.421 
Epoch 186/1000 
	 loss: 48.4674, MinusLogProbMetric: 48.4674, val_loss: 49.0574, val_MinusLogProbMetric: 49.0574

Epoch 186: val_loss did not improve from 48.04237
196/196 - 35s - loss: 48.4674 - MinusLogProbMetric: 48.4674 - val_loss: 49.0574 - val_MinusLogProbMetric: 49.0574 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 187/1000
2023-10-28 19:26:38.531 
Epoch 187/1000 
	 loss: 48.5921, MinusLogProbMetric: 48.5921, val_loss: 50.6145, val_MinusLogProbMetric: 50.6145

Epoch 187: val_loss did not improve from 48.04237
196/196 - 34s - loss: 48.5921 - MinusLogProbMetric: 48.5921 - val_loss: 50.6145 - val_MinusLogProbMetric: 50.6145 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 188/1000
2023-10-28 19:27:12.352 
Epoch 188/1000 
	 loss: 48.6263, MinusLogProbMetric: 48.6263, val_loss: 52.9428, val_MinusLogProbMetric: 52.9428

Epoch 188: val_loss did not improve from 48.04237
196/196 - 34s - loss: 48.6263 - MinusLogProbMetric: 48.6263 - val_loss: 52.9428 - val_MinusLogProbMetric: 52.9428 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 189/1000
2023-10-28 19:27:46.253 
Epoch 189/1000 
	 loss: 48.6549, MinusLogProbMetric: 48.6549, val_loss: 52.2550, val_MinusLogProbMetric: 52.2550

Epoch 189: val_loss did not improve from 48.04237
196/196 - 34s - loss: 48.6549 - MinusLogProbMetric: 48.6549 - val_loss: 52.2550 - val_MinusLogProbMetric: 52.2550 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 190/1000
2023-10-28 19:28:19.833 
Epoch 190/1000 
	 loss: 48.6508, MinusLogProbMetric: 48.6508, val_loss: 47.4991, val_MinusLogProbMetric: 47.4991

Epoch 190: val_loss improved from 48.04237 to 47.49908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 48.6508 - MinusLogProbMetric: 48.6508 - val_loss: 47.4991 - val_MinusLogProbMetric: 47.4991 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 191/1000
2023-10-28 19:28:53.898 
Epoch 191/1000 
	 loss: 48.4161, MinusLogProbMetric: 48.4161, val_loss: 49.5019, val_MinusLogProbMetric: 49.5019

Epoch 191: val_loss did not improve from 47.49908
196/196 - 33s - loss: 48.4161 - MinusLogProbMetric: 48.4161 - val_loss: 49.5019 - val_MinusLogProbMetric: 49.5019 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 192/1000
2023-10-28 19:29:27.348 
Epoch 192/1000 
	 loss: 48.2556, MinusLogProbMetric: 48.2556, val_loss: 48.9373, val_MinusLogProbMetric: 48.9373

Epoch 192: val_loss did not improve from 47.49908
196/196 - 33s - loss: 48.2556 - MinusLogProbMetric: 48.2556 - val_loss: 48.9373 - val_MinusLogProbMetric: 48.9373 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 193/1000
2023-10-28 19:30:00.774 
Epoch 193/1000 
	 loss: 48.2939, MinusLogProbMetric: 48.2939, val_loss: 50.3166, val_MinusLogProbMetric: 50.3166

Epoch 193: val_loss did not improve from 47.49908
196/196 - 33s - loss: 48.2939 - MinusLogProbMetric: 48.2939 - val_loss: 50.3166 - val_MinusLogProbMetric: 50.3166 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 194/1000
2023-10-28 19:30:34.892 
Epoch 194/1000 
	 loss: 48.5337, MinusLogProbMetric: 48.5337, val_loss: 49.5909, val_MinusLogProbMetric: 49.5909

Epoch 194: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.5337 - MinusLogProbMetric: 48.5337 - val_loss: 49.5909 - val_MinusLogProbMetric: 49.5909 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 195/1000
2023-10-28 19:31:09.012 
Epoch 195/1000 
	 loss: 48.1028, MinusLogProbMetric: 48.1028, val_loss: 49.5108, val_MinusLogProbMetric: 49.5108

Epoch 195: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.1028 - MinusLogProbMetric: 48.1028 - val_loss: 49.5108 - val_MinusLogProbMetric: 49.5108 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 196/1000
2023-10-28 19:31:43.233 
Epoch 196/1000 
	 loss: 48.4679, MinusLogProbMetric: 48.4679, val_loss: 50.2582, val_MinusLogProbMetric: 50.2582

Epoch 196: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.4679 - MinusLogProbMetric: 48.4679 - val_loss: 50.2582 - val_MinusLogProbMetric: 50.2582 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 197/1000
2023-10-28 19:32:17.481 
Epoch 197/1000 
	 loss: 48.3008, MinusLogProbMetric: 48.3008, val_loss: 48.0771, val_MinusLogProbMetric: 48.0771

Epoch 197: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.3008 - MinusLogProbMetric: 48.3008 - val_loss: 48.0771 - val_MinusLogProbMetric: 48.0771 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 198/1000
2023-10-28 19:32:51.852 
Epoch 198/1000 
	 loss: 48.2097, MinusLogProbMetric: 48.2097, val_loss: 48.5649, val_MinusLogProbMetric: 48.5649

Epoch 198: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.2097 - MinusLogProbMetric: 48.2097 - val_loss: 48.5649 - val_MinusLogProbMetric: 48.5649 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 199/1000
2023-10-28 19:33:25.438 
Epoch 199/1000 
	 loss: 48.4665, MinusLogProbMetric: 48.4665, val_loss: 48.2459, val_MinusLogProbMetric: 48.2459

Epoch 199: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.4665 - MinusLogProbMetric: 48.4665 - val_loss: 48.2459 - val_MinusLogProbMetric: 48.2459 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 200/1000
2023-10-28 19:33:59.450 
Epoch 200/1000 
	 loss: 48.0095, MinusLogProbMetric: 48.0095, val_loss: 49.8611, val_MinusLogProbMetric: 49.8611

Epoch 200: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.0095 - MinusLogProbMetric: 48.0095 - val_loss: 49.8611 - val_MinusLogProbMetric: 49.8611 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 201/1000
2023-10-28 19:34:32.967 
Epoch 201/1000 
	 loss: 48.3944, MinusLogProbMetric: 48.3944, val_loss: 50.5033, val_MinusLogProbMetric: 50.5033

Epoch 201: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.3944 - MinusLogProbMetric: 48.3944 - val_loss: 50.5033 - val_MinusLogProbMetric: 50.5033 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 202/1000
2023-10-28 19:35:06.465 
Epoch 202/1000 
	 loss: 48.1932, MinusLogProbMetric: 48.1932, val_loss: 49.3088, val_MinusLogProbMetric: 49.3088

Epoch 202: val_loss did not improve from 47.49908
196/196 - 33s - loss: 48.1932 - MinusLogProbMetric: 48.1932 - val_loss: 49.3088 - val_MinusLogProbMetric: 49.3088 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 203/1000
2023-10-28 19:35:40.217 
Epoch 203/1000 
	 loss: 48.3304, MinusLogProbMetric: 48.3304, val_loss: 49.0156, val_MinusLogProbMetric: 49.0156

Epoch 203: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.3304 - MinusLogProbMetric: 48.3304 - val_loss: 49.0156 - val_MinusLogProbMetric: 49.0156 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 204/1000
2023-10-28 19:36:13.644 
Epoch 204/1000 
	 loss: 48.2006, MinusLogProbMetric: 48.2006, val_loss: 50.3688, val_MinusLogProbMetric: 50.3688

Epoch 204: val_loss did not improve from 47.49908
196/196 - 33s - loss: 48.2006 - MinusLogProbMetric: 48.2006 - val_loss: 50.3688 - val_MinusLogProbMetric: 50.3688 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 205/1000
2023-10-28 19:36:47.425 
Epoch 205/1000 
	 loss: 48.0505, MinusLogProbMetric: 48.0505, val_loss: 49.0467, val_MinusLogProbMetric: 49.0467

Epoch 205: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.0505 - MinusLogProbMetric: 48.0505 - val_loss: 49.0467 - val_MinusLogProbMetric: 49.0467 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 206/1000
2023-10-28 19:37:20.796 
Epoch 206/1000 
	 loss: 48.4673, MinusLogProbMetric: 48.4673, val_loss: 49.1757, val_MinusLogProbMetric: 49.1757

Epoch 206: val_loss did not improve from 47.49908
196/196 - 33s - loss: 48.4673 - MinusLogProbMetric: 48.4673 - val_loss: 49.1757 - val_MinusLogProbMetric: 49.1757 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 207/1000
2023-10-28 19:37:54.065 
Epoch 207/1000 
	 loss: 48.1025, MinusLogProbMetric: 48.1025, val_loss: 49.9581, val_MinusLogProbMetric: 49.9581

Epoch 207: val_loss did not improve from 47.49908
196/196 - 33s - loss: 48.1025 - MinusLogProbMetric: 48.1025 - val_loss: 49.9581 - val_MinusLogProbMetric: 49.9581 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 208/1000
2023-10-28 19:38:27.585 
Epoch 208/1000 
	 loss: 47.9557, MinusLogProbMetric: 47.9557, val_loss: 49.3897, val_MinusLogProbMetric: 49.3897

Epoch 208: val_loss did not improve from 47.49908
196/196 - 34s - loss: 47.9557 - MinusLogProbMetric: 47.9557 - val_loss: 49.3897 - val_MinusLogProbMetric: 49.3897 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 209/1000
2023-10-28 19:39:02.112 
Epoch 209/1000 
	 loss: 48.4380, MinusLogProbMetric: 48.4380, val_loss: 48.0564, val_MinusLogProbMetric: 48.0564

Epoch 209: val_loss did not improve from 47.49908
196/196 - 35s - loss: 48.4380 - MinusLogProbMetric: 48.4380 - val_loss: 48.0564 - val_MinusLogProbMetric: 48.0564 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 210/1000
2023-10-28 19:39:36.379 
Epoch 210/1000 
	 loss: 47.6547, MinusLogProbMetric: 47.6547, val_loss: 48.8436, val_MinusLogProbMetric: 48.8436

Epoch 210: val_loss did not improve from 47.49908
196/196 - 34s - loss: 47.6547 - MinusLogProbMetric: 47.6547 - val_loss: 48.8436 - val_MinusLogProbMetric: 48.8436 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 211/1000
2023-10-28 19:40:10.598 
Epoch 211/1000 
	 loss: 48.2236, MinusLogProbMetric: 48.2236, val_loss: 49.2218, val_MinusLogProbMetric: 49.2218

Epoch 211: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.2236 - MinusLogProbMetric: 48.2236 - val_loss: 49.2218 - val_MinusLogProbMetric: 49.2218 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 212/1000
2023-10-28 19:40:44.929 
Epoch 212/1000 
	 loss: 47.8405, MinusLogProbMetric: 47.8405, val_loss: 48.3009, val_MinusLogProbMetric: 48.3009

Epoch 212: val_loss did not improve from 47.49908
196/196 - 34s - loss: 47.8405 - MinusLogProbMetric: 47.8405 - val_loss: 48.3009 - val_MinusLogProbMetric: 48.3009 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 213/1000
2023-10-28 19:41:18.651 
Epoch 213/1000 
	 loss: 48.1481, MinusLogProbMetric: 48.1481, val_loss: 48.3001, val_MinusLogProbMetric: 48.3001

Epoch 213: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.1481 - MinusLogProbMetric: 48.1481 - val_loss: 48.3001 - val_MinusLogProbMetric: 48.3001 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 214/1000
2023-10-28 19:41:52.744 
Epoch 214/1000 
	 loss: 47.8524, MinusLogProbMetric: 47.8524, val_loss: 48.6837, val_MinusLogProbMetric: 48.6837

Epoch 214: val_loss did not improve from 47.49908
196/196 - 34s - loss: 47.8524 - MinusLogProbMetric: 47.8524 - val_loss: 48.6837 - val_MinusLogProbMetric: 48.6837 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 215/1000
2023-10-28 19:42:26.827 
Epoch 215/1000 
	 loss: 48.0454, MinusLogProbMetric: 48.0454, val_loss: 50.4632, val_MinusLogProbMetric: 50.4632

Epoch 215: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.0454 - MinusLogProbMetric: 48.0454 - val_loss: 50.4632 - val_MinusLogProbMetric: 50.4632 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 216/1000
2023-10-28 19:43:01.327 
Epoch 216/1000 
	 loss: 47.5845, MinusLogProbMetric: 47.5845, val_loss: 49.0793, val_MinusLogProbMetric: 49.0793

Epoch 216: val_loss did not improve from 47.49908
196/196 - 34s - loss: 47.5845 - MinusLogProbMetric: 47.5845 - val_loss: 49.0793 - val_MinusLogProbMetric: 49.0793 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 217/1000
2023-10-28 19:43:35.106 
Epoch 217/1000 
	 loss: 48.3293, MinusLogProbMetric: 48.3293, val_loss: 50.0008, val_MinusLogProbMetric: 50.0008

Epoch 217: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.3293 - MinusLogProbMetric: 48.3293 - val_loss: 50.0008 - val_MinusLogProbMetric: 50.0008 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 218/1000
2023-10-28 19:44:09.166 
Epoch 218/1000 
	 loss: 48.5526, MinusLogProbMetric: 48.5526, val_loss: 48.2129, val_MinusLogProbMetric: 48.2129

Epoch 218: val_loss did not improve from 47.49908
196/196 - 34s - loss: 48.5526 - MinusLogProbMetric: 48.5526 - val_loss: 48.2129 - val_MinusLogProbMetric: 48.2129 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 219/1000
2023-10-28 19:44:43.092 
Epoch 219/1000 
	 loss: 47.8077, MinusLogProbMetric: 47.8077, val_loss: 48.9334, val_MinusLogProbMetric: 48.9334

Epoch 219: val_loss did not improve from 47.49908
196/196 - 34s - loss: 47.8077 - MinusLogProbMetric: 47.8077 - val_loss: 48.9334 - val_MinusLogProbMetric: 48.9334 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 220/1000
2023-10-28 19:45:16.783 
Epoch 220/1000 
	 loss: 47.6600, MinusLogProbMetric: 47.6600, val_loss: 47.0787, val_MinusLogProbMetric: 47.0787

Epoch 220: val_loss improved from 47.49908 to 47.07871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 47.6600 - MinusLogProbMetric: 47.6600 - val_loss: 47.0787 - val_MinusLogProbMetric: 47.0787 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 221/1000
2023-10-28 19:45:50.474 
Epoch 221/1000 
	 loss: 47.8526, MinusLogProbMetric: 47.8526, val_loss: 48.5211, val_MinusLogProbMetric: 48.5211

Epoch 221: val_loss did not improve from 47.07871
196/196 - 33s - loss: 47.8526 - MinusLogProbMetric: 47.8526 - val_loss: 48.5211 - val_MinusLogProbMetric: 48.5211 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 222/1000
2023-10-28 19:46:24.154 
Epoch 222/1000 
	 loss: 47.7141, MinusLogProbMetric: 47.7141, val_loss: 49.4328, val_MinusLogProbMetric: 49.4328

Epoch 222: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.7141 - MinusLogProbMetric: 47.7141 - val_loss: 49.4328 - val_MinusLogProbMetric: 49.4328 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 223/1000
2023-10-28 19:46:57.564 
Epoch 223/1000 
	 loss: 47.8458, MinusLogProbMetric: 47.8458, val_loss: 48.6462, val_MinusLogProbMetric: 48.6462

Epoch 223: val_loss did not improve from 47.07871
196/196 - 33s - loss: 47.8458 - MinusLogProbMetric: 47.8458 - val_loss: 48.6462 - val_MinusLogProbMetric: 48.6462 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 224/1000
2023-10-28 19:47:31.381 
Epoch 224/1000 
	 loss: 48.0046, MinusLogProbMetric: 48.0046, val_loss: 48.5620, val_MinusLogProbMetric: 48.5620

Epoch 224: val_loss did not improve from 47.07871
196/196 - 34s - loss: 48.0046 - MinusLogProbMetric: 48.0046 - val_loss: 48.5620 - val_MinusLogProbMetric: 48.5620 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 225/1000
2023-10-28 19:48:05.517 
Epoch 225/1000 
	 loss: 47.6951, MinusLogProbMetric: 47.6951, val_loss: 48.8816, val_MinusLogProbMetric: 48.8816

Epoch 225: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.6951 - MinusLogProbMetric: 47.6951 - val_loss: 48.8816 - val_MinusLogProbMetric: 48.8816 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 226/1000
2023-10-28 19:48:39.771 
Epoch 226/1000 
	 loss: 47.8674, MinusLogProbMetric: 47.8674, val_loss: 48.4698, val_MinusLogProbMetric: 48.4698

Epoch 226: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.8674 - MinusLogProbMetric: 47.8674 - val_loss: 48.4698 - val_MinusLogProbMetric: 48.4698 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 227/1000
2023-10-28 19:49:13.487 
Epoch 227/1000 
	 loss: 47.7946, MinusLogProbMetric: 47.7946, val_loss: 48.4450, val_MinusLogProbMetric: 48.4450

Epoch 227: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.7946 - MinusLogProbMetric: 47.7946 - val_loss: 48.4450 - val_MinusLogProbMetric: 48.4450 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 228/1000
2023-10-28 19:49:47.628 
Epoch 228/1000 
	 loss: 47.5739, MinusLogProbMetric: 47.5739, val_loss: 49.3601, val_MinusLogProbMetric: 49.3601

Epoch 228: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5739 - MinusLogProbMetric: 47.5739 - val_loss: 49.3601 - val_MinusLogProbMetric: 49.3601 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 229/1000
2023-10-28 19:50:21.694 
Epoch 229/1000 
	 loss: 47.5464, MinusLogProbMetric: 47.5464, val_loss: 48.3186, val_MinusLogProbMetric: 48.3186

Epoch 229: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5464 - MinusLogProbMetric: 47.5464 - val_loss: 48.3186 - val_MinusLogProbMetric: 48.3186 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 230/1000
2023-10-28 19:50:55.263 
Epoch 230/1000 
	 loss: 48.0992, MinusLogProbMetric: 48.0992, val_loss: 50.3977, val_MinusLogProbMetric: 50.3977

Epoch 230: val_loss did not improve from 47.07871
196/196 - 34s - loss: 48.0992 - MinusLogProbMetric: 48.0992 - val_loss: 50.3977 - val_MinusLogProbMetric: 50.3977 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 231/1000
2023-10-28 19:51:29.214 
Epoch 231/1000 
	 loss: 47.8817, MinusLogProbMetric: 47.8817, val_loss: 47.9537, val_MinusLogProbMetric: 47.9537

Epoch 231: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.8817 - MinusLogProbMetric: 47.8817 - val_loss: 47.9537 - val_MinusLogProbMetric: 47.9537 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 232/1000
2023-10-28 19:52:02.743 
Epoch 232/1000 
	 loss: 47.5690, MinusLogProbMetric: 47.5690, val_loss: 48.0238, val_MinusLogProbMetric: 48.0238

Epoch 232: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5690 - MinusLogProbMetric: 47.5690 - val_loss: 48.0238 - val_MinusLogProbMetric: 48.0238 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 233/1000
2023-10-28 19:52:36.667 
Epoch 233/1000 
	 loss: 47.5846, MinusLogProbMetric: 47.5846, val_loss: 48.0151, val_MinusLogProbMetric: 48.0151

Epoch 233: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5846 - MinusLogProbMetric: 47.5846 - val_loss: 48.0151 - val_MinusLogProbMetric: 48.0151 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 234/1000
2023-10-28 19:53:10.839 
Epoch 234/1000 
	 loss: 47.6758, MinusLogProbMetric: 47.6758, val_loss: 49.4174, val_MinusLogProbMetric: 49.4174

Epoch 234: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.6758 - MinusLogProbMetric: 47.6758 - val_loss: 49.4174 - val_MinusLogProbMetric: 49.4174 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 235/1000
2023-10-28 19:53:43.894 
Epoch 235/1000 
	 loss: 47.4897, MinusLogProbMetric: 47.4897, val_loss: 47.4192, val_MinusLogProbMetric: 47.4192

Epoch 235: val_loss did not improve from 47.07871
196/196 - 33s - loss: 47.4897 - MinusLogProbMetric: 47.4897 - val_loss: 47.4192 - val_MinusLogProbMetric: 47.4192 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 236/1000
2023-10-28 19:54:17.666 
Epoch 236/1000 
	 loss: 47.5974, MinusLogProbMetric: 47.5974, val_loss: 48.0771, val_MinusLogProbMetric: 48.0771

Epoch 236: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5974 - MinusLogProbMetric: 47.5974 - val_loss: 48.0771 - val_MinusLogProbMetric: 48.0771 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 237/1000
2023-10-28 19:54:51.500 
Epoch 237/1000 
	 loss: 47.4707, MinusLogProbMetric: 47.4707, val_loss: 48.8751, val_MinusLogProbMetric: 48.8751

Epoch 237: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.4707 - MinusLogProbMetric: 47.4707 - val_loss: 48.8751 - val_MinusLogProbMetric: 48.8751 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 238/1000
2023-10-28 19:55:24.512 
Epoch 238/1000 
	 loss: 47.6051, MinusLogProbMetric: 47.6051, val_loss: 47.2170, val_MinusLogProbMetric: 47.2170

Epoch 238: val_loss did not improve from 47.07871
196/196 - 33s - loss: 47.6051 - MinusLogProbMetric: 47.6051 - val_loss: 47.2170 - val_MinusLogProbMetric: 47.2170 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 239/1000
2023-10-28 19:55:58.056 
Epoch 239/1000 
	 loss: 47.5895, MinusLogProbMetric: 47.5895, val_loss: 47.3061, val_MinusLogProbMetric: 47.3061

Epoch 239: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5895 - MinusLogProbMetric: 47.5895 - val_loss: 47.3061 - val_MinusLogProbMetric: 47.3061 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 240/1000
2023-10-28 19:56:30.896 
Epoch 240/1000 
	 loss: 47.6863, MinusLogProbMetric: 47.6863, val_loss: 48.0717, val_MinusLogProbMetric: 48.0717

Epoch 240: val_loss did not improve from 47.07871
196/196 - 33s - loss: 47.6863 - MinusLogProbMetric: 47.6863 - val_loss: 48.0717 - val_MinusLogProbMetric: 48.0717 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 241/1000
2023-10-28 19:57:04.602 
Epoch 241/1000 
	 loss: 47.6409, MinusLogProbMetric: 47.6409, val_loss: 47.7928, val_MinusLogProbMetric: 47.7928

Epoch 241: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.6409 - MinusLogProbMetric: 47.6409 - val_loss: 47.7928 - val_MinusLogProbMetric: 47.7928 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 242/1000
2023-10-28 19:57:38.637 
Epoch 242/1000 
	 loss: 47.6234, MinusLogProbMetric: 47.6234, val_loss: 49.4905, val_MinusLogProbMetric: 49.4905

Epoch 242: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.6234 - MinusLogProbMetric: 47.6234 - val_loss: 49.4905 - val_MinusLogProbMetric: 49.4905 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 243/1000
2023-10-28 19:58:13.150 
Epoch 243/1000 
	 loss: 47.4896, MinusLogProbMetric: 47.4896, val_loss: 48.6411, val_MinusLogProbMetric: 48.6411

Epoch 243: val_loss did not improve from 47.07871
196/196 - 35s - loss: 47.4896 - MinusLogProbMetric: 47.4896 - val_loss: 48.6411 - val_MinusLogProbMetric: 48.6411 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 244/1000
2023-10-28 19:58:47.311 
Epoch 244/1000 
	 loss: 47.9556, MinusLogProbMetric: 47.9556, val_loss: 48.1771, val_MinusLogProbMetric: 48.1771

Epoch 244: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.9556 - MinusLogProbMetric: 47.9556 - val_loss: 48.1771 - val_MinusLogProbMetric: 48.1771 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 245/1000
2023-10-28 19:59:21.306 
Epoch 245/1000 
	 loss: 47.1838, MinusLogProbMetric: 47.1838, val_loss: 47.9852, val_MinusLogProbMetric: 47.9852

Epoch 245: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.1838 - MinusLogProbMetric: 47.1838 - val_loss: 47.9852 - val_MinusLogProbMetric: 47.9852 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 246/1000
2023-10-28 19:59:55.477 
Epoch 246/1000 
	 loss: 47.5285, MinusLogProbMetric: 47.5285, val_loss: 50.2554, val_MinusLogProbMetric: 50.2554

Epoch 246: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5285 - MinusLogProbMetric: 47.5285 - val_loss: 50.2554 - val_MinusLogProbMetric: 50.2554 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 247/1000
2023-10-28 20:00:29.417 
Epoch 247/1000 
	 loss: 47.1816, MinusLogProbMetric: 47.1816, val_loss: 47.4781, val_MinusLogProbMetric: 47.4781

Epoch 247: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.1816 - MinusLogProbMetric: 47.1816 - val_loss: 47.4781 - val_MinusLogProbMetric: 47.4781 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 248/1000
2023-10-28 20:01:03.659 
Epoch 248/1000 
	 loss: 47.5753, MinusLogProbMetric: 47.5753, val_loss: 49.2852, val_MinusLogProbMetric: 49.2852

Epoch 248: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5753 - MinusLogProbMetric: 47.5753 - val_loss: 49.2852 - val_MinusLogProbMetric: 49.2852 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 249/1000
2023-10-28 20:01:38.023 
Epoch 249/1000 
	 loss: 47.5356, MinusLogProbMetric: 47.5356, val_loss: 47.9946, val_MinusLogProbMetric: 47.9946

Epoch 249: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.5356 - MinusLogProbMetric: 47.5356 - val_loss: 47.9946 - val_MinusLogProbMetric: 47.9946 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 250/1000
2023-10-28 20:02:12.361 
Epoch 250/1000 
	 loss: 47.3888, MinusLogProbMetric: 47.3888, val_loss: 50.3766, val_MinusLogProbMetric: 50.3766

Epoch 250: val_loss did not improve from 47.07871
196/196 - 34s - loss: 47.3888 - MinusLogProbMetric: 47.3888 - val_loss: 50.3766 - val_MinusLogProbMetric: 50.3766 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 251/1000
2023-10-28 20:02:45.855 
Epoch 251/1000 
	 loss: 47.5766, MinusLogProbMetric: 47.5766, val_loss: 47.5589, val_MinusLogProbMetric: 47.5589

Epoch 251: val_loss did not improve from 47.07871
196/196 - 33s - loss: 47.5766 - MinusLogProbMetric: 47.5766 - val_loss: 47.5589 - val_MinusLogProbMetric: 47.5589 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 252/1000
2023-10-28 20:03:19.125 
Epoch 252/1000 
	 loss: 47.4798, MinusLogProbMetric: 47.4798, val_loss: 46.8730, val_MinusLogProbMetric: 46.8730

Epoch 252: val_loss improved from 47.07871 to 46.87300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 47.4798 - MinusLogProbMetric: 47.4798 - val_loss: 46.8730 - val_MinusLogProbMetric: 46.8730 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 253/1000
2023-10-28 20:03:53.692 
Epoch 253/1000 
	 loss: 47.0241, MinusLogProbMetric: 47.0241, val_loss: 46.9780, val_MinusLogProbMetric: 46.9780

Epoch 253: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.0241 - MinusLogProbMetric: 47.0241 - val_loss: 46.9780 - val_MinusLogProbMetric: 46.9780 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 254/1000
2023-10-28 20:04:28.527 
Epoch 254/1000 
	 loss: 47.3955, MinusLogProbMetric: 47.3955, val_loss: 49.7354, val_MinusLogProbMetric: 49.7354

Epoch 254: val_loss did not improve from 46.87300
196/196 - 35s - loss: 47.3955 - MinusLogProbMetric: 47.3955 - val_loss: 49.7354 - val_MinusLogProbMetric: 49.7354 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 255/1000
2023-10-28 20:05:02.323 
Epoch 255/1000 
	 loss: 47.4368, MinusLogProbMetric: 47.4368, val_loss: 48.4574, val_MinusLogProbMetric: 48.4574

Epoch 255: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.4368 - MinusLogProbMetric: 47.4368 - val_loss: 48.4574 - val_MinusLogProbMetric: 48.4574 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 256/1000
2023-10-28 20:05:35.350 
Epoch 256/1000 
	 loss: 47.4583, MinusLogProbMetric: 47.4583, val_loss: 46.9587, val_MinusLogProbMetric: 46.9587

Epoch 256: val_loss did not improve from 46.87300
196/196 - 33s - loss: 47.4583 - MinusLogProbMetric: 47.4583 - val_loss: 46.9587 - val_MinusLogProbMetric: 46.9587 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 257/1000
2023-10-28 20:06:08.471 
Epoch 257/1000 
	 loss: 47.0490, MinusLogProbMetric: 47.0490, val_loss: 52.5892, val_MinusLogProbMetric: 52.5892

Epoch 257: val_loss did not improve from 46.87300
196/196 - 33s - loss: 47.0490 - MinusLogProbMetric: 47.0490 - val_loss: 52.5892 - val_MinusLogProbMetric: 52.5892 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 258/1000
2023-10-28 20:06:42.011 
Epoch 258/1000 
	 loss: 47.4681, MinusLogProbMetric: 47.4681, val_loss: 48.5714, val_MinusLogProbMetric: 48.5714

Epoch 258: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.4681 - MinusLogProbMetric: 47.4681 - val_loss: 48.5714 - val_MinusLogProbMetric: 48.5714 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 259/1000
2023-10-28 20:07:16.041 
Epoch 259/1000 
	 loss: 47.8878, MinusLogProbMetric: 47.8878, val_loss: 47.7150, val_MinusLogProbMetric: 47.7150

Epoch 259: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.8878 - MinusLogProbMetric: 47.8878 - val_loss: 47.7150 - val_MinusLogProbMetric: 47.7150 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 260/1000
2023-10-28 20:07:50.138 
Epoch 260/1000 
	 loss: 47.1284, MinusLogProbMetric: 47.1284, val_loss: 47.7213, val_MinusLogProbMetric: 47.7213

Epoch 260: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.1284 - MinusLogProbMetric: 47.1284 - val_loss: 47.7213 - val_MinusLogProbMetric: 47.7213 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 261/1000
2023-10-28 20:08:24.139 
Epoch 261/1000 
	 loss: 47.1588, MinusLogProbMetric: 47.1588, val_loss: 48.6076, val_MinusLogProbMetric: 48.6076

Epoch 261: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.1588 - MinusLogProbMetric: 47.1588 - val_loss: 48.6076 - val_MinusLogProbMetric: 48.6076 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 262/1000
2023-10-28 20:08:57.815 
Epoch 262/1000 
	 loss: 47.3518, MinusLogProbMetric: 47.3518, val_loss: 47.0337, val_MinusLogProbMetric: 47.0337

Epoch 262: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.3518 - MinusLogProbMetric: 47.3518 - val_loss: 47.0337 - val_MinusLogProbMetric: 47.0337 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 263/1000
2023-10-28 20:09:31.783 
Epoch 263/1000 
	 loss: 47.3229, MinusLogProbMetric: 47.3229, val_loss: 47.5138, val_MinusLogProbMetric: 47.5138

Epoch 263: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.3229 - MinusLogProbMetric: 47.3229 - val_loss: 47.5138 - val_MinusLogProbMetric: 47.5138 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 264/1000
2023-10-28 20:10:05.832 
Epoch 264/1000 
	 loss: 47.4326, MinusLogProbMetric: 47.4326, val_loss: 48.2359, val_MinusLogProbMetric: 48.2359

Epoch 264: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.4326 - MinusLogProbMetric: 47.4326 - val_loss: 48.2359 - val_MinusLogProbMetric: 48.2359 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 265/1000
2023-10-28 20:10:39.757 
Epoch 265/1000 
	 loss: 47.2280, MinusLogProbMetric: 47.2280, val_loss: 48.2581, val_MinusLogProbMetric: 48.2581

Epoch 265: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.2280 - MinusLogProbMetric: 47.2280 - val_loss: 48.2581 - val_MinusLogProbMetric: 48.2581 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 266/1000
2023-10-28 20:11:13.850 
Epoch 266/1000 
	 loss: 47.6037, MinusLogProbMetric: 47.6037, val_loss: 48.2663, val_MinusLogProbMetric: 48.2663

Epoch 266: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.6037 - MinusLogProbMetric: 47.6037 - val_loss: 48.2663 - val_MinusLogProbMetric: 48.2663 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 267/1000
2023-10-28 20:11:47.974 
Epoch 267/1000 
	 loss: 47.0037, MinusLogProbMetric: 47.0037, val_loss: 48.1307, val_MinusLogProbMetric: 48.1307

Epoch 267: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.0037 - MinusLogProbMetric: 47.0037 - val_loss: 48.1307 - val_MinusLogProbMetric: 48.1307 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 268/1000
2023-10-28 20:12:22.605 
Epoch 268/1000 
	 loss: 47.5006, MinusLogProbMetric: 47.5006, val_loss: 47.3053, val_MinusLogProbMetric: 47.3053

Epoch 268: val_loss did not improve from 46.87300
196/196 - 35s - loss: 47.5006 - MinusLogProbMetric: 47.5006 - val_loss: 47.3053 - val_MinusLogProbMetric: 47.3053 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 269/1000
2023-10-28 20:12:57.209 
Epoch 269/1000 
	 loss: 47.3794, MinusLogProbMetric: 47.3794, val_loss: 48.0540, val_MinusLogProbMetric: 48.0540

Epoch 269: val_loss did not improve from 46.87300
196/196 - 35s - loss: 47.3794 - MinusLogProbMetric: 47.3794 - val_loss: 48.0540 - val_MinusLogProbMetric: 48.0540 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 270/1000
2023-10-28 20:13:30.923 
Epoch 270/1000 
	 loss: 46.7269, MinusLogProbMetric: 46.7269, val_loss: 47.8937, val_MinusLogProbMetric: 47.8937

Epoch 270: val_loss did not improve from 46.87300
196/196 - 34s - loss: 46.7269 - MinusLogProbMetric: 46.7269 - val_loss: 47.8937 - val_MinusLogProbMetric: 47.8937 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 271/1000
2023-10-28 20:14:04.895 
Epoch 271/1000 
	 loss: 47.3477, MinusLogProbMetric: 47.3477, val_loss: 47.5023, val_MinusLogProbMetric: 47.5023

Epoch 271: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.3477 - MinusLogProbMetric: 47.3477 - val_loss: 47.5023 - val_MinusLogProbMetric: 47.5023 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 272/1000
2023-10-28 20:14:38.422 
Epoch 272/1000 
	 loss: 47.0937, MinusLogProbMetric: 47.0937, val_loss: 48.7265, val_MinusLogProbMetric: 48.7265

Epoch 272: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.0937 - MinusLogProbMetric: 47.0937 - val_loss: 48.7265 - val_MinusLogProbMetric: 48.7265 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 273/1000
2023-10-28 20:15:12.357 
Epoch 273/1000 
	 loss: 46.8282, MinusLogProbMetric: 46.8282, val_loss: 47.8387, val_MinusLogProbMetric: 47.8387

Epoch 273: val_loss did not improve from 46.87300
196/196 - 34s - loss: 46.8282 - MinusLogProbMetric: 46.8282 - val_loss: 47.8387 - val_MinusLogProbMetric: 47.8387 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 274/1000
2023-10-28 20:15:46.370 
Epoch 274/1000 
	 loss: 47.1790, MinusLogProbMetric: 47.1790, val_loss: 52.9920, val_MinusLogProbMetric: 52.9920

Epoch 274: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.1790 - MinusLogProbMetric: 47.1790 - val_loss: 52.9920 - val_MinusLogProbMetric: 52.9920 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 275/1000
2023-10-28 20:16:20.636 
Epoch 275/1000 
	 loss: 47.2343, MinusLogProbMetric: 47.2343, val_loss: 47.6929, val_MinusLogProbMetric: 47.6929

Epoch 275: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.2343 - MinusLogProbMetric: 47.2343 - val_loss: 47.6929 - val_MinusLogProbMetric: 47.6929 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 276/1000
2023-10-28 20:16:54.615 
Epoch 276/1000 
	 loss: 47.1461, MinusLogProbMetric: 47.1461, val_loss: 48.1779, val_MinusLogProbMetric: 48.1779

Epoch 276: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.1461 - MinusLogProbMetric: 47.1461 - val_loss: 48.1779 - val_MinusLogProbMetric: 48.1779 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 277/1000
2023-10-28 20:17:28.645 
Epoch 277/1000 
	 loss: 46.9178, MinusLogProbMetric: 46.9178, val_loss: 47.3039, val_MinusLogProbMetric: 47.3039

Epoch 277: val_loss did not improve from 46.87300
196/196 - 34s - loss: 46.9178 - MinusLogProbMetric: 46.9178 - val_loss: 47.3039 - val_MinusLogProbMetric: 47.3039 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 278/1000
2023-10-28 20:18:02.678 
Epoch 278/1000 
	 loss: 46.8176, MinusLogProbMetric: 46.8176, val_loss: 48.3612, val_MinusLogProbMetric: 48.3612

Epoch 278: val_loss did not improve from 46.87300
196/196 - 34s - loss: 46.8176 - MinusLogProbMetric: 46.8176 - val_loss: 48.3612 - val_MinusLogProbMetric: 48.3612 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 279/1000
2023-10-28 20:18:36.881 
Epoch 279/1000 
	 loss: 47.2906, MinusLogProbMetric: 47.2906, val_loss: 47.9253, val_MinusLogProbMetric: 47.9253

Epoch 279: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.2906 - MinusLogProbMetric: 47.2906 - val_loss: 47.9253 - val_MinusLogProbMetric: 47.9253 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 280/1000
2023-10-28 20:19:11.808 
Epoch 280/1000 
	 loss: 47.3518, MinusLogProbMetric: 47.3518, val_loss: 47.6708, val_MinusLogProbMetric: 47.6708

Epoch 280: val_loss did not improve from 46.87300
196/196 - 35s - loss: 47.3518 - MinusLogProbMetric: 47.3518 - val_loss: 47.6708 - val_MinusLogProbMetric: 47.6708 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 281/1000
2023-10-28 20:19:46.464 
Epoch 281/1000 
	 loss: 46.6591, MinusLogProbMetric: 46.6591, val_loss: 47.3443, val_MinusLogProbMetric: 47.3443

Epoch 281: val_loss did not improve from 46.87300
196/196 - 35s - loss: 46.6591 - MinusLogProbMetric: 46.6591 - val_loss: 47.3443 - val_MinusLogProbMetric: 47.3443 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 282/1000
2023-10-28 20:20:20.270 
Epoch 282/1000 
	 loss: 46.9845, MinusLogProbMetric: 46.9845, val_loss: 48.8407, val_MinusLogProbMetric: 48.8407

Epoch 282: val_loss did not improve from 46.87300
196/196 - 34s - loss: 46.9845 - MinusLogProbMetric: 46.9845 - val_loss: 48.8407 - val_MinusLogProbMetric: 48.8407 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 283/1000
2023-10-28 20:20:54.136 
Epoch 283/1000 
	 loss: 47.0937, MinusLogProbMetric: 47.0937, val_loss: 47.5919, val_MinusLogProbMetric: 47.5919

Epoch 283: val_loss did not improve from 46.87300
196/196 - 34s - loss: 47.0937 - MinusLogProbMetric: 47.0937 - val_loss: 47.5919 - val_MinusLogProbMetric: 47.5919 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 284/1000
2023-10-28 20:21:27.945 
Epoch 284/1000 
	 loss: 46.9158, MinusLogProbMetric: 46.9158, val_loss: 46.1944, val_MinusLogProbMetric: 46.1944

Epoch 284: val_loss improved from 46.87300 to 46.19440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 46.9158 - MinusLogProbMetric: 46.9158 - val_loss: 46.1944 - val_MinusLogProbMetric: 46.1944 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 285/1000
2023-10-28 20:22:02.844 
Epoch 285/1000 
	 loss: 46.7917, MinusLogProbMetric: 46.7917, val_loss: 47.8980, val_MinusLogProbMetric: 47.8980

Epoch 285: val_loss did not improve from 46.19440
196/196 - 34s - loss: 46.7917 - MinusLogProbMetric: 46.7917 - val_loss: 47.8980 - val_MinusLogProbMetric: 47.8980 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 286/1000
2023-10-28 20:22:37.183 
Epoch 286/1000 
	 loss: 46.8381, MinusLogProbMetric: 46.8381, val_loss: 47.1533, val_MinusLogProbMetric: 47.1533

Epoch 286: val_loss did not improve from 46.19440
196/196 - 34s - loss: 46.8381 - MinusLogProbMetric: 46.8381 - val_loss: 47.1533 - val_MinusLogProbMetric: 47.1533 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 287/1000
2023-10-28 20:23:11.857 
Epoch 287/1000 
	 loss: 46.8364, MinusLogProbMetric: 46.8364, val_loss: 47.4499, val_MinusLogProbMetric: 47.4499

Epoch 287: val_loss did not improve from 46.19440
196/196 - 35s - loss: 46.8364 - MinusLogProbMetric: 46.8364 - val_loss: 47.4499 - val_MinusLogProbMetric: 47.4499 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 288/1000
2023-10-28 20:23:45.848 
Epoch 288/1000 
	 loss: 47.0681, MinusLogProbMetric: 47.0681, val_loss: 47.1148, val_MinusLogProbMetric: 47.1148

Epoch 288: val_loss did not improve from 46.19440
196/196 - 34s - loss: 47.0681 - MinusLogProbMetric: 47.0681 - val_loss: 47.1148 - val_MinusLogProbMetric: 47.1148 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 289/1000
2023-10-28 20:24:19.761 
Epoch 289/1000 
	 loss: 46.7561, MinusLogProbMetric: 46.7561, val_loss: 47.2536, val_MinusLogProbMetric: 47.2536

Epoch 289: val_loss did not improve from 46.19440
196/196 - 34s - loss: 46.7561 - MinusLogProbMetric: 46.7561 - val_loss: 47.2536 - val_MinusLogProbMetric: 47.2536 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 290/1000
2023-10-28 20:24:53.621 
Epoch 290/1000 
	 loss: 46.8125, MinusLogProbMetric: 46.8125, val_loss: 46.8478, val_MinusLogProbMetric: 46.8478

Epoch 290: val_loss did not improve from 46.19440
196/196 - 34s - loss: 46.8125 - MinusLogProbMetric: 46.8125 - val_loss: 46.8478 - val_MinusLogProbMetric: 46.8478 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 291/1000
2023-10-28 20:25:27.546 
Epoch 291/1000 
	 loss: 46.7844, MinusLogProbMetric: 46.7844, val_loss: 47.7215, val_MinusLogProbMetric: 47.7215

Epoch 291: val_loss did not improve from 46.19440
196/196 - 34s - loss: 46.7844 - MinusLogProbMetric: 46.7844 - val_loss: 47.7215 - val_MinusLogProbMetric: 47.7215 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 292/1000
2023-10-28 20:26:01.747 
Epoch 292/1000 
	 loss: 46.7344, MinusLogProbMetric: 46.7344, val_loss: 48.7915, val_MinusLogProbMetric: 48.7915

Epoch 292: val_loss did not improve from 46.19440
196/196 - 34s - loss: 46.7344 - MinusLogProbMetric: 46.7344 - val_loss: 48.7915 - val_MinusLogProbMetric: 48.7915 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 293/1000
2023-10-28 20:26:36.354 
Epoch 293/1000 
	 loss: 47.4338, MinusLogProbMetric: 47.4338, val_loss: 48.3850, val_MinusLogProbMetric: 48.3850

Epoch 293: val_loss did not improve from 46.19440
196/196 - 35s - loss: 47.4338 - MinusLogProbMetric: 47.4338 - val_loss: 48.3850 - val_MinusLogProbMetric: 48.3850 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 294/1000
2023-10-28 20:27:10.158 
Epoch 294/1000 
	 loss: 46.7547, MinusLogProbMetric: 46.7547, val_loss: 49.3048, val_MinusLogProbMetric: 49.3048

Epoch 294: val_loss did not improve from 46.19440
196/196 - 34s - loss: 46.7547 - MinusLogProbMetric: 46.7547 - val_loss: 49.3048 - val_MinusLogProbMetric: 49.3048 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 295/1000
2023-10-28 20:27:43.789 
Epoch 295/1000 
	 loss: 46.7873, MinusLogProbMetric: 46.7873, val_loss: 46.6060, val_MinusLogProbMetric: 46.6060

Epoch 295: val_loss did not improve from 46.19440
196/196 - 34s - loss: 46.7873 - MinusLogProbMetric: 46.7873 - val_loss: 46.6060 - val_MinusLogProbMetric: 46.6060 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 296/1000
2023-10-28 20:28:17.150 
Epoch 296/1000 
	 loss: 46.6121, MinusLogProbMetric: 46.6121, val_loss: 46.6621, val_MinusLogProbMetric: 46.6621

Epoch 296: val_loss did not improve from 46.19440
196/196 - 33s - loss: 46.6121 - MinusLogProbMetric: 46.6121 - val_loss: 46.6621 - val_MinusLogProbMetric: 46.6621 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 297/1000
2023-10-28 20:28:51.074 
Epoch 297/1000 
	 loss: 47.1861, MinusLogProbMetric: 47.1861, val_loss: 46.1123, val_MinusLogProbMetric: 46.1123

Epoch 297: val_loss improved from 46.19440 to 46.11231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 47.1861 - MinusLogProbMetric: 47.1861 - val_loss: 46.1123 - val_MinusLogProbMetric: 46.1123 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 298/1000
2023-10-28 20:29:25.301 
Epoch 298/1000 
	 loss: 46.6092, MinusLogProbMetric: 46.6092, val_loss: 47.1340, val_MinusLogProbMetric: 47.1340

Epoch 298: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.6092 - MinusLogProbMetric: 46.6092 - val_loss: 47.1340 - val_MinusLogProbMetric: 47.1340 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 299/1000
2023-10-28 20:30:00.001 
Epoch 299/1000 
	 loss: 46.7090, MinusLogProbMetric: 46.7090, val_loss: 48.3678, val_MinusLogProbMetric: 48.3678

Epoch 299: val_loss did not improve from 46.11231
196/196 - 35s - loss: 46.7090 - MinusLogProbMetric: 46.7090 - val_loss: 48.3678 - val_MinusLogProbMetric: 48.3678 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 300/1000
2023-10-28 20:30:35.056 
Epoch 300/1000 
	 loss: 46.6663, MinusLogProbMetric: 46.6663, val_loss: 47.1244, val_MinusLogProbMetric: 47.1244

Epoch 300: val_loss did not improve from 46.11231
196/196 - 35s - loss: 46.6663 - MinusLogProbMetric: 46.6663 - val_loss: 47.1244 - val_MinusLogProbMetric: 47.1244 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 301/1000
2023-10-28 20:31:09.921 
Epoch 301/1000 
	 loss: 46.5788, MinusLogProbMetric: 46.5788, val_loss: 48.3551, val_MinusLogProbMetric: 48.3551

Epoch 301: val_loss did not improve from 46.11231
196/196 - 35s - loss: 46.5788 - MinusLogProbMetric: 46.5788 - val_loss: 48.3551 - val_MinusLogProbMetric: 48.3551 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 302/1000
2023-10-28 20:31:44.192 
Epoch 302/1000 
	 loss: 48.1502, MinusLogProbMetric: 48.1502, val_loss: 47.8200, val_MinusLogProbMetric: 47.8200

Epoch 302: val_loss did not improve from 46.11231
196/196 - 34s - loss: 48.1502 - MinusLogProbMetric: 48.1502 - val_loss: 47.8200 - val_MinusLogProbMetric: 47.8200 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 303/1000
2023-10-28 20:32:18.179 
Epoch 303/1000 
	 loss: 46.4793, MinusLogProbMetric: 46.4793, val_loss: 46.9319, val_MinusLogProbMetric: 46.9319

Epoch 303: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.4793 - MinusLogProbMetric: 46.4793 - val_loss: 46.9319 - val_MinusLogProbMetric: 46.9319 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 304/1000
2023-10-28 20:32:51.945 
Epoch 304/1000 
	 loss: 46.9508, MinusLogProbMetric: 46.9508, val_loss: 48.3318, val_MinusLogProbMetric: 48.3318

Epoch 304: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.9508 - MinusLogProbMetric: 46.9508 - val_loss: 48.3318 - val_MinusLogProbMetric: 48.3318 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 305/1000
2023-10-28 20:33:25.888 
Epoch 305/1000 
	 loss: 46.5322, MinusLogProbMetric: 46.5322, val_loss: 47.6456, val_MinusLogProbMetric: 47.6456

Epoch 305: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.5322 - MinusLogProbMetric: 46.5322 - val_loss: 47.6456 - val_MinusLogProbMetric: 47.6456 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 306/1000
2023-10-28 20:34:00.143 
Epoch 306/1000 
	 loss: 46.8255, MinusLogProbMetric: 46.8255, val_loss: 46.2761, val_MinusLogProbMetric: 46.2761

Epoch 306: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.8255 - MinusLogProbMetric: 46.8255 - val_loss: 46.2761 - val_MinusLogProbMetric: 46.2761 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 307/1000
2023-10-28 20:34:33.713 
Epoch 307/1000 
	 loss: 47.1632, MinusLogProbMetric: 47.1632, val_loss: 48.4460, val_MinusLogProbMetric: 48.4460

Epoch 307: val_loss did not improve from 46.11231
196/196 - 34s - loss: 47.1632 - MinusLogProbMetric: 47.1632 - val_loss: 48.4460 - val_MinusLogProbMetric: 48.4460 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 308/1000
2023-10-28 20:35:07.127 
Epoch 308/1000 
	 loss: 46.7946, MinusLogProbMetric: 46.7946, val_loss: 51.5195, val_MinusLogProbMetric: 51.5195

Epoch 308: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.7946 - MinusLogProbMetric: 46.7946 - val_loss: 51.5195 - val_MinusLogProbMetric: 51.5195 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 309/1000
2023-10-28 20:35:41.223 
Epoch 309/1000 
	 loss: 46.7846, MinusLogProbMetric: 46.7846, val_loss: 47.2098, val_MinusLogProbMetric: 47.2098

Epoch 309: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.7846 - MinusLogProbMetric: 46.7846 - val_loss: 47.2098 - val_MinusLogProbMetric: 47.2098 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 310/1000
2023-10-28 20:36:14.964 
Epoch 310/1000 
	 loss: 46.4647, MinusLogProbMetric: 46.4647, val_loss: 46.3321, val_MinusLogProbMetric: 46.3321

Epoch 310: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.4647 - MinusLogProbMetric: 46.4647 - val_loss: 46.3321 - val_MinusLogProbMetric: 46.3321 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 311/1000
2023-10-28 20:36:48.384 
Epoch 311/1000 
	 loss: 46.9789, MinusLogProbMetric: 46.9789, val_loss: 49.3752, val_MinusLogProbMetric: 49.3752

Epoch 311: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.9789 - MinusLogProbMetric: 46.9789 - val_loss: 49.3752 - val_MinusLogProbMetric: 49.3752 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 312/1000
2023-10-28 20:37:22.169 
Epoch 312/1000 
	 loss: 46.6254, MinusLogProbMetric: 46.6254, val_loss: 48.4297, val_MinusLogProbMetric: 48.4297

Epoch 312: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.6254 - MinusLogProbMetric: 46.6254 - val_loss: 48.4297 - val_MinusLogProbMetric: 48.4297 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 313/1000
2023-10-28 20:37:56.007 
Epoch 313/1000 
	 loss: 46.5570, MinusLogProbMetric: 46.5570, val_loss: 48.2494, val_MinusLogProbMetric: 48.2494

Epoch 313: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.5570 - MinusLogProbMetric: 46.5570 - val_loss: 48.2494 - val_MinusLogProbMetric: 48.2494 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 314/1000
2023-10-28 20:38:29.283 
Epoch 314/1000 
	 loss: 46.9120, MinusLogProbMetric: 46.9120, val_loss: 47.7524, val_MinusLogProbMetric: 47.7524

Epoch 314: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.9120 - MinusLogProbMetric: 46.9120 - val_loss: 47.7524 - val_MinusLogProbMetric: 47.7524 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 315/1000
2023-10-28 20:39:02.417 
Epoch 315/1000 
	 loss: 46.3250, MinusLogProbMetric: 46.3250, val_loss: 50.7689, val_MinusLogProbMetric: 50.7689

Epoch 315: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.3250 - MinusLogProbMetric: 46.3250 - val_loss: 50.7689 - val_MinusLogProbMetric: 50.7689 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 316/1000
2023-10-28 20:39:35.946 
Epoch 316/1000 
	 loss: 46.7798, MinusLogProbMetric: 46.7798, val_loss: 46.4770, val_MinusLogProbMetric: 46.4770

Epoch 316: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.7798 - MinusLogProbMetric: 46.7798 - val_loss: 46.4770 - val_MinusLogProbMetric: 46.4770 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 317/1000
2023-10-28 20:40:09.484 
Epoch 317/1000 
	 loss: 46.5256, MinusLogProbMetric: 46.5256, val_loss: 48.2573, val_MinusLogProbMetric: 48.2573

Epoch 317: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.5256 - MinusLogProbMetric: 46.5256 - val_loss: 48.2573 - val_MinusLogProbMetric: 48.2573 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 318/1000
2023-10-28 20:40:43.779 
Epoch 318/1000 
	 loss: 46.5843, MinusLogProbMetric: 46.5843, val_loss: 47.2609, val_MinusLogProbMetric: 47.2609

Epoch 318: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.5843 - MinusLogProbMetric: 46.5843 - val_loss: 47.2609 - val_MinusLogProbMetric: 47.2609 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 319/1000
2023-10-28 20:41:17.108 
Epoch 319/1000 
	 loss: 46.4100, MinusLogProbMetric: 46.4100, val_loss: 47.5851, val_MinusLogProbMetric: 47.5851

Epoch 319: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.4100 - MinusLogProbMetric: 46.4100 - val_loss: 47.5851 - val_MinusLogProbMetric: 47.5851 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 320/1000
2023-10-28 20:41:50.298 
Epoch 320/1000 
	 loss: 46.6590, MinusLogProbMetric: 46.6590, val_loss: 46.6741, val_MinusLogProbMetric: 46.6741

Epoch 320: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.6590 - MinusLogProbMetric: 46.6590 - val_loss: 46.6741 - val_MinusLogProbMetric: 46.6741 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 321/1000
2023-10-28 20:42:23.655 
Epoch 321/1000 
	 loss: 46.6512, MinusLogProbMetric: 46.6512, val_loss: 46.4273, val_MinusLogProbMetric: 46.4273

Epoch 321: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.6512 - MinusLogProbMetric: 46.6512 - val_loss: 46.4273 - val_MinusLogProbMetric: 46.4273 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 322/1000
2023-10-28 20:42:56.975 
Epoch 322/1000 
	 loss: 46.8398, MinusLogProbMetric: 46.8398, val_loss: 46.6043, val_MinusLogProbMetric: 46.6043

Epoch 322: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.8398 - MinusLogProbMetric: 46.8398 - val_loss: 46.6043 - val_MinusLogProbMetric: 46.6043 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 323/1000
2023-10-28 20:43:30.650 
Epoch 323/1000 
	 loss: 46.8500, MinusLogProbMetric: 46.8500, val_loss: 47.1756, val_MinusLogProbMetric: 47.1756

Epoch 323: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.8500 - MinusLogProbMetric: 46.8500 - val_loss: 47.1756 - val_MinusLogProbMetric: 47.1756 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 324/1000
2023-10-28 20:44:04.380 
Epoch 324/1000 
	 loss: 46.8772, MinusLogProbMetric: 46.8772, val_loss: 49.6261, val_MinusLogProbMetric: 49.6261

Epoch 324: val_loss did not improve from 46.11231
196/196 - 34s - loss: 46.8772 - MinusLogProbMetric: 46.8772 - val_loss: 49.6261 - val_MinusLogProbMetric: 49.6261 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 325/1000
2023-10-28 20:44:37.761 
Epoch 325/1000 
	 loss: 46.6204, MinusLogProbMetric: 46.6204, val_loss: 46.9440, val_MinusLogProbMetric: 46.9440

Epoch 325: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.6204 - MinusLogProbMetric: 46.6204 - val_loss: 46.9440 - val_MinusLogProbMetric: 46.9440 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 326/1000
2023-10-28 20:45:10.845 
Epoch 326/1000 
	 loss: 46.4775, MinusLogProbMetric: 46.4775, val_loss: 49.0144, val_MinusLogProbMetric: 49.0144

Epoch 326: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.4775 - MinusLogProbMetric: 46.4775 - val_loss: 49.0144 - val_MinusLogProbMetric: 49.0144 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 327/1000
2023-10-28 20:45:43.710 
Epoch 327/1000 
	 loss: 46.5450, MinusLogProbMetric: 46.5450, val_loss: 46.9494, val_MinusLogProbMetric: 46.9494

Epoch 327: val_loss did not improve from 46.11231
196/196 - 33s - loss: 46.5450 - MinusLogProbMetric: 46.5450 - val_loss: 46.9494 - val_MinusLogProbMetric: 46.9494 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 328/1000
2023-10-28 20:46:16.646 
Epoch 328/1000 
	 loss: 46.4375, MinusLogProbMetric: 46.4375, val_loss: 45.9301, val_MinusLogProbMetric: 45.9301

Epoch 328: val_loss improved from 46.11231 to 45.93005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 33s - loss: 46.4375 - MinusLogProbMetric: 46.4375 - val_loss: 45.9301 - val_MinusLogProbMetric: 45.9301 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 329/1000
2023-10-28 20:46:50.291 
Epoch 329/1000 
	 loss: 46.5901, MinusLogProbMetric: 46.5901, val_loss: 47.4749, val_MinusLogProbMetric: 47.4749

Epoch 329: val_loss did not improve from 45.93005
196/196 - 33s - loss: 46.5901 - MinusLogProbMetric: 46.5901 - val_loss: 47.4749 - val_MinusLogProbMetric: 47.4749 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 330/1000
2023-10-28 20:47:24.014 
Epoch 330/1000 
	 loss: 46.3617, MinusLogProbMetric: 46.3617, val_loss: 47.3655, val_MinusLogProbMetric: 47.3655

Epoch 330: val_loss did not improve from 45.93005
196/196 - 34s - loss: 46.3617 - MinusLogProbMetric: 46.3617 - val_loss: 47.3655 - val_MinusLogProbMetric: 47.3655 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 331/1000
2023-10-28 20:47:57.742 
Epoch 331/1000 
	 loss: 46.2430, MinusLogProbMetric: 46.2430, val_loss: 46.3584, val_MinusLogProbMetric: 46.3584

Epoch 331: val_loss did not improve from 45.93005
196/196 - 34s - loss: 46.2430 - MinusLogProbMetric: 46.2430 - val_loss: 46.3584 - val_MinusLogProbMetric: 46.3584 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 332/1000
2023-10-28 20:48:31.485 
Epoch 332/1000 
	 loss: 47.0222, MinusLogProbMetric: 47.0222, val_loss: 46.7756, val_MinusLogProbMetric: 46.7756

Epoch 332: val_loss did not improve from 45.93005
196/196 - 34s - loss: 47.0222 - MinusLogProbMetric: 47.0222 - val_loss: 46.7756 - val_MinusLogProbMetric: 46.7756 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 333/1000
2023-10-28 20:49:05.078 
Epoch 333/1000 
	 loss: 46.3036, MinusLogProbMetric: 46.3036, val_loss: 46.4945, val_MinusLogProbMetric: 46.4945

Epoch 333: val_loss did not improve from 45.93005
196/196 - 34s - loss: 46.3036 - MinusLogProbMetric: 46.3036 - val_loss: 46.4945 - val_MinusLogProbMetric: 46.4945 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 334/1000
2023-10-28 20:49:39.154 
Epoch 334/1000 
	 loss: 46.5919, MinusLogProbMetric: 46.5919, val_loss: 47.5378, val_MinusLogProbMetric: 47.5378

Epoch 334: val_loss did not improve from 45.93005
196/196 - 34s - loss: 46.5919 - MinusLogProbMetric: 46.5919 - val_loss: 47.5378 - val_MinusLogProbMetric: 47.5378 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 335/1000
2023-10-28 20:50:12.959 
Epoch 335/1000 
	 loss: 46.2477, MinusLogProbMetric: 46.2477, val_loss: 46.9345, val_MinusLogProbMetric: 46.9345

Epoch 335: val_loss did not improve from 45.93005
196/196 - 34s - loss: 46.2477 - MinusLogProbMetric: 46.2477 - val_loss: 46.9345 - val_MinusLogProbMetric: 46.9345 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 336/1000
2023-10-28 20:50:47.241 
Epoch 336/1000 
	 loss: 46.2020, MinusLogProbMetric: 46.2020, val_loss: 45.8771, val_MinusLogProbMetric: 45.8771

Epoch 336: val_loss improved from 45.93005 to 45.87706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 46.2020 - MinusLogProbMetric: 46.2020 - val_loss: 45.8771 - val_MinusLogProbMetric: 45.8771 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 337/1000
2023-10-28 20:51:21.822 
Epoch 337/1000 
	 loss: 46.2078, MinusLogProbMetric: 46.2078, val_loss: 53.8159, val_MinusLogProbMetric: 53.8159

Epoch 337: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.2078 - MinusLogProbMetric: 46.2078 - val_loss: 53.8159 - val_MinusLogProbMetric: 53.8159 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 338/1000
2023-10-28 20:51:56.077 
Epoch 338/1000 
	 loss: 46.8097, MinusLogProbMetric: 46.8097, val_loss: 49.8353, val_MinusLogProbMetric: 49.8353

Epoch 338: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.8097 - MinusLogProbMetric: 46.8097 - val_loss: 49.8353 - val_MinusLogProbMetric: 49.8353 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 339/1000
2023-10-28 20:52:30.384 
Epoch 339/1000 
	 loss: 46.4267, MinusLogProbMetric: 46.4267, val_loss: 46.7539, val_MinusLogProbMetric: 46.7539

Epoch 339: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.4267 - MinusLogProbMetric: 46.4267 - val_loss: 46.7539 - val_MinusLogProbMetric: 46.7539 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 340/1000
2023-10-28 20:53:04.211 
Epoch 340/1000 
	 loss: 46.2876, MinusLogProbMetric: 46.2876, val_loss: 47.6724, val_MinusLogProbMetric: 47.6724

Epoch 340: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.2876 - MinusLogProbMetric: 46.2876 - val_loss: 47.6724 - val_MinusLogProbMetric: 47.6724 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 341/1000
2023-10-28 20:53:38.331 
Epoch 341/1000 
	 loss: 46.7357, MinusLogProbMetric: 46.7357, val_loss: 47.5571, val_MinusLogProbMetric: 47.5571

Epoch 341: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.7357 - MinusLogProbMetric: 46.7357 - val_loss: 47.5571 - val_MinusLogProbMetric: 47.5571 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 342/1000
2023-10-28 20:54:12.622 
Epoch 342/1000 
	 loss: 46.4727, MinusLogProbMetric: 46.4727, val_loss: 54.3602, val_MinusLogProbMetric: 54.3602

Epoch 342: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.4727 - MinusLogProbMetric: 46.4727 - val_loss: 54.3602 - val_MinusLogProbMetric: 54.3602 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 343/1000
2023-10-28 20:54:46.183 
Epoch 343/1000 
	 loss: 46.7152, MinusLogProbMetric: 46.7152, val_loss: 52.5022, val_MinusLogProbMetric: 52.5022

Epoch 343: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.7152 - MinusLogProbMetric: 46.7152 - val_loss: 52.5022 - val_MinusLogProbMetric: 52.5022 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 344/1000
2023-10-28 20:55:20.345 
Epoch 344/1000 
	 loss: 46.8450, MinusLogProbMetric: 46.8450, val_loss: 47.0180, val_MinusLogProbMetric: 47.0180

Epoch 344: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.8450 - MinusLogProbMetric: 46.8450 - val_loss: 47.0180 - val_MinusLogProbMetric: 47.0180 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 345/1000
2023-10-28 20:55:54.374 
Epoch 345/1000 
	 loss: 46.0643, MinusLogProbMetric: 46.0643, val_loss: 46.1563, val_MinusLogProbMetric: 46.1563

Epoch 345: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.0643 - MinusLogProbMetric: 46.0643 - val_loss: 46.1563 - val_MinusLogProbMetric: 46.1563 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 346/1000
2023-10-28 20:56:28.380 
Epoch 346/1000 
	 loss: 45.9960, MinusLogProbMetric: 45.9960, val_loss: 47.6526, val_MinusLogProbMetric: 47.6526

Epoch 346: val_loss did not improve from 45.87706
196/196 - 34s - loss: 45.9960 - MinusLogProbMetric: 45.9960 - val_loss: 47.6526 - val_MinusLogProbMetric: 47.6526 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 347/1000
2023-10-28 20:57:02.436 
Epoch 347/1000 
	 loss: 46.7411, MinusLogProbMetric: 46.7411, val_loss: 45.8923, val_MinusLogProbMetric: 45.8923

Epoch 347: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.7411 - MinusLogProbMetric: 46.7411 - val_loss: 45.8923 - val_MinusLogProbMetric: 45.8923 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 348/1000
2023-10-28 20:57:36.157 
Epoch 348/1000 
	 loss: 46.4891, MinusLogProbMetric: 46.4891, val_loss: 47.3179, val_MinusLogProbMetric: 47.3179

Epoch 348: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.4891 - MinusLogProbMetric: 46.4891 - val_loss: 47.3179 - val_MinusLogProbMetric: 47.3179 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 349/1000
2023-10-28 20:58:09.903 
Epoch 349/1000 
	 loss: 46.2143, MinusLogProbMetric: 46.2143, val_loss: 47.6489, val_MinusLogProbMetric: 47.6489

Epoch 349: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.2143 - MinusLogProbMetric: 46.2143 - val_loss: 47.6489 - val_MinusLogProbMetric: 47.6489 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 350/1000
2023-10-28 20:58:44.137 
Epoch 350/1000 
	 loss: 45.9084, MinusLogProbMetric: 45.9084, val_loss: 46.2271, val_MinusLogProbMetric: 46.2271

Epoch 350: val_loss did not improve from 45.87706
196/196 - 34s - loss: 45.9084 - MinusLogProbMetric: 45.9084 - val_loss: 46.2271 - val_MinusLogProbMetric: 46.2271 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 351/1000
2023-10-28 20:59:18.042 
Epoch 351/1000 
	 loss: 46.4146, MinusLogProbMetric: 46.4146, val_loss: 46.6412, val_MinusLogProbMetric: 46.6412

Epoch 351: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.4146 - MinusLogProbMetric: 46.4146 - val_loss: 46.6412 - val_MinusLogProbMetric: 46.6412 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 352/1000
2023-10-28 20:59:51.653 
Epoch 352/1000 
	 loss: 45.9698, MinusLogProbMetric: 45.9698, val_loss: 47.6443, val_MinusLogProbMetric: 47.6443

Epoch 352: val_loss did not improve from 45.87706
196/196 - 34s - loss: 45.9698 - MinusLogProbMetric: 45.9698 - val_loss: 47.6443 - val_MinusLogProbMetric: 47.6443 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 353/1000
2023-10-28 21:00:25.580 
Epoch 353/1000 
	 loss: 46.2985, MinusLogProbMetric: 46.2985, val_loss: 48.4137, val_MinusLogProbMetric: 48.4137

Epoch 353: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.2985 - MinusLogProbMetric: 46.2985 - val_loss: 48.4137 - val_MinusLogProbMetric: 48.4137 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 354/1000
2023-10-28 21:01:00.221 
Epoch 354/1000 
	 loss: 46.0494, MinusLogProbMetric: 46.0494, val_loss: 48.5068, val_MinusLogProbMetric: 48.5068

Epoch 354: val_loss did not improve from 45.87706
196/196 - 35s - loss: 46.0494 - MinusLogProbMetric: 46.0494 - val_loss: 48.5068 - val_MinusLogProbMetric: 48.5068 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 355/1000
2023-10-28 21:01:34.735 
Epoch 355/1000 
	 loss: 46.1293, MinusLogProbMetric: 46.1293, val_loss: 46.3632, val_MinusLogProbMetric: 46.3632

Epoch 355: val_loss did not improve from 45.87706
196/196 - 35s - loss: 46.1293 - MinusLogProbMetric: 46.1293 - val_loss: 46.3632 - val_MinusLogProbMetric: 46.3632 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 356/1000
2023-10-28 21:02:09.456 
Epoch 356/1000 
	 loss: 46.3283, MinusLogProbMetric: 46.3283, val_loss: 46.7464, val_MinusLogProbMetric: 46.7464

Epoch 356: val_loss did not improve from 45.87706
196/196 - 35s - loss: 46.3283 - MinusLogProbMetric: 46.3283 - val_loss: 46.7464 - val_MinusLogProbMetric: 46.7464 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 357/1000
2023-10-28 21:02:43.530 
Epoch 357/1000 
	 loss: 45.9223, MinusLogProbMetric: 45.9223, val_loss: 46.5481, val_MinusLogProbMetric: 46.5481

Epoch 357: val_loss did not improve from 45.87706
196/196 - 34s - loss: 45.9223 - MinusLogProbMetric: 45.9223 - val_loss: 46.5481 - val_MinusLogProbMetric: 46.5481 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 358/1000
2023-10-28 21:03:18.249 
Epoch 358/1000 
	 loss: 46.4980, MinusLogProbMetric: 46.4980, val_loss: 45.9184, val_MinusLogProbMetric: 45.9184

Epoch 358: val_loss did not improve from 45.87706
196/196 - 35s - loss: 46.4980 - MinusLogProbMetric: 46.4980 - val_loss: 45.9184 - val_MinusLogProbMetric: 45.9184 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 359/1000
2023-10-28 21:03:52.608 
Epoch 359/1000 
	 loss: 46.2978, MinusLogProbMetric: 46.2978, val_loss: 46.5192, val_MinusLogProbMetric: 46.5192

Epoch 359: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.2978 - MinusLogProbMetric: 46.2978 - val_loss: 46.5192 - val_MinusLogProbMetric: 46.5192 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 360/1000
2023-10-28 21:04:26.340 
Epoch 360/1000 
	 loss: 45.8550, MinusLogProbMetric: 45.8550, val_loss: 46.7080, val_MinusLogProbMetric: 46.7080

Epoch 360: val_loss did not improve from 45.87706
196/196 - 34s - loss: 45.8550 - MinusLogProbMetric: 45.8550 - val_loss: 46.7080 - val_MinusLogProbMetric: 46.7080 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 361/1000
2023-10-28 21:05:00.582 
Epoch 361/1000 
	 loss: 46.2334, MinusLogProbMetric: 46.2334, val_loss: 45.9021, val_MinusLogProbMetric: 45.9021

Epoch 361: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.2334 - MinusLogProbMetric: 46.2334 - val_loss: 45.9021 - val_MinusLogProbMetric: 45.9021 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 362/1000
2023-10-28 21:05:34.660 
Epoch 362/1000 
	 loss: 46.1818, MinusLogProbMetric: 46.1818, val_loss: 46.5989, val_MinusLogProbMetric: 46.5989

Epoch 362: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.1818 - MinusLogProbMetric: 46.1818 - val_loss: 46.5989 - val_MinusLogProbMetric: 46.5989 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 363/1000
2023-10-28 21:06:08.444 
Epoch 363/1000 
	 loss: 46.1128, MinusLogProbMetric: 46.1128, val_loss: 45.8788, val_MinusLogProbMetric: 45.8788

Epoch 363: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.1128 - MinusLogProbMetric: 46.1128 - val_loss: 45.8788 - val_MinusLogProbMetric: 45.8788 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 364/1000
2023-10-28 21:06:42.689 
Epoch 364/1000 
	 loss: 46.0379, MinusLogProbMetric: 46.0379, val_loss: 46.3751, val_MinusLogProbMetric: 46.3751

Epoch 364: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.0379 - MinusLogProbMetric: 46.0379 - val_loss: 46.3751 - val_MinusLogProbMetric: 46.3751 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 365/1000
2023-10-28 21:07:17.532 
Epoch 365/1000 
	 loss: 46.0564, MinusLogProbMetric: 46.0564, val_loss: 46.5958, val_MinusLogProbMetric: 46.5958

Epoch 365: val_loss did not improve from 45.87706
196/196 - 35s - loss: 46.0564 - MinusLogProbMetric: 46.0564 - val_loss: 46.5958 - val_MinusLogProbMetric: 46.5958 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 366/1000
2023-10-28 21:07:52.217 
Epoch 366/1000 
	 loss: 46.0680, MinusLogProbMetric: 46.0680, val_loss: 49.0090, val_MinusLogProbMetric: 49.0090

Epoch 366: val_loss did not improve from 45.87706
196/196 - 35s - loss: 46.0680 - MinusLogProbMetric: 46.0680 - val_loss: 49.0090 - val_MinusLogProbMetric: 49.0090 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 367/1000
2023-10-28 21:08:26.605 
Epoch 367/1000 
	 loss: 46.1774, MinusLogProbMetric: 46.1774, val_loss: 46.0224, val_MinusLogProbMetric: 46.0224

Epoch 367: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.1774 - MinusLogProbMetric: 46.1774 - val_loss: 46.0224 - val_MinusLogProbMetric: 46.0224 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 368/1000
2023-10-28 21:09:01.312 
Epoch 368/1000 
	 loss: 45.9605, MinusLogProbMetric: 45.9605, val_loss: 46.0437, val_MinusLogProbMetric: 46.0437

Epoch 368: val_loss did not improve from 45.87706
196/196 - 35s - loss: 45.9605 - MinusLogProbMetric: 45.9605 - val_loss: 46.0437 - val_MinusLogProbMetric: 46.0437 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 369/1000
2023-10-28 21:09:36.199 
Epoch 369/1000 
	 loss: 45.7964, MinusLogProbMetric: 45.7964, val_loss: 47.6021, val_MinusLogProbMetric: 47.6021

Epoch 369: val_loss did not improve from 45.87706
196/196 - 35s - loss: 45.7964 - MinusLogProbMetric: 45.7964 - val_loss: 47.6021 - val_MinusLogProbMetric: 47.6021 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 370/1000
2023-10-28 21:10:10.181 
Epoch 370/1000 
	 loss: 45.9993, MinusLogProbMetric: 45.9993, val_loss: 46.4479, val_MinusLogProbMetric: 46.4479

Epoch 370: val_loss did not improve from 45.87706
196/196 - 34s - loss: 45.9993 - MinusLogProbMetric: 45.9993 - val_loss: 46.4479 - val_MinusLogProbMetric: 46.4479 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 371/1000
2023-10-28 21:10:44.605 
Epoch 371/1000 
	 loss: 46.0247, MinusLogProbMetric: 46.0247, val_loss: 47.4556, val_MinusLogProbMetric: 47.4556

Epoch 371: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.0247 - MinusLogProbMetric: 46.0247 - val_loss: 47.4556 - val_MinusLogProbMetric: 47.4556 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 372/1000
2023-10-28 21:11:18.906 
Epoch 372/1000 
	 loss: 46.8339, MinusLogProbMetric: 46.8339, val_loss: 46.7742, val_MinusLogProbMetric: 46.7742

Epoch 372: val_loss did not improve from 45.87706
196/196 - 34s - loss: 46.8339 - MinusLogProbMetric: 46.8339 - val_loss: 46.7742 - val_MinusLogProbMetric: 46.7742 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 373/1000
2023-10-28 21:11:53.314 
Epoch 373/1000 
	 loss: 45.6559, MinusLogProbMetric: 45.6559, val_loss: 46.5073, val_MinusLogProbMetric: 46.5073

Epoch 373: val_loss did not improve from 45.87706
196/196 - 34s - loss: 45.6559 - MinusLogProbMetric: 45.6559 - val_loss: 46.5073 - val_MinusLogProbMetric: 46.5073 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 374/1000
2023-10-28 21:12:27.456 
Epoch 374/1000 
	 loss: 46.0731, MinusLogProbMetric: 46.0731, val_loss: 45.6836, val_MinusLogProbMetric: 45.6836

Epoch 374: val_loss improved from 45.87706 to 45.68363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 46.0731 - MinusLogProbMetric: 46.0731 - val_loss: 45.6836 - val_MinusLogProbMetric: 45.6836 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 375/1000
2023-10-28 21:13:02.241 
Epoch 375/1000 
	 loss: 45.8882, MinusLogProbMetric: 45.8882, val_loss: 45.8927, val_MinusLogProbMetric: 45.8927

Epoch 375: val_loss did not improve from 45.68363
196/196 - 34s - loss: 45.8882 - MinusLogProbMetric: 45.8882 - val_loss: 45.8927 - val_MinusLogProbMetric: 45.8927 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 376/1000
2023-10-28 21:13:36.157 
Epoch 376/1000 
	 loss: 45.8233, MinusLogProbMetric: 45.8233, val_loss: 46.2065, val_MinusLogProbMetric: 46.2065

Epoch 376: val_loss did not improve from 45.68363
196/196 - 34s - loss: 45.8233 - MinusLogProbMetric: 45.8233 - val_loss: 46.2065 - val_MinusLogProbMetric: 46.2065 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 377/1000
2023-10-28 21:14:10.384 
Epoch 377/1000 
	 loss: 46.5448, MinusLogProbMetric: 46.5448, val_loss: 46.3536, val_MinusLogProbMetric: 46.3536

Epoch 377: val_loss did not improve from 45.68363
196/196 - 34s - loss: 46.5448 - MinusLogProbMetric: 46.5448 - val_loss: 46.3536 - val_MinusLogProbMetric: 46.3536 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 378/1000
2023-10-28 21:14:44.927 
Epoch 378/1000 
	 loss: 46.0361, MinusLogProbMetric: 46.0361, val_loss: 47.7052, val_MinusLogProbMetric: 47.7052

Epoch 378: val_loss did not improve from 45.68363
196/196 - 35s - loss: 46.0361 - MinusLogProbMetric: 46.0361 - val_loss: 47.7052 - val_MinusLogProbMetric: 47.7052 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 379/1000
2023-10-28 21:15:19.774 
Epoch 379/1000 
	 loss: 46.0790, MinusLogProbMetric: 46.0790, val_loss: 46.4876, val_MinusLogProbMetric: 46.4876

Epoch 379: val_loss did not improve from 45.68363
196/196 - 35s - loss: 46.0790 - MinusLogProbMetric: 46.0790 - val_loss: 46.4876 - val_MinusLogProbMetric: 46.4876 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 380/1000
2023-10-28 21:15:54.460 
Epoch 380/1000 
	 loss: 45.8410, MinusLogProbMetric: 45.8410, val_loss: 46.8170, val_MinusLogProbMetric: 46.8170

Epoch 380: val_loss did not improve from 45.68363
196/196 - 35s - loss: 45.8410 - MinusLogProbMetric: 45.8410 - val_loss: 46.8170 - val_MinusLogProbMetric: 46.8170 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 381/1000
2023-10-28 21:16:29.248 
Epoch 381/1000 
	 loss: 45.9391, MinusLogProbMetric: 45.9391, val_loss: 46.3501, val_MinusLogProbMetric: 46.3501

Epoch 381: val_loss did not improve from 45.68363
196/196 - 35s - loss: 45.9391 - MinusLogProbMetric: 45.9391 - val_loss: 46.3501 - val_MinusLogProbMetric: 46.3501 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 382/1000
2023-10-28 21:17:03.830 
Epoch 382/1000 
	 loss: 46.1974, MinusLogProbMetric: 46.1974, val_loss: 45.6790, val_MinusLogProbMetric: 45.6790

Epoch 382: val_loss improved from 45.68363 to 45.67900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 46.1974 - MinusLogProbMetric: 46.1974 - val_loss: 45.6790 - val_MinusLogProbMetric: 45.6790 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 383/1000
2023-10-28 21:17:38.804 
Epoch 383/1000 
	 loss: 45.9988, MinusLogProbMetric: 45.9988, val_loss: 49.7580, val_MinusLogProbMetric: 49.7580

Epoch 383: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.9988 - MinusLogProbMetric: 45.9988 - val_loss: 49.7580 - val_MinusLogProbMetric: 49.7580 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 384/1000
2023-10-28 21:18:13.104 
Epoch 384/1000 
	 loss: 45.7921, MinusLogProbMetric: 45.7921, val_loss: 46.2071, val_MinusLogProbMetric: 46.2071

Epoch 384: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.7921 - MinusLogProbMetric: 45.7921 - val_loss: 46.2071 - val_MinusLogProbMetric: 46.2071 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 385/1000
2023-10-28 21:18:46.797 
Epoch 385/1000 
	 loss: 46.0432, MinusLogProbMetric: 46.0432, val_loss: 46.0099, val_MinusLogProbMetric: 46.0099

Epoch 385: val_loss did not improve from 45.67900
196/196 - 34s - loss: 46.0432 - MinusLogProbMetric: 46.0432 - val_loss: 46.0099 - val_MinusLogProbMetric: 46.0099 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 386/1000
2023-10-28 21:19:20.697 
Epoch 386/1000 
	 loss: 45.8690, MinusLogProbMetric: 45.8690, val_loss: 45.8329, val_MinusLogProbMetric: 45.8329

Epoch 386: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.8690 - MinusLogProbMetric: 45.8690 - val_loss: 45.8329 - val_MinusLogProbMetric: 45.8329 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 387/1000
2023-10-28 21:19:55.403 
Epoch 387/1000 
	 loss: 45.9833, MinusLogProbMetric: 45.9833, val_loss: 46.6211, val_MinusLogProbMetric: 46.6211

Epoch 387: val_loss did not improve from 45.67900
196/196 - 35s - loss: 45.9833 - MinusLogProbMetric: 45.9833 - val_loss: 46.6211 - val_MinusLogProbMetric: 46.6211 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 388/1000
2023-10-28 21:20:30.324 
Epoch 388/1000 
	 loss: 46.1211, MinusLogProbMetric: 46.1211, val_loss: 46.1686, val_MinusLogProbMetric: 46.1686

Epoch 388: val_loss did not improve from 45.67900
196/196 - 35s - loss: 46.1211 - MinusLogProbMetric: 46.1211 - val_loss: 46.1686 - val_MinusLogProbMetric: 46.1686 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 389/1000
2023-10-28 21:21:04.672 
Epoch 389/1000 
	 loss: 45.9995, MinusLogProbMetric: 45.9995, val_loss: 46.2260, val_MinusLogProbMetric: 46.2260

Epoch 389: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.9995 - MinusLogProbMetric: 45.9995 - val_loss: 46.2260 - val_MinusLogProbMetric: 46.2260 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 390/1000
2023-10-28 21:21:39.083 
Epoch 390/1000 
	 loss: 46.0384, MinusLogProbMetric: 46.0384, val_loss: 49.0294, val_MinusLogProbMetric: 49.0294

Epoch 390: val_loss did not improve from 45.67900
196/196 - 34s - loss: 46.0384 - MinusLogProbMetric: 46.0384 - val_loss: 49.0294 - val_MinusLogProbMetric: 49.0294 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 391/1000
2023-10-28 21:22:13.364 
Epoch 391/1000 
	 loss: 45.9514, MinusLogProbMetric: 45.9514, val_loss: 46.0276, val_MinusLogProbMetric: 46.0276

Epoch 391: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.9514 - MinusLogProbMetric: 45.9514 - val_loss: 46.0276 - val_MinusLogProbMetric: 46.0276 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 392/1000
2023-10-28 21:22:47.419 
Epoch 392/1000 
	 loss: 45.8135, MinusLogProbMetric: 45.8135, val_loss: 46.9080, val_MinusLogProbMetric: 46.9080

Epoch 392: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.8135 - MinusLogProbMetric: 45.8135 - val_loss: 46.9080 - val_MinusLogProbMetric: 46.9080 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 393/1000
2023-10-28 21:23:21.597 
Epoch 393/1000 
	 loss: 45.9769, MinusLogProbMetric: 45.9769, val_loss: 46.8047, val_MinusLogProbMetric: 46.8047

Epoch 393: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.9769 - MinusLogProbMetric: 45.9769 - val_loss: 46.8047 - val_MinusLogProbMetric: 46.8047 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 394/1000
2023-10-28 21:23:55.923 
Epoch 394/1000 
	 loss: 45.8888, MinusLogProbMetric: 45.8888, val_loss: 46.6161, val_MinusLogProbMetric: 46.6161

Epoch 394: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.8888 - MinusLogProbMetric: 45.8888 - val_loss: 46.6161 - val_MinusLogProbMetric: 46.6161 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 395/1000
2023-10-28 21:24:30.299 
Epoch 395/1000 
	 loss: 45.7656, MinusLogProbMetric: 45.7656, val_loss: 47.7745, val_MinusLogProbMetric: 47.7745

Epoch 395: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.7656 - MinusLogProbMetric: 45.7656 - val_loss: 47.7745 - val_MinusLogProbMetric: 47.7745 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 396/1000
2023-10-28 21:25:04.283 
Epoch 396/1000 
	 loss: 45.8090, MinusLogProbMetric: 45.8090, val_loss: 45.6976, val_MinusLogProbMetric: 45.6976

Epoch 396: val_loss did not improve from 45.67900
196/196 - 34s - loss: 45.8090 - MinusLogProbMetric: 45.8090 - val_loss: 45.6976 - val_MinusLogProbMetric: 45.6976 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 397/1000
2023-10-28 21:25:38.972 
Epoch 397/1000 
	 loss: 45.8243, MinusLogProbMetric: 45.8243, val_loss: 47.3809, val_MinusLogProbMetric: 47.3809

Epoch 397: val_loss did not improve from 45.67900
196/196 - 35s - loss: 45.8243 - MinusLogProbMetric: 45.8243 - val_loss: 47.3809 - val_MinusLogProbMetric: 47.3809 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 398/1000
2023-10-28 21:26:13.443 
Epoch 398/1000 
	 loss: 46.1480, MinusLogProbMetric: 46.1480, val_loss: 46.1282, val_MinusLogProbMetric: 46.1282

Epoch 398: val_loss did not improve from 45.67900
196/196 - 34s - loss: 46.1480 - MinusLogProbMetric: 46.1480 - val_loss: 46.1282 - val_MinusLogProbMetric: 46.1282 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 399/1000
2023-10-28 21:26:47.203 
Epoch 399/1000 
	 loss: 45.6185, MinusLogProbMetric: 45.6185, val_loss: 45.6593, val_MinusLogProbMetric: 45.6593

Epoch 399: val_loss improved from 45.67900 to 45.65931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 45.6185 - MinusLogProbMetric: 45.6185 - val_loss: 45.6593 - val_MinusLogProbMetric: 45.6593 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 400/1000
2023-10-28 21:27:22.060 
Epoch 400/1000 
	 loss: 45.9685, MinusLogProbMetric: 45.9685, val_loss: 46.4077, val_MinusLogProbMetric: 46.4077

Epoch 400: val_loss did not improve from 45.65931
196/196 - 34s - loss: 45.9685 - MinusLogProbMetric: 45.9685 - val_loss: 46.4077 - val_MinusLogProbMetric: 46.4077 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 401/1000
2023-10-28 21:27:57.010 
Epoch 401/1000 
	 loss: 45.7484, MinusLogProbMetric: 45.7484, val_loss: 47.4978, val_MinusLogProbMetric: 47.4978

Epoch 401: val_loss did not improve from 45.65931
196/196 - 35s - loss: 45.7484 - MinusLogProbMetric: 45.7484 - val_loss: 47.4978 - val_MinusLogProbMetric: 47.4978 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 402/1000
2023-10-28 21:28:31.840 
Epoch 402/1000 
	 loss: 45.8754, MinusLogProbMetric: 45.8754, val_loss: 46.1003, val_MinusLogProbMetric: 46.1003

Epoch 402: val_loss did not improve from 45.65931
196/196 - 35s - loss: 45.8754 - MinusLogProbMetric: 45.8754 - val_loss: 46.1003 - val_MinusLogProbMetric: 46.1003 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 403/1000
2023-10-28 21:29:06.694 
Epoch 403/1000 
	 loss: 45.7871, MinusLogProbMetric: 45.7871, val_loss: 45.9542, val_MinusLogProbMetric: 45.9542

Epoch 403: val_loss did not improve from 45.65931
196/196 - 35s - loss: 45.7871 - MinusLogProbMetric: 45.7871 - val_loss: 45.9542 - val_MinusLogProbMetric: 45.9542 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 404/1000
2023-10-28 21:29:40.878 
Epoch 404/1000 
	 loss: 45.8286, MinusLogProbMetric: 45.8286, val_loss: 46.0058, val_MinusLogProbMetric: 46.0058

Epoch 404: val_loss did not improve from 45.65931
196/196 - 34s - loss: 45.8286 - MinusLogProbMetric: 45.8286 - val_loss: 46.0058 - val_MinusLogProbMetric: 46.0058 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 405/1000
2023-10-28 21:30:14.989 
Epoch 405/1000 
	 loss: 45.8803, MinusLogProbMetric: 45.8803, val_loss: 46.7568, val_MinusLogProbMetric: 46.7568

Epoch 405: val_loss did not improve from 45.65931
196/196 - 34s - loss: 45.8803 - MinusLogProbMetric: 45.8803 - val_loss: 46.7568 - val_MinusLogProbMetric: 46.7568 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 406/1000
2023-10-28 21:30:49.313 
Epoch 406/1000 
	 loss: 45.9431, MinusLogProbMetric: 45.9431, val_loss: 47.7090, val_MinusLogProbMetric: 47.7090

Epoch 406: val_loss did not improve from 45.65931
196/196 - 34s - loss: 45.9431 - MinusLogProbMetric: 45.9431 - val_loss: 47.7090 - val_MinusLogProbMetric: 47.7090 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 407/1000
2023-10-28 21:31:23.829 
Epoch 407/1000 
	 loss: 45.8752, MinusLogProbMetric: 45.8752, val_loss: 46.7236, val_MinusLogProbMetric: 46.7236

Epoch 407: val_loss did not improve from 45.65931
196/196 - 35s - loss: 45.8752 - MinusLogProbMetric: 45.8752 - val_loss: 46.7236 - val_MinusLogProbMetric: 46.7236 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 408/1000
2023-10-28 21:31:58.526 
Epoch 408/1000 
	 loss: 45.7293, MinusLogProbMetric: 45.7293, val_loss: 47.7207, val_MinusLogProbMetric: 47.7207

Epoch 408: val_loss did not improve from 45.65931
196/196 - 35s - loss: 45.7293 - MinusLogProbMetric: 45.7293 - val_loss: 47.7207 - val_MinusLogProbMetric: 47.7207 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 409/1000
2023-10-28 21:32:33.228 
Epoch 409/1000 
	 loss: 45.6261, MinusLogProbMetric: 45.6261, val_loss: 45.8781, val_MinusLogProbMetric: 45.8781

Epoch 409: val_loss did not improve from 45.65931
196/196 - 35s - loss: 45.6261 - MinusLogProbMetric: 45.6261 - val_loss: 45.8781 - val_MinusLogProbMetric: 45.8781 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 410/1000
2023-10-28 21:33:08.053 
Epoch 410/1000 
	 loss: 46.0402, MinusLogProbMetric: 46.0402, val_loss: 48.6178, val_MinusLogProbMetric: 48.6178

Epoch 410: val_loss did not improve from 45.65931
196/196 - 35s - loss: 46.0402 - MinusLogProbMetric: 46.0402 - val_loss: 48.6178 - val_MinusLogProbMetric: 48.6178 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 411/1000
2023-10-28 21:33:42.951 
Epoch 411/1000 
	 loss: 45.7219, MinusLogProbMetric: 45.7219, val_loss: 45.5509, val_MinusLogProbMetric: 45.5509

Epoch 411: val_loss improved from 45.65931 to 45.55094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 45.7219 - MinusLogProbMetric: 45.7219 - val_loss: 45.5509 - val_MinusLogProbMetric: 45.5509 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 412/1000
2023-10-28 21:34:18.538 
Epoch 412/1000 
	 loss: 45.6401, MinusLogProbMetric: 45.6401, val_loss: 46.6338, val_MinusLogProbMetric: 46.6338

Epoch 412: val_loss did not improve from 45.55094
196/196 - 35s - loss: 45.6401 - MinusLogProbMetric: 45.6401 - val_loss: 46.6338 - val_MinusLogProbMetric: 46.6338 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 413/1000
2023-10-28 21:34:53.217 
Epoch 413/1000 
	 loss: 45.4625, MinusLogProbMetric: 45.4625, val_loss: 46.4174, val_MinusLogProbMetric: 46.4174

Epoch 413: val_loss did not improve from 45.55094
196/196 - 35s - loss: 45.4625 - MinusLogProbMetric: 45.4625 - val_loss: 46.4174 - val_MinusLogProbMetric: 46.4174 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 414/1000
2023-10-28 21:35:27.797 
Epoch 414/1000 
	 loss: 45.8716, MinusLogProbMetric: 45.8716, val_loss: 46.3443, val_MinusLogProbMetric: 46.3443

Epoch 414: val_loss did not improve from 45.55094
196/196 - 35s - loss: 45.8716 - MinusLogProbMetric: 45.8716 - val_loss: 46.3443 - val_MinusLogProbMetric: 46.3443 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 415/1000
2023-10-28 21:36:02.322 
Epoch 415/1000 
	 loss: 45.5310, MinusLogProbMetric: 45.5310, val_loss: 46.1481, val_MinusLogProbMetric: 46.1481

Epoch 415: val_loss did not improve from 45.55094
196/196 - 35s - loss: 45.5310 - MinusLogProbMetric: 45.5310 - val_loss: 46.1481 - val_MinusLogProbMetric: 46.1481 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 416/1000
2023-10-28 21:36:36.949 
Epoch 416/1000 
	 loss: 45.8570, MinusLogProbMetric: 45.8570, val_loss: 45.9372, val_MinusLogProbMetric: 45.9372

Epoch 416: val_loss did not improve from 45.55094
196/196 - 35s - loss: 45.8570 - MinusLogProbMetric: 45.8570 - val_loss: 45.9372 - val_MinusLogProbMetric: 45.9372 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 417/1000
2023-10-28 21:37:11.371 
Epoch 417/1000 
	 loss: 45.5684, MinusLogProbMetric: 45.5684, val_loss: 45.7773, val_MinusLogProbMetric: 45.7773

Epoch 417: val_loss did not improve from 45.55094
196/196 - 34s - loss: 45.5684 - MinusLogProbMetric: 45.5684 - val_loss: 45.7773 - val_MinusLogProbMetric: 45.7773 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 418/1000
2023-10-28 21:37:45.868 
Epoch 418/1000 
	 loss: 45.3888, MinusLogProbMetric: 45.3888, val_loss: 49.2139, val_MinusLogProbMetric: 49.2139

Epoch 418: val_loss did not improve from 45.55094
196/196 - 34s - loss: 45.3888 - MinusLogProbMetric: 45.3888 - val_loss: 49.2139 - val_MinusLogProbMetric: 49.2139 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 419/1000
2023-10-28 21:38:19.874 
Epoch 419/1000 
	 loss: 45.6050, MinusLogProbMetric: 45.6050, val_loss: 46.6912, val_MinusLogProbMetric: 46.6912

Epoch 419: val_loss did not improve from 45.55094
196/196 - 34s - loss: 45.6050 - MinusLogProbMetric: 45.6050 - val_loss: 46.6912 - val_MinusLogProbMetric: 46.6912 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 420/1000
2023-10-28 21:38:54.439 
Epoch 420/1000 
	 loss: 45.9183, MinusLogProbMetric: 45.9183, val_loss: 45.2885, val_MinusLogProbMetric: 45.2885

Epoch 420: val_loss improved from 45.55094 to 45.28853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 45.9183 - MinusLogProbMetric: 45.9183 - val_loss: 45.2885 - val_MinusLogProbMetric: 45.2885 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 421/1000
2023-10-28 21:39:29.262 
Epoch 421/1000 
	 loss: 45.7059, MinusLogProbMetric: 45.7059, val_loss: 47.0294, val_MinusLogProbMetric: 47.0294

Epoch 421: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.7059 - MinusLogProbMetric: 45.7059 - val_loss: 47.0294 - val_MinusLogProbMetric: 47.0294 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 422/1000
2023-10-28 21:40:03.577 
Epoch 422/1000 
	 loss: 45.3678, MinusLogProbMetric: 45.3678, val_loss: 45.4944, val_MinusLogProbMetric: 45.4944

Epoch 422: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.3678 - MinusLogProbMetric: 45.3678 - val_loss: 45.4944 - val_MinusLogProbMetric: 45.4944 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 423/1000
2023-10-28 21:40:37.753 
Epoch 423/1000 
	 loss: 45.3816, MinusLogProbMetric: 45.3816, val_loss: 47.4123, val_MinusLogProbMetric: 47.4123

Epoch 423: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.3816 - MinusLogProbMetric: 45.3816 - val_loss: 47.4123 - val_MinusLogProbMetric: 47.4123 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 424/1000
2023-10-28 21:41:12.148 
Epoch 424/1000 
	 loss: 46.1843, MinusLogProbMetric: 46.1843, val_loss: 45.7353, val_MinusLogProbMetric: 45.7353

Epoch 424: val_loss did not improve from 45.28853
196/196 - 34s - loss: 46.1843 - MinusLogProbMetric: 46.1843 - val_loss: 45.7353 - val_MinusLogProbMetric: 45.7353 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 425/1000
2023-10-28 21:41:46.463 
Epoch 425/1000 
	 loss: 45.4324, MinusLogProbMetric: 45.4324, val_loss: 49.1369, val_MinusLogProbMetric: 49.1369

Epoch 425: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.4324 - MinusLogProbMetric: 45.4324 - val_loss: 49.1369 - val_MinusLogProbMetric: 49.1369 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 426/1000
2023-10-28 21:42:21.038 
Epoch 426/1000 
	 loss: 45.8861, MinusLogProbMetric: 45.8861, val_loss: 45.6611, val_MinusLogProbMetric: 45.6611

Epoch 426: val_loss did not improve from 45.28853
196/196 - 35s - loss: 45.8861 - MinusLogProbMetric: 45.8861 - val_loss: 45.6611 - val_MinusLogProbMetric: 45.6611 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 427/1000
2023-10-28 21:42:55.943 
Epoch 427/1000 
	 loss: 45.3607, MinusLogProbMetric: 45.3607, val_loss: 45.8100, val_MinusLogProbMetric: 45.8100

Epoch 427: val_loss did not improve from 45.28853
196/196 - 35s - loss: 45.3607 - MinusLogProbMetric: 45.3607 - val_loss: 45.8100 - val_MinusLogProbMetric: 45.8100 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 428/1000
2023-10-28 21:43:30.733 
Epoch 428/1000 
	 loss: 45.4127, MinusLogProbMetric: 45.4127, val_loss: 46.6464, val_MinusLogProbMetric: 46.6464

Epoch 428: val_loss did not improve from 45.28853
196/196 - 35s - loss: 45.4127 - MinusLogProbMetric: 45.4127 - val_loss: 46.6464 - val_MinusLogProbMetric: 46.6464 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 429/1000
2023-10-28 21:44:05.316 
Epoch 429/1000 
	 loss: 45.7739, MinusLogProbMetric: 45.7739, val_loss: 46.0703, val_MinusLogProbMetric: 46.0703

Epoch 429: val_loss did not improve from 45.28853
196/196 - 35s - loss: 45.7739 - MinusLogProbMetric: 45.7739 - val_loss: 46.0703 - val_MinusLogProbMetric: 46.0703 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 430/1000
2023-10-28 21:44:40.075 
Epoch 430/1000 
	 loss: 45.8303, MinusLogProbMetric: 45.8303, val_loss: 46.7720, val_MinusLogProbMetric: 46.7720

Epoch 430: val_loss did not improve from 45.28853
196/196 - 35s - loss: 45.8303 - MinusLogProbMetric: 45.8303 - val_loss: 46.7720 - val_MinusLogProbMetric: 46.7720 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 431/1000
2023-10-28 21:45:14.290 
Epoch 431/1000 
	 loss: 45.5930, MinusLogProbMetric: 45.5930, val_loss: 48.5709, val_MinusLogProbMetric: 48.5709

Epoch 431: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.5930 - MinusLogProbMetric: 45.5930 - val_loss: 48.5709 - val_MinusLogProbMetric: 48.5709 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 432/1000
2023-10-28 21:45:48.554 
Epoch 432/1000 
	 loss: 45.9820, MinusLogProbMetric: 45.9820, val_loss: 47.8789, val_MinusLogProbMetric: 47.8789

Epoch 432: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.9820 - MinusLogProbMetric: 45.9820 - val_loss: 47.8789 - val_MinusLogProbMetric: 47.8789 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 433/1000
2023-10-28 21:46:23.139 
Epoch 433/1000 
	 loss: 45.7819, MinusLogProbMetric: 45.7819, val_loss: 46.5289, val_MinusLogProbMetric: 46.5289

Epoch 433: val_loss did not improve from 45.28853
196/196 - 35s - loss: 45.7819 - MinusLogProbMetric: 45.7819 - val_loss: 46.5289 - val_MinusLogProbMetric: 46.5289 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 434/1000
2023-10-28 21:46:57.356 
Epoch 434/1000 
	 loss: 45.9738, MinusLogProbMetric: 45.9738, val_loss: 47.9652, val_MinusLogProbMetric: 47.9652

Epoch 434: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.9738 - MinusLogProbMetric: 45.9738 - val_loss: 47.9652 - val_MinusLogProbMetric: 47.9652 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 435/1000
2023-10-28 21:47:31.190 
Epoch 435/1000 
	 loss: 45.3193, MinusLogProbMetric: 45.3193, val_loss: 49.4002, val_MinusLogProbMetric: 49.4002

Epoch 435: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.3193 - MinusLogProbMetric: 45.3193 - val_loss: 49.4002 - val_MinusLogProbMetric: 49.4002 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 436/1000
2023-10-28 21:48:05.159 
Epoch 436/1000 
	 loss: 45.5302, MinusLogProbMetric: 45.5302, val_loss: 45.9441, val_MinusLogProbMetric: 45.9441

Epoch 436: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.5302 - MinusLogProbMetric: 45.5302 - val_loss: 45.9441 - val_MinusLogProbMetric: 45.9441 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 437/1000
2023-10-28 21:48:39.582 
Epoch 437/1000 
	 loss: 45.3378, MinusLogProbMetric: 45.3378, val_loss: 46.5573, val_MinusLogProbMetric: 46.5573

Epoch 437: val_loss did not improve from 45.28853
196/196 - 34s - loss: 45.3378 - MinusLogProbMetric: 45.3378 - val_loss: 46.5573 - val_MinusLogProbMetric: 46.5573 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 438/1000
2023-10-28 21:49:14.312 
Epoch 438/1000 
	 loss: 45.7413, MinusLogProbMetric: 45.7413, val_loss: 45.8423, val_MinusLogProbMetric: 45.8423

Epoch 438: val_loss did not improve from 45.28853
196/196 - 35s - loss: 45.7413 - MinusLogProbMetric: 45.7413 - val_loss: 45.8423 - val_MinusLogProbMetric: 45.8423 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 439/1000
2023-10-28 21:49:49.469 
Epoch 439/1000 
	 loss: 45.5034, MinusLogProbMetric: 45.5034, val_loss: 48.5258, val_MinusLogProbMetric: 48.5258

Epoch 439: val_loss did not improve from 45.28853
196/196 - 35s - loss: 45.5034 - MinusLogProbMetric: 45.5034 - val_loss: 48.5258 - val_MinusLogProbMetric: 48.5258 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 440/1000
2023-10-28 21:50:23.947 
Epoch 440/1000 
	 loss: 45.5555, MinusLogProbMetric: 45.5555, val_loss: 45.0303, val_MinusLogProbMetric: 45.0303

Epoch 440: val_loss improved from 45.28853 to 45.03030, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 45.5555 - MinusLogProbMetric: 45.5555 - val_loss: 45.0303 - val_MinusLogProbMetric: 45.0303 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 441/1000
2023-10-28 21:50:58.688 
Epoch 441/1000 
	 loss: 45.3250, MinusLogProbMetric: 45.3250, val_loss: 46.2437, val_MinusLogProbMetric: 46.2437

Epoch 441: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.3250 - MinusLogProbMetric: 45.3250 - val_loss: 46.2437 - val_MinusLogProbMetric: 46.2437 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 442/1000
2023-10-28 21:51:33.058 
Epoch 442/1000 
	 loss: 45.7126, MinusLogProbMetric: 45.7126, val_loss: 45.3004, val_MinusLogProbMetric: 45.3004

Epoch 442: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.7126 - MinusLogProbMetric: 45.7126 - val_loss: 45.3004 - val_MinusLogProbMetric: 45.3004 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 443/1000
2023-10-28 21:52:07.825 
Epoch 443/1000 
	 loss: 45.3440, MinusLogProbMetric: 45.3440, val_loss: 47.8873, val_MinusLogProbMetric: 47.8873

Epoch 443: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3440 - MinusLogProbMetric: 45.3440 - val_loss: 47.8873 - val_MinusLogProbMetric: 47.8873 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 444/1000
2023-10-28 21:52:42.899 
Epoch 444/1000 
	 loss: 45.3324, MinusLogProbMetric: 45.3324, val_loss: 45.7382, val_MinusLogProbMetric: 45.7382

Epoch 444: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3324 - MinusLogProbMetric: 45.3324 - val_loss: 45.7382 - val_MinusLogProbMetric: 45.7382 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 445/1000
2023-10-28 21:53:17.820 
Epoch 445/1000 
	 loss: 45.3582, MinusLogProbMetric: 45.3582, val_loss: 45.9187, val_MinusLogProbMetric: 45.9187

Epoch 445: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3582 - MinusLogProbMetric: 45.3582 - val_loss: 45.9187 - val_MinusLogProbMetric: 45.9187 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 446/1000
2023-10-28 21:53:52.744 
Epoch 446/1000 
	 loss: 45.3708, MinusLogProbMetric: 45.3708, val_loss: 47.2798, val_MinusLogProbMetric: 47.2798

Epoch 446: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3708 - MinusLogProbMetric: 45.3708 - val_loss: 47.2798 - val_MinusLogProbMetric: 47.2798 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 447/1000
2023-10-28 21:54:27.397 
Epoch 447/1000 
	 loss: 45.4767, MinusLogProbMetric: 45.4767, val_loss: 46.1618, val_MinusLogProbMetric: 46.1618

Epoch 447: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.4767 - MinusLogProbMetric: 45.4767 - val_loss: 46.1618 - val_MinusLogProbMetric: 46.1618 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 448/1000
2023-10-28 21:55:02.128 
Epoch 448/1000 
	 loss: 46.0670, MinusLogProbMetric: 46.0670, val_loss: 46.1918, val_MinusLogProbMetric: 46.1918

Epoch 448: val_loss did not improve from 45.03030
196/196 - 35s - loss: 46.0670 - MinusLogProbMetric: 46.0670 - val_loss: 46.1918 - val_MinusLogProbMetric: 46.1918 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 449/1000
2023-10-28 21:55:36.534 
Epoch 449/1000 
	 loss: 45.2214, MinusLogProbMetric: 45.2214, val_loss: 47.3528, val_MinusLogProbMetric: 47.3528

Epoch 449: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.2214 - MinusLogProbMetric: 45.2214 - val_loss: 47.3528 - val_MinusLogProbMetric: 47.3528 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 450/1000
2023-10-28 21:56:10.886 
Epoch 450/1000 
	 loss: 45.3700, MinusLogProbMetric: 45.3700, val_loss: 45.5714, val_MinusLogProbMetric: 45.5714

Epoch 450: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.3700 - MinusLogProbMetric: 45.3700 - val_loss: 45.5714 - val_MinusLogProbMetric: 45.5714 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 451/1000
2023-10-28 21:56:45.474 
Epoch 451/1000 
	 loss: 45.3804, MinusLogProbMetric: 45.3804, val_loss: 45.5708, val_MinusLogProbMetric: 45.5708

Epoch 451: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3804 - MinusLogProbMetric: 45.3804 - val_loss: 45.5708 - val_MinusLogProbMetric: 45.5708 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 452/1000
2023-10-28 21:57:20.263 
Epoch 452/1000 
	 loss: 45.3859, MinusLogProbMetric: 45.3859, val_loss: 47.1384, val_MinusLogProbMetric: 47.1384

Epoch 452: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3859 - MinusLogProbMetric: 45.3859 - val_loss: 47.1384 - val_MinusLogProbMetric: 47.1384 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 453/1000
2023-10-28 21:57:54.850 
Epoch 453/1000 
	 loss: 45.2278, MinusLogProbMetric: 45.2278, val_loss: 46.0357, val_MinusLogProbMetric: 46.0357

Epoch 453: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.2278 - MinusLogProbMetric: 45.2278 - val_loss: 46.0357 - val_MinusLogProbMetric: 46.0357 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 454/1000
2023-10-28 21:58:29.324 
Epoch 454/1000 
	 loss: 45.4765, MinusLogProbMetric: 45.4765, val_loss: 46.8249, val_MinusLogProbMetric: 46.8249

Epoch 454: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.4765 - MinusLogProbMetric: 45.4765 - val_loss: 46.8249 - val_MinusLogProbMetric: 46.8249 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 455/1000
2023-10-28 21:59:03.892 
Epoch 455/1000 
	 loss: 45.1232, MinusLogProbMetric: 45.1232, val_loss: 45.6607, val_MinusLogProbMetric: 45.6607

Epoch 455: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.1232 - MinusLogProbMetric: 45.1232 - val_loss: 45.6607 - val_MinusLogProbMetric: 45.6607 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 456/1000
2023-10-28 21:59:38.011 
Epoch 456/1000 
	 loss: 45.6060, MinusLogProbMetric: 45.6060, val_loss: 46.5120, val_MinusLogProbMetric: 46.5120

Epoch 456: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.6060 - MinusLogProbMetric: 45.6060 - val_loss: 46.5120 - val_MinusLogProbMetric: 46.5120 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 457/1000
2023-10-28 22:00:12.190 
Epoch 457/1000 
	 loss: 45.3831, MinusLogProbMetric: 45.3831, val_loss: 45.1158, val_MinusLogProbMetric: 45.1158

Epoch 457: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.3831 - MinusLogProbMetric: 45.3831 - val_loss: 45.1158 - val_MinusLogProbMetric: 45.1158 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 458/1000
2023-10-28 22:00:46.783 
Epoch 458/1000 
	 loss: 45.3755, MinusLogProbMetric: 45.3755, val_loss: 46.9890, val_MinusLogProbMetric: 46.9890

Epoch 458: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3755 - MinusLogProbMetric: 45.3755 - val_loss: 46.9890 - val_MinusLogProbMetric: 46.9890 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 459/1000
2023-10-28 22:01:21.025 
Epoch 459/1000 
	 loss: 45.5087, MinusLogProbMetric: 45.5087, val_loss: 45.1470, val_MinusLogProbMetric: 45.1470

Epoch 459: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.5087 - MinusLogProbMetric: 45.5087 - val_loss: 45.1470 - val_MinusLogProbMetric: 45.1470 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 460/1000
2023-10-28 22:01:54.998 
Epoch 460/1000 
	 loss: 45.2643, MinusLogProbMetric: 45.2643, val_loss: 48.3778, val_MinusLogProbMetric: 48.3778

Epoch 460: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.2643 - MinusLogProbMetric: 45.2643 - val_loss: 48.3778 - val_MinusLogProbMetric: 48.3778 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 461/1000
2023-10-28 22:02:28.555 
Epoch 461/1000 
	 loss: 45.6702, MinusLogProbMetric: 45.6702, val_loss: 46.1086, val_MinusLogProbMetric: 46.1086

Epoch 461: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.6702 - MinusLogProbMetric: 45.6702 - val_loss: 46.1086 - val_MinusLogProbMetric: 46.1086 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 462/1000
2023-10-28 22:03:02.924 
Epoch 462/1000 
	 loss: 45.2291, MinusLogProbMetric: 45.2291, val_loss: 48.9249, val_MinusLogProbMetric: 48.9249

Epoch 462: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.2291 - MinusLogProbMetric: 45.2291 - val_loss: 48.9249 - val_MinusLogProbMetric: 48.9249 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 463/1000
2023-10-28 22:03:37.294 
Epoch 463/1000 
	 loss: 45.3194, MinusLogProbMetric: 45.3194, val_loss: 45.1533, val_MinusLogProbMetric: 45.1533

Epoch 463: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.3194 - MinusLogProbMetric: 45.3194 - val_loss: 45.1533 - val_MinusLogProbMetric: 45.1533 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 464/1000
2023-10-28 22:04:11.882 
Epoch 464/1000 
	 loss: 45.3308, MinusLogProbMetric: 45.3308, val_loss: 46.1354, val_MinusLogProbMetric: 46.1354

Epoch 464: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3308 - MinusLogProbMetric: 45.3308 - val_loss: 46.1354 - val_MinusLogProbMetric: 46.1354 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 465/1000
2023-10-28 22:04:45.908 
Epoch 465/1000 
	 loss: 45.3310, MinusLogProbMetric: 45.3310, val_loss: 45.3188, val_MinusLogProbMetric: 45.3188

Epoch 465: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.3310 - MinusLogProbMetric: 45.3310 - val_loss: 45.3188 - val_MinusLogProbMetric: 45.3188 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 466/1000
2023-10-28 22:05:20.259 
Epoch 466/1000 
	 loss: 45.4404, MinusLogProbMetric: 45.4404, val_loss: 47.1250, val_MinusLogProbMetric: 47.1250

Epoch 466: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.4404 - MinusLogProbMetric: 45.4404 - val_loss: 47.1250 - val_MinusLogProbMetric: 47.1250 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 467/1000
2023-10-28 22:05:54.910 
Epoch 467/1000 
	 loss: 45.5765, MinusLogProbMetric: 45.5765, val_loss: 45.7594, val_MinusLogProbMetric: 45.7594

Epoch 467: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.5765 - MinusLogProbMetric: 45.5765 - val_loss: 45.7594 - val_MinusLogProbMetric: 45.7594 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 468/1000
2023-10-28 22:06:29.495 
Epoch 468/1000 
	 loss: 45.3734, MinusLogProbMetric: 45.3734, val_loss: 50.1229, val_MinusLogProbMetric: 50.1229

Epoch 468: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3734 - MinusLogProbMetric: 45.3734 - val_loss: 50.1229 - val_MinusLogProbMetric: 50.1229 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 469/1000
2023-10-28 22:07:03.560 
Epoch 469/1000 
	 loss: 45.6774, MinusLogProbMetric: 45.6774, val_loss: 45.1157, val_MinusLogProbMetric: 45.1157

Epoch 469: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.6774 - MinusLogProbMetric: 45.6774 - val_loss: 45.1157 - val_MinusLogProbMetric: 45.1157 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 470/1000
2023-10-28 22:07:37.937 
Epoch 470/1000 
	 loss: 45.1567, MinusLogProbMetric: 45.1567, val_loss: 46.2108, val_MinusLogProbMetric: 46.2108

Epoch 470: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.1567 - MinusLogProbMetric: 45.1567 - val_loss: 46.2108 - val_MinusLogProbMetric: 46.2108 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 471/1000
2023-10-28 22:08:12.897 
Epoch 471/1000 
	 loss: 45.5783, MinusLogProbMetric: 45.5783, val_loss: 46.3694, val_MinusLogProbMetric: 46.3694

Epoch 471: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.5783 - MinusLogProbMetric: 45.5783 - val_loss: 46.3694 - val_MinusLogProbMetric: 46.3694 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 472/1000
2023-10-28 22:08:47.355 
Epoch 472/1000 
	 loss: 45.1976, MinusLogProbMetric: 45.1976, val_loss: 46.1511, val_MinusLogProbMetric: 46.1511

Epoch 472: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.1976 - MinusLogProbMetric: 45.1976 - val_loss: 46.1511 - val_MinusLogProbMetric: 46.1511 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 473/1000
2023-10-28 22:09:22.339 
Epoch 473/1000 
	 loss: 45.3049, MinusLogProbMetric: 45.3049, val_loss: 45.6796, val_MinusLogProbMetric: 45.6796

Epoch 473: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.3049 - MinusLogProbMetric: 45.3049 - val_loss: 45.6796 - val_MinusLogProbMetric: 45.6796 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 474/1000
2023-10-28 22:09:57.075 
Epoch 474/1000 
	 loss: 45.2275, MinusLogProbMetric: 45.2275, val_loss: 46.1539, val_MinusLogProbMetric: 46.1539

Epoch 474: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.2275 - MinusLogProbMetric: 45.2275 - val_loss: 46.1539 - val_MinusLogProbMetric: 46.1539 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 475/1000
2023-10-28 22:10:31.809 
Epoch 475/1000 
	 loss: 45.7050, MinusLogProbMetric: 45.7050, val_loss: 47.4349, val_MinusLogProbMetric: 47.4349

Epoch 475: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.7050 - MinusLogProbMetric: 45.7050 - val_loss: 47.4349 - val_MinusLogProbMetric: 47.4349 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 476/1000
2023-10-28 22:11:06.358 
Epoch 476/1000 
	 loss: 45.6044, MinusLogProbMetric: 45.6044, val_loss: 45.9469, val_MinusLogProbMetric: 45.9469

Epoch 476: val_loss did not improve from 45.03030
196/196 - 35s - loss: 45.6044 - MinusLogProbMetric: 45.6044 - val_loss: 45.9469 - val_MinusLogProbMetric: 45.9469 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 477/1000
2023-10-28 22:11:40.620 
Epoch 477/1000 
	 loss: 45.2017, MinusLogProbMetric: 45.2017, val_loss: 45.2241, val_MinusLogProbMetric: 45.2241

Epoch 477: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.2017 - MinusLogProbMetric: 45.2017 - val_loss: 45.2241 - val_MinusLogProbMetric: 45.2241 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 478/1000
2023-10-28 22:12:14.920 
Epoch 478/1000 
	 loss: 45.1221, MinusLogProbMetric: 45.1221, val_loss: 47.1299, val_MinusLogProbMetric: 47.1299

Epoch 478: val_loss did not improve from 45.03030
196/196 - 34s - loss: 45.1221 - MinusLogProbMetric: 45.1221 - val_loss: 47.1299 - val_MinusLogProbMetric: 47.1299 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 479/1000
2023-10-28 22:12:49.708 
Epoch 479/1000 
	 loss: 45.1377, MinusLogProbMetric: 45.1377, val_loss: 45.0207, val_MinusLogProbMetric: 45.0207

Epoch 479: val_loss improved from 45.03030 to 45.02067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 45.1377 - MinusLogProbMetric: 45.1377 - val_loss: 45.0207 - val_MinusLogProbMetric: 45.0207 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 480/1000
2023-10-28 22:13:24.326 
Epoch 480/1000 
	 loss: 44.9622, MinusLogProbMetric: 44.9622, val_loss: 46.1199, val_MinusLogProbMetric: 46.1199

Epoch 480: val_loss did not improve from 45.02067
196/196 - 34s - loss: 44.9622 - MinusLogProbMetric: 44.9622 - val_loss: 46.1199 - val_MinusLogProbMetric: 46.1199 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 481/1000
2023-10-28 22:13:57.821 
Epoch 481/1000 
	 loss: 45.2199, MinusLogProbMetric: 45.2199, val_loss: 46.4013, val_MinusLogProbMetric: 46.4013

Epoch 481: val_loss did not improve from 45.02067
196/196 - 33s - loss: 45.2199 - MinusLogProbMetric: 45.2199 - val_loss: 46.4013 - val_MinusLogProbMetric: 46.4013 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 482/1000
2023-10-28 22:14:31.066 
Epoch 482/1000 
	 loss: 45.2586, MinusLogProbMetric: 45.2586, val_loss: 45.3372, val_MinusLogProbMetric: 45.3372

Epoch 482: val_loss did not improve from 45.02067
196/196 - 33s - loss: 45.2586 - MinusLogProbMetric: 45.2586 - val_loss: 45.3372 - val_MinusLogProbMetric: 45.3372 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 483/1000
2023-10-28 22:15:05.747 
Epoch 483/1000 
	 loss: 45.2858, MinusLogProbMetric: 45.2858, val_loss: 47.1401, val_MinusLogProbMetric: 47.1401

Epoch 483: val_loss did not improve from 45.02067
196/196 - 35s - loss: 45.2858 - MinusLogProbMetric: 45.2858 - val_loss: 47.1401 - val_MinusLogProbMetric: 47.1401 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 484/1000
2023-10-28 22:15:40.148 
Epoch 484/1000 
	 loss: 45.1065, MinusLogProbMetric: 45.1065, val_loss: 46.7481, val_MinusLogProbMetric: 46.7481

Epoch 484: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.1065 - MinusLogProbMetric: 45.1065 - val_loss: 46.7481 - val_MinusLogProbMetric: 46.7481 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 485/1000
2023-10-28 22:16:14.741 
Epoch 485/1000 
	 loss: 44.9711, MinusLogProbMetric: 44.9711, val_loss: 45.2969, val_MinusLogProbMetric: 45.2969

Epoch 485: val_loss did not improve from 45.02067
196/196 - 35s - loss: 44.9711 - MinusLogProbMetric: 44.9711 - val_loss: 45.2969 - val_MinusLogProbMetric: 45.2969 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 486/1000
2023-10-28 22:16:49.343 
Epoch 486/1000 
	 loss: 45.4429, MinusLogProbMetric: 45.4429, val_loss: 47.2259, val_MinusLogProbMetric: 47.2259

Epoch 486: val_loss did not improve from 45.02067
196/196 - 35s - loss: 45.4429 - MinusLogProbMetric: 45.4429 - val_loss: 47.2259 - val_MinusLogProbMetric: 47.2259 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 487/1000
2023-10-28 22:17:23.927 
Epoch 487/1000 
	 loss: 45.0016, MinusLogProbMetric: 45.0016, val_loss: 46.6471, val_MinusLogProbMetric: 46.6471

Epoch 487: val_loss did not improve from 45.02067
196/196 - 35s - loss: 45.0016 - MinusLogProbMetric: 45.0016 - val_loss: 46.6471 - val_MinusLogProbMetric: 46.6471 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 488/1000
2023-10-28 22:17:58.232 
Epoch 488/1000 
	 loss: 45.0844, MinusLogProbMetric: 45.0844, val_loss: 49.6267, val_MinusLogProbMetric: 49.6267

Epoch 488: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.0844 - MinusLogProbMetric: 45.0844 - val_loss: 49.6267 - val_MinusLogProbMetric: 49.6267 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 489/1000
2023-10-28 22:18:32.749 
Epoch 489/1000 
	 loss: 45.2636, MinusLogProbMetric: 45.2636, val_loss: 46.0835, val_MinusLogProbMetric: 46.0835

Epoch 489: val_loss did not improve from 45.02067
196/196 - 35s - loss: 45.2636 - MinusLogProbMetric: 45.2636 - val_loss: 46.0835 - val_MinusLogProbMetric: 46.0835 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 490/1000
2023-10-28 22:19:06.937 
Epoch 490/1000 
	 loss: 45.3590, MinusLogProbMetric: 45.3590, val_loss: 46.2465, val_MinusLogProbMetric: 46.2465

Epoch 490: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.3590 - MinusLogProbMetric: 45.3590 - val_loss: 46.2465 - val_MinusLogProbMetric: 46.2465 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 491/1000
2023-10-28 22:19:41.830 
Epoch 491/1000 
	 loss: 44.9764, MinusLogProbMetric: 44.9764, val_loss: 45.3838, val_MinusLogProbMetric: 45.3838

Epoch 491: val_loss did not improve from 45.02067
196/196 - 35s - loss: 44.9764 - MinusLogProbMetric: 44.9764 - val_loss: 45.3838 - val_MinusLogProbMetric: 45.3838 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 492/1000
2023-10-28 22:20:15.922 
Epoch 492/1000 
	 loss: 45.0642, MinusLogProbMetric: 45.0642, val_loss: 46.8519, val_MinusLogProbMetric: 46.8519

Epoch 492: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.0642 - MinusLogProbMetric: 45.0642 - val_loss: 46.8519 - val_MinusLogProbMetric: 46.8519 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 493/1000
2023-10-28 22:20:50.133 
Epoch 493/1000 
	 loss: 45.3868, MinusLogProbMetric: 45.3868, val_loss: 46.3123, val_MinusLogProbMetric: 46.3123

Epoch 493: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.3868 - MinusLogProbMetric: 45.3868 - val_loss: 46.3123 - val_MinusLogProbMetric: 46.3123 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 494/1000
2023-10-28 22:21:24.603 
Epoch 494/1000 
	 loss: 45.4561, MinusLogProbMetric: 45.4561, val_loss: 46.4988, val_MinusLogProbMetric: 46.4988

Epoch 494: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.4561 - MinusLogProbMetric: 45.4561 - val_loss: 46.4988 - val_MinusLogProbMetric: 46.4988 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 495/1000
2023-10-28 22:21:58.823 
Epoch 495/1000 
	 loss: 45.2696, MinusLogProbMetric: 45.2696, val_loss: 45.3253, val_MinusLogProbMetric: 45.3253

Epoch 495: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.2696 - MinusLogProbMetric: 45.2696 - val_loss: 45.3253 - val_MinusLogProbMetric: 45.3253 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 496/1000
2023-10-28 22:22:32.881 
Epoch 496/1000 
	 loss: 45.0318, MinusLogProbMetric: 45.0318, val_loss: 45.0549, val_MinusLogProbMetric: 45.0549

Epoch 496: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.0318 - MinusLogProbMetric: 45.0318 - val_loss: 45.0549 - val_MinusLogProbMetric: 45.0549 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 497/1000
2023-10-28 22:23:06.578 
Epoch 497/1000 
	 loss: 45.1434, MinusLogProbMetric: 45.1434, val_loss: 45.6813, val_MinusLogProbMetric: 45.6813

Epoch 497: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.1434 - MinusLogProbMetric: 45.1434 - val_loss: 45.6813 - val_MinusLogProbMetric: 45.6813 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 498/1000
2023-10-28 22:23:40.364 
Epoch 498/1000 
	 loss: 45.1695, MinusLogProbMetric: 45.1695, val_loss: 45.6212, val_MinusLogProbMetric: 45.6212

Epoch 498: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.1695 - MinusLogProbMetric: 45.1695 - val_loss: 45.6212 - val_MinusLogProbMetric: 45.6212 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 499/1000
2023-10-28 22:24:13.708 
Epoch 499/1000 
	 loss: 44.9485, MinusLogProbMetric: 44.9485, val_loss: 48.2228, val_MinusLogProbMetric: 48.2228

Epoch 499: val_loss did not improve from 45.02067
196/196 - 33s - loss: 44.9485 - MinusLogProbMetric: 44.9485 - val_loss: 48.2228 - val_MinusLogProbMetric: 48.2228 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 500/1000
2023-10-28 22:24:47.809 
Epoch 500/1000 
	 loss: 45.6273, MinusLogProbMetric: 45.6273, val_loss: 45.7177, val_MinusLogProbMetric: 45.7177

Epoch 500: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.6273 - MinusLogProbMetric: 45.6273 - val_loss: 45.7177 - val_MinusLogProbMetric: 45.7177 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 501/1000
2023-10-28 22:25:22.381 
Epoch 501/1000 
	 loss: 44.9902, MinusLogProbMetric: 44.9902, val_loss: 46.3185, val_MinusLogProbMetric: 46.3185

Epoch 501: val_loss did not improve from 45.02067
196/196 - 35s - loss: 44.9902 - MinusLogProbMetric: 44.9902 - val_loss: 46.3185 - val_MinusLogProbMetric: 46.3185 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 502/1000
2023-10-28 22:25:56.678 
Epoch 502/1000 
	 loss: 45.1433, MinusLogProbMetric: 45.1433, val_loss: 46.3822, val_MinusLogProbMetric: 46.3822

Epoch 502: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.1433 - MinusLogProbMetric: 45.1433 - val_loss: 46.3822 - val_MinusLogProbMetric: 46.3822 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 503/1000
2023-10-28 22:26:30.951 
Epoch 503/1000 
	 loss: 45.1394, MinusLogProbMetric: 45.1394, val_loss: 46.4865, val_MinusLogProbMetric: 46.4865

Epoch 503: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.1394 - MinusLogProbMetric: 45.1394 - val_loss: 46.4865 - val_MinusLogProbMetric: 46.4865 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 504/1000
2023-10-28 22:27:05.450 
Epoch 504/1000 
	 loss: 45.0688, MinusLogProbMetric: 45.0688, val_loss: 45.3559, val_MinusLogProbMetric: 45.3559

Epoch 504: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.0688 - MinusLogProbMetric: 45.0688 - val_loss: 45.3559 - val_MinusLogProbMetric: 45.3559 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 505/1000
2023-10-28 22:27:38.941 
Epoch 505/1000 
	 loss: 45.0211, MinusLogProbMetric: 45.0211, val_loss: 45.6769, val_MinusLogProbMetric: 45.6769

Epoch 505: val_loss did not improve from 45.02067
196/196 - 33s - loss: 45.0211 - MinusLogProbMetric: 45.0211 - val_loss: 45.6769 - val_MinusLogProbMetric: 45.6769 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 506/1000
2023-10-28 22:28:13.254 
Epoch 506/1000 
	 loss: 45.0827, MinusLogProbMetric: 45.0827, val_loss: 47.5650, val_MinusLogProbMetric: 47.5650

Epoch 506: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.0827 - MinusLogProbMetric: 45.0827 - val_loss: 47.5650 - val_MinusLogProbMetric: 47.5650 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 507/1000
2023-10-28 22:28:47.231 
Epoch 507/1000 
	 loss: 45.5482, MinusLogProbMetric: 45.5482, val_loss: 45.8908, val_MinusLogProbMetric: 45.8908

Epoch 507: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.5482 - MinusLogProbMetric: 45.5482 - val_loss: 45.8908 - val_MinusLogProbMetric: 45.8908 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 508/1000
2023-10-28 22:29:21.304 
Epoch 508/1000 
	 loss: 44.8360, MinusLogProbMetric: 44.8360, val_loss: 47.0519, val_MinusLogProbMetric: 47.0519

Epoch 508: val_loss did not improve from 45.02067
196/196 - 34s - loss: 44.8360 - MinusLogProbMetric: 44.8360 - val_loss: 47.0519 - val_MinusLogProbMetric: 47.0519 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 509/1000
2023-10-28 22:29:55.258 
Epoch 509/1000 
	 loss: 45.3018, MinusLogProbMetric: 45.3018, val_loss: 47.0814, val_MinusLogProbMetric: 47.0814

Epoch 509: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.3018 - MinusLogProbMetric: 45.3018 - val_loss: 47.0814 - val_MinusLogProbMetric: 47.0814 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 510/1000
2023-10-28 22:30:29.223 
Epoch 510/1000 
	 loss: 45.0295, MinusLogProbMetric: 45.0295, val_loss: 46.4743, val_MinusLogProbMetric: 46.4743

Epoch 510: val_loss did not improve from 45.02067
196/196 - 34s - loss: 45.0295 - MinusLogProbMetric: 45.0295 - val_loss: 46.4743 - val_MinusLogProbMetric: 46.4743 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 511/1000
2023-10-28 22:31:03.333 
Epoch 511/1000 
	 loss: 44.8427, MinusLogProbMetric: 44.8427, val_loss: 46.6597, val_MinusLogProbMetric: 46.6597

Epoch 511: val_loss did not improve from 45.02067
196/196 - 34s - loss: 44.8427 - MinusLogProbMetric: 44.8427 - val_loss: 46.6597 - val_MinusLogProbMetric: 46.6597 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 512/1000
2023-10-28 22:31:37.446 
Epoch 512/1000 
	 loss: 45.2093, MinusLogProbMetric: 45.2093, val_loss: 44.9214, val_MinusLogProbMetric: 44.9214

Epoch 512: val_loss improved from 45.02067 to 44.92143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 45.2093 - MinusLogProbMetric: 45.2093 - val_loss: 44.9214 - val_MinusLogProbMetric: 44.9214 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 513/1000
2023-10-28 22:32:12.035 
Epoch 513/1000 
	 loss: 45.0922, MinusLogProbMetric: 45.0922, val_loss: 45.6079, val_MinusLogProbMetric: 45.6079

Epoch 513: val_loss did not improve from 44.92143
196/196 - 34s - loss: 45.0922 - MinusLogProbMetric: 45.0922 - val_loss: 45.6079 - val_MinusLogProbMetric: 45.6079 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 514/1000
2023-10-28 22:32:45.971 
Epoch 514/1000 
	 loss: 44.8727, MinusLogProbMetric: 44.8727, val_loss: 45.0144, val_MinusLogProbMetric: 45.0144

Epoch 514: val_loss did not improve from 44.92143
196/196 - 34s - loss: 44.8727 - MinusLogProbMetric: 44.8727 - val_loss: 45.0144 - val_MinusLogProbMetric: 45.0144 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 515/1000
2023-10-28 22:33:19.924 
Epoch 515/1000 
	 loss: 45.2002, MinusLogProbMetric: 45.2002, val_loss: 45.3830, val_MinusLogProbMetric: 45.3830

Epoch 515: val_loss did not improve from 44.92143
196/196 - 34s - loss: 45.2002 - MinusLogProbMetric: 45.2002 - val_loss: 45.3830 - val_MinusLogProbMetric: 45.3830 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 516/1000
2023-10-28 22:33:53.335 
Epoch 516/1000 
	 loss: 45.4226, MinusLogProbMetric: 45.4226, val_loss: 45.4136, val_MinusLogProbMetric: 45.4136

Epoch 516: val_loss did not improve from 44.92143
196/196 - 33s - loss: 45.4226 - MinusLogProbMetric: 45.4226 - val_loss: 45.4136 - val_MinusLogProbMetric: 45.4136 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 517/1000
2023-10-28 22:34:27.595 
Epoch 517/1000 
	 loss: 45.0554, MinusLogProbMetric: 45.0554, val_loss: 45.3249, val_MinusLogProbMetric: 45.3249

Epoch 517: val_loss did not improve from 44.92143
196/196 - 34s - loss: 45.0554 - MinusLogProbMetric: 45.0554 - val_loss: 45.3249 - val_MinusLogProbMetric: 45.3249 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 518/1000
2023-10-28 22:35:02.223 
Epoch 518/1000 
	 loss: 44.9833, MinusLogProbMetric: 44.9833, val_loss: 46.7441, val_MinusLogProbMetric: 46.7441

Epoch 518: val_loss did not improve from 44.92143
196/196 - 35s - loss: 44.9833 - MinusLogProbMetric: 44.9833 - val_loss: 46.7441 - val_MinusLogProbMetric: 46.7441 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 519/1000
2023-10-28 22:35:36.836 
Epoch 519/1000 
	 loss: 44.7205, MinusLogProbMetric: 44.7205, val_loss: 45.1692, val_MinusLogProbMetric: 45.1692

Epoch 519: val_loss did not improve from 44.92143
196/196 - 35s - loss: 44.7205 - MinusLogProbMetric: 44.7205 - val_loss: 45.1692 - val_MinusLogProbMetric: 45.1692 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 520/1000
2023-10-28 22:36:11.234 
Epoch 520/1000 
	 loss: 45.0202, MinusLogProbMetric: 45.0202, val_loss: 45.1286, val_MinusLogProbMetric: 45.1286

Epoch 520: val_loss did not improve from 44.92143
196/196 - 34s - loss: 45.0202 - MinusLogProbMetric: 45.0202 - val_loss: 45.1286 - val_MinusLogProbMetric: 45.1286 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 521/1000
2023-10-28 22:36:45.943 
Epoch 521/1000 
	 loss: 44.8587, MinusLogProbMetric: 44.8587, val_loss: 45.4805, val_MinusLogProbMetric: 45.4805

Epoch 521: val_loss did not improve from 44.92143
196/196 - 35s - loss: 44.8587 - MinusLogProbMetric: 44.8587 - val_loss: 45.4805 - val_MinusLogProbMetric: 45.4805 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 522/1000
2023-10-28 22:37:20.791 
Epoch 522/1000 
	 loss: 44.8260, MinusLogProbMetric: 44.8260, val_loss: 45.3190, val_MinusLogProbMetric: 45.3190

Epoch 522: val_loss did not improve from 44.92143
196/196 - 35s - loss: 44.8260 - MinusLogProbMetric: 44.8260 - val_loss: 45.3190 - val_MinusLogProbMetric: 45.3190 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 523/1000
2023-10-28 22:37:54.979 
Epoch 523/1000 
	 loss: 45.1125, MinusLogProbMetric: 45.1125, val_loss: 46.8650, val_MinusLogProbMetric: 46.8650

Epoch 523: val_loss did not improve from 44.92143
196/196 - 34s - loss: 45.1125 - MinusLogProbMetric: 45.1125 - val_loss: 46.8650 - val_MinusLogProbMetric: 46.8650 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 524/1000
2023-10-28 22:38:29.145 
Epoch 524/1000 
	 loss: 44.9857, MinusLogProbMetric: 44.9857, val_loss: 45.7608, val_MinusLogProbMetric: 45.7608

Epoch 524: val_loss did not improve from 44.92143
196/196 - 34s - loss: 44.9857 - MinusLogProbMetric: 44.9857 - val_loss: 45.7608 - val_MinusLogProbMetric: 45.7608 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 525/1000
2023-10-28 22:39:03.273 
Epoch 525/1000 
	 loss: 45.0401, MinusLogProbMetric: 45.0401, val_loss: 45.7442, val_MinusLogProbMetric: 45.7442

Epoch 525: val_loss did not improve from 44.92143
196/196 - 34s - loss: 45.0401 - MinusLogProbMetric: 45.0401 - val_loss: 45.7442 - val_MinusLogProbMetric: 45.7442 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 526/1000
2023-10-28 22:39:37.546 
Epoch 526/1000 
	 loss: 44.8662, MinusLogProbMetric: 44.8662, val_loss: 46.2431, val_MinusLogProbMetric: 46.2431

Epoch 526: val_loss did not improve from 44.92143
196/196 - 34s - loss: 44.8662 - MinusLogProbMetric: 44.8662 - val_loss: 46.2431 - val_MinusLogProbMetric: 46.2431 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 527/1000
2023-10-28 22:40:11.517 
Epoch 527/1000 
	 loss: 44.9052, MinusLogProbMetric: 44.9052, val_loss: 45.5855, val_MinusLogProbMetric: 45.5855

Epoch 527: val_loss did not improve from 44.92143
196/196 - 34s - loss: 44.9052 - MinusLogProbMetric: 44.9052 - val_loss: 45.5855 - val_MinusLogProbMetric: 45.5855 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 528/1000
2023-10-28 22:40:45.839 
Epoch 528/1000 
	 loss: 45.2215, MinusLogProbMetric: 45.2215, val_loss: 45.3141, val_MinusLogProbMetric: 45.3141

Epoch 528: val_loss did not improve from 44.92143
196/196 - 34s - loss: 45.2215 - MinusLogProbMetric: 45.2215 - val_loss: 45.3141 - val_MinusLogProbMetric: 45.3141 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 529/1000
2023-10-28 22:41:19.705 
Epoch 529/1000 
	 loss: 44.8246, MinusLogProbMetric: 44.8246, val_loss: 46.4419, val_MinusLogProbMetric: 46.4419

Epoch 529: val_loss did not improve from 44.92143
196/196 - 34s - loss: 44.8246 - MinusLogProbMetric: 44.8246 - val_loss: 46.4419 - val_MinusLogProbMetric: 46.4419 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 530/1000
2023-10-28 22:41:53.791 
Epoch 530/1000 
	 loss: 44.7827, MinusLogProbMetric: 44.7827, val_loss: 45.3812, val_MinusLogProbMetric: 45.3812

Epoch 530: val_loss did not improve from 44.92143
196/196 - 34s - loss: 44.7827 - MinusLogProbMetric: 44.7827 - val_loss: 45.3812 - val_MinusLogProbMetric: 45.3812 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 531/1000
2023-10-28 22:42:28.028 
Epoch 531/1000 
	 loss: 44.9479, MinusLogProbMetric: 44.9479, val_loss: 46.5663, val_MinusLogProbMetric: 46.5663

Epoch 531: val_loss did not improve from 44.92143
196/196 - 34s - loss: 44.9479 - MinusLogProbMetric: 44.9479 - val_loss: 46.5663 - val_MinusLogProbMetric: 46.5663 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 532/1000
2023-10-28 22:43:02.168 
Epoch 532/1000 
	 loss: 44.7843, MinusLogProbMetric: 44.7843, val_loss: 45.5261, val_MinusLogProbMetric: 45.5261

Epoch 532: val_loss did not improve from 44.92143
196/196 - 34s - loss: 44.7843 - MinusLogProbMetric: 44.7843 - val_loss: 45.5261 - val_MinusLogProbMetric: 45.5261 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 533/1000
2023-10-28 22:43:35.943 
Epoch 533/1000 
	 loss: 45.0010, MinusLogProbMetric: 45.0010, val_loss: 44.7497, val_MinusLogProbMetric: 44.7497

Epoch 533: val_loss improved from 44.92143 to 44.74969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 45.0010 - MinusLogProbMetric: 45.0010 - val_loss: 44.7497 - val_MinusLogProbMetric: 44.7497 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 534/1000
2023-10-28 22:44:10.838 
Epoch 534/1000 
	 loss: 45.5521, MinusLogProbMetric: 45.5521, val_loss: 47.5397, val_MinusLogProbMetric: 47.5397

Epoch 534: val_loss did not improve from 44.74969
196/196 - 34s - loss: 45.5521 - MinusLogProbMetric: 45.5521 - val_loss: 47.5397 - val_MinusLogProbMetric: 47.5397 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 535/1000
2023-10-28 22:44:45.535 
Epoch 535/1000 
	 loss: 45.0963, MinusLogProbMetric: 45.0963, val_loss: 45.6109, val_MinusLogProbMetric: 45.6109

Epoch 535: val_loss did not improve from 44.74969
196/196 - 35s - loss: 45.0963 - MinusLogProbMetric: 45.0963 - val_loss: 45.6109 - val_MinusLogProbMetric: 45.6109 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 536/1000
2023-10-28 22:45:20.005 
Epoch 536/1000 
	 loss: 44.7871, MinusLogProbMetric: 44.7871, val_loss: 45.4746, val_MinusLogProbMetric: 45.4746

Epoch 536: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.7871 - MinusLogProbMetric: 44.7871 - val_loss: 45.4746 - val_MinusLogProbMetric: 45.4746 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 537/1000
2023-10-28 22:45:54.322 
Epoch 537/1000 
	 loss: 45.1856, MinusLogProbMetric: 45.1856, val_loss: 46.3548, val_MinusLogProbMetric: 46.3548

Epoch 537: val_loss did not improve from 44.74969
196/196 - 34s - loss: 45.1856 - MinusLogProbMetric: 45.1856 - val_loss: 46.3548 - val_MinusLogProbMetric: 46.3548 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 538/1000
2023-10-28 22:46:28.539 
Epoch 538/1000 
	 loss: 44.8849, MinusLogProbMetric: 44.8849, val_loss: 46.7822, val_MinusLogProbMetric: 46.7822

Epoch 538: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.8849 - MinusLogProbMetric: 44.8849 - val_loss: 46.7822 - val_MinusLogProbMetric: 46.7822 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 539/1000
2023-10-28 22:47:02.129 
Epoch 539/1000 
	 loss: 44.7717, MinusLogProbMetric: 44.7717, val_loss: 47.7293, val_MinusLogProbMetric: 47.7293

Epoch 539: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.7717 - MinusLogProbMetric: 44.7717 - val_loss: 47.7293 - val_MinusLogProbMetric: 47.7293 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 540/1000
2023-10-28 22:47:35.771 
Epoch 540/1000 
	 loss: 44.8400, MinusLogProbMetric: 44.8400, val_loss: 46.9910, val_MinusLogProbMetric: 46.9910

Epoch 540: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.8400 - MinusLogProbMetric: 44.8400 - val_loss: 46.9910 - val_MinusLogProbMetric: 46.9910 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 541/1000
2023-10-28 22:48:10.245 
Epoch 541/1000 
	 loss: 44.8439, MinusLogProbMetric: 44.8439, val_loss: 45.5100, val_MinusLogProbMetric: 45.5100

Epoch 541: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.8439 - MinusLogProbMetric: 44.8439 - val_loss: 45.5100 - val_MinusLogProbMetric: 45.5100 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 542/1000
2023-10-28 22:48:44.403 
Epoch 542/1000 
	 loss: 44.9447, MinusLogProbMetric: 44.9447, val_loss: 45.4166, val_MinusLogProbMetric: 45.4166

Epoch 542: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.9447 - MinusLogProbMetric: 44.9447 - val_loss: 45.4166 - val_MinusLogProbMetric: 45.4166 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 543/1000
2023-10-28 22:49:18.658 
Epoch 543/1000 
	 loss: 44.7478, MinusLogProbMetric: 44.7478, val_loss: 48.2785, val_MinusLogProbMetric: 48.2785

Epoch 543: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.7478 - MinusLogProbMetric: 44.7478 - val_loss: 48.2785 - val_MinusLogProbMetric: 48.2785 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 544/1000
2023-10-28 22:49:52.668 
Epoch 544/1000 
	 loss: 45.0224, MinusLogProbMetric: 45.0224, val_loss: 46.7388, val_MinusLogProbMetric: 46.7388

Epoch 544: val_loss did not improve from 44.74969
196/196 - 34s - loss: 45.0224 - MinusLogProbMetric: 45.0224 - val_loss: 46.7388 - val_MinusLogProbMetric: 46.7388 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 545/1000
2023-10-28 22:50:26.715 
Epoch 545/1000 
	 loss: 44.7620, MinusLogProbMetric: 44.7620, val_loss: 44.7925, val_MinusLogProbMetric: 44.7925

Epoch 545: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.7620 - MinusLogProbMetric: 44.7620 - val_loss: 44.7925 - val_MinusLogProbMetric: 44.7925 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 546/1000
2023-10-28 22:51:00.949 
Epoch 546/1000 
	 loss: 45.1359, MinusLogProbMetric: 45.1359, val_loss: 44.8983, val_MinusLogProbMetric: 44.8983

Epoch 546: val_loss did not improve from 44.74969
196/196 - 34s - loss: 45.1359 - MinusLogProbMetric: 45.1359 - val_loss: 44.8983 - val_MinusLogProbMetric: 44.8983 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 547/1000
2023-10-28 22:51:35.326 
Epoch 547/1000 
	 loss: 44.6185, MinusLogProbMetric: 44.6185, val_loss: 47.0938, val_MinusLogProbMetric: 47.0938

Epoch 547: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.6185 - MinusLogProbMetric: 44.6185 - val_loss: 47.0938 - val_MinusLogProbMetric: 47.0938 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 548/1000
2023-10-28 22:52:09.732 
Epoch 548/1000 
	 loss: 44.8940, MinusLogProbMetric: 44.8940, val_loss: 48.0825, val_MinusLogProbMetric: 48.0825

Epoch 548: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.8940 - MinusLogProbMetric: 44.8940 - val_loss: 48.0825 - val_MinusLogProbMetric: 48.0825 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 549/1000
2023-10-28 22:52:44.123 
Epoch 549/1000 
	 loss: 44.9542, MinusLogProbMetric: 44.9542, val_loss: 44.9605, val_MinusLogProbMetric: 44.9605

Epoch 549: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.9542 - MinusLogProbMetric: 44.9542 - val_loss: 44.9605 - val_MinusLogProbMetric: 44.9605 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 550/1000
2023-10-28 22:53:18.426 
Epoch 550/1000 
	 loss: 44.7810, MinusLogProbMetric: 44.7810, val_loss: 45.9075, val_MinusLogProbMetric: 45.9075

Epoch 550: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.7810 - MinusLogProbMetric: 44.7810 - val_loss: 45.9075 - val_MinusLogProbMetric: 45.9075 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 551/1000
2023-10-28 22:53:52.515 
Epoch 551/1000 
	 loss: 44.7716, MinusLogProbMetric: 44.7716, val_loss: 45.5208, val_MinusLogProbMetric: 45.5208

Epoch 551: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.7716 - MinusLogProbMetric: 44.7716 - val_loss: 45.5208 - val_MinusLogProbMetric: 45.5208 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 552/1000
2023-10-28 22:54:27.279 
Epoch 552/1000 
	 loss: 44.7523, MinusLogProbMetric: 44.7523, val_loss: 45.3864, val_MinusLogProbMetric: 45.3864

Epoch 552: val_loss did not improve from 44.74969
196/196 - 35s - loss: 44.7523 - MinusLogProbMetric: 44.7523 - val_loss: 45.3864 - val_MinusLogProbMetric: 45.3864 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 553/1000
2023-10-28 22:55:01.817 
Epoch 553/1000 
	 loss: 44.7113, MinusLogProbMetric: 44.7113, val_loss: 45.4315, val_MinusLogProbMetric: 45.4315

Epoch 553: val_loss did not improve from 44.74969
196/196 - 35s - loss: 44.7113 - MinusLogProbMetric: 44.7113 - val_loss: 45.4315 - val_MinusLogProbMetric: 45.4315 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 554/1000
2023-10-28 22:55:35.460 
Epoch 554/1000 
	 loss: 44.8344, MinusLogProbMetric: 44.8344, val_loss: 48.4651, val_MinusLogProbMetric: 48.4651

Epoch 554: val_loss did not improve from 44.74969
196/196 - 34s - loss: 44.8344 - MinusLogProbMetric: 44.8344 - val_loss: 48.4651 - val_MinusLogProbMetric: 48.4651 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 555/1000
2023-10-28 22:56:08.988 
Epoch 555/1000 
	 loss: 45.0653, MinusLogProbMetric: 45.0653, val_loss: 45.8773, val_MinusLogProbMetric: 45.8773

Epoch 555: val_loss did not improve from 44.74969
196/196 - 34s - loss: 45.0653 - MinusLogProbMetric: 45.0653 - val_loss: 45.8773 - val_MinusLogProbMetric: 45.8773 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 556/1000
2023-10-28 22:56:42.212 
Epoch 556/1000 
	 loss: 44.9190, MinusLogProbMetric: 44.9190, val_loss: 44.4890, val_MinusLogProbMetric: 44.4890

Epoch 556: val_loss improved from 44.74969 to 44.48899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 44.9190 - MinusLogProbMetric: 44.9190 - val_loss: 44.4890 - val_MinusLogProbMetric: 44.4890 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 557/1000
2023-10-28 22:57:16.393 
Epoch 557/1000 
	 loss: 44.8653, MinusLogProbMetric: 44.8653, val_loss: 45.5463, val_MinusLogProbMetric: 45.5463

Epoch 557: val_loss did not improve from 44.48899
196/196 - 34s - loss: 44.8653 - MinusLogProbMetric: 44.8653 - val_loss: 45.5463 - val_MinusLogProbMetric: 45.5463 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 558/1000
2023-10-28 22:57:50.963 
Epoch 558/1000 
	 loss: 44.6213, MinusLogProbMetric: 44.6213, val_loss: 46.0988, val_MinusLogProbMetric: 46.0988

Epoch 558: val_loss did not improve from 44.48899
196/196 - 35s - loss: 44.6213 - MinusLogProbMetric: 44.6213 - val_loss: 46.0988 - val_MinusLogProbMetric: 46.0988 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 559/1000
2023-10-28 22:58:25.509 
Epoch 559/1000 
	 loss: 44.6217, MinusLogProbMetric: 44.6217, val_loss: 46.0253, val_MinusLogProbMetric: 46.0253

Epoch 559: val_loss did not improve from 44.48899
196/196 - 35s - loss: 44.6217 - MinusLogProbMetric: 44.6217 - val_loss: 46.0253 - val_MinusLogProbMetric: 46.0253 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 560/1000
2023-10-28 22:58:59.564 
Epoch 560/1000 
	 loss: 44.9253, MinusLogProbMetric: 44.9253, val_loss: 45.8472, val_MinusLogProbMetric: 45.8472

Epoch 560: val_loss did not improve from 44.48899
196/196 - 34s - loss: 44.9253 - MinusLogProbMetric: 44.9253 - val_loss: 45.8472 - val_MinusLogProbMetric: 45.8472 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 561/1000
2023-10-28 22:59:33.741 
Epoch 561/1000 
	 loss: 44.6411, MinusLogProbMetric: 44.6411, val_loss: 44.4553, val_MinusLogProbMetric: 44.4553

Epoch 561: val_loss improved from 44.48899 to 44.45530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 44.6411 - MinusLogProbMetric: 44.6411 - val_loss: 44.4553 - val_MinusLogProbMetric: 44.4553 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 562/1000
2023-10-28 23:00:08.215 
Epoch 562/1000 
	 loss: 44.9365, MinusLogProbMetric: 44.9365, val_loss: 45.1677, val_MinusLogProbMetric: 45.1677

Epoch 562: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.9365 - MinusLogProbMetric: 44.9365 - val_loss: 45.1677 - val_MinusLogProbMetric: 45.1677 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 563/1000
2023-10-28 23:00:42.416 
Epoch 563/1000 
	 loss: 44.5237, MinusLogProbMetric: 44.5237, val_loss: 46.5209, val_MinusLogProbMetric: 46.5209

Epoch 563: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.5237 - MinusLogProbMetric: 44.5237 - val_loss: 46.5209 - val_MinusLogProbMetric: 46.5209 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 564/1000
2023-10-28 23:01:16.923 
Epoch 564/1000 
	 loss: 44.6886, MinusLogProbMetric: 44.6886, val_loss: 46.2298, val_MinusLogProbMetric: 46.2298

Epoch 564: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.6886 - MinusLogProbMetric: 44.6886 - val_loss: 46.2298 - val_MinusLogProbMetric: 46.2298 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 565/1000
2023-10-28 23:01:51.240 
Epoch 565/1000 
	 loss: 44.6619, MinusLogProbMetric: 44.6619, val_loss: 45.6174, val_MinusLogProbMetric: 45.6174

Epoch 565: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.6619 - MinusLogProbMetric: 44.6619 - val_loss: 45.6174 - val_MinusLogProbMetric: 45.6174 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 566/1000
2023-10-28 23:02:25.214 
Epoch 566/1000 
	 loss: 44.7585, MinusLogProbMetric: 44.7585, val_loss: 45.1583, val_MinusLogProbMetric: 45.1583

Epoch 566: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.7585 - MinusLogProbMetric: 44.7585 - val_loss: 45.1583 - val_MinusLogProbMetric: 45.1583 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 567/1000
2023-10-28 23:02:58.968 
Epoch 567/1000 
	 loss: 44.7449, MinusLogProbMetric: 44.7449, val_loss: 44.9530, val_MinusLogProbMetric: 44.9530

Epoch 567: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.7449 - MinusLogProbMetric: 44.7449 - val_loss: 44.9530 - val_MinusLogProbMetric: 44.9530 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 568/1000
2023-10-28 23:03:32.671 
Epoch 568/1000 
	 loss: 44.5531, MinusLogProbMetric: 44.5531, val_loss: 44.7462, val_MinusLogProbMetric: 44.7462

Epoch 568: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.5531 - MinusLogProbMetric: 44.5531 - val_loss: 44.7462 - val_MinusLogProbMetric: 44.7462 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 569/1000
2023-10-28 23:04:07.091 
Epoch 569/1000 
	 loss: 44.9815, MinusLogProbMetric: 44.9815, val_loss: 45.4197, val_MinusLogProbMetric: 45.4197

Epoch 569: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.9815 - MinusLogProbMetric: 44.9815 - val_loss: 45.4197 - val_MinusLogProbMetric: 45.4197 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 570/1000
2023-10-28 23:04:41.504 
Epoch 570/1000 
	 loss: 44.6944, MinusLogProbMetric: 44.6944, val_loss: 45.8115, val_MinusLogProbMetric: 45.8115

Epoch 570: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.6944 - MinusLogProbMetric: 44.6944 - val_loss: 45.8115 - val_MinusLogProbMetric: 45.8115 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 571/1000
2023-10-28 23:05:16.171 
Epoch 571/1000 
	 loss: 44.7415, MinusLogProbMetric: 44.7415, val_loss: 45.3617, val_MinusLogProbMetric: 45.3617

Epoch 571: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.7415 - MinusLogProbMetric: 44.7415 - val_loss: 45.3617 - val_MinusLogProbMetric: 45.3617 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 572/1000
2023-10-28 23:05:50.590 
Epoch 572/1000 
	 loss: 44.5603, MinusLogProbMetric: 44.5603, val_loss: 45.2414, val_MinusLogProbMetric: 45.2414

Epoch 572: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.5603 - MinusLogProbMetric: 44.5603 - val_loss: 45.2414 - val_MinusLogProbMetric: 45.2414 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 573/1000
2023-10-28 23:06:24.761 
Epoch 573/1000 
	 loss: 44.6943, MinusLogProbMetric: 44.6943, val_loss: 45.1415, val_MinusLogProbMetric: 45.1415

Epoch 573: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.6943 - MinusLogProbMetric: 44.6943 - val_loss: 45.1415 - val_MinusLogProbMetric: 45.1415 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 574/1000
2023-10-28 23:06:59.392 
Epoch 574/1000 
	 loss: 44.7779, MinusLogProbMetric: 44.7779, val_loss: 46.3812, val_MinusLogProbMetric: 46.3812

Epoch 574: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.7779 - MinusLogProbMetric: 44.7779 - val_loss: 46.3812 - val_MinusLogProbMetric: 46.3812 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 575/1000
2023-10-28 23:07:34.282 
Epoch 575/1000 
	 loss: 44.5073, MinusLogProbMetric: 44.5073, val_loss: 44.7191, val_MinusLogProbMetric: 44.7191

Epoch 575: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.5073 - MinusLogProbMetric: 44.5073 - val_loss: 44.7191 - val_MinusLogProbMetric: 44.7191 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 576/1000
2023-10-28 23:08:09.096 
Epoch 576/1000 
	 loss: 44.7677, MinusLogProbMetric: 44.7677, val_loss: 44.8243, val_MinusLogProbMetric: 44.8243

Epoch 576: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.7677 - MinusLogProbMetric: 44.7677 - val_loss: 44.8243 - val_MinusLogProbMetric: 44.8243 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 577/1000
2023-10-28 23:08:43.632 
Epoch 577/1000 
	 loss: 44.6667, MinusLogProbMetric: 44.6667, val_loss: 45.3553, val_MinusLogProbMetric: 45.3553

Epoch 577: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.6667 - MinusLogProbMetric: 44.6667 - val_loss: 45.3553 - val_MinusLogProbMetric: 45.3553 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 578/1000
2023-10-28 23:09:18.288 
Epoch 578/1000 
	 loss: 44.5068, MinusLogProbMetric: 44.5068, val_loss: 46.3824, val_MinusLogProbMetric: 46.3824

Epoch 578: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.5068 - MinusLogProbMetric: 44.5068 - val_loss: 46.3824 - val_MinusLogProbMetric: 46.3824 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 579/1000
2023-10-28 23:09:52.899 
Epoch 579/1000 
	 loss: 44.5727, MinusLogProbMetric: 44.5727, val_loss: 44.7514, val_MinusLogProbMetric: 44.7514

Epoch 579: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.5727 - MinusLogProbMetric: 44.5727 - val_loss: 44.7514 - val_MinusLogProbMetric: 44.7514 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 580/1000
2023-10-28 23:10:27.660 
Epoch 580/1000 
	 loss: 44.5605, MinusLogProbMetric: 44.5605, val_loss: 45.7758, val_MinusLogProbMetric: 45.7758

Epoch 580: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.5605 - MinusLogProbMetric: 44.5605 - val_loss: 45.7758 - val_MinusLogProbMetric: 45.7758 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 581/1000
2023-10-28 23:11:02.191 
Epoch 581/1000 
	 loss: 44.7412, MinusLogProbMetric: 44.7412, val_loss: 44.8335, val_MinusLogProbMetric: 44.8335

Epoch 581: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.7412 - MinusLogProbMetric: 44.7412 - val_loss: 44.8335 - val_MinusLogProbMetric: 44.8335 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 582/1000
2023-10-28 23:11:37.340 
Epoch 582/1000 
	 loss: 44.6876, MinusLogProbMetric: 44.6876, val_loss: 45.5922, val_MinusLogProbMetric: 45.5922

Epoch 582: val_loss did not improve from 44.45530
196/196 - 35s - loss: 44.6876 - MinusLogProbMetric: 44.6876 - val_loss: 45.5922 - val_MinusLogProbMetric: 45.5922 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 583/1000
2023-10-28 23:12:11.326 
Epoch 583/1000 
	 loss: 45.1165, MinusLogProbMetric: 45.1165, val_loss: 46.0437, val_MinusLogProbMetric: 46.0437

Epoch 583: val_loss did not improve from 44.45530
196/196 - 34s - loss: 45.1165 - MinusLogProbMetric: 45.1165 - val_loss: 46.0437 - val_MinusLogProbMetric: 46.0437 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 584/1000
2023-10-28 23:12:45.710 
Epoch 584/1000 
	 loss: 44.4415, MinusLogProbMetric: 44.4415, val_loss: 45.8780, val_MinusLogProbMetric: 45.8780

Epoch 584: val_loss did not improve from 44.45530
196/196 - 34s - loss: 44.4415 - MinusLogProbMetric: 44.4415 - val_loss: 45.8780 - val_MinusLogProbMetric: 45.8780 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 585/1000
2023-10-28 23:13:20.273 
Epoch 585/1000 
	 loss: 44.8689, MinusLogProbMetric: 44.8689, val_loss: 44.4500, val_MinusLogProbMetric: 44.4500

Epoch 585: val_loss improved from 44.45530 to 44.45004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 44.8689 - MinusLogProbMetric: 44.8689 - val_loss: 44.4500 - val_MinusLogProbMetric: 44.4500 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 586/1000
2023-10-28 23:13:55.531 
Epoch 586/1000 
	 loss: 44.5598, MinusLogProbMetric: 44.5598, val_loss: 44.7637, val_MinusLogProbMetric: 44.7637

Epoch 586: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.5598 - MinusLogProbMetric: 44.5598 - val_loss: 44.7637 - val_MinusLogProbMetric: 44.7637 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 587/1000
2023-10-28 23:14:30.161 
Epoch 587/1000 
	 loss: 44.5493, MinusLogProbMetric: 44.5493, val_loss: 45.2100, val_MinusLogProbMetric: 45.2100

Epoch 587: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.5493 - MinusLogProbMetric: 44.5493 - val_loss: 45.2100 - val_MinusLogProbMetric: 45.2100 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 588/1000
2023-10-28 23:15:04.637 
Epoch 588/1000 
	 loss: 44.6998, MinusLogProbMetric: 44.6998, val_loss: 45.1557, val_MinusLogProbMetric: 45.1557

Epoch 588: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.6998 - MinusLogProbMetric: 44.6998 - val_loss: 45.1557 - val_MinusLogProbMetric: 45.1557 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 589/1000
2023-10-28 23:15:38.855 
Epoch 589/1000 
	 loss: 44.7415, MinusLogProbMetric: 44.7415, val_loss: 45.2424, val_MinusLogProbMetric: 45.2424

Epoch 589: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.7415 - MinusLogProbMetric: 44.7415 - val_loss: 45.2424 - val_MinusLogProbMetric: 45.2424 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 590/1000
2023-10-28 23:16:13.006 
Epoch 590/1000 
	 loss: 44.5818, MinusLogProbMetric: 44.5818, val_loss: 45.8647, val_MinusLogProbMetric: 45.8647

Epoch 590: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.5818 - MinusLogProbMetric: 44.5818 - val_loss: 45.8647 - val_MinusLogProbMetric: 45.8647 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 591/1000
2023-10-28 23:16:46.740 
Epoch 591/1000 
	 loss: 44.7251, MinusLogProbMetric: 44.7251, val_loss: 45.5535, val_MinusLogProbMetric: 45.5535

Epoch 591: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.7251 - MinusLogProbMetric: 44.7251 - val_loss: 45.5535 - val_MinusLogProbMetric: 45.5535 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 592/1000
2023-10-28 23:17:21.232 
Epoch 592/1000 
	 loss: 44.4866, MinusLogProbMetric: 44.4866, val_loss: 49.5201, val_MinusLogProbMetric: 49.5201

Epoch 592: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4866 - MinusLogProbMetric: 44.4866 - val_loss: 49.5201 - val_MinusLogProbMetric: 49.5201 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 593/1000
2023-10-28 23:17:55.534 
Epoch 593/1000 
	 loss: 44.8338, MinusLogProbMetric: 44.8338, val_loss: 45.0668, val_MinusLogProbMetric: 45.0668

Epoch 593: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.8338 - MinusLogProbMetric: 44.8338 - val_loss: 45.0668 - val_MinusLogProbMetric: 45.0668 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 594/1000
2023-10-28 23:18:30.242 
Epoch 594/1000 
	 loss: 44.8345, MinusLogProbMetric: 44.8345, val_loss: 46.6121, val_MinusLogProbMetric: 46.6121

Epoch 594: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.8345 - MinusLogProbMetric: 44.8345 - val_loss: 46.6121 - val_MinusLogProbMetric: 46.6121 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 595/1000
2023-10-28 23:19:04.986 
Epoch 595/1000 
	 loss: 44.6244, MinusLogProbMetric: 44.6244, val_loss: 44.8549, val_MinusLogProbMetric: 44.8549

Epoch 595: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.6244 - MinusLogProbMetric: 44.6244 - val_loss: 44.8549 - val_MinusLogProbMetric: 44.8549 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 596/1000
2023-10-28 23:19:39.450 
Epoch 596/1000 
	 loss: 44.6803, MinusLogProbMetric: 44.6803, val_loss: 44.9269, val_MinusLogProbMetric: 44.9269

Epoch 596: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.6803 - MinusLogProbMetric: 44.6803 - val_loss: 44.9269 - val_MinusLogProbMetric: 44.9269 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 597/1000
2023-10-28 23:20:13.980 
Epoch 597/1000 
	 loss: 44.4944, MinusLogProbMetric: 44.4944, val_loss: 46.7188, val_MinusLogProbMetric: 46.7188

Epoch 597: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.4944 - MinusLogProbMetric: 44.4944 - val_loss: 46.7188 - val_MinusLogProbMetric: 46.7188 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 598/1000
2023-10-28 23:20:48.467 
Epoch 598/1000 
	 loss: 44.7259, MinusLogProbMetric: 44.7259, val_loss: 45.2384, val_MinusLogProbMetric: 45.2384

Epoch 598: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.7259 - MinusLogProbMetric: 44.7259 - val_loss: 45.2384 - val_MinusLogProbMetric: 45.2384 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 599/1000
2023-10-28 23:21:22.970 
Epoch 599/1000 
	 loss: 44.5241, MinusLogProbMetric: 44.5241, val_loss: 45.2203, val_MinusLogProbMetric: 45.2203

Epoch 599: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.5241 - MinusLogProbMetric: 44.5241 - val_loss: 45.2203 - val_MinusLogProbMetric: 45.2203 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 600/1000
2023-10-28 23:21:56.933 
Epoch 600/1000 
	 loss: 44.4471, MinusLogProbMetric: 44.4471, val_loss: 44.7964, val_MinusLogProbMetric: 44.7964

Epoch 600: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4471 - MinusLogProbMetric: 44.4471 - val_loss: 44.7964 - val_MinusLogProbMetric: 44.7964 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 601/1000
2023-10-28 23:22:31.669 
Epoch 601/1000 
	 loss: 44.4621, MinusLogProbMetric: 44.4621, val_loss: 45.5298, val_MinusLogProbMetric: 45.5298

Epoch 601: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.4621 - MinusLogProbMetric: 44.4621 - val_loss: 45.5298 - val_MinusLogProbMetric: 45.5298 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 602/1000
2023-10-28 23:23:06.771 
Epoch 602/1000 
	 loss: 44.5077, MinusLogProbMetric: 44.5077, val_loss: 44.8940, val_MinusLogProbMetric: 44.8940

Epoch 602: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.5077 - MinusLogProbMetric: 44.5077 - val_loss: 44.8940 - val_MinusLogProbMetric: 44.8940 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 603/1000
2023-10-28 23:23:41.294 
Epoch 603/1000 
	 loss: 44.6388, MinusLogProbMetric: 44.6388, val_loss: 46.0083, val_MinusLogProbMetric: 46.0083

Epoch 603: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.6388 - MinusLogProbMetric: 44.6388 - val_loss: 46.0083 - val_MinusLogProbMetric: 46.0083 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 604/1000
2023-10-28 23:24:15.785 
Epoch 604/1000 
	 loss: 44.4724, MinusLogProbMetric: 44.4724, val_loss: 45.6074, val_MinusLogProbMetric: 45.6074

Epoch 604: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4724 - MinusLogProbMetric: 44.4724 - val_loss: 45.6074 - val_MinusLogProbMetric: 45.6074 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 605/1000
2023-10-28 23:24:50.330 
Epoch 605/1000 
	 loss: 44.3993, MinusLogProbMetric: 44.3993, val_loss: 44.9581, val_MinusLogProbMetric: 44.9581

Epoch 605: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.3993 - MinusLogProbMetric: 44.3993 - val_loss: 44.9581 - val_MinusLogProbMetric: 44.9581 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 606/1000
2023-10-28 23:25:24.339 
Epoch 606/1000 
	 loss: 44.5602, MinusLogProbMetric: 44.5602, val_loss: 45.0297, val_MinusLogProbMetric: 45.0297

Epoch 606: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.5602 - MinusLogProbMetric: 44.5602 - val_loss: 45.0297 - val_MinusLogProbMetric: 45.0297 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 607/1000
2023-10-28 23:25:59.105 
Epoch 607/1000 
	 loss: 44.7825, MinusLogProbMetric: 44.7825, val_loss: 44.7188, val_MinusLogProbMetric: 44.7188

Epoch 607: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.7825 - MinusLogProbMetric: 44.7825 - val_loss: 44.7188 - val_MinusLogProbMetric: 44.7188 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 608/1000
2023-10-28 23:26:33.854 
Epoch 608/1000 
	 loss: 44.4330, MinusLogProbMetric: 44.4330, val_loss: 46.2942, val_MinusLogProbMetric: 46.2942

Epoch 608: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.4330 - MinusLogProbMetric: 44.4330 - val_loss: 46.2942 - val_MinusLogProbMetric: 46.2942 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 609/1000
2023-10-28 23:27:08.138 
Epoch 609/1000 
	 loss: 44.4803, MinusLogProbMetric: 44.4803, val_loss: 45.9456, val_MinusLogProbMetric: 45.9456

Epoch 609: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4803 - MinusLogProbMetric: 44.4803 - val_loss: 45.9456 - val_MinusLogProbMetric: 45.9456 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 610/1000
2023-10-28 23:27:42.507 
Epoch 610/1000 
	 loss: 44.6116, MinusLogProbMetric: 44.6116, val_loss: 47.6777, val_MinusLogProbMetric: 47.6777

Epoch 610: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.6116 - MinusLogProbMetric: 44.6116 - val_loss: 47.6777 - val_MinusLogProbMetric: 47.6777 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 611/1000
2023-10-28 23:28:16.248 
Epoch 611/1000 
	 loss: 44.6662, MinusLogProbMetric: 44.6662, val_loss: 45.2106, val_MinusLogProbMetric: 45.2106

Epoch 611: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.6662 - MinusLogProbMetric: 44.6662 - val_loss: 45.2106 - val_MinusLogProbMetric: 45.2106 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 612/1000
2023-10-28 23:28:50.377 
Epoch 612/1000 
	 loss: 44.6970, MinusLogProbMetric: 44.6970, val_loss: 44.5966, val_MinusLogProbMetric: 44.5966

Epoch 612: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.6970 - MinusLogProbMetric: 44.6970 - val_loss: 44.5966 - val_MinusLogProbMetric: 44.5966 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 613/1000
2023-10-28 23:29:24.335 
Epoch 613/1000 
	 loss: 44.4310, MinusLogProbMetric: 44.4310, val_loss: 45.9428, val_MinusLogProbMetric: 45.9428

Epoch 613: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4310 - MinusLogProbMetric: 44.4310 - val_loss: 45.9428 - val_MinusLogProbMetric: 45.9428 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 614/1000
2023-10-28 23:29:58.682 
Epoch 614/1000 
	 loss: 44.3283, MinusLogProbMetric: 44.3283, val_loss: 45.2980, val_MinusLogProbMetric: 45.2980

Epoch 614: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.3283 - MinusLogProbMetric: 44.3283 - val_loss: 45.2980 - val_MinusLogProbMetric: 45.2980 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 615/1000
2023-10-28 23:30:32.886 
Epoch 615/1000 
	 loss: 44.4757, MinusLogProbMetric: 44.4757, val_loss: 44.9384, val_MinusLogProbMetric: 44.9384

Epoch 615: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4757 - MinusLogProbMetric: 44.4757 - val_loss: 44.9384 - val_MinusLogProbMetric: 44.9384 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 616/1000
2023-10-28 23:31:07.129 
Epoch 616/1000 
	 loss: 44.2704, MinusLogProbMetric: 44.2704, val_loss: 44.7094, val_MinusLogProbMetric: 44.7094

Epoch 616: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.2704 - MinusLogProbMetric: 44.2704 - val_loss: 44.7094 - val_MinusLogProbMetric: 44.7094 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 617/1000
2023-10-28 23:31:41.312 
Epoch 617/1000 
	 loss: 44.7529, MinusLogProbMetric: 44.7529, val_loss: 44.8162, val_MinusLogProbMetric: 44.8162

Epoch 617: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.7529 - MinusLogProbMetric: 44.7529 - val_loss: 44.8162 - val_MinusLogProbMetric: 44.8162 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 618/1000
2023-10-28 23:32:15.691 
Epoch 618/1000 
	 loss: 44.6521, MinusLogProbMetric: 44.6521, val_loss: 46.7561, val_MinusLogProbMetric: 46.7561

Epoch 618: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.6521 - MinusLogProbMetric: 44.6521 - val_loss: 46.7561 - val_MinusLogProbMetric: 46.7561 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 619/1000
2023-10-28 23:32:50.142 
Epoch 619/1000 
	 loss: 44.5171, MinusLogProbMetric: 44.5171, val_loss: 45.2893, val_MinusLogProbMetric: 45.2893

Epoch 619: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.5171 - MinusLogProbMetric: 44.5171 - val_loss: 45.2893 - val_MinusLogProbMetric: 45.2893 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 620/1000
2023-10-28 23:33:24.575 
Epoch 620/1000 
	 loss: 44.4028, MinusLogProbMetric: 44.4028, val_loss: 45.1580, val_MinusLogProbMetric: 45.1580

Epoch 620: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4028 - MinusLogProbMetric: 44.4028 - val_loss: 45.1580 - val_MinusLogProbMetric: 45.1580 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 621/1000
2023-10-28 23:33:59.657 
Epoch 621/1000 
	 loss: 44.6657, MinusLogProbMetric: 44.6657, val_loss: 47.6950, val_MinusLogProbMetric: 47.6950

Epoch 621: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.6657 - MinusLogProbMetric: 44.6657 - val_loss: 47.6950 - val_MinusLogProbMetric: 47.6950 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 622/1000
2023-10-28 23:34:33.699 
Epoch 622/1000 
	 loss: 44.4614, MinusLogProbMetric: 44.4614, val_loss: 45.1868, val_MinusLogProbMetric: 45.1868

Epoch 622: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4614 - MinusLogProbMetric: 44.4614 - val_loss: 45.1868 - val_MinusLogProbMetric: 45.1868 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 623/1000
2023-10-28 23:35:08.205 
Epoch 623/1000 
	 loss: 44.4482, MinusLogProbMetric: 44.4482, val_loss: 44.9850, val_MinusLogProbMetric: 44.9850

Epoch 623: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.4482 - MinusLogProbMetric: 44.4482 - val_loss: 44.9850 - val_MinusLogProbMetric: 44.9850 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 624/1000
2023-10-28 23:35:42.426 
Epoch 624/1000 
	 loss: 44.5228, MinusLogProbMetric: 44.5228, val_loss: 44.6906, val_MinusLogProbMetric: 44.6906

Epoch 624: val_loss did not improve from 44.45004
196/196 - 34s - loss: 44.5228 - MinusLogProbMetric: 44.5228 - val_loss: 44.6906 - val_MinusLogProbMetric: 44.6906 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 625/1000
2023-10-28 23:36:17.273 
Epoch 625/1000 
	 loss: 44.4177, MinusLogProbMetric: 44.4177, val_loss: 44.5268, val_MinusLogProbMetric: 44.5268

Epoch 625: val_loss did not improve from 44.45004
196/196 - 35s - loss: 44.4177 - MinusLogProbMetric: 44.4177 - val_loss: 44.5268 - val_MinusLogProbMetric: 44.5268 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 626/1000
2023-10-28 23:36:51.636 
Epoch 626/1000 
	 loss: 44.5948, MinusLogProbMetric: 44.5948, val_loss: 44.4404, val_MinusLogProbMetric: 44.4404

Epoch 626: val_loss improved from 44.45004 to 44.44041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 44.5948 - MinusLogProbMetric: 44.5948 - val_loss: 44.4404 - val_MinusLogProbMetric: 44.4404 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 627/1000
2023-10-28 23:37:26.366 
Epoch 627/1000 
	 loss: 44.3149, MinusLogProbMetric: 44.3149, val_loss: 45.2422, val_MinusLogProbMetric: 45.2422

Epoch 627: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.3149 - MinusLogProbMetric: 44.3149 - val_loss: 45.2422 - val_MinusLogProbMetric: 45.2422 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 628/1000
2023-10-28 23:38:00.863 
Epoch 628/1000 
	 loss: 44.5545, MinusLogProbMetric: 44.5545, val_loss: 45.0539, val_MinusLogProbMetric: 45.0539

Epoch 628: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.5545 - MinusLogProbMetric: 44.5545 - val_loss: 45.0539 - val_MinusLogProbMetric: 45.0539 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 629/1000
2023-10-28 23:38:35.326 
Epoch 629/1000 
	 loss: 44.5783, MinusLogProbMetric: 44.5783, val_loss: 45.0180, val_MinusLogProbMetric: 45.0180

Epoch 629: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.5783 - MinusLogProbMetric: 44.5783 - val_loss: 45.0180 - val_MinusLogProbMetric: 45.0180 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 630/1000
2023-10-28 23:39:09.582 
Epoch 630/1000 
	 loss: 44.4314, MinusLogProbMetric: 44.4314, val_loss: 46.3391, val_MinusLogProbMetric: 46.3391

Epoch 630: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.4314 - MinusLogProbMetric: 44.4314 - val_loss: 46.3391 - val_MinusLogProbMetric: 46.3391 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 631/1000
2023-10-28 23:39:43.563 
Epoch 631/1000 
	 loss: 44.2754, MinusLogProbMetric: 44.2754, val_loss: 44.7724, val_MinusLogProbMetric: 44.7724

Epoch 631: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.2754 - MinusLogProbMetric: 44.2754 - val_loss: 44.7724 - val_MinusLogProbMetric: 44.7724 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 632/1000
2023-10-28 23:40:17.560 
Epoch 632/1000 
	 loss: 44.4371, MinusLogProbMetric: 44.4371, val_loss: 46.6791, val_MinusLogProbMetric: 46.6791

Epoch 632: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.4371 - MinusLogProbMetric: 44.4371 - val_loss: 46.6791 - val_MinusLogProbMetric: 46.6791 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 633/1000
2023-10-28 23:40:52.012 
Epoch 633/1000 
	 loss: 44.5067, MinusLogProbMetric: 44.5067, val_loss: 46.5748, val_MinusLogProbMetric: 46.5748

Epoch 633: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.5067 - MinusLogProbMetric: 44.5067 - val_loss: 46.5748 - val_MinusLogProbMetric: 46.5748 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 634/1000
2023-10-28 23:41:26.582 
Epoch 634/1000 
	 loss: 44.2780, MinusLogProbMetric: 44.2780, val_loss: 49.9964, val_MinusLogProbMetric: 49.9964

Epoch 634: val_loss did not improve from 44.44041
196/196 - 35s - loss: 44.2780 - MinusLogProbMetric: 44.2780 - val_loss: 49.9964 - val_MinusLogProbMetric: 49.9964 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 635/1000
2023-10-28 23:42:00.797 
Epoch 635/1000 
	 loss: 44.6966, MinusLogProbMetric: 44.6966, val_loss: 44.7340, val_MinusLogProbMetric: 44.7340

Epoch 635: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.6966 - MinusLogProbMetric: 44.6966 - val_loss: 44.7340 - val_MinusLogProbMetric: 44.7340 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 636/1000
2023-10-28 23:42:35.257 
Epoch 636/1000 
	 loss: 44.6435, MinusLogProbMetric: 44.6435, val_loss: 45.7928, val_MinusLogProbMetric: 45.7928

Epoch 636: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.6435 - MinusLogProbMetric: 44.6435 - val_loss: 45.7928 - val_MinusLogProbMetric: 45.7928 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 637/1000
2023-10-28 23:43:09.669 
Epoch 637/1000 
	 loss: 44.6801, MinusLogProbMetric: 44.6801, val_loss: 44.9725, val_MinusLogProbMetric: 44.9725

Epoch 637: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.6801 - MinusLogProbMetric: 44.6801 - val_loss: 44.9725 - val_MinusLogProbMetric: 44.9725 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 638/1000
2023-10-28 23:43:44.071 
Epoch 638/1000 
	 loss: 44.5975, MinusLogProbMetric: 44.5975, val_loss: 45.8791, val_MinusLogProbMetric: 45.8791

Epoch 638: val_loss did not improve from 44.44041
196/196 - 34s - loss: 44.5975 - MinusLogProbMetric: 44.5975 - val_loss: 45.8791 - val_MinusLogProbMetric: 45.8791 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 639/1000
2023-10-28 23:44:18.414 
Epoch 639/1000 
	 loss: 44.5922, MinusLogProbMetric: 44.5922, val_loss: 44.1780, val_MinusLogProbMetric: 44.1780

Epoch 639: val_loss improved from 44.44041 to 44.17795, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 44.5922 - MinusLogProbMetric: 44.5922 - val_loss: 44.1780 - val_MinusLogProbMetric: 44.1780 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 640/1000
2023-10-28 23:44:52.914 
Epoch 640/1000 
	 loss: 44.5768, MinusLogProbMetric: 44.5768, val_loss: 45.1939, val_MinusLogProbMetric: 45.1939

Epoch 640: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5768 - MinusLogProbMetric: 44.5768 - val_loss: 45.1939 - val_MinusLogProbMetric: 45.1939 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 641/1000
2023-10-28 23:45:27.050 
Epoch 641/1000 
	 loss: 44.8153, MinusLogProbMetric: 44.8153, val_loss: 45.1960, val_MinusLogProbMetric: 45.1960

Epoch 641: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.8153 - MinusLogProbMetric: 44.8153 - val_loss: 45.1960 - val_MinusLogProbMetric: 45.1960 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 642/1000
2023-10-28 23:46:01.545 
Epoch 642/1000 
	 loss: 44.3440, MinusLogProbMetric: 44.3440, val_loss: 44.8767, val_MinusLogProbMetric: 44.8767

Epoch 642: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.3440 - MinusLogProbMetric: 44.3440 - val_loss: 44.8767 - val_MinusLogProbMetric: 44.8767 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 643/1000
2023-10-28 23:46:36.126 
Epoch 643/1000 
	 loss: 44.6743, MinusLogProbMetric: 44.6743, val_loss: 45.2616, val_MinusLogProbMetric: 45.2616

Epoch 643: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.6743 - MinusLogProbMetric: 44.6743 - val_loss: 45.2616 - val_MinusLogProbMetric: 45.2616 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 644/1000
2023-10-28 23:47:10.775 
Epoch 644/1000 
	 loss: 44.3714, MinusLogProbMetric: 44.3714, val_loss: 44.5365, val_MinusLogProbMetric: 44.5365

Epoch 644: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.3714 - MinusLogProbMetric: 44.3714 - val_loss: 44.5365 - val_MinusLogProbMetric: 44.5365 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 645/1000
2023-10-28 23:47:45.398 
Epoch 645/1000 
	 loss: 44.6052, MinusLogProbMetric: 44.6052, val_loss: 44.4913, val_MinusLogProbMetric: 44.4913

Epoch 645: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.6052 - MinusLogProbMetric: 44.6052 - val_loss: 44.4913 - val_MinusLogProbMetric: 44.4913 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 646/1000
2023-10-28 23:48:20.099 
Epoch 646/1000 
	 loss: 44.3339, MinusLogProbMetric: 44.3339, val_loss: 44.8331, val_MinusLogProbMetric: 44.8331

Epoch 646: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.3339 - MinusLogProbMetric: 44.3339 - val_loss: 44.8331 - val_MinusLogProbMetric: 44.8331 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 647/1000
2023-10-28 23:48:54.417 
Epoch 647/1000 
	 loss: 44.4294, MinusLogProbMetric: 44.4294, val_loss: 46.2148, val_MinusLogProbMetric: 46.2148

Epoch 647: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4294 - MinusLogProbMetric: 44.4294 - val_loss: 46.2148 - val_MinusLogProbMetric: 46.2148 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 648/1000
2023-10-28 23:49:28.843 
Epoch 648/1000 
	 loss: 44.6531, MinusLogProbMetric: 44.6531, val_loss: 44.8914, val_MinusLogProbMetric: 44.8914

Epoch 648: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.6531 - MinusLogProbMetric: 44.6531 - val_loss: 44.8914 - val_MinusLogProbMetric: 44.8914 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 649/1000
2023-10-28 23:50:03.009 
Epoch 649/1000 
	 loss: 44.3426, MinusLogProbMetric: 44.3426, val_loss: 45.0766, val_MinusLogProbMetric: 45.0766

Epoch 649: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.3426 - MinusLogProbMetric: 44.3426 - val_loss: 45.0766 - val_MinusLogProbMetric: 45.0766 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 650/1000
2023-10-28 23:50:37.216 
Epoch 650/1000 
	 loss: 44.4081, MinusLogProbMetric: 44.4081, val_loss: 44.8341, val_MinusLogProbMetric: 44.8341

Epoch 650: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4081 - MinusLogProbMetric: 44.4081 - val_loss: 44.8341 - val_MinusLogProbMetric: 44.8341 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 651/1000
2023-10-28 23:51:11.844 
Epoch 651/1000 
	 loss: 44.2900, MinusLogProbMetric: 44.2900, val_loss: 44.6225, val_MinusLogProbMetric: 44.6225

Epoch 651: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.2900 - MinusLogProbMetric: 44.2900 - val_loss: 44.6225 - val_MinusLogProbMetric: 44.6225 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 652/1000
2023-10-28 23:51:46.392 
Epoch 652/1000 
	 loss: 44.2674, MinusLogProbMetric: 44.2674, val_loss: 44.5933, val_MinusLogProbMetric: 44.5933

Epoch 652: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.2674 - MinusLogProbMetric: 44.2674 - val_loss: 44.5933 - val_MinusLogProbMetric: 44.5933 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 653/1000
2023-10-28 23:52:20.664 
Epoch 653/1000 
	 loss: 44.3674, MinusLogProbMetric: 44.3674, val_loss: 45.8523, val_MinusLogProbMetric: 45.8523

Epoch 653: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.3674 - MinusLogProbMetric: 44.3674 - val_loss: 45.8523 - val_MinusLogProbMetric: 45.8523 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 654/1000
2023-10-28 23:52:54.625 
Epoch 654/1000 
	 loss: 44.6504, MinusLogProbMetric: 44.6504, val_loss: 45.6362, val_MinusLogProbMetric: 45.6362

Epoch 654: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.6504 - MinusLogProbMetric: 44.6504 - val_loss: 45.6362 - val_MinusLogProbMetric: 45.6362 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 655/1000
2023-10-28 23:53:29.400 
Epoch 655/1000 
	 loss: 44.4055, MinusLogProbMetric: 44.4055, val_loss: 45.0647, val_MinusLogProbMetric: 45.0647

Epoch 655: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.4055 - MinusLogProbMetric: 44.4055 - val_loss: 45.0647 - val_MinusLogProbMetric: 45.0647 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 656/1000
2023-10-28 23:54:03.882 
Epoch 656/1000 
	 loss: 44.8507, MinusLogProbMetric: 44.8507, val_loss: 45.1279, val_MinusLogProbMetric: 45.1279

Epoch 656: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.8507 - MinusLogProbMetric: 44.8507 - val_loss: 45.1279 - val_MinusLogProbMetric: 45.1279 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 657/1000
2023-10-28 23:54:38.290 
Epoch 657/1000 
	 loss: 44.4439, MinusLogProbMetric: 44.4439, val_loss: 44.8412, val_MinusLogProbMetric: 44.8412

Epoch 657: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4439 - MinusLogProbMetric: 44.4439 - val_loss: 44.8412 - val_MinusLogProbMetric: 44.8412 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 658/1000
2023-10-28 23:55:12.243 
Epoch 658/1000 
	 loss: 44.3522, MinusLogProbMetric: 44.3522, val_loss: 45.9694, val_MinusLogProbMetric: 45.9694

Epoch 658: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.3522 - MinusLogProbMetric: 44.3522 - val_loss: 45.9694 - val_MinusLogProbMetric: 45.9694 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 659/1000
2023-10-28 23:55:46.640 
Epoch 659/1000 
	 loss: 44.5315, MinusLogProbMetric: 44.5315, val_loss: 45.4351, val_MinusLogProbMetric: 45.4351

Epoch 659: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5315 - MinusLogProbMetric: 44.5315 - val_loss: 45.4351 - val_MinusLogProbMetric: 45.4351 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 660/1000
2023-10-28 23:56:21.065 
Epoch 660/1000 
	 loss: 44.2431, MinusLogProbMetric: 44.2431, val_loss: 46.2826, val_MinusLogProbMetric: 46.2826

Epoch 660: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.2431 - MinusLogProbMetric: 44.2431 - val_loss: 46.2826 - val_MinusLogProbMetric: 46.2826 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 661/1000
2023-10-28 23:56:55.018 
Epoch 661/1000 
	 loss: 44.7103, MinusLogProbMetric: 44.7103, val_loss: 44.8453, val_MinusLogProbMetric: 44.8453

Epoch 661: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.7103 - MinusLogProbMetric: 44.7103 - val_loss: 44.8453 - val_MinusLogProbMetric: 44.8453 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 662/1000
2023-10-28 23:57:29.480 
Epoch 662/1000 
	 loss: 44.5461, MinusLogProbMetric: 44.5461, val_loss: 44.6360, val_MinusLogProbMetric: 44.6360

Epoch 662: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5461 - MinusLogProbMetric: 44.5461 - val_loss: 44.6360 - val_MinusLogProbMetric: 44.6360 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 663/1000
2023-10-28 23:58:03.605 
Epoch 663/1000 
	 loss: 44.4920, MinusLogProbMetric: 44.4920, val_loss: 45.1567, val_MinusLogProbMetric: 45.1567

Epoch 663: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4920 - MinusLogProbMetric: 44.4920 - val_loss: 45.1567 - val_MinusLogProbMetric: 45.1567 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 664/1000
2023-10-28 23:58:37.778 
Epoch 664/1000 
	 loss: 44.5587, MinusLogProbMetric: 44.5587, val_loss: 44.9722, val_MinusLogProbMetric: 44.9722

Epoch 664: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5587 - MinusLogProbMetric: 44.5587 - val_loss: 44.9722 - val_MinusLogProbMetric: 44.9722 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 665/1000
2023-10-28 23:59:12.079 
Epoch 665/1000 
	 loss: 44.3907, MinusLogProbMetric: 44.3907, val_loss: 44.6819, val_MinusLogProbMetric: 44.6819

Epoch 665: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.3907 - MinusLogProbMetric: 44.3907 - val_loss: 44.6819 - val_MinusLogProbMetric: 44.6819 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 666/1000
2023-10-28 23:59:46.309 
Epoch 666/1000 
	 loss: 44.7342, MinusLogProbMetric: 44.7342, val_loss: 46.5288, val_MinusLogProbMetric: 46.5288

Epoch 666: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.7342 - MinusLogProbMetric: 44.7342 - val_loss: 46.5288 - val_MinusLogProbMetric: 46.5288 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 667/1000
2023-10-29 00:00:20.813 
Epoch 667/1000 
	 loss: 44.2790, MinusLogProbMetric: 44.2790, val_loss: 44.9172, val_MinusLogProbMetric: 44.9172

Epoch 667: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.2790 - MinusLogProbMetric: 44.2790 - val_loss: 44.9172 - val_MinusLogProbMetric: 44.9172 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 668/1000
2023-10-29 00:00:55.157 
Epoch 668/1000 
	 loss: 44.5611, MinusLogProbMetric: 44.5611, val_loss: 44.7361, val_MinusLogProbMetric: 44.7361

Epoch 668: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5611 - MinusLogProbMetric: 44.5611 - val_loss: 44.7361 - val_MinusLogProbMetric: 44.7361 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 669/1000
2023-10-29 00:01:29.352 
Epoch 669/1000 
	 loss: 44.2081, MinusLogProbMetric: 44.2081, val_loss: 44.7047, val_MinusLogProbMetric: 44.7047

Epoch 669: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.2081 - MinusLogProbMetric: 44.2081 - val_loss: 44.7047 - val_MinusLogProbMetric: 44.7047 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 670/1000
2023-10-29 00:02:02.688 
Epoch 670/1000 
	 loss: 44.5754, MinusLogProbMetric: 44.5754, val_loss: 44.8251, val_MinusLogProbMetric: 44.8251

Epoch 670: val_loss did not improve from 44.17795
196/196 - 33s - loss: 44.5754 - MinusLogProbMetric: 44.5754 - val_loss: 44.8251 - val_MinusLogProbMetric: 44.8251 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 671/1000
2023-10-29 00:02:36.193 
Epoch 671/1000 
	 loss: 44.3888, MinusLogProbMetric: 44.3888, val_loss: 45.1643, val_MinusLogProbMetric: 45.1643

Epoch 671: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.3888 - MinusLogProbMetric: 44.3888 - val_loss: 45.1643 - val_MinusLogProbMetric: 45.1643 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 672/1000
2023-10-29 00:03:09.972 
Epoch 672/1000 
	 loss: 44.4582, MinusLogProbMetric: 44.4582, val_loss: 45.8426, val_MinusLogProbMetric: 45.8426

Epoch 672: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4582 - MinusLogProbMetric: 44.4582 - val_loss: 45.8426 - val_MinusLogProbMetric: 45.8426 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 673/1000
2023-10-29 00:03:44.198 
Epoch 673/1000 
	 loss: 44.5626, MinusLogProbMetric: 44.5626, val_loss: 45.6727, val_MinusLogProbMetric: 45.6727

Epoch 673: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5626 - MinusLogProbMetric: 44.5626 - val_loss: 45.6727 - val_MinusLogProbMetric: 45.6727 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 674/1000
2023-10-29 00:04:18.671 
Epoch 674/1000 
	 loss: 44.5493, MinusLogProbMetric: 44.5493, val_loss: 45.1091, val_MinusLogProbMetric: 45.1091

Epoch 674: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5493 - MinusLogProbMetric: 44.5493 - val_loss: 45.1091 - val_MinusLogProbMetric: 45.1091 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 675/1000
2023-10-29 00:04:53.126 
Epoch 675/1000 
	 loss: 44.0888, MinusLogProbMetric: 44.0888, val_loss: 44.5844, val_MinusLogProbMetric: 44.5844

Epoch 675: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.0888 - MinusLogProbMetric: 44.0888 - val_loss: 44.5844 - val_MinusLogProbMetric: 44.5844 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 676/1000
2023-10-29 00:05:27.000 
Epoch 676/1000 
	 loss: 44.2887, MinusLogProbMetric: 44.2887, val_loss: 45.9195, val_MinusLogProbMetric: 45.9195

Epoch 676: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.2887 - MinusLogProbMetric: 44.2887 - val_loss: 45.9195 - val_MinusLogProbMetric: 45.9195 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 677/1000
2023-10-29 00:06:00.838 
Epoch 677/1000 
	 loss: 44.4332, MinusLogProbMetric: 44.4332, val_loss: 44.5931, val_MinusLogProbMetric: 44.5931

Epoch 677: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4332 - MinusLogProbMetric: 44.4332 - val_loss: 44.5931 - val_MinusLogProbMetric: 44.5931 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 678/1000
2023-10-29 00:06:34.579 
Epoch 678/1000 
	 loss: 44.7091, MinusLogProbMetric: 44.7091, val_loss: 45.1654, val_MinusLogProbMetric: 45.1654

Epoch 678: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.7091 - MinusLogProbMetric: 44.7091 - val_loss: 45.1654 - val_MinusLogProbMetric: 45.1654 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 679/1000
2023-10-29 00:07:08.848 
Epoch 679/1000 
	 loss: 44.4837, MinusLogProbMetric: 44.4837, val_loss: 45.4994, val_MinusLogProbMetric: 45.4994

Epoch 679: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4837 - MinusLogProbMetric: 44.4837 - val_loss: 45.4994 - val_MinusLogProbMetric: 45.4994 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 680/1000
2023-10-29 00:07:42.881 
Epoch 680/1000 
	 loss: 44.4106, MinusLogProbMetric: 44.4106, val_loss: 44.3961, val_MinusLogProbMetric: 44.3961

Epoch 680: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4106 - MinusLogProbMetric: 44.4106 - val_loss: 44.3961 - val_MinusLogProbMetric: 44.3961 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 681/1000
2023-10-29 00:08:17.570 
Epoch 681/1000 
	 loss: 44.0457, MinusLogProbMetric: 44.0457, val_loss: 44.9268, val_MinusLogProbMetric: 44.9268

Epoch 681: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.0457 - MinusLogProbMetric: 44.0457 - val_loss: 44.9268 - val_MinusLogProbMetric: 44.9268 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 682/1000
2023-10-29 00:08:52.048 
Epoch 682/1000 
	 loss: 44.5889, MinusLogProbMetric: 44.5889, val_loss: 44.7317, val_MinusLogProbMetric: 44.7317

Epoch 682: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5889 - MinusLogProbMetric: 44.5889 - val_loss: 44.7317 - val_MinusLogProbMetric: 44.7317 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 683/1000
2023-10-29 00:09:26.353 
Epoch 683/1000 
	 loss: 44.3811, MinusLogProbMetric: 44.3811, val_loss: 44.2351, val_MinusLogProbMetric: 44.2351

Epoch 683: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.3811 - MinusLogProbMetric: 44.3811 - val_loss: 44.2351 - val_MinusLogProbMetric: 44.2351 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 684/1000
2023-10-29 00:10:00.506 
Epoch 684/1000 
	 loss: 44.5073, MinusLogProbMetric: 44.5073, val_loss: 45.0250, val_MinusLogProbMetric: 45.0250

Epoch 684: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.5073 - MinusLogProbMetric: 44.5073 - val_loss: 45.0250 - val_MinusLogProbMetric: 45.0250 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 685/1000
2023-10-29 00:10:35.075 
Epoch 685/1000 
	 loss: 44.2029, MinusLogProbMetric: 44.2029, val_loss: 46.9311, val_MinusLogProbMetric: 46.9311

Epoch 685: val_loss did not improve from 44.17795
196/196 - 35s - loss: 44.2029 - MinusLogProbMetric: 44.2029 - val_loss: 46.9311 - val_MinusLogProbMetric: 46.9311 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 686/1000
2023-10-29 00:11:09.062 
Epoch 686/1000 
	 loss: 44.2959, MinusLogProbMetric: 44.2959, val_loss: 45.2382, val_MinusLogProbMetric: 45.2382

Epoch 686: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.2959 - MinusLogProbMetric: 44.2959 - val_loss: 45.2382 - val_MinusLogProbMetric: 45.2382 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 687/1000
2023-10-29 00:11:42.970 
Epoch 687/1000 
	 loss: 44.6681, MinusLogProbMetric: 44.6681, val_loss: 45.0192, val_MinusLogProbMetric: 45.0192

Epoch 687: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.6681 - MinusLogProbMetric: 44.6681 - val_loss: 45.0192 - val_MinusLogProbMetric: 45.0192 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 688/1000
2023-10-29 00:12:17.124 
Epoch 688/1000 
	 loss: 44.4092, MinusLogProbMetric: 44.4092, val_loss: 46.2895, val_MinusLogProbMetric: 46.2895

Epoch 688: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.4092 - MinusLogProbMetric: 44.4092 - val_loss: 46.2895 - val_MinusLogProbMetric: 46.2895 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 689/1000
2023-10-29 00:12:51.020 
Epoch 689/1000 
	 loss: 44.3987, MinusLogProbMetric: 44.3987, val_loss: 45.0399, val_MinusLogProbMetric: 45.0399

Epoch 689: val_loss did not improve from 44.17795
196/196 - 34s - loss: 44.3987 - MinusLogProbMetric: 44.3987 - val_loss: 45.0399 - val_MinusLogProbMetric: 45.0399 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 690/1000
2023-10-29 00:13:24.838 
Epoch 690/1000 
	 loss: 42.8281, MinusLogProbMetric: 42.8281, val_loss: 43.5098, val_MinusLogProbMetric: 43.5098

Epoch 690: val_loss improved from 44.17795 to 43.50977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 42.8281 - MinusLogProbMetric: 42.8281 - val_loss: 43.5098 - val_MinusLogProbMetric: 43.5098 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 691/1000
2023-10-29 00:13:59.480 
Epoch 691/1000 
	 loss: 42.7687, MinusLogProbMetric: 42.7687, val_loss: 44.1962, val_MinusLogProbMetric: 44.1962

Epoch 691: val_loss did not improve from 43.50977
196/196 - 34s - loss: 42.7687 - MinusLogProbMetric: 42.7687 - val_loss: 44.1962 - val_MinusLogProbMetric: 44.1962 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 692/1000
2023-10-29 00:14:33.515 
Epoch 692/1000 
	 loss: 43.1046, MinusLogProbMetric: 43.1046, val_loss: 43.4861, val_MinusLogProbMetric: 43.4861

Epoch 692: val_loss improved from 43.50977 to 43.48606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 43.1046 - MinusLogProbMetric: 43.1046 - val_loss: 43.4861 - val_MinusLogProbMetric: 43.4861 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 693/1000
2023-10-29 00:15:07.817 
Epoch 693/1000 
	 loss: 42.8167, MinusLogProbMetric: 42.8167, val_loss: 43.5425, val_MinusLogProbMetric: 43.5425

Epoch 693: val_loss did not improve from 43.48606
196/196 - 34s - loss: 42.8167 - MinusLogProbMetric: 42.8167 - val_loss: 43.5425 - val_MinusLogProbMetric: 43.5425 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 694/1000
2023-10-29 00:15:41.105 
Epoch 694/1000 
	 loss: 43.0178, MinusLogProbMetric: 43.0178, val_loss: 43.6835, val_MinusLogProbMetric: 43.6835

Epoch 694: val_loss did not improve from 43.48606
196/196 - 33s - loss: 43.0178 - MinusLogProbMetric: 43.0178 - val_loss: 43.6835 - val_MinusLogProbMetric: 43.6835 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 695/1000
2023-10-29 00:16:14.250 
Epoch 695/1000 
	 loss: 43.1053, MinusLogProbMetric: 43.1053, val_loss: 43.8577, val_MinusLogProbMetric: 43.8577

Epoch 695: val_loss did not improve from 43.48606
196/196 - 33s - loss: 43.1053 - MinusLogProbMetric: 43.1053 - val_loss: 43.8577 - val_MinusLogProbMetric: 43.8577 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 696/1000
2023-10-29 00:16:47.637 
Epoch 696/1000 
	 loss: 42.9558, MinusLogProbMetric: 42.9558, val_loss: 43.5700, val_MinusLogProbMetric: 43.5700

Epoch 696: val_loss did not improve from 43.48606
196/196 - 33s - loss: 42.9558 - MinusLogProbMetric: 42.9558 - val_loss: 43.5700 - val_MinusLogProbMetric: 43.5700 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 697/1000
2023-10-29 00:17:20.719 
Epoch 697/1000 
	 loss: 43.0458, MinusLogProbMetric: 43.0458, val_loss: 44.5716, val_MinusLogProbMetric: 44.5716

Epoch 697: val_loss did not improve from 43.48606
196/196 - 33s - loss: 43.0458 - MinusLogProbMetric: 43.0458 - val_loss: 44.5716 - val_MinusLogProbMetric: 44.5716 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 698/1000
2023-10-29 00:17:53.635 
Epoch 698/1000 
	 loss: 42.9795, MinusLogProbMetric: 42.9795, val_loss: 43.4581, val_MinusLogProbMetric: 43.4581

Epoch 698: val_loss improved from 43.48606 to 43.45813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 33s - loss: 42.9795 - MinusLogProbMetric: 42.9795 - val_loss: 43.4581 - val_MinusLogProbMetric: 43.4581 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 699/1000
2023-10-29 00:18:27.242 
Epoch 699/1000 
	 loss: 42.8018, MinusLogProbMetric: 42.8018, val_loss: 43.6967, val_MinusLogProbMetric: 43.6967

Epoch 699: val_loss did not improve from 43.45813
196/196 - 33s - loss: 42.8018 - MinusLogProbMetric: 42.8018 - val_loss: 43.6967 - val_MinusLogProbMetric: 43.6967 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 700/1000
2023-10-29 00:19:00.022 
Epoch 700/1000 
	 loss: 42.9859, MinusLogProbMetric: 42.9859, val_loss: 43.5853, val_MinusLogProbMetric: 43.5853

Epoch 700: val_loss did not improve from 43.45813
196/196 - 33s - loss: 42.9859 - MinusLogProbMetric: 42.9859 - val_loss: 43.5853 - val_MinusLogProbMetric: 43.5853 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 701/1000
2023-10-29 00:19:32.302 
Epoch 701/1000 
	 loss: 43.4772, MinusLogProbMetric: 43.4772, val_loss: 44.4987, val_MinusLogProbMetric: 44.4987

Epoch 701: val_loss did not improve from 43.45813
196/196 - 32s - loss: 43.4772 - MinusLogProbMetric: 43.4772 - val_loss: 44.4987 - val_MinusLogProbMetric: 44.4987 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 702/1000
2023-10-29 00:20:05.670 
Epoch 702/1000 
	 loss: 42.7764, MinusLogProbMetric: 42.7764, val_loss: 43.6013, val_MinusLogProbMetric: 43.6013

Epoch 702: val_loss did not improve from 43.45813
196/196 - 33s - loss: 42.7764 - MinusLogProbMetric: 42.7764 - val_loss: 43.6013 - val_MinusLogProbMetric: 43.6013 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 703/1000
2023-10-29 00:20:39.351 
Epoch 703/1000 
	 loss: 42.8261, MinusLogProbMetric: 42.8261, val_loss: 44.0290, val_MinusLogProbMetric: 44.0290

Epoch 703: val_loss did not improve from 43.45813
196/196 - 34s - loss: 42.8261 - MinusLogProbMetric: 42.8261 - val_loss: 44.0290 - val_MinusLogProbMetric: 44.0290 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 704/1000
2023-10-29 00:21:12.555 
Epoch 704/1000 
	 loss: 42.8460, MinusLogProbMetric: 42.8460, val_loss: 43.9365, val_MinusLogProbMetric: 43.9365

Epoch 704: val_loss did not improve from 43.45813
196/196 - 33s - loss: 42.8460 - MinusLogProbMetric: 42.8460 - val_loss: 43.9365 - val_MinusLogProbMetric: 43.9365 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 705/1000
2023-10-29 00:21:46.790 
Epoch 705/1000 
	 loss: 43.0990, MinusLogProbMetric: 43.0990, val_loss: 44.0689, val_MinusLogProbMetric: 44.0689

Epoch 705: val_loss did not improve from 43.45813
196/196 - 34s - loss: 43.0990 - MinusLogProbMetric: 43.0990 - val_loss: 44.0689 - val_MinusLogProbMetric: 44.0689 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 706/1000
2023-10-29 00:22:20.384 
Epoch 706/1000 
	 loss: 42.9576, MinusLogProbMetric: 42.9576, val_loss: 44.5625, val_MinusLogProbMetric: 44.5625

Epoch 706: val_loss did not improve from 43.45813
196/196 - 34s - loss: 42.9576 - MinusLogProbMetric: 42.9576 - val_loss: 44.5625 - val_MinusLogProbMetric: 44.5625 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 707/1000
2023-10-29 00:22:53.702 
Epoch 707/1000 
	 loss: 42.8112, MinusLogProbMetric: 42.8112, val_loss: 43.7343, val_MinusLogProbMetric: 43.7343

Epoch 707: val_loss did not improve from 43.45813
196/196 - 33s - loss: 42.8112 - MinusLogProbMetric: 42.8112 - val_loss: 43.7343 - val_MinusLogProbMetric: 43.7343 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 708/1000
2023-10-29 00:23:27.086 
Epoch 708/1000 
	 loss: 43.0233, MinusLogProbMetric: 43.0233, val_loss: 45.4057, val_MinusLogProbMetric: 45.4057

Epoch 708: val_loss did not improve from 43.45813
196/196 - 33s - loss: 43.0233 - MinusLogProbMetric: 43.0233 - val_loss: 45.4057 - val_MinusLogProbMetric: 45.4057 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 709/1000
2023-10-29 00:24:00.068 
Epoch 709/1000 
	 loss: 43.0730, MinusLogProbMetric: 43.0730, val_loss: 44.3200, val_MinusLogProbMetric: 44.3200

Epoch 709: val_loss did not improve from 43.45813
196/196 - 33s - loss: 43.0730 - MinusLogProbMetric: 43.0730 - val_loss: 44.3200 - val_MinusLogProbMetric: 44.3200 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 710/1000
2023-10-29 00:24:34.509 
Epoch 710/1000 
	 loss: 42.9882, MinusLogProbMetric: 42.9882, val_loss: 45.6663, val_MinusLogProbMetric: 45.6663

Epoch 710: val_loss did not improve from 43.45813
196/196 - 34s - loss: 42.9882 - MinusLogProbMetric: 42.9882 - val_loss: 45.6663 - val_MinusLogProbMetric: 45.6663 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 711/1000
2023-10-29 00:25:09.457 
Epoch 711/1000 
	 loss: 42.9281, MinusLogProbMetric: 42.9281, val_loss: 44.6726, val_MinusLogProbMetric: 44.6726

Epoch 711: val_loss did not improve from 43.45813
196/196 - 35s - loss: 42.9281 - MinusLogProbMetric: 42.9281 - val_loss: 44.6726 - val_MinusLogProbMetric: 44.6726 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 712/1000
2023-10-29 00:25:43.858 
Epoch 712/1000 
	 loss: 43.0849, MinusLogProbMetric: 43.0849, val_loss: 43.7366, val_MinusLogProbMetric: 43.7366

Epoch 712: val_loss did not improve from 43.45813
196/196 - 34s - loss: 43.0849 - MinusLogProbMetric: 43.0849 - val_loss: 43.7366 - val_MinusLogProbMetric: 43.7366 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 713/1000
2023-10-29 00:26:18.215 
Epoch 713/1000 
	 loss: 42.9020, MinusLogProbMetric: 42.9020, val_loss: 43.8550, val_MinusLogProbMetric: 43.8550

Epoch 713: val_loss did not improve from 43.45813
196/196 - 34s - loss: 42.9020 - MinusLogProbMetric: 42.9020 - val_loss: 43.8550 - val_MinusLogProbMetric: 43.8550 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 714/1000
2023-10-29 00:26:52.882 
Epoch 714/1000 
	 loss: 42.8826, MinusLogProbMetric: 42.8826, val_loss: 43.8516, val_MinusLogProbMetric: 43.8516

Epoch 714: val_loss did not improve from 43.45813
196/196 - 35s - loss: 42.8826 - MinusLogProbMetric: 42.8826 - val_loss: 43.8516 - val_MinusLogProbMetric: 43.8516 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 715/1000
2023-10-29 00:27:27.115 
Epoch 715/1000 
	 loss: 43.1438, MinusLogProbMetric: 43.1438, val_loss: 43.4635, val_MinusLogProbMetric: 43.4635

Epoch 715: val_loss did not improve from 43.45813
196/196 - 34s - loss: 43.1438 - MinusLogProbMetric: 43.1438 - val_loss: 43.4635 - val_MinusLogProbMetric: 43.4635 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 716/1000
2023-10-29 00:28:01.415 
Epoch 716/1000 
	 loss: 43.0737, MinusLogProbMetric: 43.0737, val_loss: 44.1013, val_MinusLogProbMetric: 44.1013

Epoch 716: val_loss did not improve from 43.45813
196/196 - 34s - loss: 43.0737 - MinusLogProbMetric: 43.0737 - val_loss: 44.1013 - val_MinusLogProbMetric: 44.1013 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 717/1000
2023-10-29 00:28:36.099 
Epoch 717/1000 
	 loss: 42.8996, MinusLogProbMetric: 42.8996, val_loss: 44.2066, val_MinusLogProbMetric: 44.2066

Epoch 717: val_loss did not improve from 43.45813
196/196 - 35s - loss: 42.8996 - MinusLogProbMetric: 42.8996 - val_loss: 44.2066 - val_MinusLogProbMetric: 44.2066 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 718/1000
2023-10-29 00:29:10.612 
Epoch 718/1000 
	 loss: 43.0648, MinusLogProbMetric: 43.0648, val_loss: 45.6416, val_MinusLogProbMetric: 45.6416

Epoch 718: val_loss did not improve from 43.45813
196/196 - 35s - loss: 43.0648 - MinusLogProbMetric: 43.0648 - val_loss: 45.6416 - val_MinusLogProbMetric: 45.6416 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 719/1000
2023-10-29 00:29:45.300 
Epoch 719/1000 
	 loss: 42.8946, MinusLogProbMetric: 42.8946, val_loss: 43.8974, val_MinusLogProbMetric: 43.8974

Epoch 719: val_loss did not improve from 43.45813
196/196 - 35s - loss: 42.8946 - MinusLogProbMetric: 42.8946 - val_loss: 43.8974 - val_MinusLogProbMetric: 43.8974 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 720/1000
2023-10-29 00:30:19.798 
Epoch 720/1000 
	 loss: 42.9187, MinusLogProbMetric: 42.9187, val_loss: 43.4354, val_MinusLogProbMetric: 43.4354

Epoch 720: val_loss improved from 43.45813 to 43.43541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 42.9187 - MinusLogProbMetric: 42.9187 - val_loss: 43.4354 - val_MinusLogProbMetric: 43.4354 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 721/1000
2023-10-29 00:30:54.475 
Epoch 721/1000 
	 loss: 42.8837, MinusLogProbMetric: 42.8837, val_loss: 43.5854, val_MinusLogProbMetric: 43.5854

Epoch 721: val_loss did not improve from 43.43541
196/196 - 34s - loss: 42.8837 - MinusLogProbMetric: 42.8837 - val_loss: 43.5854 - val_MinusLogProbMetric: 43.5854 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 722/1000
2023-10-29 00:31:28.770 
Epoch 722/1000 
	 loss: 43.0088, MinusLogProbMetric: 43.0088, val_loss: 43.5703, val_MinusLogProbMetric: 43.5703

Epoch 722: val_loss did not improve from 43.43541
196/196 - 34s - loss: 43.0088 - MinusLogProbMetric: 43.0088 - val_loss: 43.5703 - val_MinusLogProbMetric: 43.5703 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 723/1000
2023-10-29 00:32:03.148 
Epoch 723/1000 
	 loss: 42.9842, MinusLogProbMetric: 42.9842, val_loss: 43.6581, val_MinusLogProbMetric: 43.6581

Epoch 723: val_loss did not improve from 43.43541
196/196 - 34s - loss: 42.9842 - MinusLogProbMetric: 42.9842 - val_loss: 43.6581 - val_MinusLogProbMetric: 43.6581 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 724/1000
2023-10-29 00:32:37.624 
Epoch 724/1000 
	 loss: 42.9712, MinusLogProbMetric: 42.9712, val_loss: 43.8881, val_MinusLogProbMetric: 43.8881

Epoch 724: val_loss did not improve from 43.43541
196/196 - 34s - loss: 42.9712 - MinusLogProbMetric: 42.9712 - val_loss: 43.8881 - val_MinusLogProbMetric: 43.8881 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 725/1000
2023-10-29 00:33:11.675 
Epoch 725/1000 
	 loss: 43.1382, MinusLogProbMetric: 43.1382, val_loss: 43.5716, val_MinusLogProbMetric: 43.5716

Epoch 725: val_loss did not improve from 43.43541
196/196 - 34s - loss: 43.1382 - MinusLogProbMetric: 43.1382 - val_loss: 43.5716 - val_MinusLogProbMetric: 43.5716 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 726/1000
2023-10-29 00:33:45.853 
Epoch 726/1000 
	 loss: 42.9951, MinusLogProbMetric: 42.9951, val_loss: 43.4219, val_MinusLogProbMetric: 43.4219

Epoch 726: val_loss improved from 43.43541 to 43.42187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 42.9951 - MinusLogProbMetric: 42.9951 - val_loss: 43.4219 - val_MinusLogProbMetric: 43.4219 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 727/1000
2023-10-29 00:34:20.650 
Epoch 727/1000 
	 loss: 43.1567, MinusLogProbMetric: 43.1567, val_loss: 44.0451, val_MinusLogProbMetric: 44.0451

Epoch 727: val_loss did not improve from 43.42187
196/196 - 34s - loss: 43.1567 - MinusLogProbMetric: 43.1567 - val_loss: 44.0451 - val_MinusLogProbMetric: 44.0451 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 728/1000
2023-10-29 00:34:54.813 
Epoch 728/1000 
	 loss: 42.8669, MinusLogProbMetric: 42.8669, val_loss: 43.7336, val_MinusLogProbMetric: 43.7336

Epoch 728: val_loss did not improve from 43.42187
196/196 - 34s - loss: 42.8669 - MinusLogProbMetric: 42.8669 - val_loss: 43.7336 - val_MinusLogProbMetric: 43.7336 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 729/1000
2023-10-29 00:35:28.758 
Epoch 729/1000 
	 loss: 42.8474, MinusLogProbMetric: 42.8474, val_loss: 43.4771, val_MinusLogProbMetric: 43.4771

Epoch 729: val_loss did not improve from 43.42187
196/196 - 34s - loss: 42.8474 - MinusLogProbMetric: 42.8474 - val_loss: 43.4771 - val_MinusLogProbMetric: 43.4771 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 730/1000
2023-10-29 00:36:02.899 
Epoch 730/1000 
	 loss: 43.1722, MinusLogProbMetric: 43.1722, val_loss: 43.7306, val_MinusLogProbMetric: 43.7306

Epoch 730: val_loss did not improve from 43.42187
196/196 - 34s - loss: 43.1722 - MinusLogProbMetric: 43.1722 - val_loss: 43.7306 - val_MinusLogProbMetric: 43.7306 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 731/1000
2023-10-29 00:36:37.126 
Epoch 731/1000 
	 loss: 42.7361, MinusLogProbMetric: 42.7361, val_loss: 43.9509, val_MinusLogProbMetric: 43.9509

Epoch 731: val_loss did not improve from 43.42187
196/196 - 34s - loss: 42.7361 - MinusLogProbMetric: 42.7361 - val_loss: 43.9509 - val_MinusLogProbMetric: 43.9509 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 732/1000
2023-10-29 00:37:11.629 
Epoch 732/1000 
	 loss: 42.9933, MinusLogProbMetric: 42.9933, val_loss: 43.6156, val_MinusLogProbMetric: 43.6156

Epoch 732: val_loss did not improve from 43.42187
196/196 - 34s - loss: 42.9933 - MinusLogProbMetric: 42.9933 - val_loss: 43.6156 - val_MinusLogProbMetric: 43.6156 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 733/1000
2023-10-29 00:37:46.510 
Epoch 733/1000 
	 loss: 43.3429, MinusLogProbMetric: 43.3429, val_loss: 43.5357, val_MinusLogProbMetric: 43.5357

Epoch 733: val_loss did not improve from 43.42187
196/196 - 35s - loss: 43.3429 - MinusLogProbMetric: 43.3429 - val_loss: 43.5357 - val_MinusLogProbMetric: 43.5357 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 734/1000
2023-10-29 00:38:21.069 
Epoch 734/1000 
	 loss: 42.7359, MinusLogProbMetric: 42.7359, val_loss: 43.8634, val_MinusLogProbMetric: 43.8634

Epoch 734: val_loss did not improve from 43.42187
196/196 - 35s - loss: 42.7359 - MinusLogProbMetric: 42.7359 - val_loss: 43.8634 - val_MinusLogProbMetric: 43.8634 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 735/1000
2023-10-29 00:38:55.258 
Epoch 735/1000 
	 loss: 43.0661, MinusLogProbMetric: 43.0661, val_loss: 43.8736, val_MinusLogProbMetric: 43.8736

Epoch 735: val_loss did not improve from 43.42187
196/196 - 34s - loss: 43.0661 - MinusLogProbMetric: 43.0661 - val_loss: 43.8736 - val_MinusLogProbMetric: 43.8736 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 736/1000
2023-10-29 00:39:29.657 
Epoch 736/1000 
	 loss: 42.7787, MinusLogProbMetric: 42.7787, val_loss: 43.4425, val_MinusLogProbMetric: 43.4425

Epoch 736: val_loss did not improve from 43.42187
196/196 - 34s - loss: 42.7787 - MinusLogProbMetric: 42.7787 - val_loss: 43.4425 - val_MinusLogProbMetric: 43.4425 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 737/1000
2023-10-29 00:40:04.259 
Epoch 737/1000 
	 loss: 43.0258, MinusLogProbMetric: 43.0258, val_loss: 45.7577, val_MinusLogProbMetric: 45.7577

Epoch 737: val_loss did not improve from 43.42187
196/196 - 35s - loss: 43.0258 - MinusLogProbMetric: 43.0258 - val_loss: 45.7577 - val_MinusLogProbMetric: 45.7577 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 738/1000
2023-10-29 00:40:38.798 
Epoch 738/1000 
	 loss: 43.0121, MinusLogProbMetric: 43.0121, val_loss: 43.7149, val_MinusLogProbMetric: 43.7149

Epoch 738: val_loss did not improve from 43.42187
196/196 - 35s - loss: 43.0121 - MinusLogProbMetric: 43.0121 - val_loss: 43.7149 - val_MinusLogProbMetric: 43.7149 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 739/1000
2023-10-29 00:41:13.539 
Epoch 739/1000 
	 loss: 42.9473, MinusLogProbMetric: 42.9473, val_loss: 43.8426, val_MinusLogProbMetric: 43.8426

Epoch 739: val_loss did not improve from 43.42187
196/196 - 35s - loss: 42.9473 - MinusLogProbMetric: 42.9473 - val_loss: 43.8426 - val_MinusLogProbMetric: 43.8426 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 740/1000
2023-10-29 00:41:48.501 
Epoch 740/1000 
	 loss: 42.9434, MinusLogProbMetric: 42.9434, val_loss: 43.7090, val_MinusLogProbMetric: 43.7090

Epoch 740: val_loss did not improve from 43.42187
196/196 - 35s - loss: 42.9434 - MinusLogProbMetric: 42.9434 - val_loss: 43.7090 - val_MinusLogProbMetric: 43.7090 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 741/1000
2023-10-29 00:42:23.796 
Epoch 741/1000 
	 loss: 42.9626, MinusLogProbMetric: 42.9626, val_loss: 43.8674, val_MinusLogProbMetric: 43.8674

Epoch 741: val_loss did not improve from 43.42187
196/196 - 35s - loss: 42.9626 - MinusLogProbMetric: 42.9626 - val_loss: 43.8674 - val_MinusLogProbMetric: 43.8674 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 742/1000
2023-10-29 00:42:58.851 
Epoch 742/1000 
	 loss: 43.0379, MinusLogProbMetric: 43.0379, val_loss: 43.6031, val_MinusLogProbMetric: 43.6031

Epoch 742: val_loss did not improve from 43.42187
196/196 - 35s - loss: 43.0379 - MinusLogProbMetric: 43.0379 - val_loss: 43.6031 - val_MinusLogProbMetric: 43.6031 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 743/1000
2023-10-29 00:43:33.340 
Epoch 743/1000 
	 loss: 42.9303, MinusLogProbMetric: 42.9303, val_loss: 43.8480, val_MinusLogProbMetric: 43.8480

Epoch 743: val_loss did not improve from 43.42187
196/196 - 34s - loss: 42.9303 - MinusLogProbMetric: 42.9303 - val_loss: 43.8480 - val_MinusLogProbMetric: 43.8480 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 744/1000
2023-10-29 00:44:07.781 
Epoch 744/1000 
	 loss: 43.0667, MinusLogProbMetric: 43.0667, val_loss: 44.0748, val_MinusLogProbMetric: 44.0748

Epoch 744: val_loss did not improve from 43.42187
196/196 - 34s - loss: 43.0667 - MinusLogProbMetric: 43.0667 - val_loss: 44.0748 - val_MinusLogProbMetric: 44.0748 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 745/1000
2023-10-29 00:44:41.960 
Epoch 745/1000 
	 loss: 42.9491, MinusLogProbMetric: 42.9491, val_loss: 43.7670, val_MinusLogProbMetric: 43.7670

Epoch 745: val_loss did not improve from 43.42187
196/196 - 34s - loss: 42.9491 - MinusLogProbMetric: 42.9491 - val_loss: 43.7670 - val_MinusLogProbMetric: 43.7670 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 746/1000
2023-10-29 00:45:16.451 
Epoch 746/1000 
	 loss: 42.8746, MinusLogProbMetric: 42.8746, val_loss: 43.6886, val_MinusLogProbMetric: 43.6886

Epoch 746: val_loss did not improve from 43.42187
196/196 - 34s - loss: 42.8746 - MinusLogProbMetric: 42.8746 - val_loss: 43.6886 - val_MinusLogProbMetric: 43.6886 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 747/1000
2023-10-29 00:45:51.053 
Epoch 747/1000 
	 loss: 42.9927, MinusLogProbMetric: 42.9927, val_loss: 43.5656, val_MinusLogProbMetric: 43.5656

Epoch 747: val_loss did not improve from 43.42187
196/196 - 35s - loss: 42.9927 - MinusLogProbMetric: 42.9927 - val_loss: 43.5656 - val_MinusLogProbMetric: 43.5656 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 748/1000
2023-10-29 00:46:25.471 
Epoch 748/1000 
	 loss: 43.0072, MinusLogProbMetric: 43.0072, val_loss: 43.3563, val_MinusLogProbMetric: 43.3563

Epoch 748: val_loss improved from 43.42187 to 43.35635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 43.0072 - MinusLogProbMetric: 43.0072 - val_loss: 43.3563 - val_MinusLogProbMetric: 43.3563 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 749/1000
2023-10-29 00:47:00.159 
Epoch 749/1000 
	 loss: 42.9636, MinusLogProbMetric: 42.9636, val_loss: 45.7819, val_MinusLogProbMetric: 45.7819

Epoch 749: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.9636 - MinusLogProbMetric: 42.9636 - val_loss: 45.7819 - val_MinusLogProbMetric: 45.7819 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 750/1000
2023-10-29 00:47:34.227 
Epoch 750/1000 
	 loss: 42.8831, MinusLogProbMetric: 42.8831, val_loss: 43.6909, val_MinusLogProbMetric: 43.6909

Epoch 750: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.8831 - MinusLogProbMetric: 42.8831 - val_loss: 43.6909 - val_MinusLogProbMetric: 43.6909 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 751/1000
2023-10-29 00:48:08.593 
Epoch 751/1000 
	 loss: 43.0763, MinusLogProbMetric: 43.0763, val_loss: 44.2110, val_MinusLogProbMetric: 44.2110

Epoch 751: val_loss did not improve from 43.35635
196/196 - 34s - loss: 43.0763 - MinusLogProbMetric: 43.0763 - val_loss: 44.2110 - val_MinusLogProbMetric: 44.2110 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 752/1000
2023-10-29 00:48:42.418 
Epoch 752/1000 
	 loss: 43.0750, MinusLogProbMetric: 43.0750, val_loss: 43.4811, val_MinusLogProbMetric: 43.4811

Epoch 752: val_loss did not improve from 43.35635
196/196 - 34s - loss: 43.0750 - MinusLogProbMetric: 43.0750 - val_loss: 43.4811 - val_MinusLogProbMetric: 43.4811 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 753/1000
2023-10-29 00:49:17.237 
Epoch 753/1000 
	 loss: 42.6954, MinusLogProbMetric: 42.6954, val_loss: 43.9180, val_MinusLogProbMetric: 43.9180

Epoch 753: val_loss did not improve from 43.35635
196/196 - 35s - loss: 42.6954 - MinusLogProbMetric: 42.6954 - val_loss: 43.9180 - val_MinusLogProbMetric: 43.9180 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 754/1000
2023-10-29 00:49:51.704 
Epoch 754/1000 
	 loss: 42.9986, MinusLogProbMetric: 42.9986, val_loss: 43.6482, val_MinusLogProbMetric: 43.6482

Epoch 754: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.9986 - MinusLogProbMetric: 42.9986 - val_loss: 43.6482 - val_MinusLogProbMetric: 43.6482 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 755/1000
2023-10-29 00:50:26.304 
Epoch 755/1000 
	 loss: 42.8857, MinusLogProbMetric: 42.8857, val_loss: 43.7715, val_MinusLogProbMetric: 43.7715

Epoch 755: val_loss did not improve from 43.35635
196/196 - 35s - loss: 42.8857 - MinusLogProbMetric: 42.8857 - val_loss: 43.7715 - val_MinusLogProbMetric: 43.7715 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 756/1000
2023-10-29 00:51:00.336 
Epoch 756/1000 
	 loss: 43.0831, MinusLogProbMetric: 43.0831, val_loss: 43.4108, val_MinusLogProbMetric: 43.4108

Epoch 756: val_loss did not improve from 43.35635
196/196 - 34s - loss: 43.0831 - MinusLogProbMetric: 43.0831 - val_loss: 43.4108 - val_MinusLogProbMetric: 43.4108 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 757/1000
2023-10-29 00:51:35.045 
Epoch 757/1000 
	 loss: 42.8952, MinusLogProbMetric: 42.8952, val_loss: 43.7982, val_MinusLogProbMetric: 43.7982

Epoch 757: val_loss did not improve from 43.35635
196/196 - 35s - loss: 42.8952 - MinusLogProbMetric: 42.8952 - val_loss: 43.7982 - val_MinusLogProbMetric: 43.7982 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 758/1000
2023-10-29 00:52:09.141 
Epoch 758/1000 
	 loss: 42.9636, MinusLogProbMetric: 42.9636, val_loss: 43.6562, val_MinusLogProbMetric: 43.6562

Epoch 758: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.9636 - MinusLogProbMetric: 42.9636 - val_loss: 43.6562 - val_MinusLogProbMetric: 43.6562 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 759/1000
2023-10-29 00:52:42.956 
Epoch 759/1000 
	 loss: 42.7860, MinusLogProbMetric: 42.7860, val_loss: 45.0125, val_MinusLogProbMetric: 45.0125

Epoch 759: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.7860 - MinusLogProbMetric: 42.7860 - val_loss: 45.0125 - val_MinusLogProbMetric: 45.0125 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 760/1000
2023-10-29 00:53:17.086 
Epoch 760/1000 
	 loss: 43.0107, MinusLogProbMetric: 43.0107, val_loss: 43.7770, val_MinusLogProbMetric: 43.7770

Epoch 760: val_loss did not improve from 43.35635
196/196 - 34s - loss: 43.0107 - MinusLogProbMetric: 43.0107 - val_loss: 43.7770 - val_MinusLogProbMetric: 43.7770 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 761/1000
2023-10-29 00:53:51.753 
Epoch 761/1000 
	 loss: 42.7009, MinusLogProbMetric: 42.7009, val_loss: 43.5240, val_MinusLogProbMetric: 43.5240

Epoch 761: val_loss did not improve from 43.35635
196/196 - 35s - loss: 42.7009 - MinusLogProbMetric: 42.7009 - val_loss: 43.5240 - val_MinusLogProbMetric: 43.5240 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 762/1000
2023-10-29 00:54:26.448 
Epoch 762/1000 
	 loss: 43.1303, MinusLogProbMetric: 43.1303, val_loss: 44.0376, val_MinusLogProbMetric: 44.0376

Epoch 762: val_loss did not improve from 43.35635
196/196 - 35s - loss: 43.1303 - MinusLogProbMetric: 43.1303 - val_loss: 44.0376 - val_MinusLogProbMetric: 44.0376 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 763/1000
2023-10-29 00:55:01.085 
Epoch 763/1000 
	 loss: 42.7996, MinusLogProbMetric: 42.7996, val_loss: 44.2983, val_MinusLogProbMetric: 44.2983

Epoch 763: val_loss did not improve from 43.35635
196/196 - 35s - loss: 42.7996 - MinusLogProbMetric: 42.7996 - val_loss: 44.2983 - val_MinusLogProbMetric: 44.2983 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 764/1000
2023-10-29 00:55:35.449 
Epoch 764/1000 
	 loss: 43.0337, MinusLogProbMetric: 43.0337, val_loss: 44.1881, val_MinusLogProbMetric: 44.1881

Epoch 764: val_loss did not improve from 43.35635
196/196 - 34s - loss: 43.0337 - MinusLogProbMetric: 43.0337 - val_loss: 44.1881 - val_MinusLogProbMetric: 44.1881 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 765/1000
2023-10-29 00:56:10.183 
Epoch 765/1000 
	 loss: 42.9324, MinusLogProbMetric: 42.9324, val_loss: 44.3319, val_MinusLogProbMetric: 44.3319

Epoch 765: val_loss did not improve from 43.35635
196/196 - 35s - loss: 42.9324 - MinusLogProbMetric: 42.9324 - val_loss: 44.3319 - val_MinusLogProbMetric: 44.3319 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 766/1000
2023-10-29 00:56:44.259 
Epoch 766/1000 
	 loss: 42.9544, MinusLogProbMetric: 42.9544, val_loss: 43.6663, val_MinusLogProbMetric: 43.6663

Epoch 766: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.9544 - MinusLogProbMetric: 42.9544 - val_loss: 43.6663 - val_MinusLogProbMetric: 43.6663 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 767/1000
2023-10-29 00:57:18.561 
Epoch 767/1000 
	 loss: 42.9698, MinusLogProbMetric: 42.9698, val_loss: 44.1711, val_MinusLogProbMetric: 44.1711

Epoch 767: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.9698 - MinusLogProbMetric: 42.9698 - val_loss: 44.1711 - val_MinusLogProbMetric: 44.1711 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 768/1000
2023-10-29 00:57:52.675 
Epoch 768/1000 
	 loss: 43.1330, MinusLogProbMetric: 43.1330, val_loss: 44.6931, val_MinusLogProbMetric: 44.6931

Epoch 768: val_loss did not improve from 43.35635
196/196 - 34s - loss: 43.1330 - MinusLogProbMetric: 43.1330 - val_loss: 44.6931 - val_MinusLogProbMetric: 44.6931 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 769/1000
2023-10-29 00:58:27.096 
Epoch 769/1000 
	 loss: 42.9057, MinusLogProbMetric: 42.9057, val_loss: 43.9254, val_MinusLogProbMetric: 43.9254

Epoch 769: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.9057 - MinusLogProbMetric: 42.9057 - val_loss: 43.9254 - val_MinusLogProbMetric: 43.9254 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 770/1000
2023-10-29 00:59:02.199 
Epoch 770/1000 
	 loss: 43.1059, MinusLogProbMetric: 43.1059, val_loss: 43.4708, val_MinusLogProbMetric: 43.4708

Epoch 770: val_loss did not improve from 43.35635
196/196 - 35s - loss: 43.1059 - MinusLogProbMetric: 43.1059 - val_loss: 43.4708 - val_MinusLogProbMetric: 43.4708 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 771/1000
2023-10-29 00:59:37.418 
Epoch 771/1000 
	 loss: 43.0009, MinusLogProbMetric: 43.0009, val_loss: 44.5558, val_MinusLogProbMetric: 44.5558

Epoch 771: val_loss did not improve from 43.35635
196/196 - 35s - loss: 43.0009 - MinusLogProbMetric: 43.0009 - val_loss: 44.5558 - val_MinusLogProbMetric: 44.5558 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 772/1000
2023-10-29 01:00:11.943 
Epoch 772/1000 
	 loss: 43.2169, MinusLogProbMetric: 43.2169, val_loss: 44.4554, val_MinusLogProbMetric: 44.4554

Epoch 772: val_loss did not improve from 43.35635
196/196 - 35s - loss: 43.2169 - MinusLogProbMetric: 43.2169 - val_loss: 44.4554 - val_MinusLogProbMetric: 44.4554 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 773/1000
2023-10-29 01:00:46.200 
Epoch 773/1000 
	 loss: 42.7736, MinusLogProbMetric: 42.7736, val_loss: 43.6054, val_MinusLogProbMetric: 43.6054

Epoch 773: val_loss did not improve from 43.35635
196/196 - 34s - loss: 42.7736 - MinusLogProbMetric: 42.7736 - val_loss: 43.6054 - val_MinusLogProbMetric: 43.6054 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 774/1000
2023-10-29 01:01:21.065 
Epoch 774/1000 
	 loss: 43.0628, MinusLogProbMetric: 43.0628, val_loss: 43.7227, val_MinusLogProbMetric: 43.7227

Epoch 774: val_loss did not improve from 43.35635
196/196 - 35s - loss: 43.0628 - MinusLogProbMetric: 43.0628 - val_loss: 43.7227 - val_MinusLogProbMetric: 43.7227 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 775/1000
2023-10-29 01:01:55.720 
Epoch 775/1000 
	 loss: 42.7842, MinusLogProbMetric: 42.7842, val_loss: 43.2362, val_MinusLogProbMetric: 43.2362

Epoch 775: val_loss improved from 43.35635 to 43.23618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 42.7842 - MinusLogProbMetric: 42.7842 - val_loss: 43.2362 - val_MinusLogProbMetric: 43.2362 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 776/1000
2023-10-29 01:02:31.055 
Epoch 776/1000 
	 loss: 42.9181, MinusLogProbMetric: 42.9181, val_loss: 43.3817, val_MinusLogProbMetric: 43.3817

Epoch 776: val_loss did not improve from 43.23618
196/196 - 35s - loss: 42.9181 - MinusLogProbMetric: 42.9181 - val_loss: 43.3817 - val_MinusLogProbMetric: 43.3817 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 777/1000
2023-10-29 01:03:05.631 
Epoch 777/1000 
	 loss: 42.7954, MinusLogProbMetric: 42.7954, val_loss: 43.6792, val_MinusLogProbMetric: 43.6792

Epoch 777: val_loss did not improve from 43.23618
196/196 - 35s - loss: 42.7954 - MinusLogProbMetric: 42.7954 - val_loss: 43.6792 - val_MinusLogProbMetric: 43.6792 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 778/1000
2023-10-29 01:03:40.007 
Epoch 778/1000 
	 loss: 42.9042, MinusLogProbMetric: 42.9042, val_loss: 43.3816, val_MinusLogProbMetric: 43.3816

Epoch 778: val_loss did not improve from 43.23618
196/196 - 34s - loss: 42.9042 - MinusLogProbMetric: 42.9042 - val_loss: 43.3816 - val_MinusLogProbMetric: 43.3816 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 779/1000
2023-10-29 01:04:14.900 
Epoch 779/1000 
	 loss: 43.0102, MinusLogProbMetric: 43.0102, val_loss: 43.8322, val_MinusLogProbMetric: 43.8322

Epoch 779: val_loss did not improve from 43.23618
196/196 - 35s - loss: 43.0102 - MinusLogProbMetric: 43.0102 - val_loss: 43.8322 - val_MinusLogProbMetric: 43.8322 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 780/1000
2023-10-29 01:04:49.868 
Epoch 780/1000 
	 loss: 43.0882, MinusLogProbMetric: 43.0882, val_loss: 43.3327, val_MinusLogProbMetric: 43.3327

Epoch 780: val_loss did not improve from 43.23618
196/196 - 35s - loss: 43.0882 - MinusLogProbMetric: 43.0882 - val_loss: 43.3327 - val_MinusLogProbMetric: 43.3327 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 781/1000
2023-10-29 01:05:25.168 
Epoch 781/1000 
	 loss: 42.9890, MinusLogProbMetric: 42.9890, val_loss: 43.3115, val_MinusLogProbMetric: 43.3115

Epoch 781: val_loss did not improve from 43.23618
196/196 - 35s - loss: 42.9890 - MinusLogProbMetric: 42.9890 - val_loss: 43.3115 - val_MinusLogProbMetric: 43.3115 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 782/1000
2023-10-29 01:05:59.770 
Epoch 782/1000 
	 loss: 42.8500, MinusLogProbMetric: 42.8500, val_loss: 43.2125, val_MinusLogProbMetric: 43.2125

Epoch 782: val_loss improved from 43.23618 to 43.21250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 42.8500 - MinusLogProbMetric: 42.8500 - val_loss: 43.2125 - val_MinusLogProbMetric: 43.2125 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 783/1000
2023-10-29 01:06:34.715 
Epoch 783/1000 
	 loss: 42.9431, MinusLogProbMetric: 42.9431, val_loss: 43.5305, val_MinusLogProbMetric: 43.5305

Epoch 783: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.9431 - MinusLogProbMetric: 42.9431 - val_loss: 43.5305 - val_MinusLogProbMetric: 43.5305 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 784/1000
2023-10-29 01:07:09.130 
Epoch 784/1000 
	 loss: 42.7572, MinusLogProbMetric: 42.7572, val_loss: 44.4176, val_MinusLogProbMetric: 44.4176

Epoch 784: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.7572 - MinusLogProbMetric: 42.7572 - val_loss: 44.4176 - val_MinusLogProbMetric: 44.4176 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 785/1000
2023-10-29 01:07:44.103 
Epoch 785/1000 
	 loss: 42.9473, MinusLogProbMetric: 42.9473, val_loss: 43.3626, val_MinusLogProbMetric: 43.3626

Epoch 785: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.9473 - MinusLogProbMetric: 42.9473 - val_loss: 43.3626 - val_MinusLogProbMetric: 43.3626 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 786/1000
2023-10-29 01:08:18.303 
Epoch 786/1000 
	 loss: 43.0373, MinusLogProbMetric: 43.0373, val_loss: 43.5214, val_MinusLogProbMetric: 43.5214

Epoch 786: val_loss did not improve from 43.21250
196/196 - 34s - loss: 43.0373 - MinusLogProbMetric: 43.0373 - val_loss: 43.5214 - val_MinusLogProbMetric: 43.5214 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 787/1000
2023-10-29 01:08:53.343 
Epoch 787/1000 
	 loss: 42.8228, MinusLogProbMetric: 42.8228, val_loss: 43.3427, val_MinusLogProbMetric: 43.3427

Epoch 787: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.8228 - MinusLogProbMetric: 42.8228 - val_loss: 43.3427 - val_MinusLogProbMetric: 43.3427 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 788/1000
2023-10-29 01:09:28.559 
Epoch 788/1000 
	 loss: 43.4530, MinusLogProbMetric: 43.4530, val_loss: 43.7465, val_MinusLogProbMetric: 43.7465

Epoch 788: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.4530 - MinusLogProbMetric: 43.4530 - val_loss: 43.7465 - val_MinusLogProbMetric: 43.7465 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 789/1000
2023-10-29 01:10:03.307 
Epoch 789/1000 
	 loss: 42.7355, MinusLogProbMetric: 42.7355, val_loss: 44.7216, val_MinusLogProbMetric: 44.7216

Epoch 789: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.7355 - MinusLogProbMetric: 42.7355 - val_loss: 44.7216 - val_MinusLogProbMetric: 44.7216 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 790/1000
2023-10-29 01:10:37.966 
Epoch 790/1000 
	 loss: 43.0719, MinusLogProbMetric: 43.0719, val_loss: 43.9980, val_MinusLogProbMetric: 43.9980

Epoch 790: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.0719 - MinusLogProbMetric: 43.0719 - val_loss: 43.9980 - val_MinusLogProbMetric: 43.9980 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 791/1000
2023-10-29 01:11:12.316 
Epoch 791/1000 
	 loss: 42.9322, MinusLogProbMetric: 42.9322, val_loss: 44.3061, val_MinusLogProbMetric: 44.3061

Epoch 791: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.9322 - MinusLogProbMetric: 42.9322 - val_loss: 44.3061 - val_MinusLogProbMetric: 44.3061 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 792/1000
2023-10-29 01:11:46.727 
Epoch 792/1000 
	 loss: 43.0955, MinusLogProbMetric: 43.0955, val_loss: 44.8504, val_MinusLogProbMetric: 44.8504

Epoch 792: val_loss did not improve from 43.21250
196/196 - 34s - loss: 43.0955 - MinusLogProbMetric: 43.0955 - val_loss: 44.8504 - val_MinusLogProbMetric: 44.8504 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 793/1000
2023-10-29 01:12:21.615 
Epoch 793/1000 
	 loss: 42.9995, MinusLogProbMetric: 42.9995, val_loss: 44.9334, val_MinusLogProbMetric: 44.9334

Epoch 793: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.9995 - MinusLogProbMetric: 42.9995 - val_loss: 44.9334 - val_MinusLogProbMetric: 44.9334 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 794/1000
2023-10-29 01:12:56.360 
Epoch 794/1000 
	 loss: 43.0685, MinusLogProbMetric: 43.0685, val_loss: 43.3953, val_MinusLogProbMetric: 43.3953

Epoch 794: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.0685 - MinusLogProbMetric: 43.0685 - val_loss: 43.3953 - val_MinusLogProbMetric: 43.3953 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 795/1000
2023-10-29 01:13:30.906 
Epoch 795/1000 
	 loss: 42.7675, MinusLogProbMetric: 42.7675, val_loss: 44.0123, val_MinusLogProbMetric: 44.0123

Epoch 795: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.7675 - MinusLogProbMetric: 42.7675 - val_loss: 44.0123 - val_MinusLogProbMetric: 44.0123 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 796/1000
2023-10-29 01:14:05.815 
Epoch 796/1000 
	 loss: 43.0728, MinusLogProbMetric: 43.0728, val_loss: 44.5758, val_MinusLogProbMetric: 44.5758

Epoch 796: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.0728 - MinusLogProbMetric: 43.0728 - val_loss: 44.5758 - val_MinusLogProbMetric: 44.5758 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 797/1000
2023-10-29 01:14:41.006 
Epoch 797/1000 
	 loss: 42.9531, MinusLogProbMetric: 42.9531, val_loss: 44.6532, val_MinusLogProbMetric: 44.6532

Epoch 797: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.9531 - MinusLogProbMetric: 42.9531 - val_loss: 44.6532 - val_MinusLogProbMetric: 44.6532 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 798/1000
2023-10-29 01:15:15.585 
Epoch 798/1000 
	 loss: 42.9069, MinusLogProbMetric: 42.9069, val_loss: 43.9907, val_MinusLogProbMetric: 43.9907

Epoch 798: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.9069 - MinusLogProbMetric: 42.9069 - val_loss: 43.9907 - val_MinusLogProbMetric: 43.9907 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 799/1000
2023-10-29 01:15:49.958 
Epoch 799/1000 
	 loss: 42.7176, MinusLogProbMetric: 42.7176, val_loss: 43.6925, val_MinusLogProbMetric: 43.6925

Epoch 799: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.7176 - MinusLogProbMetric: 42.7176 - val_loss: 43.6925 - val_MinusLogProbMetric: 43.6925 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 800/1000
2023-10-29 01:16:24.186 
Epoch 800/1000 
	 loss: 43.1718, MinusLogProbMetric: 43.1718, val_loss: 43.5838, val_MinusLogProbMetric: 43.5838

Epoch 800: val_loss did not improve from 43.21250
196/196 - 34s - loss: 43.1718 - MinusLogProbMetric: 43.1718 - val_loss: 43.5838 - val_MinusLogProbMetric: 43.5838 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 801/1000
2023-10-29 01:16:58.926 
Epoch 801/1000 
	 loss: 43.0238, MinusLogProbMetric: 43.0238, val_loss: 43.7489, val_MinusLogProbMetric: 43.7489

Epoch 801: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.0238 - MinusLogProbMetric: 43.0238 - val_loss: 43.7489 - val_MinusLogProbMetric: 43.7489 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 802/1000
2023-10-29 01:17:33.842 
Epoch 802/1000 
	 loss: 42.8413, MinusLogProbMetric: 42.8413, val_loss: 43.5242, val_MinusLogProbMetric: 43.5242

Epoch 802: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.8413 - MinusLogProbMetric: 42.8413 - val_loss: 43.5242 - val_MinusLogProbMetric: 43.5242 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 803/1000
2023-10-29 01:18:08.941 
Epoch 803/1000 
	 loss: 43.0155, MinusLogProbMetric: 43.0155, val_loss: 43.3170, val_MinusLogProbMetric: 43.3170

Epoch 803: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.0155 - MinusLogProbMetric: 43.0155 - val_loss: 43.3170 - val_MinusLogProbMetric: 43.3170 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 804/1000
2023-10-29 01:18:43.934 
Epoch 804/1000 
	 loss: 43.0710, MinusLogProbMetric: 43.0710, val_loss: 44.4550, val_MinusLogProbMetric: 44.4550

Epoch 804: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.0710 - MinusLogProbMetric: 43.0710 - val_loss: 44.4550 - val_MinusLogProbMetric: 44.4550 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 805/1000
2023-10-29 01:19:18.693 
Epoch 805/1000 
	 loss: 42.7611, MinusLogProbMetric: 42.7611, val_loss: 43.8772, val_MinusLogProbMetric: 43.8772

Epoch 805: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.7611 - MinusLogProbMetric: 42.7611 - val_loss: 43.8772 - val_MinusLogProbMetric: 43.8772 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 806/1000
2023-10-29 01:19:53.280 
Epoch 806/1000 
	 loss: 42.7046, MinusLogProbMetric: 42.7046, val_loss: 43.9638, val_MinusLogProbMetric: 43.9638

Epoch 806: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.7046 - MinusLogProbMetric: 42.7046 - val_loss: 43.9638 - val_MinusLogProbMetric: 43.9638 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 807/1000
2023-10-29 01:20:28.457 
Epoch 807/1000 
	 loss: 43.2085, MinusLogProbMetric: 43.2085, val_loss: 44.5071, val_MinusLogProbMetric: 44.5071

Epoch 807: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.2085 - MinusLogProbMetric: 43.2085 - val_loss: 44.5071 - val_MinusLogProbMetric: 44.5071 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 808/1000
2023-10-29 01:21:03.594 
Epoch 808/1000 
	 loss: 42.6685, MinusLogProbMetric: 42.6685, val_loss: 44.0710, val_MinusLogProbMetric: 44.0710

Epoch 808: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.6685 - MinusLogProbMetric: 42.6685 - val_loss: 44.0710 - val_MinusLogProbMetric: 44.0710 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 809/1000
2023-10-29 01:21:38.632 
Epoch 809/1000 
	 loss: 43.1152, MinusLogProbMetric: 43.1152, val_loss: 43.8636, val_MinusLogProbMetric: 43.8636

Epoch 809: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.1152 - MinusLogProbMetric: 43.1152 - val_loss: 43.8636 - val_MinusLogProbMetric: 43.8636 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 810/1000
2023-10-29 01:22:13.626 
Epoch 810/1000 
	 loss: 43.0359, MinusLogProbMetric: 43.0359, val_loss: 43.5755, val_MinusLogProbMetric: 43.5755

Epoch 810: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.0359 - MinusLogProbMetric: 43.0359 - val_loss: 43.5755 - val_MinusLogProbMetric: 43.5755 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 811/1000
2023-10-29 01:22:48.673 
Epoch 811/1000 
	 loss: 42.6464, MinusLogProbMetric: 42.6464, val_loss: 44.5674, val_MinusLogProbMetric: 44.5674

Epoch 811: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.6464 - MinusLogProbMetric: 42.6464 - val_loss: 44.5674 - val_MinusLogProbMetric: 44.5674 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 812/1000
2023-10-29 01:23:23.330 
Epoch 812/1000 
	 loss: 43.1145, MinusLogProbMetric: 43.1145, val_loss: 43.7068, val_MinusLogProbMetric: 43.7068

Epoch 812: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.1145 - MinusLogProbMetric: 43.1145 - val_loss: 43.7068 - val_MinusLogProbMetric: 43.7068 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 813/1000
2023-10-29 01:23:57.550 
Epoch 813/1000 
	 loss: 42.7261, MinusLogProbMetric: 42.7261, val_loss: 45.8972, val_MinusLogProbMetric: 45.8972

Epoch 813: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.7261 - MinusLogProbMetric: 42.7261 - val_loss: 45.8972 - val_MinusLogProbMetric: 45.8972 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 814/1000
2023-10-29 01:24:31.743 
Epoch 814/1000 
	 loss: 43.0162, MinusLogProbMetric: 43.0162, val_loss: 43.4065, val_MinusLogProbMetric: 43.4065

Epoch 814: val_loss did not improve from 43.21250
196/196 - 34s - loss: 43.0162 - MinusLogProbMetric: 43.0162 - val_loss: 43.4065 - val_MinusLogProbMetric: 43.4065 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 815/1000
2023-10-29 01:25:06.004 
Epoch 815/1000 
	 loss: 43.1520, MinusLogProbMetric: 43.1520, val_loss: 43.7860, val_MinusLogProbMetric: 43.7860

Epoch 815: val_loss did not improve from 43.21250
196/196 - 34s - loss: 43.1520 - MinusLogProbMetric: 43.1520 - val_loss: 43.7860 - val_MinusLogProbMetric: 43.7860 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 816/1000
2023-10-29 01:25:40.268 
Epoch 816/1000 
	 loss: 42.5880, MinusLogProbMetric: 42.5880, val_loss: 43.5779, val_MinusLogProbMetric: 43.5779

Epoch 816: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.5880 - MinusLogProbMetric: 42.5880 - val_loss: 43.5779 - val_MinusLogProbMetric: 43.5779 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 817/1000
2023-10-29 01:26:14.602 
Epoch 817/1000 
	 loss: 42.9138, MinusLogProbMetric: 42.9138, val_loss: 43.5191, val_MinusLogProbMetric: 43.5191

Epoch 817: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.9138 - MinusLogProbMetric: 42.9138 - val_loss: 43.5191 - val_MinusLogProbMetric: 43.5191 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 818/1000
2023-10-29 01:26:48.854 
Epoch 818/1000 
	 loss: 42.9223, MinusLogProbMetric: 42.9223, val_loss: 44.3544, val_MinusLogProbMetric: 44.3544

Epoch 818: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.9223 - MinusLogProbMetric: 42.9223 - val_loss: 44.3544 - val_MinusLogProbMetric: 44.3544 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 819/1000
2023-10-29 01:27:23.189 
Epoch 819/1000 
	 loss: 43.0723, MinusLogProbMetric: 43.0723, val_loss: 44.2133, val_MinusLogProbMetric: 44.2133

Epoch 819: val_loss did not improve from 43.21250
196/196 - 34s - loss: 43.0723 - MinusLogProbMetric: 43.0723 - val_loss: 44.2133 - val_MinusLogProbMetric: 44.2133 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 820/1000
2023-10-29 01:27:57.966 
Epoch 820/1000 
	 loss: 42.9694, MinusLogProbMetric: 42.9694, val_loss: 43.9526, val_MinusLogProbMetric: 43.9526

Epoch 820: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.9694 - MinusLogProbMetric: 42.9694 - val_loss: 43.9526 - val_MinusLogProbMetric: 43.9526 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 821/1000
2023-10-29 01:28:33.048 
Epoch 821/1000 
	 loss: 42.6638, MinusLogProbMetric: 42.6638, val_loss: 43.4381, val_MinusLogProbMetric: 43.4381

Epoch 821: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.6638 - MinusLogProbMetric: 42.6638 - val_loss: 43.4381 - val_MinusLogProbMetric: 43.4381 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 822/1000
2023-10-29 01:29:07.939 
Epoch 822/1000 
	 loss: 42.9785, MinusLogProbMetric: 42.9785, val_loss: 43.8339, val_MinusLogProbMetric: 43.8339

Epoch 822: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.9785 - MinusLogProbMetric: 42.9785 - val_loss: 43.8339 - val_MinusLogProbMetric: 43.8339 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 823/1000
2023-10-29 01:29:42.480 
Epoch 823/1000 
	 loss: 42.8956, MinusLogProbMetric: 42.8956, val_loss: 43.7999, val_MinusLogProbMetric: 43.7999

Epoch 823: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.8956 - MinusLogProbMetric: 42.8956 - val_loss: 43.7999 - val_MinusLogProbMetric: 43.7999 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 824/1000
2023-10-29 01:30:16.885 
Epoch 824/1000 
	 loss: 42.9057, MinusLogProbMetric: 42.9057, val_loss: 44.4178, val_MinusLogProbMetric: 44.4178

Epoch 824: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.9057 - MinusLogProbMetric: 42.9057 - val_loss: 44.4178 - val_MinusLogProbMetric: 44.4178 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 825/1000
2023-10-29 01:30:51.268 
Epoch 825/1000 
	 loss: 42.9274, MinusLogProbMetric: 42.9274, val_loss: 44.0008, val_MinusLogProbMetric: 44.0008

Epoch 825: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.9274 - MinusLogProbMetric: 42.9274 - val_loss: 44.0008 - val_MinusLogProbMetric: 44.0008 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 826/1000
2023-10-29 01:31:26.197 
Epoch 826/1000 
	 loss: 42.7988, MinusLogProbMetric: 42.7988, val_loss: 46.8460, val_MinusLogProbMetric: 46.8460

Epoch 826: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.7988 - MinusLogProbMetric: 42.7988 - val_loss: 46.8460 - val_MinusLogProbMetric: 46.8460 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 827/1000
2023-10-29 01:32:00.367 
Epoch 827/1000 
	 loss: 42.8604, MinusLogProbMetric: 42.8604, val_loss: 43.8318, val_MinusLogProbMetric: 43.8318

Epoch 827: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.8604 - MinusLogProbMetric: 42.8604 - val_loss: 43.8318 - val_MinusLogProbMetric: 43.8318 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 828/1000
2023-10-29 01:32:34.622 
Epoch 828/1000 
	 loss: 42.6690, MinusLogProbMetric: 42.6690, val_loss: 43.6846, val_MinusLogProbMetric: 43.6846

Epoch 828: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.6690 - MinusLogProbMetric: 42.6690 - val_loss: 43.6846 - val_MinusLogProbMetric: 43.6846 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 829/1000
2023-10-29 01:33:08.980 
Epoch 829/1000 
	 loss: 42.8137, MinusLogProbMetric: 42.8137, val_loss: 44.2794, val_MinusLogProbMetric: 44.2794

Epoch 829: val_loss did not improve from 43.21250
196/196 - 34s - loss: 42.8137 - MinusLogProbMetric: 42.8137 - val_loss: 44.2794 - val_MinusLogProbMetric: 44.2794 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 830/1000
2023-10-29 01:33:43.545 
Epoch 830/1000 
	 loss: 42.8456, MinusLogProbMetric: 42.8456, val_loss: 43.8279, val_MinusLogProbMetric: 43.8279

Epoch 830: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.8456 - MinusLogProbMetric: 42.8456 - val_loss: 43.8279 - val_MinusLogProbMetric: 43.8279 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 831/1000
2023-10-29 01:34:18.244 
Epoch 831/1000 
	 loss: 42.9503, MinusLogProbMetric: 42.9503, val_loss: 43.9133, val_MinusLogProbMetric: 43.9133

Epoch 831: val_loss did not improve from 43.21250
196/196 - 35s - loss: 42.9503 - MinusLogProbMetric: 42.9503 - val_loss: 43.9133 - val_MinusLogProbMetric: 43.9133 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 832/1000
2023-10-29 01:34:53.179 
Epoch 832/1000 
	 loss: 43.1127, MinusLogProbMetric: 43.1127, val_loss: 43.4509, val_MinusLogProbMetric: 43.4509

Epoch 832: val_loss did not improve from 43.21250
196/196 - 35s - loss: 43.1127 - MinusLogProbMetric: 43.1127 - val_loss: 43.4509 - val_MinusLogProbMetric: 43.4509 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 833/1000
2023-10-29 01:35:27.580 
Epoch 833/1000 
	 loss: 41.9797, MinusLogProbMetric: 41.9797, val_loss: 43.0060, val_MinusLogProbMetric: 43.0060

Epoch 833: val_loss improved from 43.21250 to 43.00600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 41.9797 - MinusLogProbMetric: 41.9797 - val_loss: 43.0060 - val_MinusLogProbMetric: 43.0060 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 834/1000
2023-10-29 01:36:01.530 
Epoch 834/1000 
	 loss: 41.9189, MinusLogProbMetric: 41.9189, val_loss: 42.8948, val_MinusLogProbMetric: 42.8948

Epoch 834: val_loss improved from 43.00600 to 42.89482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 41.9189 - MinusLogProbMetric: 41.9189 - val_loss: 42.8948 - val_MinusLogProbMetric: 42.8948 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 835/1000
2023-10-29 01:36:36.190 
Epoch 835/1000 
	 loss: 41.9524, MinusLogProbMetric: 41.9524, val_loss: 42.8201, val_MinusLogProbMetric: 42.8201

Epoch 835: val_loss improved from 42.89482 to 42.82011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 41.9524 - MinusLogProbMetric: 41.9524 - val_loss: 42.8201 - val_MinusLogProbMetric: 42.8201 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 836/1000
2023-10-29 01:37:10.279 
Epoch 836/1000 
	 loss: 41.9791, MinusLogProbMetric: 41.9791, val_loss: 42.7961, val_MinusLogProbMetric: 42.7961

Epoch 836: val_loss improved from 42.82011 to 42.79611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 41.9791 - MinusLogProbMetric: 41.9791 - val_loss: 42.7961 - val_MinusLogProbMetric: 42.7961 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 837/1000
2023-10-29 01:37:44.883 
Epoch 837/1000 
	 loss: 41.9678, MinusLogProbMetric: 41.9678, val_loss: 43.0460, val_MinusLogProbMetric: 43.0460

Epoch 837: val_loss did not improve from 42.79611
196/196 - 34s - loss: 41.9678 - MinusLogProbMetric: 41.9678 - val_loss: 43.0460 - val_MinusLogProbMetric: 43.0460 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 838/1000
2023-10-29 01:38:18.982 
Epoch 838/1000 
	 loss: 42.1379, MinusLogProbMetric: 42.1379, val_loss: 43.8172, val_MinusLogProbMetric: 43.8172

Epoch 838: val_loss did not improve from 42.79611
196/196 - 34s - loss: 42.1379 - MinusLogProbMetric: 42.1379 - val_loss: 43.8172 - val_MinusLogProbMetric: 43.8172 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 839/1000
2023-10-29 01:38:52.669 
Epoch 839/1000 
	 loss: 42.1333, MinusLogProbMetric: 42.1333, val_loss: 43.0233, val_MinusLogProbMetric: 43.0233

Epoch 839: val_loss did not improve from 42.79611
196/196 - 34s - loss: 42.1333 - MinusLogProbMetric: 42.1333 - val_loss: 43.0233 - val_MinusLogProbMetric: 43.0233 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 840/1000
2023-10-29 01:39:27.096 
Epoch 840/1000 
	 loss: 41.9448, MinusLogProbMetric: 41.9448, val_loss: 42.9042, val_MinusLogProbMetric: 42.9042

Epoch 840: val_loss did not improve from 42.79611
196/196 - 34s - loss: 41.9448 - MinusLogProbMetric: 41.9448 - val_loss: 42.9042 - val_MinusLogProbMetric: 42.9042 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 841/1000
2023-10-29 01:40:01.812 
Epoch 841/1000 
	 loss: 41.9650, MinusLogProbMetric: 41.9650, val_loss: 42.9927, val_MinusLogProbMetric: 42.9927

Epoch 841: val_loss did not improve from 42.79611
196/196 - 35s - loss: 41.9650 - MinusLogProbMetric: 41.9650 - val_loss: 42.9927 - val_MinusLogProbMetric: 42.9927 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 842/1000
2023-10-29 01:40:36.201 
Epoch 842/1000 
	 loss: 42.3116, MinusLogProbMetric: 42.3116, val_loss: 42.8237, val_MinusLogProbMetric: 42.8237

Epoch 842: val_loss did not improve from 42.79611
196/196 - 34s - loss: 42.3116 - MinusLogProbMetric: 42.3116 - val_loss: 42.8237 - val_MinusLogProbMetric: 42.8237 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 843/1000
2023-10-29 01:41:10.351 
Epoch 843/1000 
	 loss: 42.0556, MinusLogProbMetric: 42.0556, val_loss: 42.7393, val_MinusLogProbMetric: 42.7393

Epoch 843: val_loss improved from 42.79611 to 42.73934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 42.0556 - MinusLogProbMetric: 42.0556 - val_loss: 42.7393 - val_MinusLogProbMetric: 42.7393 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 844/1000
2023-10-29 01:41:44.910 
Epoch 844/1000 
	 loss: 42.2078, MinusLogProbMetric: 42.2078, val_loss: 44.4206, val_MinusLogProbMetric: 44.4206

Epoch 844: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.2078 - MinusLogProbMetric: 42.2078 - val_loss: 44.4206 - val_MinusLogProbMetric: 44.4206 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 845/1000
2023-10-29 01:42:18.582 
Epoch 845/1000 
	 loss: 41.9598, MinusLogProbMetric: 41.9598, val_loss: 42.9543, val_MinusLogProbMetric: 42.9543

Epoch 845: val_loss did not improve from 42.73934
196/196 - 34s - loss: 41.9598 - MinusLogProbMetric: 41.9598 - val_loss: 42.9543 - val_MinusLogProbMetric: 42.9543 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 846/1000
2023-10-29 01:42:52.533 
Epoch 846/1000 
	 loss: 42.3781, MinusLogProbMetric: 42.3781, val_loss: 43.4179, val_MinusLogProbMetric: 43.4179

Epoch 846: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.3781 - MinusLogProbMetric: 42.3781 - val_loss: 43.4179 - val_MinusLogProbMetric: 43.4179 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 847/1000
2023-10-29 01:43:26.659 
Epoch 847/1000 
	 loss: 41.9274, MinusLogProbMetric: 41.9274, val_loss: 42.7670, val_MinusLogProbMetric: 42.7670

Epoch 847: val_loss did not improve from 42.73934
196/196 - 34s - loss: 41.9274 - MinusLogProbMetric: 41.9274 - val_loss: 42.7670 - val_MinusLogProbMetric: 42.7670 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 848/1000
2023-10-29 01:44:00.626 
Epoch 848/1000 
	 loss: 42.4839, MinusLogProbMetric: 42.4839, val_loss: 42.8298, val_MinusLogProbMetric: 42.8298

Epoch 848: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.4839 - MinusLogProbMetric: 42.4839 - val_loss: 42.8298 - val_MinusLogProbMetric: 42.8298 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 849/1000
2023-10-29 01:44:34.884 
Epoch 849/1000 
	 loss: 41.9407, MinusLogProbMetric: 41.9407, val_loss: 42.8839, val_MinusLogProbMetric: 42.8839

Epoch 849: val_loss did not improve from 42.73934
196/196 - 34s - loss: 41.9407 - MinusLogProbMetric: 41.9407 - val_loss: 42.8839 - val_MinusLogProbMetric: 42.8839 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 850/1000
2023-10-29 01:45:08.602 
Epoch 850/1000 
	 loss: 42.1115, MinusLogProbMetric: 42.1115, val_loss: 42.9826, val_MinusLogProbMetric: 42.9826

Epoch 850: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.1115 - MinusLogProbMetric: 42.1115 - val_loss: 42.9826 - val_MinusLogProbMetric: 42.9826 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 851/1000
2023-10-29 01:45:42.464 
Epoch 851/1000 
	 loss: 42.0752, MinusLogProbMetric: 42.0752, val_loss: 45.4705, val_MinusLogProbMetric: 45.4705

Epoch 851: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.0752 - MinusLogProbMetric: 42.0752 - val_loss: 45.4705 - val_MinusLogProbMetric: 45.4705 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 852/1000
2023-10-29 01:46:16.062 
Epoch 852/1000 
	 loss: 42.1795, MinusLogProbMetric: 42.1795, val_loss: 42.8244, val_MinusLogProbMetric: 42.8244

Epoch 852: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.1795 - MinusLogProbMetric: 42.1795 - val_loss: 42.8244 - val_MinusLogProbMetric: 42.8244 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 853/1000
2023-10-29 01:46:49.924 
Epoch 853/1000 
	 loss: 42.1582, MinusLogProbMetric: 42.1582, val_loss: 42.8216, val_MinusLogProbMetric: 42.8216

Epoch 853: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.1582 - MinusLogProbMetric: 42.1582 - val_loss: 42.8216 - val_MinusLogProbMetric: 42.8216 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 854/1000
2023-10-29 01:47:24.962 
Epoch 854/1000 
	 loss: 42.4220, MinusLogProbMetric: 42.4220, val_loss: 43.1320, val_MinusLogProbMetric: 43.1320

Epoch 854: val_loss did not improve from 42.73934
196/196 - 35s - loss: 42.4220 - MinusLogProbMetric: 42.4220 - val_loss: 43.1320 - val_MinusLogProbMetric: 43.1320 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 855/1000
2023-10-29 01:47:59.722 
Epoch 855/1000 
	 loss: 41.9003, MinusLogProbMetric: 41.9003, val_loss: 42.7972, val_MinusLogProbMetric: 42.7972

Epoch 855: val_loss did not improve from 42.73934
196/196 - 35s - loss: 41.9003 - MinusLogProbMetric: 41.9003 - val_loss: 42.7972 - val_MinusLogProbMetric: 42.7972 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 856/1000
2023-10-29 01:48:33.582 
Epoch 856/1000 
	 loss: 42.2903, MinusLogProbMetric: 42.2903, val_loss: 42.7640, val_MinusLogProbMetric: 42.7640

Epoch 856: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.2903 - MinusLogProbMetric: 42.2903 - val_loss: 42.7640 - val_MinusLogProbMetric: 42.7640 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 857/1000
2023-10-29 01:49:07.018 
Epoch 857/1000 
	 loss: 42.2065, MinusLogProbMetric: 42.2065, val_loss: 42.8557, val_MinusLogProbMetric: 42.8557

Epoch 857: val_loss did not improve from 42.73934
196/196 - 33s - loss: 42.2065 - MinusLogProbMetric: 42.2065 - val_loss: 42.8557 - val_MinusLogProbMetric: 42.8557 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 858/1000
2023-10-29 01:49:40.840 
Epoch 858/1000 
	 loss: 41.9707, MinusLogProbMetric: 41.9707, val_loss: 43.1379, val_MinusLogProbMetric: 43.1379

Epoch 858: val_loss did not improve from 42.73934
196/196 - 34s - loss: 41.9707 - MinusLogProbMetric: 41.9707 - val_loss: 43.1379 - val_MinusLogProbMetric: 43.1379 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 859/1000
2023-10-29 01:50:15.146 
Epoch 859/1000 
	 loss: 42.1164, MinusLogProbMetric: 42.1164, val_loss: 42.9967, val_MinusLogProbMetric: 42.9967

Epoch 859: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.1164 - MinusLogProbMetric: 42.1164 - val_loss: 42.9967 - val_MinusLogProbMetric: 42.9967 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 860/1000
2023-10-29 01:50:49.349 
Epoch 860/1000 
	 loss: 42.2476, MinusLogProbMetric: 42.2476, val_loss: 43.4600, val_MinusLogProbMetric: 43.4600

Epoch 860: val_loss did not improve from 42.73934
196/196 - 34s - loss: 42.2476 - MinusLogProbMetric: 42.2476 - val_loss: 43.4600 - val_MinusLogProbMetric: 43.4600 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 861/1000
2023-10-29 01:51:23.323 
Epoch 861/1000 
	 loss: 41.9622, MinusLogProbMetric: 41.9622, val_loss: 42.6752, val_MinusLogProbMetric: 42.6752

Epoch 861: val_loss improved from 42.73934 to 42.67519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 41.9622 - MinusLogProbMetric: 41.9622 - val_loss: 42.6752 - val_MinusLogProbMetric: 42.6752 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 862/1000
2023-10-29 01:51:57.855 
Epoch 862/1000 
	 loss: 42.1750, MinusLogProbMetric: 42.1750, val_loss: 43.6758, val_MinusLogProbMetric: 43.6758

Epoch 862: val_loss did not improve from 42.67519
196/196 - 34s - loss: 42.1750 - MinusLogProbMetric: 42.1750 - val_loss: 43.6758 - val_MinusLogProbMetric: 43.6758 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 863/1000
2023-10-29 01:52:31.403 
Epoch 863/1000 
	 loss: 41.9454, MinusLogProbMetric: 41.9454, val_loss: 42.9895, val_MinusLogProbMetric: 42.9895

Epoch 863: val_loss did not improve from 42.67519
196/196 - 34s - loss: 41.9454 - MinusLogProbMetric: 41.9454 - val_loss: 42.9895 - val_MinusLogProbMetric: 42.9895 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 864/1000
2023-10-29 01:53:05.351 
Epoch 864/1000 
	 loss: 42.0080, MinusLogProbMetric: 42.0080, val_loss: 44.0654, val_MinusLogProbMetric: 44.0654

Epoch 864: val_loss did not improve from 42.67519
196/196 - 34s - loss: 42.0080 - MinusLogProbMetric: 42.0080 - val_loss: 44.0654 - val_MinusLogProbMetric: 44.0654 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 865/1000
2023-10-29 01:53:39.289 
Epoch 865/1000 
	 loss: 42.4548, MinusLogProbMetric: 42.4548, val_loss: 42.9944, val_MinusLogProbMetric: 42.9944

Epoch 865: val_loss did not improve from 42.67519
196/196 - 34s - loss: 42.4548 - MinusLogProbMetric: 42.4548 - val_loss: 42.9944 - val_MinusLogProbMetric: 42.9944 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 866/1000
2023-10-29 01:54:13.290 
Epoch 866/1000 
	 loss: 41.9306, MinusLogProbMetric: 41.9306, val_loss: 43.2361, val_MinusLogProbMetric: 43.2361

Epoch 866: val_loss did not improve from 42.67519
196/196 - 34s - loss: 41.9306 - MinusLogProbMetric: 41.9306 - val_loss: 43.2361 - val_MinusLogProbMetric: 43.2361 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 867/1000
2023-10-29 01:54:47.155 
Epoch 867/1000 
	 loss: 41.9434, MinusLogProbMetric: 41.9434, val_loss: 42.7849, val_MinusLogProbMetric: 42.7849

Epoch 867: val_loss did not improve from 42.67519
196/196 - 34s - loss: 41.9434 - MinusLogProbMetric: 41.9434 - val_loss: 42.7849 - val_MinusLogProbMetric: 42.7849 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 868/1000
2023-10-29 01:55:20.850 
Epoch 868/1000 
	 loss: 42.0355, MinusLogProbMetric: 42.0355, val_loss: 42.9527, val_MinusLogProbMetric: 42.9527

Epoch 868: val_loss did not improve from 42.67519
196/196 - 34s - loss: 42.0355 - MinusLogProbMetric: 42.0355 - val_loss: 42.9527 - val_MinusLogProbMetric: 42.9527 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 869/1000
2023-10-29 01:55:54.822 
Epoch 869/1000 
	 loss: 42.3848, MinusLogProbMetric: 42.3848, val_loss: 43.1471, val_MinusLogProbMetric: 43.1471

Epoch 869: val_loss did not improve from 42.67519
196/196 - 34s - loss: 42.3848 - MinusLogProbMetric: 42.3848 - val_loss: 43.1471 - val_MinusLogProbMetric: 43.1471 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 870/1000
2023-10-29 01:56:28.425 
Epoch 870/1000 
	 loss: 42.2014, MinusLogProbMetric: 42.2014, val_loss: 42.9174, val_MinusLogProbMetric: 42.9174

Epoch 870: val_loss did not improve from 42.67519
196/196 - 34s - loss: 42.2014 - MinusLogProbMetric: 42.2014 - val_loss: 42.9174 - val_MinusLogProbMetric: 42.9174 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 871/1000
2023-10-29 01:57:00.282 
Epoch 871/1000 
	 loss: 41.9097, MinusLogProbMetric: 41.9097, val_loss: 43.9192, val_MinusLogProbMetric: 43.9192

Epoch 871: val_loss did not improve from 42.67519
196/196 - 32s - loss: 41.9097 - MinusLogProbMetric: 41.9097 - val_loss: 43.9192 - val_MinusLogProbMetric: 43.9192 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 872/1000
2023-10-29 01:57:32.630 
Epoch 872/1000 
	 loss: 42.2631, MinusLogProbMetric: 42.2631, val_loss: 42.8549, val_MinusLogProbMetric: 42.8549

Epoch 872: val_loss did not improve from 42.67519
196/196 - 32s - loss: 42.2631 - MinusLogProbMetric: 42.2631 - val_loss: 42.8549 - val_MinusLogProbMetric: 42.8549 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 873/1000
2023-10-29 01:58:04.438 
Epoch 873/1000 
	 loss: 42.1651, MinusLogProbMetric: 42.1651, val_loss: 43.6715, val_MinusLogProbMetric: 43.6715

Epoch 873: val_loss did not improve from 42.67519
196/196 - 32s - loss: 42.1651 - MinusLogProbMetric: 42.1651 - val_loss: 43.6715 - val_MinusLogProbMetric: 43.6715 - lr: 8.3333e-05 - 32s/epoch - 162ms/step
Epoch 874/1000
2023-10-29 01:58:37.870 
Epoch 874/1000 
	 loss: 42.2887, MinusLogProbMetric: 42.2887, val_loss: 43.6972, val_MinusLogProbMetric: 43.6972

Epoch 874: val_loss did not improve from 42.67519
196/196 - 33s - loss: 42.2887 - MinusLogProbMetric: 42.2887 - val_loss: 43.6972 - val_MinusLogProbMetric: 43.6972 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 875/1000
2023-10-29 01:59:11.750 
Epoch 875/1000 
	 loss: 42.2526, MinusLogProbMetric: 42.2526, val_loss: 42.9684, val_MinusLogProbMetric: 42.9684

Epoch 875: val_loss did not improve from 42.67519
196/196 - 34s - loss: 42.2526 - MinusLogProbMetric: 42.2526 - val_loss: 42.9684 - val_MinusLogProbMetric: 42.9684 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 876/1000
2023-10-29 01:59:45.495 
Epoch 876/1000 
	 loss: 41.9385, MinusLogProbMetric: 41.9385, val_loss: 42.6696, val_MinusLogProbMetric: 42.6696

Epoch 876: val_loss improved from 42.67519 to 42.66957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 34s - loss: 41.9385 - MinusLogProbMetric: 41.9385 - val_loss: 42.6696 - val_MinusLogProbMetric: 42.6696 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 877/1000
2023-10-29 02:00:19.219 
Epoch 877/1000 
	 loss: 42.0596, MinusLogProbMetric: 42.0596, val_loss: 43.8528, val_MinusLogProbMetric: 43.8528

Epoch 877: val_loss did not improve from 42.66957
196/196 - 33s - loss: 42.0596 - MinusLogProbMetric: 42.0596 - val_loss: 43.8528 - val_MinusLogProbMetric: 43.8528 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 878/1000
2023-10-29 02:00:53.861 
Epoch 878/1000 
	 loss: 41.9532, MinusLogProbMetric: 41.9532, val_loss: 43.0653, val_MinusLogProbMetric: 43.0653

Epoch 878: val_loss did not improve from 42.66957
196/196 - 35s - loss: 41.9532 - MinusLogProbMetric: 41.9532 - val_loss: 43.0653 - val_MinusLogProbMetric: 43.0653 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 879/1000
2023-10-29 02:01:28.844 
Epoch 879/1000 
	 loss: 41.9467, MinusLogProbMetric: 41.9467, val_loss: 42.8759, val_MinusLogProbMetric: 42.8759

Epoch 879: val_loss did not improve from 42.66957
196/196 - 35s - loss: 41.9467 - MinusLogProbMetric: 41.9467 - val_loss: 42.8759 - val_MinusLogProbMetric: 42.8759 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 880/1000
2023-10-29 02:02:03.262 
Epoch 880/1000 
	 loss: 42.2154, MinusLogProbMetric: 42.2154, val_loss: 42.7732, val_MinusLogProbMetric: 42.7732

Epoch 880: val_loss did not improve from 42.66957
196/196 - 34s - loss: 42.2154 - MinusLogProbMetric: 42.2154 - val_loss: 42.7732 - val_MinusLogProbMetric: 42.7732 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 881/1000
2023-10-29 02:02:37.451 
Epoch 881/1000 
	 loss: 42.3734, MinusLogProbMetric: 42.3734, val_loss: 43.0537, val_MinusLogProbMetric: 43.0537

Epoch 881: val_loss did not improve from 42.66957
196/196 - 34s - loss: 42.3734 - MinusLogProbMetric: 42.3734 - val_loss: 43.0537 - val_MinusLogProbMetric: 43.0537 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 882/1000
2023-10-29 02:03:12.011 
Epoch 882/1000 
	 loss: 42.2541, MinusLogProbMetric: 42.2541, val_loss: 42.8987, val_MinusLogProbMetric: 42.8987

Epoch 882: val_loss did not improve from 42.66957
196/196 - 35s - loss: 42.2541 - MinusLogProbMetric: 42.2541 - val_loss: 42.8987 - val_MinusLogProbMetric: 42.8987 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 883/1000
2023-10-29 02:03:46.019 
Epoch 883/1000 
	 loss: 41.8917, MinusLogProbMetric: 41.8917, val_loss: 42.7497, val_MinusLogProbMetric: 42.7497

Epoch 883: val_loss did not improve from 42.66957
196/196 - 34s - loss: 41.8917 - MinusLogProbMetric: 41.8917 - val_loss: 42.7497 - val_MinusLogProbMetric: 42.7497 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 884/1000
2023-10-29 02:04:19.826 
Epoch 884/1000 
	 loss: 42.2582, MinusLogProbMetric: 42.2582, val_loss: 42.7299, val_MinusLogProbMetric: 42.7299

Epoch 884: val_loss did not improve from 42.66957
196/196 - 34s - loss: 42.2582 - MinusLogProbMetric: 42.2582 - val_loss: 42.7299 - val_MinusLogProbMetric: 42.7299 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 885/1000
2023-10-29 02:04:54.285 
Epoch 885/1000 
	 loss: 41.9006, MinusLogProbMetric: 41.9006, val_loss: 42.6524, val_MinusLogProbMetric: 42.6524

Epoch 885: val_loss improved from 42.66957 to 42.65237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 41.9006 - MinusLogProbMetric: 41.9006 - val_loss: 42.6524 - val_MinusLogProbMetric: 42.6524 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 886/1000
2023-10-29 02:05:28.518 
Epoch 886/1000 
	 loss: 42.3307, MinusLogProbMetric: 42.3307, val_loss: 43.4516, val_MinusLogProbMetric: 43.4516

Epoch 886: val_loss did not improve from 42.65237
196/196 - 34s - loss: 42.3307 - MinusLogProbMetric: 42.3307 - val_loss: 43.4516 - val_MinusLogProbMetric: 43.4516 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 887/1000
2023-10-29 02:06:01.831 
Epoch 887/1000 
	 loss: 42.0446, MinusLogProbMetric: 42.0446, val_loss: 43.0985, val_MinusLogProbMetric: 43.0985

Epoch 887: val_loss did not improve from 42.65237
196/196 - 33s - loss: 42.0446 - MinusLogProbMetric: 42.0446 - val_loss: 43.0985 - val_MinusLogProbMetric: 43.0985 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 888/1000
2023-10-29 02:06:35.117 
Epoch 888/1000 
	 loss: 41.8966, MinusLogProbMetric: 41.8966, val_loss: 42.8159, val_MinusLogProbMetric: 42.8159

Epoch 888: val_loss did not improve from 42.65237
196/196 - 33s - loss: 41.8966 - MinusLogProbMetric: 41.8966 - val_loss: 42.8159 - val_MinusLogProbMetric: 42.8159 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 889/1000
2023-10-29 02:07:08.402 
Epoch 889/1000 
	 loss: 42.3413, MinusLogProbMetric: 42.3413, val_loss: 43.2133, val_MinusLogProbMetric: 43.2133

Epoch 889: val_loss did not improve from 42.65237
196/196 - 33s - loss: 42.3413 - MinusLogProbMetric: 42.3413 - val_loss: 43.2133 - val_MinusLogProbMetric: 43.2133 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 890/1000
2023-10-29 02:07:41.706 
Epoch 890/1000 
	 loss: 42.1234, MinusLogProbMetric: 42.1234, val_loss: 43.0584, val_MinusLogProbMetric: 43.0584

Epoch 890: val_loss did not improve from 42.65237
196/196 - 33s - loss: 42.1234 - MinusLogProbMetric: 42.1234 - val_loss: 43.0584 - val_MinusLogProbMetric: 43.0584 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 891/1000
2023-10-29 02:08:14.916 
Epoch 891/1000 
	 loss: 41.9472, MinusLogProbMetric: 41.9472, val_loss: 42.8834, val_MinusLogProbMetric: 42.8834

Epoch 891: val_loss did not improve from 42.65237
196/196 - 33s - loss: 41.9472 - MinusLogProbMetric: 41.9472 - val_loss: 42.8834 - val_MinusLogProbMetric: 42.8834 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 892/1000
2023-10-29 02:08:47.911 
Epoch 892/1000 
	 loss: 42.2036, MinusLogProbMetric: 42.2036, val_loss: 43.2103, val_MinusLogProbMetric: 43.2103

Epoch 892: val_loss did not improve from 42.65237
196/196 - 33s - loss: 42.2036 - MinusLogProbMetric: 42.2036 - val_loss: 43.2103 - val_MinusLogProbMetric: 43.2103 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 893/1000
2023-10-29 02:09:20.774 
Epoch 893/1000 
	 loss: 41.9877, MinusLogProbMetric: 41.9877, val_loss: 46.5417, val_MinusLogProbMetric: 46.5417

Epoch 893: val_loss did not improve from 42.65237
196/196 - 33s - loss: 41.9877 - MinusLogProbMetric: 41.9877 - val_loss: 46.5417 - val_MinusLogProbMetric: 46.5417 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 894/1000
2023-10-29 02:09:54.659 
Epoch 894/1000 
	 loss: 42.0346, MinusLogProbMetric: 42.0346, val_loss: 42.8217, val_MinusLogProbMetric: 42.8217

Epoch 894: val_loss did not improve from 42.65237
196/196 - 34s - loss: 42.0346 - MinusLogProbMetric: 42.0346 - val_loss: 42.8217 - val_MinusLogProbMetric: 42.8217 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 895/1000
2023-10-29 02:10:29.373 
Epoch 895/1000 
	 loss: 42.3592, MinusLogProbMetric: 42.3592, val_loss: 42.8487, val_MinusLogProbMetric: 42.8487

Epoch 895: val_loss did not improve from 42.65237
196/196 - 35s - loss: 42.3592 - MinusLogProbMetric: 42.3592 - val_loss: 42.8487 - val_MinusLogProbMetric: 42.8487 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 896/1000
2023-10-29 02:11:03.541 
Epoch 896/1000 
	 loss: 41.9375, MinusLogProbMetric: 41.9375, val_loss: 42.9869, val_MinusLogProbMetric: 42.9869

Epoch 896: val_loss did not improve from 42.65237
196/196 - 34s - loss: 41.9375 - MinusLogProbMetric: 41.9375 - val_loss: 42.9869 - val_MinusLogProbMetric: 42.9869 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 897/1000
2023-10-29 02:11:37.645 
Epoch 897/1000 
	 loss: 42.0055, MinusLogProbMetric: 42.0055, val_loss: 43.0309, val_MinusLogProbMetric: 43.0309

Epoch 897: val_loss did not improve from 42.65237
196/196 - 34s - loss: 42.0055 - MinusLogProbMetric: 42.0055 - val_loss: 43.0309 - val_MinusLogProbMetric: 43.0309 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 898/1000
2023-10-29 02:12:11.586 
Epoch 898/1000 
	 loss: 42.1432, MinusLogProbMetric: 42.1432, val_loss: 42.7649, val_MinusLogProbMetric: 42.7649

Epoch 898: val_loss did not improve from 42.65237
196/196 - 34s - loss: 42.1432 - MinusLogProbMetric: 42.1432 - val_loss: 42.7649 - val_MinusLogProbMetric: 42.7649 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 899/1000
2023-10-29 02:12:45.896 
Epoch 899/1000 
	 loss: 42.1981, MinusLogProbMetric: 42.1981, val_loss: 42.7928, val_MinusLogProbMetric: 42.7928

Epoch 899: val_loss did not improve from 42.65237
196/196 - 34s - loss: 42.1981 - MinusLogProbMetric: 42.1981 - val_loss: 42.7928 - val_MinusLogProbMetric: 42.7928 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 900/1000
2023-10-29 02:13:19.974 
Epoch 900/1000 
	 loss: 42.2084, MinusLogProbMetric: 42.2084, val_loss: 43.2636, val_MinusLogProbMetric: 43.2636

Epoch 900: val_loss did not improve from 42.65237
196/196 - 34s - loss: 42.2084 - MinusLogProbMetric: 42.2084 - val_loss: 43.2636 - val_MinusLogProbMetric: 43.2636 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 901/1000
2023-10-29 02:13:53.803 
Epoch 901/1000 
	 loss: 41.8901, MinusLogProbMetric: 41.8901, val_loss: 42.9835, val_MinusLogProbMetric: 42.9835

Epoch 901: val_loss did not improve from 42.65237
196/196 - 34s - loss: 41.8901 - MinusLogProbMetric: 41.8901 - val_loss: 42.9835 - val_MinusLogProbMetric: 42.9835 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 902/1000
2023-10-29 02:14:28.146 
Epoch 902/1000 
	 loss: 41.9683, MinusLogProbMetric: 41.9683, val_loss: 42.7195, val_MinusLogProbMetric: 42.7195

Epoch 902: val_loss did not improve from 42.65237
196/196 - 34s - loss: 41.9683 - MinusLogProbMetric: 41.9683 - val_loss: 42.7195 - val_MinusLogProbMetric: 42.7195 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 903/1000
2023-10-29 02:15:02.098 
Epoch 903/1000 
	 loss: 42.4362, MinusLogProbMetric: 42.4362, val_loss: 42.6871, val_MinusLogProbMetric: 42.6871

Epoch 903: val_loss did not improve from 42.65237
196/196 - 34s - loss: 42.4362 - MinusLogProbMetric: 42.4362 - val_loss: 42.6871 - val_MinusLogProbMetric: 42.6871 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 904/1000
2023-10-29 02:15:36.190 
Epoch 904/1000 
	 loss: 41.8928, MinusLogProbMetric: 41.8928, val_loss: 43.0896, val_MinusLogProbMetric: 43.0896

Epoch 904: val_loss did not improve from 42.65237
196/196 - 34s - loss: 41.8928 - MinusLogProbMetric: 41.8928 - val_loss: 43.0896 - val_MinusLogProbMetric: 43.0896 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 905/1000
2023-10-29 02:16:10.984 
Epoch 905/1000 
	 loss: 42.3423, MinusLogProbMetric: 42.3423, val_loss: 44.4949, val_MinusLogProbMetric: 44.4949

Epoch 905: val_loss did not improve from 42.65237
196/196 - 35s - loss: 42.3423 - MinusLogProbMetric: 42.3423 - val_loss: 44.4949 - val_MinusLogProbMetric: 44.4949 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 906/1000
2023-10-29 02:16:45.787 
Epoch 906/1000 
	 loss: 41.9280, MinusLogProbMetric: 41.9280, val_loss: 42.9882, val_MinusLogProbMetric: 42.9882

Epoch 906: val_loss did not improve from 42.65237
196/196 - 35s - loss: 41.9280 - MinusLogProbMetric: 41.9280 - val_loss: 42.9882 - val_MinusLogProbMetric: 42.9882 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 907/1000
2023-10-29 02:17:20.396 
Epoch 907/1000 
	 loss: 42.0156, MinusLogProbMetric: 42.0156, val_loss: 43.0434, val_MinusLogProbMetric: 43.0434

Epoch 907: val_loss did not improve from 42.65237
196/196 - 35s - loss: 42.0156 - MinusLogProbMetric: 42.0156 - val_loss: 43.0434 - val_MinusLogProbMetric: 43.0434 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 908/1000
2023-10-29 02:17:54.650 
Epoch 908/1000 
	 loss: 42.1133, MinusLogProbMetric: 42.1133, val_loss: 43.0992, val_MinusLogProbMetric: 43.0992

Epoch 908: val_loss did not improve from 42.65237
196/196 - 34s - loss: 42.1133 - MinusLogProbMetric: 42.1133 - val_loss: 43.0992 - val_MinusLogProbMetric: 43.0992 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 909/1000
2023-10-29 02:18:28.880 
Epoch 909/1000 
	 loss: 42.4360, MinusLogProbMetric: 42.4360, val_loss: 42.5984, val_MinusLogProbMetric: 42.5984

Epoch 909: val_loss improved from 42.65237 to 42.59845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 42.4360 - MinusLogProbMetric: 42.4360 - val_loss: 42.5984 - val_MinusLogProbMetric: 42.5984 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 910/1000
2023-10-29 02:19:02.834 
Epoch 910/1000 
	 loss: 42.0624, MinusLogProbMetric: 42.0624, val_loss: 43.6822, val_MinusLogProbMetric: 43.6822

Epoch 910: val_loss did not improve from 42.59845
196/196 - 33s - loss: 42.0624 - MinusLogProbMetric: 42.0624 - val_loss: 43.6822 - val_MinusLogProbMetric: 43.6822 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 911/1000
2023-10-29 02:19:37.013 
Epoch 911/1000 
	 loss: 41.9437, MinusLogProbMetric: 41.9437, val_loss: 42.7221, val_MinusLogProbMetric: 42.7221

Epoch 911: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.9437 - MinusLogProbMetric: 41.9437 - val_loss: 42.7221 - val_MinusLogProbMetric: 42.7221 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 912/1000
2023-10-29 02:20:11.573 
Epoch 912/1000 
	 loss: 42.1184, MinusLogProbMetric: 42.1184, val_loss: 43.0280, val_MinusLogProbMetric: 43.0280

Epoch 912: val_loss did not improve from 42.59845
196/196 - 35s - loss: 42.1184 - MinusLogProbMetric: 42.1184 - val_loss: 43.0280 - val_MinusLogProbMetric: 43.0280 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 913/1000
2023-10-29 02:20:46.178 
Epoch 913/1000 
	 loss: 41.8578, MinusLogProbMetric: 41.8578, val_loss: 42.8748, val_MinusLogProbMetric: 42.8748

Epoch 913: val_loss did not improve from 42.59845
196/196 - 35s - loss: 41.8578 - MinusLogProbMetric: 41.8578 - val_loss: 42.8748 - val_MinusLogProbMetric: 42.8748 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 914/1000
2023-10-29 02:21:20.388 
Epoch 914/1000 
	 loss: 42.1920, MinusLogProbMetric: 42.1920, val_loss: 42.8632, val_MinusLogProbMetric: 42.8632

Epoch 914: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.1920 - MinusLogProbMetric: 42.1920 - val_loss: 42.8632 - val_MinusLogProbMetric: 42.8632 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 915/1000
2023-10-29 02:21:54.863 
Epoch 915/1000 
	 loss: 41.9316, MinusLogProbMetric: 41.9316, val_loss: 43.3927, val_MinusLogProbMetric: 43.3927

Epoch 915: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.9316 - MinusLogProbMetric: 41.9316 - val_loss: 43.3927 - val_MinusLogProbMetric: 43.3927 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 916/1000
2023-10-29 02:22:29.100 
Epoch 916/1000 
	 loss: 42.3368, MinusLogProbMetric: 42.3368, val_loss: 43.1256, val_MinusLogProbMetric: 43.1256

Epoch 916: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.3368 - MinusLogProbMetric: 42.3368 - val_loss: 43.1256 - val_MinusLogProbMetric: 43.1256 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 917/1000
2023-10-29 02:23:03.653 
Epoch 917/1000 
	 loss: 41.9090, MinusLogProbMetric: 41.9090, val_loss: 42.9818, val_MinusLogProbMetric: 42.9818

Epoch 917: val_loss did not improve from 42.59845
196/196 - 35s - loss: 41.9090 - MinusLogProbMetric: 41.9090 - val_loss: 42.9818 - val_MinusLogProbMetric: 42.9818 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 918/1000
2023-10-29 02:23:38.234 
Epoch 918/1000 
	 loss: 42.0592, MinusLogProbMetric: 42.0592, val_loss: 42.6889, val_MinusLogProbMetric: 42.6889

Epoch 918: val_loss did not improve from 42.59845
196/196 - 35s - loss: 42.0592 - MinusLogProbMetric: 42.0592 - val_loss: 42.6889 - val_MinusLogProbMetric: 42.6889 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 919/1000
2023-10-29 02:24:12.668 
Epoch 919/1000 
	 loss: 42.0128, MinusLogProbMetric: 42.0128, val_loss: 45.7986, val_MinusLogProbMetric: 45.7986

Epoch 919: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.0128 - MinusLogProbMetric: 42.0128 - val_loss: 45.7986 - val_MinusLogProbMetric: 45.7986 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 920/1000
2023-10-29 02:24:46.661 
Epoch 920/1000 
	 loss: 42.2334, MinusLogProbMetric: 42.2334, val_loss: 42.7644, val_MinusLogProbMetric: 42.7644

Epoch 920: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.2334 - MinusLogProbMetric: 42.2334 - val_loss: 42.7644 - val_MinusLogProbMetric: 42.7644 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 921/1000
2023-10-29 02:25:21.222 
Epoch 921/1000 
	 loss: 42.2134, MinusLogProbMetric: 42.2134, val_loss: 43.3565, val_MinusLogProbMetric: 43.3565

Epoch 921: val_loss did not improve from 42.59845
196/196 - 35s - loss: 42.2134 - MinusLogProbMetric: 42.2134 - val_loss: 43.3565 - val_MinusLogProbMetric: 43.3565 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 922/1000
2023-10-29 02:25:55.554 
Epoch 922/1000 
	 loss: 41.9536, MinusLogProbMetric: 41.9536, val_loss: 43.4264, val_MinusLogProbMetric: 43.4264

Epoch 922: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.9536 - MinusLogProbMetric: 41.9536 - val_loss: 43.4264 - val_MinusLogProbMetric: 43.4264 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 923/1000
2023-10-29 02:26:29.535 
Epoch 923/1000 
	 loss: 42.0979, MinusLogProbMetric: 42.0979, val_loss: 43.3195, val_MinusLogProbMetric: 43.3195

Epoch 923: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.0979 - MinusLogProbMetric: 42.0979 - val_loss: 43.3195 - val_MinusLogProbMetric: 43.3195 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 924/1000
2023-10-29 02:27:03.893 
Epoch 924/1000 
	 loss: 41.9327, MinusLogProbMetric: 41.9327, val_loss: 43.1058, val_MinusLogProbMetric: 43.1058

Epoch 924: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.9327 - MinusLogProbMetric: 41.9327 - val_loss: 43.1058 - val_MinusLogProbMetric: 43.1058 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 925/1000
2023-10-29 02:27:38.067 
Epoch 925/1000 
	 loss: 42.2155, MinusLogProbMetric: 42.2155, val_loss: 42.7423, val_MinusLogProbMetric: 42.7423

Epoch 925: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.2155 - MinusLogProbMetric: 42.2155 - val_loss: 42.7423 - val_MinusLogProbMetric: 42.7423 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 926/1000
2023-10-29 02:28:12.167 
Epoch 926/1000 
	 loss: 41.8621, MinusLogProbMetric: 41.8621, val_loss: 43.1912, val_MinusLogProbMetric: 43.1912

Epoch 926: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.8621 - MinusLogProbMetric: 41.8621 - val_loss: 43.1912 - val_MinusLogProbMetric: 43.1912 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 927/1000
2023-10-29 02:28:45.634 
Epoch 927/1000 
	 loss: 42.2468, MinusLogProbMetric: 42.2468, val_loss: 42.7567, val_MinusLogProbMetric: 42.7567

Epoch 927: val_loss did not improve from 42.59845
196/196 - 33s - loss: 42.2468 - MinusLogProbMetric: 42.2468 - val_loss: 42.7567 - val_MinusLogProbMetric: 42.7567 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 928/1000
2023-10-29 02:29:19.476 
Epoch 928/1000 
	 loss: 42.0027, MinusLogProbMetric: 42.0027, val_loss: 43.0542, val_MinusLogProbMetric: 43.0542

Epoch 928: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.0027 - MinusLogProbMetric: 42.0027 - val_loss: 43.0542 - val_MinusLogProbMetric: 43.0542 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 929/1000
2023-10-29 02:29:53.344 
Epoch 929/1000 
	 loss: 41.9609, MinusLogProbMetric: 41.9609, val_loss: 43.0103, val_MinusLogProbMetric: 43.0103

Epoch 929: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.9609 - MinusLogProbMetric: 41.9609 - val_loss: 43.0103 - val_MinusLogProbMetric: 43.0103 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 930/1000
2023-10-29 02:30:27.550 
Epoch 930/1000 
	 loss: 41.8854, MinusLogProbMetric: 41.8854, val_loss: 44.1582, val_MinusLogProbMetric: 44.1582

Epoch 930: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.8854 - MinusLogProbMetric: 41.8854 - val_loss: 44.1582 - val_MinusLogProbMetric: 44.1582 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 931/1000
2023-10-29 02:31:01.767 
Epoch 931/1000 
	 loss: 42.3831, MinusLogProbMetric: 42.3831, val_loss: 42.7985, val_MinusLogProbMetric: 42.7985

Epoch 931: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.3831 - MinusLogProbMetric: 42.3831 - val_loss: 42.7985 - val_MinusLogProbMetric: 42.7985 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 932/1000
2023-10-29 02:31:35.890 
Epoch 932/1000 
	 loss: 42.0075, MinusLogProbMetric: 42.0075, val_loss: 42.7301, val_MinusLogProbMetric: 42.7301

Epoch 932: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.0075 - MinusLogProbMetric: 42.0075 - val_loss: 42.7301 - val_MinusLogProbMetric: 42.7301 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 933/1000
2023-10-29 02:32:10.815 
Epoch 933/1000 
	 loss: 42.1439, MinusLogProbMetric: 42.1439, val_loss: 42.8031, val_MinusLogProbMetric: 42.8031

Epoch 933: val_loss did not improve from 42.59845
196/196 - 35s - loss: 42.1439 - MinusLogProbMetric: 42.1439 - val_loss: 42.8031 - val_MinusLogProbMetric: 42.8031 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 934/1000
2023-10-29 02:32:44.583 
Epoch 934/1000 
	 loss: 42.2055, MinusLogProbMetric: 42.2055, val_loss: 43.0050, val_MinusLogProbMetric: 43.0050

Epoch 934: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.2055 - MinusLogProbMetric: 42.2055 - val_loss: 43.0050 - val_MinusLogProbMetric: 43.0050 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 935/1000
2023-10-29 02:33:17.030 
Epoch 935/1000 
	 loss: 42.2193, MinusLogProbMetric: 42.2193, val_loss: 43.2151, val_MinusLogProbMetric: 43.2151

Epoch 935: val_loss did not improve from 42.59845
196/196 - 32s - loss: 42.2193 - MinusLogProbMetric: 42.2193 - val_loss: 43.2151 - val_MinusLogProbMetric: 43.2151 - lr: 8.3333e-05 - 32s/epoch - 166ms/step
Epoch 936/1000
2023-10-29 02:33:50.285 
Epoch 936/1000 
	 loss: 41.8828, MinusLogProbMetric: 41.8828, val_loss: 42.7041, val_MinusLogProbMetric: 42.7041

Epoch 936: val_loss did not improve from 42.59845
196/196 - 33s - loss: 41.8828 - MinusLogProbMetric: 41.8828 - val_loss: 42.7041 - val_MinusLogProbMetric: 42.7041 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 937/1000
2023-10-29 02:34:22.305 
Epoch 937/1000 
	 loss: 42.1048, MinusLogProbMetric: 42.1048, val_loss: 42.6842, val_MinusLogProbMetric: 42.6842

Epoch 937: val_loss did not improve from 42.59845
196/196 - 32s - loss: 42.1048 - MinusLogProbMetric: 42.1048 - val_loss: 42.6842 - val_MinusLogProbMetric: 42.6842 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 938/1000
2023-10-29 02:34:55.449 
Epoch 938/1000 
	 loss: 41.9157, MinusLogProbMetric: 41.9157, val_loss: 43.4115, val_MinusLogProbMetric: 43.4115

Epoch 938: val_loss did not improve from 42.59845
196/196 - 33s - loss: 41.9157 - MinusLogProbMetric: 41.9157 - val_loss: 43.4115 - val_MinusLogProbMetric: 43.4115 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 939/1000
2023-10-29 02:35:28.261 
Epoch 939/1000 
	 loss: 42.1370, MinusLogProbMetric: 42.1370, val_loss: 42.6401, val_MinusLogProbMetric: 42.6401

Epoch 939: val_loss did not improve from 42.59845
196/196 - 33s - loss: 42.1370 - MinusLogProbMetric: 42.1370 - val_loss: 42.6401 - val_MinusLogProbMetric: 42.6401 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 940/1000
2023-10-29 02:36:01.873 
Epoch 940/1000 
	 loss: 42.5076, MinusLogProbMetric: 42.5076, val_loss: 43.1829, val_MinusLogProbMetric: 43.1829

Epoch 940: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.5076 - MinusLogProbMetric: 42.5076 - val_loss: 43.1829 - val_MinusLogProbMetric: 43.1829 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 941/1000
2023-10-29 02:36:35.841 
Epoch 941/1000 
	 loss: 41.9607, MinusLogProbMetric: 41.9607, val_loss: 42.8560, val_MinusLogProbMetric: 42.8560

Epoch 941: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.9607 - MinusLogProbMetric: 41.9607 - val_loss: 42.8560 - val_MinusLogProbMetric: 42.8560 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 942/1000
2023-10-29 02:37:10.140 
Epoch 942/1000 
	 loss: 42.2495, MinusLogProbMetric: 42.2495, val_loss: 43.0067, val_MinusLogProbMetric: 43.0067

Epoch 942: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.2495 - MinusLogProbMetric: 42.2495 - val_loss: 43.0067 - val_MinusLogProbMetric: 43.0067 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 943/1000
2023-10-29 02:37:44.055 
Epoch 943/1000 
	 loss: 42.2647, MinusLogProbMetric: 42.2647, val_loss: 43.6245, val_MinusLogProbMetric: 43.6245

Epoch 943: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.2647 - MinusLogProbMetric: 42.2647 - val_loss: 43.6245 - val_MinusLogProbMetric: 43.6245 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 944/1000
2023-10-29 02:38:17.873 
Epoch 944/1000 
	 loss: 41.8596, MinusLogProbMetric: 41.8596, val_loss: 43.4960, val_MinusLogProbMetric: 43.4960

Epoch 944: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.8596 - MinusLogProbMetric: 41.8596 - val_loss: 43.4960 - val_MinusLogProbMetric: 43.4960 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 945/1000
2023-10-29 02:38:52.001 
Epoch 945/1000 
	 loss: 42.3033, MinusLogProbMetric: 42.3033, val_loss: 42.7703, val_MinusLogProbMetric: 42.7703

Epoch 945: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.3033 - MinusLogProbMetric: 42.3033 - val_loss: 42.7703 - val_MinusLogProbMetric: 42.7703 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 946/1000
2023-10-29 02:39:25.161 
Epoch 946/1000 
	 loss: 41.8526, MinusLogProbMetric: 41.8526, val_loss: 42.7530, val_MinusLogProbMetric: 42.7530

Epoch 946: val_loss did not improve from 42.59845
196/196 - 33s - loss: 41.8526 - MinusLogProbMetric: 41.8526 - val_loss: 42.7530 - val_MinusLogProbMetric: 42.7530 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 947/1000
2023-10-29 02:39:58.773 
Epoch 947/1000 
	 loss: 42.2646, MinusLogProbMetric: 42.2646, val_loss: 42.7343, val_MinusLogProbMetric: 42.7343

Epoch 947: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.2646 - MinusLogProbMetric: 42.2646 - val_loss: 42.7343 - val_MinusLogProbMetric: 42.7343 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 948/1000
2023-10-29 02:40:33.105 
Epoch 948/1000 
	 loss: 41.8530, MinusLogProbMetric: 41.8530, val_loss: 43.1259, val_MinusLogProbMetric: 43.1259

Epoch 948: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.8530 - MinusLogProbMetric: 41.8530 - val_loss: 43.1259 - val_MinusLogProbMetric: 43.1259 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 949/1000
2023-10-29 02:41:07.162 
Epoch 949/1000 
	 loss: 42.2224, MinusLogProbMetric: 42.2224, val_loss: 43.0924, val_MinusLogProbMetric: 43.0924

Epoch 949: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.2224 - MinusLogProbMetric: 42.2224 - val_loss: 43.0924 - val_MinusLogProbMetric: 43.0924 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 950/1000
2023-10-29 02:41:40.863 
Epoch 950/1000 
	 loss: 41.8356, MinusLogProbMetric: 41.8356, val_loss: 43.3013, val_MinusLogProbMetric: 43.3013

Epoch 950: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.8356 - MinusLogProbMetric: 41.8356 - val_loss: 43.3013 - val_MinusLogProbMetric: 43.3013 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 951/1000
2023-10-29 02:42:15.206 
Epoch 951/1000 
	 loss: 41.9385, MinusLogProbMetric: 41.9385, val_loss: 43.0193, val_MinusLogProbMetric: 43.0193

Epoch 951: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.9385 - MinusLogProbMetric: 41.9385 - val_loss: 43.0193 - val_MinusLogProbMetric: 43.0193 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 952/1000
2023-10-29 02:42:49.251 
Epoch 952/1000 
	 loss: 42.2230, MinusLogProbMetric: 42.2230, val_loss: 43.9558, val_MinusLogProbMetric: 43.9558

Epoch 952: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.2230 - MinusLogProbMetric: 42.2230 - val_loss: 43.9558 - val_MinusLogProbMetric: 43.9558 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 953/1000
2023-10-29 02:43:23.354 
Epoch 953/1000 
	 loss: 42.0326, MinusLogProbMetric: 42.0326, val_loss: 42.9950, val_MinusLogProbMetric: 42.9950

Epoch 953: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.0326 - MinusLogProbMetric: 42.0326 - val_loss: 42.9950 - val_MinusLogProbMetric: 42.9950 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 954/1000
2023-10-29 02:43:57.575 
Epoch 954/1000 
	 loss: 42.3175, MinusLogProbMetric: 42.3175, val_loss: 43.5858, val_MinusLogProbMetric: 43.5858

Epoch 954: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.3175 - MinusLogProbMetric: 42.3175 - val_loss: 43.5858 - val_MinusLogProbMetric: 43.5858 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 955/1000
2023-10-29 02:44:32.095 
Epoch 955/1000 
	 loss: 41.9373, MinusLogProbMetric: 41.9373, val_loss: 42.6925, val_MinusLogProbMetric: 42.6925

Epoch 955: val_loss did not improve from 42.59845
196/196 - 35s - loss: 41.9373 - MinusLogProbMetric: 41.9373 - val_loss: 42.6925 - val_MinusLogProbMetric: 42.6925 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 956/1000
2023-10-29 02:45:06.820 
Epoch 956/1000 
	 loss: 42.2901, MinusLogProbMetric: 42.2901, val_loss: 42.7471, val_MinusLogProbMetric: 42.7471

Epoch 956: val_loss did not improve from 42.59845
196/196 - 35s - loss: 42.2901 - MinusLogProbMetric: 42.2901 - val_loss: 42.7471 - val_MinusLogProbMetric: 42.7471 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 957/1000
2023-10-29 02:45:40.998 
Epoch 957/1000 
	 loss: 41.8185, MinusLogProbMetric: 41.8185, val_loss: 42.8051, val_MinusLogProbMetric: 42.8051

Epoch 957: val_loss did not improve from 42.59845
196/196 - 34s - loss: 41.8185 - MinusLogProbMetric: 41.8185 - val_loss: 42.8051 - val_MinusLogProbMetric: 42.8051 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 958/1000
2023-10-29 02:46:15.109 
Epoch 958/1000 
	 loss: 42.1331, MinusLogProbMetric: 42.1331, val_loss: 42.7224, val_MinusLogProbMetric: 42.7224

Epoch 958: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.1331 - MinusLogProbMetric: 42.1331 - val_loss: 42.7224 - val_MinusLogProbMetric: 42.7224 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 959/1000
2023-10-29 02:46:49.579 
Epoch 959/1000 
	 loss: 42.1100, MinusLogProbMetric: 42.1100, val_loss: 42.8418, val_MinusLogProbMetric: 42.8418

Epoch 959: val_loss did not improve from 42.59845
196/196 - 34s - loss: 42.1100 - MinusLogProbMetric: 42.1100 - val_loss: 42.8418 - val_MinusLogProbMetric: 42.8418 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 960/1000
2023-10-29 02:47:24.086 
Epoch 960/1000 
	 loss: 41.5372, MinusLogProbMetric: 41.5372, val_loss: 42.5159, val_MinusLogProbMetric: 42.5159

Epoch 960: val_loss improved from 42.59845 to 42.51587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 41.5372 - MinusLogProbMetric: 41.5372 - val_loss: 42.5159 - val_MinusLogProbMetric: 42.5159 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 961/1000
2023-10-29 02:47:58.836 
Epoch 961/1000 
	 loss: 41.5304, MinusLogProbMetric: 41.5304, val_loss: 42.5503, val_MinusLogProbMetric: 42.5503

Epoch 961: val_loss did not improve from 42.51587
196/196 - 34s - loss: 41.5304 - MinusLogProbMetric: 41.5304 - val_loss: 42.5503 - val_MinusLogProbMetric: 42.5503 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 962/1000
2023-10-29 02:48:32.785 
Epoch 962/1000 
	 loss: 41.5080, MinusLogProbMetric: 41.5080, val_loss: 42.5879, val_MinusLogProbMetric: 42.5879

Epoch 962: val_loss did not improve from 42.51587
196/196 - 34s - loss: 41.5080 - MinusLogProbMetric: 41.5080 - val_loss: 42.5879 - val_MinusLogProbMetric: 42.5879 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 963/1000
2023-10-29 02:49:07.186 
Epoch 963/1000 
	 loss: 41.4935, MinusLogProbMetric: 41.4935, val_loss: 42.6282, val_MinusLogProbMetric: 42.6282

Epoch 963: val_loss did not improve from 42.51587
196/196 - 34s - loss: 41.4935 - MinusLogProbMetric: 41.4935 - val_loss: 42.6282 - val_MinusLogProbMetric: 42.6282 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 964/1000
2023-10-29 02:49:41.812 
Epoch 964/1000 
	 loss: 41.5034, MinusLogProbMetric: 41.5034, val_loss: 42.4737, val_MinusLogProbMetric: 42.4737

Epoch 964: val_loss improved from 42.51587 to 42.47371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 41.5034 - MinusLogProbMetric: 41.5034 - val_loss: 42.4737 - val_MinusLogProbMetric: 42.4737 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 965/1000
2023-10-29 02:50:16.231 
Epoch 965/1000 
	 loss: 41.5034, MinusLogProbMetric: 41.5034, val_loss: 42.6352, val_MinusLogProbMetric: 42.6352

Epoch 965: val_loss did not improve from 42.47371
196/196 - 34s - loss: 41.5034 - MinusLogProbMetric: 41.5034 - val_loss: 42.6352 - val_MinusLogProbMetric: 42.6352 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 966/1000
2023-10-29 02:50:50.599 
Epoch 966/1000 
	 loss: 41.5231, MinusLogProbMetric: 41.5231, val_loss: 42.5897, val_MinusLogProbMetric: 42.5897

Epoch 966: val_loss did not improve from 42.47371
196/196 - 34s - loss: 41.5231 - MinusLogProbMetric: 41.5231 - val_loss: 42.5897 - val_MinusLogProbMetric: 42.5897 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 967/1000
2023-10-29 02:51:25.011 
Epoch 967/1000 
	 loss: 41.5024, MinusLogProbMetric: 41.5024, val_loss: 42.5114, val_MinusLogProbMetric: 42.5114

Epoch 967: val_loss did not improve from 42.47371
196/196 - 34s - loss: 41.5024 - MinusLogProbMetric: 41.5024 - val_loss: 42.5114 - val_MinusLogProbMetric: 42.5114 - lr: 4.1667e-05 - 34s/epoch - 176ms/step
Epoch 968/1000
2023-10-29 02:51:59.125 
Epoch 968/1000 
	 loss: 41.5108, MinusLogProbMetric: 41.5108, val_loss: 42.4925, val_MinusLogProbMetric: 42.4925

Epoch 968: val_loss did not improve from 42.47371
196/196 - 34s - loss: 41.5108 - MinusLogProbMetric: 41.5108 - val_loss: 42.4925 - val_MinusLogProbMetric: 42.4925 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 969/1000
2023-10-29 02:52:33.249 
Epoch 969/1000 
	 loss: 41.5410, MinusLogProbMetric: 41.5410, val_loss: 42.4365, val_MinusLogProbMetric: 42.4365

Epoch 969: val_loss improved from 42.47371 to 42.43653, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 41.5410 - MinusLogProbMetric: 41.5410 - val_loss: 42.4365 - val_MinusLogProbMetric: 42.4365 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 970/1000
2023-10-29 02:53:07.406 
Epoch 970/1000 
	 loss: 41.5053, MinusLogProbMetric: 41.5053, val_loss: 42.5481, val_MinusLogProbMetric: 42.5481

Epoch 970: val_loss did not improve from 42.43653
196/196 - 34s - loss: 41.5053 - MinusLogProbMetric: 41.5053 - val_loss: 42.5481 - val_MinusLogProbMetric: 42.5481 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 971/1000
2023-10-29 02:53:40.755 
Epoch 971/1000 
	 loss: 41.5246, MinusLogProbMetric: 41.5246, val_loss: 42.7394, val_MinusLogProbMetric: 42.7394

Epoch 971: val_loss did not improve from 42.43653
196/196 - 33s - loss: 41.5246 - MinusLogProbMetric: 41.5246 - val_loss: 42.7394 - val_MinusLogProbMetric: 42.7394 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 972/1000
2023-10-29 02:54:14.651 
Epoch 972/1000 
	 loss: 41.7039, MinusLogProbMetric: 41.7039, val_loss: 43.0616, val_MinusLogProbMetric: 43.0616

Epoch 972: val_loss did not improve from 42.43653
196/196 - 34s - loss: 41.7039 - MinusLogProbMetric: 41.7039 - val_loss: 43.0616 - val_MinusLogProbMetric: 43.0616 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 973/1000
2023-10-29 02:54:49.467 
Epoch 973/1000 
	 loss: 41.5500, MinusLogProbMetric: 41.5500, val_loss: 42.5999, val_MinusLogProbMetric: 42.5999

Epoch 973: val_loss did not improve from 42.43653
196/196 - 35s - loss: 41.5500 - MinusLogProbMetric: 41.5500 - val_loss: 42.5999 - val_MinusLogProbMetric: 42.5999 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 974/1000
2023-10-29 02:55:23.742 
Epoch 974/1000 
	 loss: 41.4958, MinusLogProbMetric: 41.4958, val_loss: 42.6055, val_MinusLogProbMetric: 42.6055

Epoch 974: val_loss did not improve from 42.43653
196/196 - 34s - loss: 41.4958 - MinusLogProbMetric: 41.4958 - val_loss: 42.6055 - val_MinusLogProbMetric: 42.6055 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 975/1000
2023-10-29 02:55:58.547 
Epoch 975/1000 
	 loss: 41.5080, MinusLogProbMetric: 41.5080, val_loss: 42.4896, val_MinusLogProbMetric: 42.4896

Epoch 975: val_loss did not improve from 42.43653
196/196 - 35s - loss: 41.5080 - MinusLogProbMetric: 41.5080 - val_loss: 42.4896 - val_MinusLogProbMetric: 42.4896 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 976/1000
2023-10-29 02:56:33.115 
Epoch 976/1000 
	 loss: 41.4884, MinusLogProbMetric: 41.4884, val_loss: 42.4715, val_MinusLogProbMetric: 42.4715

Epoch 976: val_loss did not improve from 42.43653
196/196 - 35s - loss: 41.4884 - MinusLogProbMetric: 41.4884 - val_loss: 42.4715 - val_MinusLogProbMetric: 42.4715 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 977/1000
2023-10-29 02:57:07.418 
Epoch 977/1000 
	 loss: 41.5048, MinusLogProbMetric: 41.5048, val_loss: 42.6952, val_MinusLogProbMetric: 42.6952

Epoch 977: val_loss did not improve from 42.43653
196/196 - 34s - loss: 41.5048 - MinusLogProbMetric: 41.5048 - val_loss: 42.6952 - val_MinusLogProbMetric: 42.6952 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 978/1000
2023-10-29 02:57:41.824 
Epoch 978/1000 
	 loss: 41.6012, MinusLogProbMetric: 41.6012, val_loss: 42.5268, val_MinusLogProbMetric: 42.5268

Epoch 978: val_loss did not improve from 42.43653
196/196 - 34s - loss: 41.6012 - MinusLogProbMetric: 41.6012 - val_loss: 42.5268 - val_MinusLogProbMetric: 42.5268 - lr: 4.1667e-05 - 34s/epoch - 176ms/step
Epoch 979/1000
2023-10-29 02:58:16.178 
Epoch 979/1000 
	 loss: 41.5570, MinusLogProbMetric: 41.5570, val_loss: 42.5870, val_MinusLogProbMetric: 42.5870

Epoch 979: val_loss did not improve from 42.43653
196/196 - 34s - loss: 41.5570 - MinusLogProbMetric: 41.5570 - val_loss: 42.5870 - val_MinusLogProbMetric: 42.5870 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 980/1000
2023-10-29 02:58:50.401 
Epoch 980/1000 
	 loss: 41.9885, MinusLogProbMetric: 41.9885, val_loss: 42.5692, val_MinusLogProbMetric: 42.5692

Epoch 980: val_loss did not improve from 42.43653
196/196 - 34s - loss: 41.9885 - MinusLogProbMetric: 41.9885 - val_loss: 42.5692 - val_MinusLogProbMetric: 42.5692 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 981/1000
2023-10-29 02:59:24.414 
Epoch 981/1000 
	 loss: 41.8537, MinusLogProbMetric: 41.8537, val_loss: 43.6541, val_MinusLogProbMetric: 43.6541

Epoch 981: val_loss did not improve from 42.43653
196/196 - 34s - loss: 41.8537 - MinusLogProbMetric: 41.8537 - val_loss: 43.6541 - val_MinusLogProbMetric: 43.6541 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 982/1000
2023-10-29 02:59:58.570 
Epoch 982/1000 
	 loss: 41.6483, MinusLogProbMetric: 41.6483, val_loss: 42.4116, val_MinusLogProbMetric: 42.4116

Epoch 982: val_loss improved from 42.43653 to 42.41162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_442/weights/best_weights.h5
196/196 - 35s - loss: 41.6483 - MinusLogProbMetric: 41.6483 - val_loss: 42.4116 - val_MinusLogProbMetric: 42.4116 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 983/1000
2023-10-29 02:00:33.462 
Epoch 983/1000 
	 loss: 41.4725, MinusLogProbMetric: 41.4725, val_loss: 42.5188, val_MinusLogProbMetric: 42.5188

Epoch 983: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.4725 - MinusLogProbMetric: 41.4725 - val_loss: 42.5188 - val_MinusLogProbMetric: 42.5188 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 984/1000
2023-10-29 02:01:07.583 
Epoch 984/1000 
	 loss: 41.5319, MinusLogProbMetric: 41.5319, val_loss: 43.9670, val_MinusLogProbMetric: 43.9670

Epoch 984: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.5319 - MinusLogProbMetric: 41.5319 - val_loss: 43.9670 - val_MinusLogProbMetric: 43.9670 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 985/1000
2023-10-29 02:01:41.733 
Epoch 985/1000 
	 loss: 41.5517, MinusLogProbMetric: 41.5517, val_loss: 42.5043, val_MinusLogProbMetric: 42.5043

Epoch 985: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.5517 - MinusLogProbMetric: 41.5517 - val_loss: 42.5043 - val_MinusLogProbMetric: 42.5043 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 986/1000
2023-10-29 02:02:16.156 
Epoch 986/1000 
	 loss: 41.7368, MinusLogProbMetric: 41.7368, val_loss: 42.5373, val_MinusLogProbMetric: 42.5373

Epoch 986: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.7368 - MinusLogProbMetric: 41.7368 - val_loss: 42.5373 - val_MinusLogProbMetric: 42.5373 - lr: 4.1667e-05 - 34s/epoch - 176ms/step
Epoch 987/1000
2023-10-29 02:02:50.644 
Epoch 987/1000 
	 loss: 41.5126, MinusLogProbMetric: 41.5126, val_loss: 42.6311, val_MinusLogProbMetric: 42.6311

Epoch 987: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.5126 - MinusLogProbMetric: 41.5126 - val_loss: 42.6311 - val_MinusLogProbMetric: 42.6311 - lr: 4.1667e-05 - 34s/epoch - 176ms/step
Epoch 988/1000
2023-10-29 02:03:25.023 
Epoch 988/1000 
	 loss: 41.4957, MinusLogProbMetric: 41.4957, val_loss: 42.4466, val_MinusLogProbMetric: 42.4466

Epoch 988: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.4957 - MinusLogProbMetric: 41.4957 - val_loss: 42.4466 - val_MinusLogProbMetric: 42.4466 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 989/1000
2023-10-29 02:03:59.680 
Epoch 989/1000 
	 loss: 42.0733, MinusLogProbMetric: 42.0733, val_loss: 42.4991, val_MinusLogProbMetric: 42.4991

Epoch 989: val_loss did not improve from 42.41162
196/196 - 35s - loss: 42.0733 - MinusLogProbMetric: 42.0733 - val_loss: 42.4991 - val_MinusLogProbMetric: 42.4991 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 990/1000
2023-10-29 02:04:34.008 
Epoch 990/1000 
	 loss: 41.4697, MinusLogProbMetric: 41.4697, val_loss: 42.5274, val_MinusLogProbMetric: 42.5274

Epoch 990: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.4697 - MinusLogProbMetric: 41.4697 - val_loss: 42.5274 - val_MinusLogProbMetric: 42.5274 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 991/1000
2023-10-29 02:05:08.578 
Epoch 991/1000 
	 loss: 42.0935, MinusLogProbMetric: 42.0935, val_loss: 42.4245, val_MinusLogProbMetric: 42.4245

Epoch 991: val_loss did not improve from 42.41162
196/196 - 35s - loss: 42.0935 - MinusLogProbMetric: 42.0935 - val_loss: 42.4245 - val_MinusLogProbMetric: 42.4245 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 992/1000
2023-10-29 02:05:42.699 
Epoch 992/1000 
	 loss: 41.4593, MinusLogProbMetric: 41.4593, val_loss: 42.5909, val_MinusLogProbMetric: 42.5909

Epoch 992: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.4593 - MinusLogProbMetric: 41.4593 - val_loss: 42.5909 - val_MinusLogProbMetric: 42.5909 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 993/1000
2023-10-29 02:06:17.036 
Epoch 993/1000 
	 loss: 41.5104, MinusLogProbMetric: 41.5104, val_loss: 42.4361, val_MinusLogProbMetric: 42.4361

Epoch 993: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.5104 - MinusLogProbMetric: 41.5104 - val_loss: 42.4361 - val_MinusLogProbMetric: 42.4361 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 994/1000
2023-10-29 02:06:51.301 
Epoch 994/1000 
	 loss: 41.5139, MinusLogProbMetric: 41.5139, val_loss: 42.5600, val_MinusLogProbMetric: 42.5600

Epoch 994: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.5139 - MinusLogProbMetric: 41.5139 - val_loss: 42.5600 - val_MinusLogProbMetric: 42.5600 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 995/1000
2023-10-29 02:07:25.457 
Epoch 995/1000 
	 loss: 41.9991, MinusLogProbMetric: 41.9991, val_loss: 42.4426, val_MinusLogProbMetric: 42.4426

Epoch 995: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.9991 - MinusLogProbMetric: 41.9991 - val_loss: 42.4426 - val_MinusLogProbMetric: 42.4426 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 996/1000
2023-10-29 02:07:59.205 
Epoch 996/1000 
	 loss: 41.4821, MinusLogProbMetric: 41.4821, val_loss: 42.6402, val_MinusLogProbMetric: 42.6402

Epoch 996: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.4821 - MinusLogProbMetric: 41.4821 - val_loss: 42.6402 - val_MinusLogProbMetric: 42.6402 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 997/1000
2023-10-29 02:08:33.721 
Epoch 997/1000 
	 loss: 41.4951, MinusLogProbMetric: 41.4951, val_loss: 42.5908, val_MinusLogProbMetric: 42.5908

Epoch 997: val_loss did not improve from 42.41162
196/196 - 35s - loss: 41.4951 - MinusLogProbMetric: 41.4951 - val_loss: 42.5908 - val_MinusLogProbMetric: 42.5908 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 998/1000
2023-10-29 02:09:08.034 
Epoch 998/1000 
	 loss: 42.0528, MinusLogProbMetric: 42.0528, val_loss: 42.9343, val_MinusLogProbMetric: 42.9343

Epoch 998: val_loss did not improve from 42.41162
196/196 - 34s - loss: 42.0528 - MinusLogProbMetric: 42.0528 - val_loss: 42.9343 - val_MinusLogProbMetric: 42.9343 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 999/1000
2023-10-29 02:09:41.875 
Epoch 999/1000 
	 loss: 41.5844, MinusLogProbMetric: 41.5844, val_loss: 42.5167, val_MinusLogProbMetric: 42.5167

Epoch 999: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.5844 - MinusLogProbMetric: 41.5844 - val_loss: 42.5167 - val_MinusLogProbMetric: 42.5167 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 1000/1000
2023-10-29 02:10:15.725 
Epoch 1000/1000 
	 loss: 41.4853, MinusLogProbMetric: 41.4853, val_loss: 42.4692, val_MinusLogProbMetric: 42.4692

Epoch 1000: val_loss did not improve from 42.41162
196/196 - 34s - loss: 41.4853 - MinusLogProbMetric: 41.4853 - val_loss: 42.4692 - val_MinusLogProbMetric: 42.4692 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 541.
Model trained in 34270.33 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.79 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.11 s.
===========
Run 442/720 done in 34341.55 s.
===========

===========
Generating train data for run 443.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_443/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_443/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_443/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_443
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_584"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_585 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_74 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_74/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_74'")
self.model: <keras.engine.functional.Functional object at 0x7f3718756d70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3742b56d40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3742b56d40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f350d5edc90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3634209d80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_443/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f363420a2f0>, <keras.callbacks.ModelCheckpoint object at 0x7f363420a3b0>, <keras.callbacks.EarlyStopping object at 0x7f363420a620>, <keras.callbacks.ReduceLROnPlateau object at 0x7f363420a650>, <keras.callbacks.TerminateOnNaN object at 0x7f363420a290>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_443/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 443/720 with hyperparameters:
timestamp = 2023-10-29 02:10:22.013423
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-29 02:11:26.315 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9710.4150, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 9710.4150 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 64s/epoch - 328ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 443.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_443/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_443/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_443/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_443
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_590"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_591 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_75 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_75/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_75'")
self.model: <keras.engine.functional.Functional object at 0x7f34ad1ebfa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34ad5cbd90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34ad5cbd90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f34ad424820>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34ad0474f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_443/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f34ad047a60>, <keras.callbacks.ModelCheckpoint object at 0x7f34ad047b20>, <keras.callbacks.EarlyStopping object at 0x7f34ad047d90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f34ad047dc0>, <keras.callbacks.TerminateOnNaN object at 0x7f34ad047a00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_443/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 443/720 with hyperparameters:
timestamp = 2023-10-29 02:11:31.703919
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
2023-10-29 02:12:50.708766: F tensorflow/tsl/platform/default/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.
