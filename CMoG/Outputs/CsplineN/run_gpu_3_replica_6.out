2023-09-26 13:24:07.117735: Importing os...
2023-09-26 13:24:07.117799: Importing sys...
2023-09-26 13:24:07.117812: Importing and initializing argparse...
Visible devices: [3]
2023-09-26 13:24:07.134005: Importing timer from timeit...
2023-09-26 13:24:07.134556: Setting env variables for tf import (only device [3] will be available)...
2023-09-26 13:24:07.134601: Importing numpy...
2023-09-26 13:24:07.294506: Importing pandas...
2023-09-26 13:24:07.433061: Importing shutil...
2023-09-26 13:24:07.433086: Importing subprocess...
2023-09-26 13:24:07.433091: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-26 13:24:09.041948: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-26 13:24:09.572826: Importing textwrap...
2023-09-26 13:24:09.572853: Importing timeit...
2023-09-26 13:24:09.572862: Importing traceback...
2023-09-26 13:24:09.572868: Importing typing...
2023-09-26 13:24:09.572877: Setting tf configs...
2023-09-26 13:24:09.679422: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-26 13:24:10.647390: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

===========
Generating train data for run 292.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_292/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_292/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_292/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_292
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1399280   
 r)                                                              
                                                                 
=================================================================
Total params: 1,399,280
Trainable params: 1,399,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7fed6069b8e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed7c349210>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed7c349210>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fed7c349a80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fed7c34b5b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fed7c34a6b0>, <keras.callbacks.ModelCheckpoint object at 0x7fed7c34acb0>, <keras.callbacks.EarlyStopping object at 0x7fed7c349c60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fed7c34a800>, <keras.callbacks.TerminateOnNaN object at 0x7fed7c349ea0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_292/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 292/720 with hyperparameters:
timestamp = 2023-09-26 13:24:15.408876
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1399280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-26 13:26:04.063 
Epoch 1/1000 
	 loss: 60.9829, MinusLogProbMetric: 60.9829, val_loss: 25.8239, val_MinusLogProbMetric: 25.8239

Epoch 1: val_loss improved from inf to 25.82394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 109s - loss: 60.9829 - MinusLogProbMetric: 60.9829 - val_loss: 25.8239 - val_MinusLogProbMetric: 25.8239 - lr: 0.0010 - 109s/epoch - 556ms/step
Epoch 2/1000
2023-09-26 13:26:37.167 
Epoch 2/1000 
	 loss: 24.5317, MinusLogProbMetric: 24.5317, val_loss: 23.3133, val_MinusLogProbMetric: 23.3133

Epoch 2: val_loss improved from 25.82394 to 23.31334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 34s - loss: 24.5317 - MinusLogProbMetric: 24.5317 - val_loss: 23.3133 - val_MinusLogProbMetric: 23.3133 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 3/1000
2023-09-26 13:27:10.095 
Epoch 3/1000 
	 loss: 22.3276, MinusLogProbMetric: 22.3276, val_loss: 22.0201, val_MinusLogProbMetric: 22.0201

Epoch 3: val_loss improved from 23.31334 to 22.02009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 32s - loss: 22.3276 - MinusLogProbMetric: 22.3276 - val_loss: 22.0201 - val_MinusLogProbMetric: 22.0201 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 4/1000
2023-09-26 13:27:43.008 
Epoch 4/1000 
	 loss: 20.7538, MinusLogProbMetric: 20.7538, val_loss: 20.5249, val_MinusLogProbMetric: 20.5249

Epoch 4: val_loss improved from 22.02009 to 20.52490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 34s - loss: 20.7538 - MinusLogProbMetric: 20.7538 - val_loss: 20.5249 - val_MinusLogProbMetric: 20.5249 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 5/1000
2023-09-26 13:28:17.827 
Epoch 5/1000 
	 loss: 19.9397, MinusLogProbMetric: 19.9397, val_loss: 19.6440, val_MinusLogProbMetric: 19.6440

Epoch 5: val_loss improved from 20.52490 to 19.64404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 35s - loss: 19.9397 - MinusLogProbMetric: 19.9397 - val_loss: 19.6440 - val_MinusLogProbMetric: 19.6440 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 6/1000
2023-09-26 13:28:56.611 
Epoch 6/1000 
	 loss: 19.7263, MinusLogProbMetric: 19.7263, val_loss: 20.2485, val_MinusLogProbMetric: 20.2485

Epoch 6: val_loss did not improve from 19.64404
196/196 - 37s - loss: 19.7263 - MinusLogProbMetric: 19.7263 - val_loss: 20.2485 - val_MinusLogProbMetric: 20.2485 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 7/1000
2023-09-26 13:29:36.234 
Epoch 7/1000 
	 loss: 19.3125, MinusLogProbMetric: 19.3125, val_loss: 18.9166, val_MinusLogProbMetric: 18.9166

Epoch 7: val_loss improved from 19.64404 to 18.91662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 40s - loss: 19.3125 - MinusLogProbMetric: 19.3125 - val_loss: 18.9166 - val_MinusLogProbMetric: 18.9166 - lr: 0.0010 - 40s/epoch - 207ms/step
Epoch 8/1000
2023-09-26 13:30:17.314 
Epoch 8/1000 
	 loss: 18.9575, MinusLogProbMetric: 18.9575, val_loss: 19.6575, val_MinusLogProbMetric: 19.6575

Epoch 8: val_loss did not improve from 18.91662
196/196 - 40s - loss: 18.9575 - MinusLogProbMetric: 18.9575 - val_loss: 19.6575 - val_MinusLogProbMetric: 19.6575 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 9/1000
2023-09-26 13:30:54.606 
Epoch 9/1000 
	 loss: 19.0532, MinusLogProbMetric: 19.0532, val_loss: 18.9488, val_MinusLogProbMetric: 18.9488

Epoch 9: val_loss did not improve from 18.91662
196/196 - 37s - loss: 19.0532 - MinusLogProbMetric: 19.0532 - val_loss: 18.9488 - val_MinusLogProbMetric: 18.9488 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 10/1000
2023-09-26 13:31:34.454 
Epoch 10/1000 
	 loss: 18.8115, MinusLogProbMetric: 18.8115, val_loss: 19.0649, val_MinusLogProbMetric: 19.0649

Epoch 10: val_loss did not improve from 18.91662
196/196 - 40s - loss: 18.8115 - MinusLogProbMetric: 18.8115 - val_loss: 19.0649 - val_MinusLogProbMetric: 19.0649 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 11/1000
2023-09-26 13:32:14.540 
Epoch 11/1000 
	 loss: 18.6396, MinusLogProbMetric: 18.6396, val_loss: 18.6892, val_MinusLogProbMetric: 18.6892

Epoch 11: val_loss improved from 18.91662 to 18.68920, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 18.6396 - MinusLogProbMetric: 18.6396 - val_loss: 18.6892 - val_MinusLogProbMetric: 18.6892 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 12/1000
2023-09-26 13:32:55.440 
Epoch 12/1000 
	 loss: 18.4241, MinusLogProbMetric: 18.4241, val_loss: 18.2570, val_MinusLogProbMetric: 18.2570

Epoch 12: val_loss improved from 18.68920 to 18.25696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 18.4241 - MinusLogProbMetric: 18.4241 - val_loss: 18.2570 - val_MinusLogProbMetric: 18.2570 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 13/1000
2023-09-26 13:33:36.012 
Epoch 13/1000 
	 loss: 18.3826, MinusLogProbMetric: 18.3826, val_loss: 18.2288, val_MinusLogProbMetric: 18.2288

Epoch 13: val_loss improved from 18.25696 to 18.22882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 40s - loss: 18.3826 - MinusLogProbMetric: 18.3826 - val_loss: 18.2288 - val_MinusLogProbMetric: 18.2288 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 14/1000
2023-09-26 13:34:16.747 
Epoch 14/1000 
	 loss: 18.3582, MinusLogProbMetric: 18.3582, val_loss: 19.0864, val_MinusLogProbMetric: 19.0864

Epoch 14: val_loss did not improve from 18.22882
196/196 - 40s - loss: 18.3582 - MinusLogProbMetric: 18.3582 - val_loss: 19.0864 - val_MinusLogProbMetric: 19.0864 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 15/1000
2023-09-26 13:34:56.822 
Epoch 15/1000 
	 loss: 18.1121, MinusLogProbMetric: 18.1121, val_loss: 17.9386, val_MinusLogProbMetric: 17.9386

Epoch 15: val_loss improved from 18.22882 to 17.93859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 18.1121 - MinusLogProbMetric: 18.1121 - val_loss: 17.9386 - val_MinusLogProbMetric: 17.9386 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 16/1000
2023-09-26 13:35:37.937 
Epoch 16/1000 
	 loss: 18.0231, MinusLogProbMetric: 18.0231, val_loss: 18.2463, val_MinusLogProbMetric: 18.2463

Epoch 16: val_loss did not improve from 17.93859
196/196 - 40s - loss: 18.0231 - MinusLogProbMetric: 18.0231 - val_loss: 18.2463 - val_MinusLogProbMetric: 18.2463 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 17/1000
2023-09-26 13:36:17.659 
Epoch 17/1000 
	 loss: 17.9532, MinusLogProbMetric: 17.9532, val_loss: 18.1077, val_MinusLogProbMetric: 18.1077

Epoch 17: val_loss did not improve from 17.93859
196/196 - 40s - loss: 17.9532 - MinusLogProbMetric: 17.9532 - val_loss: 18.1077 - val_MinusLogProbMetric: 18.1077 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 18/1000
2023-09-26 13:36:57.899 
Epoch 18/1000 
	 loss: 17.7185, MinusLogProbMetric: 17.7185, val_loss: 18.4207, val_MinusLogProbMetric: 18.4207

Epoch 18: val_loss did not improve from 17.93859
196/196 - 40s - loss: 17.7185 - MinusLogProbMetric: 17.7185 - val_loss: 18.4207 - val_MinusLogProbMetric: 18.4207 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 19/1000
2023-09-26 13:37:38.220 
Epoch 19/1000 
	 loss: 17.6583, MinusLogProbMetric: 17.6583, val_loss: 17.6992, val_MinusLogProbMetric: 17.6992

Epoch 19: val_loss improved from 17.93859 to 17.69915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 17.6583 - MinusLogProbMetric: 17.6583 - val_loss: 17.6992 - val_MinusLogProbMetric: 17.6992 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 20/1000
2023-09-26 13:38:18.543 
Epoch 20/1000 
	 loss: 17.5939, MinusLogProbMetric: 17.5939, val_loss: 17.8068, val_MinusLogProbMetric: 17.8068

Epoch 20: val_loss did not improve from 17.69915
196/196 - 40s - loss: 17.5939 - MinusLogProbMetric: 17.5939 - val_loss: 17.8068 - val_MinusLogProbMetric: 17.8068 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 21/1000
2023-09-26 13:38:58.739 
Epoch 21/1000 
	 loss: 17.6147, MinusLogProbMetric: 17.6147, val_loss: 18.1822, val_MinusLogProbMetric: 18.1822

Epoch 21: val_loss did not improve from 17.69915
196/196 - 40s - loss: 17.6147 - MinusLogProbMetric: 17.6147 - val_loss: 18.1822 - val_MinusLogProbMetric: 18.1822 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 22/1000
2023-09-26 13:39:38.135 
Epoch 22/1000 
	 loss: 17.6102, MinusLogProbMetric: 17.6102, val_loss: 17.7820, val_MinusLogProbMetric: 17.7820

Epoch 22: val_loss did not improve from 17.69915
196/196 - 39s - loss: 17.6102 - MinusLogProbMetric: 17.6102 - val_loss: 17.7820 - val_MinusLogProbMetric: 17.7820 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 23/1000
2023-09-26 13:40:18.140 
Epoch 23/1000 
	 loss: 17.4526, MinusLogProbMetric: 17.4526, val_loss: 18.0419, val_MinusLogProbMetric: 18.0419

Epoch 23: val_loss did not improve from 17.69915
196/196 - 40s - loss: 17.4526 - MinusLogProbMetric: 17.4526 - val_loss: 18.0419 - val_MinusLogProbMetric: 18.0419 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 24/1000
2023-09-26 13:40:57.725 
Epoch 24/1000 
	 loss: 17.4186, MinusLogProbMetric: 17.4186, val_loss: 17.6286, val_MinusLogProbMetric: 17.6286

Epoch 24: val_loss improved from 17.69915 to 17.62858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 40s - loss: 17.4186 - MinusLogProbMetric: 17.4186 - val_loss: 17.6286 - val_MinusLogProbMetric: 17.6286 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 25/1000
2023-09-26 13:41:37.986 
Epoch 25/1000 
	 loss: 17.4121, MinusLogProbMetric: 17.4121, val_loss: 17.5714, val_MinusLogProbMetric: 17.5714

Epoch 25: val_loss improved from 17.62858 to 17.57137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 40s - loss: 17.4121 - MinusLogProbMetric: 17.4121 - val_loss: 17.5714 - val_MinusLogProbMetric: 17.5714 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 26/1000
2023-09-26 13:42:18.675 
Epoch 26/1000 
	 loss: 17.3546, MinusLogProbMetric: 17.3546, val_loss: 17.5149, val_MinusLogProbMetric: 17.5149

Epoch 26: val_loss improved from 17.57137 to 17.51491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 17.3546 - MinusLogProbMetric: 17.3546 - val_loss: 17.5149 - val_MinusLogProbMetric: 17.5149 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 27/1000
2023-09-26 13:42:58.763 
Epoch 27/1000 
	 loss: 17.3732, MinusLogProbMetric: 17.3732, val_loss: 17.7448, val_MinusLogProbMetric: 17.7448

Epoch 27: val_loss did not improve from 17.51491
196/196 - 39s - loss: 17.3732 - MinusLogProbMetric: 17.3732 - val_loss: 17.7448 - val_MinusLogProbMetric: 17.7448 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 28/1000
2023-09-26 13:43:38.535 
Epoch 28/1000 
	 loss: 17.3177, MinusLogProbMetric: 17.3177, val_loss: 17.5855, val_MinusLogProbMetric: 17.5855

Epoch 28: val_loss did not improve from 17.51491
196/196 - 40s - loss: 17.3177 - MinusLogProbMetric: 17.3177 - val_loss: 17.5855 - val_MinusLogProbMetric: 17.5855 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 29/1000
2023-09-26 13:44:18.244 
Epoch 29/1000 
	 loss: 17.3005, MinusLogProbMetric: 17.3005, val_loss: 17.6527, val_MinusLogProbMetric: 17.6527

Epoch 29: val_loss did not improve from 17.51491
196/196 - 40s - loss: 17.3005 - MinusLogProbMetric: 17.3005 - val_loss: 17.6527 - val_MinusLogProbMetric: 17.6527 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 30/1000
2023-09-26 13:44:57.986 
Epoch 30/1000 
	 loss: 17.2713, MinusLogProbMetric: 17.2713, val_loss: 17.4135, val_MinusLogProbMetric: 17.4135

Epoch 30: val_loss improved from 17.51491 to 17.41347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 17.2713 - MinusLogProbMetric: 17.2713 - val_loss: 17.4135 - val_MinusLogProbMetric: 17.4135 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 31/1000
2023-09-26 13:45:38.620 
Epoch 31/1000 
	 loss: 17.2852, MinusLogProbMetric: 17.2852, val_loss: 17.5382, val_MinusLogProbMetric: 17.5382

Epoch 31: val_loss did not improve from 17.41347
196/196 - 40s - loss: 17.2852 - MinusLogProbMetric: 17.2852 - val_loss: 17.5382 - val_MinusLogProbMetric: 17.5382 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 32/1000
2023-09-26 13:46:18.764 
Epoch 32/1000 
	 loss: 17.2288, MinusLogProbMetric: 17.2288, val_loss: 17.4783, val_MinusLogProbMetric: 17.4783

Epoch 32: val_loss did not improve from 17.41347
196/196 - 40s - loss: 17.2288 - MinusLogProbMetric: 17.2288 - val_loss: 17.4783 - val_MinusLogProbMetric: 17.4783 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 33/1000
2023-09-26 13:46:58.956 
Epoch 33/1000 
	 loss: 17.2442, MinusLogProbMetric: 17.2442, val_loss: 17.6081, val_MinusLogProbMetric: 17.6081

Epoch 33: val_loss did not improve from 17.41347
196/196 - 40s - loss: 17.2442 - MinusLogProbMetric: 17.2442 - val_loss: 17.6081 - val_MinusLogProbMetric: 17.6081 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 34/1000
2023-09-26 13:47:38.928 
Epoch 34/1000 
	 loss: 17.1567, MinusLogProbMetric: 17.1567, val_loss: 17.6036, val_MinusLogProbMetric: 17.6036

Epoch 34: val_loss did not improve from 17.41347
196/196 - 40s - loss: 17.1567 - MinusLogProbMetric: 17.1567 - val_loss: 17.6036 - val_MinusLogProbMetric: 17.6036 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 35/1000
2023-09-26 13:48:18.855 
Epoch 35/1000 
	 loss: 17.1883, MinusLogProbMetric: 17.1883, val_loss: 17.5928, val_MinusLogProbMetric: 17.5928

Epoch 35: val_loss did not improve from 17.41347
196/196 - 40s - loss: 17.1883 - MinusLogProbMetric: 17.1883 - val_loss: 17.5928 - val_MinusLogProbMetric: 17.5928 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 36/1000
2023-09-26 13:48:58.759 
Epoch 36/1000 
	 loss: 17.1516, MinusLogProbMetric: 17.1516, val_loss: 17.6625, val_MinusLogProbMetric: 17.6625

Epoch 36: val_loss did not improve from 17.41347
196/196 - 40s - loss: 17.1516 - MinusLogProbMetric: 17.1516 - val_loss: 17.6625 - val_MinusLogProbMetric: 17.6625 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 37/1000
2023-09-26 13:49:38.660 
Epoch 37/1000 
	 loss: 17.1154, MinusLogProbMetric: 17.1154, val_loss: 17.5691, val_MinusLogProbMetric: 17.5691

Epoch 37: val_loss did not improve from 17.41347
196/196 - 40s - loss: 17.1154 - MinusLogProbMetric: 17.1154 - val_loss: 17.5691 - val_MinusLogProbMetric: 17.5691 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 38/1000
2023-09-26 13:50:18.366 
Epoch 38/1000 
	 loss: 17.1250, MinusLogProbMetric: 17.1250, val_loss: 17.4368, val_MinusLogProbMetric: 17.4368

Epoch 38: val_loss did not improve from 17.41347
196/196 - 40s - loss: 17.1250 - MinusLogProbMetric: 17.1250 - val_loss: 17.4368 - val_MinusLogProbMetric: 17.4368 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 39/1000
2023-09-26 13:50:58.335 
Epoch 39/1000 
	 loss: 17.0862, MinusLogProbMetric: 17.0862, val_loss: 17.3789, val_MinusLogProbMetric: 17.3789

Epoch 39: val_loss improved from 17.41347 to 17.37889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 17.0862 - MinusLogProbMetric: 17.0862 - val_loss: 17.3789 - val_MinusLogProbMetric: 17.3789 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 40/1000
2023-09-26 13:51:39.288 
Epoch 40/1000 
	 loss: 17.1017, MinusLogProbMetric: 17.1017, val_loss: 17.4402, val_MinusLogProbMetric: 17.4402

Epoch 40: val_loss did not improve from 17.37889
196/196 - 40s - loss: 17.1017 - MinusLogProbMetric: 17.1017 - val_loss: 17.4402 - val_MinusLogProbMetric: 17.4402 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 41/1000
2023-09-26 13:52:19.549 
Epoch 41/1000 
	 loss: 17.0417, MinusLogProbMetric: 17.0417, val_loss: 17.3405, val_MinusLogProbMetric: 17.3405

Epoch 41: val_loss improved from 17.37889 to 17.34052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 17.0417 - MinusLogProbMetric: 17.0417 - val_loss: 17.3405 - val_MinusLogProbMetric: 17.3405 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 42/1000
2023-09-26 13:52:59.805 
Epoch 42/1000 
	 loss: 17.0320, MinusLogProbMetric: 17.0320, val_loss: 17.4044, val_MinusLogProbMetric: 17.4044

Epoch 42: val_loss did not improve from 17.34052
196/196 - 40s - loss: 17.0320 - MinusLogProbMetric: 17.0320 - val_loss: 17.4044 - val_MinusLogProbMetric: 17.4044 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 43/1000
2023-09-26 13:53:40.103 
Epoch 43/1000 
	 loss: 17.0070, MinusLogProbMetric: 17.0070, val_loss: 17.5917, val_MinusLogProbMetric: 17.5917

Epoch 43: val_loss did not improve from 17.34052
196/196 - 40s - loss: 17.0070 - MinusLogProbMetric: 17.0070 - val_loss: 17.5917 - val_MinusLogProbMetric: 17.5917 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 44/1000
2023-09-26 13:54:20.346 
Epoch 44/1000 
	 loss: 17.0271, MinusLogProbMetric: 17.0271, val_loss: 17.7489, val_MinusLogProbMetric: 17.7489

Epoch 44: val_loss did not improve from 17.34052
196/196 - 40s - loss: 17.0271 - MinusLogProbMetric: 17.0271 - val_loss: 17.7489 - val_MinusLogProbMetric: 17.7489 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 45/1000
2023-09-26 13:55:00.318 
Epoch 45/1000 
	 loss: 17.0104, MinusLogProbMetric: 17.0104, val_loss: 17.7500, val_MinusLogProbMetric: 17.7500

Epoch 45: val_loss did not improve from 17.34052
196/196 - 40s - loss: 17.0104 - MinusLogProbMetric: 17.0104 - val_loss: 17.7500 - val_MinusLogProbMetric: 17.7500 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 46/1000
2023-09-26 13:55:40.104 
Epoch 46/1000 
	 loss: 16.9721, MinusLogProbMetric: 16.9721, val_loss: 17.6172, val_MinusLogProbMetric: 17.6172

Epoch 46: val_loss did not improve from 17.34052
196/196 - 40s - loss: 16.9721 - MinusLogProbMetric: 16.9721 - val_loss: 17.6172 - val_MinusLogProbMetric: 17.6172 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 47/1000
2023-09-26 13:56:19.727 
Epoch 47/1000 
	 loss: 16.9837, MinusLogProbMetric: 16.9837, val_loss: 17.3026, val_MinusLogProbMetric: 17.3026

Epoch 47: val_loss improved from 17.34052 to 17.30264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 40s - loss: 16.9837 - MinusLogProbMetric: 16.9837 - val_loss: 17.3026 - val_MinusLogProbMetric: 17.3026 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 48/1000
2023-09-26 13:57:00.152 
Epoch 48/1000 
	 loss: 16.9704, MinusLogProbMetric: 16.9704, val_loss: 17.3063, val_MinusLogProbMetric: 17.3063

Epoch 48: val_loss did not improve from 17.30264
196/196 - 40s - loss: 16.9704 - MinusLogProbMetric: 16.9704 - val_loss: 17.3063 - val_MinusLogProbMetric: 17.3063 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 49/1000
2023-09-26 13:57:40.434 
Epoch 49/1000 
	 loss: 16.9500, MinusLogProbMetric: 16.9500, val_loss: 17.3200, val_MinusLogProbMetric: 17.3200

Epoch 49: val_loss did not improve from 17.30264
196/196 - 40s - loss: 16.9500 - MinusLogProbMetric: 16.9500 - val_loss: 17.3200 - val_MinusLogProbMetric: 17.3200 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 50/1000
2023-09-26 13:58:21.336 
Epoch 50/1000 
	 loss: 16.9454, MinusLogProbMetric: 16.9454, val_loss: 17.2417, val_MinusLogProbMetric: 17.2417

Epoch 50: val_loss improved from 17.30264 to 17.24165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 42s - loss: 16.9454 - MinusLogProbMetric: 16.9454 - val_loss: 17.2417 - val_MinusLogProbMetric: 17.2417 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 51/1000
2023-09-26 13:59:02.305 
Epoch 51/1000 
	 loss: 16.9345, MinusLogProbMetric: 16.9345, val_loss: 17.4365, val_MinusLogProbMetric: 17.4365

Epoch 51: val_loss did not improve from 17.24165
196/196 - 40s - loss: 16.9345 - MinusLogProbMetric: 16.9345 - val_loss: 17.4365 - val_MinusLogProbMetric: 17.4365 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 52/1000
2023-09-26 13:59:42.381 
Epoch 52/1000 
	 loss: 16.9296, MinusLogProbMetric: 16.9296, val_loss: 17.3775, val_MinusLogProbMetric: 17.3775

Epoch 52: val_loss did not improve from 17.24165
196/196 - 40s - loss: 16.9296 - MinusLogProbMetric: 16.9296 - val_loss: 17.3775 - val_MinusLogProbMetric: 17.3775 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 53/1000
2023-09-26 14:00:22.200 
Epoch 53/1000 
	 loss: 16.9197, MinusLogProbMetric: 16.9197, val_loss: 17.4895, val_MinusLogProbMetric: 17.4895

Epoch 53: val_loss did not improve from 17.24165
196/196 - 40s - loss: 16.9197 - MinusLogProbMetric: 16.9197 - val_loss: 17.4895 - val_MinusLogProbMetric: 17.4895 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 54/1000
2023-09-26 14:01:01.821 
Epoch 54/1000 
	 loss: 16.8808, MinusLogProbMetric: 16.8808, val_loss: 17.4718, val_MinusLogProbMetric: 17.4718

Epoch 54: val_loss did not improve from 17.24165
196/196 - 40s - loss: 16.8808 - MinusLogProbMetric: 16.8808 - val_loss: 17.4718 - val_MinusLogProbMetric: 17.4718 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 55/1000
2023-09-26 14:01:41.785 
Epoch 55/1000 
	 loss: 16.8798, MinusLogProbMetric: 16.8798, val_loss: 17.4623, val_MinusLogProbMetric: 17.4623

Epoch 55: val_loss did not improve from 17.24165
196/196 - 40s - loss: 16.8798 - MinusLogProbMetric: 16.8798 - val_loss: 17.4623 - val_MinusLogProbMetric: 17.4623 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 56/1000
2023-09-26 14:02:21.549 
Epoch 56/1000 
	 loss: 16.8825, MinusLogProbMetric: 16.8825, val_loss: 17.2274, val_MinusLogProbMetric: 17.2274

Epoch 56: val_loss improved from 17.24165 to 17.22740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 40s - loss: 16.8825 - MinusLogProbMetric: 16.8825 - val_loss: 17.2274 - val_MinusLogProbMetric: 17.2274 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 57/1000
2023-09-26 14:03:02.297 
Epoch 57/1000 
	 loss: 16.8367, MinusLogProbMetric: 16.8367, val_loss: 17.9282, val_MinusLogProbMetric: 17.9282

Epoch 57: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.8367 - MinusLogProbMetric: 16.8367 - val_loss: 17.9282 - val_MinusLogProbMetric: 17.9282 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 58/1000
2023-09-26 14:03:41.887 
Epoch 58/1000 
	 loss: 16.8637, MinusLogProbMetric: 16.8637, val_loss: 17.2943, val_MinusLogProbMetric: 17.2943

Epoch 58: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.8637 - MinusLogProbMetric: 16.8637 - val_loss: 17.2943 - val_MinusLogProbMetric: 17.2943 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 59/1000
2023-09-26 14:04:21.923 
Epoch 59/1000 
	 loss: 16.8352, MinusLogProbMetric: 16.8352, val_loss: 17.5233, val_MinusLogProbMetric: 17.5233

Epoch 59: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.8352 - MinusLogProbMetric: 16.8352 - val_loss: 17.5233 - val_MinusLogProbMetric: 17.5233 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 60/1000
2023-09-26 14:05:01.504 
Epoch 60/1000 
	 loss: 16.8394, MinusLogProbMetric: 16.8394, val_loss: 17.3318, val_MinusLogProbMetric: 17.3318

Epoch 60: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.8394 - MinusLogProbMetric: 16.8394 - val_loss: 17.3318 - val_MinusLogProbMetric: 17.3318 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 61/1000
2023-09-26 14:05:41.462 
Epoch 61/1000 
	 loss: 16.7830, MinusLogProbMetric: 16.7830, val_loss: 17.3040, val_MinusLogProbMetric: 17.3040

Epoch 61: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.7830 - MinusLogProbMetric: 16.7830 - val_loss: 17.3040 - val_MinusLogProbMetric: 17.3040 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 62/1000
2023-09-26 14:06:21.179 
Epoch 62/1000 
	 loss: 16.7674, MinusLogProbMetric: 16.7674, val_loss: 17.4266, val_MinusLogProbMetric: 17.4266

Epoch 62: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.7674 - MinusLogProbMetric: 16.7674 - val_loss: 17.4266 - val_MinusLogProbMetric: 17.4266 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 63/1000
2023-09-26 14:07:01.070 
Epoch 63/1000 
	 loss: 16.7585, MinusLogProbMetric: 16.7585, val_loss: 17.3594, val_MinusLogProbMetric: 17.3594

Epoch 63: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.7585 - MinusLogProbMetric: 16.7585 - val_loss: 17.3594 - val_MinusLogProbMetric: 17.3594 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 64/1000
2023-09-26 14:07:40.611 
Epoch 64/1000 
	 loss: 16.8142, MinusLogProbMetric: 16.8142, val_loss: 17.2424, val_MinusLogProbMetric: 17.2424

Epoch 64: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.8142 - MinusLogProbMetric: 16.8142 - val_loss: 17.2424 - val_MinusLogProbMetric: 17.2424 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 65/1000
2023-09-26 14:08:20.634 
Epoch 65/1000 
	 loss: 16.7812, MinusLogProbMetric: 16.7812, val_loss: 17.4648, val_MinusLogProbMetric: 17.4648

Epoch 65: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.7812 - MinusLogProbMetric: 16.7812 - val_loss: 17.4648 - val_MinusLogProbMetric: 17.4648 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 66/1000
2023-09-26 14:09:00.191 
Epoch 66/1000 
	 loss: 16.7549, MinusLogProbMetric: 16.7549, val_loss: 17.4479, val_MinusLogProbMetric: 17.4479

Epoch 66: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.7549 - MinusLogProbMetric: 16.7549 - val_loss: 17.4479 - val_MinusLogProbMetric: 17.4479 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 67/1000
2023-09-26 14:09:40.065 
Epoch 67/1000 
	 loss: 16.7361, MinusLogProbMetric: 16.7361, val_loss: 17.3798, val_MinusLogProbMetric: 17.3798

Epoch 67: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.7361 - MinusLogProbMetric: 16.7361 - val_loss: 17.3798 - val_MinusLogProbMetric: 17.3798 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 68/1000
2023-09-26 14:10:19.481 
Epoch 68/1000 
	 loss: 16.7452, MinusLogProbMetric: 16.7452, val_loss: 17.3408, val_MinusLogProbMetric: 17.3408

Epoch 68: val_loss did not improve from 17.22740
196/196 - 39s - loss: 16.7452 - MinusLogProbMetric: 16.7452 - val_loss: 17.3408 - val_MinusLogProbMetric: 17.3408 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 69/1000
2023-09-26 14:10:59.131 
Epoch 69/1000 
	 loss: 16.7241, MinusLogProbMetric: 16.7241, val_loss: 17.2937, val_MinusLogProbMetric: 17.2937

Epoch 69: val_loss did not improve from 17.22740
196/196 - 40s - loss: 16.7241 - MinusLogProbMetric: 16.7241 - val_loss: 17.2937 - val_MinusLogProbMetric: 17.2937 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 70/1000
2023-09-26 14:11:39.118 
Epoch 70/1000 
	 loss: 16.7781, MinusLogProbMetric: 16.7781, val_loss: 17.2127, val_MinusLogProbMetric: 17.2127

Epoch 70: val_loss improved from 17.22740 to 17.21273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 16.7781 - MinusLogProbMetric: 16.7781 - val_loss: 17.2127 - val_MinusLogProbMetric: 17.2127 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 71/1000
2023-09-26 14:12:19.748 
Epoch 71/1000 
	 loss: 16.7081, MinusLogProbMetric: 16.7081, val_loss: 17.4899, val_MinusLogProbMetric: 17.4899

Epoch 71: val_loss did not improve from 17.21273
196/196 - 40s - loss: 16.7081 - MinusLogProbMetric: 16.7081 - val_loss: 17.4899 - val_MinusLogProbMetric: 17.4899 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 72/1000
2023-09-26 14:12:59.358 
Epoch 72/1000 
	 loss: 16.7068, MinusLogProbMetric: 16.7068, val_loss: 17.1506, val_MinusLogProbMetric: 17.1506

Epoch 72: val_loss improved from 17.21273 to 17.15057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 40s - loss: 16.7068 - MinusLogProbMetric: 16.7068 - val_loss: 17.1506 - val_MinusLogProbMetric: 17.1506 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 73/1000
2023-09-26 14:13:39.931 
Epoch 73/1000 
	 loss: 16.7096, MinusLogProbMetric: 16.7096, val_loss: 17.4161, val_MinusLogProbMetric: 17.4161

Epoch 73: val_loss did not improve from 17.15057
196/196 - 40s - loss: 16.7096 - MinusLogProbMetric: 16.7096 - val_loss: 17.4161 - val_MinusLogProbMetric: 17.4161 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 74/1000
2023-09-26 14:14:20.067 
Epoch 74/1000 
	 loss: 16.6757, MinusLogProbMetric: 16.6757, val_loss: 17.7188, val_MinusLogProbMetric: 17.7188

Epoch 74: val_loss did not improve from 17.15057
196/196 - 40s - loss: 16.6757 - MinusLogProbMetric: 16.6757 - val_loss: 17.7188 - val_MinusLogProbMetric: 17.7188 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 75/1000
2023-09-26 14:14:59.834 
Epoch 75/1000 
	 loss: 16.6784, MinusLogProbMetric: 16.6784, val_loss: 17.2226, val_MinusLogProbMetric: 17.2226

Epoch 75: val_loss did not improve from 17.15057
196/196 - 40s - loss: 16.6784 - MinusLogProbMetric: 16.6784 - val_loss: 17.2226 - val_MinusLogProbMetric: 17.2226 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 76/1000
2023-09-26 14:15:39.895 
Epoch 76/1000 
	 loss: 16.6361, MinusLogProbMetric: 16.6361, val_loss: 17.5249, val_MinusLogProbMetric: 17.5249

Epoch 76: val_loss did not improve from 17.15057
196/196 - 40s - loss: 16.6361 - MinusLogProbMetric: 16.6361 - val_loss: 17.5249 - val_MinusLogProbMetric: 17.5249 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 77/1000
2023-09-26 14:16:20.339 
Epoch 77/1000 
	 loss: 16.6485, MinusLogProbMetric: 16.6485, val_loss: 17.1753, val_MinusLogProbMetric: 17.1753

Epoch 77: val_loss did not improve from 17.15057
196/196 - 40s - loss: 16.6485 - MinusLogProbMetric: 16.6485 - val_loss: 17.1753 - val_MinusLogProbMetric: 17.1753 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 78/1000
2023-09-26 14:17:00.108 
Epoch 78/1000 
	 loss: 16.6569, MinusLogProbMetric: 16.6569, val_loss: 17.6112, val_MinusLogProbMetric: 17.6112

Epoch 78: val_loss did not improve from 17.15057
196/196 - 40s - loss: 16.6569 - MinusLogProbMetric: 16.6569 - val_loss: 17.6112 - val_MinusLogProbMetric: 17.6112 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 79/1000
2023-09-26 14:17:40.446 
Epoch 79/1000 
	 loss: 16.6222, MinusLogProbMetric: 16.6222, val_loss: 17.3753, val_MinusLogProbMetric: 17.3753

Epoch 79: val_loss did not improve from 17.15057
196/196 - 40s - loss: 16.6222 - MinusLogProbMetric: 16.6222 - val_loss: 17.3753 - val_MinusLogProbMetric: 17.3753 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 80/1000
2023-09-26 14:18:20.437 
Epoch 80/1000 
	 loss: 16.5951, MinusLogProbMetric: 16.5951, val_loss: 17.1472, val_MinusLogProbMetric: 17.1472

Epoch 80: val_loss improved from 17.15057 to 17.14722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_292/weights/best_weights.h5
196/196 - 41s - loss: 16.5951 - MinusLogProbMetric: 16.5951 - val_loss: 17.1472 - val_MinusLogProbMetric: 17.1472 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 81/1000
2023-09-26 14:19:00.927 
Epoch 81/1000 
	 loss: 16.5918, MinusLogProbMetric: 16.5918, val_loss: 17.2196, val_MinusLogProbMetric: 17.2196

Epoch 81: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.5918 - MinusLogProbMetric: 16.5918 - val_loss: 17.2196 - val_MinusLogProbMetric: 17.2196 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 82/1000
2023-09-26 14:19:41.310 
Epoch 82/1000 
	 loss: 16.6103, MinusLogProbMetric: 16.6103, val_loss: 17.2310, val_MinusLogProbMetric: 17.2310

Epoch 82: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.6103 - MinusLogProbMetric: 16.6103 - val_loss: 17.2310 - val_MinusLogProbMetric: 17.2310 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 83/1000
2023-09-26 14:20:22.374 
Epoch 83/1000 
	 loss: 16.6174, MinusLogProbMetric: 16.6174, val_loss: 17.3953, val_MinusLogProbMetric: 17.3953

Epoch 83: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.6174 - MinusLogProbMetric: 16.6174 - val_loss: 17.3953 - val_MinusLogProbMetric: 17.3953 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 84/1000
2023-09-26 14:21:04.180 
Epoch 84/1000 
	 loss: 16.5965, MinusLogProbMetric: 16.5965, val_loss: 17.4430, val_MinusLogProbMetric: 17.4430

Epoch 84: val_loss did not improve from 17.14722
196/196 - 42s - loss: 16.5965 - MinusLogProbMetric: 16.5965 - val_loss: 17.4430 - val_MinusLogProbMetric: 17.4430 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 85/1000
2023-09-26 14:21:45.638 
Epoch 85/1000 
	 loss: 16.5879, MinusLogProbMetric: 16.5879, val_loss: 17.2031, val_MinusLogProbMetric: 17.2031

Epoch 85: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.5879 - MinusLogProbMetric: 16.5879 - val_loss: 17.2031 - val_MinusLogProbMetric: 17.2031 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 86/1000
2023-09-26 14:22:26.678 
Epoch 86/1000 
	 loss: 16.5746, MinusLogProbMetric: 16.5746, val_loss: 17.5861, val_MinusLogProbMetric: 17.5861

Epoch 86: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.5746 - MinusLogProbMetric: 16.5746 - val_loss: 17.5861 - val_MinusLogProbMetric: 17.5861 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 87/1000
2023-09-26 14:23:07.531 
Epoch 87/1000 
	 loss: 16.5781, MinusLogProbMetric: 16.5781, val_loss: 17.3558, val_MinusLogProbMetric: 17.3558

Epoch 87: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.5781 - MinusLogProbMetric: 16.5781 - val_loss: 17.3558 - val_MinusLogProbMetric: 17.3558 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 88/1000
2023-09-26 14:23:48.484 
Epoch 88/1000 
	 loss: 16.5772, MinusLogProbMetric: 16.5772, val_loss: 17.4079, val_MinusLogProbMetric: 17.4079

Epoch 88: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.5772 - MinusLogProbMetric: 16.5772 - val_loss: 17.4079 - val_MinusLogProbMetric: 17.4079 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 89/1000
2023-09-26 14:24:28.957 
Epoch 89/1000 
	 loss: 16.5557, MinusLogProbMetric: 16.5557, val_loss: 17.3432, val_MinusLogProbMetric: 17.3432

Epoch 89: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.5557 - MinusLogProbMetric: 16.5557 - val_loss: 17.3432 - val_MinusLogProbMetric: 17.3432 - lr: 0.0010 - 40s/epoch - 207ms/step
Epoch 90/1000
2023-09-26 14:25:09.168 
Epoch 90/1000 
	 loss: 16.5378, MinusLogProbMetric: 16.5378, val_loss: 17.2816, val_MinusLogProbMetric: 17.2816

Epoch 90: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.5378 - MinusLogProbMetric: 16.5378 - val_loss: 17.2816 - val_MinusLogProbMetric: 17.2816 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 91/1000
2023-09-26 14:25:49.715 
Epoch 91/1000 
	 loss: 16.5345, MinusLogProbMetric: 16.5345, val_loss: 17.2805, val_MinusLogProbMetric: 17.2805

Epoch 91: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.5345 - MinusLogProbMetric: 16.5345 - val_loss: 17.2805 - val_MinusLogProbMetric: 17.2805 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 92/1000
2023-09-26 14:26:30.215 
Epoch 92/1000 
	 loss: 16.5622, MinusLogProbMetric: 16.5622, val_loss: 17.2905, val_MinusLogProbMetric: 17.2905

Epoch 92: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.5622 - MinusLogProbMetric: 16.5622 - val_loss: 17.2905 - val_MinusLogProbMetric: 17.2905 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 93/1000
2023-09-26 14:27:10.991 
Epoch 93/1000 
	 loss: 16.5454, MinusLogProbMetric: 16.5454, val_loss: 17.3332, val_MinusLogProbMetric: 17.3332

Epoch 93: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.5454 - MinusLogProbMetric: 16.5454 - val_loss: 17.3332 - val_MinusLogProbMetric: 17.3332 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 94/1000
2023-09-26 14:27:51.524 
Epoch 94/1000 
	 loss: 16.5182, MinusLogProbMetric: 16.5182, val_loss: 17.1520, val_MinusLogProbMetric: 17.1520

Epoch 94: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.5182 - MinusLogProbMetric: 16.5182 - val_loss: 17.1520 - val_MinusLogProbMetric: 17.1520 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 95/1000
2023-09-26 14:28:32.403 
Epoch 95/1000 
	 loss: 16.5034, MinusLogProbMetric: 16.5034, val_loss: 17.4490, val_MinusLogProbMetric: 17.4490

Epoch 95: val_loss did not improve from 17.14722
196/196 - 41s - loss: 16.5034 - MinusLogProbMetric: 16.5034 - val_loss: 17.4490 - val_MinusLogProbMetric: 17.4490 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 96/1000
2023-09-26 14:29:12.755 
Epoch 96/1000 
	 loss: 16.4719, MinusLogProbMetric: 16.4719, val_loss: 17.2170, val_MinusLogProbMetric: 17.2170

Epoch 96: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4719 - MinusLogProbMetric: 16.4719 - val_loss: 17.2170 - val_MinusLogProbMetric: 17.2170 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 97/1000
2023-09-26 14:29:52.972 
Epoch 97/1000 
	 loss: 16.5111, MinusLogProbMetric: 16.5111, val_loss: 17.3078, val_MinusLogProbMetric: 17.3078

Epoch 97: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.5111 - MinusLogProbMetric: 16.5111 - val_loss: 17.3078 - val_MinusLogProbMetric: 17.3078 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 98/1000
2023-09-26 14:30:33.011 
Epoch 98/1000 
	 loss: 16.4887, MinusLogProbMetric: 16.4887, val_loss: 17.3856, val_MinusLogProbMetric: 17.3856

Epoch 98: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4887 - MinusLogProbMetric: 16.4887 - val_loss: 17.3856 - val_MinusLogProbMetric: 17.3856 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 99/1000
2023-09-26 14:31:13.218 
Epoch 99/1000 
	 loss: 16.4673, MinusLogProbMetric: 16.4673, val_loss: 17.4122, val_MinusLogProbMetric: 17.4122

Epoch 99: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4673 - MinusLogProbMetric: 16.4673 - val_loss: 17.4122 - val_MinusLogProbMetric: 17.4122 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 100/1000
2023-09-26 14:31:53.405 
Epoch 100/1000 
	 loss: 16.4802, MinusLogProbMetric: 16.4802, val_loss: 17.7366, val_MinusLogProbMetric: 17.7366

Epoch 100: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4802 - MinusLogProbMetric: 16.4802 - val_loss: 17.7366 - val_MinusLogProbMetric: 17.7366 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 101/1000
2023-09-26 14:32:33.453 
Epoch 101/1000 
	 loss: 16.4545, MinusLogProbMetric: 16.4545, val_loss: 17.3824, val_MinusLogProbMetric: 17.3824

Epoch 101: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4545 - MinusLogProbMetric: 16.4545 - val_loss: 17.3824 - val_MinusLogProbMetric: 17.3824 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 102/1000
2023-09-26 14:33:13.210 
Epoch 102/1000 
	 loss: 16.4424, MinusLogProbMetric: 16.4424, val_loss: 17.3661, val_MinusLogProbMetric: 17.3661

Epoch 102: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4424 - MinusLogProbMetric: 16.4424 - val_loss: 17.3661 - val_MinusLogProbMetric: 17.3661 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 103/1000
2023-09-26 14:33:52.739 
Epoch 103/1000 
	 loss: 16.4500, MinusLogProbMetric: 16.4500, val_loss: 17.5597, val_MinusLogProbMetric: 17.5597

Epoch 103: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4500 - MinusLogProbMetric: 16.4500 - val_loss: 17.5597 - val_MinusLogProbMetric: 17.5597 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 104/1000
2023-09-26 14:34:32.619 
Epoch 104/1000 
	 loss: 16.4441, MinusLogProbMetric: 16.4441, val_loss: 17.5100, val_MinusLogProbMetric: 17.5100

Epoch 104: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4441 - MinusLogProbMetric: 16.4441 - val_loss: 17.5100 - val_MinusLogProbMetric: 17.5100 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 105/1000
2023-09-26 14:35:12.300 
Epoch 105/1000 
	 loss: 16.4473, MinusLogProbMetric: 16.4473, val_loss: 17.2867, val_MinusLogProbMetric: 17.2867

Epoch 105: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4473 - MinusLogProbMetric: 16.4473 - val_loss: 17.2867 - val_MinusLogProbMetric: 17.2867 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 106/1000
2023-09-26 14:35:52.076 
Epoch 106/1000 
	 loss: 16.4680, MinusLogProbMetric: 16.4680, val_loss: 17.3739, val_MinusLogProbMetric: 17.3739

Epoch 106: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4680 - MinusLogProbMetric: 16.4680 - val_loss: 17.3739 - val_MinusLogProbMetric: 17.3739 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 107/1000
2023-09-26 14:36:32.113 
Epoch 107/1000 
	 loss: 16.4207, MinusLogProbMetric: 16.4207, val_loss: 17.3432, val_MinusLogProbMetric: 17.3432

Epoch 107: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4207 - MinusLogProbMetric: 16.4207 - val_loss: 17.3432 - val_MinusLogProbMetric: 17.3432 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 108/1000
2023-09-26 14:37:11.853 
Epoch 108/1000 
	 loss: 16.3966, MinusLogProbMetric: 16.3966, val_loss: 17.2908, val_MinusLogProbMetric: 17.2908

Epoch 108: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3966 - MinusLogProbMetric: 16.3966 - val_loss: 17.2908 - val_MinusLogProbMetric: 17.2908 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 109/1000
2023-09-26 14:37:51.577 
Epoch 109/1000 
	 loss: 16.4165, MinusLogProbMetric: 16.4165, val_loss: 17.3392, val_MinusLogProbMetric: 17.3392

Epoch 109: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4165 - MinusLogProbMetric: 16.4165 - val_loss: 17.3392 - val_MinusLogProbMetric: 17.3392 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 110/1000
2023-09-26 14:38:31.360 
Epoch 110/1000 
	 loss: 16.4204, MinusLogProbMetric: 16.4204, val_loss: 17.4831, val_MinusLogProbMetric: 17.4831

Epoch 110: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.4204 - MinusLogProbMetric: 16.4204 - val_loss: 17.4831 - val_MinusLogProbMetric: 17.4831 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 111/1000
2023-09-26 14:39:11.173 
Epoch 111/1000 
	 loss: 16.3770, MinusLogProbMetric: 16.3770, val_loss: 17.3611, val_MinusLogProbMetric: 17.3611

Epoch 111: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3770 - MinusLogProbMetric: 16.3770 - val_loss: 17.3611 - val_MinusLogProbMetric: 17.3611 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 112/1000
2023-09-26 14:39:50.961 
Epoch 112/1000 
	 loss: 16.3899, MinusLogProbMetric: 16.3899, val_loss: 17.2986, val_MinusLogProbMetric: 17.2986

Epoch 112: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3899 - MinusLogProbMetric: 16.3899 - val_loss: 17.2986 - val_MinusLogProbMetric: 17.2986 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 113/1000
2023-09-26 14:40:30.695 
Epoch 113/1000 
	 loss: 16.3756, MinusLogProbMetric: 16.3756, val_loss: 17.3572, val_MinusLogProbMetric: 17.3572

Epoch 113: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3756 - MinusLogProbMetric: 16.3756 - val_loss: 17.3572 - val_MinusLogProbMetric: 17.3572 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 114/1000
2023-09-26 14:41:10.543 
Epoch 114/1000 
	 loss: 16.3944, MinusLogProbMetric: 16.3944, val_loss: 17.3232, val_MinusLogProbMetric: 17.3232

Epoch 114: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3944 - MinusLogProbMetric: 16.3944 - val_loss: 17.3232 - val_MinusLogProbMetric: 17.3232 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 115/1000
2023-09-26 14:41:49.949 
Epoch 115/1000 
	 loss: 16.3581, MinusLogProbMetric: 16.3581, val_loss: 17.5297, val_MinusLogProbMetric: 17.5297

Epoch 115: val_loss did not improve from 17.14722
196/196 - 39s - loss: 16.3581 - MinusLogProbMetric: 16.3581 - val_loss: 17.5297 - val_MinusLogProbMetric: 17.5297 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 116/1000
2023-09-26 14:42:29.759 
Epoch 116/1000 
	 loss: 16.3923, MinusLogProbMetric: 16.3923, val_loss: 17.4984, val_MinusLogProbMetric: 17.4984

Epoch 116: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3923 - MinusLogProbMetric: 16.3923 - val_loss: 17.4984 - val_MinusLogProbMetric: 17.4984 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 117/1000
2023-09-26 14:43:09.431 
Epoch 117/1000 
	 loss: 16.3763, MinusLogProbMetric: 16.3763, val_loss: 17.3050, val_MinusLogProbMetric: 17.3050

Epoch 117: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3763 - MinusLogProbMetric: 16.3763 - val_loss: 17.3050 - val_MinusLogProbMetric: 17.3050 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 118/1000
2023-09-26 14:43:49.135 
Epoch 118/1000 
	 loss: 16.3515, MinusLogProbMetric: 16.3515, val_loss: 17.6273, val_MinusLogProbMetric: 17.6273

Epoch 118: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3515 - MinusLogProbMetric: 16.3515 - val_loss: 17.6273 - val_MinusLogProbMetric: 17.6273 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 119/1000
2023-09-26 14:44:28.940 
Epoch 119/1000 
	 loss: 16.3340, MinusLogProbMetric: 16.3340, val_loss: 17.3482, val_MinusLogProbMetric: 17.3482

Epoch 119: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3340 - MinusLogProbMetric: 16.3340 - val_loss: 17.3482 - val_MinusLogProbMetric: 17.3482 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 120/1000
2023-09-26 14:45:08.779 
Epoch 120/1000 
	 loss: 16.3233, MinusLogProbMetric: 16.3233, val_loss: 17.6162, val_MinusLogProbMetric: 17.6162

Epoch 120: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3233 - MinusLogProbMetric: 16.3233 - val_loss: 17.6162 - val_MinusLogProbMetric: 17.6162 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 121/1000
2023-09-26 14:45:48.550 
Epoch 121/1000 
	 loss: 16.3300, MinusLogProbMetric: 16.3300, val_loss: 17.2377, val_MinusLogProbMetric: 17.2377

Epoch 121: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3300 - MinusLogProbMetric: 16.3300 - val_loss: 17.2377 - val_MinusLogProbMetric: 17.2377 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 122/1000
2023-09-26 14:46:28.143 
Epoch 122/1000 
	 loss: 16.3216, MinusLogProbMetric: 16.3216, val_loss: 17.3749, val_MinusLogProbMetric: 17.3749

Epoch 122: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3216 - MinusLogProbMetric: 16.3216 - val_loss: 17.3749 - val_MinusLogProbMetric: 17.3749 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 123/1000
2023-09-26 14:47:08.127 
Epoch 123/1000 
	 loss: 16.3357, MinusLogProbMetric: 16.3357, val_loss: 17.3652, val_MinusLogProbMetric: 17.3652

Epoch 123: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3357 - MinusLogProbMetric: 16.3357 - val_loss: 17.3652 - val_MinusLogProbMetric: 17.3652 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 124/1000
2023-09-26 14:47:48.023 
Epoch 124/1000 
	 loss: 16.3072, MinusLogProbMetric: 16.3072, val_loss: 17.3249, val_MinusLogProbMetric: 17.3249

Epoch 124: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3072 - MinusLogProbMetric: 16.3072 - val_loss: 17.3249 - val_MinusLogProbMetric: 17.3249 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 125/1000
2023-09-26 14:48:27.938 
Epoch 125/1000 
	 loss: 16.2831, MinusLogProbMetric: 16.2831, val_loss: 17.3377, val_MinusLogProbMetric: 17.3377

Epoch 125: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.2831 - MinusLogProbMetric: 16.2831 - val_loss: 17.3377 - val_MinusLogProbMetric: 17.3377 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 126/1000
2023-09-26 14:49:08.093 
Epoch 126/1000 
	 loss: 16.3036, MinusLogProbMetric: 16.3036, val_loss: 17.4063, val_MinusLogProbMetric: 17.4063

Epoch 126: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.3036 - MinusLogProbMetric: 16.3036 - val_loss: 17.4063 - val_MinusLogProbMetric: 17.4063 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 127/1000
2023-09-26 14:49:47.426 
Epoch 127/1000 
	 loss: 16.2766, MinusLogProbMetric: 16.2766, val_loss: 17.4049, val_MinusLogProbMetric: 17.4049

Epoch 127: val_loss did not improve from 17.14722
196/196 - 39s - loss: 16.2766 - MinusLogProbMetric: 16.2766 - val_loss: 17.4049 - val_MinusLogProbMetric: 17.4049 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 128/1000
2023-09-26 14:50:26.986 
Epoch 128/1000 
	 loss: 16.2783, MinusLogProbMetric: 16.2783, val_loss: 17.3988, val_MinusLogProbMetric: 17.3988

Epoch 128: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.2783 - MinusLogProbMetric: 16.2783 - val_loss: 17.3988 - val_MinusLogProbMetric: 17.3988 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 129/1000
2023-09-26 14:51:06.904 
Epoch 129/1000 
	 loss: 16.2640, MinusLogProbMetric: 16.2640, val_loss: 17.3445, val_MinusLogProbMetric: 17.3445

Epoch 129: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.2640 - MinusLogProbMetric: 16.2640 - val_loss: 17.3445 - val_MinusLogProbMetric: 17.3445 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 130/1000
2023-09-26 14:51:46.566 
Epoch 130/1000 
	 loss: 16.2591, MinusLogProbMetric: 16.2591, val_loss: 17.3482, val_MinusLogProbMetric: 17.3482

Epoch 130: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.2591 - MinusLogProbMetric: 16.2591 - val_loss: 17.3482 - val_MinusLogProbMetric: 17.3482 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 131/1000
2023-09-26 14:52:26.518 
Epoch 131/1000 
	 loss: 16.0492, MinusLogProbMetric: 16.0492, val_loss: 17.2594, val_MinusLogProbMetric: 17.2594

Epoch 131: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.0492 - MinusLogProbMetric: 16.0492 - val_loss: 17.2594 - val_MinusLogProbMetric: 17.2594 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 132/1000
2023-09-26 14:53:05.950 
Epoch 132/1000 
	 loss: 16.0443, MinusLogProbMetric: 16.0443, val_loss: 17.3089, val_MinusLogProbMetric: 17.3089

Epoch 132: val_loss did not improve from 17.14722
196/196 - 39s - loss: 16.0443 - MinusLogProbMetric: 16.0443 - val_loss: 17.3089 - val_MinusLogProbMetric: 17.3089 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 133/1000
2023-09-26 14:53:45.667 
Epoch 133/1000 
	 loss: 16.0505, MinusLogProbMetric: 16.0505, val_loss: 17.3687, val_MinusLogProbMetric: 17.3687

Epoch 133: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.0505 - MinusLogProbMetric: 16.0505 - val_loss: 17.3687 - val_MinusLogProbMetric: 17.3687 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 134/1000
2023-09-26 14:54:25.422 
Epoch 134/1000 
	 loss: 16.0333, MinusLogProbMetric: 16.0333, val_loss: 17.3514, val_MinusLogProbMetric: 17.3514

Epoch 134: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.0333 - MinusLogProbMetric: 16.0333 - val_loss: 17.3514 - val_MinusLogProbMetric: 17.3514 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 135/1000
2023-09-26 14:55:04.888 
Epoch 135/1000 
	 loss: 16.0400, MinusLogProbMetric: 16.0400, val_loss: 17.3298, val_MinusLogProbMetric: 17.3298

Epoch 135: val_loss did not improve from 17.14722
196/196 - 39s - loss: 16.0400 - MinusLogProbMetric: 16.0400 - val_loss: 17.3298 - val_MinusLogProbMetric: 17.3298 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 136/1000
2023-09-26 14:55:44.804 
Epoch 136/1000 
	 loss: 16.0328, MinusLogProbMetric: 16.0328, val_loss: 17.3579, val_MinusLogProbMetric: 17.3579

Epoch 136: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.0328 - MinusLogProbMetric: 16.0328 - val_loss: 17.3579 - val_MinusLogProbMetric: 17.3579 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 137/1000
2023-09-26 14:56:24.911 
Epoch 137/1000 
	 loss: 16.0229, MinusLogProbMetric: 16.0229, val_loss: 17.3772, val_MinusLogProbMetric: 17.3772

Epoch 137: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.0229 - MinusLogProbMetric: 16.0229 - val_loss: 17.3772 - val_MinusLogProbMetric: 17.3772 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 138/1000
2023-09-26 14:57:05.056 
Epoch 138/1000 
	 loss: 16.0215, MinusLogProbMetric: 16.0215, val_loss: 17.3674, val_MinusLogProbMetric: 17.3674

Epoch 138: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.0215 - MinusLogProbMetric: 16.0215 - val_loss: 17.3674 - val_MinusLogProbMetric: 17.3674 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 139/1000
2023-09-26 14:57:44.265 
Epoch 139/1000 
	 loss: 16.0235, MinusLogProbMetric: 16.0235, val_loss: 17.3413, val_MinusLogProbMetric: 17.3413

Epoch 139: val_loss did not improve from 17.14722
196/196 - 39s - loss: 16.0235 - MinusLogProbMetric: 16.0235 - val_loss: 17.3413 - val_MinusLogProbMetric: 17.3413 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 140/1000
2023-09-26 14:58:23.325 
Epoch 140/1000 
	 loss: 16.0037, MinusLogProbMetric: 16.0037, val_loss: 17.3997, val_MinusLogProbMetric: 17.3997

Epoch 140: val_loss did not improve from 17.14722
196/196 - 39s - loss: 16.0037 - MinusLogProbMetric: 16.0037 - val_loss: 17.3997 - val_MinusLogProbMetric: 17.3997 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 141/1000
2023-09-26 14:59:03.029 
Epoch 141/1000 
	 loss: 16.0159, MinusLogProbMetric: 16.0159, val_loss: 17.3617, val_MinusLogProbMetric: 17.3617

Epoch 141: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.0159 - MinusLogProbMetric: 16.0159 - val_loss: 17.3617 - val_MinusLogProbMetric: 17.3617 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 142/1000
2023-09-26 14:59:42.718 
Epoch 142/1000 
	 loss: 16.0033, MinusLogProbMetric: 16.0033, val_loss: 17.3860, val_MinusLogProbMetric: 17.3860

Epoch 142: val_loss did not improve from 17.14722
196/196 - 40s - loss: 16.0033 - MinusLogProbMetric: 16.0033 - val_loss: 17.3860 - val_MinusLogProbMetric: 17.3860 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 143/1000
2023-09-26 15:00:22.526 
Epoch 143/1000 
	 loss: 15.9941, MinusLogProbMetric: 15.9941, val_loss: 17.3795, val_MinusLogProbMetric: 17.3795

Epoch 143: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9941 - MinusLogProbMetric: 15.9941 - val_loss: 17.3795 - val_MinusLogProbMetric: 17.3795 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 144/1000
2023-09-26 15:01:01.922 
Epoch 144/1000 
	 loss: 15.9984, MinusLogProbMetric: 15.9984, val_loss: 17.3528, val_MinusLogProbMetric: 17.3528

Epoch 144: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.9984 - MinusLogProbMetric: 15.9984 - val_loss: 17.3528 - val_MinusLogProbMetric: 17.3528 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 145/1000
2023-09-26 15:01:41.740 
Epoch 145/1000 
	 loss: 15.9867, MinusLogProbMetric: 15.9867, val_loss: 17.3990, val_MinusLogProbMetric: 17.3990

Epoch 145: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9867 - MinusLogProbMetric: 15.9867 - val_loss: 17.3990 - val_MinusLogProbMetric: 17.3990 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 146/1000
2023-09-26 15:02:21.041 
Epoch 146/1000 
	 loss: 15.9945, MinusLogProbMetric: 15.9945, val_loss: 17.4293, val_MinusLogProbMetric: 17.4293

Epoch 146: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.9945 - MinusLogProbMetric: 15.9945 - val_loss: 17.4293 - val_MinusLogProbMetric: 17.4293 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 147/1000
2023-09-26 15:03:00.602 
Epoch 147/1000 
	 loss: 15.9923, MinusLogProbMetric: 15.9923, val_loss: 17.4690, val_MinusLogProbMetric: 17.4690

Epoch 147: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9923 - MinusLogProbMetric: 15.9923 - val_loss: 17.4690 - val_MinusLogProbMetric: 17.4690 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 148/1000
2023-09-26 15:03:40.296 
Epoch 148/1000 
	 loss: 15.9796, MinusLogProbMetric: 15.9796, val_loss: 17.4394, val_MinusLogProbMetric: 17.4394

Epoch 148: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9796 - MinusLogProbMetric: 15.9796 - val_loss: 17.4394 - val_MinusLogProbMetric: 17.4394 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 149/1000
2023-09-26 15:04:20.006 
Epoch 149/1000 
	 loss: 15.9873, MinusLogProbMetric: 15.9873, val_loss: 17.4275, val_MinusLogProbMetric: 17.4275

Epoch 149: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9873 - MinusLogProbMetric: 15.9873 - val_loss: 17.4275 - val_MinusLogProbMetric: 17.4275 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 150/1000
2023-09-26 15:04:59.366 
Epoch 150/1000 
	 loss: 15.9695, MinusLogProbMetric: 15.9695, val_loss: 17.5832, val_MinusLogProbMetric: 17.5832

Epoch 150: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.9695 - MinusLogProbMetric: 15.9695 - val_loss: 17.5832 - val_MinusLogProbMetric: 17.5832 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 151/1000
2023-09-26 15:05:39.360 
Epoch 151/1000 
	 loss: 15.9817, MinusLogProbMetric: 15.9817, val_loss: 17.4016, val_MinusLogProbMetric: 17.4016

Epoch 151: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9817 - MinusLogProbMetric: 15.9817 - val_loss: 17.4016 - val_MinusLogProbMetric: 17.4016 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 152/1000
2023-09-26 15:06:19.394 
Epoch 152/1000 
	 loss: 15.9809, MinusLogProbMetric: 15.9809, val_loss: 17.4144, val_MinusLogProbMetric: 17.4144

Epoch 152: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9809 - MinusLogProbMetric: 15.9809 - val_loss: 17.4144 - val_MinusLogProbMetric: 17.4144 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 153/1000
2023-09-26 15:06:58.974 
Epoch 153/1000 
	 loss: 15.9663, MinusLogProbMetric: 15.9663, val_loss: 17.5523, val_MinusLogProbMetric: 17.5523

Epoch 153: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9663 - MinusLogProbMetric: 15.9663 - val_loss: 17.5523 - val_MinusLogProbMetric: 17.5523 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 154/1000
2023-09-26 15:07:38.437 
Epoch 154/1000 
	 loss: 15.9656, MinusLogProbMetric: 15.9656, val_loss: 17.4183, val_MinusLogProbMetric: 17.4183

Epoch 154: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.9656 - MinusLogProbMetric: 15.9656 - val_loss: 17.4183 - val_MinusLogProbMetric: 17.4183 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 155/1000
2023-09-26 15:08:17.818 
Epoch 155/1000 
	 loss: 15.9545, MinusLogProbMetric: 15.9545, val_loss: 17.4186, val_MinusLogProbMetric: 17.4186

Epoch 155: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.9545 - MinusLogProbMetric: 15.9545 - val_loss: 17.4186 - val_MinusLogProbMetric: 17.4186 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 156/1000
2023-09-26 15:08:57.469 
Epoch 156/1000 
	 loss: 15.9455, MinusLogProbMetric: 15.9455, val_loss: 17.4261, val_MinusLogProbMetric: 17.4261

Epoch 156: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9455 - MinusLogProbMetric: 15.9455 - val_loss: 17.4261 - val_MinusLogProbMetric: 17.4261 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 157/1000
2023-09-26 15:09:37.280 
Epoch 157/1000 
	 loss: 15.9354, MinusLogProbMetric: 15.9354, val_loss: 17.4743, val_MinusLogProbMetric: 17.4743

Epoch 157: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9354 - MinusLogProbMetric: 15.9354 - val_loss: 17.4743 - val_MinusLogProbMetric: 17.4743 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 158/1000
2023-09-26 15:10:16.728 
Epoch 158/1000 
	 loss: 15.9379, MinusLogProbMetric: 15.9379, val_loss: 17.4501, val_MinusLogProbMetric: 17.4501

Epoch 158: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.9379 - MinusLogProbMetric: 15.9379 - val_loss: 17.4501 - val_MinusLogProbMetric: 17.4501 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 159/1000
2023-09-26 15:10:56.002 
Epoch 159/1000 
	 loss: 15.9354, MinusLogProbMetric: 15.9354, val_loss: 17.4872, val_MinusLogProbMetric: 17.4872

Epoch 159: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.9354 - MinusLogProbMetric: 15.9354 - val_loss: 17.4872 - val_MinusLogProbMetric: 17.4872 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 160/1000
2023-09-26 15:11:35.587 
Epoch 160/1000 
	 loss: 15.9379, MinusLogProbMetric: 15.9379, val_loss: 17.5711, val_MinusLogProbMetric: 17.5711

Epoch 160: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9379 - MinusLogProbMetric: 15.9379 - val_loss: 17.5711 - val_MinusLogProbMetric: 17.5711 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 161/1000
2023-09-26 15:12:15.133 
Epoch 161/1000 
	 loss: 15.9403, MinusLogProbMetric: 15.9403, val_loss: 17.4236, val_MinusLogProbMetric: 17.4236

Epoch 161: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9403 - MinusLogProbMetric: 15.9403 - val_loss: 17.4236 - val_MinusLogProbMetric: 17.4236 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 162/1000
2023-09-26 15:12:54.920 
Epoch 162/1000 
	 loss: 15.9169, MinusLogProbMetric: 15.9169, val_loss: 17.4517, val_MinusLogProbMetric: 17.4517

Epoch 162: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9169 - MinusLogProbMetric: 15.9169 - val_loss: 17.4517 - val_MinusLogProbMetric: 17.4517 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 163/1000
2023-09-26 15:13:34.730 
Epoch 163/1000 
	 loss: 15.9214, MinusLogProbMetric: 15.9214, val_loss: 17.4678, val_MinusLogProbMetric: 17.4678

Epoch 163: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9214 - MinusLogProbMetric: 15.9214 - val_loss: 17.4678 - val_MinusLogProbMetric: 17.4678 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 164/1000
2023-09-26 15:14:14.499 
Epoch 164/1000 
	 loss: 15.9376, MinusLogProbMetric: 15.9376, val_loss: 17.4811, val_MinusLogProbMetric: 17.4811

Epoch 164: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9376 - MinusLogProbMetric: 15.9376 - val_loss: 17.4811 - val_MinusLogProbMetric: 17.4811 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 165/1000
2023-09-26 15:14:54.514 
Epoch 165/1000 
	 loss: 15.9233, MinusLogProbMetric: 15.9233, val_loss: 17.4442, val_MinusLogProbMetric: 17.4442

Epoch 165: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9233 - MinusLogProbMetric: 15.9233 - val_loss: 17.4442 - val_MinusLogProbMetric: 17.4442 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 166/1000
2023-09-26 15:15:34.243 
Epoch 166/1000 
	 loss: 15.9115, MinusLogProbMetric: 15.9115, val_loss: 17.4995, val_MinusLogProbMetric: 17.4995

Epoch 166: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9115 - MinusLogProbMetric: 15.9115 - val_loss: 17.4995 - val_MinusLogProbMetric: 17.4995 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 167/1000
2023-09-26 15:16:14.062 
Epoch 167/1000 
	 loss: 15.9091, MinusLogProbMetric: 15.9091, val_loss: 17.4650, val_MinusLogProbMetric: 17.4650

Epoch 167: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9091 - MinusLogProbMetric: 15.9091 - val_loss: 17.4650 - val_MinusLogProbMetric: 17.4650 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 168/1000
2023-09-26 15:16:53.500 
Epoch 168/1000 
	 loss: 15.9031, MinusLogProbMetric: 15.9031, val_loss: 17.5490, val_MinusLogProbMetric: 17.5490

Epoch 168: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.9031 - MinusLogProbMetric: 15.9031 - val_loss: 17.5490 - val_MinusLogProbMetric: 17.5490 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 169/1000
2023-09-26 15:17:33.395 
Epoch 169/1000 
	 loss: 15.9146, MinusLogProbMetric: 15.9146, val_loss: 17.5245, val_MinusLogProbMetric: 17.5245

Epoch 169: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9146 - MinusLogProbMetric: 15.9146 - val_loss: 17.5245 - val_MinusLogProbMetric: 17.5245 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 170/1000
2023-09-26 15:18:13.019 
Epoch 170/1000 
	 loss: 15.8999, MinusLogProbMetric: 15.8999, val_loss: 17.5011, val_MinusLogProbMetric: 17.5011

Epoch 170: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.8999 - MinusLogProbMetric: 15.8999 - val_loss: 17.5011 - val_MinusLogProbMetric: 17.5011 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 171/1000
2023-09-26 15:18:52.690 
Epoch 171/1000 
	 loss: 15.9081, MinusLogProbMetric: 15.9081, val_loss: 17.5430, val_MinusLogProbMetric: 17.5430

Epoch 171: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.9081 - MinusLogProbMetric: 15.9081 - val_loss: 17.5430 - val_MinusLogProbMetric: 17.5430 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 172/1000
2023-09-26 15:19:32.487 
Epoch 172/1000 
	 loss: 15.8941, MinusLogProbMetric: 15.8941, val_loss: 17.5069, val_MinusLogProbMetric: 17.5069

Epoch 172: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.8941 - MinusLogProbMetric: 15.8941 - val_loss: 17.5069 - val_MinusLogProbMetric: 17.5069 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 173/1000
2023-09-26 15:20:13.225 
Epoch 173/1000 
	 loss: 15.9024, MinusLogProbMetric: 15.9024, val_loss: 17.4843, val_MinusLogProbMetric: 17.4843

Epoch 173: val_loss did not improve from 17.14722
196/196 - 41s - loss: 15.9024 - MinusLogProbMetric: 15.9024 - val_loss: 17.4843 - val_MinusLogProbMetric: 17.4843 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 174/1000
2023-09-26 15:20:54.836 
Epoch 174/1000 
	 loss: 15.8818, MinusLogProbMetric: 15.8818, val_loss: 17.4952, val_MinusLogProbMetric: 17.4952

Epoch 174: val_loss did not improve from 17.14722
196/196 - 42s - loss: 15.8818 - MinusLogProbMetric: 15.8818 - val_loss: 17.4952 - val_MinusLogProbMetric: 17.4952 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 175/1000
2023-09-26 15:21:35.621 
Epoch 175/1000 
	 loss: 15.8881, MinusLogProbMetric: 15.8881, val_loss: 17.4922, val_MinusLogProbMetric: 17.4922

Epoch 175: val_loss did not improve from 17.14722
196/196 - 41s - loss: 15.8881 - MinusLogProbMetric: 15.8881 - val_loss: 17.4922 - val_MinusLogProbMetric: 17.4922 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 176/1000
2023-09-26 15:22:15.395 
Epoch 176/1000 
	 loss: 15.8766, MinusLogProbMetric: 15.8766, val_loss: 17.6040, val_MinusLogProbMetric: 17.6040

Epoch 176: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.8766 - MinusLogProbMetric: 15.8766 - val_loss: 17.6040 - val_MinusLogProbMetric: 17.6040 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 177/1000
2023-09-26 15:22:54.678 
Epoch 177/1000 
	 loss: 15.8744, MinusLogProbMetric: 15.8744, val_loss: 17.5797, val_MinusLogProbMetric: 17.5797

Epoch 177: val_loss did not improve from 17.14722
196/196 - 39s - loss: 15.8744 - MinusLogProbMetric: 15.8744 - val_loss: 17.5797 - val_MinusLogProbMetric: 17.5797 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 178/1000
2023-09-26 15:23:34.500 
Epoch 178/1000 
	 loss: 15.8734, MinusLogProbMetric: 15.8734, val_loss: 17.4935, val_MinusLogProbMetric: 17.4935

Epoch 178: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.8734 - MinusLogProbMetric: 15.8734 - val_loss: 17.4935 - val_MinusLogProbMetric: 17.4935 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 179/1000
2023-09-26 15:24:14.129 
Epoch 179/1000 
	 loss: 15.8622, MinusLogProbMetric: 15.8622, val_loss: 17.5557, val_MinusLogProbMetric: 17.5557

Epoch 179: val_loss did not improve from 17.14722
196/196 - 40s - loss: 15.8622 - MinusLogProbMetric: 15.8622 - val_loss: 17.5557 - val_MinusLogProbMetric: 17.5557 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 180/1000
2023-09-26 15:24:54.448 
Epoch 180/1000 
	 loss: 15.8677, MinusLogProbMetric: 15.8677, val_loss: 17.6005, val_MinusLogProbMetric: 17.6005

Epoch 180: val_loss did not improve from 17.14722
Restoring model weights from the end of the best epoch: 80.
196/196 - 41s - loss: 15.8677 - MinusLogProbMetric: 15.8677 - val_loss: 17.6005 - val_MinusLogProbMetric: 17.6005 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 180: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 15.98412996501429 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 11.237115369993262 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 7.450159579981118 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 8.404434623022098 seconds.
Training succeeded with seed 721.
Model trained in 7239.61 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 45.01 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 45.46 s.
===========
Run 292/720 done in 7288.83 s.
===========

===========
Generating train data for run 293.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_293/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_293/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_293/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_293
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7fea94633e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0fd7d9fc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0fd7d9fc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0fd57a320>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0fd57a5c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0fd57bd00>, <keras.callbacks.ModelCheckpoint object at 0x7ff0fd579ea0>, <keras.callbacks.EarlyStopping object at 0x7ff0fd579fc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0fd57bca0>, <keras.callbacks.TerminateOnNaN object at 0x7ff0fd579d80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_293/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 293/720 with hyperparameters:
timestamp = 2023-09-26 15:25:50.728245
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 24: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-26 15:28:19.599 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2501.1021, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 2501.1021 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 149s/epoch - 759ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 293.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_293/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_293/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_293/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_293
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7ff0eabebeb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0eb165570>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0eb165570>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0ea449ed0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0ea474f70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0ea4754e0>, <keras.callbacks.ModelCheckpoint object at 0x7ff0ea4755a0>, <keras.callbacks.EarlyStopping object at 0x7ff0ea475810>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0ea475840>, <keras.callbacks.TerminateOnNaN object at 0x7ff0ea475480>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_293/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 293/720 with hyperparameters:
timestamp = 2023-09-26 15:28:30.295047
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-26 15:32:16.286 
Epoch 1/1000 
	 loss: 889.7615, MinusLogProbMetric: 889.7615, val_loss: 593.5534, val_MinusLogProbMetric: 593.5534

Epoch 1: val_loss improved from inf to 593.55341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 226s - loss: 889.7615 - MinusLogProbMetric: 889.7615 - val_loss: 593.5534 - val_MinusLogProbMetric: 593.5534 - lr: 3.3333e-04 - 226s/epoch - 1s/step
Epoch 2/1000
2023-09-26 15:33:23.588 
Epoch 2/1000 
	 loss: 558.5445, MinusLogProbMetric: 558.5445, val_loss: 424.8462, val_MinusLogProbMetric: 424.8462

Epoch 2: val_loss improved from 593.55341 to 424.84622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 558.5445 - MinusLogProbMetric: 558.5445 - val_loss: 424.8462 - val_MinusLogProbMetric: 424.8462 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 3/1000
2023-09-26 15:34:33.388 
Epoch 3/1000 
	 loss: 341.4344, MinusLogProbMetric: 341.4344, val_loss: 296.8604, val_MinusLogProbMetric: 296.8604

Epoch 3: val_loss improved from 424.84622 to 296.86035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 341.4344 - MinusLogProbMetric: 341.4344 - val_loss: 296.8604 - val_MinusLogProbMetric: 296.8604 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 4/1000
2023-09-26 15:35:44.191 
Epoch 4/1000 
	 loss: 255.3874, MinusLogProbMetric: 255.3874, val_loss: 140.3180, val_MinusLogProbMetric: 140.3180

Epoch 4: val_loss improved from 296.86035 to 140.31798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 255.3874 - MinusLogProbMetric: 255.3874 - val_loss: 140.3180 - val_MinusLogProbMetric: 140.3180 - lr: 3.3333e-04 - 71s/epoch - 362ms/step
Epoch 5/1000
2023-09-26 15:36:55.453 
Epoch 5/1000 
	 loss: 124.7254, MinusLogProbMetric: 124.7254, val_loss: 118.7946, val_MinusLogProbMetric: 118.7946

Epoch 5: val_loss improved from 140.31798 to 118.79459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 72s - loss: 124.7254 - MinusLogProbMetric: 124.7254 - val_loss: 118.7946 - val_MinusLogProbMetric: 118.7946 - lr: 3.3333e-04 - 72s/epoch - 365ms/step
Epoch 6/1000
2023-09-26 15:38:06.996 
Epoch 6/1000 
	 loss: 103.5428, MinusLogProbMetric: 103.5428, val_loss: 94.2294, val_MinusLogProbMetric: 94.2294

Epoch 6: val_loss improved from 118.79459 to 94.22940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 103.5428 - MinusLogProbMetric: 103.5428 - val_loss: 94.2294 - val_MinusLogProbMetric: 94.2294 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 7/1000
2023-09-26 15:39:18.226 
Epoch 7/1000 
	 loss: 95.9474, MinusLogProbMetric: 95.9474, val_loss: 93.2299, val_MinusLogProbMetric: 93.2299

Epoch 7: val_loss improved from 94.22940 to 93.22995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 95.9474 - MinusLogProbMetric: 95.9474 - val_loss: 93.2299 - val_MinusLogProbMetric: 93.2299 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 8/1000
2023-09-26 15:40:29.698 
Epoch 8/1000 
	 loss: 92.7604, MinusLogProbMetric: 92.7604, val_loss: 84.9935, val_MinusLogProbMetric: 84.9935

Epoch 8: val_loss improved from 93.22995 to 84.99350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 92.7604 - MinusLogProbMetric: 92.7604 - val_loss: 84.9935 - val_MinusLogProbMetric: 84.9935 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 9/1000
2023-09-26 15:41:41.512 
Epoch 9/1000 
	 loss: 82.8060, MinusLogProbMetric: 82.8060, val_loss: 74.6764, val_MinusLogProbMetric: 74.6764

Epoch 9: val_loss improved from 84.99350 to 74.67637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 72s - loss: 82.8060 - MinusLogProbMetric: 82.8060 - val_loss: 74.6764 - val_MinusLogProbMetric: 74.6764 - lr: 3.3333e-04 - 72s/epoch - 366ms/step
Epoch 10/1000
2023-09-26 15:42:51.255 
Epoch 10/1000 
	 loss: 100.4464, MinusLogProbMetric: 100.4464, val_loss: 82.9998, val_MinusLogProbMetric: 82.9998

Epoch 10: val_loss did not improve from 74.67637
196/196 - 69s - loss: 100.4464 - MinusLogProbMetric: 100.4464 - val_loss: 82.9998 - val_MinusLogProbMetric: 82.9998 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 11/1000
2023-09-26 15:43:59.233 
Epoch 11/1000 
	 loss: 82.9571, MinusLogProbMetric: 82.9571, val_loss: 73.4438, val_MinusLogProbMetric: 73.4438

Epoch 11: val_loss improved from 74.67637 to 73.44380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 69s - loss: 82.9571 - MinusLogProbMetric: 82.9571 - val_loss: 73.4438 - val_MinusLogProbMetric: 73.4438 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 12/1000
2023-09-26 15:45:10.201 
Epoch 12/1000 
	 loss: 68.9827, MinusLogProbMetric: 68.9827, val_loss: 67.8318, val_MinusLogProbMetric: 67.8318

Epoch 12: val_loss improved from 73.44380 to 67.83184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 68.9827 - MinusLogProbMetric: 68.9827 - val_loss: 67.8318 - val_MinusLogProbMetric: 67.8318 - lr: 3.3333e-04 - 71s/epoch - 362ms/step
Epoch 13/1000
2023-09-26 15:46:19.899 
Epoch 13/1000 
	 loss: 67.2637, MinusLogProbMetric: 67.2637, val_loss: 63.8171, val_MinusLogProbMetric: 63.8171

Epoch 13: val_loss improved from 67.83184 to 63.81707, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 67.2637 - MinusLogProbMetric: 67.2637 - val_loss: 63.8171 - val_MinusLogProbMetric: 63.8171 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 14/1000
2023-09-26 15:47:28.490 
Epoch 14/1000 
	 loss: 63.8394, MinusLogProbMetric: 63.8394, val_loss: 60.5335, val_MinusLogProbMetric: 60.5335

Epoch 14: val_loss improved from 63.81707 to 60.53348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 69s - loss: 63.8394 - MinusLogProbMetric: 63.8394 - val_loss: 60.5335 - val_MinusLogProbMetric: 60.5335 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 15/1000
2023-09-26 15:48:38.172 
Epoch 15/1000 
	 loss: 58.9859, MinusLogProbMetric: 58.9859, val_loss: 60.0473, val_MinusLogProbMetric: 60.0473

Epoch 15: val_loss improved from 60.53348 to 60.04731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 58.9859 - MinusLogProbMetric: 58.9859 - val_loss: 60.0473 - val_MinusLogProbMetric: 60.0473 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 16/1000
2023-09-26 15:49:48.665 
Epoch 16/1000 
	 loss: 56.5077, MinusLogProbMetric: 56.5077, val_loss: 55.1349, val_MinusLogProbMetric: 55.1349

Epoch 16: val_loss improved from 60.04731 to 55.13487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 56.5077 - MinusLogProbMetric: 56.5077 - val_loss: 55.1349 - val_MinusLogProbMetric: 55.1349 - lr: 3.3333e-04 - 70s/epoch - 360ms/step
Epoch 17/1000
2023-09-26 15:51:00.021 
Epoch 17/1000 
	 loss: 53.8297, MinusLogProbMetric: 53.8297, val_loss: 57.6945, val_MinusLogProbMetric: 57.6945

Epoch 17: val_loss did not improve from 55.13487
196/196 - 70s - loss: 53.8297 - MinusLogProbMetric: 53.8297 - val_loss: 57.6945 - val_MinusLogProbMetric: 57.6945 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 18/1000
2023-09-26 15:52:09.294 
Epoch 18/1000 
	 loss: 52.8337, MinusLogProbMetric: 52.8337, val_loss: 50.5016, val_MinusLogProbMetric: 50.5016

Epoch 18: val_loss improved from 55.13487 to 50.50158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 52.8337 - MinusLogProbMetric: 52.8337 - val_loss: 50.5016 - val_MinusLogProbMetric: 50.5016 - lr: 3.3333e-04 - 71s/epoch - 360ms/step
Epoch 19/1000
2023-09-26 15:53:19.988 
Epoch 19/1000 
	 loss: 50.6336, MinusLogProbMetric: 50.6336, val_loss: 48.4563, val_MinusLogProbMetric: 48.4563

Epoch 19: val_loss improved from 50.50158 to 48.45630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 50.6336 - MinusLogProbMetric: 50.6336 - val_loss: 48.4563 - val_MinusLogProbMetric: 48.4563 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 20/1000
2023-09-26 15:54:30.725 
Epoch 20/1000 
	 loss: 47.8843, MinusLogProbMetric: 47.8843, val_loss: 46.6271, val_MinusLogProbMetric: 46.6271

Epoch 20: val_loss improved from 48.45630 to 46.62708, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 47.8843 - MinusLogProbMetric: 47.8843 - val_loss: 46.6271 - val_MinusLogProbMetric: 46.6271 - lr: 3.3333e-04 - 71s/epoch - 362ms/step
Epoch 21/1000
2023-09-26 15:55:41.109 
Epoch 21/1000 
	 loss: 46.1246, MinusLogProbMetric: 46.1246, val_loss: 45.6533, val_MinusLogProbMetric: 45.6533

Epoch 21: val_loss improved from 46.62708 to 45.65327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 46.1246 - MinusLogProbMetric: 46.1246 - val_loss: 45.6533 - val_MinusLogProbMetric: 45.6533 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 22/1000
2023-09-26 15:56:47.481 
Epoch 22/1000 
	 loss: 44.6107, MinusLogProbMetric: 44.6107, val_loss: 45.7075, val_MinusLogProbMetric: 45.7075

Epoch 22: val_loss did not improve from 45.65327
196/196 - 65s - loss: 44.6107 - MinusLogProbMetric: 44.6107 - val_loss: 45.7075 - val_MinusLogProbMetric: 45.7075 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 23/1000
2023-09-26 15:57:45.209 
Epoch 23/1000 
	 loss: 43.4353, MinusLogProbMetric: 43.4353, val_loss: 43.2401, val_MinusLogProbMetric: 43.2401

Epoch 23: val_loss improved from 45.65327 to 43.24006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 59s - loss: 43.4353 - MinusLogProbMetric: 43.4353 - val_loss: 43.2401 - val_MinusLogProbMetric: 43.2401 - lr: 3.3333e-04 - 59s/epoch - 300ms/step
Epoch 24/1000
2023-09-26 15:58:54.801 
Epoch 24/1000 
	 loss: 42.2268, MinusLogProbMetric: 42.2268, val_loss: 40.6820, val_MinusLogProbMetric: 40.6820

Epoch 24: val_loss improved from 43.24006 to 40.68200, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 42.2268 - MinusLogProbMetric: 42.2268 - val_loss: 40.6820 - val_MinusLogProbMetric: 40.6820 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 25/1000
2023-09-26 16:00:04.542 
Epoch 25/1000 
	 loss: 41.4006, MinusLogProbMetric: 41.4006, val_loss: 40.7224, val_MinusLogProbMetric: 40.7224

Epoch 25: val_loss did not improve from 40.68200
196/196 - 69s - loss: 41.4006 - MinusLogProbMetric: 41.4006 - val_loss: 40.7224 - val_MinusLogProbMetric: 40.7224 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 26/1000
2023-09-26 16:01:13.601 
Epoch 26/1000 
	 loss: 41.2169, MinusLogProbMetric: 41.2169, val_loss: 42.0971, val_MinusLogProbMetric: 42.0971

Epoch 26: val_loss did not improve from 40.68200
196/196 - 69s - loss: 41.2169 - MinusLogProbMetric: 41.2169 - val_loss: 42.0971 - val_MinusLogProbMetric: 42.0971 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 27/1000
2023-09-26 16:02:22.811 
Epoch 27/1000 
	 loss: 40.8551, MinusLogProbMetric: 40.8551, val_loss: 39.4892, val_MinusLogProbMetric: 39.4892

Epoch 27: val_loss improved from 40.68200 to 39.48923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 40.8551 - MinusLogProbMetric: 40.8551 - val_loss: 39.4892 - val_MinusLogProbMetric: 39.4892 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 28/1000
2023-09-26 16:03:32.898 
Epoch 28/1000 
	 loss: 38.5569, MinusLogProbMetric: 38.5569, val_loss: 38.0409, val_MinusLogProbMetric: 38.0409

Epoch 28: val_loss improved from 39.48923 to 38.04091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 38.5569 - MinusLogProbMetric: 38.5569 - val_loss: 38.0409 - val_MinusLogProbMetric: 38.0409 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 29/1000
2023-09-26 16:04:43.501 
Epoch 29/1000 
	 loss: 36.7921, MinusLogProbMetric: 36.7921, val_loss: 35.3776, val_MinusLogProbMetric: 35.3776

Epoch 29: val_loss improved from 38.04091 to 35.37762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 36.7921 - MinusLogProbMetric: 36.7921 - val_loss: 35.3776 - val_MinusLogProbMetric: 35.3776 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 30/1000
2023-09-26 16:05:53.849 
Epoch 30/1000 
	 loss: 35.1916, MinusLogProbMetric: 35.1916, val_loss: 34.4274, val_MinusLogProbMetric: 34.4274

Epoch 30: val_loss improved from 35.37762 to 34.42740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 35.1916 - MinusLogProbMetric: 35.1916 - val_loss: 34.4274 - val_MinusLogProbMetric: 34.4274 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 31/1000
2023-09-26 16:07:04.634 
Epoch 31/1000 
	 loss: 34.2413, MinusLogProbMetric: 34.2413, val_loss: 33.6621, val_MinusLogProbMetric: 33.6621

Epoch 31: val_loss improved from 34.42740 to 33.66209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 34.2413 - MinusLogProbMetric: 34.2413 - val_loss: 33.6621 - val_MinusLogProbMetric: 33.6621 - lr: 3.3333e-04 - 71s/epoch - 361ms/step
Epoch 32/1000
2023-09-26 16:08:15.396 
Epoch 32/1000 
	 loss: 33.6293, MinusLogProbMetric: 33.6293, val_loss: 32.9489, val_MinusLogProbMetric: 32.9489

Epoch 32: val_loss improved from 33.66209 to 32.94891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 33.6293 - MinusLogProbMetric: 33.6293 - val_loss: 32.9489 - val_MinusLogProbMetric: 32.9489 - lr: 3.3333e-04 - 71s/epoch - 362ms/step
Epoch 33/1000
2023-09-26 16:09:25.559 
Epoch 33/1000 
	 loss: 32.4879, MinusLogProbMetric: 32.4879, val_loss: 32.3214, val_MinusLogProbMetric: 32.3214

Epoch 33: val_loss improved from 32.94891 to 32.32141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 32.4879 - MinusLogProbMetric: 32.4879 - val_loss: 32.3214 - val_MinusLogProbMetric: 32.3214 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 34/1000
2023-09-26 16:10:35.558 
Epoch 34/1000 
	 loss: 31.9852, MinusLogProbMetric: 31.9852, val_loss: 31.5489, val_MinusLogProbMetric: 31.5489

Epoch 34: val_loss improved from 32.32141 to 31.54885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 70s - loss: 31.9852 - MinusLogProbMetric: 31.9852 - val_loss: 31.5489 - val_MinusLogProbMetric: 31.5489 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 35/1000
2023-09-26 16:11:46.532 
Epoch 35/1000 
	 loss: 31.6265, MinusLogProbMetric: 31.6265, val_loss: 30.4449, val_MinusLogProbMetric: 30.4449

Epoch 35: val_loss improved from 31.54885 to 30.44493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 71s - loss: 31.6265 - MinusLogProbMetric: 31.6265 - val_loss: 30.4449 - val_MinusLogProbMetric: 30.4449 - lr: 3.3333e-04 - 71s/epoch - 361ms/step
Epoch 36/1000
2023-09-26 16:12:57.163 
Epoch 36/1000 
	 loss: 30.9744, MinusLogProbMetric: 30.9744, val_loss: 30.8513, val_MinusLogProbMetric: 30.8513

Epoch 36: val_loss did not improve from 30.44493
196/196 - 70s - loss: 30.9744 - MinusLogProbMetric: 30.9744 - val_loss: 30.8513 - val_MinusLogProbMetric: 30.8513 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 37/1000
2023-09-26 16:14:04.173 
Epoch 37/1000 
	 loss: 30.4116, MinusLogProbMetric: 30.4116, val_loss: 30.3675, val_MinusLogProbMetric: 30.3675

Epoch 37: val_loss improved from 30.44493 to 30.36750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 68s - loss: 30.4116 - MinusLogProbMetric: 30.4116 - val_loss: 30.3675 - val_MinusLogProbMetric: 30.3675 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 38/1000
2023-09-26 16:15:10.842 
Epoch 38/1000 
	 loss: 29.8254, MinusLogProbMetric: 29.8254, val_loss: 29.8254, val_MinusLogProbMetric: 29.8254

Epoch 38: val_loss improved from 30.36750 to 29.82536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 29.8254 - MinusLogProbMetric: 29.8254 - val_loss: 29.8254 - val_MinusLogProbMetric: 29.8254 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 39/1000
2023-09-26 16:16:18.162 
Epoch 39/1000 
	 loss: 29.6100, MinusLogProbMetric: 29.6100, val_loss: 29.2063, val_MinusLogProbMetric: 29.2063

Epoch 39: val_loss improved from 29.82536 to 29.20630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 29.6100 - MinusLogProbMetric: 29.6100 - val_loss: 29.2063 - val_MinusLogProbMetric: 29.2063 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 40/1000
2023-09-26 16:17:24.683 
Epoch 40/1000 
	 loss: 28.8488, MinusLogProbMetric: 28.8488, val_loss: 29.1765, val_MinusLogProbMetric: 29.1765

Epoch 40: val_loss improved from 29.20630 to 29.17648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 28.8488 - MinusLogProbMetric: 28.8488 - val_loss: 29.1765 - val_MinusLogProbMetric: 29.1765 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 41/1000
2023-09-26 16:18:31.623 
Epoch 41/1000 
	 loss: 28.5585, MinusLogProbMetric: 28.5585, val_loss: 28.5562, val_MinusLogProbMetric: 28.5562

Epoch 41: val_loss improved from 29.17648 to 28.55618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 28.5585 - MinusLogProbMetric: 28.5585 - val_loss: 28.5562 - val_MinusLogProbMetric: 28.5562 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 42/1000
2023-09-26 16:19:38.447 
Epoch 42/1000 
	 loss: 28.0764, MinusLogProbMetric: 28.0764, val_loss: 28.4364, val_MinusLogProbMetric: 28.4364

Epoch 42: val_loss improved from 28.55618 to 28.43642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 28.0764 - MinusLogProbMetric: 28.0764 - val_loss: 28.4364 - val_MinusLogProbMetric: 28.4364 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 43/1000
2023-09-26 16:20:45.108 
Epoch 43/1000 
	 loss: 27.9915, MinusLogProbMetric: 27.9915, val_loss: 27.7856, val_MinusLogProbMetric: 27.7856

Epoch 43: val_loss improved from 28.43642 to 27.78563, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 27.9915 - MinusLogProbMetric: 27.9915 - val_loss: 27.7856 - val_MinusLogProbMetric: 27.7856 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 44/1000
2023-09-26 16:21:50.830 
Epoch 44/1000 
	 loss: 27.5374, MinusLogProbMetric: 27.5374, val_loss: 26.7946, val_MinusLogProbMetric: 26.7946

Epoch 44: val_loss improved from 27.78563 to 26.79456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 27.5374 - MinusLogProbMetric: 27.5374 - val_loss: 26.7946 - val_MinusLogProbMetric: 26.7946 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 45/1000
2023-09-26 16:22:56.932 
Epoch 45/1000 
	 loss: 27.4101, MinusLogProbMetric: 27.4101, val_loss: 27.1529, val_MinusLogProbMetric: 27.1529

Epoch 45: val_loss did not improve from 26.79456
196/196 - 65s - loss: 27.4101 - MinusLogProbMetric: 27.4101 - val_loss: 27.1529 - val_MinusLogProbMetric: 27.1529 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 46/1000
2023-09-26 16:24:03.543 
Epoch 46/1000 
	 loss: 26.9381, MinusLogProbMetric: 26.9381, val_loss: 27.1733, val_MinusLogProbMetric: 27.1733

Epoch 46: val_loss did not improve from 26.79456
196/196 - 67s - loss: 26.9381 - MinusLogProbMetric: 26.9381 - val_loss: 27.1733 - val_MinusLogProbMetric: 27.1733 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 47/1000
2023-09-26 16:25:10.072 
Epoch 47/1000 
	 loss: 26.7601, MinusLogProbMetric: 26.7601, val_loss: 26.2390, val_MinusLogProbMetric: 26.2390

Epoch 47: val_loss improved from 26.79456 to 26.23897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 68s - loss: 26.7601 - MinusLogProbMetric: 26.7601 - val_loss: 26.2390 - val_MinusLogProbMetric: 26.2390 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 48/1000
2023-09-26 16:26:17.209 
Epoch 48/1000 
	 loss: 26.4805, MinusLogProbMetric: 26.4805, val_loss: 26.9364, val_MinusLogProbMetric: 26.9364

Epoch 48: val_loss did not improve from 26.23897
196/196 - 66s - loss: 26.4805 - MinusLogProbMetric: 26.4805 - val_loss: 26.9364 - val_MinusLogProbMetric: 26.9364 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 49/1000
2023-09-26 16:27:23.809 
Epoch 49/1000 
	 loss: 26.3730, MinusLogProbMetric: 26.3730, val_loss: 27.2057, val_MinusLogProbMetric: 27.2057

Epoch 49: val_loss did not improve from 26.23897
196/196 - 67s - loss: 26.3730 - MinusLogProbMetric: 26.3730 - val_loss: 27.2057 - val_MinusLogProbMetric: 27.2057 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 50/1000
2023-09-26 16:28:30.828 
Epoch 50/1000 
	 loss: 26.0230, MinusLogProbMetric: 26.0230, val_loss: 25.8907, val_MinusLogProbMetric: 25.8907

Epoch 50: val_loss improved from 26.23897 to 25.89073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 68s - loss: 26.0230 - MinusLogProbMetric: 26.0230 - val_loss: 25.8907 - val_MinusLogProbMetric: 25.8907 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 51/1000
2023-09-26 16:29:37.276 
Epoch 51/1000 
	 loss: 25.9520, MinusLogProbMetric: 25.9520, val_loss: 26.2656, val_MinusLogProbMetric: 26.2656

Epoch 51: val_loss did not improve from 25.89073
196/196 - 65s - loss: 25.9520 - MinusLogProbMetric: 25.9520 - val_loss: 26.2656 - val_MinusLogProbMetric: 26.2656 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 52/1000
2023-09-26 16:30:41.466 
Epoch 52/1000 
	 loss: 25.6684, MinusLogProbMetric: 25.6684, val_loss: 25.9638, val_MinusLogProbMetric: 25.9638

Epoch 52: val_loss did not improve from 25.89073
196/196 - 64s - loss: 25.6684 - MinusLogProbMetric: 25.6684 - val_loss: 25.9638 - val_MinusLogProbMetric: 25.9638 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 53/1000
2023-09-26 16:31:48.660 
Epoch 53/1000 
	 loss: 25.5894, MinusLogProbMetric: 25.5894, val_loss: 26.1635, val_MinusLogProbMetric: 26.1635

Epoch 53: val_loss did not improve from 25.89073
196/196 - 67s - loss: 25.5894 - MinusLogProbMetric: 25.5894 - val_loss: 26.1635 - val_MinusLogProbMetric: 26.1635 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 54/1000
2023-09-26 16:32:54.981 
Epoch 54/1000 
	 loss: 25.3677, MinusLogProbMetric: 25.3677, val_loss: 25.7207, val_MinusLogProbMetric: 25.7207

Epoch 54: val_loss improved from 25.89073 to 25.72071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 25.3677 - MinusLogProbMetric: 25.3677 - val_loss: 25.7207 - val_MinusLogProbMetric: 25.7207 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 55/1000
2023-09-26 16:34:02.297 
Epoch 55/1000 
	 loss: 25.3322, MinusLogProbMetric: 25.3322, val_loss: 25.2204, val_MinusLogProbMetric: 25.2204

Epoch 55: val_loss improved from 25.72071 to 25.22040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 25.3322 - MinusLogProbMetric: 25.3322 - val_loss: 25.2204 - val_MinusLogProbMetric: 25.2204 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 56/1000
2023-09-26 16:35:07.880 
Epoch 56/1000 
	 loss: 24.8873, MinusLogProbMetric: 24.8873, val_loss: 25.1925, val_MinusLogProbMetric: 25.1925

Epoch 56: val_loss improved from 25.22040 to 25.19250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 24.8873 - MinusLogProbMetric: 24.8873 - val_loss: 25.1925 - val_MinusLogProbMetric: 25.1925 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 57/1000
2023-09-26 16:36:14.644 
Epoch 57/1000 
	 loss: 24.9540, MinusLogProbMetric: 24.9540, val_loss: 26.1033, val_MinusLogProbMetric: 26.1033

Epoch 57: val_loss did not improve from 25.19250
196/196 - 66s - loss: 24.9540 - MinusLogProbMetric: 24.9540 - val_loss: 26.1033 - val_MinusLogProbMetric: 26.1033 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 58/1000
2023-09-26 16:37:18.793 
Epoch 58/1000 
	 loss: 25.0482, MinusLogProbMetric: 25.0482, val_loss: 26.3098, val_MinusLogProbMetric: 26.3098

Epoch 58: val_loss did not improve from 25.19250
196/196 - 64s - loss: 25.0482 - MinusLogProbMetric: 25.0482 - val_loss: 26.3098 - val_MinusLogProbMetric: 26.3098 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 59/1000
2023-09-26 16:38:23.754 
Epoch 59/1000 
	 loss: 24.6879, MinusLogProbMetric: 24.6879, val_loss: 25.1007, val_MinusLogProbMetric: 25.1007

Epoch 59: val_loss improved from 25.19250 to 25.10075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 24.6879 - MinusLogProbMetric: 24.6879 - val_loss: 25.1007 - val_MinusLogProbMetric: 25.1007 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 60/1000
2023-09-26 16:39:30.029 
Epoch 60/1000 
	 loss: 24.5124, MinusLogProbMetric: 24.5124, val_loss: 25.0118, val_MinusLogProbMetric: 25.0118

Epoch 60: val_loss improved from 25.10075 to 25.01181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 24.5124 - MinusLogProbMetric: 24.5124 - val_loss: 25.0118 - val_MinusLogProbMetric: 25.0118 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 61/1000
2023-09-26 16:40:35.430 
Epoch 61/1000 
	 loss: 24.3948, MinusLogProbMetric: 24.3948, val_loss: 24.4912, val_MinusLogProbMetric: 24.4912

Epoch 61: val_loss improved from 25.01181 to 24.49119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 24.3948 - MinusLogProbMetric: 24.3948 - val_loss: 24.4912 - val_MinusLogProbMetric: 24.4912 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 62/1000
2023-09-26 16:41:40.919 
Epoch 62/1000 
	 loss: 24.2916, MinusLogProbMetric: 24.2916, val_loss: 26.1663, val_MinusLogProbMetric: 26.1663

Epoch 62: val_loss did not improve from 24.49119
196/196 - 64s - loss: 24.2916 - MinusLogProbMetric: 24.2916 - val_loss: 26.1663 - val_MinusLogProbMetric: 26.1663 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 63/1000
2023-09-26 16:42:45.368 
Epoch 63/1000 
	 loss: 24.1224, MinusLogProbMetric: 24.1224, val_loss: 23.9855, val_MinusLogProbMetric: 23.9855

Epoch 63: val_loss improved from 24.49119 to 23.98551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 24.1224 - MinusLogProbMetric: 24.1224 - val_loss: 23.9855 - val_MinusLogProbMetric: 23.9855 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 64/1000
2023-09-26 16:43:52.977 
Epoch 64/1000 
	 loss: 23.9821, MinusLogProbMetric: 23.9821, val_loss: 25.0774, val_MinusLogProbMetric: 25.0774

Epoch 64: val_loss did not improve from 23.98551
196/196 - 66s - loss: 23.9821 - MinusLogProbMetric: 23.9821 - val_loss: 25.0774 - val_MinusLogProbMetric: 25.0774 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 65/1000
2023-09-26 16:44:57.985 
Epoch 65/1000 
	 loss: 23.8963, MinusLogProbMetric: 23.8963, val_loss: 23.3545, val_MinusLogProbMetric: 23.3545

Epoch 65: val_loss improved from 23.98551 to 23.35447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 23.8963 - MinusLogProbMetric: 23.8963 - val_loss: 23.3545 - val_MinusLogProbMetric: 23.3545 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 66/1000
2023-09-26 16:46:02.584 
Epoch 66/1000 
	 loss: 23.7637, MinusLogProbMetric: 23.7637, val_loss: 23.5077, val_MinusLogProbMetric: 23.5077

Epoch 66: val_loss did not improve from 23.35447
196/196 - 63s - loss: 23.7637 - MinusLogProbMetric: 23.7637 - val_loss: 23.5077 - val_MinusLogProbMetric: 23.5077 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 67/1000
2023-09-26 16:47:07.208 
Epoch 67/1000 
	 loss: 23.7775, MinusLogProbMetric: 23.7775, val_loss: 23.5692, val_MinusLogProbMetric: 23.5692

Epoch 67: val_loss did not improve from 23.35447
196/196 - 65s - loss: 23.7775 - MinusLogProbMetric: 23.7775 - val_loss: 23.5692 - val_MinusLogProbMetric: 23.5692 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 68/1000
2023-09-26 16:48:11.218 
Epoch 68/1000 
	 loss: 23.7831, MinusLogProbMetric: 23.7831, val_loss: 23.2882, val_MinusLogProbMetric: 23.2882

Epoch 68: val_loss improved from 23.35447 to 23.28818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 23.7831 - MinusLogProbMetric: 23.7831 - val_loss: 23.2882 - val_MinusLogProbMetric: 23.2882 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 69/1000
2023-09-26 16:49:18.661 
Epoch 69/1000 
	 loss: 23.5278, MinusLogProbMetric: 23.5278, val_loss: 23.6322, val_MinusLogProbMetric: 23.6322

Epoch 69: val_loss did not improve from 23.28818
196/196 - 66s - loss: 23.5278 - MinusLogProbMetric: 23.5278 - val_loss: 23.6322 - val_MinusLogProbMetric: 23.6322 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 70/1000
2023-09-26 16:50:23.808 
Epoch 70/1000 
	 loss: 23.3057, MinusLogProbMetric: 23.3057, val_loss: 23.8575, val_MinusLogProbMetric: 23.8575

Epoch 70: val_loss did not improve from 23.28818
196/196 - 65s - loss: 23.3057 - MinusLogProbMetric: 23.3057 - val_loss: 23.8575 - val_MinusLogProbMetric: 23.8575 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 71/1000
2023-09-26 16:51:30.006 
Epoch 71/1000 
	 loss: 23.3949, MinusLogProbMetric: 23.3949, val_loss: 23.4263, val_MinusLogProbMetric: 23.4263

Epoch 71: val_loss did not improve from 23.28818
196/196 - 66s - loss: 23.3949 - MinusLogProbMetric: 23.3949 - val_loss: 23.4263 - val_MinusLogProbMetric: 23.4263 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 72/1000
2023-09-26 16:52:37.280 
Epoch 72/1000 
	 loss: 23.3253, MinusLogProbMetric: 23.3253, val_loss: 23.3724, val_MinusLogProbMetric: 23.3724

Epoch 72: val_loss did not improve from 23.28818
196/196 - 67s - loss: 23.3253 - MinusLogProbMetric: 23.3253 - val_loss: 23.3724 - val_MinusLogProbMetric: 23.3724 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 73/1000
2023-09-26 16:53:42.560 
Epoch 73/1000 
	 loss: 23.1184, MinusLogProbMetric: 23.1184, val_loss: 23.6926, val_MinusLogProbMetric: 23.6926

Epoch 73: val_loss did not improve from 23.28818
196/196 - 65s - loss: 23.1184 - MinusLogProbMetric: 23.1184 - val_loss: 23.6926 - val_MinusLogProbMetric: 23.6926 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 74/1000
2023-09-26 16:54:48.270 
Epoch 74/1000 
	 loss: 23.1030, MinusLogProbMetric: 23.1030, val_loss: 23.2565, val_MinusLogProbMetric: 23.2565

Epoch 74: val_loss improved from 23.28818 to 23.25646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 23.1030 - MinusLogProbMetric: 23.1030 - val_loss: 23.2565 - val_MinusLogProbMetric: 23.2565 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 75/1000
2023-09-26 16:55:54.727 
Epoch 75/1000 
	 loss: 22.9411, MinusLogProbMetric: 22.9411, val_loss: 23.8721, val_MinusLogProbMetric: 23.8721

Epoch 75: val_loss did not improve from 23.25646
196/196 - 65s - loss: 22.9411 - MinusLogProbMetric: 22.9411 - val_loss: 23.8721 - val_MinusLogProbMetric: 23.8721 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 76/1000
2023-09-26 16:57:01.945 
Epoch 76/1000 
	 loss: 22.7988, MinusLogProbMetric: 22.7988, val_loss: 22.8593, val_MinusLogProbMetric: 22.8593

Epoch 76: val_loss improved from 23.25646 to 22.85934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 68s - loss: 22.7988 - MinusLogProbMetric: 22.7988 - val_loss: 22.8593 - val_MinusLogProbMetric: 22.8593 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 77/1000
2023-09-26 16:58:08.265 
Epoch 77/1000 
	 loss: 22.8389, MinusLogProbMetric: 22.8389, val_loss: 22.7584, val_MinusLogProbMetric: 22.7584

Epoch 77: val_loss improved from 22.85934 to 22.75842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 22.8389 - MinusLogProbMetric: 22.8389 - val_loss: 22.7584 - val_MinusLogProbMetric: 22.7584 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 78/1000
2023-09-26 16:59:15.862 
Epoch 78/1000 
	 loss: 22.7451, MinusLogProbMetric: 22.7451, val_loss: 22.8302, val_MinusLogProbMetric: 22.8302

Epoch 78: val_loss did not improve from 22.75842
196/196 - 67s - loss: 22.7451 - MinusLogProbMetric: 22.7451 - val_loss: 22.8302 - val_MinusLogProbMetric: 22.8302 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 79/1000
2023-09-26 17:00:21.459 
Epoch 79/1000 
	 loss: 23.2704, MinusLogProbMetric: 23.2704, val_loss: 23.4242, val_MinusLogProbMetric: 23.4242

Epoch 79: val_loss did not improve from 22.75842
196/196 - 66s - loss: 23.2704 - MinusLogProbMetric: 23.2704 - val_loss: 23.4242 - val_MinusLogProbMetric: 23.4242 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 80/1000
2023-09-26 17:01:25.921 
Epoch 80/1000 
	 loss: 22.6587, MinusLogProbMetric: 22.6587, val_loss: 22.5188, val_MinusLogProbMetric: 22.5188

Epoch 80: val_loss improved from 22.75842 to 22.51878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 22.6587 - MinusLogProbMetric: 22.6587 - val_loss: 22.5188 - val_MinusLogProbMetric: 22.5188 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 81/1000
2023-09-26 17:02:31.319 
Epoch 81/1000 
	 loss: 22.6616, MinusLogProbMetric: 22.6616, val_loss: 23.2249, val_MinusLogProbMetric: 23.2249

Epoch 81: val_loss did not improve from 22.51878
196/196 - 64s - loss: 22.6616 - MinusLogProbMetric: 22.6616 - val_loss: 23.2249 - val_MinusLogProbMetric: 23.2249 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 82/1000
2023-09-26 17:03:37.420 
Epoch 82/1000 
	 loss: 22.3204, MinusLogProbMetric: 22.3204, val_loss: 23.2514, val_MinusLogProbMetric: 23.2514

Epoch 82: val_loss did not improve from 22.51878
196/196 - 66s - loss: 22.3204 - MinusLogProbMetric: 22.3204 - val_loss: 23.2514 - val_MinusLogProbMetric: 23.2514 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 83/1000
2023-09-26 17:04:43.037 
Epoch 83/1000 
	 loss: 22.2661, MinusLogProbMetric: 22.2661, val_loss: 22.1230, val_MinusLogProbMetric: 22.1230

Epoch 83: val_loss improved from 22.51878 to 22.12296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 22.2661 - MinusLogProbMetric: 22.2661 - val_loss: 22.1230 - val_MinusLogProbMetric: 22.1230 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 84/1000
2023-09-26 17:05:49.026 
Epoch 84/1000 
	 loss: 22.2324, MinusLogProbMetric: 22.2324, val_loss: 22.3874, val_MinusLogProbMetric: 22.3874

Epoch 84: val_loss did not improve from 22.12296
196/196 - 65s - loss: 22.2324 - MinusLogProbMetric: 22.2324 - val_loss: 22.3874 - val_MinusLogProbMetric: 22.3874 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 85/1000
2023-09-26 17:06:54.513 
Epoch 85/1000 
	 loss: 22.3857, MinusLogProbMetric: 22.3857, val_loss: 22.2580, val_MinusLogProbMetric: 22.2580

Epoch 85: val_loss did not improve from 22.12296
196/196 - 65s - loss: 22.3857 - MinusLogProbMetric: 22.3857 - val_loss: 22.2580 - val_MinusLogProbMetric: 22.2580 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 86/1000
2023-09-26 17:08:00.720 
Epoch 86/1000 
	 loss: 22.1751, MinusLogProbMetric: 22.1751, val_loss: 22.5674, val_MinusLogProbMetric: 22.5674

Epoch 86: val_loss did not improve from 22.12296
196/196 - 66s - loss: 22.1751 - MinusLogProbMetric: 22.1751 - val_loss: 22.5674 - val_MinusLogProbMetric: 22.5674 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 87/1000
2023-09-26 17:09:06.555 
Epoch 87/1000 
	 loss: 22.1009, MinusLogProbMetric: 22.1009, val_loss: 22.0807, val_MinusLogProbMetric: 22.0807

Epoch 87: val_loss improved from 22.12296 to 22.08066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 22.1009 - MinusLogProbMetric: 22.1009 - val_loss: 22.0807 - val_MinusLogProbMetric: 22.0807 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 88/1000
2023-09-26 17:10:13.172 
Epoch 88/1000 
	 loss: 22.0300, MinusLogProbMetric: 22.0300, val_loss: 22.2702, val_MinusLogProbMetric: 22.2702

Epoch 88: val_loss did not improve from 22.08066
196/196 - 66s - loss: 22.0300 - MinusLogProbMetric: 22.0300 - val_loss: 22.2702 - val_MinusLogProbMetric: 22.2702 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 89/1000
2023-09-26 17:11:19.853 
Epoch 89/1000 
	 loss: 22.1625, MinusLogProbMetric: 22.1625, val_loss: 21.9319, val_MinusLogProbMetric: 21.9319

Epoch 89: val_loss improved from 22.08066 to 21.93189, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 68s - loss: 22.1625 - MinusLogProbMetric: 22.1625 - val_loss: 21.9319 - val_MinusLogProbMetric: 21.9319 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 90/1000
2023-09-26 17:12:26.050 
Epoch 90/1000 
	 loss: 21.9317, MinusLogProbMetric: 21.9317, val_loss: 21.5103, val_MinusLogProbMetric: 21.5103

Epoch 90: val_loss improved from 21.93189 to 21.51026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 21.9317 - MinusLogProbMetric: 21.9317 - val_loss: 21.5103 - val_MinusLogProbMetric: 21.5103 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 91/1000
2023-09-26 17:13:32.844 
Epoch 91/1000 
	 loss: 21.7707, MinusLogProbMetric: 21.7707, val_loss: 22.2007, val_MinusLogProbMetric: 22.2007

Epoch 91: val_loss did not improve from 21.51026
196/196 - 66s - loss: 21.7707 - MinusLogProbMetric: 21.7707 - val_loss: 22.2007 - val_MinusLogProbMetric: 22.2007 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 92/1000
2023-09-26 17:14:37.570 
Epoch 92/1000 
	 loss: 22.0148, MinusLogProbMetric: 22.0148, val_loss: 22.6099, val_MinusLogProbMetric: 22.6099

Epoch 92: val_loss did not improve from 21.51026
196/196 - 65s - loss: 22.0148 - MinusLogProbMetric: 22.0148 - val_loss: 22.6099 - val_MinusLogProbMetric: 22.6099 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 93/1000
2023-09-26 17:15:43.043 
Epoch 93/1000 
	 loss: 21.7482, MinusLogProbMetric: 21.7482, val_loss: 22.4018, val_MinusLogProbMetric: 22.4018

Epoch 93: val_loss did not improve from 21.51026
196/196 - 65s - loss: 21.7482 - MinusLogProbMetric: 21.7482 - val_loss: 22.4018 - val_MinusLogProbMetric: 22.4018 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 94/1000
2023-09-26 17:16:49.624 
Epoch 94/1000 
	 loss: 21.7815, MinusLogProbMetric: 21.7815, val_loss: 22.0693, val_MinusLogProbMetric: 22.0693

Epoch 94: val_loss did not improve from 21.51026
196/196 - 67s - loss: 21.7815 - MinusLogProbMetric: 21.7815 - val_loss: 22.0693 - val_MinusLogProbMetric: 22.0693 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 95/1000
2023-09-26 17:17:54.008 
Epoch 95/1000 
	 loss: 21.8538, MinusLogProbMetric: 21.8538, val_loss: 23.0371, val_MinusLogProbMetric: 23.0371

Epoch 95: val_loss did not improve from 21.51026
196/196 - 64s - loss: 21.8538 - MinusLogProbMetric: 21.8538 - val_loss: 23.0371 - val_MinusLogProbMetric: 23.0371 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 96/1000
2023-09-26 17:18:57.853 
Epoch 96/1000 
	 loss: 21.5757, MinusLogProbMetric: 21.5757, val_loss: 21.9386, val_MinusLogProbMetric: 21.9386

Epoch 96: val_loss did not improve from 21.51026
196/196 - 64s - loss: 21.5757 - MinusLogProbMetric: 21.5757 - val_loss: 21.9386 - val_MinusLogProbMetric: 21.9386 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 97/1000
2023-09-26 17:20:03.039 
Epoch 97/1000 
	 loss: 21.7320, MinusLogProbMetric: 21.7320, val_loss: 21.9591, val_MinusLogProbMetric: 21.9591

Epoch 97: val_loss did not improve from 21.51026
196/196 - 65s - loss: 21.7320 - MinusLogProbMetric: 21.7320 - val_loss: 21.9591 - val_MinusLogProbMetric: 21.9591 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 98/1000
2023-09-26 17:21:07.571 
Epoch 98/1000 
	 loss: 21.4293, MinusLogProbMetric: 21.4293, val_loss: 21.6512, val_MinusLogProbMetric: 21.6512

Epoch 98: val_loss did not improve from 21.51026
196/196 - 65s - loss: 21.4293 - MinusLogProbMetric: 21.4293 - val_loss: 21.6512 - val_MinusLogProbMetric: 21.6512 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 99/1000
2023-09-26 17:22:11.843 
Epoch 99/1000 
	 loss: 21.5026, MinusLogProbMetric: 21.5026, val_loss: 21.5861, val_MinusLogProbMetric: 21.5861

Epoch 99: val_loss did not improve from 21.51026
196/196 - 64s - loss: 21.5026 - MinusLogProbMetric: 21.5026 - val_loss: 21.5861 - val_MinusLogProbMetric: 21.5861 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 100/1000
2023-09-26 17:23:17.110 
Epoch 100/1000 
	 loss: 21.4718, MinusLogProbMetric: 21.4718, val_loss: 21.7787, val_MinusLogProbMetric: 21.7787

Epoch 100: val_loss did not improve from 21.51026
196/196 - 65s - loss: 21.4718 - MinusLogProbMetric: 21.4718 - val_loss: 21.7787 - val_MinusLogProbMetric: 21.7787 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 101/1000
2023-09-26 17:24:21.103 
Epoch 101/1000 
	 loss: 21.5504, MinusLogProbMetric: 21.5504, val_loss: 21.3073, val_MinusLogProbMetric: 21.3073

Epoch 101: val_loss improved from 21.51026 to 21.30734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 21.5504 - MinusLogProbMetric: 21.5504 - val_loss: 21.3073 - val_MinusLogProbMetric: 21.3073 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 102/1000
2023-09-26 17:25:28.597 
Epoch 102/1000 
	 loss: 21.4413, MinusLogProbMetric: 21.4413, val_loss: 21.3168, val_MinusLogProbMetric: 21.3168

Epoch 102: val_loss did not improve from 21.30734
196/196 - 66s - loss: 21.4413 - MinusLogProbMetric: 21.4413 - val_loss: 21.3168 - val_MinusLogProbMetric: 21.3168 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 103/1000
2023-09-26 17:26:33.490 
Epoch 103/1000 
	 loss: 21.3742, MinusLogProbMetric: 21.3742, val_loss: 21.2809, val_MinusLogProbMetric: 21.2809

Epoch 103: val_loss improved from 21.30734 to 21.28092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 21.3742 - MinusLogProbMetric: 21.3742 - val_loss: 21.2809 - val_MinusLogProbMetric: 21.2809 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 104/1000
2023-09-26 17:27:39.468 
Epoch 104/1000 
	 loss: 21.3269, MinusLogProbMetric: 21.3269, val_loss: 21.3542, val_MinusLogProbMetric: 21.3542

Epoch 104: val_loss did not improve from 21.28092
196/196 - 65s - loss: 21.3269 - MinusLogProbMetric: 21.3269 - val_loss: 21.3542 - val_MinusLogProbMetric: 21.3542 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 105/1000
2023-09-26 17:28:43.554 
Epoch 105/1000 
	 loss: 21.2578, MinusLogProbMetric: 21.2578, val_loss: 21.7243, val_MinusLogProbMetric: 21.7243

Epoch 105: val_loss did not improve from 21.28092
196/196 - 64s - loss: 21.2578 - MinusLogProbMetric: 21.2578 - val_loss: 21.7243 - val_MinusLogProbMetric: 21.7243 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 106/1000
2023-09-26 17:29:48.159 
Epoch 106/1000 
	 loss: 21.3971, MinusLogProbMetric: 21.3971, val_loss: 21.3520, val_MinusLogProbMetric: 21.3520

Epoch 106: val_loss did not improve from 21.28092
196/196 - 65s - loss: 21.3971 - MinusLogProbMetric: 21.3971 - val_loss: 21.3520 - val_MinusLogProbMetric: 21.3520 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 107/1000
2023-09-26 17:30:53.615 
Epoch 107/1000 
	 loss: 21.2399, MinusLogProbMetric: 21.2399, val_loss: 21.7170, val_MinusLogProbMetric: 21.7170

Epoch 107: val_loss did not improve from 21.28092
196/196 - 65s - loss: 21.2399 - MinusLogProbMetric: 21.2399 - val_loss: 21.7170 - val_MinusLogProbMetric: 21.7170 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 108/1000
2023-09-26 17:31:58.589 
Epoch 108/1000 
	 loss: 21.2351, MinusLogProbMetric: 21.2351, val_loss: 22.3537, val_MinusLogProbMetric: 22.3537

Epoch 108: val_loss did not improve from 21.28092
196/196 - 65s - loss: 21.2351 - MinusLogProbMetric: 21.2351 - val_loss: 22.3537 - val_MinusLogProbMetric: 22.3537 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 109/1000
2023-09-26 17:33:02.602 
Epoch 109/1000 
	 loss: 21.0078, MinusLogProbMetric: 21.0078, val_loss: 20.8512, val_MinusLogProbMetric: 20.8512

Epoch 109: val_loss improved from 21.28092 to 20.85117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 21.0078 - MinusLogProbMetric: 21.0078 - val_loss: 20.8512 - val_MinusLogProbMetric: 20.8512 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 110/1000
2023-09-26 17:34:08.518 
Epoch 110/1000 
	 loss: 21.0157, MinusLogProbMetric: 21.0157, val_loss: 21.0905, val_MinusLogProbMetric: 21.0905

Epoch 110: val_loss did not improve from 20.85117
196/196 - 65s - loss: 21.0157 - MinusLogProbMetric: 21.0157 - val_loss: 21.0905 - val_MinusLogProbMetric: 21.0905 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 111/1000
2023-09-26 17:35:13.233 
Epoch 111/1000 
	 loss: 21.0547, MinusLogProbMetric: 21.0547, val_loss: 21.2331, val_MinusLogProbMetric: 21.2331

Epoch 111: val_loss did not improve from 20.85117
196/196 - 65s - loss: 21.0547 - MinusLogProbMetric: 21.0547 - val_loss: 21.2331 - val_MinusLogProbMetric: 21.2331 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 112/1000
2023-09-26 17:36:18.189 
Epoch 112/1000 
	 loss: 21.0356, MinusLogProbMetric: 21.0356, val_loss: 21.1002, val_MinusLogProbMetric: 21.1002

Epoch 112: val_loss did not improve from 20.85117
196/196 - 65s - loss: 21.0356 - MinusLogProbMetric: 21.0356 - val_loss: 21.1002 - val_MinusLogProbMetric: 21.1002 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 113/1000
2023-09-26 17:37:23.475 
Epoch 113/1000 
	 loss: 20.8570, MinusLogProbMetric: 20.8570, val_loss: 21.5532, val_MinusLogProbMetric: 21.5532

Epoch 113: val_loss did not improve from 20.85117
196/196 - 65s - loss: 20.8570 - MinusLogProbMetric: 20.8570 - val_loss: 21.5532 - val_MinusLogProbMetric: 21.5532 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 114/1000
2023-09-26 17:38:29.296 
Epoch 114/1000 
	 loss: 20.8675, MinusLogProbMetric: 20.8675, val_loss: 21.1795, val_MinusLogProbMetric: 21.1795

Epoch 114: val_loss did not improve from 20.85117
196/196 - 66s - loss: 20.8675 - MinusLogProbMetric: 20.8675 - val_loss: 21.1795 - val_MinusLogProbMetric: 21.1795 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 115/1000
2023-09-26 17:39:34.552 
Epoch 115/1000 
	 loss: 20.8015, MinusLogProbMetric: 20.8015, val_loss: 20.8774, val_MinusLogProbMetric: 20.8774

Epoch 115: val_loss did not improve from 20.85117
196/196 - 65s - loss: 20.8015 - MinusLogProbMetric: 20.8015 - val_loss: 20.8774 - val_MinusLogProbMetric: 20.8774 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 116/1000
2023-09-26 17:40:38.118 
Epoch 116/1000 
	 loss: 20.8945, MinusLogProbMetric: 20.8945, val_loss: 21.0240, val_MinusLogProbMetric: 21.0240

Epoch 116: val_loss did not improve from 20.85117
196/196 - 64s - loss: 20.8945 - MinusLogProbMetric: 20.8945 - val_loss: 21.0240 - val_MinusLogProbMetric: 21.0240 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 117/1000
2023-09-26 17:41:43.552 
Epoch 117/1000 
	 loss: 20.8606, MinusLogProbMetric: 20.8606, val_loss: 20.9905, val_MinusLogProbMetric: 20.9905

Epoch 117: val_loss did not improve from 20.85117
196/196 - 65s - loss: 20.8606 - MinusLogProbMetric: 20.8606 - val_loss: 20.9905 - val_MinusLogProbMetric: 20.9905 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 118/1000
2023-09-26 17:42:48.856 
Epoch 118/1000 
	 loss: 20.8923, MinusLogProbMetric: 20.8923, val_loss: 22.0946, val_MinusLogProbMetric: 22.0946

Epoch 118: val_loss did not improve from 20.85117
196/196 - 65s - loss: 20.8923 - MinusLogProbMetric: 20.8923 - val_loss: 22.0946 - val_MinusLogProbMetric: 22.0946 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 119/1000
2023-09-26 17:43:54.862 
Epoch 119/1000 
	 loss: 20.7468, MinusLogProbMetric: 20.7468, val_loss: 21.5561, val_MinusLogProbMetric: 21.5561

Epoch 119: val_loss did not improve from 20.85117
196/196 - 66s - loss: 20.7468 - MinusLogProbMetric: 20.7468 - val_loss: 21.5561 - val_MinusLogProbMetric: 21.5561 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 120/1000
2023-09-26 17:44:59.073 
Epoch 120/1000 
	 loss: 20.7503, MinusLogProbMetric: 20.7503, val_loss: 20.6435, val_MinusLogProbMetric: 20.6435

Epoch 120: val_loss improved from 20.85117 to 20.64353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 20.7503 - MinusLogProbMetric: 20.7503 - val_loss: 20.6435 - val_MinusLogProbMetric: 20.6435 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 121/1000
2023-09-26 17:46:04.766 
Epoch 121/1000 
	 loss: 20.6492, MinusLogProbMetric: 20.6492, val_loss: 21.0181, val_MinusLogProbMetric: 21.0181

Epoch 121: val_loss did not improve from 20.64353
196/196 - 65s - loss: 20.6492 - MinusLogProbMetric: 20.6492 - val_loss: 21.0181 - val_MinusLogProbMetric: 21.0181 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 122/1000
2023-09-26 17:47:08.162 
Epoch 122/1000 
	 loss: 20.7418, MinusLogProbMetric: 20.7418, val_loss: 20.5444, val_MinusLogProbMetric: 20.5444

Epoch 122: val_loss improved from 20.64353 to 20.54441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 20.7418 - MinusLogProbMetric: 20.7418 - val_loss: 20.5444 - val_MinusLogProbMetric: 20.5444 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 123/1000
2023-09-26 17:48:14.022 
Epoch 123/1000 
	 loss: 20.5784, MinusLogProbMetric: 20.5784, val_loss: 22.4996, val_MinusLogProbMetric: 22.4996

Epoch 123: val_loss did not improve from 20.54441
196/196 - 65s - loss: 20.5784 - MinusLogProbMetric: 20.5784 - val_loss: 22.4996 - val_MinusLogProbMetric: 22.4996 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 124/1000
2023-09-26 17:49:18.702 
Epoch 124/1000 
	 loss: 20.6384, MinusLogProbMetric: 20.6384, val_loss: 21.1260, val_MinusLogProbMetric: 21.1260

Epoch 124: val_loss did not improve from 20.54441
196/196 - 65s - loss: 20.6384 - MinusLogProbMetric: 20.6384 - val_loss: 21.1260 - val_MinusLogProbMetric: 21.1260 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 125/1000
2023-09-26 17:50:23.243 
Epoch 125/1000 
	 loss: 20.5754, MinusLogProbMetric: 20.5754, val_loss: 21.0299, val_MinusLogProbMetric: 21.0299

Epoch 125: val_loss did not improve from 20.54441
196/196 - 65s - loss: 20.5754 - MinusLogProbMetric: 20.5754 - val_loss: 21.0299 - val_MinusLogProbMetric: 21.0299 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 126/1000
2023-09-26 17:51:28.017 
Epoch 126/1000 
	 loss: 20.5165, MinusLogProbMetric: 20.5165, val_loss: 20.5041, val_MinusLogProbMetric: 20.5041

Epoch 126: val_loss improved from 20.54441 to 20.50406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 20.5165 - MinusLogProbMetric: 20.5165 - val_loss: 20.5041 - val_MinusLogProbMetric: 20.5041 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 127/1000
2023-09-26 17:52:33.480 
Epoch 127/1000 
	 loss: 20.5872, MinusLogProbMetric: 20.5872, val_loss: 20.7708, val_MinusLogProbMetric: 20.7708

Epoch 127: val_loss did not improve from 20.50406
196/196 - 64s - loss: 20.5872 - MinusLogProbMetric: 20.5872 - val_loss: 20.7708 - val_MinusLogProbMetric: 20.7708 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 128/1000
2023-09-26 17:53:37.329 
Epoch 128/1000 
	 loss: 20.7097, MinusLogProbMetric: 20.7097, val_loss: 20.5880, val_MinusLogProbMetric: 20.5880

Epoch 128: val_loss did not improve from 20.50406
196/196 - 64s - loss: 20.7097 - MinusLogProbMetric: 20.7097 - val_loss: 20.5880 - val_MinusLogProbMetric: 20.5880 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 129/1000
2023-09-26 17:54:41.581 
Epoch 129/1000 
	 loss: 20.5512, MinusLogProbMetric: 20.5512, val_loss: 20.7232, val_MinusLogProbMetric: 20.7232

Epoch 129: val_loss did not improve from 20.50406
196/196 - 64s - loss: 20.5512 - MinusLogProbMetric: 20.5512 - val_loss: 20.7232 - val_MinusLogProbMetric: 20.7232 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 130/1000
2023-09-26 17:55:46.826 
Epoch 130/1000 
	 loss: 20.5693, MinusLogProbMetric: 20.5693, val_loss: 20.6881, val_MinusLogProbMetric: 20.6881

Epoch 130: val_loss did not improve from 20.50406
196/196 - 65s - loss: 20.5693 - MinusLogProbMetric: 20.5693 - val_loss: 20.6881 - val_MinusLogProbMetric: 20.6881 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 131/1000
2023-09-26 17:56:51.235 
Epoch 131/1000 
	 loss: 20.4200, MinusLogProbMetric: 20.4200, val_loss: 20.1829, val_MinusLogProbMetric: 20.1829

Epoch 131: val_loss improved from 20.50406 to 20.18287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 20.4200 - MinusLogProbMetric: 20.4200 - val_loss: 20.1829 - val_MinusLogProbMetric: 20.1829 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 132/1000
2023-09-26 17:57:57.219 
Epoch 132/1000 
	 loss: 20.3680, MinusLogProbMetric: 20.3680, val_loss: 20.7471, val_MinusLogProbMetric: 20.7471

Epoch 132: val_loss did not improve from 20.18287
196/196 - 65s - loss: 20.3680 - MinusLogProbMetric: 20.3680 - val_loss: 20.7471 - val_MinusLogProbMetric: 20.7471 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 133/1000
2023-09-26 17:59:01.331 
Epoch 133/1000 
	 loss: 20.2589, MinusLogProbMetric: 20.2589, val_loss: 20.3345, val_MinusLogProbMetric: 20.3345

Epoch 133: val_loss did not improve from 20.18287
196/196 - 64s - loss: 20.2589 - MinusLogProbMetric: 20.2589 - val_loss: 20.3345 - val_MinusLogProbMetric: 20.3345 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 134/1000
2023-09-26 18:00:06.624 
Epoch 134/1000 
	 loss: 20.1972, MinusLogProbMetric: 20.1972, val_loss: 20.3971, val_MinusLogProbMetric: 20.3971

Epoch 134: val_loss did not improve from 20.18287
196/196 - 65s - loss: 20.1972 - MinusLogProbMetric: 20.1972 - val_loss: 20.3971 - val_MinusLogProbMetric: 20.3971 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 135/1000
2023-09-26 18:01:10.271 
Epoch 135/1000 
	 loss: 20.3134, MinusLogProbMetric: 20.3134, val_loss: 20.4569, val_MinusLogProbMetric: 20.4569

Epoch 135: val_loss did not improve from 20.18287
196/196 - 64s - loss: 20.3134 - MinusLogProbMetric: 20.3134 - val_loss: 20.4569 - val_MinusLogProbMetric: 20.4569 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 136/1000
2023-09-26 18:02:13.129 
Epoch 136/1000 
	 loss: 20.2550, MinusLogProbMetric: 20.2550, val_loss: 20.5480, val_MinusLogProbMetric: 20.5480

Epoch 136: val_loss did not improve from 20.18287
196/196 - 63s - loss: 20.2550 - MinusLogProbMetric: 20.2550 - val_loss: 20.5480 - val_MinusLogProbMetric: 20.5480 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 137/1000
2023-09-26 18:03:16.743 
Epoch 137/1000 
	 loss: 20.2251, MinusLogProbMetric: 20.2251, val_loss: 20.1950, val_MinusLogProbMetric: 20.1950

Epoch 137: val_loss did not improve from 20.18287
196/196 - 64s - loss: 20.2251 - MinusLogProbMetric: 20.2251 - val_loss: 20.1950 - val_MinusLogProbMetric: 20.1950 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 138/1000
2023-09-26 18:04:21.517 
Epoch 138/1000 
	 loss: 20.1726, MinusLogProbMetric: 20.1726, val_loss: 20.6678, val_MinusLogProbMetric: 20.6678

Epoch 138: val_loss did not improve from 20.18287
196/196 - 65s - loss: 20.1726 - MinusLogProbMetric: 20.1726 - val_loss: 20.6678 - val_MinusLogProbMetric: 20.6678 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 139/1000
2023-09-26 18:05:25.844 
Epoch 139/1000 
	 loss: 20.1588, MinusLogProbMetric: 20.1588, val_loss: 20.5119, val_MinusLogProbMetric: 20.5119

Epoch 139: val_loss did not improve from 20.18287
196/196 - 64s - loss: 20.1588 - MinusLogProbMetric: 20.1588 - val_loss: 20.5119 - val_MinusLogProbMetric: 20.5119 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 140/1000
2023-09-26 18:06:29.866 
Epoch 140/1000 
	 loss: 20.1847, MinusLogProbMetric: 20.1847, val_loss: 20.3197, val_MinusLogProbMetric: 20.3197

Epoch 140: val_loss did not improve from 20.18287
196/196 - 64s - loss: 20.1847 - MinusLogProbMetric: 20.1847 - val_loss: 20.3197 - val_MinusLogProbMetric: 20.3197 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 141/1000
2023-09-26 18:07:35.523 
Epoch 141/1000 
	 loss: 20.1674, MinusLogProbMetric: 20.1674, val_loss: 21.0297, val_MinusLogProbMetric: 21.0297

Epoch 141: val_loss did not improve from 20.18287
196/196 - 66s - loss: 20.1674 - MinusLogProbMetric: 20.1674 - val_loss: 21.0297 - val_MinusLogProbMetric: 21.0297 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 142/1000
2023-09-26 18:08:40.381 
Epoch 142/1000 
	 loss: 20.1124, MinusLogProbMetric: 20.1124, val_loss: 20.4901, val_MinusLogProbMetric: 20.4901

Epoch 142: val_loss did not improve from 20.18287
196/196 - 65s - loss: 20.1124 - MinusLogProbMetric: 20.1124 - val_loss: 20.4901 - val_MinusLogProbMetric: 20.4901 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 143/1000
2023-09-26 18:09:43.597 
Epoch 143/1000 
	 loss: 20.0957, MinusLogProbMetric: 20.0957, val_loss: 20.1958, val_MinusLogProbMetric: 20.1958

Epoch 143: val_loss did not improve from 20.18287
196/196 - 63s - loss: 20.0957 - MinusLogProbMetric: 20.0957 - val_loss: 20.1958 - val_MinusLogProbMetric: 20.1958 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 144/1000
2023-09-26 18:10:47.328 
Epoch 144/1000 
	 loss: 20.0497, MinusLogProbMetric: 20.0497, val_loss: 20.0217, val_MinusLogProbMetric: 20.0217

Epoch 144: val_loss improved from 20.18287 to 20.02168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 20.0497 - MinusLogProbMetric: 20.0497 - val_loss: 20.0217 - val_MinusLogProbMetric: 20.0217 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 145/1000
2023-09-26 18:11:52.395 
Epoch 145/1000 
	 loss: 20.0715, MinusLogProbMetric: 20.0715, val_loss: 20.7703, val_MinusLogProbMetric: 20.7703

Epoch 145: val_loss did not improve from 20.02168
196/196 - 64s - loss: 20.0715 - MinusLogProbMetric: 20.0715 - val_loss: 20.7703 - val_MinusLogProbMetric: 20.7703 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 146/1000
2023-09-26 18:12:57.944 
Epoch 146/1000 
	 loss: 20.0813, MinusLogProbMetric: 20.0813, val_loss: 20.8722, val_MinusLogProbMetric: 20.8722

Epoch 146: val_loss did not improve from 20.02168
196/196 - 66s - loss: 20.0813 - MinusLogProbMetric: 20.0813 - val_loss: 20.8722 - val_MinusLogProbMetric: 20.8722 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 147/1000
2023-09-26 18:14:01.769 
Epoch 147/1000 
	 loss: 19.9568, MinusLogProbMetric: 19.9568, val_loss: 20.2304, val_MinusLogProbMetric: 20.2304

Epoch 147: val_loss did not improve from 20.02168
196/196 - 64s - loss: 19.9568 - MinusLogProbMetric: 19.9568 - val_loss: 20.2304 - val_MinusLogProbMetric: 20.2304 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 148/1000
2023-09-26 18:15:05.035 
Epoch 148/1000 
	 loss: 19.8765, MinusLogProbMetric: 19.8765, val_loss: 20.3855, val_MinusLogProbMetric: 20.3855

Epoch 148: val_loss did not improve from 20.02168
196/196 - 63s - loss: 19.8765 - MinusLogProbMetric: 19.8765 - val_loss: 20.3855 - val_MinusLogProbMetric: 20.3855 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 149/1000
2023-09-26 18:16:09.025 
Epoch 149/1000 
	 loss: 19.9560, MinusLogProbMetric: 19.9560, val_loss: 20.3101, val_MinusLogProbMetric: 20.3101

Epoch 149: val_loss did not improve from 20.02168
196/196 - 64s - loss: 19.9560 - MinusLogProbMetric: 19.9560 - val_loss: 20.3101 - val_MinusLogProbMetric: 20.3101 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 150/1000
2023-09-26 18:17:13.096 
Epoch 150/1000 
	 loss: 19.8613, MinusLogProbMetric: 19.8613, val_loss: 20.4587, val_MinusLogProbMetric: 20.4587

Epoch 150: val_loss did not improve from 20.02168
196/196 - 64s - loss: 19.8613 - MinusLogProbMetric: 19.8613 - val_loss: 20.4587 - val_MinusLogProbMetric: 20.4587 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 151/1000
2023-09-26 18:18:16.286 
Epoch 151/1000 
	 loss: 19.9575, MinusLogProbMetric: 19.9575, val_loss: 20.9642, val_MinusLogProbMetric: 20.9642

Epoch 151: val_loss did not improve from 20.02168
196/196 - 63s - loss: 19.9575 - MinusLogProbMetric: 19.9575 - val_loss: 20.9642 - val_MinusLogProbMetric: 20.9642 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 152/1000
2023-09-26 18:19:19.927 
Epoch 152/1000 
	 loss: 19.8387, MinusLogProbMetric: 19.8387, val_loss: 19.9489, val_MinusLogProbMetric: 19.9489

Epoch 152: val_loss improved from 20.02168 to 19.94887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 19.8387 - MinusLogProbMetric: 19.8387 - val_loss: 19.9489 - val_MinusLogProbMetric: 19.9489 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 153/1000
2023-09-26 18:20:25.210 
Epoch 153/1000 
	 loss: 19.8790, MinusLogProbMetric: 19.8790, val_loss: 20.1159, val_MinusLogProbMetric: 20.1159

Epoch 153: val_loss did not improve from 19.94887
196/196 - 64s - loss: 19.8790 - MinusLogProbMetric: 19.8790 - val_loss: 20.1159 - val_MinusLogProbMetric: 20.1159 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 154/1000
2023-09-26 18:21:30.647 
Epoch 154/1000 
	 loss: 19.7879, MinusLogProbMetric: 19.7879, val_loss: 20.4764, val_MinusLogProbMetric: 20.4764

Epoch 154: val_loss did not improve from 19.94887
196/196 - 65s - loss: 19.7879 - MinusLogProbMetric: 19.7879 - val_loss: 20.4764 - val_MinusLogProbMetric: 20.4764 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 155/1000
2023-09-26 18:22:34.565 
Epoch 155/1000 
	 loss: 19.8540, MinusLogProbMetric: 19.8540, val_loss: 21.0535, val_MinusLogProbMetric: 21.0535

Epoch 155: val_loss did not improve from 19.94887
196/196 - 64s - loss: 19.8540 - MinusLogProbMetric: 19.8540 - val_loss: 21.0535 - val_MinusLogProbMetric: 21.0535 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 156/1000
2023-09-26 18:23:39.413 
Epoch 156/1000 
	 loss: 19.7841, MinusLogProbMetric: 19.7841, val_loss: 20.1583, val_MinusLogProbMetric: 20.1583

Epoch 156: val_loss did not improve from 19.94887
196/196 - 65s - loss: 19.7841 - MinusLogProbMetric: 19.7841 - val_loss: 20.1583 - val_MinusLogProbMetric: 20.1583 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 157/1000
2023-09-26 18:24:44.077 
Epoch 157/1000 
	 loss: 19.7265, MinusLogProbMetric: 19.7265, val_loss: 21.0109, val_MinusLogProbMetric: 21.0109

Epoch 157: val_loss did not improve from 19.94887
196/196 - 65s - loss: 19.7265 - MinusLogProbMetric: 19.7265 - val_loss: 21.0109 - val_MinusLogProbMetric: 21.0109 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 158/1000
2023-09-26 18:25:49.896 
Epoch 158/1000 
	 loss: 19.7605, MinusLogProbMetric: 19.7605, val_loss: 20.1407, val_MinusLogProbMetric: 20.1407

Epoch 158: val_loss did not improve from 19.94887
196/196 - 66s - loss: 19.7605 - MinusLogProbMetric: 19.7605 - val_loss: 20.1407 - val_MinusLogProbMetric: 20.1407 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 159/1000
2023-09-26 18:26:55.444 
Epoch 159/1000 
	 loss: 19.7254, MinusLogProbMetric: 19.7254, val_loss: 19.9036, val_MinusLogProbMetric: 19.9036

Epoch 159: val_loss improved from 19.94887 to 19.90359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 19.7254 - MinusLogProbMetric: 19.7254 - val_loss: 19.9036 - val_MinusLogProbMetric: 19.9036 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 160/1000
2023-09-26 18:28:01.998 
Epoch 160/1000 
	 loss: 19.6644, MinusLogProbMetric: 19.6644, val_loss: 19.8776, val_MinusLogProbMetric: 19.8776

Epoch 160: val_loss improved from 19.90359 to 19.87763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 19.6644 - MinusLogProbMetric: 19.6644 - val_loss: 19.8776 - val_MinusLogProbMetric: 19.8776 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 161/1000
2023-09-26 18:29:08.401 
Epoch 161/1000 
	 loss: 19.7340, MinusLogProbMetric: 19.7340, val_loss: 19.4928, val_MinusLogProbMetric: 19.4928

Epoch 161: val_loss improved from 19.87763 to 19.49284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 19.7340 - MinusLogProbMetric: 19.7340 - val_loss: 19.4928 - val_MinusLogProbMetric: 19.4928 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 162/1000
2023-09-26 18:30:14.230 
Epoch 162/1000 
	 loss: 19.7748, MinusLogProbMetric: 19.7748, val_loss: 19.7083, val_MinusLogProbMetric: 19.7083

Epoch 162: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.7748 - MinusLogProbMetric: 19.7748 - val_loss: 19.7083 - val_MinusLogProbMetric: 19.7083 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 163/1000
2023-09-26 18:31:18.409 
Epoch 163/1000 
	 loss: 19.5841, MinusLogProbMetric: 19.5841, val_loss: 20.2230, val_MinusLogProbMetric: 20.2230

Epoch 163: val_loss did not improve from 19.49284
196/196 - 64s - loss: 19.5841 - MinusLogProbMetric: 19.5841 - val_loss: 20.2230 - val_MinusLogProbMetric: 20.2230 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 164/1000
2023-09-26 18:32:22.412 
Epoch 164/1000 
	 loss: 19.8040, MinusLogProbMetric: 19.8040, val_loss: 19.7150, val_MinusLogProbMetric: 19.7150

Epoch 164: val_loss did not improve from 19.49284
196/196 - 64s - loss: 19.8040 - MinusLogProbMetric: 19.8040 - val_loss: 19.7150 - val_MinusLogProbMetric: 19.7150 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 165/1000
2023-09-26 18:33:26.557 
Epoch 165/1000 
	 loss: 19.6194, MinusLogProbMetric: 19.6194, val_loss: 19.9231, val_MinusLogProbMetric: 19.9231

Epoch 165: val_loss did not improve from 19.49284
196/196 - 64s - loss: 19.6194 - MinusLogProbMetric: 19.6194 - val_loss: 19.9231 - val_MinusLogProbMetric: 19.9231 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 166/1000
2023-09-26 18:34:30.641 
Epoch 166/1000 
	 loss: 19.6769, MinusLogProbMetric: 19.6769, val_loss: 19.5652, val_MinusLogProbMetric: 19.5652

Epoch 166: val_loss did not improve from 19.49284
196/196 - 64s - loss: 19.6769 - MinusLogProbMetric: 19.6769 - val_loss: 19.5652 - val_MinusLogProbMetric: 19.5652 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 167/1000
2023-09-26 18:35:35.655 
Epoch 167/1000 
	 loss: 19.5049, MinusLogProbMetric: 19.5049, val_loss: 19.9552, val_MinusLogProbMetric: 19.9552

Epoch 167: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.5049 - MinusLogProbMetric: 19.5049 - val_loss: 19.9552 - val_MinusLogProbMetric: 19.9552 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 168/1000
2023-09-26 18:36:40.272 
Epoch 168/1000 
	 loss: 19.6408, MinusLogProbMetric: 19.6408, val_loss: 19.9305, val_MinusLogProbMetric: 19.9305

Epoch 168: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.6408 - MinusLogProbMetric: 19.6408 - val_loss: 19.9305 - val_MinusLogProbMetric: 19.9305 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 169/1000
2023-09-26 18:37:46.086 
Epoch 169/1000 
	 loss: 19.4561, MinusLogProbMetric: 19.4561, val_loss: 19.5401, val_MinusLogProbMetric: 19.5401

Epoch 169: val_loss did not improve from 19.49284
196/196 - 66s - loss: 19.4561 - MinusLogProbMetric: 19.4561 - val_loss: 19.5401 - val_MinusLogProbMetric: 19.5401 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 170/1000
2023-09-26 18:38:49.936 
Epoch 170/1000 
	 loss: 19.5053, MinusLogProbMetric: 19.5053, val_loss: 19.5897, val_MinusLogProbMetric: 19.5897

Epoch 170: val_loss did not improve from 19.49284
196/196 - 64s - loss: 19.5053 - MinusLogProbMetric: 19.5053 - val_loss: 19.5897 - val_MinusLogProbMetric: 19.5897 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 171/1000
2023-09-26 18:39:54.734 
Epoch 171/1000 
	 loss: 19.5128, MinusLogProbMetric: 19.5128, val_loss: 20.3687, val_MinusLogProbMetric: 20.3687

Epoch 171: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.5128 - MinusLogProbMetric: 19.5128 - val_loss: 20.3687 - val_MinusLogProbMetric: 20.3687 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 172/1000
2023-09-26 18:41:00.644 
Epoch 172/1000 
	 loss: 19.4846, MinusLogProbMetric: 19.4846, val_loss: 19.6753, val_MinusLogProbMetric: 19.6753

Epoch 172: val_loss did not improve from 19.49284
196/196 - 66s - loss: 19.4846 - MinusLogProbMetric: 19.4846 - val_loss: 19.6753 - val_MinusLogProbMetric: 19.6753 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 173/1000
2023-09-26 18:42:06.063 
Epoch 173/1000 
	 loss: 19.4340, MinusLogProbMetric: 19.4340, val_loss: 19.8354, val_MinusLogProbMetric: 19.8354

Epoch 173: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.4340 - MinusLogProbMetric: 19.4340 - val_loss: 19.8354 - val_MinusLogProbMetric: 19.8354 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 174/1000
2023-09-26 18:43:10.128 
Epoch 174/1000 
	 loss: 19.4505, MinusLogProbMetric: 19.4505, val_loss: 19.4929, val_MinusLogProbMetric: 19.4929

Epoch 174: val_loss did not improve from 19.49284
196/196 - 64s - loss: 19.4505 - MinusLogProbMetric: 19.4505 - val_loss: 19.4929 - val_MinusLogProbMetric: 19.4929 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 175/1000
2023-09-26 18:44:14.569 
Epoch 175/1000 
	 loss: 19.4702, MinusLogProbMetric: 19.4702, val_loss: 20.3315, val_MinusLogProbMetric: 20.3315

Epoch 175: val_loss did not improve from 19.49284
196/196 - 64s - loss: 19.4702 - MinusLogProbMetric: 19.4702 - val_loss: 20.3315 - val_MinusLogProbMetric: 20.3315 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 176/1000
2023-09-26 18:45:20.062 
Epoch 176/1000 
	 loss: 19.4573, MinusLogProbMetric: 19.4573, val_loss: 19.8490, val_MinusLogProbMetric: 19.8490

Epoch 176: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.4573 - MinusLogProbMetric: 19.4573 - val_loss: 19.8490 - val_MinusLogProbMetric: 19.8490 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 177/1000
2023-09-26 18:46:25.374 
Epoch 177/1000 
	 loss: 19.3636, MinusLogProbMetric: 19.3636, val_loss: 20.2900, val_MinusLogProbMetric: 20.2900

Epoch 177: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.3636 - MinusLogProbMetric: 19.3636 - val_loss: 20.2900 - val_MinusLogProbMetric: 20.2900 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 178/1000
2023-09-26 18:47:30.921 
Epoch 178/1000 
	 loss: 19.4015, MinusLogProbMetric: 19.4015, val_loss: 19.7091, val_MinusLogProbMetric: 19.7091

Epoch 178: val_loss did not improve from 19.49284
196/196 - 66s - loss: 19.4015 - MinusLogProbMetric: 19.4015 - val_loss: 19.7091 - val_MinusLogProbMetric: 19.7091 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 179/1000
2023-09-26 18:48:35.924 
Epoch 179/1000 
	 loss: 19.4839, MinusLogProbMetric: 19.4839, val_loss: 19.9508, val_MinusLogProbMetric: 19.9508

Epoch 179: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.4839 - MinusLogProbMetric: 19.4839 - val_loss: 19.9508 - val_MinusLogProbMetric: 19.9508 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 180/1000
2023-09-26 18:49:39.313 
Epoch 180/1000 
	 loss: 19.4947, MinusLogProbMetric: 19.4947, val_loss: 19.6129, val_MinusLogProbMetric: 19.6129

Epoch 180: val_loss did not improve from 19.49284
196/196 - 63s - loss: 19.4947 - MinusLogProbMetric: 19.4947 - val_loss: 19.6129 - val_MinusLogProbMetric: 19.6129 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 181/1000
2023-09-26 18:50:44.630 
Epoch 181/1000 
	 loss: 19.3133, MinusLogProbMetric: 19.3133, val_loss: 19.7089, val_MinusLogProbMetric: 19.7089

Epoch 181: val_loss did not improve from 19.49284
196/196 - 65s - loss: 19.3133 - MinusLogProbMetric: 19.3133 - val_loss: 19.7089 - val_MinusLogProbMetric: 19.7089 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 182/1000
2023-09-26 18:51:50.974 
Epoch 182/1000 
	 loss: 19.3835, MinusLogProbMetric: 19.3835, val_loss: 19.9122, val_MinusLogProbMetric: 19.9122

Epoch 182: val_loss did not improve from 19.49284
196/196 - 66s - loss: 19.3835 - MinusLogProbMetric: 19.3835 - val_loss: 19.9122 - val_MinusLogProbMetric: 19.9122 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 183/1000
2023-09-26 18:52:56.578 
Epoch 183/1000 
	 loss: 19.3133, MinusLogProbMetric: 19.3133, val_loss: 19.4789, val_MinusLogProbMetric: 19.4789

Epoch 183: val_loss improved from 19.49284 to 19.47888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 19.3133 - MinusLogProbMetric: 19.3133 - val_loss: 19.4789 - val_MinusLogProbMetric: 19.4789 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 184/1000
2023-09-26 18:54:03.082 
Epoch 184/1000 
	 loss: 19.3051, MinusLogProbMetric: 19.3051, val_loss: 19.6055, val_MinusLogProbMetric: 19.6055

Epoch 184: val_loss did not improve from 19.47888
196/196 - 66s - loss: 19.3051 - MinusLogProbMetric: 19.3051 - val_loss: 19.6055 - val_MinusLogProbMetric: 19.6055 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 185/1000
2023-09-26 18:55:08.730 
Epoch 185/1000 
	 loss: 19.3076, MinusLogProbMetric: 19.3076, val_loss: 19.2487, val_MinusLogProbMetric: 19.2487

Epoch 185: val_loss improved from 19.47888 to 19.24870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 19.3076 - MinusLogProbMetric: 19.3076 - val_loss: 19.2487 - val_MinusLogProbMetric: 19.2487 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 186/1000
2023-09-26 18:56:16.438 
Epoch 186/1000 
	 loss: 19.2841, MinusLogProbMetric: 19.2841, val_loss: 19.3547, val_MinusLogProbMetric: 19.3547

Epoch 186: val_loss did not improve from 19.24870
196/196 - 67s - loss: 19.2841 - MinusLogProbMetric: 19.2841 - val_loss: 19.3547 - val_MinusLogProbMetric: 19.3547 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 187/1000
2023-09-26 18:57:21.498 
Epoch 187/1000 
	 loss: 19.2161, MinusLogProbMetric: 19.2161, val_loss: 19.6118, val_MinusLogProbMetric: 19.6118

Epoch 187: val_loss did not improve from 19.24870
196/196 - 65s - loss: 19.2161 - MinusLogProbMetric: 19.2161 - val_loss: 19.6118 - val_MinusLogProbMetric: 19.6118 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 188/1000
2023-09-26 18:58:26.193 
Epoch 188/1000 
	 loss: 19.2134, MinusLogProbMetric: 19.2134, val_loss: 19.4343, val_MinusLogProbMetric: 19.4343

Epoch 188: val_loss did not improve from 19.24870
196/196 - 65s - loss: 19.2134 - MinusLogProbMetric: 19.2134 - val_loss: 19.4343 - val_MinusLogProbMetric: 19.4343 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 189/1000
2023-09-26 18:59:30.352 
Epoch 189/1000 
	 loss: 19.2405, MinusLogProbMetric: 19.2405, val_loss: 19.9455, val_MinusLogProbMetric: 19.9455

Epoch 189: val_loss did not improve from 19.24870
196/196 - 64s - loss: 19.2405 - MinusLogProbMetric: 19.2405 - val_loss: 19.9455 - val_MinusLogProbMetric: 19.9455 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 190/1000
2023-09-26 19:00:34.838 
Epoch 190/1000 
	 loss: 19.1735, MinusLogProbMetric: 19.1735, val_loss: 20.1581, val_MinusLogProbMetric: 20.1581

Epoch 190: val_loss did not improve from 19.24870
196/196 - 64s - loss: 19.1735 - MinusLogProbMetric: 19.1735 - val_loss: 20.1581 - val_MinusLogProbMetric: 20.1581 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 191/1000
2023-09-26 19:01:39.601 
Epoch 191/1000 
	 loss: 19.2693, MinusLogProbMetric: 19.2693, val_loss: 19.5289, val_MinusLogProbMetric: 19.5289

Epoch 191: val_loss did not improve from 19.24870
196/196 - 65s - loss: 19.2693 - MinusLogProbMetric: 19.2693 - val_loss: 19.5289 - val_MinusLogProbMetric: 19.5289 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 192/1000
2023-09-26 19:02:43.863 
Epoch 192/1000 
	 loss: 19.2401, MinusLogProbMetric: 19.2401, val_loss: 19.1994, val_MinusLogProbMetric: 19.1994

Epoch 192: val_loss improved from 19.24870 to 19.19944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 19.2401 - MinusLogProbMetric: 19.2401 - val_loss: 19.1994 - val_MinusLogProbMetric: 19.1994 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 193/1000
2023-09-26 19:03:50.228 
Epoch 193/1000 
	 loss: 19.1801, MinusLogProbMetric: 19.1801, val_loss: 19.8456, val_MinusLogProbMetric: 19.8456

Epoch 193: val_loss did not improve from 19.19944
196/196 - 65s - loss: 19.1801 - MinusLogProbMetric: 19.1801 - val_loss: 19.8456 - val_MinusLogProbMetric: 19.8456 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 194/1000
2023-09-26 19:04:55.299 
Epoch 194/1000 
	 loss: 19.1629, MinusLogProbMetric: 19.1629, val_loss: 19.2739, val_MinusLogProbMetric: 19.2739

Epoch 194: val_loss did not improve from 19.19944
196/196 - 65s - loss: 19.1629 - MinusLogProbMetric: 19.1629 - val_loss: 19.2739 - val_MinusLogProbMetric: 19.2739 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 195/1000
2023-09-26 19:06:01.107 
Epoch 195/1000 
	 loss: 19.1448, MinusLogProbMetric: 19.1448, val_loss: 19.2399, val_MinusLogProbMetric: 19.2399

Epoch 195: val_loss did not improve from 19.19944
196/196 - 66s - loss: 19.1448 - MinusLogProbMetric: 19.1448 - val_loss: 19.2399 - val_MinusLogProbMetric: 19.2399 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 196/1000
2023-09-26 19:07:06.115 
Epoch 196/1000 
	 loss: 19.1433, MinusLogProbMetric: 19.1433, val_loss: 19.7173, val_MinusLogProbMetric: 19.7173

Epoch 196: val_loss did not improve from 19.19944
196/196 - 65s - loss: 19.1433 - MinusLogProbMetric: 19.1433 - val_loss: 19.7173 - val_MinusLogProbMetric: 19.7173 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 197/1000
2023-09-26 19:08:11.147 
Epoch 197/1000 
	 loss: 19.1513, MinusLogProbMetric: 19.1513, val_loss: 19.1498, val_MinusLogProbMetric: 19.1498

Epoch 197: val_loss improved from 19.19944 to 19.14978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 19.1513 - MinusLogProbMetric: 19.1513 - val_loss: 19.1498 - val_MinusLogProbMetric: 19.1498 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 198/1000
2023-09-26 19:09:18.284 
Epoch 198/1000 
	 loss: 19.2108, MinusLogProbMetric: 19.2108, val_loss: 19.1500, val_MinusLogProbMetric: 19.1500

Epoch 198: val_loss did not improve from 19.14978
196/196 - 66s - loss: 19.2108 - MinusLogProbMetric: 19.2108 - val_loss: 19.1500 - val_MinusLogProbMetric: 19.1500 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 199/1000
2023-09-26 19:10:23.626 
Epoch 199/1000 
	 loss: 19.1571, MinusLogProbMetric: 19.1571, val_loss: 19.3589, val_MinusLogProbMetric: 19.3589

Epoch 199: val_loss did not improve from 19.14978
196/196 - 65s - loss: 19.1571 - MinusLogProbMetric: 19.1571 - val_loss: 19.3589 - val_MinusLogProbMetric: 19.3589 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 200/1000
2023-09-26 19:11:27.256 
Epoch 200/1000 
	 loss: 19.1615, MinusLogProbMetric: 19.1615, val_loss: 19.6139, val_MinusLogProbMetric: 19.6139

Epoch 200: val_loss did not improve from 19.14978
196/196 - 64s - loss: 19.1615 - MinusLogProbMetric: 19.1615 - val_loss: 19.6139 - val_MinusLogProbMetric: 19.6139 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 201/1000
2023-09-26 19:12:31.712 
Epoch 201/1000 
	 loss: 19.0199, MinusLogProbMetric: 19.0199, val_loss: 19.3451, val_MinusLogProbMetric: 19.3451

Epoch 201: val_loss did not improve from 19.14978
196/196 - 64s - loss: 19.0199 - MinusLogProbMetric: 19.0199 - val_loss: 19.3451 - val_MinusLogProbMetric: 19.3451 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 202/1000
2023-09-26 19:13:35.751 
Epoch 202/1000 
	 loss: 19.0648, MinusLogProbMetric: 19.0648, val_loss: 19.3353, val_MinusLogProbMetric: 19.3353

Epoch 202: val_loss did not improve from 19.14978
196/196 - 64s - loss: 19.0648 - MinusLogProbMetric: 19.0648 - val_loss: 19.3353 - val_MinusLogProbMetric: 19.3353 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 203/1000
2023-09-26 19:14:41.893 
Epoch 203/1000 
	 loss: 19.1012, MinusLogProbMetric: 19.1012, val_loss: 19.1276, val_MinusLogProbMetric: 19.1276

Epoch 203: val_loss improved from 19.14978 to 19.12764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 19.1012 - MinusLogProbMetric: 19.1012 - val_loss: 19.1276 - val_MinusLogProbMetric: 19.1276 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 204/1000
2023-09-26 19:15:47.562 
Epoch 204/1000 
	 loss: 19.0635, MinusLogProbMetric: 19.0635, val_loss: 19.1909, val_MinusLogProbMetric: 19.1909

Epoch 204: val_loss did not improve from 19.12764
196/196 - 64s - loss: 19.0635 - MinusLogProbMetric: 19.0635 - val_loss: 19.1909 - val_MinusLogProbMetric: 19.1909 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 205/1000
2023-09-26 19:16:53.677 
Epoch 205/1000 
	 loss: 19.0703, MinusLogProbMetric: 19.0703, val_loss: 19.8645, val_MinusLogProbMetric: 19.8645

Epoch 205: val_loss did not improve from 19.12764
196/196 - 66s - loss: 19.0703 - MinusLogProbMetric: 19.0703 - val_loss: 19.8645 - val_MinusLogProbMetric: 19.8645 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 206/1000
2023-09-26 19:17:57.154 
Epoch 206/1000 
	 loss: 19.1164, MinusLogProbMetric: 19.1164, val_loss: 19.3420, val_MinusLogProbMetric: 19.3420

Epoch 206: val_loss did not improve from 19.12764
196/196 - 63s - loss: 19.1164 - MinusLogProbMetric: 19.1164 - val_loss: 19.3420 - val_MinusLogProbMetric: 19.3420 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 207/1000
2023-09-26 19:18:53.751 
Epoch 207/1000 
	 loss: 19.0044, MinusLogProbMetric: 19.0044, val_loss: 19.2529, val_MinusLogProbMetric: 19.2529

Epoch 207: val_loss did not improve from 19.12764
196/196 - 57s - loss: 19.0044 - MinusLogProbMetric: 19.0044 - val_loss: 19.2529 - val_MinusLogProbMetric: 19.2529 - lr: 3.3333e-04 - 57s/epoch - 289ms/step
Epoch 208/1000
2023-09-26 19:19:45.073 
Epoch 208/1000 
	 loss: 19.0073, MinusLogProbMetric: 19.0073, val_loss: 19.3438, val_MinusLogProbMetric: 19.3438

Epoch 208: val_loss did not improve from 19.12764
196/196 - 51s - loss: 19.0073 - MinusLogProbMetric: 19.0073 - val_loss: 19.3438 - val_MinusLogProbMetric: 19.3438 - lr: 3.3333e-04 - 51s/epoch - 262ms/step
Epoch 209/1000
2023-09-26 19:20:42.798 
Epoch 209/1000 
	 loss: 19.0126, MinusLogProbMetric: 19.0126, val_loss: 19.0823, val_MinusLogProbMetric: 19.0823

Epoch 209: val_loss improved from 19.12764 to 19.08231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 59s - loss: 19.0126 - MinusLogProbMetric: 19.0126 - val_loss: 19.0823 - val_MinusLogProbMetric: 19.0823 - lr: 3.3333e-04 - 59s/epoch - 299ms/step
Epoch 210/1000
2023-09-26 19:21:43.971 
Epoch 210/1000 
	 loss: 18.9841, MinusLogProbMetric: 18.9841, val_loss: 19.1818, val_MinusLogProbMetric: 19.1818

Epoch 210: val_loss did not improve from 19.08231
196/196 - 60s - loss: 18.9841 - MinusLogProbMetric: 18.9841 - val_loss: 19.1818 - val_MinusLogProbMetric: 19.1818 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 211/1000
2023-09-26 19:22:37.594 
Epoch 211/1000 
	 loss: 18.9663, MinusLogProbMetric: 18.9663, val_loss: 19.5924, val_MinusLogProbMetric: 19.5924

Epoch 211: val_loss did not improve from 19.08231
196/196 - 54s - loss: 18.9663 - MinusLogProbMetric: 18.9663 - val_loss: 19.5924 - val_MinusLogProbMetric: 19.5924 - lr: 3.3333e-04 - 54s/epoch - 274ms/step
Epoch 212/1000
2023-09-26 19:23:31.389 
Epoch 212/1000 
	 loss: 19.0257, MinusLogProbMetric: 19.0257, val_loss: 19.1334, val_MinusLogProbMetric: 19.1334

Epoch 212: val_loss did not improve from 19.08231
196/196 - 54s - loss: 19.0257 - MinusLogProbMetric: 19.0257 - val_loss: 19.1334 - val_MinusLogProbMetric: 19.1334 - lr: 3.3333e-04 - 54s/epoch - 274ms/step
Epoch 213/1000
2023-09-26 19:24:32.778 
Epoch 213/1000 
	 loss: 19.0054, MinusLogProbMetric: 19.0054, val_loss: 19.0077, val_MinusLogProbMetric: 19.0077

Epoch 213: val_loss improved from 19.08231 to 19.00765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 63s - loss: 19.0054 - MinusLogProbMetric: 19.0054 - val_loss: 19.0077 - val_MinusLogProbMetric: 19.0077 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 214/1000
2023-09-26 19:25:37.505 
Epoch 214/1000 
	 loss: 18.9968, MinusLogProbMetric: 18.9968, val_loss: 19.0925, val_MinusLogProbMetric: 19.0925

Epoch 214: val_loss did not improve from 19.00765
196/196 - 64s - loss: 18.9968 - MinusLogProbMetric: 18.9968 - val_loss: 19.0925 - val_MinusLogProbMetric: 19.0925 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 215/1000
2023-09-26 19:26:42.688 
Epoch 215/1000 
	 loss: 18.9178, MinusLogProbMetric: 18.9178, val_loss: 19.1972, val_MinusLogProbMetric: 19.1972

Epoch 215: val_loss did not improve from 19.00765
196/196 - 65s - loss: 18.9178 - MinusLogProbMetric: 18.9178 - val_loss: 19.1972 - val_MinusLogProbMetric: 19.1972 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 216/1000
2023-09-26 19:27:45.600 
Epoch 216/1000 
	 loss: 18.8776, MinusLogProbMetric: 18.8776, val_loss: 19.0953, val_MinusLogProbMetric: 19.0953

Epoch 216: val_loss did not improve from 19.00765
196/196 - 63s - loss: 18.8776 - MinusLogProbMetric: 18.8776 - val_loss: 19.0953 - val_MinusLogProbMetric: 19.0953 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 217/1000
2023-09-26 19:28:39.834 
Epoch 217/1000 
	 loss: 18.8875, MinusLogProbMetric: 18.8875, val_loss: 19.5147, val_MinusLogProbMetric: 19.5147

Epoch 217: val_loss did not improve from 19.00765
196/196 - 54s - loss: 18.8875 - MinusLogProbMetric: 18.8875 - val_loss: 19.5147 - val_MinusLogProbMetric: 19.5147 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 218/1000
2023-09-26 19:29:32.230 
Epoch 218/1000 
	 loss: 18.9120, MinusLogProbMetric: 18.9120, val_loss: 19.0718, val_MinusLogProbMetric: 19.0718

Epoch 218: val_loss did not improve from 19.00765
196/196 - 52s - loss: 18.9120 - MinusLogProbMetric: 18.9120 - val_loss: 19.0718 - val_MinusLogProbMetric: 19.0718 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 219/1000
2023-09-26 19:30:33.069 
Epoch 219/1000 
	 loss: 18.8938, MinusLogProbMetric: 18.8938, val_loss: 19.2105, val_MinusLogProbMetric: 19.2105

Epoch 219: val_loss did not improve from 19.00765
196/196 - 61s - loss: 18.8938 - MinusLogProbMetric: 18.8938 - val_loss: 19.2105 - val_MinusLogProbMetric: 19.2105 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 220/1000
2023-09-26 19:31:37.044 
Epoch 220/1000 
	 loss: 18.8670, MinusLogProbMetric: 18.8670, val_loss: 19.0315, val_MinusLogProbMetric: 19.0315

Epoch 220: val_loss did not improve from 19.00765
196/196 - 64s - loss: 18.8670 - MinusLogProbMetric: 18.8670 - val_loss: 19.0315 - val_MinusLogProbMetric: 19.0315 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 221/1000
2023-09-26 19:32:42.157 
Epoch 221/1000 
	 loss: 18.8180, MinusLogProbMetric: 18.8180, val_loss: 18.9528, val_MinusLogProbMetric: 18.9528

Epoch 221: val_loss improved from 19.00765 to 18.95277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 18.8180 - MinusLogProbMetric: 18.8180 - val_loss: 18.9528 - val_MinusLogProbMetric: 18.9528 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 222/1000
2023-09-26 19:33:47.544 
Epoch 222/1000 
	 loss: 18.8994, MinusLogProbMetric: 18.8994, val_loss: 19.7966, val_MinusLogProbMetric: 19.7966

Epoch 222: val_loss did not improve from 18.95277
196/196 - 64s - loss: 18.8994 - MinusLogProbMetric: 18.8994 - val_loss: 19.7966 - val_MinusLogProbMetric: 19.7966 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 223/1000
2023-09-26 19:34:51.425 
Epoch 223/1000 
	 loss: 18.9637, MinusLogProbMetric: 18.9637, val_loss: 19.1851, val_MinusLogProbMetric: 19.1851

Epoch 223: val_loss did not improve from 18.95277
196/196 - 64s - loss: 18.9637 - MinusLogProbMetric: 18.9637 - val_loss: 19.1851 - val_MinusLogProbMetric: 19.1851 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 224/1000
2023-09-26 19:35:56.254 
Epoch 224/1000 
	 loss: 18.9451, MinusLogProbMetric: 18.9451, val_loss: 19.2735, val_MinusLogProbMetric: 19.2735

Epoch 224: val_loss did not improve from 18.95277
196/196 - 65s - loss: 18.9451 - MinusLogProbMetric: 18.9451 - val_loss: 19.2735 - val_MinusLogProbMetric: 19.2735 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 225/1000
2023-09-26 19:37:01.308 
Epoch 225/1000 
	 loss: 18.8545, MinusLogProbMetric: 18.8545, val_loss: 19.2269, val_MinusLogProbMetric: 19.2269

Epoch 225: val_loss did not improve from 18.95277
196/196 - 65s - loss: 18.8545 - MinusLogProbMetric: 18.8545 - val_loss: 19.2269 - val_MinusLogProbMetric: 19.2269 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 226/1000
2023-09-26 19:38:05.133 
Epoch 226/1000 
	 loss: 18.7990, MinusLogProbMetric: 18.7990, val_loss: 19.1635, val_MinusLogProbMetric: 19.1635

Epoch 226: val_loss did not improve from 18.95277
196/196 - 64s - loss: 18.7990 - MinusLogProbMetric: 18.7990 - val_loss: 19.1635 - val_MinusLogProbMetric: 19.1635 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 227/1000
2023-09-26 19:39:10.745 
Epoch 227/1000 
	 loss: 18.8389, MinusLogProbMetric: 18.8389, val_loss: 18.9991, val_MinusLogProbMetric: 18.9991

Epoch 227: val_loss did not improve from 18.95277
196/196 - 66s - loss: 18.8389 - MinusLogProbMetric: 18.8389 - val_loss: 18.9991 - val_MinusLogProbMetric: 18.9991 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 228/1000
2023-09-26 19:40:15.834 
Epoch 228/1000 
	 loss: 18.7524, MinusLogProbMetric: 18.7524, val_loss: 18.9812, val_MinusLogProbMetric: 18.9812

Epoch 228: val_loss did not improve from 18.95277
196/196 - 65s - loss: 18.7524 - MinusLogProbMetric: 18.7524 - val_loss: 18.9812 - val_MinusLogProbMetric: 18.9812 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 229/1000
2023-09-26 19:41:19.738 
Epoch 229/1000 
	 loss: 18.8431, MinusLogProbMetric: 18.8431, val_loss: 18.8458, val_MinusLogProbMetric: 18.8458

Epoch 229: val_loss improved from 18.95277 to 18.84582, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.8431 - MinusLogProbMetric: 18.8431 - val_loss: 18.8458 - val_MinusLogProbMetric: 18.8458 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 230/1000
2023-09-26 19:42:24.993 
Epoch 230/1000 
	 loss: 18.8202, MinusLogProbMetric: 18.8202, val_loss: 19.0440, val_MinusLogProbMetric: 19.0440

Epoch 230: val_loss did not improve from 18.84582
196/196 - 64s - loss: 18.8202 - MinusLogProbMetric: 18.8202 - val_loss: 19.0440 - val_MinusLogProbMetric: 19.0440 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 231/1000
2023-09-26 19:43:28.329 
Epoch 231/1000 
	 loss: 18.8123, MinusLogProbMetric: 18.8123, val_loss: 18.9325, val_MinusLogProbMetric: 18.9325

Epoch 231: val_loss did not improve from 18.84582
196/196 - 63s - loss: 18.8123 - MinusLogProbMetric: 18.8123 - val_loss: 18.9325 - val_MinusLogProbMetric: 18.9325 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 232/1000
2023-09-26 19:44:31.599 
Epoch 232/1000 
	 loss: 18.7555, MinusLogProbMetric: 18.7555, val_loss: 19.0466, val_MinusLogProbMetric: 19.0466

Epoch 232: val_loss did not improve from 18.84582
196/196 - 63s - loss: 18.7555 - MinusLogProbMetric: 18.7555 - val_loss: 19.0466 - val_MinusLogProbMetric: 19.0466 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 233/1000
2023-09-26 19:45:35.627 
Epoch 233/1000 
	 loss: 18.8043, MinusLogProbMetric: 18.8043, val_loss: 19.3118, val_MinusLogProbMetric: 19.3118

Epoch 233: val_loss did not improve from 18.84582
196/196 - 64s - loss: 18.8043 - MinusLogProbMetric: 18.8043 - val_loss: 19.3118 - val_MinusLogProbMetric: 19.3118 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 234/1000
2023-09-26 19:46:39.613 
Epoch 234/1000 
	 loss: 18.8035, MinusLogProbMetric: 18.8035, val_loss: 19.0859, val_MinusLogProbMetric: 19.0859

Epoch 234: val_loss did not improve from 18.84582
196/196 - 64s - loss: 18.8035 - MinusLogProbMetric: 18.8035 - val_loss: 19.0859 - val_MinusLogProbMetric: 19.0859 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 235/1000
2023-09-26 19:47:43.057 
Epoch 235/1000 
	 loss: 18.7702, MinusLogProbMetric: 18.7702, val_loss: 18.9933, val_MinusLogProbMetric: 18.9933

Epoch 235: val_loss did not improve from 18.84582
196/196 - 63s - loss: 18.7702 - MinusLogProbMetric: 18.7702 - val_loss: 18.9933 - val_MinusLogProbMetric: 18.9933 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 236/1000
2023-09-26 19:48:46.116 
Epoch 236/1000 
	 loss: 18.7834, MinusLogProbMetric: 18.7834, val_loss: 19.3457, val_MinusLogProbMetric: 19.3457

Epoch 236: val_loss did not improve from 18.84582
196/196 - 63s - loss: 18.7834 - MinusLogProbMetric: 18.7834 - val_loss: 19.3457 - val_MinusLogProbMetric: 19.3457 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 237/1000
2023-09-26 19:49:49.220 
Epoch 237/1000 
	 loss: 18.6677, MinusLogProbMetric: 18.6677, val_loss: 19.0112, val_MinusLogProbMetric: 19.0112

Epoch 237: val_loss did not improve from 18.84582
196/196 - 63s - loss: 18.6677 - MinusLogProbMetric: 18.6677 - val_loss: 19.0112 - val_MinusLogProbMetric: 19.0112 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 238/1000
2023-09-26 19:50:53.376 
Epoch 238/1000 
	 loss: 18.7624, MinusLogProbMetric: 18.7624, val_loss: 18.7101, val_MinusLogProbMetric: 18.7101

Epoch 238: val_loss improved from 18.84582 to 18.71012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.7624 - MinusLogProbMetric: 18.7624 - val_loss: 18.7101 - val_MinusLogProbMetric: 18.7101 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 239/1000
2023-09-26 19:51:58.013 
Epoch 239/1000 
	 loss: 18.7133, MinusLogProbMetric: 18.7133, val_loss: 18.8825, val_MinusLogProbMetric: 18.8825

Epoch 239: val_loss did not improve from 18.71012
196/196 - 64s - loss: 18.7133 - MinusLogProbMetric: 18.7133 - val_loss: 18.8825 - val_MinusLogProbMetric: 18.8825 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 240/1000
2023-09-26 19:53:02.945 
Epoch 240/1000 
	 loss: 18.7344, MinusLogProbMetric: 18.7344, val_loss: 19.0774, val_MinusLogProbMetric: 19.0774

Epoch 240: val_loss did not improve from 18.71012
196/196 - 65s - loss: 18.7344 - MinusLogProbMetric: 18.7344 - val_loss: 19.0774 - val_MinusLogProbMetric: 19.0774 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 241/1000
2023-09-26 19:54:06.662 
Epoch 241/1000 
	 loss: 18.7148, MinusLogProbMetric: 18.7148, val_loss: 19.0854, val_MinusLogProbMetric: 19.0854

Epoch 241: val_loss did not improve from 18.71012
196/196 - 64s - loss: 18.7148 - MinusLogProbMetric: 18.7148 - val_loss: 19.0854 - val_MinusLogProbMetric: 19.0854 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 242/1000
2023-09-26 19:55:10.111 
Epoch 242/1000 
	 loss: 18.7446, MinusLogProbMetric: 18.7446, val_loss: 18.9979, val_MinusLogProbMetric: 18.9979

Epoch 242: val_loss did not improve from 18.71012
196/196 - 63s - loss: 18.7446 - MinusLogProbMetric: 18.7446 - val_loss: 18.9979 - val_MinusLogProbMetric: 18.9979 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 243/1000
2023-09-26 19:56:13.679 
Epoch 243/1000 
	 loss: 18.6647, MinusLogProbMetric: 18.6647, val_loss: 18.9279, val_MinusLogProbMetric: 18.9279

Epoch 243: val_loss did not improve from 18.71012
196/196 - 64s - loss: 18.6647 - MinusLogProbMetric: 18.6647 - val_loss: 18.9279 - val_MinusLogProbMetric: 18.9279 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 244/1000
2023-09-26 19:57:18.559 
Epoch 244/1000 
	 loss: 18.6542, MinusLogProbMetric: 18.6542, val_loss: 19.2110, val_MinusLogProbMetric: 19.2110

Epoch 244: val_loss did not improve from 18.71012
196/196 - 65s - loss: 18.6542 - MinusLogProbMetric: 18.6542 - val_loss: 19.2110 - val_MinusLogProbMetric: 19.2110 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 245/1000
2023-09-26 19:58:23.019 
Epoch 245/1000 
	 loss: 18.6364, MinusLogProbMetric: 18.6364, val_loss: 18.7024, val_MinusLogProbMetric: 18.7024

Epoch 245: val_loss improved from 18.71012 to 18.70241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 18.6364 - MinusLogProbMetric: 18.6364 - val_loss: 18.7024 - val_MinusLogProbMetric: 18.7024 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 246/1000
2023-09-26 19:59:27.206 
Epoch 246/1000 
	 loss: 18.6924, MinusLogProbMetric: 18.6924, val_loss: 19.2283, val_MinusLogProbMetric: 19.2283

Epoch 246: val_loss did not improve from 18.70241
196/196 - 63s - loss: 18.6924 - MinusLogProbMetric: 18.6924 - val_loss: 19.2283 - val_MinusLogProbMetric: 19.2283 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 247/1000
2023-09-26 20:00:30.577 
Epoch 247/1000 
	 loss: 18.6577, MinusLogProbMetric: 18.6577, val_loss: 18.9551, val_MinusLogProbMetric: 18.9551

Epoch 247: val_loss did not improve from 18.70241
196/196 - 63s - loss: 18.6577 - MinusLogProbMetric: 18.6577 - val_loss: 18.9551 - val_MinusLogProbMetric: 18.9551 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 248/1000
2023-09-26 20:01:35.473 
Epoch 248/1000 
	 loss: 18.7316, MinusLogProbMetric: 18.7316, val_loss: 18.6575, val_MinusLogProbMetric: 18.6575

Epoch 248: val_loss improved from 18.70241 to 18.65747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 18.7316 - MinusLogProbMetric: 18.7316 - val_loss: 18.6575 - val_MinusLogProbMetric: 18.6575 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 249/1000
2023-09-26 20:02:42.079 
Epoch 249/1000 
	 loss: 18.6104, MinusLogProbMetric: 18.6104, val_loss: 18.7646, val_MinusLogProbMetric: 18.7646

Epoch 249: val_loss did not improve from 18.65747
196/196 - 66s - loss: 18.6104 - MinusLogProbMetric: 18.6104 - val_loss: 18.7646 - val_MinusLogProbMetric: 18.7646 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 250/1000
2023-09-26 20:03:45.921 
Epoch 250/1000 
	 loss: 18.6388, MinusLogProbMetric: 18.6388, val_loss: 18.5486, val_MinusLogProbMetric: 18.5486

Epoch 250: val_loss improved from 18.65747 to 18.54861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.6388 - MinusLogProbMetric: 18.6388 - val_loss: 18.5486 - val_MinusLogProbMetric: 18.5486 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 251/1000
2023-09-26 20:04:51.645 
Epoch 251/1000 
	 loss: 18.6193, MinusLogProbMetric: 18.6193, val_loss: 18.7001, val_MinusLogProbMetric: 18.7001

Epoch 251: val_loss did not improve from 18.54861
196/196 - 65s - loss: 18.6193 - MinusLogProbMetric: 18.6193 - val_loss: 18.7001 - val_MinusLogProbMetric: 18.7001 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 252/1000
2023-09-26 20:05:55.464 
Epoch 252/1000 
	 loss: 18.6314, MinusLogProbMetric: 18.6314, val_loss: 19.2244, val_MinusLogProbMetric: 19.2244

Epoch 252: val_loss did not improve from 18.54861
196/196 - 64s - loss: 18.6314 - MinusLogProbMetric: 18.6314 - val_loss: 19.2244 - val_MinusLogProbMetric: 19.2244 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 253/1000
2023-09-26 20:06:59.205 
Epoch 253/1000 
	 loss: 18.5990, MinusLogProbMetric: 18.5990, val_loss: 18.8392, val_MinusLogProbMetric: 18.8392

Epoch 253: val_loss did not improve from 18.54861
196/196 - 64s - loss: 18.5990 - MinusLogProbMetric: 18.5990 - val_loss: 18.8392 - val_MinusLogProbMetric: 18.8392 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 254/1000
2023-09-26 20:08:01.534 
Epoch 254/1000 
	 loss: 19.2086, MinusLogProbMetric: 19.2086, val_loss: 19.3600, val_MinusLogProbMetric: 19.3600

Epoch 254: val_loss did not improve from 18.54861
196/196 - 62s - loss: 19.2086 - MinusLogProbMetric: 19.2086 - val_loss: 19.3600 - val_MinusLogProbMetric: 19.3600 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 255/1000
2023-09-26 20:09:06.170 
Epoch 255/1000 
	 loss: 18.8022, MinusLogProbMetric: 18.8022, val_loss: 18.9337, val_MinusLogProbMetric: 18.9337

Epoch 255: val_loss did not improve from 18.54861
196/196 - 65s - loss: 18.8022 - MinusLogProbMetric: 18.8022 - val_loss: 18.9337 - val_MinusLogProbMetric: 18.9337 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 256/1000
2023-09-26 20:10:10.626 
Epoch 256/1000 
	 loss: 18.6727, MinusLogProbMetric: 18.6727, val_loss: 18.8264, val_MinusLogProbMetric: 18.8264

Epoch 256: val_loss did not improve from 18.54861
196/196 - 64s - loss: 18.6727 - MinusLogProbMetric: 18.6727 - val_loss: 18.8264 - val_MinusLogProbMetric: 18.8264 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 257/1000
2023-09-26 20:11:14.553 
Epoch 257/1000 
	 loss: 18.6726, MinusLogProbMetric: 18.6726, val_loss: 18.7986, val_MinusLogProbMetric: 18.7986

Epoch 257: val_loss did not improve from 18.54861
196/196 - 64s - loss: 18.6726 - MinusLogProbMetric: 18.6726 - val_loss: 18.7986 - val_MinusLogProbMetric: 18.7986 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 258/1000
2023-09-26 20:12:17.015 
Epoch 258/1000 
	 loss: 18.6773, MinusLogProbMetric: 18.6773, val_loss: 18.9830, val_MinusLogProbMetric: 18.9830

Epoch 258: val_loss did not improve from 18.54861
196/196 - 62s - loss: 18.6773 - MinusLogProbMetric: 18.6773 - val_loss: 18.9830 - val_MinusLogProbMetric: 18.9830 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 259/1000
2023-09-26 20:13:21.336 
Epoch 259/1000 
	 loss: 18.6788, MinusLogProbMetric: 18.6788, val_loss: 19.0547, val_MinusLogProbMetric: 19.0547

Epoch 259: val_loss did not improve from 18.54861
196/196 - 64s - loss: 18.6788 - MinusLogProbMetric: 18.6788 - val_loss: 19.0547 - val_MinusLogProbMetric: 19.0547 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 260/1000
2023-09-26 20:14:24.734 
Epoch 260/1000 
	 loss: 18.6143, MinusLogProbMetric: 18.6143, val_loss: 20.0037, val_MinusLogProbMetric: 20.0037

Epoch 260: val_loss did not improve from 18.54861
196/196 - 63s - loss: 18.6143 - MinusLogProbMetric: 18.6143 - val_loss: 20.0037 - val_MinusLogProbMetric: 20.0037 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 261/1000
2023-09-26 20:15:29.157 
Epoch 261/1000 
	 loss: 18.6703, MinusLogProbMetric: 18.6703, val_loss: 18.9839, val_MinusLogProbMetric: 18.9839

Epoch 261: val_loss did not improve from 18.54861
196/196 - 64s - loss: 18.6703 - MinusLogProbMetric: 18.6703 - val_loss: 18.9839 - val_MinusLogProbMetric: 18.9839 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 262/1000
2023-09-26 20:16:32.151 
Epoch 262/1000 
	 loss: 18.6081, MinusLogProbMetric: 18.6081, val_loss: 18.5274, val_MinusLogProbMetric: 18.5274

Epoch 262: val_loss improved from 18.54861 to 18.52738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 64s - loss: 18.6081 - MinusLogProbMetric: 18.6081 - val_loss: 18.5274 - val_MinusLogProbMetric: 18.5274 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 263/1000
2023-09-26 20:17:37.986 
Epoch 263/1000 
	 loss: 18.6115, MinusLogProbMetric: 18.6115, val_loss: 18.6399, val_MinusLogProbMetric: 18.6399

Epoch 263: val_loss did not improve from 18.52738
196/196 - 65s - loss: 18.6115 - MinusLogProbMetric: 18.6115 - val_loss: 18.6399 - val_MinusLogProbMetric: 18.6399 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 264/1000
2023-09-26 20:18:41.373 
Epoch 264/1000 
	 loss: 18.6383, MinusLogProbMetric: 18.6383, val_loss: 18.9377, val_MinusLogProbMetric: 18.9377

Epoch 264: val_loss did not improve from 18.52738
196/196 - 63s - loss: 18.6383 - MinusLogProbMetric: 18.6383 - val_loss: 18.9377 - val_MinusLogProbMetric: 18.9377 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 265/1000
2023-09-26 20:19:45.155 
Epoch 265/1000 
	 loss: 18.5538, MinusLogProbMetric: 18.5538, val_loss: 18.8084, val_MinusLogProbMetric: 18.8084

Epoch 265: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.5538 - MinusLogProbMetric: 18.5538 - val_loss: 18.8084 - val_MinusLogProbMetric: 18.8084 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 266/1000
2023-09-26 20:20:49.214 
Epoch 266/1000 
	 loss: 18.5427, MinusLogProbMetric: 18.5427, val_loss: 18.9334, val_MinusLogProbMetric: 18.9334

Epoch 266: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.5427 - MinusLogProbMetric: 18.5427 - val_loss: 18.9334 - val_MinusLogProbMetric: 18.9334 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 267/1000
2023-09-26 20:21:53.360 
Epoch 267/1000 
	 loss: 18.5708, MinusLogProbMetric: 18.5708, val_loss: 18.8229, val_MinusLogProbMetric: 18.8229

Epoch 267: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.5708 - MinusLogProbMetric: 18.5708 - val_loss: 18.8229 - val_MinusLogProbMetric: 18.8229 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 268/1000
2023-09-26 20:22:57.114 
Epoch 268/1000 
	 loss: 18.5602, MinusLogProbMetric: 18.5602, val_loss: 18.7002, val_MinusLogProbMetric: 18.7002

Epoch 268: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.5602 - MinusLogProbMetric: 18.5602 - val_loss: 18.7002 - val_MinusLogProbMetric: 18.7002 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 269/1000
2023-09-26 20:24:01.597 
Epoch 269/1000 
	 loss: 18.5933, MinusLogProbMetric: 18.5933, val_loss: 19.1951, val_MinusLogProbMetric: 19.1951

Epoch 269: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.5933 - MinusLogProbMetric: 18.5933 - val_loss: 19.1951 - val_MinusLogProbMetric: 19.1951 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 270/1000
2023-09-26 20:25:06.138 
Epoch 270/1000 
	 loss: 18.5332, MinusLogProbMetric: 18.5332, val_loss: 18.7460, val_MinusLogProbMetric: 18.7460

Epoch 270: val_loss did not improve from 18.52738
196/196 - 65s - loss: 18.5332 - MinusLogProbMetric: 18.5332 - val_loss: 18.7460 - val_MinusLogProbMetric: 18.7460 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 271/1000
2023-09-26 20:26:09.672 
Epoch 271/1000 
	 loss: 18.5454, MinusLogProbMetric: 18.5454, val_loss: 19.0095, val_MinusLogProbMetric: 19.0095

Epoch 271: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.5454 - MinusLogProbMetric: 18.5454 - val_loss: 19.0095 - val_MinusLogProbMetric: 19.0095 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 272/1000
2023-09-26 20:27:13.122 
Epoch 272/1000 
	 loss: 18.5496, MinusLogProbMetric: 18.5496, val_loss: 19.0180, val_MinusLogProbMetric: 19.0180

Epoch 272: val_loss did not improve from 18.52738
196/196 - 63s - loss: 18.5496 - MinusLogProbMetric: 18.5496 - val_loss: 19.0180 - val_MinusLogProbMetric: 19.0180 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 273/1000
2023-09-26 20:28:17.543 
Epoch 273/1000 
	 loss: 18.7353, MinusLogProbMetric: 18.7353, val_loss: 18.9153, val_MinusLogProbMetric: 18.9153

Epoch 273: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.7353 - MinusLogProbMetric: 18.7353 - val_loss: 18.9153 - val_MinusLogProbMetric: 18.9153 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 274/1000
2023-09-26 20:29:21.373 
Epoch 274/1000 
	 loss: 18.4376, MinusLogProbMetric: 18.4376, val_loss: 18.8546, val_MinusLogProbMetric: 18.8546

Epoch 274: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.4376 - MinusLogProbMetric: 18.4376 - val_loss: 18.8546 - val_MinusLogProbMetric: 18.8546 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 275/1000
2023-09-26 20:30:25.826 
Epoch 275/1000 
	 loss: 18.5002, MinusLogProbMetric: 18.5002, val_loss: 18.8072, val_MinusLogProbMetric: 18.8072

Epoch 275: val_loss did not improve from 18.52738
196/196 - 64s - loss: 18.5002 - MinusLogProbMetric: 18.5002 - val_loss: 18.8072 - val_MinusLogProbMetric: 18.8072 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 276/1000
2023-09-26 20:31:30.035 
Epoch 276/1000 
	 loss: 18.4914, MinusLogProbMetric: 18.4914, val_loss: 18.4090, val_MinusLogProbMetric: 18.4090

Epoch 276: val_loss improved from 18.52738 to 18.40899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.4914 - MinusLogProbMetric: 18.4914 - val_loss: 18.4090 - val_MinusLogProbMetric: 18.4090 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 277/1000
2023-09-26 20:32:35.026 
Epoch 277/1000 
	 loss: 18.4542, MinusLogProbMetric: 18.4542, val_loss: 18.6136, val_MinusLogProbMetric: 18.6136

Epoch 277: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.4542 - MinusLogProbMetric: 18.4542 - val_loss: 18.6136 - val_MinusLogProbMetric: 18.6136 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 278/1000
2023-09-26 20:33:38.936 
Epoch 278/1000 
	 loss: 18.4659, MinusLogProbMetric: 18.4659, val_loss: 18.6632, val_MinusLogProbMetric: 18.6632

Epoch 278: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.4659 - MinusLogProbMetric: 18.4659 - val_loss: 18.6632 - val_MinusLogProbMetric: 18.6632 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 279/1000
2023-09-26 20:34:43.252 
Epoch 279/1000 
	 loss: 18.5074, MinusLogProbMetric: 18.5074, val_loss: 18.4715, val_MinusLogProbMetric: 18.4715

Epoch 279: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.5074 - MinusLogProbMetric: 18.5074 - val_loss: 18.4715 - val_MinusLogProbMetric: 18.4715 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 280/1000
2023-09-26 20:35:46.359 
Epoch 280/1000 
	 loss: 18.4579, MinusLogProbMetric: 18.4579, val_loss: 18.8203, val_MinusLogProbMetric: 18.8203

Epoch 280: val_loss did not improve from 18.40899
196/196 - 63s - loss: 18.4579 - MinusLogProbMetric: 18.4579 - val_loss: 18.8203 - val_MinusLogProbMetric: 18.8203 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 281/1000
2023-09-26 20:36:51.130 
Epoch 281/1000 
	 loss: 18.4263, MinusLogProbMetric: 18.4263, val_loss: 18.5673, val_MinusLogProbMetric: 18.5673

Epoch 281: val_loss did not improve from 18.40899
196/196 - 65s - loss: 18.4263 - MinusLogProbMetric: 18.4263 - val_loss: 18.5673 - val_MinusLogProbMetric: 18.5673 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 282/1000
2023-09-26 20:37:55.776 
Epoch 282/1000 
	 loss: 18.4524, MinusLogProbMetric: 18.4524, val_loss: 18.8336, val_MinusLogProbMetric: 18.8336

Epoch 282: val_loss did not improve from 18.40899
196/196 - 65s - loss: 18.4524 - MinusLogProbMetric: 18.4524 - val_loss: 18.8336 - val_MinusLogProbMetric: 18.8336 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 283/1000
2023-09-26 20:38:58.813 
Epoch 283/1000 
	 loss: 18.4724, MinusLogProbMetric: 18.4724, val_loss: 18.9502, val_MinusLogProbMetric: 18.9502

Epoch 283: val_loss did not improve from 18.40899
196/196 - 63s - loss: 18.4724 - MinusLogProbMetric: 18.4724 - val_loss: 18.9502 - val_MinusLogProbMetric: 18.9502 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 284/1000
2023-09-26 20:40:02.766 
Epoch 284/1000 
	 loss: 18.4926, MinusLogProbMetric: 18.4926, val_loss: 18.7422, val_MinusLogProbMetric: 18.7422

Epoch 284: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.4926 - MinusLogProbMetric: 18.4926 - val_loss: 18.7422 - val_MinusLogProbMetric: 18.7422 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 285/1000
2023-09-26 20:41:06.867 
Epoch 285/1000 
	 loss: 18.4056, MinusLogProbMetric: 18.4056, val_loss: 18.5924, val_MinusLogProbMetric: 18.5924

Epoch 285: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.4056 - MinusLogProbMetric: 18.4056 - val_loss: 18.5924 - val_MinusLogProbMetric: 18.5924 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 286/1000
2023-09-26 20:42:12.311 
Epoch 286/1000 
	 loss: 18.4508, MinusLogProbMetric: 18.4508, val_loss: 18.7262, val_MinusLogProbMetric: 18.7262

Epoch 286: val_loss did not improve from 18.40899
196/196 - 65s - loss: 18.4508 - MinusLogProbMetric: 18.4508 - val_loss: 18.7262 - val_MinusLogProbMetric: 18.7262 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 287/1000
2023-09-26 20:43:17.274 
Epoch 287/1000 
	 loss: 18.4262, MinusLogProbMetric: 18.4262, val_loss: 18.6555, val_MinusLogProbMetric: 18.6555

Epoch 287: val_loss did not improve from 18.40899
196/196 - 65s - loss: 18.4262 - MinusLogProbMetric: 18.4262 - val_loss: 18.6555 - val_MinusLogProbMetric: 18.6555 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 288/1000
2023-09-26 20:44:21.420 
Epoch 288/1000 
	 loss: 18.3769, MinusLogProbMetric: 18.3769, val_loss: 18.6021, val_MinusLogProbMetric: 18.6021

Epoch 288: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.3769 - MinusLogProbMetric: 18.3769 - val_loss: 18.6021 - val_MinusLogProbMetric: 18.6021 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 289/1000
2023-09-26 20:45:25.609 
Epoch 289/1000 
	 loss: 18.3932, MinusLogProbMetric: 18.3932, val_loss: 18.6203, val_MinusLogProbMetric: 18.6203

Epoch 289: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.3932 - MinusLogProbMetric: 18.3932 - val_loss: 18.6203 - val_MinusLogProbMetric: 18.6203 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 290/1000
2023-09-26 20:46:29.335 
Epoch 290/1000 
	 loss: 18.4356, MinusLogProbMetric: 18.4356, val_loss: 18.9334, val_MinusLogProbMetric: 18.9334

Epoch 290: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.4356 - MinusLogProbMetric: 18.4356 - val_loss: 18.9334 - val_MinusLogProbMetric: 18.9334 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 291/1000
2023-09-26 20:47:32.188 
Epoch 291/1000 
	 loss: 18.4280, MinusLogProbMetric: 18.4280, val_loss: 19.4075, val_MinusLogProbMetric: 19.4075

Epoch 291: val_loss did not improve from 18.40899
196/196 - 63s - loss: 18.4280 - MinusLogProbMetric: 18.4280 - val_loss: 19.4075 - val_MinusLogProbMetric: 19.4075 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 292/1000
2023-09-26 20:48:36.183 
Epoch 292/1000 
	 loss: 18.4627, MinusLogProbMetric: 18.4627, val_loss: 18.8256, val_MinusLogProbMetric: 18.8256

Epoch 292: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.4627 - MinusLogProbMetric: 18.4627 - val_loss: 18.8256 - val_MinusLogProbMetric: 18.8256 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 293/1000
2023-09-26 20:49:38.751 
Epoch 293/1000 
	 loss: 18.3502, MinusLogProbMetric: 18.3502, val_loss: 18.5865, val_MinusLogProbMetric: 18.5865

Epoch 293: val_loss did not improve from 18.40899
196/196 - 63s - loss: 18.3502 - MinusLogProbMetric: 18.3502 - val_loss: 18.5865 - val_MinusLogProbMetric: 18.5865 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 294/1000
2023-09-26 20:50:42.703 
Epoch 294/1000 
	 loss: 18.3522, MinusLogProbMetric: 18.3522, val_loss: 18.4192, val_MinusLogProbMetric: 18.4192

Epoch 294: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.3522 - MinusLogProbMetric: 18.3522 - val_loss: 18.4192 - val_MinusLogProbMetric: 18.4192 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 295/1000
2023-09-26 20:51:46.145 
Epoch 295/1000 
	 loss: 18.3872, MinusLogProbMetric: 18.3872, val_loss: 18.6087, val_MinusLogProbMetric: 18.6087

Epoch 295: val_loss did not improve from 18.40899
196/196 - 63s - loss: 18.3872 - MinusLogProbMetric: 18.3872 - val_loss: 18.6087 - val_MinusLogProbMetric: 18.6087 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 296/1000
2023-09-26 20:52:49.734 
Epoch 296/1000 
	 loss: 18.3628, MinusLogProbMetric: 18.3628, val_loss: 18.5876, val_MinusLogProbMetric: 18.5876

Epoch 296: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.3628 - MinusLogProbMetric: 18.3628 - val_loss: 18.5876 - val_MinusLogProbMetric: 18.5876 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 297/1000
2023-09-26 20:53:54.003 
Epoch 297/1000 
	 loss: 18.3328, MinusLogProbMetric: 18.3328, val_loss: 18.9698, val_MinusLogProbMetric: 18.9698

Epoch 297: val_loss did not improve from 18.40899
196/196 - 64s - loss: 18.3328 - MinusLogProbMetric: 18.3328 - val_loss: 18.9698 - val_MinusLogProbMetric: 18.9698 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 298/1000
2023-09-26 20:54:58.355 
Epoch 298/1000 
	 loss: 18.3847, MinusLogProbMetric: 18.3847, val_loss: 18.3122, val_MinusLogProbMetric: 18.3122

Epoch 298: val_loss improved from 18.40899 to 18.31224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.3847 - MinusLogProbMetric: 18.3847 - val_loss: 18.3122 - val_MinusLogProbMetric: 18.3122 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 299/1000
2023-09-26 20:56:03.790 
Epoch 299/1000 
	 loss: 18.3912, MinusLogProbMetric: 18.3912, val_loss: 18.6263, val_MinusLogProbMetric: 18.6263

Epoch 299: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.3912 - MinusLogProbMetric: 18.3912 - val_loss: 18.6263 - val_MinusLogProbMetric: 18.6263 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 300/1000
2023-09-26 20:57:08.074 
Epoch 300/1000 
	 loss: 18.6564, MinusLogProbMetric: 18.6564, val_loss: 19.4695, val_MinusLogProbMetric: 19.4695

Epoch 300: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.6564 - MinusLogProbMetric: 18.6564 - val_loss: 19.4695 - val_MinusLogProbMetric: 19.4695 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 301/1000
2023-09-26 20:58:11.507 
Epoch 301/1000 
	 loss: 18.4149, MinusLogProbMetric: 18.4149, val_loss: 18.6384, val_MinusLogProbMetric: 18.6384

Epoch 301: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.4149 - MinusLogProbMetric: 18.4149 - val_loss: 18.6384 - val_MinusLogProbMetric: 18.6384 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 302/1000
2023-09-26 20:59:14.800 
Epoch 302/1000 
	 loss: 18.3844, MinusLogProbMetric: 18.3844, val_loss: 18.5247, val_MinusLogProbMetric: 18.5247

Epoch 302: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.3844 - MinusLogProbMetric: 18.3844 - val_loss: 18.5247 - val_MinusLogProbMetric: 18.5247 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 303/1000
2023-09-26 21:00:17.337 
Epoch 303/1000 
	 loss: 18.3964, MinusLogProbMetric: 18.3964, val_loss: 18.7358, val_MinusLogProbMetric: 18.7358

Epoch 303: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.3964 - MinusLogProbMetric: 18.3964 - val_loss: 18.7358 - val_MinusLogProbMetric: 18.7358 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 304/1000
2023-09-26 21:01:20.882 
Epoch 304/1000 
	 loss: 18.3470, MinusLogProbMetric: 18.3470, val_loss: 18.4883, val_MinusLogProbMetric: 18.4883

Epoch 304: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.3470 - MinusLogProbMetric: 18.3470 - val_loss: 18.4883 - val_MinusLogProbMetric: 18.4883 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 305/1000
2023-09-26 21:02:24.079 
Epoch 305/1000 
	 loss: 18.3331, MinusLogProbMetric: 18.3331, val_loss: 18.3986, val_MinusLogProbMetric: 18.3986

Epoch 305: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.3331 - MinusLogProbMetric: 18.3331 - val_loss: 18.3986 - val_MinusLogProbMetric: 18.3986 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 306/1000
2023-09-26 21:03:27.666 
Epoch 306/1000 
	 loss: 18.3769, MinusLogProbMetric: 18.3769, val_loss: 18.6924, val_MinusLogProbMetric: 18.6924

Epoch 306: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.3769 - MinusLogProbMetric: 18.3769 - val_loss: 18.6924 - val_MinusLogProbMetric: 18.6924 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 307/1000
2023-09-26 21:04:30.861 
Epoch 307/1000 
	 loss: 18.3579, MinusLogProbMetric: 18.3579, val_loss: 18.3407, val_MinusLogProbMetric: 18.3407

Epoch 307: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.3579 - MinusLogProbMetric: 18.3579 - val_loss: 18.3407 - val_MinusLogProbMetric: 18.3407 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 308/1000
2023-09-26 21:05:35.388 
Epoch 308/1000 
	 loss: 18.3211, MinusLogProbMetric: 18.3211, val_loss: 18.4975, val_MinusLogProbMetric: 18.4975

Epoch 308: val_loss did not improve from 18.31224
196/196 - 65s - loss: 18.3211 - MinusLogProbMetric: 18.3211 - val_loss: 18.4975 - val_MinusLogProbMetric: 18.4975 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 309/1000
2023-09-26 21:06:39.536 
Epoch 309/1000 
	 loss: 18.2778, MinusLogProbMetric: 18.2778, val_loss: 18.5147, val_MinusLogProbMetric: 18.5147

Epoch 309: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.2778 - MinusLogProbMetric: 18.2778 - val_loss: 18.5147 - val_MinusLogProbMetric: 18.5147 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 310/1000
2023-09-26 21:07:43.374 
Epoch 310/1000 
	 loss: 18.3579, MinusLogProbMetric: 18.3579, val_loss: 18.7677, val_MinusLogProbMetric: 18.7677

Epoch 310: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.3579 - MinusLogProbMetric: 18.3579 - val_loss: 18.7677 - val_MinusLogProbMetric: 18.7677 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 311/1000
2023-09-26 21:08:46.541 
Epoch 311/1000 
	 loss: 18.2825, MinusLogProbMetric: 18.2825, val_loss: 18.5057, val_MinusLogProbMetric: 18.5057

Epoch 311: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.2825 - MinusLogProbMetric: 18.2825 - val_loss: 18.5057 - val_MinusLogProbMetric: 18.5057 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 312/1000
2023-09-26 21:09:50.321 
Epoch 312/1000 
	 loss: 18.2943, MinusLogProbMetric: 18.2943, val_loss: 18.3985, val_MinusLogProbMetric: 18.3985

Epoch 312: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.2943 - MinusLogProbMetric: 18.2943 - val_loss: 18.3985 - val_MinusLogProbMetric: 18.3985 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 313/1000
2023-09-26 21:10:54.358 
Epoch 313/1000 
	 loss: 18.2854, MinusLogProbMetric: 18.2854, val_loss: 18.5133, val_MinusLogProbMetric: 18.5133

Epoch 313: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.2854 - MinusLogProbMetric: 18.2854 - val_loss: 18.5133 - val_MinusLogProbMetric: 18.5133 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 314/1000
2023-09-26 21:11:58.177 
Epoch 314/1000 
	 loss: 18.3455, MinusLogProbMetric: 18.3455, val_loss: 18.5686, val_MinusLogProbMetric: 18.5686

Epoch 314: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.3455 - MinusLogProbMetric: 18.3455 - val_loss: 18.5686 - val_MinusLogProbMetric: 18.5686 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 315/1000
2023-09-26 21:13:01.777 
Epoch 315/1000 
	 loss: 18.3536, MinusLogProbMetric: 18.3536, val_loss: 18.4582, val_MinusLogProbMetric: 18.4582

Epoch 315: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.3536 - MinusLogProbMetric: 18.3536 - val_loss: 18.4582 - val_MinusLogProbMetric: 18.4582 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 316/1000
2023-09-26 21:14:04.704 
Epoch 316/1000 
	 loss: 18.2603, MinusLogProbMetric: 18.2603, val_loss: 18.9310, val_MinusLogProbMetric: 18.9310

Epoch 316: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.2603 - MinusLogProbMetric: 18.2603 - val_loss: 18.9310 - val_MinusLogProbMetric: 18.9310 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 317/1000
2023-09-26 21:15:08.334 
Epoch 317/1000 
	 loss: 18.2941, MinusLogProbMetric: 18.2941, val_loss: 18.6908, val_MinusLogProbMetric: 18.6908

Epoch 317: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.2941 - MinusLogProbMetric: 18.2941 - val_loss: 18.6908 - val_MinusLogProbMetric: 18.6908 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 318/1000
2023-09-26 21:16:12.954 
Epoch 318/1000 
	 loss: 18.2922, MinusLogProbMetric: 18.2922, val_loss: 18.4519, val_MinusLogProbMetric: 18.4519

Epoch 318: val_loss did not improve from 18.31224
196/196 - 65s - loss: 18.2922 - MinusLogProbMetric: 18.2922 - val_loss: 18.4519 - val_MinusLogProbMetric: 18.4519 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 319/1000
2023-09-26 21:17:17.592 
Epoch 319/1000 
	 loss: 19.2428, MinusLogProbMetric: 19.2428, val_loss: 18.9161, val_MinusLogProbMetric: 18.9161

Epoch 319: val_loss did not improve from 18.31224
196/196 - 65s - loss: 19.2428 - MinusLogProbMetric: 19.2428 - val_loss: 18.9161 - val_MinusLogProbMetric: 18.9161 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 320/1000
2023-09-26 21:18:20.770 
Epoch 320/1000 
	 loss: 18.4350, MinusLogProbMetric: 18.4350, val_loss: 18.7630, val_MinusLogProbMetric: 18.7630

Epoch 320: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.4350 - MinusLogProbMetric: 18.4350 - val_loss: 18.7630 - val_MinusLogProbMetric: 18.7630 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 321/1000
2023-09-26 21:19:24.985 
Epoch 321/1000 
	 loss: 18.3257, MinusLogProbMetric: 18.3257, val_loss: 18.4179, val_MinusLogProbMetric: 18.4179

Epoch 321: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.3257 - MinusLogProbMetric: 18.3257 - val_loss: 18.4179 - val_MinusLogProbMetric: 18.4179 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 322/1000
2023-09-26 21:20:28.037 
Epoch 322/1000 
	 loss: 18.3376, MinusLogProbMetric: 18.3376, val_loss: 18.6874, val_MinusLogProbMetric: 18.6874

Epoch 322: val_loss did not improve from 18.31224
196/196 - 63s - loss: 18.3376 - MinusLogProbMetric: 18.3376 - val_loss: 18.6874 - val_MinusLogProbMetric: 18.6874 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 323/1000
2023-09-26 21:21:33.071 
Epoch 323/1000 
	 loss: 18.3754, MinusLogProbMetric: 18.3754, val_loss: 18.5229, val_MinusLogProbMetric: 18.5229

Epoch 323: val_loss did not improve from 18.31224
196/196 - 65s - loss: 18.3754 - MinusLogProbMetric: 18.3754 - val_loss: 18.5229 - val_MinusLogProbMetric: 18.5229 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 324/1000
2023-09-26 21:22:36.926 
Epoch 324/1000 
	 loss: 18.3456, MinusLogProbMetric: 18.3456, val_loss: 18.5906, val_MinusLogProbMetric: 18.5906

Epoch 324: val_loss did not improve from 18.31224
196/196 - 64s - loss: 18.3456 - MinusLogProbMetric: 18.3456 - val_loss: 18.5906 - val_MinusLogProbMetric: 18.5906 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 325/1000
2023-09-26 21:23:40.965 
Epoch 325/1000 
	 loss: 18.2723, MinusLogProbMetric: 18.2723, val_loss: 18.2278, val_MinusLogProbMetric: 18.2278

Epoch 325: val_loss improved from 18.31224 to 18.22778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.2723 - MinusLogProbMetric: 18.2723 - val_loss: 18.2278 - val_MinusLogProbMetric: 18.2278 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 326/1000
2023-09-26 21:24:45.225 
Epoch 326/1000 
	 loss: 18.2768, MinusLogProbMetric: 18.2768, val_loss: 18.3596, val_MinusLogProbMetric: 18.3596

Epoch 326: val_loss did not improve from 18.22778
196/196 - 63s - loss: 18.2768 - MinusLogProbMetric: 18.2768 - val_loss: 18.3596 - val_MinusLogProbMetric: 18.3596 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 327/1000
2023-09-26 21:25:50.640 
Epoch 327/1000 
	 loss: 18.2852, MinusLogProbMetric: 18.2852, val_loss: 18.6271, val_MinusLogProbMetric: 18.6271

Epoch 327: val_loss did not improve from 18.22778
196/196 - 65s - loss: 18.2852 - MinusLogProbMetric: 18.2852 - val_loss: 18.6271 - val_MinusLogProbMetric: 18.6271 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 328/1000
2023-09-26 21:26:55.389 
Epoch 328/1000 
	 loss: 18.3220, MinusLogProbMetric: 18.3220, val_loss: 18.5008, val_MinusLogProbMetric: 18.5008

Epoch 328: val_loss did not improve from 18.22778
196/196 - 65s - loss: 18.3220 - MinusLogProbMetric: 18.3220 - val_loss: 18.5008 - val_MinusLogProbMetric: 18.5008 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 329/1000
2023-09-26 21:27:59.692 
Epoch 329/1000 
	 loss: 18.2766, MinusLogProbMetric: 18.2766, val_loss: 18.5374, val_MinusLogProbMetric: 18.5374

Epoch 329: val_loss did not improve from 18.22778
196/196 - 64s - loss: 18.2766 - MinusLogProbMetric: 18.2766 - val_loss: 18.5374 - val_MinusLogProbMetric: 18.5374 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 330/1000
2023-09-26 21:29:03.001 
Epoch 330/1000 
	 loss: 18.2877, MinusLogProbMetric: 18.2877, val_loss: 18.5916, val_MinusLogProbMetric: 18.5916

Epoch 330: val_loss did not improve from 18.22778
196/196 - 63s - loss: 18.2877 - MinusLogProbMetric: 18.2877 - val_loss: 18.5916 - val_MinusLogProbMetric: 18.5916 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 331/1000
2023-09-26 21:30:07.167 
Epoch 331/1000 
	 loss: 18.2915, MinusLogProbMetric: 18.2915, val_loss: 18.2359, val_MinusLogProbMetric: 18.2359

Epoch 331: val_loss did not improve from 18.22778
196/196 - 64s - loss: 18.2915 - MinusLogProbMetric: 18.2915 - val_loss: 18.2359 - val_MinusLogProbMetric: 18.2359 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 332/1000
2023-09-26 21:31:12.697 
Epoch 332/1000 
	 loss: 18.2518, MinusLogProbMetric: 18.2518, val_loss: 18.4581, val_MinusLogProbMetric: 18.4581

Epoch 332: val_loss did not improve from 18.22778
196/196 - 66s - loss: 18.2518 - MinusLogProbMetric: 18.2518 - val_loss: 18.4581 - val_MinusLogProbMetric: 18.4581 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 333/1000
2023-09-26 21:32:16.088 
Epoch 333/1000 
	 loss: 18.2789, MinusLogProbMetric: 18.2789, val_loss: 18.7221, val_MinusLogProbMetric: 18.7221

Epoch 333: val_loss did not improve from 18.22778
196/196 - 63s - loss: 18.2789 - MinusLogProbMetric: 18.2789 - val_loss: 18.7221 - val_MinusLogProbMetric: 18.7221 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 334/1000
2023-09-26 21:33:19.219 
Epoch 334/1000 
	 loss: 18.2796, MinusLogProbMetric: 18.2796, val_loss: 18.6213, val_MinusLogProbMetric: 18.6213

Epoch 334: val_loss did not improve from 18.22778
196/196 - 63s - loss: 18.2796 - MinusLogProbMetric: 18.2796 - val_loss: 18.6213 - val_MinusLogProbMetric: 18.6213 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 335/1000
2023-09-26 21:34:22.581 
Epoch 335/1000 
	 loss: 18.1851, MinusLogProbMetric: 18.1851, val_loss: 18.2635, val_MinusLogProbMetric: 18.2635

Epoch 335: val_loss did not improve from 18.22778
196/196 - 63s - loss: 18.1851 - MinusLogProbMetric: 18.1851 - val_loss: 18.2635 - val_MinusLogProbMetric: 18.2635 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 336/1000
2023-09-26 21:35:27.270 
Epoch 336/1000 
	 loss: 18.2287, MinusLogProbMetric: 18.2287, val_loss: 18.5838, val_MinusLogProbMetric: 18.5838

Epoch 336: val_loss did not improve from 18.22778
196/196 - 65s - loss: 18.2287 - MinusLogProbMetric: 18.2287 - val_loss: 18.5838 - val_MinusLogProbMetric: 18.5838 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 337/1000
2023-09-26 21:36:33.662 
Epoch 337/1000 
	 loss: 18.2375, MinusLogProbMetric: 18.2375, val_loss: 18.5601, val_MinusLogProbMetric: 18.5601

Epoch 337: val_loss did not improve from 18.22778
196/196 - 66s - loss: 18.2375 - MinusLogProbMetric: 18.2375 - val_loss: 18.5601 - val_MinusLogProbMetric: 18.5601 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 338/1000
2023-09-26 21:37:38.424 
Epoch 338/1000 
	 loss: 18.2633, MinusLogProbMetric: 18.2633, val_loss: 18.5193, val_MinusLogProbMetric: 18.5193

Epoch 338: val_loss did not improve from 18.22778
196/196 - 65s - loss: 18.2633 - MinusLogProbMetric: 18.2633 - val_loss: 18.5193 - val_MinusLogProbMetric: 18.5193 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 339/1000
2023-09-26 21:38:43.213 
Epoch 339/1000 
	 loss: 18.1651, MinusLogProbMetric: 18.1651, val_loss: 18.4208, val_MinusLogProbMetric: 18.4208

Epoch 339: val_loss did not improve from 18.22778
196/196 - 65s - loss: 18.1651 - MinusLogProbMetric: 18.1651 - val_loss: 18.4208 - val_MinusLogProbMetric: 18.4208 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 340/1000
2023-09-26 21:39:47.996 
Epoch 340/1000 
	 loss: 18.2068, MinusLogProbMetric: 18.2068, val_loss: 18.7640, val_MinusLogProbMetric: 18.7640

Epoch 340: val_loss did not improve from 18.22778
196/196 - 65s - loss: 18.2068 - MinusLogProbMetric: 18.2068 - val_loss: 18.7640 - val_MinusLogProbMetric: 18.7640 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 341/1000
2023-09-26 21:40:52.390 
Epoch 341/1000 
	 loss: 18.2659, MinusLogProbMetric: 18.2659, val_loss: 18.6747, val_MinusLogProbMetric: 18.6747

Epoch 341: val_loss did not improve from 18.22778
196/196 - 64s - loss: 18.2659 - MinusLogProbMetric: 18.2659 - val_loss: 18.6747 - val_MinusLogProbMetric: 18.6747 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 342/1000
2023-09-26 21:41:55.917 
Epoch 342/1000 
	 loss: 18.2239, MinusLogProbMetric: 18.2239, val_loss: 18.3908, val_MinusLogProbMetric: 18.3908

Epoch 342: val_loss did not improve from 18.22778
196/196 - 64s - loss: 18.2239 - MinusLogProbMetric: 18.2239 - val_loss: 18.3908 - val_MinusLogProbMetric: 18.3908 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 343/1000
2023-09-26 21:42:59.655 
Epoch 343/1000 
	 loss: 18.2121, MinusLogProbMetric: 18.2121, val_loss: 18.1759, val_MinusLogProbMetric: 18.1759

Epoch 343: val_loss improved from 18.22778 to 18.17589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.2121 - MinusLogProbMetric: 18.2121 - val_loss: 18.1759 - val_MinusLogProbMetric: 18.1759 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 344/1000
2023-09-26 21:44:03.785 
Epoch 344/1000 
	 loss: 18.1922, MinusLogProbMetric: 18.1922, val_loss: 18.2438, val_MinusLogProbMetric: 18.2438

Epoch 344: val_loss did not improve from 18.17589
196/196 - 63s - loss: 18.1922 - MinusLogProbMetric: 18.1922 - val_loss: 18.2438 - val_MinusLogProbMetric: 18.2438 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 345/1000
2023-09-26 21:45:06.786 
Epoch 345/1000 
	 loss: 18.1476, MinusLogProbMetric: 18.1476, val_loss: 18.3057, val_MinusLogProbMetric: 18.3057

Epoch 345: val_loss did not improve from 18.17589
196/196 - 63s - loss: 18.1476 - MinusLogProbMetric: 18.1476 - val_loss: 18.3057 - val_MinusLogProbMetric: 18.3057 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 346/1000
2023-09-26 21:46:10.198 
Epoch 346/1000 
	 loss: 18.1959, MinusLogProbMetric: 18.1959, val_loss: 18.3691, val_MinusLogProbMetric: 18.3691

Epoch 346: val_loss did not improve from 18.17589
196/196 - 63s - loss: 18.1959 - MinusLogProbMetric: 18.1959 - val_loss: 18.3691 - val_MinusLogProbMetric: 18.3691 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 347/1000
2023-09-26 21:47:15.002 
Epoch 347/1000 
	 loss: 18.2098, MinusLogProbMetric: 18.2098, val_loss: 18.2350, val_MinusLogProbMetric: 18.2350

Epoch 347: val_loss did not improve from 18.17589
196/196 - 65s - loss: 18.2098 - MinusLogProbMetric: 18.2098 - val_loss: 18.2350 - val_MinusLogProbMetric: 18.2350 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 348/1000
2023-09-26 21:48:18.947 
Epoch 348/1000 
	 loss: 18.1398, MinusLogProbMetric: 18.1398, val_loss: 18.1496, val_MinusLogProbMetric: 18.1496

Epoch 348: val_loss improved from 18.17589 to 18.14960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.1398 - MinusLogProbMetric: 18.1398 - val_loss: 18.1496 - val_MinusLogProbMetric: 18.1496 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 349/1000
2023-09-26 21:49:23.600 
Epoch 349/1000 
	 loss: 18.1706, MinusLogProbMetric: 18.1706, val_loss: 18.4304, val_MinusLogProbMetric: 18.4304

Epoch 349: val_loss did not improve from 18.14960
196/196 - 64s - loss: 18.1706 - MinusLogProbMetric: 18.1706 - val_loss: 18.4304 - val_MinusLogProbMetric: 18.4304 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 350/1000
2023-09-26 21:50:26.972 
Epoch 350/1000 
	 loss: 18.2353, MinusLogProbMetric: 18.2353, val_loss: 18.5854, val_MinusLogProbMetric: 18.5854

Epoch 350: val_loss did not improve from 18.14960
196/196 - 63s - loss: 18.2353 - MinusLogProbMetric: 18.2353 - val_loss: 18.5854 - val_MinusLogProbMetric: 18.5854 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 351/1000
2023-09-26 21:51:31.436 
Epoch 351/1000 
	 loss: 18.1383, MinusLogProbMetric: 18.1383, val_loss: 18.6221, val_MinusLogProbMetric: 18.6221

Epoch 351: val_loss did not improve from 18.14960
196/196 - 64s - loss: 18.1383 - MinusLogProbMetric: 18.1383 - val_loss: 18.6221 - val_MinusLogProbMetric: 18.6221 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 352/1000
2023-09-26 21:52:36.036 
Epoch 352/1000 
	 loss: 18.1826, MinusLogProbMetric: 18.1826, val_loss: 18.4779, val_MinusLogProbMetric: 18.4779

Epoch 352: val_loss did not improve from 18.14960
196/196 - 65s - loss: 18.1826 - MinusLogProbMetric: 18.1826 - val_loss: 18.4779 - val_MinusLogProbMetric: 18.4779 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 353/1000
2023-09-26 21:53:39.698 
Epoch 353/1000 
	 loss: 18.2353, MinusLogProbMetric: 18.2353, val_loss: 18.8984, val_MinusLogProbMetric: 18.8984

Epoch 353: val_loss did not improve from 18.14960
196/196 - 64s - loss: 18.2353 - MinusLogProbMetric: 18.2353 - val_loss: 18.8984 - val_MinusLogProbMetric: 18.8984 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 354/1000
2023-09-26 21:54:41.480 
Epoch 354/1000 
	 loss: 18.1643, MinusLogProbMetric: 18.1643, val_loss: 18.2845, val_MinusLogProbMetric: 18.2845

Epoch 354: val_loss did not improve from 18.14960
196/196 - 62s - loss: 18.1643 - MinusLogProbMetric: 18.1643 - val_loss: 18.2845 - val_MinusLogProbMetric: 18.2845 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 355/1000
2023-09-26 21:55:44.299 
Epoch 355/1000 
	 loss: 18.1738, MinusLogProbMetric: 18.1738, val_loss: 18.5789, val_MinusLogProbMetric: 18.5789

Epoch 355: val_loss did not improve from 18.14960
196/196 - 63s - loss: 18.1738 - MinusLogProbMetric: 18.1738 - val_loss: 18.5789 - val_MinusLogProbMetric: 18.5789 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 356/1000
2023-09-26 21:56:46.823 
Epoch 356/1000 
	 loss: 18.1722, MinusLogProbMetric: 18.1722, val_loss: 18.5334, val_MinusLogProbMetric: 18.5334

Epoch 356: val_loss did not improve from 18.14960
196/196 - 63s - loss: 18.1722 - MinusLogProbMetric: 18.1722 - val_loss: 18.5334 - val_MinusLogProbMetric: 18.5334 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 357/1000
2023-09-26 21:57:49.968 
Epoch 357/1000 
	 loss: 18.1663, MinusLogProbMetric: 18.1663, val_loss: 18.3992, val_MinusLogProbMetric: 18.3992

Epoch 357: val_loss did not improve from 18.14960
196/196 - 63s - loss: 18.1663 - MinusLogProbMetric: 18.1663 - val_loss: 18.3992 - val_MinusLogProbMetric: 18.3992 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 358/1000
2023-09-26 21:58:53.832 
Epoch 358/1000 
	 loss: 18.1750, MinusLogProbMetric: 18.1750, val_loss: 18.2929, val_MinusLogProbMetric: 18.2929

Epoch 358: val_loss did not improve from 18.14960
196/196 - 64s - loss: 18.1750 - MinusLogProbMetric: 18.1750 - val_loss: 18.2929 - val_MinusLogProbMetric: 18.2929 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 359/1000
2023-09-26 21:59:57.596 
Epoch 359/1000 
	 loss: 18.1492, MinusLogProbMetric: 18.1492, val_loss: 18.1207, val_MinusLogProbMetric: 18.1207

Epoch 359: val_loss improved from 18.14960 to 18.12072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.1492 - MinusLogProbMetric: 18.1492 - val_loss: 18.1207 - val_MinusLogProbMetric: 18.1207 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 360/1000
2023-09-26 22:01:01.410 
Epoch 360/1000 
	 loss: 18.2254, MinusLogProbMetric: 18.2254, val_loss: 18.2054, val_MinusLogProbMetric: 18.2054

Epoch 360: val_loss did not improve from 18.12072
196/196 - 63s - loss: 18.2254 - MinusLogProbMetric: 18.2254 - val_loss: 18.2054 - val_MinusLogProbMetric: 18.2054 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 361/1000
2023-09-26 22:02:04.945 
Epoch 361/1000 
	 loss: 18.1155, MinusLogProbMetric: 18.1155, val_loss: 18.7560, val_MinusLogProbMetric: 18.7560

Epoch 361: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1155 - MinusLogProbMetric: 18.1155 - val_loss: 18.7560 - val_MinusLogProbMetric: 18.7560 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 362/1000
2023-09-26 22:03:08.634 
Epoch 362/1000 
	 loss: 18.1582, MinusLogProbMetric: 18.1582, val_loss: 18.2993, val_MinusLogProbMetric: 18.2993

Epoch 362: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1582 - MinusLogProbMetric: 18.1582 - val_loss: 18.2993 - val_MinusLogProbMetric: 18.2993 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 363/1000
2023-09-26 22:04:13.130 
Epoch 363/1000 
	 loss: 18.2245, MinusLogProbMetric: 18.2245, val_loss: 18.1295, val_MinusLogProbMetric: 18.1295

Epoch 363: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.2245 - MinusLogProbMetric: 18.2245 - val_loss: 18.1295 - val_MinusLogProbMetric: 18.1295 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 364/1000
2023-09-26 22:05:15.998 
Epoch 364/1000 
	 loss: 18.1128, MinusLogProbMetric: 18.1128, val_loss: 18.4515, val_MinusLogProbMetric: 18.4515

Epoch 364: val_loss did not improve from 18.12072
196/196 - 63s - loss: 18.1128 - MinusLogProbMetric: 18.1128 - val_loss: 18.4515 - val_MinusLogProbMetric: 18.4515 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 365/1000
2023-09-26 22:06:19.835 
Epoch 365/1000 
	 loss: 18.0982, MinusLogProbMetric: 18.0982, val_loss: 18.3002, val_MinusLogProbMetric: 18.3002

Epoch 365: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.0982 - MinusLogProbMetric: 18.0982 - val_loss: 18.3002 - val_MinusLogProbMetric: 18.3002 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 366/1000
2023-09-26 22:07:24.816 
Epoch 366/1000 
	 loss: 18.1139, MinusLogProbMetric: 18.1139, val_loss: 18.8507, val_MinusLogProbMetric: 18.8507

Epoch 366: val_loss did not improve from 18.12072
196/196 - 65s - loss: 18.1139 - MinusLogProbMetric: 18.1139 - val_loss: 18.8507 - val_MinusLogProbMetric: 18.8507 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 367/1000
2023-09-26 22:08:28.534 
Epoch 367/1000 
	 loss: 18.1178, MinusLogProbMetric: 18.1178, val_loss: 18.2494, val_MinusLogProbMetric: 18.2494

Epoch 367: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1178 - MinusLogProbMetric: 18.1178 - val_loss: 18.2494 - val_MinusLogProbMetric: 18.2494 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 368/1000
2023-09-26 22:09:32.403 
Epoch 368/1000 
	 loss: 18.1152, MinusLogProbMetric: 18.1152, val_loss: 18.1597, val_MinusLogProbMetric: 18.1597

Epoch 368: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1152 - MinusLogProbMetric: 18.1152 - val_loss: 18.1597 - val_MinusLogProbMetric: 18.1597 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 369/1000
2023-09-26 22:10:35.910 
Epoch 369/1000 
	 loss: 18.1012, MinusLogProbMetric: 18.1012, val_loss: 18.3351, val_MinusLogProbMetric: 18.3351

Epoch 369: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1012 - MinusLogProbMetric: 18.1012 - val_loss: 18.3351 - val_MinusLogProbMetric: 18.3351 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 370/1000
2023-09-26 22:11:38.735 
Epoch 370/1000 
	 loss: 18.1096, MinusLogProbMetric: 18.1096, val_loss: 18.3631, val_MinusLogProbMetric: 18.3631

Epoch 370: val_loss did not improve from 18.12072
196/196 - 63s - loss: 18.1096 - MinusLogProbMetric: 18.1096 - val_loss: 18.3631 - val_MinusLogProbMetric: 18.3631 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 371/1000
2023-09-26 22:12:40.975 
Epoch 371/1000 
	 loss: 18.1016, MinusLogProbMetric: 18.1016, val_loss: 18.3807, val_MinusLogProbMetric: 18.3807

Epoch 371: val_loss did not improve from 18.12072
196/196 - 62s - loss: 18.1016 - MinusLogProbMetric: 18.1016 - val_loss: 18.3807 - val_MinusLogProbMetric: 18.3807 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 372/1000
2023-09-26 22:13:45.823 
Epoch 372/1000 
	 loss: 18.1160, MinusLogProbMetric: 18.1160, val_loss: 18.3095, val_MinusLogProbMetric: 18.3095

Epoch 372: val_loss did not improve from 18.12072
196/196 - 65s - loss: 18.1160 - MinusLogProbMetric: 18.1160 - val_loss: 18.3095 - val_MinusLogProbMetric: 18.3095 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 373/1000
2023-09-26 22:14:49.021 
Epoch 373/1000 
	 loss: 18.0459, MinusLogProbMetric: 18.0459, val_loss: 18.1385, val_MinusLogProbMetric: 18.1385

Epoch 373: val_loss did not improve from 18.12072
196/196 - 63s - loss: 18.0459 - MinusLogProbMetric: 18.0459 - val_loss: 18.1385 - val_MinusLogProbMetric: 18.1385 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 374/1000
2023-09-26 22:15:54.345 
Epoch 374/1000 
	 loss: 18.1011, MinusLogProbMetric: 18.1011, val_loss: 18.9783, val_MinusLogProbMetric: 18.9783

Epoch 374: val_loss did not improve from 18.12072
196/196 - 65s - loss: 18.1011 - MinusLogProbMetric: 18.1011 - val_loss: 18.9783 - val_MinusLogProbMetric: 18.9783 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 375/1000
2023-09-26 22:16:59.040 
Epoch 375/1000 
	 loss: 18.1872, MinusLogProbMetric: 18.1872, val_loss: 18.8416, val_MinusLogProbMetric: 18.8416

Epoch 375: val_loss did not improve from 18.12072
196/196 - 65s - loss: 18.1872 - MinusLogProbMetric: 18.1872 - val_loss: 18.8416 - val_MinusLogProbMetric: 18.8416 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 376/1000
2023-09-26 22:18:03.175 
Epoch 376/1000 
	 loss: 18.1413, MinusLogProbMetric: 18.1413, val_loss: 18.1421, val_MinusLogProbMetric: 18.1421

Epoch 376: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1413 - MinusLogProbMetric: 18.1413 - val_loss: 18.1421 - val_MinusLogProbMetric: 18.1421 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 377/1000
2023-09-26 22:19:07.256 
Epoch 377/1000 
	 loss: 18.0656, MinusLogProbMetric: 18.0656, val_loss: 18.5261, val_MinusLogProbMetric: 18.5261

Epoch 377: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.0656 - MinusLogProbMetric: 18.0656 - val_loss: 18.5261 - val_MinusLogProbMetric: 18.5261 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 378/1000
2023-09-26 22:20:11.982 
Epoch 378/1000 
	 loss: 18.0772, MinusLogProbMetric: 18.0772, val_loss: 18.5383, val_MinusLogProbMetric: 18.5383

Epoch 378: val_loss did not improve from 18.12072
196/196 - 65s - loss: 18.0772 - MinusLogProbMetric: 18.0772 - val_loss: 18.5383 - val_MinusLogProbMetric: 18.5383 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 379/1000
2023-09-26 22:21:15.684 
Epoch 379/1000 
	 loss: 18.1011, MinusLogProbMetric: 18.1011, val_loss: 18.1779, val_MinusLogProbMetric: 18.1779

Epoch 379: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1011 - MinusLogProbMetric: 18.1011 - val_loss: 18.1779 - val_MinusLogProbMetric: 18.1779 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 380/1000
2023-09-26 22:22:19.553 
Epoch 380/1000 
	 loss: 18.0739, MinusLogProbMetric: 18.0739, val_loss: 18.4324, val_MinusLogProbMetric: 18.4324

Epoch 380: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.0739 - MinusLogProbMetric: 18.0739 - val_loss: 18.4324 - val_MinusLogProbMetric: 18.4324 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 381/1000
2023-09-26 22:23:23.730 
Epoch 381/1000 
	 loss: 18.1042, MinusLogProbMetric: 18.1042, val_loss: 18.5895, val_MinusLogProbMetric: 18.5895

Epoch 381: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1042 - MinusLogProbMetric: 18.1042 - val_loss: 18.5895 - val_MinusLogProbMetric: 18.5895 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 382/1000
2023-09-26 22:24:27.501 
Epoch 382/1000 
	 loss: 18.1019, MinusLogProbMetric: 18.1019, val_loss: 18.3637, val_MinusLogProbMetric: 18.3637

Epoch 382: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.1019 - MinusLogProbMetric: 18.1019 - val_loss: 18.3637 - val_MinusLogProbMetric: 18.3637 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 383/1000
2023-09-26 22:25:30.573 
Epoch 383/1000 
	 loss: 18.0897, MinusLogProbMetric: 18.0897, val_loss: 19.1901, val_MinusLogProbMetric: 19.1901

Epoch 383: val_loss did not improve from 18.12072
196/196 - 63s - loss: 18.0897 - MinusLogProbMetric: 18.0897 - val_loss: 19.1901 - val_MinusLogProbMetric: 19.1901 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 384/1000
2023-09-26 22:26:34.310 
Epoch 384/1000 
	 loss: 18.0844, MinusLogProbMetric: 18.0844, val_loss: 18.3213, val_MinusLogProbMetric: 18.3213

Epoch 384: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.0844 - MinusLogProbMetric: 18.0844 - val_loss: 18.3213 - val_MinusLogProbMetric: 18.3213 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 385/1000
2023-09-26 22:27:38.743 
Epoch 385/1000 
	 loss: 18.0308, MinusLogProbMetric: 18.0308, val_loss: 18.1421, val_MinusLogProbMetric: 18.1421

Epoch 385: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.0308 - MinusLogProbMetric: 18.0308 - val_loss: 18.1421 - val_MinusLogProbMetric: 18.1421 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 386/1000
2023-09-26 22:28:42.274 
Epoch 386/1000 
	 loss: 17.9960, MinusLogProbMetric: 17.9960, val_loss: 18.5751, val_MinusLogProbMetric: 18.5751

Epoch 386: val_loss did not improve from 18.12072
196/196 - 64s - loss: 17.9960 - MinusLogProbMetric: 17.9960 - val_loss: 18.5751 - val_MinusLogProbMetric: 18.5751 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 387/1000
2023-09-26 22:29:46.023 
Epoch 387/1000 
	 loss: 18.0322, MinusLogProbMetric: 18.0322, val_loss: 18.2193, val_MinusLogProbMetric: 18.2193

Epoch 387: val_loss did not improve from 18.12072
196/196 - 64s - loss: 18.0322 - MinusLogProbMetric: 18.0322 - val_loss: 18.2193 - val_MinusLogProbMetric: 18.2193 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 388/1000
2023-09-26 22:30:49.554 
Epoch 388/1000 
	 loss: 18.0758, MinusLogProbMetric: 18.0758, val_loss: 18.0111, val_MinusLogProbMetric: 18.0111

Epoch 388: val_loss improved from 18.12072 to 18.01113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.0758 - MinusLogProbMetric: 18.0758 - val_loss: 18.0111 - val_MinusLogProbMetric: 18.0111 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 389/1000
2023-09-26 22:31:55.595 
Epoch 389/1000 
	 loss: 18.0641, MinusLogProbMetric: 18.0641, val_loss: 18.1865, val_MinusLogProbMetric: 18.1865

Epoch 389: val_loss did not improve from 18.01113
196/196 - 65s - loss: 18.0641 - MinusLogProbMetric: 18.0641 - val_loss: 18.1865 - val_MinusLogProbMetric: 18.1865 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 390/1000
2023-09-26 22:32:57.767 
Epoch 390/1000 
	 loss: 18.0450, MinusLogProbMetric: 18.0450, val_loss: 18.6642, val_MinusLogProbMetric: 18.6642

Epoch 390: val_loss did not improve from 18.01113
196/196 - 62s - loss: 18.0450 - MinusLogProbMetric: 18.0450 - val_loss: 18.6642 - val_MinusLogProbMetric: 18.6642 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 391/1000
2023-09-26 22:33:55.784 
Epoch 391/1000 
	 loss: 18.0482, MinusLogProbMetric: 18.0482, val_loss: 18.2283, val_MinusLogProbMetric: 18.2283

Epoch 391: val_loss did not improve from 18.01113
196/196 - 58s - loss: 18.0482 - MinusLogProbMetric: 18.0482 - val_loss: 18.2283 - val_MinusLogProbMetric: 18.2283 - lr: 3.3333e-04 - 58s/epoch - 296ms/step
Epoch 392/1000
2023-09-26 22:34:58.640 
Epoch 392/1000 
	 loss: 18.0172, MinusLogProbMetric: 18.0172, val_loss: 18.6959, val_MinusLogProbMetric: 18.6959

Epoch 392: val_loss did not improve from 18.01113
196/196 - 63s - loss: 18.0172 - MinusLogProbMetric: 18.0172 - val_loss: 18.6959 - val_MinusLogProbMetric: 18.6959 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 393/1000
2023-09-26 22:36:01.291 
Epoch 393/1000 
	 loss: 18.0276, MinusLogProbMetric: 18.0276, val_loss: 18.7469, val_MinusLogProbMetric: 18.7469

Epoch 393: val_loss did not improve from 18.01113
196/196 - 63s - loss: 18.0276 - MinusLogProbMetric: 18.0276 - val_loss: 18.7469 - val_MinusLogProbMetric: 18.7469 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 394/1000
2023-09-26 22:36:57.825 
Epoch 394/1000 
	 loss: 18.0502, MinusLogProbMetric: 18.0502, val_loss: 18.3105, val_MinusLogProbMetric: 18.3105

Epoch 394: val_loss did not improve from 18.01113
196/196 - 57s - loss: 18.0502 - MinusLogProbMetric: 18.0502 - val_loss: 18.3105 - val_MinusLogProbMetric: 18.3105 - lr: 3.3333e-04 - 57s/epoch - 288ms/step
Epoch 395/1000
2023-09-26 22:37:58.152 
Epoch 395/1000 
	 loss: 18.0330, MinusLogProbMetric: 18.0330, val_loss: 18.0145, val_MinusLogProbMetric: 18.0145

Epoch 395: val_loss did not improve from 18.01113
196/196 - 60s - loss: 18.0330 - MinusLogProbMetric: 18.0330 - val_loss: 18.0145 - val_MinusLogProbMetric: 18.0145 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 396/1000
2023-09-26 22:39:02.599 
Epoch 396/1000 
	 loss: 18.0116, MinusLogProbMetric: 18.0116, val_loss: 18.2032, val_MinusLogProbMetric: 18.2032

Epoch 396: val_loss did not improve from 18.01113
196/196 - 64s - loss: 18.0116 - MinusLogProbMetric: 18.0116 - val_loss: 18.2032 - val_MinusLogProbMetric: 18.2032 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 397/1000
2023-09-26 22:40:07.211 
Epoch 397/1000 
	 loss: 17.9876, MinusLogProbMetric: 17.9876, val_loss: 18.2049, val_MinusLogProbMetric: 18.2049

Epoch 397: val_loss did not improve from 18.01113
196/196 - 65s - loss: 17.9876 - MinusLogProbMetric: 17.9876 - val_loss: 18.2049 - val_MinusLogProbMetric: 18.2049 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 398/1000
2023-09-26 22:41:11.465 
Epoch 398/1000 
	 loss: 18.0250, MinusLogProbMetric: 18.0250, val_loss: 18.4393, val_MinusLogProbMetric: 18.4393

Epoch 398: val_loss did not improve from 18.01113
196/196 - 64s - loss: 18.0250 - MinusLogProbMetric: 18.0250 - val_loss: 18.4393 - val_MinusLogProbMetric: 18.4393 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 399/1000
2023-09-26 22:42:15.807 
Epoch 399/1000 
	 loss: 17.9597, MinusLogProbMetric: 17.9597, val_loss: 18.4888, val_MinusLogProbMetric: 18.4888

Epoch 399: val_loss did not improve from 18.01113
196/196 - 64s - loss: 17.9597 - MinusLogProbMetric: 17.9597 - val_loss: 18.4888 - val_MinusLogProbMetric: 18.4888 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 400/1000
2023-09-26 22:43:19.267 
Epoch 400/1000 
	 loss: 18.0042, MinusLogProbMetric: 18.0042, val_loss: 18.6534, val_MinusLogProbMetric: 18.6534

Epoch 400: val_loss did not improve from 18.01113
196/196 - 63s - loss: 18.0042 - MinusLogProbMetric: 18.0042 - val_loss: 18.6534 - val_MinusLogProbMetric: 18.6534 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 401/1000
2023-09-26 22:44:22.595 
Epoch 401/1000 
	 loss: 17.9801, MinusLogProbMetric: 17.9801, val_loss: 18.0024, val_MinusLogProbMetric: 18.0024

Epoch 401: val_loss improved from 18.01113 to 18.00239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 64s - loss: 17.9801 - MinusLogProbMetric: 17.9801 - val_loss: 18.0024 - val_MinusLogProbMetric: 18.0024 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 402/1000
2023-09-26 22:45:26.313 
Epoch 402/1000 
	 loss: 17.9884, MinusLogProbMetric: 17.9884, val_loss: 18.2037, val_MinusLogProbMetric: 18.2037

Epoch 402: val_loss did not improve from 18.00239
196/196 - 63s - loss: 17.9884 - MinusLogProbMetric: 17.9884 - val_loss: 18.2037 - val_MinusLogProbMetric: 18.2037 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 403/1000
2023-09-26 22:46:30.936 
Epoch 403/1000 
	 loss: 18.0320, MinusLogProbMetric: 18.0320, val_loss: 18.6049, val_MinusLogProbMetric: 18.6049

Epoch 403: val_loss did not improve from 18.00239
196/196 - 65s - loss: 18.0320 - MinusLogProbMetric: 18.0320 - val_loss: 18.6049 - val_MinusLogProbMetric: 18.6049 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 404/1000
2023-09-26 22:47:34.102 
Epoch 404/1000 
	 loss: 18.0116, MinusLogProbMetric: 18.0116, val_loss: 18.3683, val_MinusLogProbMetric: 18.3683

Epoch 404: val_loss did not improve from 18.00239
196/196 - 63s - loss: 18.0116 - MinusLogProbMetric: 18.0116 - val_loss: 18.3683 - val_MinusLogProbMetric: 18.3683 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 405/1000
2023-09-26 22:48:38.822 
Epoch 405/1000 
	 loss: 18.0118, MinusLogProbMetric: 18.0118, val_loss: 18.4000, val_MinusLogProbMetric: 18.4000

Epoch 405: val_loss did not improve from 18.00239
196/196 - 65s - loss: 18.0118 - MinusLogProbMetric: 18.0118 - val_loss: 18.4000 - val_MinusLogProbMetric: 18.4000 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 406/1000
2023-09-26 22:49:42.217 
Epoch 406/1000 
	 loss: 18.0121, MinusLogProbMetric: 18.0121, val_loss: 18.4296, val_MinusLogProbMetric: 18.4296

Epoch 406: val_loss did not improve from 18.00239
196/196 - 63s - loss: 18.0121 - MinusLogProbMetric: 18.0121 - val_loss: 18.4296 - val_MinusLogProbMetric: 18.4296 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 407/1000
2023-09-26 22:50:46.499 
Epoch 407/1000 
	 loss: 18.0165, MinusLogProbMetric: 18.0165, val_loss: 18.3712, val_MinusLogProbMetric: 18.3712

Epoch 407: val_loss did not improve from 18.00239
196/196 - 64s - loss: 18.0165 - MinusLogProbMetric: 18.0165 - val_loss: 18.3712 - val_MinusLogProbMetric: 18.3712 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 408/1000
2023-09-26 22:51:51.331 
Epoch 408/1000 
	 loss: 18.0240, MinusLogProbMetric: 18.0240, val_loss: 19.4368, val_MinusLogProbMetric: 19.4368

Epoch 408: val_loss did not improve from 18.00239
196/196 - 65s - loss: 18.0240 - MinusLogProbMetric: 18.0240 - val_loss: 19.4368 - val_MinusLogProbMetric: 19.4368 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 409/1000
2023-09-26 22:52:55.584 
Epoch 409/1000 
	 loss: 17.9731, MinusLogProbMetric: 17.9731, val_loss: 18.1753, val_MinusLogProbMetric: 18.1753

Epoch 409: val_loss did not improve from 18.00239
196/196 - 64s - loss: 17.9731 - MinusLogProbMetric: 17.9731 - val_loss: 18.1753 - val_MinusLogProbMetric: 18.1753 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 410/1000
2023-09-26 22:53:59.601 
Epoch 410/1000 
	 loss: 17.9865, MinusLogProbMetric: 17.9865, val_loss: 18.0859, val_MinusLogProbMetric: 18.0859

Epoch 410: val_loss did not improve from 18.00239
196/196 - 64s - loss: 17.9865 - MinusLogProbMetric: 17.9865 - val_loss: 18.0859 - val_MinusLogProbMetric: 18.0859 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 411/1000
2023-09-26 22:55:04.412 
Epoch 411/1000 
	 loss: 17.9576, MinusLogProbMetric: 17.9576, val_loss: 18.2613, val_MinusLogProbMetric: 18.2613

Epoch 411: val_loss did not improve from 18.00239
196/196 - 65s - loss: 17.9576 - MinusLogProbMetric: 17.9576 - val_loss: 18.2613 - val_MinusLogProbMetric: 18.2613 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 412/1000
2023-09-26 22:56:10.524 
Epoch 412/1000 
	 loss: 17.9733, MinusLogProbMetric: 17.9733, val_loss: 18.1672, val_MinusLogProbMetric: 18.1672

Epoch 412: val_loss did not improve from 18.00239
196/196 - 66s - loss: 17.9733 - MinusLogProbMetric: 17.9733 - val_loss: 18.1672 - val_MinusLogProbMetric: 18.1672 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 413/1000
2023-09-26 22:57:14.620 
Epoch 413/1000 
	 loss: 17.9905, MinusLogProbMetric: 17.9905, val_loss: 18.2048, val_MinusLogProbMetric: 18.2048

Epoch 413: val_loss did not improve from 18.00239
196/196 - 64s - loss: 17.9905 - MinusLogProbMetric: 17.9905 - val_loss: 18.2048 - val_MinusLogProbMetric: 18.2048 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 414/1000
2023-09-26 22:58:19.653 
Epoch 414/1000 
	 loss: 17.9613, MinusLogProbMetric: 17.9613, val_loss: 17.9930, val_MinusLogProbMetric: 17.9930

Epoch 414: val_loss improved from 18.00239 to 17.99296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 17.9613 - MinusLogProbMetric: 17.9613 - val_loss: 17.9930 - val_MinusLogProbMetric: 17.9930 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 415/1000
2023-09-26 22:59:24.779 
Epoch 415/1000 
	 loss: 18.0233, MinusLogProbMetric: 18.0233, val_loss: 18.2376, val_MinusLogProbMetric: 18.2376

Epoch 415: val_loss did not improve from 17.99296
196/196 - 64s - loss: 18.0233 - MinusLogProbMetric: 18.0233 - val_loss: 18.2376 - val_MinusLogProbMetric: 18.2376 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 416/1000
2023-09-26 23:00:28.784 
Epoch 416/1000 
	 loss: 17.9147, MinusLogProbMetric: 17.9147, val_loss: 18.3235, val_MinusLogProbMetric: 18.3235

Epoch 416: val_loss did not improve from 17.99296
196/196 - 64s - loss: 17.9147 - MinusLogProbMetric: 17.9147 - val_loss: 18.3235 - val_MinusLogProbMetric: 18.3235 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 417/1000
2023-09-26 23:01:32.539 
Epoch 417/1000 
	 loss: 18.0069, MinusLogProbMetric: 18.0069, val_loss: 17.9197, val_MinusLogProbMetric: 17.9197

Epoch 417: val_loss improved from 17.99296 to 17.91968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 18.0069 - MinusLogProbMetric: 18.0069 - val_loss: 17.9197 - val_MinusLogProbMetric: 17.9197 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 418/1000
2023-09-26 23:02:38.428 
Epoch 418/1000 
	 loss: 17.9860, MinusLogProbMetric: 17.9860, val_loss: 18.3913, val_MinusLogProbMetric: 18.3913

Epoch 418: val_loss did not improve from 17.91968
196/196 - 65s - loss: 17.9860 - MinusLogProbMetric: 17.9860 - val_loss: 18.3913 - val_MinusLogProbMetric: 18.3913 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 419/1000
2023-09-26 23:03:42.711 
Epoch 419/1000 
	 loss: 17.9676, MinusLogProbMetric: 17.9676, val_loss: 18.2054, val_MinusLogProbMetric: 18.2054

Epoch 419: val_loss did not improve from 17.91968
196/196 - 64s - loss: 17.9676 - MinusLogProbMetric: 17.9676 - val_loss: 18.2054 - val_MinusLogProbMetric: 18.2054 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 420/1000
2023-09-26 23:04:47.909 
Epoch 420/1000 
	 loss: 17.9768, MinusLogProbMetric: 17.9768, val_loss: 18.5330, val_MinusLogProbMetric: 18.5330

Epoch 420: val_loss did not improve from 17.91968
196/196 - 65s - loss: 17.9768 - MinusLogProbMetric: 17.9768 - val_loss: 18.5330 - val_MinusLogProbMetric: 18.5330 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 421/1000
2023-09-26 23:05:51.913 
Epoch 421/1000 
	 loss: 18.3252, MinusLogProbMetric: 18.3252, val_loss: 18.3444, val_MinusLogProbMetric: 18.3444

Epoch 421: val_loss did not improve from 17.91968
196/196 - 64s - loss: 18.3252 - MinusLogProbMetric: 18.3252 - val_loss: 18.3444 - val_MinusLogProbMetric: 18.3444 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 422/1000
2023-09-26 23:06:55.143 
Epoch 422/1000 
	 loss: 17.9813, MinusLogProbMetric: 17.9813, val_loss: 18.2011, val_MinusLogProbMetric: 18.2011

Epoch 422: val_loss did not improve from 17.91968
196/196 - 63s - loss: 17.9813 - MinusLogProbMetric: 17.9813 - val_loss: 18.2011 - val_MinusLogProbMetric: 18.2011 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 423/1000
2023-09-26 23:07:59.002 
Epoch 423/1000 
	 loss: 18.0133, MinusLogProbMetric: 18.0133, val_loss: 18.4153, val_MinusLogProbMetric: 18.4153

Epoch 423: val_loss did not improve from 17.91968
196/196 - 64s - loss: 18.0133 - MinusLogProbMetric: 18.0133 - val_loss: 18.4153 - val_MinusLogProbMetric: 18.4153 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 424/1000
2023-09-26 23:09:03.405 
Epoch 424/1000 
	 loss: 17.9324, MinusLogProbMetric: 17.9324, val_loss: 18.0861, val_MinusLogProbMetric: 18.0861

Epoch 424: val_loss did not improve from 17.91968
196/196 - 64s - loss: 17.9324 - MinusLogProbMetric: 17.9324 - val_loss: 18.0861 - val_MinusLogProbMetric: 18.0861 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 425/1000
2023-09-26 23:10:08.340 
Epoch 425/1000 
	 loss: 17.9346, MinusLogProbMetric: 17.9346, val_loss: 18.5815, val_MinusLogProbMetric: 18.5815

Epoch 425: val_loss did not improve from 17.91968
196/196 - 65s - loss: 17.9346 - MinusLogProbMetric: 17.9346 - val_loss: 18.5815 - val_MinusLogProbMetric: 18.5815 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 426/1000
2023-09-26 23:11:13.004 
Epoch 426/1000 
	 loss: 17.9686, MinusLogProbMetric: 17.9686, val_loss: 18.4322, val_MinusLogProbMetric: 18.4322

Epoch 426: val_loss did not improve from 17.91968
196/196 - 65s - loss: 17.9686 - MinusLogProbMetric: 17.9686 - val_loss: 18.4322 - val_MinusLogProbMetric: 18.4322 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 427/1000
2023-09-26 23:12:17.314 
Epoch 427/1000 
	 loss: 17.9674, MinusLogProbMetric: 17.9674, val_loss: 18.6611, val_MinusLogProbMetric: 18.6611

Epoch 427: val_loss did not improve from 17.91968
196/196 - 64s - loss: 17.9674 - MinusLogProbMetric: 17.9674 - val_loss: 18.6611 - val_MinusLogProbMetric: 18.6611 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 428/1000
2023-09-26 23:13:22.657 
Epoch 428/1000 
	 loss: 17.9436, MinusLogProbMetric: 17.9436, val_loss: 18.1971, val_MinusLogProbMetric: 18.1971

Epoch 428: val_loss did not improve from 17.91968
196/196 - 65s - loss: 17.9436 - MinusLogProbMetric: 17.9436 - val_loss: 18.1971 - val_MinusLogProbMetric: 18.1971 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 429/1000
2023-09-26 23:14:26.063 
Epoch 429/1000 
	 loss: 18.0031, MinusLogProbMetric: 18.0031, val_loss: 18.4639, val_MinusLogProbMetric: 18.4639

Epoch 429: val_loss did not improve from 17.91968
196/196 - 63s - loss: 18.0031 - MinusLogProbMetric: 18.0031 - val_loss: 18.4639 - val_MinusLogProbMetric: 18.4639 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 430/1000
2023-09-26 23:15:30.890 
Epoch 430/1000 
	 loss: 18.0083, MinusLogProbMetric: 18.0083, val_loss: 17.9223, val_MinusLogProbMetric: 17.9223

Epoch 430: val_loss did not improve from 17.91968
196/196 - 65s - loss: 18.0083 - MinusLogProbMetric: 18.0083 - val_loss: 17.9223 - val_MinusLogProbMetric: 17.9223 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 431/1000
2023-09-26 23:16:35.115 
Epoch 431/1000 
	 loss: 17.8851, MinusLogProbMetric: 17.8851, val_loss: 18.2825, val_MinusLogProbMetric: 18.2825

Epoch 431: val_loss did not improve from 17.91968
196/196 - 64s - loss: 17.8851 - MinusLogProbMetric: 17.8851 - val_loss: 18.2825 - val_MinusLogProbMetric: 18.2825 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 432/1000
2023-09-26 23:17:40.409 
Epoch 432/1000 
	 loss: 17.9214, MinusLogProbMetric: 17.9214, val_loss: 17.9842, val_MinusLogProbMetric: 17.9842

Epoch 432: val_loss did not improve from 17.91968
196/196 - 65s - loss: 17.9214 - MinusLogProbMetric: 17.9214 - val_loss: 17.9842 - val_MinusLogProbMetric: 17.9842 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 433/1000
2023-09-26 23:18:46.212 
Epoch 433/1000 
	 loss: 17.9063, MinusLogProbMetric: 17.9063, val_loss: 17.9869, val_MinusLogProbMetric: 17.9869

Epoch 433: val_loss did not improve from 17.91968
196/196 - 66s - loss: 17.9063 - MinusLogProbMetric: 17.9063 - val_loss: 17.9869 - val_MinusLogProbMetric: 17.9869 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 434/1000
2023-09-26 23:19:51.470 
Epoch 434/1000 
	 loss: 17.9379, MinusLogProbMetric: 17.9379, val_loss: 17.9145, val_MinusLogProbMetric: 17.9145

Epoch 434: val_loss improved from 17.91968 to 17.91447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 17.9379 - MinusLogProbMetric: 17.9379 - val_loss: 17.9145 - val_MinusLogProbMetric: 17.9145 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 435/1000
2023-09-26 23:20:56.289 
Epoch 435/1000 
	 loss: 17.9055, MinusLogProbMetric: 17.9055, val_loss: 18.0620, val_MinusLogProbMetric: 18.0620

Epoch 435: val_loss did not improve from 17.91447
196/196 - 64s - loss: 17.9055 - MinusLogProbMetric: 17.9055 - val_loss: 18.0620 - val_MinusLogProbMetric: 18.0620 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 436/1000
2023-09-26 23:22:01.249 
Epoch 436/1000 
	 loss: 17.9054, MinusLogProbMetric: 17.9054, val_loss: 18.1093, val_MinusLogProbMetric: 18.1093

Epoch 436: val_loss did not improve from 17.91447
196/196 - 65s - loss: 17.9054 - MinusLogProbMetric: 17.9054 - val_loss: 18.1093 - val_MinusLogProbMetric: 18.1093 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 437/1000
2023-09-26 23:23:06.218 
Epoch 437/1000 
	 loss: 17.9535, MinusLogProbMetric: 17.9535, val_loss: 18.3860, val_MinusLogProbMetric: 18.3860

Epoch 437: val_loss did not improve from 17.91447
196/196 - 65s - loss: 17.9535 - MinusLogProbMetric: 17.9535 - val_loss: 18.3860 - val_MinusLogProbMetric: 18.3860 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 438/1000
2023-09-26 23:24:11.828 
Epoch 438/1000 
	 loss: 17.8949, MinusLogProbMetric: 17.8949, val_loss: 18.2083, val_MinusLogProbMetric: 18.2083

Epoch 438: val_loss did not improve from 17.91447
196/196 - 66s - loss: 17.8949 - MinusLogProbMetric: 17.8949 - val_loss: 18.2083 - val_MinusLogProbMetric: 18.2083 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 439/1000
2023-09-26 23:25:15.416 
Epoch 439/1000 
	 loss: 17.8864, MinusLogProbMetric: 17.8864, val_loss: 18.0817, val_MinusLogProbMetric: 18.0817

Epoch 439: val_loss did not improve from 17.91447
196/196 - 64s - loss: 17.8864 - MinusLogProbMetric: 17.8864 - val_loss: 18.0817 - val_MinusLogProbMetric: 18.0817 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 440/1000
2023-09-26 23:26:20.132 
Epoch 440/1000 
	 loss: 17.9599, MinusLogProbMetric: 17.9599, val_loss: 18.2147, val_MinusLogProbMetric: 18.2147

Epoch 440: val_loss did not improve from 17.91447
196/196 - 65s - loss: 17.9599 - MinusLogProbMetric: 17.9599 - val_loss: 18.2147 - val_MinusLogProbMetric: 18.2147 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 441/1000
2023-09-26 23:27:23.840 
Epoch 441/1000 
	 loss: 17.9128, MinusLogProbMetric: 17.9128, val_loss: 18.1751, val_MinusLogProbMetric: 18.1751

Epoch 441: val_loss did not improve from 17.91447
196/196 - 64s - loss: 17.9128 - MinusLogProbMetric: 17.9128 - val_loss: 18.1751 - val_MinusLogProbMetric: 18.1751 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 442/1000
2023-09-26 23:28:27.848 
Epoch 442/1000 
	 loss: 17.9585, MinusLogProbMetric: 17.9585, val_loss: 18.2779, val_MinusLogProbMetric: 18.2779

Epoch 442: val_loss did not improve from 17.91447
196/196 - 64s - loss: 17.9585 - MinusLogProbMetric: 17.9585 - val_loss: 18.2779 - val_MinusLogProbMetric: 18.2779 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 443/1000
2023-09-26 23:29:33.605 
Epoch 443/1000 
	 loss: 17.8807, MinusLogProbMetric: 17.8807, val_loss: 18.9643, val_MinusLogProbMetric: 18.9643

Epoch 443: val_loss did not improve from 17.91447
196/196 - 66s - loss: 17.8807 - MinusLogProbMetric: 17.8807 - val_loss: 18.9643 - val_MinusLogProbMetric: 18.9643 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 444/1000
2023-09-26 23:30:37.265 
Epoch 444/1000 
	 loss: 17.9780, MinusLogProbMetric: 17.9780, val_loss: 18.0646, val_MinusLogProbMetric: 18.0646

Epoch 444: val_loss did not improve from 17.91447
196/196 - 64s - loss: 17.9780 - MinusLogProbMetric: 17.9780 - val_loss: 18.0646 - val_MinusLogProbMetric: 18.0646 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 445/1000
2023-09-26 23:31:41.952 
Epoch 445/1000 
	 loss: 17.8547, MinusLogProbMetric: 17.8547, val_loss: 17.8183, val_MinusLogProbMetric: 17.8183

Epoch 445: val_loss improved from 17.91447 to 17.81833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 17.8547 - MinusLogProbMetric: 17.8547 - val_loss: 17.8183 - val_MinusLogProbMetric: 17.8183 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 446/1000
2023-09-26 23:32:47.705 
Epoch 446/1000 
	 loss: 17.8802, MinusLogProbMetric: 17.8802, val_loss: 18.1088, val_MinusLogProbMetric: 18.1088

Epoch 446: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8802 - MinusLogProbMetric: 17.8802 - val_loss: 18.1088 - val_MinusLogProbMetric: 18.1088 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 447/1000
2023-09-26 23:33:52.305 
Epoch 447/1000 
	 loss: 17.8637, MinusLogProbMetric: 17.8637, val_loss: 18.2335, val_MinusLogProbMetric: 18.2335

Epoch 447: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8637 - MinusLogProbMetric: 17.8637 - val_loss: 18.2335 - val_MinusLogProbMetric: 18.2335 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 448/1000
2023-09-26 23:34:58.019 
Epoch 448/1000 
	 loss: 17.9080, MinusLogProbMetric: 17.9080, val_loss: 17.9684, val_MinusLogProbMetric: 17.9684

Epoch 448: val_loss did not improve from 17.81833
196/196 - 66s - loss: 17.9080 - MinusLogProbMetric: 17.9080 - val_loss: 17.9684 - val_MinusLogProbMetric: 17.9684 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 449/1000
2023-09-26 23:36:02.486 
Epoch 449/1000 
	 loss: 17.9112, MinusLogProbMetric: 17.9112, val_loss: 17.9659, val_MinusLogProbMetric: 17.9659

Epoch 449: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.9112 - MinusLogProbMetric: 17.9112 - val_loss: 17.9659 - val_MinusLogProbMetric: 17.9659 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 450/1000
2023-09-26 23:37:05.990 
Epoch 450/1000 
	 loss: 17.8601, MinusLogProbMetric: 17.8601, val_loss: 18.1590, val_MinusLogProbMetric: 18.1590

Epoch 450: val_loss did not improve from 17.81833
196/196 - 63s - loss: 17.8601 - MinusLogProbMetric: 17.8601 - val_loss: 18.1590 - val_MinusLogProbMetric: 18.1590 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 451/1000
2023-09-26 23:38:09.869 
Epoch 451/1000 
	 loss: 17.9056, MinusLogProbMetric: 17.9056, val_loss: 18.3392, val_MinusLogProbMetric: 18.3392

Epoch 451: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.9056 - MinusLogProbMetric: 17.9056 - val_loss: 18.3392 - val_MinusLogProbMetric: 18.3392 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 452/1000
2023-09-26 23:39:15.445 
Epoch 452/1000 
	 loss: 17.9214, MinusLogProbMetric: 17.9214, val_loss: 17.9264, val_MinusLogProbMetric: 17.9264

Epoch 452: val_loss did not improve from 17.81833
196/196 - 66s - loss: 17.9214 - MinusLogProbMetric: 17.9214 - val_loss: 17.9264 - val_MinusLogProbMetric: 17.9264 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 453/1000
2023-09-26 23:40:19.089 
Epoch 453/1000 
	 loss: 17.9107, MinusLogProbMetric: 17.9107, val_loss: 18.3579, val_MinusLogProbMetric: 18.3579

Epoch 453: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.9107 - MinusLogProbMetric: 17.9107 - val_loss: 18.3579 - val_MinusLogProbMetric: 18.3579 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 454/1000
2023-09-26 23:41:24.303 
Epoch 454/1000 
	 loss: 17.8574, MinusLogProbMetric: 17.8574, val_loss: 18.0611, val_MinusLogProbMetric: 18.0611

Epoch 454: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8574 - MinusLogProbMetric: 17.8574 - val_loss: 18.0611 - val_MinusLogProbMetric: 18.0611 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 455/1000
2023-09-26 23:42:27.813 
Epoch 455/1000 
	 loss: 17.8790, MinusLogProbMetric: 17.8790, val_loss: 18.1227, val_MinusLogProbMetric: 18.1227

Epoch 455: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8790 - MinusLogProbMetric: 17.8790 - val_loss: 18.1227 - val_MinusLogProbMetric: 18.1227 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 456/1000
2023-09-26 23:43:32.203 
Epoch 456/1000 
	 loss: 17.8432, MinusLogProbMetric: 17.8432, val_loss: 17.9675, val_MinusLogProbMetric: 17.9675

Epoch 456: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8432 - MinusLogProbMetric: 17.8432 - val_loss: 17.9675 - val_MinusLogProbMetric: 17.9675 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 457/1000
2023-09-26 23:44:36.278 
Epoch 457/1000 
	 loss: 17.8620, MinusLogProbMetric: 17.8620, val_loss: 18.4471, val_MinusLogProbMetric: 18.4471

Epoch 457: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8620 - MinusLogProbMetric: 17.8620 - val_loss: 18.4471 - val_MinusLogProbMetric: 18.4471 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 458/1000
2023-09-26 23:45:42.170 
Epoch 458/1000 
	 loss: 17.8996, MinusLogProbMetric: 17.8996, val_loss: 18.1454, val_MinusLogProbMetric: 18.1454

Epoch 458: val_loss did not improve from 17.81833
196/196 - 66s - loss: 17.8996 - MinusLogProbMetric: 17.8996 - val_loss: 18.1454 - val_MinusLogProbMetric: 18.1454 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 459/1000
2023-09-26 23:46:46.762 
Epoch 459/1000 
	 loss: 17.8491, MinusLogProbMetric: 17.8491, val_loss: 17.8999, val_MinusLogProbMetric: 17.8999

Epoch 459: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8491 - MinusLogProbMetric: 17.8491 - val_loss: 17.8999 - val_MinusLogProbMetric: 17.8999 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 460/1000
2023-09-26 23:47:52.242 
Epoch 460/1000 
	 loss: 17.8373, MinusLogProbMetric: 17.8373, val_loss: 17.8677, val_MinusLogProbMetric: 17.8677

Epoch 460: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8373 - MinusLogProbMetric: 17.8373 - val_loss: 17.8677 - val_MinusLogProbMetric: 17.8677 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 461/1000
2023-09-26 23:48:56.694 
Epoch 461/1000 
	 loss: 17.8558, MinusLogProbMetric: 17.8558, val_loss: 18.1522, val_MinusLogProbMetric: 18.1522

Epoch 461: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8558 - MinusLogProbMetric: 17.8558 - val_loss: 18.1522 - val_MinusLogProbMetric: 18.1522 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 462/1000
2023-09-26 23:50:01.450 
Epoch 462/1000 
	 loss: 17.8315, MinusLogProbMetric: 17.8315, val_loss: 17.9297, val_MinusLogProbMetric: 17.9297

Epoch 462: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8315 - MinusLogProbMetric: 17.8315 - val_loss: 17.9297 - val_MinusLogProbMetric: 17.9297 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 463/1000
2023-09-26 23:51:04.130 
Epoch 463/1000 
	 loss: 17.8941, MinusLogProbMetric: 17.8941, val_loss: 18.2411, val_MinusLogProbMetric: 18.2411

Epoch 463: val_loss did not improve from 17.81833
196/196 - 63s - loss: 17.8941 - MinusLogProbMetric: 17.8941 - val_loss: 18.2411 - val_MinusLogProbMetric: 18.2411 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 464/1000
2023-09-26 23:52:07.557 
Epoch 464/1000 
	 loss: 17.8631, MinusLogProbMetric: 17.8631, val_loss: 17.9470, val_MinusLogProbMetric: 17.9470

Epoch 464: val_loss did not improve from 17.81833
196/196 - 63s - loss: 17.8631 - MinusLogProbMetric: 17.8631 - val_loss: 17.9470 - val_MinusLogProbMetric: 17.9470 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 465/1000
2023-09-26 23:53:11.175 
Epoch 465/1000 
	 loss: 17.8246, MinusLogProbMetric: 17.8246, val_loss: 17.9899, val_MinusLogProbMetric: 17.9899

Epoch 465: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8246 - MinusLogProbMetric: 17.8246 - val_loss: 17.9899 - val_MinusLogProbMetric: 17.9899 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 466/1000
2023-09-26 23:54:13.999 
Epoch 466/1000 
	 loss: 17.8848, MinusLogProbMetric: 17.8848, val_loss: 17.9735, val_MinusLogProbMetric: 17.9735

Epoch 466: val_loss did not improve from 17.81833
196/196 - 63s - loss: 17.8848 - MinusLogProbMetric: 17.8848 - val_loss: 17.9735 - val_MinusLogProbMetric: 17.9735 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 467/1000
2023-09-26 23:55:18.460 
Epoch 467/1000 
	 loss: 17.8232, MinusLogProbMetric: 17.8232, val_loss: 18.0266, val_MinusLogProbMetric: 18.0266

Epoch 467: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8232 - MinusLogProbMetric: 17.8232 - val_loss: 18.0266 - val_MinusLogProbMetric: 18.0266 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 468/1000
2023-09-26 23:56:22.204 
Epoch 468/1000 
	 loss: 17.8550, MinusLogProbMetric: 17.8550, val_loss: 17.9409, val_MinusLogProbMetric: 17.9409

Epoch 468: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8550 - MinusLogProbMetric: 17.8550 - val_loss: 17.9409 - val_MinusLogProbMetric: 17.9409 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 469/1000
2023-09-26 23:57:26.976 
Epoch 469/1000 
	 loss: 17.8832, MinusLogProbMetric: 17.8832, val_loss: 18.2322, val_MinusLogProbMetric: 18.2322

Epoch 469: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8832 - MinusLogProbMetric: 17.8832 - val_loss: 18.2322 - val_MinusLogProbMetric: 18.2322 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 470/1000
2023-09-26 23:58:30.817 
Epoch 470/1000 
	 loss: 17.8666, MinusLogProbMetric: 17.8666, val_loss: 17.9726, val_MinusLogProbMetric: 17.9726

Epoch 470: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8666 - MinusLogProbMetric: 17.8666 - val_loss: 17.9726 - val_MinusLogProbMetric: 17.9726 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 471/1000
2023-09-26 23:59:34.610 
Epoch 471/1000 
	 loss: 17.8485, MinusLogProbMetric: 17.8485, val_loss: 18.1326, val_MinusLogProbMetric: 18.1326

Epoch 471: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8485 - MinusLogProbMetric: 17.8485 - val_loss: 18.1326 - val_MinusLogProbMetric: 18.1326 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 472/1000
2023-09-27 00:00:38.041 
Epoch 472/1000 
	 loss: 17.8230, MinusLogProbMetric: 17.8230, val_loss: 18.0515, val_MinusLogProbMetric: 18.0515

Epoch 472: val_loss did not improve from 17.81833
196/196 - 63s - loss: 17.8230 - MinusLogProbMetric: 17.8230 - val_loss: 18.0515 - val_MinusLogProbMetric: 18.0515 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 473/1000
2023-09-27 00:01:43.026 
Epoch 473/1000 
	 loss: 17.8706, MinusLogProbMetric: 17.8706, val_loss: 17.8513, val_MinusLogProbMetric: 17.8513

Epoch 473: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8706 - MinusLogProbMetric: 17.8706 - val_loss: 17.8513 - val_MinusLogProbMetric: 17.8513 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 474/1000
2023-09-27 00:02:48.687 
Epoch 474/1000 
	 loss: 17.8150, MinusLogProbMetric: 17.8150, val_loss: 18.3796, val_MinusLogProbMetric: 18.3796

Epoch 474: val_loss did not improve from 17.81833
196/196 - 66s - loss: 17.8150 - MinusLogProbMetric: 17.8150 - val_loss: 18.3796 - val_MinusLogProbMetric: 18.3796 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 475/1000
2023-09-27 00:03:53.110 
Epoch 475/1000 
	 loss: 17.8180, MinusLogProbMetric: 17.8180, val_loss: 18.0353, val_MinusLogProbMetric: 18.0353

Epoch 475: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8180 - MinusLogProbMetric: 17.8180 - val_loss: 18.0353 - val_MinusLogProbMetric: 18.0353 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 476/1000
2023-09-27 00:04:57.107 
Epoch 476/1000 
	 loss: 17.8129, MinusLogProbMetric: 17.8129, val_loss: 18.3588, val_MinusLogProbMetric: 18.3588

Epoch 476: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8129 - MinusLogProbMetric: 17.8129 - val_loss: 18.3588 - val_MinusLogProbMetric: 18.3588 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 477/1000
2023-09-27 00:06:01.649 
Epoch 477/1000 
	 loss: 17.8253, MinusLogProbMetric: 17.8253, val_loss: 18.6066, val_MinusLogProbMetric: 18.6066

Epoch 477: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8253 - MinusLogProbMetric: 17.8253 - val_loss: 18.6066 - val_MinusLogProbMetric: 18.6066 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 478/1000
2023-09-27 00:07:05.459 
Epoch 478/1000 
	 loss: 17.8280, MinusLogProbMetric: 17.8280, val_loss: 17.9582, val_MinusLogProbMetric: 17.9582

Epoch 478: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8280 - MinusLogProbMetric: 17.8280 - val_loss: 17.9582 - val_MinusLogProbMetric: 17.9582 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 479/1000
2023-09-27 00:08:09.261 
Epoch 479/1000 
	 loss: 17.8060, MinusLogProbMetric: 17.8060, val_loss: 18.4604, val_MinusLogProbMetric: 18.4604

Epoch 479: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8060 - MinusLogProbMetric: 17.8060 - val_loss: 18.4604 - val_MinusLogProbMetric: 18.4604 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 480/1000
2023-09-27 00:09:13.110 
Epoch 480/1000 
	 loss: 17.8625, MinusLogProbMetric: 17.8625, val_loss: 17.8207, val_MinusLogProbMetric: 17.8207

Epoch 480: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8625 - MinusLogProbMetric: 17.8625 - val_loss: 17.8207 - val_MinusLogProbMetric: 17.8207 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 481/1000
2023-09-27 00:10:17.616 
Epoch 481/1000 
	 loss: 17.8090, MinusLogProbMetric: 17.8090, val_loss: 18.0695, val_MinusLogProbMetric: 18.0695

Epoch 481: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8090 - MinusLogProbMetric: 17.8090 - val_loss: 18.0695 - val_MinusLogProbMetric: 18.0695 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 482/1000
2023-09-27 00:11:22.849 
Epoch 482/1000 
	 loss: 17.8087, MinusLogProbMetric: 17.8087, val_loss: 18.0815, val_MinusLogProbMetric: 18.0815

Epoch 482: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8087 - MinusLogProbMetric: 17.8087 - val_loss: 18.0815 - val_MinusLogProbMetric: 18.0815 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 483/1000
2023-09-27 00:12:28.227 
Epoch 483/1000 
	 loss: 17.8411, MinusLogProbMetric: 17.8411, val_loss: 18.0127, val_MinusLogProbMetric: 18.0127

Epoch 483: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8411 - MinusLogProbMetric: 17.8411 - val_loss: 18.0127 - val_MinusLogProbMetric: 18.0127 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 484/1000
2023-09-27 00:13:33.211 
Epoch 484/1000 
	 loss: 17.8660, MinusLogProbMetric: 17.8660, val_loss: 17.8903, val_MinusLogProbMetric: 17.8903

Epoch 484: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8660 - MinusLogProbMetric: 17.8660 - val_loss: 17.8903 - val_MinusLogProbMetric: 17.8903 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 485/1000
2023-09-27 00:14:37.973 
Epoch 485/1000 
	 loss: 17.7872, MinusLogProbMetric: 17.7872, val_loss: 18.3435, val_MinusLogProbMetric: 18.3435

Epoch 485: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.7872 - MinusLogProbMetric: 17.7872 - val_loss: 18.3435 - val_MinusLogProbMetric: 18.3435 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 486/1000
2023-09-27 00:15:43.098 
Epoch 486/1000 
	 loss: 17.8326, MinusLogProbMetric: 17.8326, val_loss: 18.0338, val_MinusLogProbMetric: 18.0338

Epoch 486: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8326 - MinusLogProbMetric: 17.8326 - val_loss: 18.0338 - val_MinusLogProbMetric: 18.0338 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 487/1000
2023-09-27 00:16:47.353 
Epoch 487/1000 
	 loss: 17.8535, MinusLogProbMetric: 17.8535, val_loss: 18.2606, val_MinusLogProbMetric: 18.2606

Epoch 487: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.8535 - MinusLogProbMetric: 17.8535 - val_loss: 18.2606 - val_MinusLogProbMetric: 18.2606 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 488/1000
2023-09-27 00:17:51.945 
Epoch 488/1000 
	 loss: 17.7760, MinusLogProbMetric: 17.7760, val_loss: 18.0462, val_MinusLogProbMetric: 18.0462

Epoch 488: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.7760 - MinusLogProbMetric: 17.7760 - val_loss: 18.0462 - val_MinusLogProbMetric: 18.0462 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 489/1000
2023-09-27 00:18:56.772 
Epoch 489/1000 
	 loss: 17.8188, MinusLogProbMetric: 17.8188, val_loss: 18.1428, val_MinusLogProbMetric: 18.1428

Epoch 489: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.8188 - MinusLogProbMetric: 17.8188 - val_loss: 18.1428 - val_MinusLogProbMetric: 18.1428 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 490/1000
2023-09-27 00:20:01.535 
Epoch 490/1000 
	 loss: 17.7883, MinusLogProbMetric: 17.7883, val_loss: 18.0098, val_MinusLogProbMetric: 18.0098

Epoch 490: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.7883 - MinusLogProbMetric: 17.7883 - val_loss: 18.0098 - val_MinusLogProbMetric: 18.0098 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 491/1000
2023-09-27 00:21:05.533 
Epoch 491/1000 
	 loss: 17.7867, MinusLogProbMetric: 17.7867, val_loss: 18.3870, val_MinusLogProbMetric: 18.3870

Epoch 491: val_loss did not improve from 17.81833
196/196 - 64s - loss: 17.7867 - MinusLogProbMetric: 17.7867 - val_loss: 18.3870 - val_MinusLogProbMetric: 18.3870 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 492/1000
2023-09-27 00:22:10.288 
Epoch 492/1000 
	 loss: 17.7664, MinusLogProbMetric: 17.7664, val_loss: 17.9529, val_MinusLogProbMetric: 17.9529

Epoch 492: val_loss did not improve from 17.81833
196/196 - 65s - loss: 17.7664 - MinusLogProbMetric: 17.7664 - val_loss: 17.9529 - val_MinusLogProbMetric: 17.9529 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 493/1000
2023-09-27 00:23:13.717 
Epoch 493/1000 
	 loss: 17.7390, MinusLogProbMetric: 17.7390, val_loss: 17.9594, val_MinusLogProbMetric: 17.9594

Epoch 493: val_loss did not improve from 17.81833
196/196 - 63s - loss: 17.7390 - MinusLogProbMetric: 17.7390 - val_loss: 17.9594 - val_MinusLogProbMetric: 17.9594 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 494/1000
2023-09-27 00:24:14.271 
Epoch 494/1000 
	 loss: 17.7709, MinusLogProbMetric: 17.7709, val_loss: 17.9762, val_MinusLogProbMetric: 17.9762

Epoch 494: val_loss did not improve from 17.81833
196/196 - 61s - loss: 17.7709 - MinusLogProbMetric: 17.7709 - val_loss: 17.9762 - val_MinusLogProbMetric: 17.9762 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 495/1000
2023-09-27 00:25:08.767 
Epoch 495/1000 
	 loss: 17.7481, MinusLogProbMetric: 17.7481, val_loss: 17.9828, val_MinusLogProbMetric: 17.9828

Epoch 495: val_loss did not improve from 17.81833
196/196 - 54s - loss: 17.7481 - MinusLogProbMetric: 17.7481 - val_loss: 17.9828 - val_MinusLogProbMetric: 17.9828 - lr: 3.3333e-04 - 54s/epoch - 278ms/step
Epoch 496/1000
2023-09-27 00:26:08.215 
Epoch 496/1000 
	 loss: 17.4843, MinusLogProbMetric: 17.4843, val_loss: 17.7124, val_MinusLogProbMetric: 17.7124

Epoch 496: val_loss improved from 17.81833 to 17.71240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 60s - loss: 17.4843 - MinusLogProbMetric: 17.4843 - val_loss: 17.7124 - val_MinusLogProbMetric: 17.7124 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 497/1000
2023-09-27 00:27:12.526 
Epoch 497/1000 
	 loss: 17.4922, MinusLogProbMetric: 17.4922, val_loss: 17.6144, val_MinusLogProbMetric: 17.6144

Epoch 497: val_loss improved from 17.71240 to 17.61445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 64s - loss: 17.4922 - MinusLogProbMetric: 17.4922 - val_loss: 17.6144 - val_MinusLogProbMetric: 17.6144 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 498/1000
2023-09-27 00:28:16.434 
Epoch 498/1000 
	 loss: 17.4794, MinusLogProbMetric: 17.4794, val_loss: 17.6569, val_MinusLogProbMetric: 17.6569

Epoch 498: val_loss did not improve from 17.61445
196/196 - 63s - loss: 17.4794 - MinusLogProbMetric: 17.4794 - val_loss: 17.6569 - val_MinusLogProbMetric: 17.6569 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 499/1000
2023-09-27 00:29:19.349 
Epoch 499/1000 
	 loss: 17.4872, MinusLogProbMetric: 17.4872, val_loss: 17.6282, val_MinusLogProbMetric: 17.6282

Epoch 499: val_loss did not improve from 17.61445
196/196 - 63s - loss: 17.4872 - MinusLogProbMetric: 17.4872 - val_loss: 17.6282 - val_MinusLogProbMetric: 17.6282 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 500/1000
2023-09-27 00:30:22.148 
Epoch 500/1000 
	 loss: 17.4838, MinusLogProbMetric: 17.4838, val_loss: 17.7002, val_MinusLogProbMetric: 17.7002

Epoch 500: val_loss did not improve from 17.61445
196/196 - 63s - loss: 17.4838 - MinusLogProbMetric: 17.4838 - val_loss: 17.7002 - val_MinusLogProbMetric: 17.7002 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 501/1000
2023-09-27 00:31:25.210 
Epoch 501/1000 
	 loss: 17.4762, MinusLogProbMetric: 17.4762, val_loss: 17.6545, val_MinusLogProbMetric: 17.6545

Epoch 501: val_loss did not improve from 17.61445
196/196 - 63s - loss: 17.4762 - MinusLogProbMetric: 17.4762 - val_loss: 17.6545 - val_MinusLogProbMetric: 17.6545 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 502/1000
2023-09-27 00:32:28.807 
Epoch 502/1000 
	 loss: 17.4879, MinusLogProbMetric: 17.4879, val_loss: 17.6505, val_MinusLogProbMetric: 17.6505

Epoch 502: val_loss did not improve from 17.61445
196/196 - 64s - loss: 17.4879 - MinusLogProbMetric: 17.4879 - val_loss: 17.6505 - val_MinusLogProbMetric: 17.6505 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 503/1000
2023-09-27 00:33:31.913 
Epoch 503/1000 
	 loss: 17.4829, MinusLogProbMetric: 17.4829, val_loss: 17.8736, val_MinusLogProbMetric: 17.8736

Epoch 503: val_loss did not improve from 17.61445
196/196 - 63s - loss: 17.4829 - MinusLogProbMetric: 17.4829 - val_loss: 17.8736 - val_MinusLogProbMetric: 17.8736 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 504/1000
2023-09-27 00:34:35.404 
Epoch 504/1000 
	 loss: 17.4968, MinusLogProbMetric: 17.4968, val_loss: 17.7223, val_MinusLogProbMetric: 17.7223

Epoch 504: val_loss did not improve from 17.61445
196/196 - 63s - loss: 17.4968 - MinusLogProbMetric: 17.4968 - val_loss: 17.7223 - val_MinusLogProbMetric: 17.7223 - lr: 1.6667e-04 - 63s/epoch - 324ms/step
Epoch 505/1000
2023-09-27 00:35:38.609 
Epoch 505/1000 
	 loss: 17.4763, MinusLogProbMetric: 17.4763, val_loss: 17.5283, val_MinusLogProbMetric: 17.5283

Epoch 505: val_loss improved from 17.61445 to 17.52832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 64s - loss: 17.4763 - MinusLogProbMetric: 17.4763 - val_loss: 17.5283 - val_MinusLogProbMetric: 17.5283 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 506/1000
2023-09-27 00:36:42.706 
Epoch 506/1000 
	 loss: 17.4725, MinusLogProbMetric: 17.4725, val_loss: 17.6470, val_MinusLogProbMetric: 17.6470

Epoch 506: val_loss did not improve from 17.52832
196/196 - 63s - loss: 17.4725 - MinusLogProbMetric: 17.4725 - val_loss: 17.6470 - val_MinusLogProbMetric: 17.6470 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 507/1000
2023-09-27 00:37:48.039 
Epoch 507/1000 
	 loss: 17.4831, MinusLogProbMetric: 17.4831, val_loss: 17.6780, val_MinusLogProbMetric: 17.6780

Epoch 507: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4831 - MinusLogProbMetric: 17.4831 - val_loss: 17.6780 - val_MinusLogProbMetric: 17.6780 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 508/1000
2023-09-27 00:38:54.445 
Epoch 508/1000 
	 loss: 17.4908, MinusLogProbMetric: 17.4908, val_loss: 17.6578, val_MinusLogProbMetric: 17.6578

Epoch 508: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4908 - MinusLogProbMetric: 17.4908 - val_loss: 17.6578 - val_MinusLogProbMetric: 17.6578 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 509/1000
2023-09-27 00:40:01.497 
Epoch 509/1000 
	 loss: 17.4721, MinusLogProbMetric: 17.4721, val_loss: 17.8664, val_MinusLogProbMetric: 17.8664

Epoch 509: val_loss did not improve from 17.52832
196/196 - 67s - loss: 17.4721 - MinusLogProbMetric: 17.4721 - val_loss: 17.8664 - val_MinusLogProbMetric: 17.8664 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 510/1000
2023-09-27 00:41:08.479 
Epoch 510/1000 
	 loss: 17.4634, MinusLogProbMetric: 17.4634, val_loss: 17.6600, val_MinusLogProbMetric: 17.6600

Epoch 510: val_loss did not improve from 17.52832
196/196 - 67s - loss: 17.4634 - MinusLogProbMetric: 17.4634 - val_loss: 17.6600 - val_MinusLogProbMetric: 17.6600 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 511/1000
2023-09-27 00:42:15.142 
Epoch 511/1000 
	 loss: 17.4747, MinusLogProbMetric: 17.4747, val_loss: 17.7370, val_MinusLogProbMetric: 17.7370

Epoch 511: val_loss did not improve from 17.52832
196/196 - 67s - loss: 17.4747 - MinusLogProbMetric: 17.4747 - val_loss: 17.7370 - val_MinusLogProbMetric: 17.7370 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 512/1000
2023-09-27 00:43:21.660 
Epoch 512/1000 
	 loss: 17.5140, MinusLogProbMetric: 17.5140, val_loss: 17.6310, val_MinusLogProbMetric: 17.6310

Epoch 512: val_loss did not improve from 17.52832
196/196 - 67s - loss: 17.5140 - MinusLogProbMetric: 17.5140 - val_loss: 17.6310 - val_MinusLogProbMetric: 17.6310 - lr: 1.6667e-04 - 67s/epoch - 339ms/step
Epoch 513/1000
2023-09-27 00:44:27.684 
Epoch 513/1000 
	 loss: 17.4688, MinusLogProbMetric: 17.4688, val_loss: 17.5794, val_MinusLogProbMetric: 17.5794

Epoch 513: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4688 - MinusLogProbMetric: 17.4688 - val_loss: 17.5794 - val_MinusLogProbMetric: 17.5794 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 514/1000
2023-09-27 00:45:34.071 
Epoch 514/1000 
	 loss: 17.4732, MinusLogProbMetric: 17.4732, val_loss: 17.6640, val_MinusLogProbMetric: 17.6640

Epoch 514: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4732 - MinusLogProbMetric: 17.4732 - val_loss: 17.6640 - val_MinusLogProbMetric: 17.6640 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 515/1000
2023-09-27 00:46:40.820 
Epoch 515/1000 
	 loss: 17.4806, MinusLogProbMetric: 17.4806, val_loss: 17.7904, val_MinusLogProbMetric: 17.7904

Epoch 515: val_loss did not improve from 17.52832
196/196 - 67s - loss: 17.4806 - MinusLogProbMetric: 17.4806 - val_loss: 17.7904 - val_MinusLogProbMetric: 17.7904 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 516/1000
2023-09-27 00:47:47.845 
Epoch 516/1000 
	 loss: 17.4845, MinusLogProbMetric: 17.4845, val_loss: 17.6185, val_MinusLogProbMetric: 17.6185

Epoch 516: val_loss did not improve from 17.52832
196/196 - 67s - loss: 17.4845 - MinusLogProbMetric: 17.4845 - val_loss: 17.6185 - val_MinusLogProbMetric: 17.6185 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 517/1000
2023-09-27 00:48:54.464 
Epoch 517/1000 
	 loss: 17.5160, MinusLogProbMetric: 17.5160, val_loss: 17.7413, val_MinusLogProbMetric: 17.7413

Epoch 517: val_loss did not improve from 17.52832
196/196 - 67s - loss: 17.5160 - MinusLogProbMetric: 17.5160 - val_loss: 17.7413 - val_MinusLogProbMetric: 17.7413 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 518/1000
2023-09-27 00:49:59.594 
Epoch 518/1000 
	 loss: 17.4709, MinusLogProbMetric: 17.4709, val_loss: 17.5966, val_MinusLogProbMetric: 17.5966

Epoch 518: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4709 - MinusLogProbMetric: 17.4709 - val_loss: 17.5966 - val_MinusLogProbMetric: 17.5966 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 519/1000
2023-09-27 00:51:05.483 
Epoch 519/1000 
	 loss: 17.4636, MinusLogProbMetric: 17.4636, val_loss: 17.7241, val_MinusLogProbMetric: 17.7241

Epoch 519: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4636 - MinusLogProbMetric: 17.4636 - val_loss: 17.7241 - val_MinusLogProbMetric: 17.7241 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 520/1000
2023-09-27 00:52:11.705 
Epoch 520/1000 
	 loss: 17.4603, MinusLogProbMetric: 17.4603, val_loss: 17.7351, val_MinusLogProbMetric: 17.7351

Epoch 520: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4603 - MinusLogProbMetric: 17.4603 - val_loss: 17.7351 - val_MinusLogProbMetric: 17.7351 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 521/1000
2023-09-27 00:53:18.331 
Epoch 521/1000 
	 loss: 17.4769, MinusLogProbMetric: 17.4769, val_loss: 17.7229, val_MinusLogProbMetric: 17.7229

Epoch 521: val_loss did not improve from 17.52832
196/196 - 67s - loss: 17.4769 - MinusLogProbMetric: 17.4769 - val_loss: 17.7229 - val_MinusLogProbMetric: 17.7229 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 522/1000
2023-09-27 00:54:23.621 
Epoch 522/1000 
	 loss: 17.4720, MinusLogProbMetric: 17.4720, val_loss: 17.6565, val_MinusLogProbMetric: 17.6565

Epoch 522: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4720 - MinusLogProbMetric: 17.4720 - val_loss: 17.6565 - val_MinusLogProbMetric: 17.6565 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 523/1000
2023-09-27 00:55:28.080 
Epoch 523/1000 
	 loss: 17.4541, MinusLogProbMetric: 17.4541, val_loss: 17.5786, val_MinusLogProbMetric: 17.5786

Epoch 523: val_loss did not improve from 17.52832
196/196 - 64s - loss: 17.4541 - MinusLogProbMetric: 17.4541 - val_loss: 17.5786 - val_MinusLogProbMetric: 17.5786 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 524/1000
2023-09-27 00:56:33.036 
Epoch 524/1000 
	 loss: 17.4634, MinusLogProbMetric: 17.4634, val_loss: 17.6635, val_MinusLogProbMetric: 17.6635

Epoch 524: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4634 - MinusLogProbMetric: 17.4634 - val_loss: 17.6635 - val_MinusLogProbMetric: 17.6635 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 525/1000
2023-09-27 00:57:37.622 
Epoch 525/1000 
	 loss: 17.4582, MinusLogProbMetric: 17.4582, val_loss: 17.6397, val_MinusLogProbMetric: 17.6397

Epoch 525: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4582 - MinusLogProbMetric: 17.4582 - val_loss: 17.6397 - val_MinusLogProbMetric: 17.6397 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 526/1000
2023-09-27 00:58:43.033 
Epoch 526/1000 
	 loss: 17.4519, MinusLogProbMetric: 17.4519, val_loss: 17.6618, val_MinusLogProbMetric: 17.6618

Epoch 526: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4519 - MinusLogProbMetric: 17.4519 - val_loss: 17.6618 - val_MinusLogProbMetric: 17.6618 - lr: 1.6667e-04 - 65s/epoch - 334ms/step
Epoch 527/1000
2023-09-27 00:59:47.198 
Epoch 527/1000 
	 loss: 17.4647, MinusLogProbMetric: 17.4647, val_loss: 17.6714, val_MinusLogProbMetric: 17.6714

Epoch 527: val_loss did not improve from 17.52832
196/196 - 64s - loss: 17.4647 - MinusLogProbMetric: 17.4647 - val_loss: 17.6714 - val_MinusLogProbMetric: 17.6714 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 528/1000
2023-09-27 01:00:52.816 
Epoch 528/1000 
	 loss: 17.4616, MinusLogProbMetric: 17.4616, val_loss: 17.6553, val_MinusLogProbMetric: 17.6553

Epoch 528: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4616 - MinusLogProbMetric: 17.4616 - val_loss: 17.6553 - val_MinusLogProbMetric: 17.6553 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 529/1000
2023-09-27 01:01:56.567 
Epoch 529/1000 
	 loss: 17.4753, MinusLogProbMetric: 17.4753, val_loss: 17.7120, val_MinusLogProbMetric: 17.7120

Epoch 529: val_loss did not improve from 17.52832
196/196 - 64s - loss: 17.4753 - MinusLogProbMetric: 17.4753 - val_loss: 17.7120 - val_MinusLogProbMetric: 17.7120 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 530/1000
2023-09-27 01:03:01.509 
Epoch 530/1000 
	 loss: 17.4651, MinusLogProbMetric: 17.4651, val_loss: 17.6685, val_MinusLogProbMetric: 17.6685

Epoch 530: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4651 - MinusLogProbMetric: 17.4651 - val_loss: 17.6685 - val_MinusLogProbMetric: 17.6685 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 531/1000
2023-09-27 01:04:06.826 
Epoch 531/1000 
	 loss: 17.4882, MinusLogProbMetric: 17.4882, val_loss: 17.5557, val_MinusLogProbMetric: 17.5557

Epoch 531: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4882 - MinusLogProbMetric: 17.4882 - val_loss: 17.5557 - val_MinusLogProbMetric: 17.5557 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 532/1000
2023-09-27 01:05:12.774 
Epoch 532/1000 
	 loss: 17.4575, MinusLogProbMetric: 17.4575, val_loss: 17.6184, val_MinusLogProbMetric: 17.6184

Epoch 532: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4575 - MinusLogProbMetric: 17.4575 - val_loss: 17.6184 - val_MinusLogProbMetric: 17.6184 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 533/1000
2023-09-27 01:06:17.644 
Epoch 533/1000 
	 loss: 17.4602, MinusLogProbMetric: 17.4602, val_loss: 17.8269, val_MinusLogProbMetric: 17.8269

Epoch 533: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4602 - MinusLogProbMetric: 17.4602 - val_loss: 17.8269 - val_MinusLogProbMetric: 17.8269 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 534/1000
2023-09-27 01:07:23.141 
Epoch 534/1000 
	 loss: 17.4640, MinusLogProbMetric: 17.4640, val_loss: 17.5899, val_MinusLogProbMetric: 17.5899

Epoch 534: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4640 - MinusLogProbMetric: 17.4640 - val_loss: 17.5899 - val_MinusLogProbMetric: 17.5899 - lr: 1.6667e-04 - 65s/epoch - 334ms/step
Epoch 535/1000
2023-09-27 01:08:29.017 
Epoch 535/1000 
	 loss: 17.4532, MinusLogProbMetric: 17.4532, val_loss: 17.6321, val_MinusLogProbMetric: 17.6321

Epoch 535: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4532 - MinusLogProbMetric: 17.4532 - val_loss: 17.6321 - val_MinusLogProbMetric: 17.6321 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 536/1000
2023-09-27 01:09:34.045 
Epoch 536/1000 
	 loss: 17.4422, MinusLogProbMetric: 17.4422, val_loss: 17.6734, val_MinusLogProbMetric: 17.6734

Epoch 536: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4422 - MinusLogProbMetric: 17.4422 - val_loss: 17.6734 - val_MinusLogProbMetric: 17.6734 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 537/1000
2023-09-27 01:10:38.693 
Epoch 537/1000 
	 loss: 17.4604, MinusLogProbMetric: 17.4604, val_loss: 17.8445, val_MinusLogProbMetric: 17.8445

Epoch 537: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4604 - MinusLogProbMetric: 17.4604 - val_loss: 17.8445 - val_MinusLogProbMetric: 17.8445 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 538/1000
2023-09-27 01:11:44.411 
Epoch 538/1000 
	 loss: 17.4734, MinusLogProbMetric: 17.4734, val_loss: 17.5927, val_MinusLogProbMetric: 17.5927

Epoch 538: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4734 - MinusLogProbMetric: 17.4734 - val_loss: 17.5927 - val_MinusLogProbMetric: 17.5927 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 539/1000
2023-09-27 01:12:49.468 
Epoch 539/1000 
	 loss: 17.4370, MinusLogProbMetric: 17.4370, val_loss: 17.6840, val_MinusLogProbMetric: 17.6840

Epoch 539: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4370 - MinusLogProbMetric: 17.4370 - val_loss: 17.6840 - val_MinusLogProbMetric: 17.6840 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 540/1000
2023-09-27 01:13:53.672 
Epoch 540/1000 
	 loss: 17.4628, MinusLogProbMetric: 17.4628, val_loss: 17.7389, val_MinusLogProbMetric: 17.7389

Epoch 540: val_loss did not improve from 17.52832
196/196 - 64s - loss: 17.4628 - MinusLogProbMetric: 17.4628 - val_loss: 17.7389 - val_MinusLogProbMetric: 17.7389 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 541/1000
2023-09-27 01:14:59.529 
Epoch 541/1000 
	 loss: 17.4555, MinusLogProbMetric: 17.4555, val_loss: 17.7080, val_MinusLogProbMetric: 17.7080

Epoch 541: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4555 - MinusLogProbMetric: 17.4555 - val_loss: 17.7080 - val_MinusLogProbMetric: 17.7080 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 542/1000
2023-09-27 01:16:04.693 
Epoch 542/1000 
	 loss: 17.4613, MinusLogProbMetric: 17.4613, val_loss: 17.5954, val_MinusLogProbMetric: 17.5954

Epoch 542: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4613 - MinusLogProbMetric: 17.4613 - val_loss: 17.5954 - val_MinusLogProbMetric: 17.5954 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 543/1000
2023-09-27 01:17:09.094 
Epoch 543/1000 
	 loss: 17.4489, MinusLogProbMetric: 17.4489, val_loss: 17.5571, val_MinusLogProbMetric: 17.5571

Epoch 543: val_loss did not improve from 17.52832
196/196 - 64s - loss: 17.4489 - MinusLogProbMetric: 17.4489 - val_loss: 17.5571 - val_MinusLogProbMetric: 17.5571 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 544/1000
2023-09-27 01:18:15.370 
Epoch 544/1000 
	 loss: 17.4485, MinusLogProbMetric: 17.4485, val_loss: 17.5947, val_MinusLogProbMetric: 17.5947

Epoch 544: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4485 - MinusLogProbMetric: 17.4485 - val_loss: 17.5947 - val_MinusLogProbMetric: 17.5947 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 545/1000
2023-09-27 01:19:21.260 
Epoch 545/1000 
	 loss: 17.4366, MinusLogProbMetric: 17.4366, val_loss: 17.5783, val_MinusLogProbMetric: 17.5783

Epoch 545: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4366 - MinusLogProbMetric: 17.4366 - val_loss: 17.5783 - val_MinusLogProbMetric: 17.5783 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 546/1000
2023-09-27 01:20:26.377 
Epoch 546/1000 
	 loss: 17.4528, MinusLogProbMetric: 17.4528, val_loss: 17.6902, val_MinusLogProbMetric: 17.6902

Epoch 546: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4528 - MinusLogProbMetric: 17.4528 - val_loss: 17.6902 - val_MinusLogProbMetric: 17.6902 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 547/1000
2023-09-27 01:21:31.654 
Epoch 547/1000 
	 loss: 17.4412, MinusLogProbMetric: 17.4412, val_loss: 17.6961, val_MinusLogProbMetric: 17.6961

Epoch 547: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4412 - MinusLogProbMetric: 17.4412 - val_loss: 17.6961 - val_MinusLogProbMetric: 17.6961 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 548/1000
2023-09-27 01:22:36.393 
Epoch 548/1000 
	 loss: 17.4280, MinusLogProbMetric: 17.4280, val_loss: 17.6175, val_MinusLogProbMetric: 17.6175

Epoch 548: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4280 - MinusLogProbMetric: 17.4280 - val_loss: 17.6175 - val_MinusLogProbMetric: 17.6175 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 549/1000
2023-09-27 01:23:42.060 
Epoch 549/1000 
	 loss: 17.4915, MinusLogProbMetric: 17.4915, val_loss: 17.6631, val_MinusLogProbMetric: 17.6631

Epoch 549: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4915 - MinusLogProbMetric: 17.4915 - val_loss: 17.6631 - val_MinusLogProbMetric: 17.6631 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 550/1000
2023-09-27 01:24:47.758 
Epoch 550/1000 
	 loss: 17.4307, MinusLogProbMetric: 17.4307, val_loss: 17.5974, val_MinusLogProbMetric: 17.5974

Epoch 550: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4307 - MinusLogProbMetric: 17.4307 - val_loss: 17.5974 - val_MinusLogProbMetric: 17.5974 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 551/1000
2023-09-27 01:25:53.040 
Epoch 551/1000 
	 loss: 17.4658, MinusLogProbMetric: 17.4658, val_loss: 17.6789, val_MinusLogProbMetric: 17.6789

Epoch 551: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4658 - MinusLogProbMetric: 17.4658 - val_loss: 17.6789 - val_MinusLogProbMetric: 17.6789 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 552/1000
2023-09-27 01:26:58.976 
Epoch 552/1000 
	 loss: 17.4796, MinusLogProbMetric: 17.4796, val_loss: 17.6590, val_MinusLogProbMetric: 17.6590

Epoch 552: val_loss did not improve from 17.52832
196/196 - 66s - loss: 17.4796 - MinusLogProbMetric: 17.4796 - val_loss: 17.6590 - val_MinusLogProbMetric: 17.6590 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 553/1000
2023-09-27 01:28:04.144 
Epoch 553/1000 
	 loss: 17.4263, MinusLogProbMetric: 17.4263, val_loss: 17.6450, val_MinusLogProbMetric: 17.6450

Epoch 553: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4263 - MinusLogProbMetric: 17.4263 - val_loss: 17.6450 - val_MinusLogProbMetric: 17.6450 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 554/1000
2023-09-27 01:29:08.977 
Epoch 554/1000 
	 loss: 17.4215, MinusLogProbMetric: 17.4215, val_loss: 17.7047, val_MinusLogProbMetric: 17.7047

Epoch 554: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4215 - MinusLogProbMetric: 17.4215 - val_loss: 17.7047 - val_MinusLogProbMetric: 17.7047 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 555/1000
2023-09-27 01:30:13.549 
Epoch 555/1000 
	 loss: 17.4316, MinusLogProbMetric: 17.4316, val_loss: 17.6788, val_MinusLogProbMetric: 17.6788

Epoch 555: val_loss did not improve from 17.52832
196/196 - 65s - loss: 17.4316 - MinusLogProbMetric: 17.4316 - val_loss: 17.6788 - val_MinusLogProbMetric: 17.6788 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 556/1000
2023-09-27 01:31:18.000 
Epoch 556/1000 
	 loss: 17.3281, MinusLogProbMetric: 17.3281, val_loss: 17.4738, val_MinusLogProbMetric: 17.4738

Epoch 556: val_loss improved from 17.52832 to 17.47376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 17.3281 - MinusLogProbMetric: 17.3281 - val_loss: 17.4738 - val_MinusLogProbMetric: 17.4738 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 557/1000
2023-09-27 01:32:24.119 
Epoch 557/1000 
	 loss: 17.3250, MinusLogProbMetric: 17.3250, val_loss: 17.4899, val_MinusLogProbMetric: 17.4899

Epoch 557: val_loss did not improve from 17.47376
196/196 - 65s - loss: 17.3250 - MinusLogProbMetric: 17.3250 - val_loss: 17.4899 - val_MinusLogProbMetric: 17.4899 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 558/1000
2023-09-27 01:33:30.025 
Epoch 558/1000 
	 loss: 17.3308, MinusLogProbMetric: 17.3308, val_loss: 17.5007, val_MinusLogProbMetric: 17.5007

Epoch 558: val_loss did not improve from 17.47376
196/196 - 66s - loss: 17.3308 - MinusLogProbMetric: 17.3308 - val_loss: 17.5007 - val_MinusLogProbMetric: 17.5007 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 559/1000
2023-09-27 01:34:36.690 
Epoch 559/1000 
	 loss: 17.3154, MinusLogProbMetric: 17.3154, val_loss: 17.4771, val_MinusLogProbMetric: 17.4771

Epoch 559: val_loss did not improve from 17.47376
196/196 - 67s - loss: 17.3154 - MinusLogProbMetric: 17.3154 - val_loss: 17.4771 - val_MinusLogProbMetric: 17.4771 - lr: 8.3333e-05 - 67s/epoch - 340ms/step
Epoch 560/1000
2023-09-27 01:35:42.374 
Epoch 560/1000 
	 loss: 17.3260, MinusLogProbMetric: 17.3260, val_loss: 17.5085, val_MinusLogProbMetric: 17.5085

Epoch 560: val_loss did not improve from 17.47376
196/196 - 66s - loss: 17.3260 - MinusLogProbMetric: 17.3260 - val_loss: 17.5085 - val_MinusLogProbMetric: 17.5085 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 561/1000
2023-09-27 01:36:49.914 
Epoch 561/1000 
	 loss: 17.3246, MinusLogProbMetric: 17.3246, val_loss: 17.4312, val_MinusLogProbMetric: 17.4312

Epoch 561: val_loss improved from 17.47376 to 17.43120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 69s - loss: 17.3246 - MinusLogProbMetric: 17.3246 - val_loss: 17.4312 - val_MinusLogProbMetric: 17.4312 - lr: 8.3333e-05 - 69s/epoch - 351ms/step
Epoch 562/1000
2023-09-27 01:37:57.194 
Epoch 562/1000 
	 loss: 17.3125, MinusLogProbMetric: 17.3125, val_loss: 17.5022, val_MinusLogProbMetric: 17.5022

Epoch 562: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3125 - MinusLogProbMetric: 17.3125 - val_loss: 17.5022 - val_MinusLogProbMetric: 17.5022 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 563/1000
2023-09-27 01:39:02.498 
Epoch 563/1000 
	 loss: 17.3064, MinusLogProbMetric: 17.3064, val_loss: 17.5195, val_MinusLogProbMetric: 17.5195

Epoch 563: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3064 - MinusLogProbMetric: 17.3064 - val_loss: 17.5195 - val_MinusLogProbMetric: 17.5195 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 564/1000
2023-09-27 01:40:06.996 
Epoch 564/1000 
	 loss: 17.3217, MinusLogProbMetric: 17.3217, val_loss: 17.4812, val_MinusLogProbMetric: 17.4812

Epoch 564: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3217 - MinusLogProbMetric: 17.3217 - val_loss: 17.4812 - val_MinusLogProbMetric: 17.4812 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 565/1000
2023-09-27 01:41:11.329 
Epoch 565/1000 
	 loss: 17.3177, MinusLogProbMetric: 17.3177, val_loss: 17.5768, val_MinusLogProbMetric: 17.5768

Epoch 565: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3177 - MinusLogProbMetric: 17.3177 - val_loss: 17.5768 - val_MinusLogProbMetric: 17.5768 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 566/1000
2023-09-27 01:42:14.211 
Epoch 566/1000 
	 loss: 17.3204, MinusLogProbMetric: 17.3204, val_loss: 17.4808, val_MinusLogProbMetric: 17.4808

Epoch 566: val_loss did not improve from 17.43120
196/196 - 63s - loss: 17.3204 - MinusLogProbMetric: 17.3204 - val_loss: 17.4808 - val_MinusLogProbMetric: 17.4808 - lr: 8.3333e-05 - 63s/epoch - 321ms/step
Epoch 567/1000
2023-09-27 01:43:16.754 
Epoch 567/1000 
	 loss: 17.3134, MinusLogProbMetric: 17.3134, val_loss: 17.5150, val_MinusLogProbMetric: 17.5150

Epoch 567: val_loss did not improve from 17.43120
196/196 - 63s - loss: 17.3134 - MinusLogProbMetric: 17.3134 - val_loss: 17.5150 - val_MinusLogProbMetric: 17.5150 - lr: 8.3333e-05 - 63s/epoch - 319ms/step
Epoch 568/1000
2023-09-27 01:44:20.694 
Epoch 568/1000 
	 loss: 17.3126, MinusLogProbMetric: 17.3126, val_loss: 17.4784, val_MinusLogProbMetric: 17.4784

Epoch 568: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3126 - MinusLogProbMetric: 17.3126 - val_loss: 17.4784 - val_MinusLogProbMetric: 17.4784 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 569/1000
2023-09-27 01:45:25.773 
Epoch 569/1000 
	 loss: 17.3132, MinusLogProbMetric: 17.3132, val_loss: 17.5123, val_MinusLogProbMetric: 17.5123

Epoch 569: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3132 - MinusLogProbMetric: 17.3132 - val_loss: 17.5123 - val_MinusLogProbMetric: 17.5123 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 570/1000
2023-09-27 01:46:30.378 
Epoch 570/1000 
	 loss: 17.3096, MinusLogProbMetric: 17.3096, val_loss: 17.5016, val_MinusLogProbMetric: 17.5016

Epoch 570: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3096 - MinusLogProbMetric: 17.3096 - val_loss: 17.5016 - val_MinusLogProbMetric: 17.5016 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 571/1000
2023-09-27 01:47:35.384 
Epoch 571/1000 
	 loss: 17.3085, MinusLogProbMetric: 17.3085, val_loss: 17.4384, val_MinusLogProbMetric: 17.4384

Epoch 571: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3085 - MinusLogProbMetric: 17.3085 - val_loss: 17.4384 - val_MinusLogProbMetric: 17.4384 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 572/1000
2023-09-27 01:48:39.981 
Epoch 572/1000 
	 loss: 17.3110, MinusLogProbMetric: 17.3110, val_loss: 17.5359, val_MinusLogProbMetric: 17.5359

Epoch 572: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3110 - MinusLogProbMetric: 17.3110 - val_loss: 17.5359 - val_MinusLogProbMetric: 17.5359 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 573/1000
2023-09-27 01:49:45.686 
Epoch 573/1000 
	 loss: 17.3137, MinusLogProbMetric: 17.3137, val_loss: 17.4663, val_MinusLogProbMetric: 17.4663

Epoch 573: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3137 - MinusLogProbMetric: 17.3137 - val_loss: 17.4663 - val_MinusLogProbMetric: 17.4663 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 574/1000
2023-09-27 01:50:49.819 
Epoch 574/1000 
	 loss: 17.3093, MinusLogProbMetric: 17.3093, val_loss: 17.4648, val_MinusLogProbMetric: 17.4648

Epoch 574: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3093 - MinusLogProbMetric: 17.3093 - val_loss: 17.4648 - val_MinusLogProbMetric: 17.4648 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 575/1000
2023-09-27 01:51:56.636 
Epoch 575/1000 
	 loss: 17.3069, MinusLogProbMetric: 17.3069, val_loss: 17.4759, val_MinusLogProbMetric: 17.4759

Epoch 575: val_loss did not improve from 17.43120
196/196 - 67s - loss: 17.3069 - MinusLogProbMetric: 17.3069 - val_loss: 17.4759 - val_MinusLogProbMetric: 17.4759 - lr: 8.3333e-05 - 67s/epoch - 341ms/step
Epoch 576/1000
2023-09-27 01:53:02.363 
Epoch 576/1000 
	 loss: 17.3108, MinusLogProbMetric: 17.3108, val_loss: 17.4990, val_MinusLogProbMetric: 17.4990

Epoch 576: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3108 - MinusLogProbMetric: 17.3108 - val_loss: 17.4990 - val_MinusLogProbMetric: 17.4990 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 577/1000
2023-09-27 01:54:06.426 
Epoch 577/1000 
	 loss: 17.3203, MinusLogProbMetric: 17.3203, val_loss: 17.5034, val_MinusLogProbMetric: 17.5034

Epoch 577: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3203 - MinusLogProbMetric: 17.3203 - val_loss: 17.5034 - val_MinusLogProbMetric: 17.5034 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 578/1000
2023-09-27 01:55:10.301 
Epoch 578/1000 
	 loss: 17.3215, MinusLogProbMetric: 17.3215, val_loss: 17.4843, val_MinusLogProbMetric: 17.4843

Epoch 578: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3215 - MinusLogProbMetric: 17.3215 - val_loss: 17.4843 - val_MinusLogProbMetric: 17.4843 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 579/1000
2023-09-27 01:56:15.608 
Epoch 579/1000 
	 loss: 17.3111, MinusLogProbMetric: 17.3111, val_loss: 17.4940, val_MinusLogProbMetric: 17.4940

Epoch 579: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3111 - MinusLogProbMetric: 17.3111 - val_loss: 17.4940 - val_MinusLogProbMetric: 17.4940 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 580/1000
2023-09-27 01:57:19.940 
Epoch 580/1000 
	 loss: 17.3047, MinusLogProbMetric: 17.3047, val_loss: 17.4943, val_MinusLogProbMetric: 17.4943

Epoch 580: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3047 - MinusLogProbMetric: 17.3047 - val_loss: 17.4943 - val_MinusLogProbMetric: 17.4943 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 581/1000
2023-09-27 01:58:25.161 
Epoch 581/1000 
	 loss: 17.3138, MinusLogProbMetric: 17.3138, val_loss: 17.4898, val_MinusLogProbMetric: 17.4898

Epoch 581: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3138 - MinusLogProbMetric: 17.3138 - val_loss: 17.4898 - val_MinusLogProbMetric: 17.4898 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 582/1000
2023-09-27 01:59:30.758 
Epoch 582/1000 
	 loss: 17.3238, MinusLogProbMetric: 17.3238, val_loss: 17.5659, val_MinusLogProbMetric: 17.5659

Epoch 582: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3238 - MinusLogProbMetric: 17.3238 - val_loss: 17.5659 - val_MinusLogProbMetric: 17.5659 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 583/1000
2023-09-27 02:00:35.659 
Epoch 583/1000 
	 loss: 17.3204, MinusLogProbMetric: 17.3204, val_loss: 17.4742, val_MinusLogProbMetric: 17.4742

Epoch 583: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3204 - MinusLogProbMetric: 17.3204 - val_loss: 17.4742 - val_MinusLogProbMetric: 17.4742 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 584/1000
2023-09-27 02:01:40.607 
Epoch 584/1000 
	 loss: 17.2964, MinusLogProbMetric: 17.2964, val_loss: 17.5127, val_MinusLogProbMetric: 17.5127

Epoch 584: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.2964 - MinusLogProbMetric: 17.2964 - val_loss: 17.5127 - val_MinusLogProbMetric: 17.5127 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 585/1000
2023-09-27 02:02:46.808 
Epoch 585/1000 
	 loss: 17.3113, MinusLogProbMetric: 17.3113, val_loss: 17.5380, val_MinusLogProbMetric: 17.5380

Epoch 585: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3113 - MinusLogProbMetric: 17.3113 - val_loss: 17.5380 - val_MinusLogProbMetric: 17.5380 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 586/1000
2023-09-27 02:03:51.843 
Epoch 586/1000 
	 loss: 17.3107, MinusLogProbMetric: 17.3107, val_loss: 17.4900, val_MinusLogProbMetric: 17.4900

Epoch 586: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3107 - MinusLogProbMetric: 17.3107 - val_loss: 17.4900 - val_MinusLogProbMetric: 17.4900 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 587/1000
2023-09-27 02:04:56.267 
Epoch 587/1000 
	 loss: 17.3113, MinusLogProbMetric: 17.3113, val_loss: 17.4615, val_MinusLogProbMetric: 17.4615

Epoch 587: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3113 - MinusLogProbMetric: 17.3113 - val_loss: 17.4615 - val_MinusLogProbMetric: 17.4615 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 588/1000
2023-09-27 02:06:02.514 
Epoch 588/1000 
	 loss: 17.3096, MinusLogProbMetric: 17.3096, val_loss: 17.4958, val_MinusLogProbMetric: 17.4958

Epoch 588: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3096 - MinusLogProbMetric: 17.3096 - val_loss: 17.4958 - val_MinusLogProbMetric: 17.4958 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 589/1000
2023-09-27 02:07:07.375 
Epoch 589/1000 
	 loss: 17.3071, MinusLogProbMetric: 17.3071, val_loss: 17.4641, val_MinusLogProbMetric: 17.4641

Epoch 589: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3071 - MinusLogProbMetric: 17.3071 - val_loss: 17.4641 - val_MinusLogProbMetric: 17.4641 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 590/1000
2023-09-27 02:08:13.166 
Epoch 590/1000 
	 loss: 17.3039, MinusLogProbMetric: 17.3039, val_loss: 17.4844, val_MinusLogProbMetric: 17.4844

Epoch 590: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3039 - MinusLogProbMetric: 17.3039 - val_loss: 17.4844 - val_MinusLogProbMetric: 17.4844 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 591/1000
2023-09-27 02:09:18.462 
Epoch 591/1000 
	 loss: 17.3225, MinusLogProbMetric: 17.3225, val_loss: 17.4871, val_MinusLogProbMetric: 17.4871

Epoch 591: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3225 - MinusLogProbMetric: 17.3225 - val_loss: 17.4871 - val_MinusLogProbMetric: 17.4871 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 592/1000
2023-09-27 02:10:23.164 
Epoch 592/1000 
	 loss: 17.3051, MinusLogProbMetric: 17.3051, val_loss: 17.5009, val_MinusLogProbMetric: 17.5009

Epoch 592: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3051 - MinusLogProbMetric: 17.3051 - val_loss: 17.5009 - val_MinusLogProbMetric: 17.5009 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 593/1000
2023-09-27 02:11:28.049 
Epoch 593/1000 
	 loss: 17.3019, MinusLogProbMetric: 17.3019, val_loss: 17.4486, val_MinusLogProbMetric: 17.4486

Epoch 593: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3019 - MinusLogProbMetric: 17.3019 - val_loss: 17.4486 - val_MinusLogProbMetric: 17.4486 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 594/1000
2023-09-27 02:12:33.511 
Epoch 594/1000 
	 loss: 17.3122, MinusLogProbMetric: 17.3122, val_loss: 17.4778, val_MinusLogProbMetric: 17.4778

Epoch 594: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3122 - MinusLogProbMetric: 17.3122 - val_loss: 17.4778 - val_MinusLogProbMetric: 17.4778 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 595/1000
2023-09-27 02:13:38.627 
Epoch 595/1000 
	 loss: 17.3155, MinusLogProbMetric: 17.3155, val_loss: 17.4405, val_MinusLogProbMetric: 17.4405

Epoch 595: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3155 - MinusLogProbMetric: 17.3155 - val_loss: 17.4405 - val_MinusLogProbMetric: 17.4405 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 596/1000
2023-09-27 02:14:43.126 
Epoch 596/1000 
	 loss: 17.3114, MinusLogProbMetric: 17.3114, val_loss: 17.4724, val_MinusLogProbMetric: 17.4724

Epoch 596: val_loss did not improve from 17.43120
196/196 - 64s - loss: 17.3114 - MinusLogProbMetric: 17.3114 - val_loss: 17.4724 - val_MinusLogProbMetric: 17.4724 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 597/1000
2023-09-27 02:15:47.759 
Epoch 597/1000 
	 loss: 17.3080, MinusLogProbMetric: 17.3080, val_loss: 17.4704, val_MinusLogProbMetric: 17.4704

Epoch 597: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3080 - MinusLogProbMetric: 17.3080 - val_loss: 17.4704 - val_MinusLogProbMetric: 17.4704 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 598/1000
2023-09-27 02:16:53.816 
Epoch 598/1000 
	 loss: 17.2973, MinusLogProbMetric: 17.2973, val_loss: 17.4512, val_MinusLogProbMetric: 17.4512

Epoch 598: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.2973 - MinusLogProbMetric: 17.2973 - val_loss: 17.4512 - val_MinusLogProbMetric: 17.4512 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 599/1000
2023-09-27 02:17:59.511 
Epoch 599/1000 
	 loss: 17.3045, MinusLogProbMetric: 17.3045, val_loss: 17.5189, val_MinusLogProbMetric: 17.5189

Epoch 599: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3045 - MinusLogProbMetric: 17.3045 - val_loss: 17.5189 - val_MinusLogProbMetric: 17.5189 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 600/1000
2023-09-27 02:19:04.452 
Epoch 600/1000 
	 loss: 17.3110, MinusLogProbMetric: 17.3110, val_loss: 17.5209, val_MinusLogProbMetric: 17.5209

Epoch 600: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3110 - MinusLogProbMetric: 17.3110 - val_loss: 17.5209 - val_MinusLogProbMetric: 17.5209 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 601/1000
2023-09-27 02:20:08.981 
Epoch 601/1000 
	 loss: 17.3126, MinusLogProbMetric: 17.3126, val_loss: 17.4638, val_MinusLogProbMetric: 17.4638

Epoch 601: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3126 - MinusLogProbMetric: 17.3126 - val_loss: 17.4638 - val_MinusLogProbMetric: 17.4638 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 602/1000
2023-09-27 02:21:14.621 
Epoch 602/1000 
	 loss: 17.3075, MinusLogProbMetric: 17.3075, val_loss: 17.5006, val_MinusLogProbMetric: 17.5006

Epoch 602: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3075 - MinusLogProbMetric: 17.3075 - val_loss: 17.5006 - val_MinusLogProbMetric: 17.5006 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 603/1000
2023-09-27 02:22:20.191 
Epoch 603/1000 
	 loss: 17.3019, MinusLogProbMetric: 17.3019, val_loss: 17.4483, val_MinusLogProbMetric: 17.4483

Epoch 603: val_loss did not improve from 17.43120
196/196 - 66s - loss: 17.3019 - MinusLogProbMetric: 17.3019 - val_loss: 17.4483 - val_MinusLogProbMetric: 17.4483 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 604/1000
2023-09-27 02:23:25.358 
Epoch 604/1000 
	 loss: 17.3207, MinusLogProbMetric: 17.3207, val_loss: 17.5297, val_MinusLogProbMetric: 17.5297

Epoch 604: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3207 - MinusLogProbMetric: 17.3207 - val_loss: 17.5297 - val_MinusLogProbMetric: 17.5297 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 605/1000
2023-09-27 02:24:29.929 
Epoch 605/1000 
	 loss: 17.3050, MinusLogProbMetric: 17.3050, val_loss: 17.6014, val_MinusLogProbMetric: 17.6014

Epoch 605: val_loss did not improve from 17.43120
196/196 - 65s - loss: 17.3050 - MinusLogProbMetric: 17.3050 - val_loss: 17.6014 - val_MinusLogProbMetric: 17.6014 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 606/1000
2023-09-27 02:25:32.687 
Epoch 606/1000 
	 loss: 17.2957, MinusLogProbMetric: 17.2957, val_loss: 17.5642, val_MinusLogProbMetric: 17.5642

Epoch 606: val_loss did not improve from 17.43120
196/196 - 63s - loss: 17.2957 - MinusLogProbMetric: 17.2957 - val_loss: 17.5642 - val_MinusLogProbMetric: 17.5642 - lr: 8.3333e-05 - 63s/epoch - 320ms/step
Epoch 607/1000
2023-09-27 02:26:35.750 
Epoch 607/1000 
	 loss: 17.3011, MinusLogProbMetric: 17.3011, val_loss: 17.4586, val_MinusLogProbMetric: 17.4586

Epoch 607: val_loss did not improve from 17.43120
196/196 - 63s - loss: 17.3011 - MinusLogProbMetric: 17.3011 - val_loss: 17.4586 - val_MinusLogProbMetric: 17.4586 - lr: 8.3333e-05 - 63s/epoch - 322ms/step
Epoch 608/1000
2023-09-27 02:27:38.501 
Epoch 608/1000 
	 loss: 17.3023, MinusLogProbMetric: 17.3023, val_loss: 17.4452, val_MinusLogProbMetric: 17.4452

Epoch 608: val_loss did not improve from 17.43120
196/196 - 63s - loss: 17.3023 - MinusLogProbMetric: 17.3023 - val_loss: 17.4452 - val_MinusLogProbMetric: 17.4452 - lr: 8.3333e-05 - 63s/epoch - 320ms/step
Epoch 609/1000
2023-09-27 02:28:40.297 
Epoch 609/1000 
	 loss: 17.3071, MinusLogProbMetric: 17.3071, val_loss: 17.5078, val_MinusLogProbMetric: 17.5078

Epoch 609: val_loss did not improve from 17.43120
196/196 - 62s - loss: 17.3071 - MinusLogProbMetric: 17.3071 - val_loss: 17.5078 - val_MinusLogProbMetric: 17.5078 - lr: 8.3333e-05 - 62s/epoch - 315ms/step
Epoch 610/1000
2023-09-27 02:29:42.650 
Epoch 610/1000 
	 loss: 17.2920, MinusLogProbMetric: 17.2920, val_loss: 17.4412, val_MinusLogProbMetric: 17.4412

Epoch 610: val_loss did not improve from 17.43120
196/196 - 62s - loss: 17.2920 - MinusLogProbMetric: 17.2920 - val_loss: 17.4412 - val_MinusLogProbMetric: 17.4412 - lr: 8.3333e-05 - 62s/epoch - 318ms/step
Epoch 611/1000
2023-09-27 02:30:44.930 
Epoch 611/1000 
	 loss: 17.2917, MinusLogProbMetric: 17.2917, val_loss: 17.4696, val_MinusLogProbMetric: 17.4696

Epoch 611: val_loss did not improve from 17.43120
196/196 - 62s - loss: 17.2917 - MinusLogProbMetric: 17.2917 - val_loss: 17.4696 - val_MinusLogProbMetric: 17.4696 - lr: 8.3333e-05 - 62s/epoch - 318ms/step
Epoch 612/1000
2023-09-27 02:31:44.971 
Epoch 612/1000 
	 loss: 17.2469, MinusLogProbMetric: 17.2469, val_loss: 17.4151, val_MinusLogProbMetric: 17.4151

Epoch 612: val_loss improved from 17.43120 to 17.41510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 61s - loss: 17.2469 - MinusLogProbMetric: 17.2469 - val_loss: 17.4151 - val_MinusLogProbMetric: 17.4151 - lr: 4.1667e-05 - 61s/epoch - 312ms/step
Epoch 613/1000
2023-09-27 02:32:42.388 
Epoch 613/1000 
	 loss: 17.2446, MinusLogProbMetric: 17.2446, val_loss: 17.4041, val_MinusLogProbMetric: 17.4041

Epoch 613: val_loss improved from 17.41510 to 17.40409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 58s - loss: 17.2446 - MinusLogProbMetric: 17.2446 - val_loss: 17.4041 - val_MinusLogProbMetric: 17.4041 - lr: 4.1667e-05 - 58s/epoch - 294ms/step
Epoch 614/1000
2023-09-27 02:33:44.926 
Epoch 614/1000 
	 loss: 17.2396, MinusLogProbMetric: 17.2396, val_loss: 17.3921, val_MinusLogProbMetric: 17.3921

Epoch 614: val_loss improved from 17.40409 to 17.39207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 63s - loss: 17.2396 - MinusLogProbMetric: 17.2396 - val_loss: 17.3921 - val_MinusLogProbMetric: 17.3921 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 615/1000
2023-09-27 02:34:47.422 
Epoch 615/1000 
	 loss: 17.2385, MinusLogProbMetric: 17.2385, val_loss: 17.4254, val_MinusLogProbMetric: 17.4254

Epoch 615: val_loss did not improve from 17.39207
196/196 - 61s - loss: 17.2385 - MinusLogProbMetric: 17.2385 - val_loss: 17.4254 - val_MinusLogProbMetric: 17.4254 - lr: 4.1667e-05 - 61s/epoch - 313ms/step
Epoch 616/1000
2023-09-27 02:35:49.106 
Epoch 616/1000 
	 loss: 17.2420, MinusLogProbMetric: 17.2420, val_loss: 17.4006, val_MinusLogProbMetric: 17.4006

Epoch 616: val_loss did not improve from 17.39207
196/196 - 62s - loss: 17.2420 - MinusLogProbMetric: 17.2420 - val_loss: 17.4006 - val_MinusLogProbMetric: 17.4006 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 617/1000
2023-09-27 02:36:51.740 
Epoch 617/1000 
	 loss: 17.2444, MinusLogProbMetric: 17.2444, val_loss: 17.4186, val_MinusLogProbMetric: 17.4186

Epoch 617: val_loss did not improve from 17.39207
196/196 - 63s - loss: 17.2444 - MinusLogProbMetric: 17.2444 - val_loss: 17.4186 - val_MinusLogProbMetric: 17.4186 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 618/1000
2023-09-27 02:37:54.222 
Epoch 618/1000 
	 loss: 17.2392, MinusLogProbMetric: 17.2392, val_loss: 17.4126, val_MinusLogProbMetric: 17.4126

Epoch 618: val_loss did not improve from 17.39207
196/196 - 62s - loss: 17.2392 - MinusLogProbMetric: 17.2392 - val_loss: 17.4126 - val_MinusLogProbMetric: 17.4126 - lr: 4.1667e-05 - 62s/epoch - 319ms/step
Epoch 619/1000
2023-09-27 02:38:56.864 
Epoch 619/1000 
	 loss: 17.2426, MinusLogProbMetric: 17.2426, val_loss: 17.4123, val_MinusLogProbMetric: 17.4123

Epoch 619: val_loss did not improve from 17.39207
196/196 - 63s - loss: 17.2426 - MinusLogProbMetric: 17.2426 - val_loss: 17.4123 - val_MinusLogProbMetric: 17.4123 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 620/1000
2023-09-27 02:39:59.399 
Epoch 620/1000 
	 loss: 17.2429, MinusLogProbMetric: 17.2429, val_loss: 17.3927, val_MinusLogProbMetric: 17.3927

Epoch 620: val_loss did not improve from 17.39207
196/196 - 63s - loss: 17.2429 - MinusLogProbMetric: 17.2429 - val_loss: 17.3927 - val_MinusLogProbMetric: 17.3927 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 621/1000
2023-09-27 02:41:02.100 
Epoch 621/1000 
	 loss: 17.2435, MinusLogProbMetric: 17.2435, val_loss: 17.4074, val_MinusLogProbMetric: 17.4074

Epoch 621: val_loss did not improve from 17.39207
196/196 - 63s - loss: 17.2435 - MinusLogProbMetric: 17.2435 - val_loss: 17.4074 - val_MinusLogProbMetric: 17.4074 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 622/1000
2023-09-27 02:42:04.697 
Epoch 622/1000 
	 loss: 17.2395, MinusLogProbMetric: 17.2395, val_loss: 17.3901, val_MinusLogProbMetric: 17.3901

Epoch 622: val_loss improved from 17.39207 to 17.39005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 64s - loss: 17.2395 - MinusLogProbMetric: 17.2395 - val_loss: 17.3901 - val_MinusLogProbMetric: 17.3901 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 623/1000
2023-09-27 02:43:08.815 
Epoch 623/1000 
	 loss: 17.2406, MinusLogProbMetric: 17.2406, val_loss: 17.4124, val_MinusLogProbMetric: 17.4124

Epoch 623: val_loss did not improve from 17.39005
196/196 - 63s - loss: 17.2406 - MinusLogProbMetric: 17.2406 - val_loss: 17.4124 - val_MinusLogProbMetric: 17.4124 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 624/1000
2023-09-27 02:44:11.206 
Epoch 624/1000 
	 loss: 17.2414, MinusLogProbMetric: 17.2414, val_loss: 17.4161, val_MinusLogProbMetric: 17.4161

Epoch 624: val_loss did not improve from 17.39005
196/196 - 62s - loss: 17.2414 - MinusLogProbMetric: 17.2414 - val_loss: 17.4161 - val_MinusLogProbMetric: 17.4161 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 625/1000
2023-09-27 02:45:13.623 
Epoch 625/1000 
	 loss: 17.2444, MinusLogProbMetric: 17.2444, val_loss: 17.4265, val_MinusLogProbMetric: 17.4265

Epoch 625: val_loss did not improve from 17.39005
196/196 - 62s - loss: 17.2444 - MinusLogProbMetric: 17.2444 - val_loss: 17.4265 - val_MinusLogProbMetric: 17.4265 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 626/1000
2023-09-27 02:46:17.111 
Epoch 626/1000 
	 loss: 17.2361, MinusLogProbMetric: 17.2361, val_loss: 17.4087, val_MinusLogProbMetric: 17.4087

Epoch 626: val_loss did not improve from 17.39005
196/196 - 63s - loss: 17.2361 - MinusLogProbMetric: 17.2361 - val_loss: 17.4087 - val_MinusLogProbMetric: 17.4087 - lr: 4.1667e-05 - 63s/epoch - 324ms/step
Epoch 627/1000
2023-09-27 02:47:22.422 
Epoch 627/1000 
	 loss: 17.2359, MinusLogProbMetric: 17.2359, val_loss: 17.4126, val_MinusLogProbMetric: 17.4126

Epoch 627: val_loss did not improve from 17.39005
196/196 - 65s - loss: 17.2359 - MinusLogProbMetric: 17.2359 - val_loss: 17.4126 - val_MinusLogProbMetric: 17.4126 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 628/1000
2023-09-27 02:48:28.280 
Epoch 628/1000 
	 loss: 17.2341, MinusLogProbMetric: 17.2341, val_loss: 17.3957, val_MinusLogProbMetric: 17.3957

Epoch 628: val_loss did not improve from 17.39005
196/196 - 66s - loss: 17.2341 - MinusLogProbMetric: 17.2341 - val_loss: 17.3957 - val_MinusLogProbMetric: 17.3957 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 629/1000
2023-09-27 02:49:32.614 
Epoch 629/1000 
	 loss: 17.2385, MinusLogProbMetric: 17.2385, val_loss: 17.4136, val_MinusLogProbMetric: 17.4136

Epoch 629: val_loss did not improve from 17.39005
196/196 - 64s - loss: 17.2385 - MinusLogProbMetric: 17.2385 - val_loss: 17.4136 - val_MinusLogProbMetric: 17.4136 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 630/1000
2023-09-27 02:50:37.804 
Epoch 630/1000 
	 loss: 17.2346, MinusLogProbMetric: 17.2346, val_loss: 17.4034, val_MinusLogProbMetric: 17.4034

Epoch 630: val_loss did not improve from 17.39005
196/196 - 65s - loss: 17.2346 - MinusLogProbMetric: 17.2346 - val_loss: 17.4034 - val_MinusLogProbMetric: 17.4034 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 631/1000
2023-09-27 02:51:43.144 
Epoch 631/1000 
	 loss: 17.2379, MinusLogProbMetric: 17.2379, val_loss: 17.4385, val_MinusLogProbMetric: 17.4385

Epoch 631: val_loss did not improve from 17.39005
196/196 - 65s - loss: 17.2379 - MinusLogProbMetric: 17.2379 - val_loss: 17.4385 - val_MinusLogProbMetric: 17.4385 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 632/1000
2023-09-27 02:52:49.035 
Epoch 632/1000 
	 loss: 17.2388, MinusLogProbMetric: 17.2388, val_loss: 17.4115, val_MinusLogProbMetric: 17.4115

Epoch 632: val_loss did not improve from 17.39005
196/196 - 66s - loss: 17.2388 - MinusLogProbMetric: 17.2388 - val_loss: 17.4115 - val_MinusLogProbMetric: 17.4115 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 633/1000
2023-09-27 02:53:54.183 
Epoch 633/1000 
	 loss: 17.2385, MinusLogProbMetric: 17.2385, val_loss: 17.4067, val_MinusLogProbMetric: 17.4067

Epoch 633: val_loss did not improve from 17.39005
196/196 - 65s - loss: 17.2385 - MinusLogProbMetric: 17.2385 - val_loss: 17.4067 - val_MinusLogProbMetric: 17.4067 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 634/1000
2023-09-27 02:55:01.160 
Epoch 634/1000 
	 loss: 17.2328, MinusLogProbMetric: 17.2328, val_loss: 17.4070, val_MinusLogProbMetric: 17.4070

Epoch 634: val_loss did not improve from 17.39005
196/196 - 67s - loss: 17.2328 - MinusLogProbMetric: 17.2328 - val_loss: 17.4070 - val_MinusLogProbMetric: 17.4070 - lr: 4.1667e-05 - 67s/epoch - 342ms/step
Epoch 635/1000
2023-09-27 02:56:05.230 
Epoch 635/1000 
	 loss: 17.2328, MinusLogProbMetric: 17.2328, val_loss: 17.4034, val_MinusLogProbMetric: 17.4034

Epoch 635: val_loss did not improve from 17.39005
196/196 - 64s - loss: 17.2328 - MinusLogProbMetric: 17.2328 - val_loss: 17.4034 - val_MinusLogProbMetric: 17.4034 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 636/1000
2023-09-27 02:57:10.293 
Epoch 636/1000 
	 loss: 17.2351, MinusLogProbMetric: 17.2351, val_loss: 17.3927, val_MinusLogProbMetric: 17.3927

Epoch 636: val_loss did not improve from 17.39005
196/196 - 65s - loss: 17.2351 - MinusLogProbMetric: 17.2351 - val_loss: 17.3927 - val_MinusLogProbMetric: 17.3927 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 637/1000
2023-09-27 02:58:14.741 
Epoch 637/1000 
	 loss: 17.2390, MinusLogProbMetric: 17.2390, val_loss: 17.4086, val_MinusLogProbMetric: 17.4086

Epoch 637: val_loss did not improve from 17.39005
196/196 - 64s - loss: 17.2390 - MinusLogProbMetric: 17.2390 - val_loss: 17.4086 - val_MinusLogProbMetric: 17.4086 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 638/1000
2023-09-27 02:59:19.775 
Epoch 638/1000 
	 loss: 17.2380, MinusLogProbMetric: 17.2380, val_loss: 17.4031, val_MinusLogProbMetric: 17.4031

Epoch 638: val_loss did not improve from 17.39005
196/196 - 65s - loss: 17.2380 - MinusLogProbMetric: 17.2380 - val_loss: 17.4031 - val_MinusLogProbMetric: 17.4031 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 639/1000
2023-09-27 03:00:25.085 
Epoch 639/1000 
	 loss: 17.2311, MinusLogProbMetric: 17.2311, val_loss: 17.4252, val_MinusLogProbMetric: 17.4252

Epoch 639: val_loss did not improve from 17.39005
196/196 - 65s - loss: 17.2311 - MinusLogProbMetric: 17.2311 - val_loss: 17.4252 - val_MinusLogProbMetric: 17.4252 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 640/1000
2023-09-27 03:01:29.837 
Epoch 640/1000 
	 loss: 17.2388, MinusLogProbMetric: 17.2388, val_loss: 17.4057, val_MinusLogProbMetric: 17.4057

Epoch 640: val_loss did not improve from 17.39005
196/196 - 65s - loss: 17.2388 - MinusLogProbMetric: 17.2388 - val_loss: 17.4057 - val_MinusLogProbMetric: 17.4057 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 641/1000
2023-09-27 03:02:35.431 
Epoch 641/1000 
	 loss: 17.2349, MinusLogProbMetric: 17.2349, val_loss: 17.3955, val_MinusLogProbMetric: 17.3955

Epoch 641: val_loss did not improve from 17.39005
196/196 - 66s - loss: 17.2349 - MinusLogProbMetric: 17.2349 - val_loss: 17.3955 - val_MinusLogProbMetric: 17.3955 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 642/1000
2023-09-27 03:03:39.592 
Epoch 642/1000 
	 loss: 17.2346, MinusLogProbMetric: 17.2346, val_loss: 17.3947, val_MinusLogProbMetric: 17.3947

Epoch 642: val_loss did not improve from 17.39005
196/196 - 64s - loss: 17.2346 - MinusLogProbMetric: 17.2346 - val_loss: 17.3947 - val_MinusLogProbMetric: 17.3947 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 643/1000
2023-09-27 03:04:44.874 
Epoch 643/1000 
	 loss: 17.2307, MinusLogProbMetric: 17.2307, val_loss: 17.3889, val_MinusLogProbMetric: 17.3889

Epoch 643: val_loss improved from 17.39005 to 17.38886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 17.2307 - MinusLogProbMetric: 17.2307 - val_loss: 17.3889 - val_MinusLogProbMetric: 17.3889 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 644/1000
2023-09-27 03:05:51.370 
Epoch 644/1000 
	 loss: 17.2374, MinusLogProbMetric: 17.2374, val_loss: 17.4007, val_MinusLogProbMetric: 17.4007

Epoch 644: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2374 - MinusLogProbMetric: 17.2374 - val_loss: 17.4007 - val_MinusLogProbMetric: 17.4007 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 645/1000
2023-09-27 03:06:55.415 
Epoch 645/1000 
	 loss: 17.2353, MinusLogProbMetric: 17.2353, val_loss: 17.4866, val_MinusLogProbMetric: 17.4866

Epoch 645: val_loss did not improve from 17.38886
196/196 - 64s - loss: 17.2353 - MinusLogProbMetric: 17.2353 - val_loss: 17.4866 - val_MinusLogProbMetric: 17.4866 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 646/1000
2023-09-27 03:07:59.656 
Epoch 646/1000 
	 loss: 17.2392, MinusLogProbMetric: 17.2392, val_loss: 17.4085, val_MinusLogProbMetric: 17.4085

Epoch 646: val_loss did not improve from 17.38886
196/196 - 64s - loss: 17.2392 - MinusLogProbMetric: 17.2392 - val_loss: 17.4085 - val_MinusLogProbMetric: 17.4085 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 647/1000
2023-09-27 03:09:04.345 
Epoch 647/1000 
	 loss: 17.2371, MinusLogProbMetric: 17.2371, val_loss: 17.4127, val_MinusLogProbMetric: 17.4127

Epoch 647: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2371 - MinusLogProbMetric: 17.2371 - val_loss: 17.4127 - val_MinusLogProbMetric: 17.4127 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 648/1000
2023-09-27 03:10:09.536 
Epoch 648/1000 
	 loss: 17.2332, MinusLogProbMetric: 17.2332, val_loss: 17.3994, val_MinusLogProbMetric: 17.3994

Epoch 648: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2332 - MinusLogProbMetric: 17.2332 - val_loss: 17.3994 - val_MinusLogProbMetric: 17.3994 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 649/1000
2023-09-27 03:11:14.503 
Epoch 649/1000 
	 loss: 17.2413, MinusLogProbMetric: 17.2413, val_loss: 17.4127, val_MinusLogProbMetric: 17.4127

Epoch 649: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2413 - MinusLogProbMetric: 17.2413 - val_loss: 17.4127 - val_MinusLogProbMetric: 17.4127 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 650/1000
2023-09-27 03:12:18.708 
Epoch 650/1000 
	 loss: 17.2331, MinusLogProbMetric: 17.2331, val_loss: 17.4656, val_MinusLogProbMetric: 17.4656

Epoch 650: val_loss did not improve from 17.38886
196/196 - 64s - loss: 17.2331 - MinusLogProbMetric: 17.2331 - val_loss: 17.4656 - val_MinusLogProbMetric: 17.4656 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 651/1000
2023-09-27 03:13:23.553 
Epoch 651/1000 
	 loss: 17.2290, MinusLogProbMetric: 17.2290, val_loss: 17.4019, val_MinusLogProbMetric: 17.4019

Epoch 651: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2290 - MinusLogProbMetric: 17.2290 - val_loss: 17.4019 - val_MinusLogProbMetric: 17.4019 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 652/1000
2023-09-27 03:14:29.763 
Epoch 652/1000 
	 loss: 17.2379, MinusLogProbMetric: 17.2379, val_loss: 17.4417, val_MinusLogProbMetric: 17.4417

Epoch 652: val_loss did not improve from 17.38886
196/196 - 66s - loss: 17.2379 - MinusLogProbMetric: 17.2379 - val_loss: 17.4417 - val_MinusLogProbMetric: 17.4417 - lr: 4.1667e-05 - 66s/epoch - 338ms/step
Epoch 653/1000
2023-09-27 03:15:34.567 
Epoch 653/1000 
	 loss: 17.2288, MinusLogProbMetric: 17.2288, val_loss: 17.4228, val_MinusLogProbMetric: 17.4228

Epoch 653: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2288 - MinusLogProbMetric: 17.2288 - val_loss: 17.4228 - val_MinusLogProbMetric: 17.4228 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 654/1000
2023-09-27 03:16:38.947 
Epoch 654/1000 
	 loss: 17.2378, MinusLogProbMetric: 17.2378, val_loss: 17.3980, val_MinusLogProbMetric: 17.3980

Epoch 654: val_loss did not improve from 17.38886
196/196 - 64s - loss: 17.2378 - MinusLogProbMetric: 17.2378 - val_loss: 17.3980 - val_MinusLogProbMetric: 17.3980 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 655/1000
2023-09-27 03:17:43.764 
Epoch 655/1000 
	 loss: 17.2339, MinusLogProbMetric: 17.2339, val_loss: 17.3913, val_MinusLogProbMetric: 17.3913

Epoch 655: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2339 - MinusLogProbMetric: 17.2339 - val_loss: 17.3913 - val_MinusLogProbMetric: 17.3913 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 656/1000
2023-09-27 03:18:49.153 
Epoch 656/1000 
	 loss: 17.2360, MinusLogProbMetric: 17.2360, val_loss: 17.4037, val_MinusLogProbMetric: 17.4037

Epoch 656: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2360 - MinusLogProbMetric: 17.2360 - val_loss: 17.4037 - val_MinusLogProbMetric: 17.4037 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 657/1000
2023-09-27 03:19:55.119 
Epoch 657/1000 
	 loss: 17.2330, MinusLogProbMetric: 17.2330, val_loss: 17.4107, val_MinusLogProbMetric: 17.4107

Epoch 657: val_loss did not improve from 17.38886
196/196 - 66s - loss: 17.2330 - MinusLogProbMetric: 17.2330 - val_loss: 17.4107 - val_MinusLogProbMetric: 17.4107 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 658/1000
2023-09-27 03:21:00.810 
Epoch 658/1000 
	 loss: 17.2297, MinusLogProbMetric: 17.2297, val_loss: 17.3964, val_MinusLogProbMetric: 17.3964

Epoch 658: val_loss did not improve from 17.38886
196/196 - 66s - loss: 17.2297 - MinusLogProbMetric: 17.2297 - val_loss: 17.3964 - val_MinusLogProbMetric: 17.3964 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 659/1000
2023-09-27 03:22:06.051 
Epoch 659/1000 
	 loss: 17.2270, MinusLogProbMetric: 17.2270, val_loss: 17.3898, val_MinusLogProbMetric: 17.3898

Epoch 659: val_loss did not improve from 17.38886
196/196 - 65s - loss: 17.2270 - MinusLogProbMetric: 17.2270 - val_loss: 17.3898 - val_MinusLogProbMetric: 17.3898 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 660/1000
2023-09-27 03:23:11.470 
Epoch 660/1000 
	 loss: 17.2270, MinusLogProbMetric: 17.2270, val_loss: 17.3819, val_MinusLogProbMetric: 17.3819

Epoch 660: val_loss improved from 17.38886 to 17.38193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 17.2270 - MinusLogProbMetric: 17.2270 - val_loss: 17.3819 - val_MinusLogProbMetric: 17.3819 - lr: 4.1667e-05 - 67s/epoch - 341ms/step
Epoch 661/1000
2023-09-27 03:24:17.201 
Epoch 661/1000 
	 loss: 17.2294, MinusLogProbMetric: 17.2294, val_loss: 17.3955, val_MinusLogProbMetric: 17.3955

Epoch 661: val_loss did not improve from 17.38193
196/196 - 64s - loss: 17.2294 - MinusLogProbMetric: 17.2294 - val_loss: 17.3955 - val_MinusLogProbMetric: 17.3955 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 662/1000
2023-09-27 03:25:23.851 
Epoch 662/1000 
	 loss: 17.2324, MinusLogProbMetric: 17.2324, val_loss: 17.4023, val_MinusLogProbMetric: 17.4023

Epoch 662: val_loss did not improve from 17.38193
196/196 - 67s - loss: 17.2324 - MinusLogProbMetric: 17.2324 - val_loss: 17.4023 - val_MinusLogProbMetric: 17.4023 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 663/1000
2023-09-27 03:26:28.423 
Epoch 663/1000 
	 loss: 17.2300, MinusLogProbMetric: 17.2300, val_loss: 17.4070, val_MinusLogProbMetric: 17.4070

Epoch 663: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2300 - MinusLogProbMetric: 17.2300 - val_loss: 17.4070 - val_MinusLogProbMetric: 17.4070 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 664/1000
2023-09-27 03:27:34.491 
Epoch 664/1000 
	 loss: 17.2295, MinusLogProbMetric: 17.2295, val_loss: 17.3950, val_MinusLogProbMetric: 17.3950

Epoch 664: val_loss did not improve from 17.38193
196/196 - 66s - loss: 17.2295 - MinusLogProbMetric: 17.2295 - val_loss: 17.3950 - val_MinusLogProbMetric: 17.3950 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 665/1000
2023-09-27 03:28:40.080 
Epoch 665/1000 
	 loss: 17.2366, MinusLogProbMetric: 17.2366, val_loss: 17.3961, val_MinusLogProbMetric: 17.3961

Epoch 665: val_loss did not improve from 17.38193
196/196 - 66s - loss: 17.2366 - MinusLogProbMetric: 17.2366 - val_loss: 17.3961 - val_MinusLogProbMetric: 17.3961 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 666/1000
2023-09-27 03:29:45.189 
Epoch 666/1000 
	 loss: 17.2292, MinusLogProbMetric: 17.2292, val_loss: 17.3982, val_MinusLogProbMetric: 17.3982

Epoch 666: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2292 - MinusLogProbMetric: 17.2292 - val_loss: 17.3982 - val_MinusLogProbMetric: 17.3982 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 667/1000
2023-09-27 03:30:49.179 
Epoch 667/1000 
	 loss: 17.2281, MinusLogProbMetric: 17.2281, val_loss: 17.4257, val_MinusLogProbMetric: 17.4257

Epoch 667: val_loss did not improve from 17.38193
196/196 - 64s - loss: 17.2281 - MinusLogProbMetric: 17.2281 - val_loss: 17.4257 - val_MinusLogProbMetric: 17.4257 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 668/1000
2023-09-27 03:31:54.067 
Epoch 668/1000 
	 loss: 17.2322, MinusLogProbMetric: 17.2322, val_loss: 17.4625, val_MinusLogProbMetric: 17.4625

Epoch 668: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2322 - MinusLogProbMetric: 17.2322 - val_loss: 17.4625 - val_MinusLogProbMetric: 17.4625 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 669/1000
2023-09-27 03:32:59.402 
Epoch 669/1000 
	 loss: 17.2276, MinusLogProbMetric: 17.2276, val_loss: 17.3992, val_MinusLogProbMetric: 17.3992

Epoch 669: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2276 - MinusLogProbMetric: 17.2276 - val_loss: 17.3992 - val_MinusLogProbMetric: 17.3992 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 670/1000
2023-09-27 03:34:04.203 
Epoch 670/1000 
	 loss: 17.2321, MinusLogProbMetric: 17.2321, val_loss: 17.4223, val_MinusLogProbMetric: 17.4223

Epoch 670: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2321 - MinusLogProbMetric: 17.2321 - val_loss: 17.4223 - val_MinusLogProbMetric: 17.4223 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 671/1000
2023-09-27 03:35:09.115 
Epoch 671/1000 
	 loss: 17.2320, MinusLogProbMetric: 17.2320, val_loss: 17.4142, val_MinusLogProbMetric: 17.4142

Epoch 671: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2320 - MinusLogProbMetric: 17.2320 - val_loss: 17.4142 - val_MinusLogProbMetric: 17.4142 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 672/1000
2023-09-27 03:36:13.462 
Epoch 672/1000 
	 loss: 17.2305, MinusLogProbMetric: 17.2305, val_loss: 17.4082, val_MinusLogProbMetric: 17.4082

Epoch 672: val_loss did not improve from 17.38193
196/196 - 64s - loss: 17.2305 - MinusLogProbMetric: 17.2305 - val_loss: 17.4082 - val_MinusLogProbMetric: 17.4082 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 673/1000
2023-09-27 03:37:18.279 
Epoch 673/1000 
	 loss: 17.2317, MinusLogProbMetric: 17.2317, val_loss: 17.4063, val_MinusLogProbMetric: 17.4063

Epoch 673: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2317 - MinusLogProbMetric: 17.2317 - val_loss: 17.4063 - val_MinusLogProbMetric: 17.4063 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 674/1000
2023-09-27 03:38:22.932 
Epoch 674/1000 
	 loss: 17.2315, MinusLogProbMetric: 17.2315, val_loss: 17.3985, val_MinusLogProbMetric: 17.3985

Epoch 674: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2315 - MinusLogProbMetric: 17.2315 - val_loss: 17.3985 - val_MinusLogProbMetric: 17.3985 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 675/1000
2023-09-27 03:39:28.045 
Epoch 675/1000 
	 loss: 17.2278, MinusLogProbMetric: 17.2278, val_loss: 17.3903, val_MinusLogProbMetric: 17.3903

Epoch 675: val_loss did not improve from 17.38193
196/196 - 65s - loss: 17.2278 - MinusLogProbMetric: 17.2278 - val_loss: 17.3903 - val_MinusLogProbMetric: 17.3903 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 676/1000
2023-09-27 03:40:32.871 
Epoch 676/1000 
	 loss: 17.2256, MinusLogProbMetric: 17.2256, val_loss: 17.3762, val_MinusLogProbMetric: 17.3762

Epoch 676: val_loss improved from 17.38193 to 17.37622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 17.2256 - MinusLogProbMetric: 17.2256 - val_loss: 17.3762 - val_MinusLogProbMetric: 17.3762 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 677/1000
2023-09-27 03:41:39.346 
Epoch 677/1000 
	 loss: 17.2279, MinusLogProbMetric: 17.2279, val_loss: 17.3956, val_MinusLogProbMetric: 17.3956

Epoch 677: val_loss did not improve from 17.37622
196/196 - 65s - loss: 17.2279 - MinusLogProbMetric: 17.2279 - val_loss: 17.3956 - val_MinusLogProbMetric: 17.3956 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 678/1000
2023-09-27 03:42:43.958 
Epoch 678/1000 
	 loss: 17.2204, MinusLogProbMetric: 17.2204, val_loss: 17.3878, val_MinusLogProbMetric: 17.3878

Epoch 678: val_loss did not improve from 17.37622
196/196 - 65s - loss: 17.2204 - MinusLogProbMetric: 17.2204 - val_loss: 17.3878 - val_MinusLogProbMetric: 17.3878 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 679/1000
2023-09-27 03:43:48.848 
Epoch 679/1000 
	 loss: 17.2325, MinusLogProbMetric: 17.2325, val_loss: 17.4363, val_MinusLogProbMetric: 17.4363

Epoch 679: val_loss did not improve from 17.37622
196/196 - 65s - loss: 17.2325 - MinusLogProbMetric: 17.2325 - val_loss: 17.4363 - val_MinusLogProbMetric: 17.4363 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 680/1000
2023-09-27 03:44:54.169 
Epoch 680/1000 
	 loss: 17.2264, MinusLogProbMetric: 17.2264, val_loss: 17.3903, val_MinusLogProbMetric: 17.3903

Epoch 680: val_loss did not improve from 17.37622
196/196 - 65s - loss: 17.2264 - MinusLogProbMetric: 17.2264 - val_loss: 17.3903 - val_MinusLogProbMetric: 17.3903 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 681/1000
2023-09-27 03:45:58.389 
Epoch 681/1000 
	 loss: 17.2247, MinusLogProbMetric: 17.2247, val_loss: 17.4145, val_MinusLogProbMetric: 17.4145

Epoch 681: val_loss did not improve from 17.37622
196/196 - 64s - loss: 17.2247 - MinusLogProbMetric: 17.2247 - val_loss: 17.4145 - val_MinusLogProbMetric: 17.4145 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 682/1000
2023-09-27 03:47:04.124 
Epoch 682/1000 
	 loss: 17.2224, MinusLogProbMetric: 17.2224, val_loss: 17.4048, val_MinusLogProbMetric: 17.4048

Epoch 682: val_loss did not improve from 17.37622
196/196 - 66s - loss: 17.2224 - MinusLogProbMetric: 17.2224 - val_loss: 17.4048 - val_MinusLogProbMetric: 17.4048 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 683/1000
2023-09-27 03:48:07.743 
Epoch 683/1000 
	 loss: 17.2244, MinusLogProbMetric: 17.2244, val_loss: 17.3646, val_MinusLogProbMetric: 17.3646

Epoch 683: val_loss improved from 17.37622 to 17.36456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 17.2244 - MinusLogProbMetric: 17.2244 - val_loss: 17.3646 - val_MinusLogProbMetric: 17.3646 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 684/1000
2023-09-27 03:49:14.020 
Epoch 684/1000 
	 loss: 17.2221, MinusLogProbMetric: 17.2221, val_loss: 17.4172, val_MinusLogProbMetric: 17.4172

Epoch 684: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2221 - MinusLogProbMetric: 17.2221 - val_loss: 17.4172 - val_MinusLogProbMetric: 17.4172 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 685/1000
2023-09-27 03:50:18.888 
Epoch 685/1000 
	 loss: 17.2258, MinusLogProbMetric: 17.2258, val_loss: 17.3976, val_MinusLogProbMetric: 17.3976

Epoch 685: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2258 - MinusLogProbMetric: 17.2258 - val_loss: 17.3976 - val_MinusLogProbMetric: 17.3976 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 686/1000
2023-09-27 03:51:23.531 
Epoch 686/1000 
	 loss: 17.2240, MinusLogProbMetric: 17.2240, val_loss: 17.3833, val_MinusLogProbMetric: 17.3833

Epoch 686: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2240 - MinusLogProbMetric: 17.2240 - val_loss: 17.3833 - val_MinusLogProbMetric: 17.3833 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 687/1000
2023-09-27 03:52:27.084 
Epoch 687/1000 
	 loss: 17.2202, MinusLogProbMetric: 17.2202, val_loss: 17.3997, val_MinusLogProbMetric: 17.3997

Epoch 687: val_loss did not improve from 17.36456
196/196 - 64s - loss: 17.2202 - MinusLogProbMetric: 17.2202 - val_loss: 17.3997 - val_MinusLogProbMetric: 17.3997 - lr: 4.1667e-05 - 64s/epoch - 324ms/step
Epoch 688/1000
2023-09-27 03:53:31.748 
Epoch 688/1000 
	 loss: 17.2297, MinusLogProbMetric: 17.2297, val_loss: 17.4499, val_MinusLogProbMetric: 17.4499

Epoch 688: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2297 - MinusLogProbMetric: 17.2297 - val_loss: 17.4499 - val_MinusLogProbMetric: 17.4499 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 689/1000
2023-09-27 03:54:38.251 
Epoch 689/1000 
	 loss: 17.2256, MinusLogProbMetric: 17.2256, val_loss: 17.3733, val_MinusLogProbMetric: 17.3733

Epoch 689: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2256 - MinusLogProbMetric: 17.2256 - val_loss: 17.3733 - val_MinusLogProbMetric: 17.3733 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 690/1000
2023-09-27 03:55:43.849 
Epoch 690/1000 
	 loss: 17.2220, MinusLogProbMetric: 17.2220, val_loss: 17.3823, val_MinusLogProbMetric: 17.3823

Epoch 690: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2220 - MinusLogProbMetric: 17.2220 - val_loss: 17.3823 - val_MinusLogProbMetric: 17.3823 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 691/1000
2023-09-27 03:56:49.576 
Epoch 691/1000 
	 loss: 17.2248, MinusLogProbMetric: 17.2248, val_loss: 17.3683, val_MinusLogProbMetric: 17.3683

Epoch 691: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2248 - MinusLogProbMetric: 17.2248 - val_loss: 17.3683 - val_MinusLogProbMetric: 17.3683 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 692/1000
2023-09-27 03:57:53.542 
Epoch 692/1000 
	 loss: 17.2205, MinusLogProbMetric: 17.2205, val_loss: 17.3854, val_MinusLogProbMetric: 17.3854

Epoch 692: val_loss did not improve from 17.36456
196/196 - 64s - loss: 17.2205 - MinusLogProbMetric: 17.2205 - val_loss: 17.3854 - val_MinusLogProbMetric: 17.3854 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 693/1000
2023-09-27 03:58:57.205 
Epoch 693/1000 
	 loss: 17.2255, MinusLogProbMetric: 17.2255, val_loss: 17.3687, val_MinusLogProbMetric: 17.3687

Epoch 693: val_loss did not improve from 17.36456
196/196 - 64s - loss: 17.2255 - MinusLogProbMetric: 17.2255 - val_loss: 17.3687 - val_MinusLogProbMetric: 17.3687 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 694/1000
2023-09-27 04:00:04.172 
Epoch 694/1000 
	 loss: 17.2205, MinusLogProbMetric: 17.2205, val_loss: 17.4275, val_MinusLogProbMetric: 17.4275

Epoch 694: val_loss did not improve from 17.36456
196/196 - 67s - loss: 17.2205 - MinusLogProbMetric: 17.2205 - val_loss: 17.4275 - val_MinusLogProbMetric: 17.4275 - lr: 4.1667e-05 - 67s/epoch - 342ms/step
Epoch 695/1000
2023-09-27 04:01:09.757 
Epoch 695/1000 
	 loss: 17.2262, MinusLogProbMetric: 17.2262, val_loss: 17.3792, val_MinusLogProbMetric: 17.3792

Epoch 695: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2262 - MinusLogProbMetric: 17.2262 - val_loss: 17.3792 - val_MinusLogProbMetric: 17.3792 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 696/1000
2023-09-27 04:02:15.529 
Epoch 696/1000 
	 loss: 17.2289, MinusLogProbMetric: 17.2289, val_loss: 17.3891, val_MinusLogProbMetric: 17.3891

Epoch 696: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2289 - MinusLogProbMetric: 17.2289 - val_loss: 17.3891 - val_MinusLogProbMetric: 17.3891 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 697/1000
2023-09-27 04:03:21.882 
Epoch 697/1000 
	 loss: 17.2261, MinusLogProbMetric: 17.2261, val_loss: 17.3705, val_MinusLogProbMetric: 17.3705

Epoch 697: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2261 - MinusLogProbMetric: 17.2261 - val_loss: 17.3705 - val_MinusLogProbMetric: 17.3705 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 698/1000
2023-09-27 04:04:26.323 
Epoch 698/1000 
	 loss: 17.2245, MinusLogProbMetric: 17.2245, val_loss: 17.4303, val_MinusLogProbMetric: 17.4303

Epoch 698: val_loss did not improve from 17.36456
196/196 - 64s - loss: 17.2245 - MinusLogProbMetric: 17.2245 - val_loss: 17.4303 - val_MinusLogProbMetric: 17.4303 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 699/1000
2023-09-27 04:05:32.240 
Epoch 699/1000 
	 loss: 17.2277, MinusLogProbMetric: 17.2277, val_loss: 17.3750, val_MinusLogProbMetric: 17.3750

Epoch 699: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2277 - MinusLogProbMetric: 17.2277 - val_loss: 17.3750 - val_MinusLogProbMetric: 17.3750 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 700/1000
2023-09-27 04:06:37.210 
Epoch 700/1000 
	 loss: 17.2314, MinusLogProbMetric: 17.2314, val_loss: 17.3687, val_MinusLogProbMetric: 17.3687

Epoch 700: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2314 - MinusLogProbMetric: 17.2314 - val_loss: 17.3687 - val_MinusLogProbMetric: 17.3687 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 701/1000
2023-09-27 04:07:43.776 
Epoch 701/1000 
	 loss: 17.2261, MinusLogProbMetric: 17.2261, val_loss: 17.3993, val_MinusLogProbMetric: 17.3993

Epoch 701: val_loss did not improve from 17.36456
196/196 - 67s - loss: 17.2261 - MinusLogProbMetric: 17.2261 - val_loss: 17.3993 - val_MinusLogProbMetric: 17.3993 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 702/1000
2023-09-27 04:08:48.357 
Epoch 702/1000 
	 loss: 17.2201, MinusLogProbMetric: 17.2201, val_loss: 17.4449, val_MinusLogProbMetric: 17.4449

Epoch 702: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2201 - MinusLogProbMetric: 17.2201 - val_loss: 17.4449 - val_MinusLogProbMetric: 17.4449 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 703/1000
2023-09-27 04:09:54.994 
Epoch 703/1000 
	 loss: 17.2288, MinusLogProbMetric: 17.2288, val_loss: 17.3812, val_MinusLogProbMetric: 17.3812

Epoch 703: val_loss did not improve from 17.36456
196/196 - 67s - loss: 17.2288 - MinusLogProbMetric: 17.2288 - val_loss: 17.3812 - val_MinusLogProbMetric: 17.3812 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 704/1000
2023-09-27 04:11:00.841 
Epoch 704/1000 
	 loss: 17.2201, MinusLogProbMetric: 17.2201, val_loss: 17.3726, val_MinusLogProbMetric: 17.3726

Epoch 704: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2201 - MinusLogProbMetric: 17.2201 - val_loss: 17.3726 - val_MinusLogProbMetric: 17.3726 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 705/1000
2023-09-27 04:12:06.595 
Epoch 705/1000 
	 loss: 17.2187, MinusLogProbMetric: 17.2187, val_loss: 17.4103, val_MinusLogProbMetric: 17.4103

Epoch 705: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2187 - MinusLogProbMetric: 17.2187 - val_loss: 17.4103 - val_MinusLogProbMetric: 17.4103 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 706/1000
2023-09-27 04:13:11.333 
Epoch 706/1000 
	 loss: 17.2273, MinusLogProbMetric: 17.2273, val_loss: 17.4448, val_MinusLogProbMetric: 17.4448

Epoch 706: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2273 - MinusLogProbMetric: 17.2273 - val_loss: 17.4448 - val_MinusLogProbMetric: 17.4448 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 707/1000
2023-09-27 04:14:14.091 
Epoch 707/1000 
	 loss: 17.2178, MinusLogProbMetric: 17.2178, val_loss: 17.4008, val_MinusLogProbMetric: 17.4008

Epoch 707: val_loss did not improve from 17.36456
196/196 - 63s - loss: 17.2178 - MinusLogProbMetric: 17.2178 - val_loss: 17.4008 - val_MinusLogProbMetric: 17.4008 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 708/1000
2023-09-27 04:15:16.927 
Epoch 708/1000 
	 loss: 17.2316, MinusLogProbMetric: 17.2316, val_loss: 17.4107, val_MinusLogProbMetric: 17.4107

Epoch 708: val_loss did not improve from 17.36456
196/196 - 63s - loss: 17.2316 - MinusLogProbMetric: 17.2316 - val_loss: 17.4107 - val_MinusLogProbMetric: 17.4107 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 709/1000
2023-09-27 04:16:18.897 
Epoch 709/1000 
	 loss: 17.2322, MinusLogProbMetric: 17.2322, val_loss: 17.4099, val_MinusLogProbMetric: 17.4099

Epoch 709: val_loss did not improve from 17.36456
196/196 - 62s - loss: 17.2322 - MinusLogProbMetric: 17.2322 - val_loss: 17.4099 - val_MinusLogProbMetric: 17.4099 - lr: 4.1667e-05 - 62s/epoch - 316ms/step
Epoch 710/1000
2023-09-27 04:17:21.318 
Epoch 710/1000 
	 loss: 17.2240, MinusLogProbMetric: 17.2240, val_loss: 17.3665, val_MinusLogProbMetric: 17.3665

Epoch 710: val_loss did not improve from 17.36456
196/196 - 62s - loss: 17.2240 - MinusLogProbMetric: 17.2240 - val_loss: 17.3665 - val_MinusLogProbMetric: 17.3665 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 711/1000
2023-09-27 04:18:23.348 
Epoch 711/1000 
	 loss: 17.2240, MinusLogProbMetric: 17.2240, val_loss: 17.3963, val_MinusLogProbMetric: 17.3963

Epoch 711: val_loss did not improve from 17.36456
196/196 - 62s - loss: 17.2240 - MinusLogProbMetric: 17.2240 - val_loss: 17.3963 - val_MinusLogProbMetric: 17.3963 - lr: 4.1667e-05 - 62s/epoch - 316ms/step
Epoch 712/1000
2023-09-27 04:19:26.400 
Epoch 712/1000 
	 loss: 17.2178, MinusLogProbMetric: 17.2178, val_loss: 17.3756, val_MinusLogProbMetric: 17.3756

Epoch 712: val_loss did not improve from 17.36456
196/196 - 63s - loss: 17.2178 - MinusLogProbMetric: 17.2178 - val_loss: 17.3756 - val_MinusLogProbMetric: 17.3756 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 713/1000
2023-09-27 04:20:29.159 
Epoch 713/1000 
	 loss: 17.2171, MinusLogProbMetric: 17.2171, val_loss: 17.4255, val_MinusLogProbMetric: 17.4255

Epoch 713: val_loss did not improve from 17.36456
196/196 - 63s - loss: 17.2171 - MinusLogProbMetric: 17.2171 - val_loss: 17.4255 - val_MinusLogProbMetric: 17.4255 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 714/1000
2023-09-27 04:21:31.489 
Epoch 714/1000 
	 loss: 17.2247, MinusLogProbMetric: 17.2247, val_loss: 17.4232, val_MinusLogProbMetric: 17.4232

Epoch 714: val_loss did not improve from 17.36456
196/196 - 62s - loss: 17.2247 - MinusLogProbMetric: 17.2247 - val_loss: 17.4232 - val_MinusLogProbMetric: 17.4232 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 715/1000
2023-09-27 04:22:34.116 
Epoch 715/1000 
	 loss: 17.2178, MinusLogProbMetric: 17.2178, val_loss: 17.4024, val_MinusLogProbMetric: 17.4024

Epoch 715: val_loss did not improve from 17.36456
196/196 - 63s - loss: 17.2178 - MinusLogProbMetric: 17.2178 - val_loss: 17.4024 - val_MinusLogProbMetric: 17.4024 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 716/1000
2023-09-27 04:23:37.088 
Epoch 716/1000 
	 loss: 17.2196, MinusLogProbMetric: 17.2196, val_loss: 17.3870, val_MinusLogProbMetric: 17.3870

Epoch 716: val_loss did not improve from 17.36456
196/196 - 63s - loss: 17.2196 - MinusLogProbMetric: 17.2196 - val_loss: 17.3870 - val_MinusLogProbMetric: 17.3870 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 717/1000
2023-09-27 04:24:40.604 
Epoch 717/1000 
	 loss: 17.2148, MinusLogProbMetric: 17.2148, val_loss: 17.3684, val_MinusLogProbMetric: 17.3684

Epoch 717: val_loss did not improve from 17.36456
196/196 - 64s - loss: 17.2148 - MinusLogProbMetric: 17.2148 - val_loss: 17.3684 - val_MinusLogProbMetric: 17.3684 - lr: 4.1667e-05 - 64s/epoch - 324ms/step
Epoch 718/1000
2023-09-27 04:25:43.295 
Epoch 718/1000 
	 loss: 17.2139, MinusLogProbMetric: 17.2139, val_loss: 17.3972, val_MinusLogProbMetric: 17.3972

Epoch 718: val_loss did not improve from 17.36456
196/196 - 63s - loss: 17.2139 - MinusLogProbMetric: 17.2139 - val_loss: 17.3972 - val_MinusLogProbMetric: 17.3972 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 719/1000
2023-09-27 04:26:47.164 
Epoch 719/1000 
	 loss: 17.2186, MinusLogProbMetric: 17.2186, val_loss: 17.3890, val_MinusLogProbMetric: 17.3890

Epoch 719: val_loss did not improve from 17.36456
196/196 - 64s - loss: 17.2186 - MinusLogProbMetric: 17.2186 - val_loss: 17.3890 - val_MinusLogProbMetric: 17.3890 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 720/1000
2023-09-27 04:27:53.339 
Epoch 720/1000 
	 loss: 17.2165, MinusLogProbMetric: 17.2165, val_loss: 17.3796, val_MinusLogProbMetric: 17.3796

Epoch 720: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2165 - MinusLogProbMetric: 17.2165 - val_loss: 17.3796 - val_MinusLogProbMetric: 17.3796 - lr: 4.1667e-05 - 66s/epoch - 338ms/step
Epoch 721/1000
2023-09-27 04:28:59.831 
Epoch 721/1000 
	 loss: 17.2179, MinusLogProbMetric: 17.2179, val_loss: 17.4349, val_MinusLogProbMetric: 17.4349

Epoch 721: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2179 - MinusLogProbMetric: 17.2179 - val_loss: 17.4349 - val_MinusLogProbMetric: 17.4349 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 722/1000
2023-09-27 04:30:05.555 
Epoch 722/1000 
	 loss: 17.2147, MinusLogProbMetric: 17.2147, val_loss: 17.3888, val_MinusLogProbMetric: 17.3888

Epoch 722: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2147 - MinusLogProbMetric: 17.2147 - val_loss: 17.3888 - val_MinusLogProbMetric: 17.3888 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 723/1000
2023-09-27 04:31:12.219 
Epoch 723/1000 
	 loss: 17.2191, MinusLogProbMetric: 17.2191, val_loss: 17.3900, val_MinusLogProbMetric: 17.3900

Epoch 723: val_loss did not improve from 17.36456
196/196 - 67s - loss: 17.2191 - MinusLogProbMetric: 17.2191 - val_loss: 17.3900 - val_MinusLogProbMetric: 17.3900 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 724/1000
2023-09-27 04:32:18.107 
Epoch 724/1000 
	 loss: 17.2135, MinusLogProbMetric: 17.2135, val_loss: 17.3981, val_MinusLogProbMetric: 17.3981

Epoch 724: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2135 - MinusLogProbMetric: 17.2135 - val_loss: 17.3981 - val_MinusLogProbMetric: 17.3981 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 725/1000
2023-09-27 04:33:23.570 
Epoch 725/1000 
	 loss: 17.2083, MinusLogProbMetric: 17.2083, val_loss: 17.3808, val_MinusLogProbMetric: 17.3808

Epoch 725: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2083 - MinusLogProbMetric: 17.2083 - val_loss: 17.3808 - val_MinusLogProbMetric: 17.3808 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 726/1000
2023-09-27 04:34:28.694 
Epoch 726/1000 
	 loss: 17.2203, MinusLogProbMetric: 17.2203, val_loss: 17.4764, val_MinusLogProbMetric: 17.4764

Epoch 726: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2203 - MinusLogProbMetric: 17.2203 - val_loss: 17.4764 - val_MinusLogProbMetric: 17.4764 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 727/1000
2023-09-27 04:35:33.933 
Epoch 727/1000 
	 loss: 17.2268, MinusLogProbMetric: 17.2268, val_loss: 17.3981, val_MinusLogProbMetric: 17.3981

Epoch 727: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2268 - MinusLogProbMetric: 17.2268 - val_loss: 17.3981 - val_MinusLogProbMetric: 17.3981 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 728/1000
2023-09-27 04:36:39.895 
Epoch 728/1000 
	 loss: 17.2136, MinusLogProbMetric: 17.2136, val_loss: 17.3730, val_MinusLogProbMetric: 17.3730

Epoch 728: val_loss did not improve from 17.36456
196/196 - 66s - loss: 17.2136 - MinusLogProbMetric: 17.2136 - val_loss: 17.3730 - val_MinusLogProbMetric: 17.3730 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 729/1000
2023-09-27 04:37:46.544 
Epoch 729/1000 
	 loss: 17.2123, MinusLogProbMetric: 17.2123, val_loss: 17.3763, val_MinusLogProbMetric: 17.3763

Epoch 729: val_loss did not improve from 17.36456
196/196 - 67s - loss: 17.2123 - MinusLogProbMetric: 17.2123 - val_loss: 17.3763 - val_MinusLogProbMetric: 17.3763 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 730/1000
2023-09-27 04:38:52.017 
Epoch 730/1000 
	 loss: 17.2134, MinusLogProbMetric: 17.2134, val_loss: 17.3684, val_MinusLogProbMetric: 17.3684

Epoch 730: val_loss did not improve from 17.36456
196/196 - 65s - loss: 17.2134 - MinusLogProbMetric: 17.2134 - val_loss: 17.3684 - val_MinusLogProbMetric: 17.3684 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 731/1000
2023-09-27 04:39:55.874 
Epoch 731/1000 
	 loss: 17.2156, MinusLogProbMetric: 17.2156, val_loss: 17.3888, val_MinusLogProbMetric: 17.3888

Epoch 731: val_loss did not improve from 17.36456
196/196 - 64s - loss: 17.2156 - MinusLogProbMetric: 17.2156 - val_loss: 17.3888 - val_MinusLogProbMetric: 17.3888 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 732/1000
2023-09-27 04:40:59.954 
Epoch 732/1000 
	 loss: 17.2167, MinusLogProbMetric: 17.2167, val_loss: 17.3640, val_MinusLogProbMetric: 17.3640

Epoch 732: val_loss improved from 17.36456 to 17.36404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 17.2167 - MinusLogProbMetric: 17.2167 - val_loss: 17.3640 - val_MinusLogProbMetric: 17.3640 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 733/1000
2023-09-27 04:42:04.428 
Epoch 733/1000 
	 loss: 17.2133, MinusLogProbMetric: 17.2133, val_loss: 17.4717, val_MinusLogProbMetric: 17.4717

Epoch 733: val_loss did not improve from 17.36404
196/196 - 63s - loss: 17.2133 - MinusLogProbMetric: 17.2133 - val_loss: 17.4717 - val_MinusLogProbMetric: 17.4717 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 734/1000
2023-09-27 04:43:07.272 
Epoch 734/1000 
	 loss: 17.2249, MinusLogProbMetric: 17.2249, val_loss: 17.3821, val_MinusLogProbMetric: 17.3821

Epoch 734: val_loss did not improve from 17.36404
196/196 - 63s - loss: 17.2249 - MinusLogProbMetric: 17.2249 - val_loss: 17.3821 - val_MinusLogProbMetric: 17.3821 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 735/1000
2023-09-27 04:44:10.542 
Epoch 735/1000 
	 loss: 17.2187, MinusLogProbMetric: 17.2187, val_loss: 17.3860, val_MinusLogProbMetric: 17.3860

Epoch 735: val_loss did not improve from 17.36404
196/196 - 63s - loss: 17.2187 - MinusLogProbMetric: 17.2187 - val_loss: 17.3860 - val_MinusLogProbMetric: 17.3860 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 736/1000
2023-09-27 04:45:13.344 
Epoch 736/1000 
	 loss: 17.2155, MinusLogProbMetric: 17.2155, val_loss: 17.3930, val_MinusLogProbMetric: 17.3930

Epoch 736: val_loss did not improve from 17.36404
196/196 - 63s - loss: 17.2155 - MinusLogProbMetric: 17.2155 - val_loss: 17.3930 - val_MinusLogProbMetric: 17.3930 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 737/1000
2023-09-27 04:46:15.489 
Epoch 737/1000 
	 loss: 17.2159, MinusLogProbMetric: 17.2159, val_loss: 17.4294, val_MinusLogProbMetric: 17.4294

Epoch 737: val_loss did not improve from 17.36404
196/196 - 62s - loss: 17.2159 - MinusLogProbMetric: 17.2159 - val_loss: 17.4294 - val_MinusLogProbMetric: 17.4294 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 738/1000
2023-09-27 04:47:17.790 
Epoch 738/1000 
	 loss: 17.2164, MinusLogProbMetric: 17.2164, val_loss: 17.3847, val_MinusLogProbMetric: 17.3847

Epoch 738: val_loss did not improve from 17.36404
196/196 - 62s - loss: 17.2164 - MinusLogProbMetric: 17.2164 - val_loss: 17.3847 - val_MinusLogProbMetric: 17.3847 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 739/1000
2023-09-27 04:48:20.525 
Epoch 739/1000 
	 loss: 17.2120, MinusLogProbMetric: 17.2120, val_loss: 17.3796, val_MinusLogProbMetric: 17.3796

Epoch 739: val_loss did not improve from 17.36404
196/196 - 63s - loss: 17.2120 - MinusLogProbMetric: 17.2120 - val_loss: 17.3796 - val_MinusLogProbMetric: 17.3796 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 740/1000
2023-09-27 04:49:22.736 
Epoch 740/1000 
	 loss: 17.2119, MinusLogProbMetric: 17.2119, val_loss: 17.4190, val_MinusLogProbMetric: 17.4190

Epoch 740: val_loss did not improve from 17.36404
196/196 - 62s - loss: 17.2119 - MinusLogProbMetric: 17.2119 - val_loss: 17.4190 - val_MinusLogProbMetric: 17.4190 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 741/1000
2023-09-27 04:50:27.277 
Epoch 741/1000 
	 loss: 17.2204, MinusLogProbMetric: 17.2204, val_loss: 17.4132, val_MinusLogProbMetric: 17.4132

Epoch 741: val_loss did not improve from 17.36404
196/196 - 65s - loss: 17.2204 - MinusLogProbMetric: 17.2204 - val_loss: 17.4132 - val_MinusLogProbMetric: 17.4132 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 742/1000
2023-09-27 04:51:32.447 
Epoch 742/1000 
	 loss: 17.2130, MinusLogProbMetric: 17.2130, val_loss: 17.3618, val_MinusLogProbMetric: 17.3618

Epoch 742: val_loss improved from 17.36404 to 17.36178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 67s - loss: 17.2130 - MinusLogProbMetric: 17.2130 - val_loss: 17.3618 - val_MinusLogProbMetric: 17.3618 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 743/1000
2023-09-27 04:52:39.399 
Epoch 743/1000 
	 loss: 17.2122, MinusLogProbMetric: 17.2122, val_loss: 17.3574, val_MinusLogProbMetric: 17.3574

Epoch 743: val_loss improved from 17.36178 to 17.35743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 17.2122 - MinusLogProbMetric: 17.2122 - val_loss: 17.3574 - val_MinusLogProbMetric: 17.3574 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 744/1000
2023-09-27 04:53:44.273 
Epoch 744/1000 
	 loss: 17.2074, MinusLogProbMetric: 17.2074, val_loss: 17.3570, val_MinusLogProbMetric: 17.3570

Epoch 744: val_loss improved from 17.35743 to 17.35705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 17.2074 - MinusLogProbMetric: 17.2074 - val_loss: 17.3570 - val_MinusLogProbMetric: 17.3570 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 745/1000
2023-09-27 04:54:50.055 
Epoch 745/1000 
	 loss: 17.2158, MinusLogProbMetric: 17.2158, val_loss: 17.4123, val_MinusLogProbMetric: 17.4123

Epoch 745: val_loss did not improve from 17.35705
196/196 - 65s - loss: 17.2158 - MinusLogProbMetric: 17.2158 - val_loss: 17.4123 - val_MinusLogProbMetric: 17.4123 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 746/1000
2023-09-27 04:55:53.913 
Epoch 746/1000 
	 loss: 17.2102, MinusLogProbMetric: 17.2102, val_loss: 17.3589, val_MinusLogProbMetric: 17.3589

Epoch 746: val_loss did not improve from 17.35705
196/196 - 64s - loss: 17.2102 - MinusLogProbMetric: 17.2102 - val_loss: 17.3589 - val_MinusLogProbMetric: 17.3589 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 747/1000
2023-09-27 04:56:59.629 
Epoch 747/1000 
	 loss: 17.2136, MinusLogProbMetric: 17.2136, val_loss: 17.3946, val_MinusLogProbMetric: 17.3946

Epoch 747: val_loss did not improve from 17.35705
196/196 - 66s - loss: 17.2136 - MinusLogProbMetric: 17.2136 - val_loss: 17.3946 - val_MinusLogProbMetric: 17.3946 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 748/1000
2023-09-27 04:57:57.987 
Epoch 748/1000 
	 loss: 17.2100, MinusLogProbMetric: 17.2100, val_loss: 17.3810, val_MinusLogProbMetric: 17.3810

Epoch 748: val_loss did not improve from 17.35705
196/196 - 58s - loss: 17.2100 - MinusLogProbMetric: 17.2100 - val_loss: 17.3810 - val_MinusLogProbMetric: 17.3810 - lr: 4.1667e-05 - 58s/epoch - 298ms/step
Epoch 749/1000
2023-09-27 04:58:55.190 
Epoch 749/1000 
	 loss: 17.2145, MinusLogProbMetric: 17.2145, val_loss: 17.3891, val_MinusLogProbMetric: 17.3891

Epoch 749: val_loss did not improve from 17.35705
196/196 - 57s - loss: 17.2145 - MinusLogProbMetric: 17.2145 - val_loss: 17.3891 - val_MinusLogProbMetric: 17.3891 - lr: 4.1667e-05 - 57s/epoch - 292ms/step
Epoch 750/1000
2023-09-27 04:59:58.337 
Epoch 750/1000 
	 loss: 17.2107, MinusLogProbMetric: 17.2107, val_loss: 17.3686, val_MinusLogProbMetric: 17.3686

Epoch 750: val_loss did not improve from 17.35705
196/196 - 63s - loss: 17.2107 - MinusLogProbMetric: 17.2107 - val_loss: 17.3686 - val_MinusLogProbMetric: 17.3686 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 751/1000
2023-09-27 05:01:03.860 
Epoch 751/1000 
	 loss: 17.2127, MinusLogProbMetric: 17.2127, val_loss: 18.4220, val_MinusLogProbMetric: 18.4220

Epoch 751: val_loss did not improve from 17.35705
196/196 - 66s - loss: 17.2127 - MinusLogProbMetric: 17.2127 - val_loss: 18.4220 - val_MinusLogProbMetric: 18.4220 - lr: 4.1667e-05 - 66s/epoch - 334ms/step
Epoch 752/1000
2023-09-27 05:02:08.514 
Epoch 752/1000 
	 loss: 17.2789, MinusLogProbMetric: 17.2789, val_loss: 17.3758, val_MinusLogProbMetric: 17.3758

Epoch 752: val_loss did not improve from 17.35705
196/196 - 65s - loss: 17.2789 - MinusLogProbMetric: 17.2789 - val_loss: 17.3758 - val_MinusLogProbMetric: 17.3758 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 753/1000
2023-09-27 05:03:13.242 
Epoch 753/1000 
	 loss: 17.2114, MinusLogProbMetric: 17.2114, val_loss: 17.3735, val_MinusLogProbMetric: 17.3735

Epoch 753: val_loss did not improve from 17.35705
196/196 - 65s - loss: 17.2114 - MinusLogProbMetric: 17.2114 - val_loss: 17.3735 - val_MinusLogProbMetric: 17.3735 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 754/1000
2023-09-27 05:04:18.369 
Epoch 754/1000 
	 loss: 17.2095, MinusLogProbMetric: 17.2095, val_loss: 17.3881, val_MinusLogProbMetric: 17.3881

Epoch 754: val_loss did not improve from 17.35705
196/196 - 65s - loss: 17.2095 - MinusLogProbMetric: 17.2095 - val_loss: 17.3881 - val_MinusLogProbMetric: 17.3881 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 755/1000
2023-09-27 05:05:23.757 
Epoch 755/1000 
	 loss: 17.2113, MinusLogProbMetric: 17.2113, val_loss: 17.3869, val_MinusLogProbMetric: 17.3869

Epoch 755: val_loss did not improve from 17.35705
196/196 - 65s - loss: 17.2113 - MinusLogProbMetric: 17.2113 - val_loss: 17.3869 - val_MinusLogProbMetric: 17.3869 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 756/1000
2023-09-27 05:06:28.216 
Epoch 756/1000 
	 loss: 17.2143, MinusLogProbMetric: 17.2143, val_loss: 17.3797, val_MinusLogProbMetric: 17.3797

Epoch 756: val_loss did not improve from 17.35705
196/196 - 64s - loss: 17.2143 - MinusLogProbMetric: 17.2143 - val_loss: 17.3797 - val_MinusLogProbMetric: 17.3797 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 757/1000
2023-09-27 05:07:32.940 
Epoch 757/1000 
	 loss: 17.2174, MinusLogProbMetric: 17.2174, val_loss: 17.3779, val_MinusLogProbMetric: 17.3779

Epoch 757: val_loss did not improve from 17.35705
196/196 - 65s - loss: 17.2174 - MinusLogProbMetric: 17.2174 - val_loss: 17.3779 - val_MinusLogProbMetric: 17.3779 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 758/1000
2023-09-27 05:08:38.937 
Epoch 758/1000 
	 loss: 17.2030, MinusLogProbMetric: 17.2030, val_loss: 17.3653, val_MinusLogProbMetric: 17.3653

Epoch 758: val_loss did not improve from 17.35705
196/196 - 66s - loss: 17.2030 - MinusLogProbMetric: 17.2030 - val_loss: 17.3653 - val_MinusLogProbMetric: 17.3653 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 759/1000
2023-09-27 05:09:42.769 
Epoch 759/1000 
	 loss: 17.2102, MinusLogProbMetric: 17.2102, val_loss: 17.4051, val_MinusLogProbMetric: 17.4051

Epoch 759: val_loss did not improve from 17.35705
196/196 - 64s - loss: 17.2102 - MinusLogProbMetric: 17.2102 - val_loss: 17.4051 - val_MinusLogProbMetric: 17.4051 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 760/1000
2023-09-27 05:10:48.234 
Epoch 760/1000 
	 loss: 17.2050, MinusLogProbMetric: 17.2050, val_loss: 17.3657, val_MinusLogProbMetric: 17.3657

Epoch 760: val_loss did not improve from 17.35705
196/196 - 65s - loss: 17.2050 - MinusLogProbMetric: 17.2050 - val_loss: 17.3657 - val_MinusLogProbMetric: 17.3657 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 761/1000
2023-09-27 05:11:53.924 
Epoch 761/1000 
	 loss: 17.2147, MinusLogProbMetric: 17.2147, val_loss: 17.3957, val_MinusLogProbMetric: 17.3957

Epoch 761: val_loss did not improve from 17.35705
196/196 - 66s - loss: 17.2147 - MinusLogProbMetric: 17.2147 - val_loss: 17.3957 - val_MinusLogProbMetric: 17.3957 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 762/1000
2023-09-27 05:12:58.921 
Epoch 762/1000 
	 loss: 17.2089, MinusLogProbMetric: 17.2089, val_loss: 17.3889, val_MinusLogProbMetric: 17.3889

Epoch 762: val_loss did not improve from 17.35705
196/196 - 65s - loss: 17.2089 - MinusLogProbMetric: 17.2089 - val_loss: 17.3889 - val_MinusLogProbMetric: 17.3889 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 763/1000
2023-09-27 05:14:02.812 
Epoch 763/1000 
	 loss: 17.2179, MinusLogProbMetric: 17.2179, val_loss: 17.3787, val_MinusLogProbMetric: 17.3787

Epoch 763: val_loss did not improve from 17.35705
196/196 - 64s - loss: 17.2179 - MinusLogProbMetric: 17.2179 - val_loss: 17.3787 - val_MinusLogProbMetric: 17.3787 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 764/1000
2023-09-27 05:15:06.984 
Epoch 764/1000 
	 loss: 17.2078, MinusLogProbMetric: 17.2078, val_loss: 17.4525, val_MinusLogProbMetric: 17.4525

Epoch 764: val_loss did not improve from 17.35705
196/196 - 64s - loss: 17.2078 - MinusLogProbMetric: 17.2078 - val_loss: 17.4525 - val_MinusLogProbMetric: 17.4525 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 765/1000
2023-09-27 05:16:11.225 
Epoch 765/1000 
	 loss: 17.2150, MinusLogProbMetric: 17.2150, val_loss: 17.3947, val_MinusLogProbMetric: 17.3947

Epoch 765: val_loss did not improve from 17.35705
196/196 - 64s - loss: 17.2150 - MinusLogProbMetric: 17.2150 - val_loss: 17.3947 - val_MinusLogProbMetric: 17.3947 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 766/1000
2023-09-27 05:17:15.524 
Epoch 766/1000 
	 loss: 17.2046, MinusLogProbMetric: 17.2046, val_loss: 17.3553, val_MinusLogProbMetric: 17.3553

Epoch 766: val_loss improved from 17.35705 to 17.35535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 17.2046 - MinusLogProbMetric: 17.2046 - val_loss: 17.3553 - val_MinusLogProbMetric: 17.3553 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 767/1000
2023-09-27 05:18:20.907 
Epoch 767/1000 
	 loss: 17.2070, MinusLogProbMetric: 17.2070, val_loss: 17.4038, val_MinusLogProbMetric: 17.4038

Epoch 767: val_loss did not improve from 17.35535
196/196 - 64s - loss: 17.2070 - MinusLogProbMetric: 17.2070 - val_loss: 17.4038 - val_MinusLogProbMetric: 17.4038 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 768/1000
2023-09-27 05:19:24.544 
Epoch 768/1000 
	 loss: 17.2075, MinusLogProbMetric: 17.2075, val_loss: 17.3755, val_MinusLogProbMetric: 17.3755

Epoch 768: val_loss did not improve from 17.35535
196/196 - 64s - loss: 17.2075 - MinusLogProbMetric: 17.2075 - val_loss: 17.3755 - val_MinusLogProbMetric: 17.3755 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 769/1000
2023-09-27 05:20:29.035 
Epoch 769/1000 
	 loss: 17.2066, MinusLogProbMetric: 17.2066, val_loss: 17.3719, val_MinusLogProbMetric: 17.3719

Epoch 769: val_loss did not improve from 17.35535
196/196 - 64s - loss: 17.2066 - MinusLogProbMetric: 17.2066 - val_loss: 17.3719 - val_MinusLogProbMetric: 17.3719 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 770/1000
2023-09-27 05:21:32.853 
Epoch 770/1000 
	 loss: 17.2079, MinusLogProbMetric: 17.2079, val_loss: 17.3919, val_MinusLogProbMetric: 17.3919

Epoch 770: val_loss did not improve from 17.35535
196/196 - 64s - loss: 17.2079 - MinusLogProbMetric: 17.2079 - val_loss: 17.3919 - val_MinusLogProbMetric: 17.3919 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 771/1000
2023-09-27 05:22:38.703 
Epoch 771/1000 
	 loss: 17.2033, MinusLogProbMetric: 17.2033, val_loss: 17.3689, val_MinusLogProbMetric: 17.3689

Epoch 771: val_loss did not improve from 17.35535
196/196 - 66s - loss: 17.2033 - MinusLogProbMetric: 17.2033 - val_loss: 17.3689 - val_MinusLogProbMetric: 17.3689 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 772/1000
2023-09-27 05:23:42.770 
Epoch 772/1000 
	 loss: 17.2085, MinusLogProbMetric: 17.2085, val_loss: 17.3692, val_MinusLogProbMetric: 17.3692

Epoch 772: val_loss did not improve from 17.35535
196/196 - 64s - loss: 17.2085 - MinusLogProbMetric: 17.2085 - val_loss: 17.3692 - val_MinusLogProbMetric: 17.3692 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 773/1000
2023-09-27 05:24:53.274 
Epoch 773/1000 
	 loss: 17.2051, MinusLogProbMetric: 17.2051, val_loss: 17.3573, val_MinusLogProbMetric: 17.3573

Epoch 773: val_loss did not improve from 17.35535
196/196 - 70s - loss: 17.2051 - MinusLogProbMetric: 17.2051 - val_loss: 17.3573 - val_MinusLogProbMetric: 17.3573 - lr: 4.1667e-05 - 70s/epoch - 360ms/step
Epoch 774/1000
2023-09-27 05:26:03.119 
Epoch 774/1000 
	 loss: 17.2068, MinusLogProbMetric: 17.2068, val_loss: 17.3595, val_MinusLogProbMetric: 17.3595

Epoch 774: val_loss did not improve from 17.35535
196/196 - 70s - loss: 17.2068 - MinusLogProbMetric: 17.2068 - val_loss: 17.3595 - val_MinusLogProbMetric: 17.3595 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 775/1000
2023-09-27 05:27:12.708 
Epoch 775/1000 
	 loss: 17.2109, MinusLogProbMetric: 17.2109, val_loss: 17.3703, val_MinusLogProbMetric: 17.3703

Epoch 775: val_loss did not improve from 17.35535
196/196 - 70s - loss: 17.2109 - MinusLogProbMetric: 17.2109 - val_loss: 17.3703 - val_MinusLogProbMetric: 17.3703 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 776/1000
2023-09-27 05:28:23.242 
Epoch 776/1000 
	 loss: 17.2064, MinusLogProbMetric: 17.2064, val_loss: 17.3695, val_MinusLogProbMetric: 17.3695

Epoch 776: val_loss did not improve from 17.35535
196/196 - 71s - loss: 17.2064 - MinusLogProbMetric: 17.2064 - val_loss: 17.3695 - val_MinusLogProbMetric: 17.3695 - lr: 4.1667e-05 - 71s/epoch - 360ms/step
Epoch 777/1000
2023-09-27 05:29:32.785 
Epoch 777/1000 
	 loss: 17.2035, MinusLogProbMetric: 17.2035, val_loss: 17.4168, val_MinusLogProbMetric: 17.4168

Epoch 777: val_loss did not improve from 17.35535
196/196 - 70s - loss: 17.2035 - MinusLogProbMetric: 17.2035 - val_loss: 17.4168 - val_MinusLogProbMetric: 17.4168 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 778/1000
2023-09-27 05:30:41.403 
Epoch 778/1000 
	 loss: 17.2051, MinusLogProbMetric: 17.2051, val_loss: 17.3779, val_MinusLogProbMetric: 17.3779

Epoch 778: val_loss did not improve from 17.35535
196/196 - 69s - loss: 17.2051 - MinusLogProbMetric: 17.2051 - val_loss: 17.3779 - val_MinusLogProbMetric: 17.3779 - lr: 4.1667e-05 - 69s/epoch - 350ms/step
Epoch 779/1000
2023-09-27 05:31:50.150 
Epoch 779/1000 
	 loss: 17.2031, MinusLogProbMetric: 17.2031, val_loss: 17.4026, val_MinusLogProbMetric: 17.4026

Epoch 779: val_loss did not improve from 17.35535
196/196 - 69s - loss: 17.2031 - MinusLogProbMetric: 17.2031 - val_loss: 17.4026 - val_MinusLogProbMetric: 17.4026 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 780/1000
2023-09-27 05:32:54.960 
Epoch 780/1000 
	 loss: 17.2081, MinusLogProbMetric: 17.2081, val_loss: 17.4191, val_MinusLogProbMetric: 17.4191

Epoch 780: val_loss did not improve from 17.35535
196/196 - 65s - loss: 17.2081 - MinusLogProbMetric: 17.2081 - val_loss: 17.4191 - val_MinusLogProbMetric: 17.4191 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 781/1000
2023-09-27 05:33:52.503 
Epoch 781/1000 
	 loss: 17.2028, MinusLogProbMetric: 17.2028, val_loss: 17.3771, val_MinusLogProbMetric: 17.3771

Epoch 781: val_loss did not improve from 17.35535
196/196 - 58s - loss: 17.2028 - MinusLogProbMetric: 17.2028 - val_loss: 17.3771 - val_MinusLogProbMetric: 17.3771 - lr: 4.1667e-05 - 58s/epoch - 294ms/step
Epoch 782/1000
2023-09-27 05:34:58.666 
Epoch 782/1000 
	 loss: 17.2011, MinusLogProbMetric: 17.2011, val_loss: 17.4037, val_MinusLogProbMetric: 17.4037

Epoch 782: val_loss did not improve from 17.35535
196/196 - 66s - loss: 17.2011 - MinusLogProbMetric: 17.2011 - val_loss: 17.4037 - val_MinusLogProbMetric: 17.4037 - lr: 4.1667e-05 - 66s/epoch - 338ms/step
Epoch 783/1000
2023-09-27 05:35:57.233 
Epoch 783/1000 
	 loss: 17.2083, MinusLogProbMetric: 17.2083, val_loss: 17.4016, val_MinusLogProbMetric: 17.4016

Epoch 783: val_loss did not improve from 17.35535
196/196 - 59s - loss: 17.2083 - MinusLogProbMetric: 17.2083 - val_loss: 17.4016 - val_MinusLogProbMetric: 17.4016 - lr: 4.1667e-05 - 59s/epoch - 299ms/step
Epoch 784/1000
2023-09-27 05:37:00.955 
Epoch 784/1000 
	 loss: 17.2094, MinusLogProbMetric: 17.2094, val_loss: 17.3483, val_MinusLogProbMetric: 17.3483

Epoch 784: val_loss improved from 17.35535 to 17.34830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 17.2094 - MinusLogProbMetric: 17.2094 - val_loss: 17.3483 - val_MinusLogProbMetric: 17.3483 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 785/1000
2023-09-27 05:38:08.133 
Epoch 785/1000 
	 loss: 17.2018, MinusLogProbMetric: 17.2018, val_loss: 17.3778, val_MinusLogProbMetric: 17.3778

Epoch 785: val_loss did not improve from 17.34830
196/196 - 66s - loss: 17.2018 - MinusLogProbMetric: 17.2018 - val_loss: 17.3778 - val_MinusLogProbMetric: 17.3778 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 786/1000
2023-09-27 05:39:14.884 
Epoch 786/1000 
	 loss: 17.1989, MinusLogProbMetric: 17.1989, val_loss: 17.3850, val_MinusLogProbMetric: 17.3850

Epoch 786: val_loss did not improve from 17.34830
196/196 - 67s - loss: 17.1989 - MinusLogProbMetric: 17.1989 - val_loss: 17.3850 - val_MinusLogProbMetric: 17.3850 - lr: 4.1667e-05 - 67s/epoch - 341ms/step
Epoch 787/1000
2023-09-27 05:40:20.544 
Epoch 787/1000 
	 loss: 17.2049, MinusLogProbMetric: 17.2049, val_loss: 17.3805, val_MinusLogProbMetric: 17.3805

Epoch 787: val_loss did not improve from 17.34830
196/196 - 66s - loss: 17.2049 - MinusLogProbMetric: 17.2049 - val_loss: 17.3805 - val_MinusLogProbMetric: 17.3805 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 788/1000
2023-09-27 05:41:26.843 
Epoch 788/1000 
	 loss: 17.2084, MinusLogProbMetric: 17.2084, val_loss: 17.3719, val_MinusLogProbMetric: 17.3719

Epoch 788: val_loss did not improve from 17.34830
196/196 - 66s - loss: 17.2084 - MinusLogProbMetric: 17.2084 - val_loss: 17.3719 - val_MinusLogProbMetric: 17.3719 - lr: 4.1667e-05 - 66s/epoch - 338ms/step
Epoch 789/1000
2023-09-27 05:42:31.590 
Epoch 789/1000 
	 loss: 17.2040, MinusLogProbMetric: 17.2040, val_loss: 17.4146, val_MinusLogProbMetric: 17.4146

Epoch 789: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2040 - MinusLogProbMetric: 17.2040 - val_loss: 17.4146 - val_MinusLogProbMetric: 17.4146 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 790/1000
2023-09-27 05:43:36.220 
Epoch 790/1000 
	 loss: 17.2026, MinusLogProbMetric: 17.2026, val_loss: 17.3627, val_MinusLogProbMetric: 17.3627

Epoch 790: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2026 - MinusLogProbMetric: 17.2026 - val_loss: 17.3627 - val_MinusLogProbMetric: 17.3627 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 791/1000
2023-09-27 05:44:41.860 
Epoch 791/1000 
	 loss: 17.2039, MinusLogProbMetric: 17.2039, val_loss: 17.3591, val_MinusLogProbMetric: 17.3591

Epoch 791: val_loss did not improve from 17.34830
196/196 - 66s - loss: 17.2039 - MinusLogProbMetric: 17.2039 - val_loss: 17.3591 - val_MinusLogProbMetric: 17.3591 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 792/1000
2023-09-27 05:45:47.127 
Epoch 792/1000 
	 loss: 17.2062, MinusLogProbMetric: 17.2062, val_loss: 17.4157, val_MinusLogProbMetric: 17.4157

Epoch 792: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2062 - MinusLogProbMetric: 17.2062 - val_loss: 17.4157 - val_MinusLogProbMetric: 17.4157 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 793/1000
2023-09-27 05:46:52.585 
Epoch 793/1000 
	 loss: 17.2035, MinusLogProbMetric: 17.2035, val_loss: 17.3917, val_MinusLogProbMetric: 17.3917

Epoch 793: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2035 - MinusLogProbMetric: 17.2035 - val_loss: 17.3917 - val_MinusLogProbMetric: 17.3917 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 794/1000
2023-09-27 05:47:56.618 
Epoch 794/1000 
	 loss: 17.2060, MinusLogProbMetric: 17.2060, val_loss: 17.3527, val_MinusLogProbMetric: 17.3527

Epoch 794: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2060 - MinusLogProbMetric: 17.2060 - val_loss: 17.3527 - val_MinusLogProbMetric: 17.3527 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 795/1000
2023-09-27 05:49:00.958 
Epoch 795/1000 
	 loss: 17.2102, MinusLogProbMetric: 17.2102, val_loss: 17.3611, val_MinusLogProbMetric: 17.3611

Epoch 795: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2102 - MinusLogProbMetric: 17.2102 - val_loss: 17.3611 - val_MinusLogProbMetric: 17.3611 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 796/1000
2023-09-27 05:50:05.689 
Epoch 796/1000 
	 loss: 17.2069, MinusLogProbMetric: 17.2069, val_loss: 17.3681, val_MinusLogProbMetric: 17.3681

Epoch 796: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2069 - MinusLogProbMetric: 17.2069 - val_loss: 17.3681 - val_MinusLogProbMetric: 17.3681 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 797/1000
2023-09-27 05:51:10.334 
Epoch 797/1000 
	 loss: 17.2048, MinusLogProbMetric: 17.2048, val_loss: 17.3937, val_MinusLogProbMetric: 17.3937

Epoch 797: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2048 - MinusLogProbMetric: 17.2048 - val_loss: 17.3937 - val_MinusLogProbMetric: 17.3937 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 798/1000
2023-09-27 05:52:15.339 
Epoch 798/1000 
	 loss: 17.2064, MinusLogProbMetric: 17.2064, val_loss: 17.3789, val_MinusLogProbMetric: 17.3789

Epoch 798: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2064 - MinusLogProbMetric: 17.2064 - val_loss: 17.3789 - val_MinusLogProbMetric: 17.3789 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 799/1000
2023-09-27 05:53:20.081 
Epoch 799/1000 
	 loss: 17.2054, MinusLogProbMetric: 17.2054, val_loss: 17.3797, val_MinusLogProbMetric: 17.3797

Epoch 799: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2054 - MinusLogProbMetric: 17.2054 - val_loss: 17.3797 - val_MinusLogProbMetric: 17.3797 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 800/1000
2023-09-27 05:54:24.263 
Epoch 800/1000 
	 loss: 17.2014, MinusLogProbMetric: 17.2014, val_loss: 17.3688, val_MinusLogProbMetric: 17.3688

Epoch 800: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2014 - MinusLogProbMetric: 17.2014 - val_loss: 17.3688 - val_MinusLogProbMetric: 17.3688 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 801/1000
2023-09-27 05:55:28.513 
Epoch 801/1000 
	 loss: 17.2012, MinusLogProbMetric: 17.2012, val_loss: 17.3950, val_MinusLogProbMetric: 17.3950

Epoch 801: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2012 - MinusLogProbMetric: 17.2012 - val_loss: 17.3950 - val_MinusLogProbMetric: 17.3950 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 802/1000
2023-09-27 05:56:33.886 
Epoch 802/1000 
	 loss: 17.2039, MinusLogProbMetric: 17.2039, val_loss: 17.3572, val_MinusLogProbMetric: 17.3572

Epoch 802: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2039 - MinusLogProbMetric: 17.2039 - val_loss: 17.3572 - val_MinusLogProbMetric: 17.3572 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 803/1000
2023-09-27 05:57:38.785 
Epoch 803/1000 
	 loss: 17.1981, MinusLogProbMetric: 17.1981, val_loss: 17.3657, val_MinusLogProbMetric: 17.3657

Epoch 803: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.1981 - MinusLogProbMetric: 17.1981 - val_loss: 17.3657 - val_MinusLogProbMetric: 17.3657 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 804/1000
2023-09-27 05:58:44.060 
Epoch 804/1000 
	 loss: 17.2012, MinusLogProbMetric: 17.2012, val_loss: 17.3676, val_MinusLogProbMetric: 17.3676

Epoch 804: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2012 - MinusLogProbMetric: 17.2012 - val_loss: 17.3676 - val_MinusLogProbMetric: 17.3676 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 805/1000
2023-09-27 05:59:49.226 
Epoch 805/1000 
	 loss: 17.2046, MinusLogProbMetric: 17.2046, val_loss: 17.3927, val_MinusLogProbMetric: 17.3927

Epoch 805: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2046 - MinusLogProbMetric: 17.2046 - val_loss: 17.3927 - val_MinusLogProbMetric: 17.3927 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 806/1000
2023-09-27 06:00:54.596 
Epoch 806/1000 
	 loss: 17.2050, MinusLogProbMetric: 17.2050, val_loss: 17.3771, val_MinusLogProbMetric: 17.3771

Epoch 806: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2050 - MinusLogProbMetric: 17.2050 - val_loss: 17.3771 - val_MinusLogProbMetric: 17.3771 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 807/1000
2023-09-27 06:01:58.198 
Epoch 807/1000 
	 loss: 17.2007, MinusLogProbMetric: 17.2007, val_loss: 17.3853, val_MinusLogProbMetric: 17.3853

Epoch 807: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2007 - MinusLogProbMetric: 17.2007 - val_loss: 17.3853 - val_MinusLogProbMetric: 17.3853 - lr: 4.1667e-05 - 64s/epoch - 324ms/step
Epoch 808/1000
2023-09-27 06:03:02.016 
Epoch 808/1000 
	 loss: 17.2045, MinusLogProbMetric: 17.2045, val_loss: 17.3642, val_MinusLogProbMetric: 17.3642

Epoch 808: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2045 - MinusLogProbMetric: 17.2045 - val_loss: 17.3642 - val_MinusLogProbMetric: 17.3642 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 809/1000
2023-09-27 06:04:05.884 
Epoch 809/1000 
	 loss: 17.1984, MinusLogProbMetric: 17.1984, val_loss: 17.3783, val_MinusLogProbMetric: 17.3783

Epoch 809: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.1984 - MinusLogProbMetric: 17.1984 - val_loss: 17.3783 - val_MinusLogProbMetric: 17.3783 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 810/1000
2023-09-27 06:05:09.904 
Epoch 810/1000 
	 loss: 17.2022, MinusLogProbMetric: 17.2022, val_loss: 17.4181, val_MinusLogProbMetric: 17.4181

Epoch 810: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2022 - MinusLogProbMetric: 17.2022 - val_loss: 17.4181 - val_MinusLogProbMetric: 17.4181 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 811/1000
2023-09-27 06:06:14.614 
Epoch 811/1000 
	 loss: 17.1995, MinusLogProbMetric: 17.1995, val_loss: 17.3870, val_MinusLogProbMetric: 17.3870

Epoch 811: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.1995 - MinusLogProbMetric: 17.1995 - val_loss: 17.3870 - val_MinusLogProbMetric: 17.3870 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 812/1000
2023-09-27 06:07:19.176 
Epoch 812/1000 
	 loss: 17.2078, MinusLogProbMetric: 17.2078, val_loss: 17.3877, val_MinusLogProbMetric: 17.3877

Epoch 812: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2078 - MinusLogProbMetric: 17.2078 - val_loss: 17.3877 - val_MinusLogProbMetric: 17.3877 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 813/1000
2023-09-27 06:08:23.182 
Epoch 813/1000 
	 loss: 17.2060, MinusLogProbMetric: 17.2060, val_loss: 17.3845, val_MinusLogProbMetric: 17.3845

Epoch 813: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2060 - MinusLogProbMetric: 17.2060 - val_loss: 17.3845 - val_MinusLogProbMetric: 17.3845 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 814/1000
2023-09-27 06:09:27.528 
Epoch 814/1000 
	 loss: 17.2018, MinusLogProbMetric: 17.2018, val_loss: 17.4496, val_MinusLogProbMetric: 17.4496

Epoch 814: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2018 - MinusLogProbMetric: 17.2018 - val_loss: 17.4496 - val_MinusLogProbMetric: 17.4496 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 815/1000
2023-09-27 06:10:32.695 
Epoch 815/1000 
	 loss: 17.2114, MinusLogProbMetric: 17.2114, val_loss: 17.3676, val_MinusLogProbMetric: 17.3676

Epoch 815: val_loss did not improve from 17.34830
196/196 - 65s - loss: 17.2114 - MinusLogProbMetric: 17.2114 - val_loss: 17.3676 - val_MinusLogProbMetric: 17.3676 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 816/1000
2023-09-27 06:11:37.029 
Epoch 816/1000 
	 loss: 17.2014, MinusLogProbMetric: 17.2014, val_loss: 17.3621, val_MinusLogProbMetric: 17.3621

Epoch 816: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2014 - MinusLogProbMetric: 17.2014 - val_loss: 17.3621 - val_MinusLogProbMetric: 17.3621 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 817/1000
2023-09-27 06:12:40.685 
Epoch 817/1000 
	 loss: 17.2062, MinusLogProbMetric: 17.2062, val_loss: 17.3787, val_MinusLogProbMetric: 17.3787

Epoch 817: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2062 - MinusLogProbMetric: 17.2062 - val_loss: 17.3787 - val_MinusLogProbMetric: 17.3787 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 818/1000
2023-09-27 06:13:43.195 
Epoch 818/1000 
	 loss: 17.1998, MinusLogProbMetric: 17.1998, val_loss: 17.3787, val_MinusLogProbMetric: 17.3787

Epoch 818: val_loss did not improve from 17.34830
196/196 - 63s - loss: 17.1998 - MinusLogProbMetric: 17.1998 - val_loss: 17.3787 - val_MinusLogProbMetric: 17.3787 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 819/1000
2023-09-27 06:14:47.325 
Epoch 819/1000 
	 loss: 17.2043, MinusLogProbMetric: 17.2043, val_loss: 17.3748, val_MinusLogProbMetric: 17.3748

Epoch 819: val_loss did not improve from 17.34830
196/196 - 64s - loss: 17.2043 - MinusLogProbMetric: 17.2043 - val_loss: 17.3748 - val_MinusLogProbMetric: 17.3748 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 820/1000
2023-09-27 06:15:50.150 
Epoch 820/1000 
	 loss: 17.1943, MinusLogProbMetric: 17.1943, val_loss: 17.3621, val_MinusLogProbMetric: 17.3621

Epoch 820: val_loss did not improve from 17.34830
196/196 - 63s - loss: 17.1943 - MinusLogProbMetric: 17.1943 - val_loss: 17.3621 - val_MinusLogProbMetric: 17.3621 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 821/1000
2023-09-27 06:16:53.130 
Epoch 821/1000 
	 loss: 17.1982, MinusLogProbMetric: 17.1982, val_loss: 17.4026, val_MinusLogProbMetric: 17.4026

Epoch 821: val_loss did not improve from 17.34830
196/196 - 63s - loss: 17.1982 - MinusLogProbMetric: 17.1982 - val_loss: 17.4026 - val_MinusLogProbMetric: 17.4026 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 822/1000
2023-09-27 06:17:55.837 
Epoch 822/1000 
	 loss: 17.1994, MinusLogProbMetric: 17.1994, val_loss: 17.3881, val_MinusLogProbMetric: 17.3881

Epoch 822: val_loss did not improve from 17.34830
196/196 - 63s - loss: 17.1994 - MinusLogProbMetric: 17.1994 - val_loss: 17.3881 - val_MinusLogProbMetric: 17.3881 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 823/1000
2023-09-27 06:18:58.827 
Epoch 823/1000 
	 loss: 17.2048, MinusLogProbMetric: 17.2048, val_loss: 17.3664, val_MinusLogProbMetric: 17.3664

Epoch 823: val_loss did not improve from 17.34830
196/196 - 63s - loss: 17.2048 - MinusLogProbMetric: 17.2048 - val_loss: 17.3664 - val_MinusLogProbMetric: 17.3664 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 824/1000
2023-09-27 06:20:02.301 
Epoch 824/1000 
	 loss: 17.1981, MinusLogProbMetric: 17.1981, val_loss: 17.3466, val_MinusLogProbMetric: 17.3466

Epoch 824: val_loss improved from 17.34830 to 17.34659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 65s - loss: 17.1981 - MinusLogProbMetric: 17.1981 - val_loss: 17.3466 - val_MinusLogProbMetric: 17.3466 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 825/1000
2023-09-27 06:21:06.511 
Epoch 825/1000 
	 loss: 17.1981, MinusLogProbMetric: 17.1981, val_loss: 17.3530, val_MinusLogProbMetric: 17.3530

Epoch 825: val_loss did not improve from 17.34659
196/196 - 63s - loss: 17.1981 - MinusLogProbMetric: 17.1981 - val_loss: 17.3530 - val_MinusLogProbMetric: 17.3530 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 826/1000
2023-09-27 06:22:09.229 
Epoch 826/1000 
	 loss: 17.1964, MinusLogProbMetric: 17.1964, val_loss: 17.3541, val_MinusLogProbMetric: 17.3541

Epoch 826: val_loss did not improve from 17.34659
196/196 - 63s - loss: 17.1964 - MinusLogProbMetric: 17.1964 - val_loss: 17.3541 - val_MinusLogProbMetric: 17.3541 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 827/1000
2023-09-27 06:23:12.296 
Epoch 827/1000 
	 loss: 17.1997, MinusLogProbMetric: 17.1997, val_loss: 17.4013, val_MinusLogProbMetric: 17.4013

Epoch 827: val_loss did not improve from 17.34659
196/196 - 63s - loss: 17.1997 - MinusLogProbMetric: 17.1997 - val_loss: 17.4013 - val_MinusLogProbMetric: 17.4013 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 828/1000
2023-09-27 06:24:15.662 
Epoch 828/1000 
	 loss: 17.1959, MinusLogProbMetric: 17.1959, val_loss: 17.3790, val_MinusLogProbMetric: 17.3790

Epoch 828: val_loss did not improve from 17.34659
196/196 - 63s - loss: 17.1959 - MinusLogProbMetric: 17.1959 - val_loss: 17.3790 - val_MinusLogProbMetric: 17.3790 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 829/1000
2023-09-27 06:25:18.745 
Epoch 829/1000 
	 loss: 17.1911, MinusLogProbMetric: 17.1911, val_loss: 17.3563, val_MinusLogProbMetric: 17.3563

Epoch 829: val_loss did not improve from 17.34659
196/196 - 63s - loss: 17.1911 - MinusLogProbMetric: 17.1911 - val_loss: 17.3563 - val_MinusLogProbMetric: 17.3563 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 830/1000
2023-09-27 06:26:22.035 
Epoch 830/1000 
	 loss: 17.1968, MinusLogProbMetric: 17.1968, val_loss: 17.3759, val_MinusLogProbMetric: 17.3759

Epoch 830: val_loss did not improve from 17.34659
196/196 - 63s - loss: 17.1968 - MinusLogProbMetric: 17.1968 - val_loss: 17.3759 - val_MinusLogProbMetric: 17.3759 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 831/1000
2023-09-27 06:27:25.495 
Epoch 831/1000 
	 loss: 17.1963, MinusLogProbMetric: 17.1963, val_loss: 17.3815, val_MinusLogProbMetric: 17.3815

Epoch 831: val_loss did not improve from 17.34659
196/196 - 63s - loss: 17.1963 - MinusLogProbMetric: 17.1963 - val_loss: 17.3815 - val_MinusLogProbMetric: 17.3815 - lr: 4.1667e-05 - 63s/epoch - 324ms/step
Epoch 832/1000
2023-09-27 06:28:29.544 
Epoch 832/1000 
	 loss: 17.2001, MinusLogProbMetric: 17.2001, val_loss: 17.3506, val_MinusLogProbMetric: 17.3506

Epoch 832: val_loss did not improve from 17.34659
196/196 - 64s - loss: 17.2001 - MinusLogProbMetric: 17.2001 - val_loss: 17.3506 - val_MinusLogProbMetric: 17.3506 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 833/1000
2023-09-27 06:29:32.390 
Epoch 833/1000 
	 loss: 17.1961, MinusLogProbMetric: 17.1961, val_loss: 17.3492, val_MinusLogProbMetric: 17.3492

Epoch 833: val_loss did not improve from 17.34659
196/196 - 63s - loss: 17.1961 - MinusLogProbMetric: 17.1961 - val_loss: 17.3492 - val_MinusLogProbMetric: 17.3492 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 834/1000
2023-09-27 06:30:37.105 
Epoch 834/1000 
	 loss: 17.1975, MinusLogProbMetric: 17.1975, val_loss: 17.3727, val_MinusLogProbMetric: 17.3727

Epoch 834: val_loss did not improve from 17.34659
196/196 - 65s - loss: 17.1975 - MinusLogProbMetric: 17.1975 - val_loss: 17.3727 - val_MinusLogProbMetric: 17.3727 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 835/1000
2023-09-27 06:31:41.837 
Epoch 835/1000 
	 loss: 17.1982, MinusLogProbMetric: 17.1982, val_loss: 17.3412, val_MinusLogProbMetric: 17.3412

Epoch 835: val_loss improved from 17.34659 to 17.34122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 66s - loss: 17.1982 - MinusLogProbMetric: 17.1982 - val_loss: 17.3412 - val_MinusLogProbMetric: 17.3412 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 836/1000
2023-09-27 06:32:47.040 
Epoch 836/1000 
	 loss: 17.1916, MinusLogProbMetric: 17.1916, val_loss: 17.3492, val_MinusLogProbMetric: 17.3492

Epoch 836: val_loss did not improve from 17.34122
196/196 - 64s - loss: 17.1916 - MinusLogProbMetric: 17.1916 - val_loss: 17.3492 - val_MinusLogProbMetric: 17.3492 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 837/1000
2023-09-27 06:33:51.044 
Epoch 837/1000 
	 loss: 17.1910, MinusLogProbMetric: 17.1910, val_loss: 17.3461, val_MinusLogProbMetric: 17.3461

Epoch 837: val_loss did not improve from 17.34122
196/196 - 64s - loss: 17.1910 - MinusLogProbMetric: 17.1910 - val_loss: 17.3461 - val_MinusLogProbMetric: 17.3461 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 838/1000
2023-09-27 06:34:55.301 
Epoch 838/1000 
	 loss: 17.1996, MinusLogProbMetric: 17.1996, val_loss: 17.4377, val_MinusLogProbMetric: 17.4377

Epoch 838: val_loss did not improve from 17.34122
196/196 - 64s - loss: 17.1996 - MinusLogProbMetric: 17.1996 - val_loss: 17.4377 - val_MinusLogProbMetric: 17.4377 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 839/1000
2023-09-27 06:35:59.205 
Epoch 839/1000 
	 loss: 17.1984, MinusLogProbMetric: 17.1984, val_loss: 17.3471, val_MinusLogProbMetric: 17.3471

Epoch 839: val_loss did not improve from 17.34122
196/196 - 64s - loss: 17.1984 - MinusLogProbMetric: 17.1984 - val_loss: 17.3471 - val_MinusLogProbMetric: 17.3471 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 840/1000
2023-09-27 06:37:02.315 
Epoch 840/1000 
	 loss: 17.1955, MinusLogProbMetric: 17.1955, val_loss: 17.3636, val_MinusLogProbMetric: 17.3636

Epoch 840: val_loss did not improve from 17.34122
196/196 - 63s - loss: 17.1955 - MinusLogProbMetric: 17.1955 - val_loss: 17.3636 - val_MinusLogProbMetric: 17.3636 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 841/1000
2023-09-27 06:38:04.458 
Epoch 841/1000 
	 loss: 17.1981, MinusLogProbMetric: 17.1981, val_loss: 17.3690, val_MinusLogProbMetric: 17.3690

Epoch 841: val_loss did not improve from 17.34122
196/196 - 62s - loss: 17.1981 - MinusLogProbMetric: 17.1981 - val_loss: 17.3690 - val_MinusLogProbMetric: 17.3690 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 842/1000
2023-09-27 06:39:08.090 
Epoch 842/1000 
	 loss: 17.1905, MinusLogProbMetric: 17.1905, val_loss: 17.3826, val_MinusLogProbMetric: 17.3826

Epoch 842: val_loss did not improve from 17.34122
196/196 - 64s - loss: 17.1905 - MinusLogProbMetric: 17.1905 - val_loss: 17.3826 - val_MinusLogProbMetric: 17.3826 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 843/1000
2023-09-27 06:40:12.746 
Epoch 843/1000 
	 loss: 17.1934, MinusLogProbMetric: 17.1934, val_loss: 17.3564, val_MinusLogProbMetric: 17.3564

Epoch 843: val_loss did not improve from 17.34122
196/196 - 65s - loss: 17.1934 - MinusLogProbMetric: 17.1934 - val_loss: 17.3564 - val_MinusLogProbMetric: 17.3564 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 844/1000
2023-09-27 06:41:15.073 
Epoch 844/1000 
	 loss: 17.1883, MinusLogProbMetric: 17.1883, val_loss: 17.4271, val_MinusLogProbMetric: 17.4271

Epoch 844: val_loss did not improve from 17.34122
196/196 - 62s - loss: 17.1883 - MinusLogProbMetric: 17.1883 - val_loss: 17.4271 - val_MinusLogProbMetric: 17.4271 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 845/1000
2023-09-27 06:42:18.013 
Epoch 845/1000 
	 loss: 17.1984, MinusLogProbMetric: 17.1984, val_loss: 17.3760, val_MinusLogProbMetric: 17.3760

Epoch 845: val_loss did not improve from 17.34122
196/196 - 63s - loss: 17.1984 - MinusLogProbMetric: 17.1984 - val_loss: 17.3760 - val_MinusLogProbMetric: 17.3760 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 846/1000
2023-09-27 06:43:20.499 
Epoch 846/1000 
	 loss: 17.1951, MinusLogProbMetric: 17.1951, val_loss: 17.3496, val_MinusLogProbMetric: 17.3496

Epoch 846: val_loss did not improve from 17.34122
196/196 - 62s - loss: 17.1951 - MinusLogProbMetric: 17.1951 - val_loss: 17.3496 - val_MinusLogProbMetric: 17.3496 - lr: 4.1667e-05 - 62s/epoch - 319ms/step
Epoch 847/1000
2023-09-27 06:44:23.635 
Epoch 847/1000 
	 loss: 17.1987, MinusLogProbMetric: 17.1987, val_loss: 17.3476, val_MinusLogProbMetric: 17.3476

Epoch 847: val_loss did not improve from 17.34122
196/196 - 63s - loss: 17.1987 - MinusLogProbMetric: 17.1987 - val_loss: 17.3476 - val_MinusLogProbMetric: 17.3476 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 848/1000
2023-09-27 06:45:26.793 
Epoch 848/1000 
	 loss: 17.1921, MinusLogProbMetric: 17.1921, val_loss: 17.3510, val_MinusLogProbMetric: 17.3510

Epoch 848: val_loss did not improve from 17.34122
196/196 - 63s - loss: 17.1921 - MinusLogProbMetric: 17.1921 - val_loss: 17.3510 - val_MinusLogProbMetric: 17.3510 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 849/1000
2023-09-27 06:46:30.452 
Epoch 849/1000 
	 loss: 17.1915, MinusLogProbMetric: 17.1915, val_loss: 17.3817, val_MinusLogProbMetric: 17.3817

Epoch 849: val_loss did not improve from 17.34122
196/196 - 64s - loss: 17.1915 - MinusLogProbMetric: 17.1915 - val_loss: 17.3817 - val_MinusLogProbMetric: 17.3817 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 850/1000
2023-09-27 06:47:33.663 
Epoch 850/1000 
	 loss: 17.1914, MinusLogProbMetric: 17.1914, val_loss: 17.3789, val_MinusLogProbMetric: 17.3789

Epoch 850: val_loss did not improve from 17.34122
196/196 - 63s - loss: 17.1914 - MinusLogProbMetric: 17.1914 - val_loss: 17.3789 - val_MinusLogProbMetric: 17.3789 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 851/1000
2023-09-27 06:48:35.723 
Epoch 851/1000 
	 loss: 17.1931, MinusLogProbMetric: 17.1931, val_loss: 17.3529, val_MinusLogProbMetric: 17.3529

Epoch 851: val_loss did not improve from 17.34122
196/196 - 62s - loss: 17.1931 - MinusLogProbMetric: 17.1931 - val_loss: 17.3529 - val_MinusLogProbMetric: 17.3529 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 852/1000
2023-09-27 06:49:39.387 
Epoch 852/1000 
	 loss: 17.2017, MinusLogProbMetric: 17.2017, val_loss: 17.3780, val_MinusLogProbMetric: 17.3780

Epoch 852: val_loss did not improve from 17.34122
196/196 - 64s - loss: 17.2017 - MinusLogProbMetric: 17.2017 - val_loss: 17.3780 - val_MinusLogProbMetric: 17.3780 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 853/1000
2023-09-27 06:50:40.828 
Epoch 853/1000 
	 loss: 17.1956, MinusLogProbMetric: 17.1956, val_loss: 17.3411, val_MinusLogProbMetric: 17.3411

Epoch 853: val_loss improved from 17.34122 to 17.34108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 63s - loss: 17.1956 - MinusLogProbMetric: 17.1956 - val_loss: 17.3411 - val_MinusLogProbMetric: 17.3411 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 854/1000
2023-09-27 06:51:45.328 
Epoch 854/1000 
	 loss: 17.1908, MinusLogProbMetric: 17.1908, val_loss: 17.3764, val_MinusLogProbMetric: 17.3764

Epoch 854: val_loss did not improve from 17.34108
196/196 - 63s - loss: 17.1908 - MinusLogProbMetric: 17.1908 - val_loss: 17.3764 - val_MinusLogProbMetric: 17.3764 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 855/1000
2023-09-27 06:52:48.642 
Epoch 855/1000 
	 loss: 17.1849, MinusLogProbMetric: 17.1849, val_loss: 17.3417, val_MinusLogProbMetric: 17.3417

Epoch 855: val_loss did not improve from 17.34108
196/196 - 63s - loss: 17.1849 - MinusLogProbMetric: 17.1849 - val_loss: 17.3417 - val_MinusLogProbMetric: 17.3417 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 856/1000
2023-09-27 06:53:51.266 
Epoch 856/1000 
	 loss: 17.1835, MinusLogProbMetric: 17.1835, val_loss: 17.4097, val_MinusLogProbMetric: 17.4097

Epoch 856: val_loss did not improve from 17.34108
196/196 - 63s - loss: 17.1835 - MinusLogProbMetric: 17.1835 - val_loss: 17.4097 - val_MinusLogProbMetric: 17.4097 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 857/1000
2023-09-27 06:54:54.841 
Epoch 857/1000 
	 loss: 17.1948, MinusLogProbMetric: 17.1948, val_loss: 17.4061, val_MinusLogProbMetric: 17.4061

Epoch 857: val_loss did not improve from 17.34108
196/196 - 64s - loss: 17.1948 - MinusLogProbMetric: 17.1948 - val_loss: 17.4061 - val_MinusLogProbMetric: 17.4061 - lr: 4.1667e-05 - 64s/epoch - 324ms/step
Epoch 858/1000
2023-09-27 06:55:57.035 
Epoch 858/1000 
	 loss: 17.1898, MinusLogProbMetric: 17.1898, val_loss: 17.3501, val_MinusLogProbMetric: 17.3501

Epoch 858: val_loss did not improve from 17.34108
196/196 - 62s - loss: 17.1898 - MinusLogProbMetric: 17.1898 - val_loss: 17.3501 - val_MinusLogProbMetric: 17.3501 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 859/1000
2023-09-27 06:56:59.950 
Epoch 859/1000 
	 loss: 17.1910, MinusLogProbMetric: 17.1910, val_loss: 17.3751, val_MinusLogProbMetric: 17.3751

Epoch 859: val_loss did not improve from 17.34108
196/196 - 63s - loss: 17.1910 - MinusLogProbMetric: 17.1910 - val_loss: 17.3751 - val_MinusLogProbMetric: 17.3751 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 860/1000
2023-09-27 06:58:02.975 
Epoch 860/1000 
	 loss: 17.1893, MinusLogProbMetric: 17.1893, val_loss: 17.3500, val_MinusLogProbMetric: 17.3500

Epoch 860: val_loss did not improve from 17.34108
196/196 - 63s - loss: 17.1893 - MinusLogProbMetric: 17.1893 - val_loss: 17.3500 - val_MinusLogProbMetric: 17.3500 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 861/1000
2023-09-27 06:59:05.966 
Epoch 861/1000 
	 loss: 17.1881, MinusLogProbMetric: 17.1881, val_loss: 17.3406, val_MinusLogProbMetric: 17.3406

Epoch 861: val_loss improved from 17.34108 to 17.34058, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 64s - loss: 17.1881 - MinusLogProbMetric: 17.1881 - val_loss: 17.3406 - val_MinusLogProbMetric: 17.3406 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 862/1000
2023-09-27 07:00:09.767 
Epoch 862/1000 
	 loss: 17.1858, MinusLogProbMetric: 17.1858, val_loss: 17.3353, val_MinusLogProbMetric: 17.3353

Epoch 862: val_loss improved from 17.34058 to 17.33526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 64s - loss: 17.1858 - MinusLogProbMetric: 17.1858 - val_loss: 17.3353 - val_MinusLogProbMetric: 17.3353 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 863/1000
2023-09-27 07:01:12.836 
Epoch 863/1000 
	 loss: 17.1937, MinusLogProbMetric: 17.1937, val_loss: 17.3441, val_MinusLogProbMetric: 17.3441

Epoch 863: val_loss did not improve from 17.33526
196/196 - 62s - loss: 17.1937 - MinusLogProbMetric: 17.1937 - val_loss: 17.3441 - val_MinusLogProbMetric: 17.3441 - lr: 4.1667e-05 - 62s/epoch - 316ms/step
Epoch 864/1000
2023-09-27 07:02:16.883 
Epoch 864/1000 
	 loss: 17.1836, MinusLogProbMetric: 17.1836, val_loss: 17.3743, val_MinusLogProbMetric: 17.3743

Epoch 864: val_loss did not improve from 17.33526
196/196 - 64s - loss: 17.1836 - MinusLogProbMetric: 17.1836 - val_loss: 17.3743 - val_MinusLogProbMetric: 17.3743 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 865/1000
2023-09-27 07:03:18.455 
Epoch 865/1000 
	 loss: 17.1916, MinusLogProbMetric: 17.1916, val_loss: 17.3710, val_MinusLogProbMetric: 17.3710

Epoch 865: val_loss did not improve from 17.33526
196/196 - 62s - loss: 17.1916 - MinusLogProbMetric: 17.1916 - val_loss: 17.3710 - val_MinusLogProbMetric: 17.3710 - lr: 4.1667e-05 - 62s/epoch - 314ms/step
Epoch 866/1000
2023-09-27 07:04:21.392 
Epoch 866/1000 
	 loss: 17.1872, MinusLogProbMetric: 17.1872, val_loss: 17.3860, val_MinusLogProbMetric: 17.3860

Epoch 866: val_loss did not improve from 17.33526
196/196 - 63s - loss: 17.1872 - MinusLogProbMetric: 17.1872 - val_loss: 17.3860 - val_MinusLogProbMetric: 17.3860 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 867/1000
2023-09-27 07:05:24.568 
Epoch 867/1000 
	 loss: 17.1946, MinusLogProbMetric: 17.1946, val_loss: 17.3624, val_MinusLogProbMetric: 17.3624

Epoch 867: val_loss did not improve from 17.33526
196/196 - 63s - loss: 17.1946 - MinusLogProbMetric: 17.1946 - val_loss: 17.3624 - val_MinusLogProbMetric: 17.3624 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 868/1000
2023-09-27 07:06:27.629 
Epoch 868/1000 
	 loss: 17.1874, MinusLogProbMetric: 17.1874, val_loss: 17.3245, val_MinusLogProbMetric: 17.3245

Epoch 868: val_loss improved from 17.33526 to 17.32446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 64s - loss: 17.1874 - MinusLogProbMetric: 17.1874 - val_loss: 17.3245 - val_MinusLogProbMetric: 17.3245 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 869/1000
2023-09-27 07:07:30.558 
Epoch 869/1000 
	 loss: 17.1931, MinusLogProbMetric: 17.1931, val_loss: 17.3525, val_MinusLogProbMetric: 17.3525

Epoch 869: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1931 - MinusLogProbMetric: 17.1931 - val_loss: 17.3525 - val_MinusLogProbMetric: 17.3525 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 870/1000
2023-09-27 07:08:33.996 
Epoch 870/1000 
	 loss: 17.1869, MinusLogProbMetric: 17.1869, val_loss: 17.3483, val_MinusLogProbMetric: 17.3483

Epoch 870: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1869 - MinusLogProbMetric: 17.1869 - val_loss: 17.3483 - val_MinusLogProbMetric: 17.3483 - lr: 4.1667e-05 - 63s/epoch - 324ms/step
Epoch 871/1000
2023-09-27 07:09:36.730 
Epoch 871/1000 
	 loss: 17.1913, MinusLogProbMetric: 17.1913, val_loss: 17.3841, val_MinusLogProbMetric: 17.3841

Epoch 871: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1913 - MinusLogProbMetric: 17.1913 - val_loss: 17.3841 - val_MinusLogProbMetric: 17.3841 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 872/1000
2023-09-27 07:10:39.262 
Epoch 872/1000 
	 loss: 17.1883, MinusLogProbMetric: 17.1883, val_loss: 17.3972, val_MinusLogProbMetric: 17.3972

Epoch 872: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1883 - MinusLogProbMetric: 17.1883 - val_loss: 17.3972 - val_MinusLogProbMetric: 17.3972 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 873/1000
2023-09-27 07:11:41.713 
Epoch 873/1000 
	 loss: 17.1939, MinusLogProbMetric: 17.1939, val_loss: 17.3591, val_MinusLogProbMetric: 17.3591

Epoch 873: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1939 - MinusLogProbMetric: 17.1939 - val_loss: 17.3591 - val_MinusLogProbMetric: 17.3591 - lr: 4.1667e-05 - 62s/epoch - 319ms/step
Epoch 874/1000
2023-09-27 07:12:45.202 
Epoch 874/1000 
	 loss: 17.1887, MinusLogProbMetric: 17.1887, val_loss: 17.3629, val_MinusLogProbMetric: 17.3629

Epoch 874: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1887 - MinusLogProbMetric: 17.1887 - val_loss: 17.3629 - val_MinusLogProbMetric: 17.3629 - lr: 4.1667e-05 - 63s/epoch - 324ms/step
Epoch 875/1000
2023-09-27 07:13:46.708 
Epoch 875/1000 
	 loss: 17.1840, MinusLogProbMetric: 17.1840, val_loss: 17.3689, val_MinusLogProbMetric: 17.3689

Epoch 875: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1840 - MinusLogProbMetric: 17.1840 - val_loss: 17.3689 - val_MinusLogProbMetric: 17.3689 - lr: 4.1667e-05 - 62s/epoch - 314ms/step
Epoch 876/1000
2023-09-27 07:14:49.981 
Epoch 876/1000 
	 loss: 17.1912, MinusLogProbMetric: 17.1912, val_loss: 17.3529, val_MinusLogProbMetric: 17.3529

Epoch 876: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1912 - MinusLogProbMetric: 17.1912 - val_loss: 17.3529 - val_MinusLogProbMetric: 17.3529 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 877/1000
2023-09-27 07:15:52.804 
Epoch 877/1000 
	 loss: 17.1878, MinusLogProbMetric: 17.1878, val_loss: 17.4023, val_MinusLogProbMetric: 17.4023

Epoch 877: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1878 - MinusLogProbMetric: 17.1878 - val_loss: 17.4023 - val_MinusLogProbMetric: 17.4023 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 878/1000
2023-09-27 07:16:55.287 
Epoch 878/1000 
	 loss: 17.1855, MinusLogProbMetric: 17.1855, val_loss: 17.3472, val_MinusLogProbMetric: 17.3472

Epoch 878: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1855 - MinusLogProbMetric: 17.1855 - val_loss: 17.3472 - val_MinusLogProbMetric: 17.3472 - lr: 4.1667e-05 - 62s/epoch - 319ms/step
Epoch 879/1000
2023-09-27 07:17:58.194 
Epoch 879/1000 
	 loss: 17.1808, MinusLogProbMetric: 17.1808, val_loss: 17.3652, val_MinusLogProbMetric: 17.3652

Epoch 879: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1808 - MinusLogProbMetric: 17.1808 - val_loss: 17.3652 - val_MinusLogProbMetric: 17.3652 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 880/1000
2023-09-27 07:18:53.480 
Epoch 880/1000 
	 loss: 17.1834, MinusLogProbMetric: 17.1834, val_loss: 17.3431, val_MinusLogProbMetric: 17.3431

Epoch 880: val_loss did not improve from 17.32446
196/196 - 55s - loss: 17.1834 - MinusLogProbMetric: 17.1834 - val_loss: 17.3431 - val_MinusLogProbMetric: 17.3431 - lr: 4.1667e-05 - 55s/epoch - 282ms/step
Epoch 881/1000
2023-09-27 07:19:57.546 
Epoch 881/1000 
	 loss: 17.1838, MinusLogProbMetric: 17.1838, val_loss: 17.3694, val_MinusLogProbMetric: 17.3694

Epoch 881: val_loss did not improve from 17.32446
196/196 - 64s - loss: 17.1838 - MinusLogProbMetric: 17.1838 - val_loss: 17.3694 - val_MinusLogProbMetric: 17.3694 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 882/1000
2023-09-27 07:20:59.233 
Epoch 882/1000 
	 loss: 17.1847, MinusLogProbMetric: 17.1847, val_loss: 17.3401, val_MinusLogProbMetric: 17.3401

Epoch 882: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1847 - MinusLogProbMetric: 17.1847 - val_loss: 17.3401 - val_MinusLogProbMetric: 17.3401 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 883/1000
2023-09-27 07:21:55.021 
Epoch 883/1000 
	 loss: 17.1821, MinusLogProbMetric: 17.1821, val_loss: 17.3420, val_MinusLogProbMetric: 17.3420

Epoch 883: val_loss did not improve from 17.32446
196/196 - 56s - loss: 17.1821 - MinusLogProbMetric: 17.1821 - val_loss: 17.3420 - val_MinusLogProbMetric: 17.3420 - lr: 4.1667e-05 - 56s/epoch - 285ms/step
Epoch 884/1000
2023-09-27 07:23:01.701 
Epoch 884/1000 
	 loss: 17.1814, MinusLogProbMetric: 17.1814, val_loss: 17.3647, val_MinusLogProbMetric: 17.3647

Epoch 884: val_loss did not improve from 17.32446
196/196 - 67s - loss: 17.1814 - MinusLogProbMetric: 17.1814 - val_loss: 17.3647 - val_MinusLogProbMetric: 17.3647 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 885/1000
2023-09-27 07:24:07.516 
Epoch 885/1000 
	 loss: 17.1863, MinusLogProbMetric: 17.1863, val_loss: 17.3529, val_MinusLogProbMetric: 17.3529

Epoch 885: val_loss did not improve from 17.32446
196/196 - 66s - loss: 17.1863 - MinusLogProbMetric: 17.1863 - val_loss: 17.3529 - val_MinusLogProbMetric: 17.3529 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 886/1000
2023-09-27 07:25:14.633 
Epoch 886/1000 
	 loss: 17.1868, MinusLogProbMetric: 17.1868, val_loss: 17.3404, val_MinusLogProbMetric: 17.3404

Epoch 886: val_loss did not improve from 17.32446
196/196 - 67s - loss: 17.1868 - MinusLogProbMetric: 17.1868 - val_loss: 17.3404 - val_MinusLogProbMetric: 17.3404 - lr: 4.1667e-05 - 67s/epoch - 342ms/step
Epoch 887/1000
2023-09-27 07:26:21.184 
Epoch 887/1000 
	 loss: 17.1908, MinusLogProbMetric: 17.1908, val_loss: 17.3445, val_MinusLogProbMetric: 17.3445

Epoch 887: val_loss did not improve from 17.32446
196/196 - 67s - loss: 17.1908 - MinusLogProbMetric: 17.1908 - val_loss: 17.3445 - val_MinusLogProbMetric: 17.3445 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 888/1000
2023-09-27 07:27:25.255 
Epoch 888/1000 
	 loss: 17.1902, MinusLogProbMetric: 17.1902, val_loss: 17.3438, val_MinusLogProbMetric: 17.3438

Epoch 888: val_loss did not improve from 17.32446
196/196 - 64s - loss: 17.1902 - MinusLogProbMetric: 17.1902 - val_loss: 17.3438 - val_MinusLogProbMetric: 17.3438 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 889/1000
2023-09-27 07:28:28.466 
Epoch 889/1000 
	 loss: 17.1819, MinusLogProbMetric: 17.1819, val_loss: 17.3364, val_MinusLogProbMetric: 17.3364

Epoch 889: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1819 - MinusLogProbMetric: 17.1819 - val_loss: 17.3364 - val_MinusLogProbMetric: 17.3364 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 890/1000
2023-09-27 07:29:29.769 
Epoch 890/1000 
	 loss: 17.1901, MinusLogProbMetric: 17.1901, val_loss: 17.3699, val_MinusLogProbMetric: 17.3699

Epoch 890: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1901 - MinusLogProbMetric: 17.1901 - val_loss: 17.3699 - val_MinusLogProbMetric: 17.3699 - lr: 4.1667e-05 - 61s/epoch - 313ms/step
Epoch 891/1000
2023-09-27 07:30:32.466 
Epoch 891/1000 
	 loss: 17.2026, MinusLogProbMetric: 17.2026, val_loss: 17.3501, val_MinusLogProbMetric: 17.3501

Epoch 891: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.2026 - MinusLogProbMetric: 17.2026 - val_loss: 17.3501 - val_MinusLogProbMetric: 17.3501 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 892/1000
2023-09-27 07:31:35.686 
Epoch 892/1000 
	 loss: 17.1892, MinusLogProbMetric: 17.1892, val_loss: 17.3698, val_MinusLogProbMetric: 17.3698

Epoch 892: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1892 - MinusLogProbMetric: 17.1892 - val_loss: 17.3698 - val_MinusLogProbMetric: 17.3698 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 893/1000
2023-09-27 07:32:37.923 
Epoch 893/1000 
	 loss: 17.1898, MinusLogProbMetric: 17.1898, val_loss: 17.3777, val_MinusLogProbMetric: 17.3777

Epoch 893: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1898 - MinusLogProbMetric: 17.1898 - val_loss: 17.3777 - val_MinusLogProbMetric: 17.3777 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 894/1000
2023-09-27 07:33:40.966 
Epoch 894/1000 
	 loss: 17.1887, MinusLogProbMetric: 17.1887, val_loss: 17.3756, val_MinusLogProbMetric: 17.3756

Epoch 894: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1887 - MinusLogProbMetric: 17.1887 - val_loss: 17.3756 - val_MinusLogProbMetric: 17.3756 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 895/1000
2023-09-27 07:34:43.008 
Epoch 895/1000 
	 loss: 17.1828, MinusLogProbMetric: 17.1828, val_loss: 17.3448, val_MinusLogProbMetric: 17.3448

Epoch 895: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1828 - MinusLogProbMetric: 17.1828 - val_loss: 17.3448 - val_MinusLogProbMetric: 17.3448 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 896/1000
2023-09-27 07:35:45.568 
Epoch 896/1000 
	 loss: 17.1760, MinusLogProbMetric: 17.1760, val_loss: 17.3557, val_MinusLogProbMetric: 17.3557

Epoch 896: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1760 - MinusLogProbMetric: 17.1760 - val_loss: 17.3557 - val_MinusLogProbMetric: 17.3557 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 897/1000
2023-09-27 07:36:48.140 
Epoch 897/1000 
	 loss: 17.1859, MinusLogProbMetric: 17.1859, val_loss: 17.3553, val_MinusLogProbMetric: 17.3553

Epoch 897: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1859 - MinusLogProbMetric: 17.1859 - val_loss: 17.3553 - val_MinusLogProbMetric: 17.3553 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 898/1000
2023-09-27 07:37:51.524 
Epoch 898/1000 
	 loss: 17.1872, MinusLogProbMetric: 17.1872, val_loss: 17.3600, val_MinusLogProbMetric: 17.3600

Epoch 898: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1872 - MinusLogProbMetric: 17.1872 - val_loss: 17.3600 - val_MinusLogProbMetric: 17.3600 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 899/1000
2023-09-27 07:38:54.438 
Epoch 899/1000 
	 loss: 17.1836, MinusLogProbMetric: 17.1836, val_loss: 17.3468, val_MinusLogProbMetric: 17.3468

Epoch 899: val_loss did not improve from 17.32446
196/196 - 63s - loss: 17.1836 - MinusLogProbMetric: 17.1836 - val_loss: 17.3468 - val_MinusLogProbMetric: 17.3468 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 900/1000
2023-09-27 07:39:56.367 
Epoch 900/1000 
	 loss: 17.1852, MinusLogProbMetric: 17.1852, val_loss: 17.3927, val_MinusLogProbMetric: 17.3927

Epoch 900: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1852 - MinusLogProbMetric: 17.1852 - val_loss: 17.3927 - val_MinusLogProbMetric: 17.3927 - lr: 4.1667e-05 - 62s/epoch - 316ms/step
Epoch 901/1000
2023-09-27 07:40:58.243 
Epoch 901/1000 
	 loss: 17.1968, MinusLogProbMetric: 17.1968, val_loss: 17.3481, val_MinusLogProbMetric: 17.3481

Epoch 901: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1968 - MinusLogProbMetric: 17.1968 - val_loss: 17.3481 - val_MinusLogProbMetric: 17.3481 - lr: 4.1667e-05 - 62s/epoch - 316ms/step
Epoch 902/1000
2023-09-27 07:42:00.745 
Epoch 902/1000 
	 loss: 17.1919, MinusLogProbMetric: 17.1919, val_loss: 17.3330, val_MinusLogProbMetric: 17.3330

Epoch 902: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1919 - MinusLogProbMetric: 17.1919 - val_loss: 17.3330 - val_MinusLogProbMetric: 17.3330 - lr: 4.1667e-05 - 62s/epoch - 319ms/step
Epoch 903/1000
2023-09-27 07:43:02.387 
Epoch 903/1000 
	 loss: 17.1882, MinusLogProbMetric: 17.1882, val_loss: 17.3855, val_MinusLogProbMetric: 17.3855

Epoch 903: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1882 - MinusLogProbMetric: 17.1882 - val_loss: 17.3855 - val_MinusLogProbMetric: 17.3855 - lr: 4.1667e-05 - 62s/epoch - 314ms/step
Epoch 904/1000
2023-09-27 07:44:04.117 
Epoch 904/1000 
	 loss: 17.1866, MinusLogProbMetric: 17.1866, val_loss: 17.3418, val_MinusLogProbMetric: 17.3418

Epoch 904: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1866 - MinusLogProbMetric: 17.1866 - val_loss: 17.3418 - val_MinusLogProbMetric: 17.3418 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 905/1000
2023-09-27 07:45:05.195 
Epoch 905/1000 
	 loss: 17.1872, MinusLogProbMetric: 17.1872, val_loss: 17.3364, val_MinusLogProbMetric: 17.3364

Epoch 905: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1872 - MinusLogProbMetric: 17.1872 - val_loss: 17.3364 - val_MinusLogProbMetric: 17.3364 - lr: 4.1667e-05 - 61s/epoch - 312ms/step
Epoch 906/1000
2023-09-27 07:46:05.792 
Epoch 906/1000 
	 loss: 17.1762, MinusLogProbMetric: 17.1762, val_loss: 17.3503, val_MinusLogProbMetric: 17.3503

Epoch 906: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1762 - MinusLogProbMetric: 17.1762 - val_loss: 17.3503 - val_MinusLogProbMetric: 17.3503 - lr: 4.1667e-05 - 61s/epoch - 309ms/step
Epoch 907/1000
2023-09-27 07:47:06.062 
Epoch 907/1000 
	 loss: 17.1812, MinusLogProbMetric: 17.1812, val_loss: 17.3705, val_MinusLogProbMetric: 17.3705

Epoch 907: val_loss did not improve from 17.32446
196/196 - 60s - loss: 17.1812 - MinusLogProbMetric: 17.1812 - val_loss: 17.3705 - val_MinusLogProbMetric: 17.3705 - lr: 4.1667e-05 - 60s/epoch - 307ms/step
Epoch 908/1000
2023-09-27 07:48:07.018 
Epoch 908/1000 
	 loss: 17.1842, MinusLogProbMetric: 17.1842, val_loss: 17.3684, val_MinusLogProbMetric: 17.3684

Epoch 908: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1842 - MinusLogProbMetric: 17.1842 - val_loss: 17.3684 - val_MinusLogProbMetric: 17.3684 - lr: 4.1667e-05 - 61s/epoch - 311ms/step
Epoch 909/1000
2023-09-27 07:49:08.612 
Epoch 909/1000 
	 loss: 17.1885, MinusLogProbMetric: 17.1885, val_loss: 17.3896, val_MinusLogProbMetric: 17.3896

Epoch 909: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1885 - MinusLogProbMetric: 17.1885 - val_loss: 17.3896 - val_MinusLogProbMetric: 17.3896 - lr: 4.1667e-05 - 62s/epoch - 314ms/step
Epoch 910/1000
2023-09-27 07:50:09.880 
Epoch 910/1000 
	 loss: 17.1840, MinusLogProbMetric: 17.1840, val_loss: 17.3417, val_MinusLogProbMetric: 17.3417

Epoch 910: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1840 - MinusLogProbMetric: 17.1840 - val_loss: 17.3417 - val_MinusLogProbMetric: 17.3417 - lr: 4.1667e-05 - 61s/epoch - 313ms/step
Epoch 911/1000
2023-09-27 07:51:09.988 
Epoch 911/1000 
	 loss: 17.1795, MinusLogProbMetric: 17.1795, val_loss: 17.3374, val_MinusLogProbMetric: 17.3374

Epoch 911: val_loss did not improve from 17.32446
196/196 - 60s - loss: 17.1795 - MinusLogProbMetric: 17.1795 - val_loss: 17.3374 - val_MinusLogProbMetric: 17.3374 - lr: 4.1667e-05 - 60s/epoch - 307ms/step
Epoch 912/1000
2023-09-27 07:52:10.364 
Epoch 912/1000 
	 loss: 17.1845, MinusLogProbMetric: 17.1845, val_loss: 17.3537, val_MinusLogProbMetric: 17.3537

Epoch 912: val_loss did not improve from 17.32446
196/196 - 60s - loss: 17.1845 - MinusLogProbMetric: 17.1845 - val_loss: 17.3537 - val_MinusLogProbMetric: 17.3537 - lr: 4.1667e-05 - 60s/epoch - 308ms/step
Epoch 913/1000
2023-09-27 07:53:11.026 
Epoch 913/1000 
	 loss: 17.1856, MinusLogProbMetric: 17.1856, val_loss: 17.3412, val_MinusLogProbMetric: 17.3412

Epoch 913: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1856 - MinusLogProbMetric: 17.1856 - val_loss: 17.3412 - val_MinusLogProbMetric: 17.3412 - lr: 4.1667e-05 - 61s/epoch - 309ms/step
Epoch 914/1000
2023-09-27 07:54:11.541 
Epoch 914/1000 
	 loss: 17.1834, MinusLogProbMetric: 17.1834, val_loss: 17.3558, val_MinusLogProbMetric: 17.3558

Epoch 914: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1834 - MinusLogProbMetric: 17.1834 - val_loss: 17.3558 - val_MinusLogProbMetric: 17.3558 - lr: 4.1667e-05 - 61s/epoch - 309ms/step
Epoch 915/1000
2023-09-27 07:55:12.572 
Epoch 915/1000 
	 loss: 17.1839, MinusLogProbMetric: 17.1839, val_loss: 17.3449, val_MinusLogProbMetric: 17.3449

Epoch 915: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1839 - MinusLogProbMetric: 17.1839 - val_loss: 17.3449 - val_MinusLogProbMetric: 17.3449 - lr: 4.1667e-05 - 61s/epoch - 311ms/step
Epoch 916/1000
2023-09-27 07:56:14.872 
Epoch 916/1000 
	 loss: 17.1870, MinusLogProbMetric: 17.1870, val_loss: 17.3605, val_MinusLogProbMetric: 17.3605

Epoch 916: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1870 - MinusLogProbMetric: 17.1870 - val_loss: 17.3605 - val_MinusLogProbMetric: 17.3605 - lr: 4.1667e-05 - 62s/epoch - 318ms/step
Epoch 917/1000
2023-09-27 07:57:16.645 
Epoch 917/1000 
	 loss: 17.1830, MinusLogProbMetric: 17.1830, val_loss: 17.3306, val_MinusLogProbMetric: 17.3306

Epoch 917: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1830 - MinusLogProbMetric: 17.1830 - val_loss: 17.3306 - val_MinusLogProbMetric: 17.3306 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 918/1000
2023-09-27 07:58:18.424 
Epoch 918/1000 
	 loss: 17.1748, MinusLogProbMetric: 17.1748, val_loss: 17.3302, val_MinusLogProbMetric: 17.3302

Epoch 918: val_loss did not improve from 17.32446
196/196 - 62s - loss: 17.1748 - MinusLogProbMetric: 17.1748 - val_loss: 17.3302 - val_MinusLogProbMetric: 17.3302 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 919/1000
2023-09-27 07:59:19.740 
Epoch 919/1000 
	 loss: 17.1552, MinusLogProbMetric: 17.1552, val_loss: 17.3296, val_MinusLogProbMetric: 17.3296

Epoch 919: val_loss did not improve from 17.32446
196/196 - 61s - loss: 17.1552 - MinusLogProbMetric: 17.1552 - val_loss: 17.3296 - val_MinusLogProbMetric: 17.3296 - lr: 2.0833e-05 - 61s/epoch - 313ms/step
Epoch 920/1000
2023-09-27 08:00:20.746 
Epoch 920/1000 
	 loss: 17.1527, MinusLogProbMetric: 17.1527, val_loss: 17.3134, val_MinusLogProbMetric: 17.3134

Epoch 920: val_loss improved from 17.32446 to 17.31338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 62s - loss: 17.1527 - MinusLogProbMetric: 17.1527 - val_loss: 17.3134 - val_MinusLogProbMetric: 17.3134 - lr: 2.0833e-05 - 62s/epoch - 318ms/step
Epoch 921/1000
2023-09-27 08:01:22.631 
Epoch 921/1000 
	 loss: 17.1524, MinusLogProbMetric: 17.1524, val_loss: 17.3292, val_MinusLogProbMetric: 17.3292

Epoch 921: val_loss did not improve from 17.31338
196/196 - 61s - loss: 17.1524 - MinusLogProbMetric: 17.1524 - val_loss: 17.3292 - val_MinusLogProbMetric: 17.3292 - lr: 2.0833e-05 - 61s/epoch - 309ms/step
Epoch 922/1000
2023-09-27 08:02:23.331 
Epoch 922/1000 
	 loss: 17.1552, MinusLogProbMetric: 17.1552, val_loss: 17.3156, val_MinusLogProbMetric: 17.3156

Epoch 922: val_loss did not improve from 17.31338
196/196 - 61s - loss: 17.1552 - MinusLogProbMetric: 17.1552 - val_loss: 17.3156 - val_MinusLogProbMetric: 17.3156 - lr: 2.0833e-05 - 61s/epoch - 310ms/step
Epoch 923/1000
2023-09-27 08:03:23.790 
Epoch 923/1000 
	 loss: 17.1547, MinusLogProbMetric: 17.1547, val_loss: 17.3230, val_MinusLogProbMetric: 17.3230

Epoch 923: val_loss did not improve from 17.31338
196/196 - 60s - loss: 17.1547 - MinusLogProbMetric: 17.1547 - val_loss: 17.3230 - val_MinusLogProbMetric: 17.3230 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 924/1000
2023-09-27 08:04:24.889 
Epoch 924/1000 
	 loss: 17.1486, MinusLogProbMetric: 17.1486, val_loss: 17.3176, val_MinusLogProbMetric: 17.3176

Epoch 924: val_loss did not improve from 17.31338
196/196 - 61s - loss: 17.1486 - MinusLogProbMetric: 17.1486 - val_loss: 17.3176 - val_MinusLogProbMetric: 17.3176 - lr: 2.0833e-05 - 61s/epoch - 312ms/step
Epoch 925/1000
2023-09-27 08:05:26.154 
Epoch 925/1000 
	 loss: 17.1544, MinusLogProbMetric: 17.1544, val_loss: 17.3411, val_MinusLogProbMetric: 17.3411

Epoch 925: val_loss did not improve from 17.31338
196/196 - 61s - loss: 17.1544 - MinusLogProbMetric: 17.1544 - val_loss: 17.3411 - val_MinusLogProbMetric: 17.3411 - lr: 2.0833e-05 - 61s/epoch - 313ms/step
Epoch 926/1000
2023-09-27 08:06:27.265 
Epoch 926/1000 
	 loss: 17.1547, MinusLogProbMetric: 17.1547, val_loss: 17.3182, val_MinusLogProbMetric: 17.3182

Epoch 926: val_loss did not improve from 17.31338
196/196 - 61s - loss: 17.1547 - MinusLogProbMetric: 17.1547 - val_loss: 17.3182 - val_MinusLogProbMetric: 17.3182 - lr: 2.0833e-05 - 61s/epoch - 312ms/step
Epoch 927/1000
2023-09-27 08:07:28.439 
Epoch 927/1000 
	 loss: 17.1549, MinusLogProbMetric: 17.1549, val_loss: 17.3192, val_MinusLogProbMetric: 17.3192

Epoch 927: val_loss did not improve from 17.31338
196/196 - 61s - loss: 17.1549 - MinusLogProbMetric: 17.1549 - val_loss: 17.3192 - val_MinusLogProbMetric: 17.3192 - lr: 2.0833e-05 - 61s/epoch - 312ms/step
Epoch 928/1000
2023-09-27 08:08:29.387 
Epoch 928/1000 
	 loss: 17.1499, MinusLogProbMetric: 17.1499, val_loss: 17.3117, val_MinusLogProbMetric: 17.3117

Epoch 928: val_loss improved from 17.31338 to 17.31166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 62s - loss: 17.1499 - MinusLogProbMetric: 17.1499 - val_loss: 17.3117 - val_MinusLogProbMetric: 17.3117 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 929/1000
2023-09-27 08:09:31.586 
Epoch 929/1000 
	 loss: 17.1507, MinusLogProbMetric: 17.1507, val_loss: 17.3328, val_MinusLogProbMetric: 17.3328

Epoch 929: val_loss did not improve from 17.31166
196/196 - 61s - loss: 17.1507 - MinusLogProbMetric: 17.1507 - val_loss: 17.3328 - val_MinusLogProbMetric: 17.3328 - lr: 2.0833e-05 - 61s/epoch - 311ms/step
Epoch 930/1000
2023-09-27 08:10:33.288 
Epoch 930/1000 
	 loss: 17.1497, MinusLogProbMetric: 17.1497, val_loss: 17.3436, val_MinusLogProbMetric: 17.3436

Epoch 930: val_loss did not improve from 17.31166
196/196 - 62s - loss: 17.1497 - MinusLogProbMetric: 17.1497 - val_loss: 17.3436 - val_MinusLogProbMetric: 17.3436 - lr: 2.0833e-05 - 62s/epoch - 315ms/step
Epoch 931/1000
2023-09-27 08:11:37.017 
Epoch 931/1000 
	 loss: 17.1546, MinusLogProbMetric: 17.1546, val_loss: 17.3233, val_MinusLogProbMetric: 17.3233

Epoch 931: val_loss did not improve from 17.31166
196/196 - 64s - loss: 17.1546 - MinusLogProbMetric: 17.1546 - val_loss: 17.3233 - val_MinusLogProbMetric: 17.3233 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 932/1000
2023-09-27 08:12:41.145 
Epoch 932/1000 
	 loss: 17.1529, MinusLogProbMetric: 17.1529, val_loss: 17.3196, val_MinusLogProbMetric: 17.3196

Epoch 932: val_loss did not improve from 17.31166
196/196 - 64s - loss: 17.1529 - MinusLogProbMetric: 17.1529 - val_loss: 17.3196 - val_MinusLogProbMetric: 17.3196 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 933/1000
2023-09-27 08:13:43.658 
Epoch 933/1000 
	 loss: 17.1509, MinusLogProbMetric: 17.1509, val_loss: 17.3132, val_MinusLogProbMetric: 17.3132

Epoch 933: val_loss did not improve from 17.31166
196/196 - 63s - loss: 17.1509 - MinusLogProbMetric: 17.1509 - val_loss: 17.3132 - val_MinusLogProbMetric: 17.3132 - lr: 2.0833e-05 - 63s/epoch - 319ms/step
Epoch 934/1000
2023-09-27 08:14:47.033 
Epoch 934/1000 
	 loss: 17.1541, MinusLogProbMetric: 17.1541, val_loss: 17.3292, val_MinusLogProbMetric: 17.3292

Epoch 934: val_loss did not improve from 17.31166
196/196 - 63s - loss: 17.1541 - MinusLogProbMetric: 17.1541 - val_loss: 17.3292 - val_MinusLogProbMetric: 17.3292 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 935/1000
2023-09-27 08:15:48.986 
Epoch 935/1000 
	 loss: 17.1508, MinusLogProbMetric: 17.1508, val_loss: 17.3371, val_MinusLogProbMetric: 17.3371

Epoch 935: val_loss did not improve from 17.31166
196/196 - 62s - loss: 17.1508 - MinusLogProbMetric: 17.1508 - val_loss: 17.3371 - val_MinusLogProbMetric: 17.3371 - lr: 2.0833e-05 - 62s/epoch - 316ms/step
Epoch 936/1000
2023-09-27 08:16:51.490 
Epoch 936/1000 
	 loss: 17.1498, MinusLogProbMetric: 17.1498, val_loss: 17.3418, val_MinusLogProbMetric: 17.3418

Epoch 936: val_loss did not improve from 17.31166
196/196 - 63s - loss: 17.1498 - MinusLogProbMetric: 17.1498 - val_loss: 17.3418 - val_MinusLogProbMetric: 17.3418 - lr: 2.0833e-05 - 63s/epoch - 319ms/step
Epoch 937/1000
2023-09-27 08:17:53.126 
Epoch 937/1000 
	 loss: 17.1509, MinusLogProbMetric: 17.1509, val_loss: 17.3271, val_MinusLogProbMetric: 17.3271

Epoch 937: val_loss did not improve from 17.31166
196/196 - 62s - loss: 17.1509 - MinusLogProbMetric: 17.1509 - val_loss: 17.3271 - val_MinusLogProbMetric: 17.3271 - lr: 2.0833e-05 - 62s/epoch - 314ms/step
Epoch 938/1000
2023-09-27 08:18:55.284 
Epoch 938/1000 
	 loss: 17.1498, MinusLogProbMetric: 17.1498, val_loss: 17.3272, val_MinusLogProbMetric: 17.3272

Epoch 938: val_loss did not improve from 17.31166
196/196 - 62s - loss: 17.1498 - MinusLogProbMetric: 17.1498 - val_loss: 17.3272 - val_MinusLogProbMetric: 17.3272 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 939/1000
2023-09-27 08:19:57.855 
Epoch 939/1000 
	 loss: 17.1504, MinusLogProbMetric: 17.1504, val_loss: 17.3248, val_MinusLogProbMetric: 17.3248

Epoch 939: val_loss did not improve from 17.31166
196/196 - 63s - loss: 17.1504 - MinusLogProbMetric: 17.1504 - val_loss: 17.3248 - val_MinusLogProbMetric: 17.3248 - lr: 2.0833e-05 - 63s/epoch - 319ms/step
Epoch 940/1000
2023-09-27 08:20:59.972 
Epoch 940/1000 
	 loss: 17.1525, MinusLogProbMetric: 17.1525, val_loss: 17.3164, val_MinusLogProbMetric: 17.3164

Epoch 940: val_loss did not improve from 17.31166
196/196 - 62s - loss: 17.1525 - MinusLogProbMetric: 17.1525 - val_loss: 17.3164 - val_MinusLogProbMetric: 17.3164 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 941/1000
2023-09-27 08:22:01.334 
Epoch 941/1000 
	 loss: 17.1546, MinusLogProbMetric: 17.1546, val_loss: 17.3109, val_MinusLogProbMetric: 17.3109

Epoch 941: val_loss improved from 17.31166 to 17.31086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 63s - loss: 17.1546 - MinusLogProbMetric: 17.1546 - val_loss: 17.3109 - val_MinusLogProbMetric: 17.3109 - lr: 2.0833e-05 - 63s/epoch - 320ms/step
Epoch 942/1000
2023-09-27 08:23:05.074 
Epoch 942/1000 
	 loss: 17.1496, MinusLogProbMetric: 17.1496, val_loss: 17.3124, val_MinusLogProbMetric: 17.3124

Epoch 942: val_loss did not improve from 17.31086
196/196 - 62s - loss: 17.1496 - MinusLogProbMetric: 17.1496 - val_loss: 17.3124 - val_MinusLogProbMetric: 17.3124 - lr: 2.0833e-05 - 62s/epoch - 319ms/step
Epoch 943/1000
2023-09-27 08:24:08.181 
Epoch 943/1000 
	 loss: 17.1522, MinusLogProbMetric: 17.1522, val_loss: 17.3152, val_MinusLogProbMetric: 17.3152

Epoch 943: val_loss did not improve from 17.31086
196/196 - 63s - loss: 17.1522 - MinusLogProbMetric: 17.1522 - val_loss: 17.3152 - val_MinusLogProbMetric: 17.3152 - lr: 2.0833e-05 - 63s/epoch - 322ms/step
Epoch 944/1000
2023-09-27 08:25:11.796 
Epoch 944/1000 
	 loss: 17.1510, MinusLogProbMetric: 17.1510, val_loss: 17.3451, val_MinusLogProbMetric: 17.3451

Epoch 944: val_loss did not improve from 17.31086
196/196 - 64s - loss: 17.1510 - MinusLogProbMetric: 17.1510 - val_loss: 17.3451 - val_MinusLogProbMetric: 17.3451 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 945/1000
2023-09-27 08:26:13.262 
Epoch 945/1000 
	 loss: 17.1501, MinusLogProbMetric: 17.1501, val_loss: 17.3298, val_MinusLogProbMetric: 17.3298

Epoch 945: val_loss did not improve from 17.31086
196/196 - 61s - loss: 17.1501 - MinusLogProbMetric: 17.1501 - val_loss: 17.3298 - val_MinusLogProbMetric: 17.3298 - lr: 2.0833e-05 - 61s/epoch - 314ms/step
Epoch 946/1000
2023-09-27 08:27:13.662 
Epoch 946/1000 
	 loss: 17.1490, MinusLogProbMetric: 17.1490, val_loss: 17.3506, val_MinusLogProbMetric: 17.3506

Epoch 946: val_loss did not improve from 17.31086
196/196 - 60s - loss: 17.1490 - MinusLogProbMetric: 17.1490 - val_loss: 17.3506 - val_MinusLogProbMetric: 17.3506 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 947/1000
2023-09-27 08:28:14.646 
Epoch 947/1000 
	 loss: 17.1497, MinusLogProbMetric: 17.1497, val_loss: 17.3172, val_MinusLogProbMetric: 17.3172

Epoch 947: val_loss did not improve from 17.31086
196/196 - 61s - loss: 17.1497 - MinusLogProbMetric: 17.1497 - val_loss: 17.3172 - val_MinusLogProbMetric: 17.3172 - lr: 2.0833e-05 - 61s/epoch - 311ms/step
Epoch 948/1000
2023-09-27 08:29:15.128 
Epoch 948/1000 
	 loss: 17.1488, MinusLogProbMetric: 17.1488, val_loss: 17.3187, val_MinusLogProbMetric: 17.3187

Epoch 948: val_loss did not improve from 17.31086
196/196 - 60s - loss: 17.1488 - MinusLogProbMetric: 17.1488 - val_loss: 17.3187 - val_MinusLogProbMetric: 17.3187 - lr: 2.0833e-05 - 60s/epoch - 309ms/step
Epoch 949/1000
2023-09-27 08:30:16.369 
Epoch 949/1000 
	 loss: 17.1497, MinusLogProbMetric: 17.1497, val_loss: 17.3507, val_MinusLogProbMetric: 17.3507

Epoch 949: val_loss did not improve from 17.31086
196/196 - 61s - loss: 17.1497 - MinusLogProbMetric: 17.1497 - val_loss: 17.3507 - val_MinusLogProbMetric: 17.3507 - lr: 2.0833e-05 - 61s/epoch - 312ms/step
Epoch 950/1000
2023-09-27 08:31:16.936 
Epoch 950/1000 
	 loss: 17.1514, MinusLogProbMetric: 17.1514, val_loss: 17.3107, val_MinusLogProbMetric: 17.3107

Epoch 950: val_loss improved from 17.31086 to 17.31066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 61s - loss: 17.1514 - MinusLogProbMetric: 17.1514 - val_loss: 17.3107 - val_MinusLogProbMetric: 17.3107 - lr: 2.0833e-05 - 61s/epoch - 314ms/step
Epoch 951/1000
2023-09-27 08:32:18.738 
Epoch 951/1000 
	 loss: 17.1514, MinusLogProbMetric: 17.1514, val_loss: 17.3252, val_MinusLogProbMetric: 17.3252

Epoch 951: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1514 - MinusLogProbMetric: 17.1514 - val_loss: 17.3252 - val_MinusLogProbMetric: 17.3252 - lr: 2.0833e-05 - 61s/epoch - 311ms/step
Epoch 952/1000
2023-09-27 08:33:19.612 
Epoch 952/1000 
	 loss: 17.1508, MinusLogProbMetric: 17.1508, val_loss: 17.3232, val_MinusLogProbMetric: 17.3232

Epoch 952: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1508 - MinusLogProbMetric: 17.1508 - val_loss: 17.3232 - val_MinusLogProbMetric: 17.3232 - lr: 2.0833e-05 - 61s/epoch - 311ms/step
Epoch 953/1000
2023-09-27 08:34:21.019 
Epoch 953/1000 
	 loss: 17.1511, MinusLogProbMetric: 17.1511, val_loss: 17.3139, val_MinusLogProbMetric: 17.3139

Epoch 953: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1511 - MinusLogProbMetric: 17.1511 - val_loss: 17.3139 - val_MinusLogProbMetric: 17.3139 - lr: 2.0833e-05 - 61s/epoch - 313ms/step
Epoch 954/1000
2023-09-27 08:35:23.880 
Epoch 954/1000 
	 loss: 17.1520, MinusLogProbMetric: 17.1520, val_loss: 17.3400, val_MinusLogProbMetric: 17.3400

Epoch 954: val_loss did not improve from 17.31066
196/196 - 63s - loss: 17.1520 - MinusLogProbMetric: 17.1520 - val_loss: 17.3400 - val_MinusLogProbMetric: 17.3400 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 955/1000
2023-09-27 08:36:25.368 
Epoch 955/1000 
	 loss: 17.1512, MinusLogProbMetric: 17.1512, val_loss: 17.3189, val_MinusLogProbMetric: 17.3189

Epoch 955: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1512 - MinusLogProbMetric: 17.1512 - val_loss: 17.3189 - val_MinusLogProbMetric: 17.3189 - lr: 2.0833e-05 - 61s/epoch - 314ms/step
Epoch 956/1000
2023-09-27 08:37:26.248 
Epoch 956/1000 
	 loss: 17.1497, MinusLogProbMetric: 17.1497, val_loss: 17.3116, val_MinusLogProbMetric: 17.3116

Epoch 956: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1497 - MinusLogProbMetric: 17.1497 - val_loss: 17.3116 - val_MinusLogProbMetric: 17.3116 - lr: 2.0833e-05 - 61s/epoch - 311ms/step
Epoch 957/1000
2023-09-27 08:38:27.536 
Epoch 957/1000 
	 loss: 17.1495, MinusLogProbMetric: 17.1495, val_loss: 17.3389, val_MinusLogProbMetric: 17.3389

Epoch 957: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1495 - MinusLogProbMetric: 17.1495 - val_loss: 17.3389 - val_MinusLogProbMetric: 17.3389 - lr: 2.0833e-05 - 61s/epoch - 313ms/step
Epoch 958/1000
2023-09-27 08:39:28.103 
Epoch 958/1000 
	 loss: 17.1529, MinusLogProbMetric: 17.1529, val_loss: 17.3195, val_MinusLogProbMetric: 17.3195

Epoch 958: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1529 - MinusLogProbMetric: 17.1529 - val_loss: 17.3195 - val_MinusLogProbMetric: 17.3195 - lr: 2.0833e-05 - 61s/epoch - 309ms/step
Epoch 959/1000
2023-09-27 08:40:28.556 
Epoch 959/1000 
	 loss: 17.1475, MinusLogProbMetric: 17.1475, val_loss: 17.3125, val_MinusLogProbMetric: 17.3125

Epoch 959: val_loss did not improve from 17.31066
196/196 - 60s - loss: 17.1475 - MinusLogProbMetric: 17.1475 - val_loss: 17.3125 - val_MinusLogProbMetric: 17.3125 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 960/1000
2023-09-27 08:41:28.997 
Epoch 960/1000 
	 loss: 17.1485, MinusLogProbMetric: 17.1485, val_loss: 17.3406, val_MinusLogProbMetric: 17.3406

Epoch 960: val_loss did not improve from 17.31066
196/196 - 60s - loss: 17.1485 - MinusLogProbMetric: 17.1485 - val_loss: 17.3406 - val_MinusLogProbMetric: 17.3406 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 961/1000
2023-09-27 08:42:29.159 
Epoch 961/1000 
	 loss: 17.1461, MinusLogProbMetric: 17.1461, val_loss: 17.3143, val_MinusLogProbMetric: 17.3143

Epoch 961: val_loss did not improve from 17.31066
196/196 - 60s - loss: 17.1461 - MinusLogProbMetric: 17.1461 - val_loss: 17.3143 - val_MinusLogProbMetric: 17.3143 - lr: 2.0833e-05 - 60s/epoch - 307ms/step
Epoch 962/1000
2023-09-27 08:43:30.065 
Epoch 962/1000 
	 loss: 17.1459, MinusLogProbMetric: 17.1459, val_loss: 17.3244, val_MinusLogProbMetric: 17.3244

Epoch 962: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1459 - MinusLogProbMetric: 17.1459 - val_loss: 17.3244 - val_MinusLogProbMetric: 17.3244 - lr: 2.0833e-05 - 61s/epoch - 311ms/step
Epoch 963/1000
2023-09-27 08:44:30.301 
Epoch 963/1000 
	 loss: 17.1441, MinusLogProbMetric: 17.1441, val_loss: 17.3132, val_MinusLogProbMetric: 17.3132

Epoch 963: val_loss did not improve from 17.31066
196/196 - 60s - loss: 17.1441 - MinusLogProbMetric: 17.1441 - val_loss: 17.3132 - val_MinusLogProbMetric: 17.3132 - lr: 2.0833e-05 - 60s/epoch - 307ms/step
Epoch 964/1000
2023-09-27 08:45:30.667 
Epoch 964/1000 
	 loss: 17.1492, MinusLogProbMetric: 17.1492, val_loss: 17.3253, val_MinusLogProbMetric: 17.3253

Epoch 964: val_loss did not improve from 17.31066
196/196 - 60s - loss: 17.1492 - MinusLogProbMetric: 17.1492 - val_loss: 17.3253 - val_MinusLogProbMetric: 17.3253 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 965/1000
2023-09-27 08:46:31.513 
Epoch 965/1000 
	 loss: 17.1457, MinusLogProbMetric: 17.1457, val_loss: 17.3141, val_MinusLogProbMetric: 17.3141

Epoch 965: val_loss did not improve from 17.31066
196/196 - 61s - loss: 17.1457 - MinusLogProbMetric: 17.1457 - val_loss: 17.3141 - val_MinusLogProbMetric: 17.3141 - lr: 2.0833e-05 - 61s/epoch - 310ms/step
Epoch 966/1000
2023-09-27 08:47:32.296 
Epoch 966/1000 
	 loss: 17.1479, MinusLogProbMetric: 17.1479, val_loss: 17.3077, val_MinusLogProbMetric: 17.3077

Epoch 966: val_loss improved from 17.31066 to 17.30772, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 62s - loss: 17.1479 - MinusLogProbMetric: 17.1479 - val_loss: 17.3077 - val_MinusLogProbMetric: 17.3077 - lr: 2.0833e-05 - 62s/epoch - 315ms/step
Epoch 967/1000
2023-09-27 08:48:34.150 
Epoch 967/1000 
	 loss: 17.1479, MinusLogProbMetric: 17.1479, val_loss: 17.3347, val_MinusLogProbMetric: 17.3347

Epoch 967: val_loss did not improve from 17.30772
196/196 - 61s - loss: 17.1479 - MinusLogProbMetric: 17.1479 - val_loss: 17.3347 - val_MinusLogProbMetric: 17.3347 - lr: 2.0833e-05 - 61s/epoch - 310ms/step
Epoch 968/1000
2023-09-27 08:49:36.101 
Epoch 968/1000 
	 loss: 17.1485, MinusLogProbMetric: 17.1485, val_loss: 17.3137, val_MinusLogProbMetric: 17.3137

Epoch 968: val_loss did not improve from 17.30772
196/196 - 62s - loss: 17.1485 - MinusLogProbMetric: 17.1485 - val_loss: 17.3137 - val_MinusLogProbMetric: 17.3137 - lr: 2.0833e-05 - 62s/epoch - 316ms/step
Epoch 969/1000
2023-09-27 08:50:36.434 
Epoch 969/1000 
	 loss: 17.1507, MinusLogProbMetric: 17.1507, val_loss: 17.3170, val_MinusLogProbMetric: 17.3170

Epoch 969: val_loss did not improve from 17.30772
196/196 - 60s - loss: 17.1507 - MinusLogProbMetric: 17.1507 - val_loss: 17.3170 - val_MinusLogProbMetric: 17.3170 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 970/1000
2023-09-27 08:51:36.662 
Epoch 970/1000 
	 loss: 17.1494, MinusLogProbMetric: 17.1494, val_loss: 17.3100, val_MinusLogProbMetric: 17.3100

Epoch 970: val_loss did not improve from 17.30772
196/196 - 60s - loss: 17.1494 - MinusLogProbMetric: 17.1494 - val_loss: 17.3100 - val_MinusLogProbMetric: 17.3100 - lr: 2.0833e-05 - 60s/epoch - 307ms/step
Epoch 971/1000
2023-09-27 08:52:36.658 
Epoch 971/1000 
	 loss: 17.1489, MinusLogProbMetric: 17.1489, val_loss: 17.3068, val_MinusLogProbMetric: 17.3068

Epoch 971: val_loss improved from 17.30772 to 17.30682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 61s - loss: 17.1489 - MinusLogProbMetric: 17.1489 - val_loss: 17.3068 - val_MinusLogProbMetric: 17.3068 - lr: 2.0833e-05 - 61s/epoch - 311ms/step
Epoch 972/1000
2023-09-27 08:53:38.638 
Epoch 972/1000 
	 loss: 17.1501, MinusLogProbMetric: 17.1501, val_loss: 17.3214, val_MinusLogProbMetric: 17.3214

Epoch 972: val_loss did not improve from 17.30682
196/196 - 61s - loss: 17.1501 - MinusLogProbMetric: 17.1501 - val_loss: 17.3214 - val_MinusLogProbMetric: 17.3214 - lr: 2.0833e-05 - 61s/epoch - 312ms/step
Epoch 973/1000
2023-09-27 08:54:39.242 
Epoch 973/1000 
	 loss: 17.1480, MinusLogProbMetric: 17.1480, val_loss: 17.3461, val_MinusLogProbMetric: 17.3461

Epoch 973: val_loss did not improve from 17.30682
196/196 - 61s - loss: 17.1480 - MinusLogProbMetric: 17.1480 - val_loss: 17.3461 - val_MinusLogProbMetric: 17.3461 - lr: 2.0833e-05 - 61s/epoch - 309ms/step
Epoch 974/1000
2023-09-27 08:55:38.478 
Epoch 974/1000 
	 loss: 17.1467, MinusLogProbMetric: 17.1467, val_loss: 17.3200, val_MinusLogProbMetric: 17.3200

Epoch 974: val_loss did not improve from 17.30682
196/196 - 59s - loss: 17.1467 - MinusLogProbMetric: 17.1467 - val_loss: 17.3200 - val_MinusLogProbMetric: 17.3200 - lr: 2.0833e-05 - 59s/epoch - 302ms/step
Epoch 975/1000
2023-09-27 08:56:38.190 
Epoch 975/1000 
	 loss: 17.1499, MinusLogProbMetric: 17.1499, val_loss: 17.3181, val_MinusLogProbMetric: 17.3181

Epoch 975: val_loss did not improve from 17.30682
196/196 - 60s - loss: 17.1499 - MinusLogProbMetric: 17.1499 - val_loss: 17.3181 - val_MinusLogProbMetric: 17.3181 - lr: 2.0833e-05 - 60s/epoch - 305ms/step
Epoch 976/1000
2023-09-27 08:57:34.381 
Epoch 976/1000 
	 loss: 17.1456, MinusLogProbMetric: 17.1456, val_loss: 17.3229, val_MinusLogProbMetric: 17.3229

Epoch 976: val_loss did not improve from 17.30682
196/196 - 56s - loss: 17.1456 - MinusLogProbMetric: 17.1456 - val_loss: 17.3229 - val_MinusLogProbMetric: 17.3229 - lr: 2.0833e-05 - 56s/epoch - 287ms/step
Epoch 977/1000
2023-09-27 08:58:34.267 
Epoch 977/1000 
	 loss: 17.1472, MinusLogProbMetric: 17.1472, val_loss: 17.3097, val_MinusLogProbMetric: 17.3097

Epoch 977: val_loss did not improve from 17.30682
196/196 - 60s - loss: 17.1472 - MinusLogProbMetric: 17.1472 - val_loss: 17.3097 - val_MinusLogProbMetric: 17.3097 - lr: 2.0833e-05 - 60s/epoch - 306ms/step
Epoch 978/1000
2023-09-27 08:59:35.982 
Epoch 978/1000 
	 loss: 17.1459, MinusLogProbMetric: 17.1459, val_loss: 17.3176, val_MinusLogProbMetric: 17.3176

Epoch 978: val_loss did not improve from 17.30682
196/196 - 62s - loss: 17.1459 - MinusLogProbMetric: 17.1459 - val_loss: 17.3176 - val_MinusLogProbMetric: 17.3176 - lr: 2.0833e-05 - 62s/epoch - 315ms/step
Epoch 979/1000
2023-09-27 09:00:35.604 
Epoch 979/1000 
	 loss: 17.1506, MinusLogProbMetric: 17.1506, val_loss: 17.3078, val_MinusLogProbMetric: 17.3078

Epoch 979: val_loss did not improve from 17.30682
196/196 - 60s - loss: 17.1506 - MinusLogProbMetric: 17.1506 - val_loss: 17.3078 - val_MinusLogProbMetric: 17.3078 - lr: 2.0833e-05 - 60s/epoch - 304ms/step
Epoch 980/1000
2023-09-27 09:01:34.209 
Epoch 980/1000 
	 loss: 17.1442, MinusLogProbMetric: 17.1442, val_loss: 17.3065, val_MinusLogProbMetric: 17.3065

Epoch 980: val_loss improved from 17.30682 to 17.30649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 60s - loss: 17.1442 - MinusLogProbMetric: 17.1442 - val_loss: 17.3065 - val_MinusLogProbMetric: 17.3065 - lr: 2.0833e-05 - 60s/epoch - 305ms/step
Epoch 981/1000
2023-09-27 09:02:36.880 
Epoch 981/1000 
	 loss: 17.1455, MinusLogProbMetric: 17.1455, val_loss: 17.3406, val_MinusLogProbMetric: 17.3406

Epoch 981: val_loss did not improve from 17.30649
196/196 - 62s - loss: 17.1455 - MinusLogProbMetric: 17.1455 - val_loss: 17.3406 - val_MinusLogProbMetric: 17.3406 - lr: 2.0833e-05 - 62s/epoch - 314ms/step
Epoch 982/1000
2023-09-27 09:03:40.612 
Epoch 982/1000 
	 loss: 17.1437, MinusLogProbMetric: 17.1437, val_loss: 17.3099, val_MinusLogProbMetric: 17.3099

Epoch 982: val_loss did not improve from 17.30649
196/196 - 64s - loss: 17.1437 - MinusLogProbMetric: 17.1437 - val_loss: 17.3099 - val_MinusLogProbMetric: 17.3099 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 983/1000
2023-09-27 09:04:42.924 
Epoch 983/1000 
	 loss: 17.1437, MinusLogProbMetric: 17.1437, val_loss: 17.3132, val_MinusLogProbMetric: 17.3132

Epoch 983: val_loss did not improve from 17.30649
196/196 - 62s - loss: 17.1437 - MinusLogProbMetric: 17.1437 - val_loss: 17.3132 - val_MinusLogProbMetric: 17.3132 - lr: 2.0833e-05 - 62s/epoch - 318ms/step
Epoch 984/1000
2023-09-27 09:05:44.279 
Epoch 984/1000 
	 loss: 17.1453, MinusLogProbMetric: 17.1453, val_loss: 17.3176, val_MinusLogProbMetric: 17.3176

Epoch 984: val_loss did not improve from 17.30649
196/196 - 61s - loss: 17.1453 - MinusLogProbMetric: 17.1453 - val_loss: 17.3176 - val_MinusLogProbMetric: 17.3176 - lr: 2.0833e-05 - 61s/epoch - 313ms/step
Epoch 985/1000
2023-09-27 09:06:44.620 
Epoch 985/1000 
	 loss: 17.1452, MinusLogProbMetric: 17.1452, val_loss: 17.3254, val_MinusLogProbMetric: 17.3254

Epoch 985: val_loss did not improve from 17.30649
196/196 - 60s - loss: 17.1452 - MinusLogProbMetric: 17.1452 - val_loss: 17.3254 - val_MinusLogProbMetric: 17.3254 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 986/1000
2023-09-27 09:07:44.881 
Epoch 986/1000 
	 loss: 17.1453, MinusLogProbMetric: 17.1453, val_loss: 17.3196, val_MinusLogProbMetric: 17.3196

Epoch 986: val_loss did not improve from 17.30649
196/196 - 60s - loss: 17.1453 - MinusLogProbMetric: 17.1453 - val_loss: 17.3196 - val_MinusLogProbMetric: 17.3196 - lr: 2.0833e-05 - 60s/epoch - 307ms/step
Epoch 987/1000
2023-09-27 09:08:44.921 
Epoch 987/1000 
	 loss: 17.1467, MinusLogProbMetric: 17.1467, val_loss: 17.3049, val_MinusLogProbMetric: 17.3049

Epoch 987: val_loss improved from 17.30649 to 17.30492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 61s - loss: 17.1467 - MinusLogProbMetric: 17.1467 - val_loss: 17.3049 - val_MinusLogProbMetric: 17.3049 - lr: 2.0833e-05 - 61s/epoch - 312ms/step
Epoch 988/1000
2023-09-27 09:09:46.176 
Epoch 988/1000 
	 loss: 17.1431, MinusLogProbMetric: 17.1431, val_loss: 17.3075, val_MinusLogProbMetric: 17.3075

Epoch 988: val_loss did not improve from 17.30492
196/196 - 60s - loss: 17.1431 - MinusLogProbMetric: 17.1431 - val_loss: 17.3075 - val_MinusLogProbMetric: 17.3075 - lr: 2.0833e-05 - 60s/epoch - 307ms/step
Epoch 989/1000
2023-09-27 09:10:46.186 
Epoch 989/1000 
	 loss: 17.1455, MinusLogProbMetric: 17.1455, val_loss: 17.3213, val_MinusLogProbMetric: 17.3213

Epoch 989: val_loss did not improve from 17.30492
196/196 - 60s - loss: 17.1455 - MinusLogProbMetric: 17.1455 - val_loss: 17.3213 - val_MinusLogProbMetric: 17.3213 - lr: 2.0833e-05 - 60s/epoch - 306ms/step
Epoch 990/1000
2023-09-27 09:11:46.478 
Epoch 990/1000 
	 loss: 17.1447, MinusLogProbMetric: 17.1447, val_loss: 17.3147, val_MinusLogProbMetric: 17.3147

Epoch 990: val_loss did not improve from 17.30492
196/196 - 60s - loss: 17.1447 - MinusLogProbMetric: 17.1447 - val_loss: 17.3147 - val_MinusLogProbMetric: 17.3147 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 991/1000
2023-09-27 09:12:46.456 
Epoch 991/1000 
	 loss: 17.1470, MinusLogProbMetric: 17.1470, val_loss: 17.3169, val_MinusLogProbMetric: 17.3169

Epoch 991: val_loss did not improve from 17.30492
196/196 - 60s - loss: 17.1470 - MinusLogProbMetric: 17.1470 - val_loss: 17.3169 - val_MinusLogProbMetric: 17.3169 - lr: 2.0833e-05 - 60s/epoch - 306ms/step
Epoch 992/1000
2023-09-27 09:13:46.734 
Epoch 992/1000 
	 loss: 17.1461, MinusLogProbMetric: 17.1461, val_loss: 17.3262, val_MinusLogProbMetric: 17.3262

Epoch 992: val_loss did not improve from 17.30492
196/196 - 60s - loss: 17.1461 - MinusLogProbMetric: 17.1461 - val_loss: 17.3262 - val_MinusLogProbMetric: 17.3262 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 993/1000
2023-09-27 09:14:46.793 
Epoch 993/1000 
	 loss: 17.1452, MinusLogProbMetric: 17.1452, val_loss: 17.3304, val_MinusLogProbMetric: 17.3304

Epoch 993: val_loss did not improve from 17.30492
196/196 - 60s - loss: 17.1452 - MinusLogProbMetric: 17.1452 - val_loss: 17.3304 - val_MinusLogProbMetric: 17.3304 - lr: 2.0833e-05 - 60s/epoch - 306ms/step
Epoch 994/1000
2023-09-27 09:15:47.264 
Epoch 994/1000 
	 loss: 17.1426, MinusLogProbMetric: 17.1426, val_loss: 17.3018, val_MinusLogProbMetric: 17.3018

Epoch 994: val_loss improved from 17.30492 to 17.30177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_293/weights/best_weights.h5
196/196 - 62s - loss: 17.1426 - MinusLogProbMetric: 17.1426 - val_loss: 17.3018 - val_MinusLogProbMetric: 17.3018 - lr: 2.0833e-05 - 62s/epoch - 315ms/step
Epoch 995/1000
2023-09-27 09:16:48.919 
Epoch 995/1000 
	 loss: 17.1421, MinusLogProbMetric: 17.1421, val_loss: 17.3100, val_MinusLogProbMetric: 17.3100

Epoch 995: val_loss did not improve from 17.30177
196/196 - 60s - loss: 17.1421 - MinusLogProbMetric: 17.1421 - val_loss: 17.3100 - val_MinusLogProbMetric: 17.3100 - lr: 2.0833e-05 - 60s/epoch - 308ms/step
Epoch 996/1000
2023-09-27 09:17:47.772 
Epoch 996/1000 
	 loss: 17.1433, MinusLogProbMetric: 17.1433, val_loss: 17.3161, val_MinusLogProbMetric: 17.3161

Epoch 996: val_loss did not improve from 17.30177
196/196 - 59s - loss: 17.1433 - MinusLogProbMetric: 17.1433 - val_loss: 17.3161 - val_MinusLogProbMetric: 17.3161 - lr: 2.0833e-05 - 59s/epoch - 300ms/step
Epoch 997/1000
2023-09-27 09:18:45.383 
Epoch 997/1000 
	 loss: 17.1435, MinusLogProbMetric: 17.1435, val_loss: 17.3264, val_MinusLogProbMetric: 17.3264

Epoch 997: val_loss did not improve from 17.30177
196/196 - 58s - loss: 17.1435 - MinusLogProbMetric: 17.1435 - val_loss: 17.3264 - val_MinusLogProbMetric: 17.3264 - lr: 2.0833e-05 - 58s/epoch - 294ms/step
Epoch 998/1000
2023-09-27 09:19:44.563 
Epoch 998/1000 
	 loss: 17.1423, MinusLogProbMetric: 17.1423, val_loss: 17.3116, val_MinusLogProbMetric: 17.3116

Epoch 998: val_loss did not improve from 17.30177
196/196 - 59s - loss: 17.1423 - MinusLogProbMetric: 17.1423 - val_loss: 17.3116 - val_MinusLogProbMetric: 17.3116 - lr: 2.0833e-05 - 59s/epoch - 302ms/step
Epoch 999/1000
2023-09-27 09:20:44.362 
Epoch 999/1000 
	 loss: 17.1430, MinusLogProbMetric: 17.1430, val_loss: 17.3210, val_MinusLogProbMetric: 17.3210

Epoch 999: val_loss did not improve from 17.30177
196/196 - 60s - loss: 17.1430 - MinusLogProbMetric: 17.1430 - val_loss: 17.3210 - val_MinusLogProbMetric: 17.3210 - lr: 2.0833e-05 - 60s/epoch - 305ms/step
Epoch 1000/1000
2023-09-27 09:21:44.178 
Epoch 1000/1000 
	 loss: 17.1447, MinusLogProbMetric: 17.1447, val_loss: 17.3405, val_MinusLogProbMetric: 17.3405

Epoch 1000: val_loss did not improve from 17.30177
196/196 - 60s - loss: 17.1447 - MinusLogProbMetric: 17.1447 - val_loss: 17.3405 - val_MinusLogProbMetric: 17.3405 - lr: 2.0833e-05 - 60s/epoch - 305ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 21.830399901024066 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 10.513772958947811 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 7.6309501670184545 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 10.187296641990542 seconds.
Training succeeded with seed 721.
Model trained in 64393.99 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 51.80 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 52.14 s.
===========
Run 293/720 done in 64616.01 s.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

===========
Generating train data for run 304.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_304/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_304/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_304/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_304
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  2798560   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,798,560
Trainable params: 2,798,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7feb345feda0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0b8e31060>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0b8e31060>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff0b95f7790>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feb8c358790>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feb8c35a620>, <keras.callbacks.ModelCheckpoint object at 0x7feb8c35a6b0>, <keras.callbacks.EarlyStopping object at 0x7feb8c35b160>, <keras.callbacks.ReduceLROnPlateau object at 0x7feb8c35b670>, <keras.callbacks.TerminateOnNaN object at 0x7feb8c35a470>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_304/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 304/720 with hyperparameters:
timestamp = 2023-09-27 09:22:50.723017
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2798560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 8: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 09:25:38.790 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2060.0884, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 2060.0884 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 168s/epoch - 856ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 304.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_304/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_304/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_304/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_304
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_49"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  2798560   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,798,560
Trainable params: 2,798,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7fea17d6b610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea17d26680>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea17d26680>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea17d53940>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea171bf8e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea171bfe50>, <keras.callbacks.ModelCheckpoint object at 0x7fea171bff10>, <keras.callbacks.EarlyStopping object at 0x7fea171bfe20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea171bffd0>, <keras.callbacks.TerminateOnNaN object at 0x7fea171f0040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_304/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 304/720 with hyperparameters:
timestamp = 2023-09-27 09:25:49.311796
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2798560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
2023-09-27 09:29:52.720 
Epoch 1/1000 
	 loss: 210.0111, MinusLogProbMetric: 210.0111, val_loss: 49.8879, val_MinusLogProbMetric: 49.8879

Epoch 1: val_loss improved from inf to 49.88789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 245s - loss: 210.0111 - MinusLogProbMetric: 210.0111 - val_loss: 49.8879 - val_MinusLogProbMetric: 49.8879 - lr: 3.3333e-04 - 245s/epoch - 1s/step
Epoch 2/1000
2023-09-27 09:31:09.929 
Epoch 2/1000 
	 loss: 40.5852, MinusLogProbMetric: 40.5852, val_loss: 34.5627, val_MinusLogProbMetric: 34.5627

Epoch 2: val_loss improved from 49.88789 to 34.56266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 40.5852 - MinusLogProbMetric: 40.5852 - val_loss: 34.5627 - val_MinusLogProbMetric: 34.5627 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 3/1000
2023-09-27 09:32:26.375 
Epoch 3/1000 
	 loss: 31.6468, MinusLogProbMetric: 31.6468, val_loss: 29.4157, val_MinusLogProbMetric: 29.4157

Epoch 3: val_loss improved from 34.56266 to 29.41574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 31.6468 - MinusLogProbMetric: 31.6468 - val_loss: 29.4157 - val_MinusLogProbMetric: 29.4157 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 4/1000
2023-09-27 09:33:43.375 
Epoch 4/1000 
	 loss: 27.5331, MinusLogProbMetric: 27.5331, val_loss: 27.7895, val_MinusLogProbMetric: 27.7895

Epoch 4: val_loss improved from 29.41574 to 27.78948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 27.5331 - MinusLogProbMetric: 27.5331 - val_loss: 27.7895 - val_MinusLogProbMetric: 27.7895 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 5/1000
2023-09-27 09:35:00.519 
Epoch 5/1000 
	 loss: 25.6057, MinusLogProbMetric: 25.6057, val_loss: 25.0634, val_MinusLogProbMetric: 25.0634

Epoch 5: val_loss improved from 27.78948 to 25.06341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 25.6057 - MinusLogProbMetric: 25.6057 - val_loss: 25.0634 - val_MinusLogProbMetric: 25.0634 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 6/1000
2023-09-27 09:36:18.437 
Epoch 6/1000 
	 loss: 24.2402, MinusLogProbMetric: 24.2402, val_loss: 25.4173, val_MinusLogProbMetric: 25.4173

Epoch 6: val_loss did not improve from 25.06341
196/196 - 76s - loss: 24.2402 - MinusLogProbMetric: 24.2402 - val_loss: 25.4173 - val_MinusLogProbMetric: 25.4173 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 7/1000
2023-09-27 09:37:34.162 
Epoch 7/1000 
	 loss: 23.2858, MinusLogProbMetric: 23.2858, val_loss: 23.3584, val_MinusLogProbMetric: 23.3584

Epoch 7: val_loss improved from 25.06341 to 23.35843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 23.2858 - MinusLogProbMetric: 23.2858 - val_loss: 23.3584 - val_MinusLogProbMetric: 23.3584 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 8/1000
2023-09-27 09:38:51.821 
Epoch 8/1000 
	 loss: 22.6501, MinusLogProbMetric: 22.6501, val_loss: 22.8526, val_MinusLogProbMetric: 22.8526

Epoch 8: val_loss improved from 23.35843 to 22.85263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 22.6501 - MinusLogProbMetric: 22.6501 - val_loss: 22.8526 - val_MinusLogProbMetric: 22.8526 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 9/1000
2023-09-27 09:40:09.107 
Epoch 9/1000 
	 loss: 22.0152, MinusLogProbMetric: 22.0152, val_loss: 21.9364, val_MinusLogProbMetric: 21.9364

Epoch 9: val_loss improved from 22.85263 to 21.93644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 22.0152 - MinusLogProbMetric: 22.0152 - val_loss: 21.9364 - val_MinusLogProbMetric: 21.9364 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 10/1000
2023-09-27 09:41:26.705 
Epoch 10/1000 
	 loss: 21.5720, MinusLogProbMetric: 21.5720, val_loss: 21.9642, val_MinusLogProbMetric: 21.9642

Epoch 10: val_loss did not improve from 21.93644
196/196 - 76s - loss: 21.5720 - MinusLogProbMetric: 21.5720 - val_loss: 21.9642 - val_MinusLogProbMetric: 21.9642 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 11/1000
2023-09-27 09:42:42.406 
Epoch 11/1000 
	 loss: 21.3181, MinusLogProbMetric: 21.3181, val_loss: 21.2994, val_MinusLogProbMetric: 21.2994

Epoch 11: val_loss improved from 21.93644 to 21.29944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 21.3181 - MinusLogProbMetric: 21.3181 - val_loss: 21.2994 - val_MinusLogProbMetric: 21.2994 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 12/1000
2023-09-27 09:43:58.994 
Epoch 12/1000 
	 loss: 20.9618, MinusLogProbMetric: 20.9618, val_loss: 21.2121, val_MinusLogProbMetric: 21.2121

Epoch 12: val_loss improved from 21.29944 to 21.21207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 20.9618 - MinusLogProbMetric: 20.9618 - val_loss: 21.2121 - val_MinusLogProbMetric: 21.2121 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 13/1000
2023-09-27 09:45:15.545 
Epoch 13/1000 
	 loss: 20.6768, MinusLogProbMetric: 20.6768, val_loss: 20.6904, val_MinusLogProbMetric: 20.6904

Epoch 13: val_loss improved from 21.21207 to 20.69037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 20.6768 - MinusLogProbMetric: 20.6768 - val_loss: 20.6904 - val_MinusLogProbMetric: 20.6904 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 14/1000
2023-09-27 09:46:32.381 
Epoch 14/1000 
	 loss: 20.4473, MinusLogProbMetric: 20.4473, val_loss: 22.1022, val_MinusLogProbMetric: 22.1022

Epoch 14: val_loss did not improve from 20.69037
196/196 - 76s - loss: 20.4473 - MinusLogProbMetric: 20.4473 - val_loss: 22.1022 - val_MinusLogProbMetric: 22.1022 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 15/1000
2023-09-27 09:47:47.290 
Epoch 15/1000 
	 loss: 20.3984, MinusLogProbMetric: 20.3984, val_loss: 21.2799, val_MinusLogProbMetric: 21.2799

Epoch 15: val_loss did not improve from 20.69037
196/196 - 75s - loss: 20.3984 - MinusLogProbMetric: 20.3984 - val_loss: 21.2799 - val_MinusLogProbMetric: 21.2799 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 16/1000
2023-09-27 09:49:02.216 
Epoch 16/1000 
	 loss: 20.0260, MinusLogProbMetric: 20.0260, val_loss: 20.6242, val_MinusLogProbMetric: 20.6242

Epoch 16: val_loss improved from 20.69037 to 20.62417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 20.0260 - MinusLogProbMetric: 20.0260 - val_loss: 20.6242 - val_MinusLogProbMetric: 20.6242 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 17/1000
2023-09-27 09:50:19.406 
Epoch 17/1000 
	 loss: 20.0385, MinusLogProbMetric: 20.0385, val_loss: 21.4054, val_MinusLogProbMetric: 21.4054

Epoch 17: val_loss did not improve from 20.62417
196/196 - 76s - loss: 20.0385 - MinusLogProbMetric: 20.0385 - val_loss: 21.4054 - val_MinusLogProbMetric: 21.4054 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 18/1000
2023-09-27 09:51:34.665 
Epoch 18/1000 
	 loss: 20.0207, MinusLogProbMetric: 20.0207, val_loss: 19.9226, val_MinusLogProbMetric: 19.9226

Epoch 18: val_loss improved from 20.62417 to 19.92265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 20.0207 - MinusLogProbMetric: 20.0207 - val_loss: 19.9226 - val_MinusLogProbMetric: 19.9226 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 19/1000
2023-09-27 09:52:51.756 
Epoch 19/1000 
	 loss: 19.7549, MinusLogProbMetric: 19.7549, val_loss: 20.2419, val_MinusLogProbMetric: 20.2419

Epoch 19: val_loss did not improve from 19.92265
196/196 - 76s - loss: 19.7549 - MinusLogProbMetric: 19.7549 - val_loss: 20.2419 - val_MinusLogProbMetric: 20.2419 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 20/1000
2023-09-27 09:54:07.272 
Epoch 20/1000 
	 loss: 19.6412, MinusLogProbMetric: 19.6412, val_loss: 20.2950, val_MinusLogProbMetric: 20.2950

Epoch 20: val_loss did not improve from 19.92265
196/196 - 76s - loss: 19.6412 - MinusLogProbMetric: 19.6412 - val_loss: 20.2950 - val_MinusLogProbMetric: 20.2950 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 21/1000
2023-09-27 09:55:22.890 
Epoch 21/1000 
	 loss: 19.5387, MinusLogProbMetric: 19.5387, val_loss: 19.9360, val_MinusLogProbMetric: 19.9360

Epoch 21: val_loss did not improve from 19.92265
196/196 - 76s - loss: 19.5387 - MinusLogProbMetric: 19.5387 - val_loss: 19.9360 - val_MinusLogProbMetric: 19.9360 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 22/1000
2023-09-27 09:56:38.256 
Epoch 22/1000 
	 loss: 19.5082, MinusLogProbMetric: 19.5082, val_loss: 20.0506, val_MinusLogProbMetric: 20.0506

Epoch 22: val_loss did not improve from 19.92265
196/196 - 75s - loss: 19.5082 - MinusLogProbMetric: 19.5082 - val_loss: 20.0506 - val_MinusLogProbMetric: 20.0506 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 23/1000
2023-09-27 09:57:54.416 
Epoch 23/1000 
	 loss: 19.3867, MinusLogProbMetric: 19.3867, val_loss: 19.2658, val_MinusLogProbMetric: 19.2658

Epoch 23: val_loss improved from 19.92265 to 19.26578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 19.3867 - MinusLogProbMetric: 19.3867 - val_loss: 19.2658 - val_MinusLogProbMetric: 19.2658 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 24/1000
2023-09-27 09:59:11.117 
Epoch 24/1000 
	 loss: 19.2718, MinusLogProbMetric: 19.2718, val_loss: 19.8668, val_MinusLogProbMetric: 19.8668

Epoch 24: val_loss did not improve from 19.26578
196/196 - 75s - loss: 19.2718 - MinusLogProbMetric: 19.2718 - val_loss: 19.8668 - val_MinusLogProbMetric: 19.8668 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 25/1000
2023-09-27 10:00:26.712 
Epoch 25/1000 
	 loss: 19.2185, MinusLogProbMetric: 19.2185, val_loss: 19.3424, val_MinusLogProbMetric: 19.3424

Epoch 25: val_loss did not improve from 19.26578
196/196 - 76s - loss: 19.2185 - MinusLogProbMetric: 19.2185 - val_loss: 19.3424 - val_MinusLogProbMetric: 19.3424 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 26/1000
2023-09-27 10:01:42.540 
Epoch 26/1000 
	 loss: 19.1266, MinusLogProbMetric: 19.1266, val_loss: 19.5311, val_MinusLogProbMetric: 19.5311

Epoch 26: val_loss did not improve from 19.26578
196/196 - 76s - loss: 19.1266 - MinusLogProbMetric: 19.1266 - val_loss: 19.5311 - val_MinusLogProbMetric: 19.5311 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 27/1000
2023-09-27 10:02:57.988 
Epoch 27/1000 
	 loss: 19.0663, MinusLogProbMetric: 19.0663, val_loss: 20.5752, val_MinusLogProbMetric: 20.5752

Epoch 27: val_loss did not improve from 19.26578
196/196 - 75s - loss: 19.0663 - MinusLogProbMetric: 19.0663 - val_loss: 20.5752 - val_MinusLogProbMetric: 20.5752 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 28/1000
2023-09-27 10:04:13.209 
Epoch 28/1000 
	 loss: 18.9936, MinusLogProbMetric: 18.9936, val_loss: 19.1773, val_MinusLogProbMetric: 19.1773

Epoch 28: val_loss improved from 19.26578 to 19.17732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 18.9936 - MinusLogProbMetric: 18.9936 - val_loss: 19.1773 - val_MinusLogProbMetric: 19.1773 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 29/1000
2023-09-27 10:05:30.568 
Epoch 29/1000 
	 loss: 18.9194, MinusLogProbMetric: 18.9194, val_loss: 18.9600, val_MinusLogProbMetric: 18.9600

Epoch 29: val_loss improved from 19.17732 to 18.96001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 18.9194 - MinusLogProbMetric: 18.9194 - val_loss: 18.9600 - val_MinusLogProbMetric: 18.9600 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 30/1000
2023-09-27 10:06:43.430 
Epoch 30/1000 
	 loss: 18.9246, MinusLogProbMetric: 18.9246, val_loss: 19.4503, val_MinusLogProbMetric: 19.4503

Epoch 30: val_loss did not improve from 18.96001
196/196 - 71s - loss: 18.9246 - MinusLogProbMetric: 18.9246 - val_loss: 19.4503 - val_MinusLogProbMetric: 19.4503 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 31/1000
2023-09-27 10:07:51.029 
Epoch 31/1000 
	 loss: 18.8837, MinusLogProbMetric: 18.8837, val_loss: 19.1849, val_MinusLogProbMetric: 19.1849

Epoch 31: val_loss did not improve from 18.96001
196/196 - 68s - loss: 18.8837 - MinusLogProbMetric: 18.8837 - val_loss: 19.1849 - val_MinusLogProbMetric: 19.1849 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 32/1000
2023-09-27 10:09:04.886 
Epoch 32/1000 
	 loss: 18.8048, MinusLogProbMetric: 18.8048, val_loss: 19.8597, val_MinusLogProbMetric: 19.8597

Epoch 32: val_loss did not improve from 18.96001
196/196 - 74s - loss: 18.8048 - MinusLogProbMetric: 18.8048 - val_loss: 19.8597 - val_MinusLogProbMetric: 19.8597 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 33/1000
2023-09-27 10:10:20.658 
Epoch 33/1000 
	 loss: 18.8046, MinusLogProbMetric: 18.8046, val_loss: 18.8685, val_MinusLogProbMetric: 18.8685

Epoch 33: val_loss improved from 18.96001 to 18.86852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 18.8046 - MinusLogProbMetric: 18.8046 - val_loss: 18.8685 - val_MinusLogProbMetric: 18.8685 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 34/1000
2023-09-27 10:11:38.147 
Epoch 34/1000 
	 loss: 18.7103, MinusLogProbMetric: 18.7103, val_loss: 19.3829, val_MinusLogProbMetric: 19.3829

Epoch 34: val_loss did not improve from 18.86852
196/196 - 76s - loss: 18.7103 - MinusLogProbMetric: 18.7103 - val_loss: 19.3829 - val_MinusLogProbMetric: 19.3829 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 35/1000
2023-09-27 10:12:54.517 
Epoch 35/1000 
	 loss: 18.6451, MinusLogProbMetric: 18.6451, val_loss: 19.2388, val_MinusLogProbMetric: 19.2388

Epoch 35: val_loss did not improve from 18.86852
196/196 - 76s - loss: 18.6451 - MinusLogProbMetric: 18.6451 - val_loss: 19.2388 - val_MinusLogProbMetric: 19.2388 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 36/1000
2023-09-27 10:14:13.006 
Epoch 36/1000 
	 loss: 18.6108, MinusLogProbMetric: 18.6108, val_loss: 18.6665, val_MinusLogProbMetric: 18.6665

Epoch 36: val_loss improved from 18.86852 to 18.66655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 80s - loss: 18.6108 - MinusLogProbMetric: 18.6108 - val_loss: 18.6665 - val_MinusLogProbMetric: 18.6665 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 37/1000
2023-09-27 10:15:31.370 
Epoch 37/1000 
	 loss: 18.5263, MinusLogProbMetric: 18.5263, val_loss: 19.2616, val_MinusLogProbMetric: 19.2616

Epoch 37: val_loss did not improve from 18.66655
196/196 - 77s - loss: 18.5263 - MinusLogProbMetric: 18.5263 - val_loss: 19.2616 - val_MinusLogProbMetric: 19.2616 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 38/1000
2023-09-27 10:16:49.141 
Epoch 38/1000 
	 loss: 18.6574, MinusLogProbMetric: 18.6574, val_loss: 19.0510, val_MinusLogProbMetric: 19.0510

Epoch 38: val_loss did not improve from 18.66655
196/196 - 78s - loss: 18.6574 - MinusLogProbMetric: 18.6574 - val_loss: 19.0510 - val_MinusLogProbMetric: 19.0510 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 39/1000
2023-09-27 10:18:05.645 
Epoch 39/1000 
	 loss: 18.5769, MinusLogProbMetric: 18.5769, val_loss: 18.9278, val_MinusLogProbMetric: 18.9278

Epoch 39: val_loss did not improve from 18.66655
196/196 - 77s - loss: 18.5769 - MinusLogProbMetric: 18.5769 - val_loss: 18.9278 - val_MinusLogProbMetric: 18.9278 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 40/1000
2023-09-27 10:19:23.407 
Epoch 40/1000 
	 loss: 18.5849, MinusLogProbMetric: 18.5849, val_loss: 18.5410, val_MinusLogProbMetric: 18.5410

Epoch 40: val_loss improved from 18.66655 to 18.54097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 79s - loss: 18.5849 - MinusLogProbMetric: 18.5849 - val_loss: 18.5410 - val_MinusLogProbMetric: 18.5410 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 41/1000
2023-09-27 10:20:42.427 
Epoch 41/1000 
	 loss: 18.5310, MinusLogProbMetric: 18.5310, val_loss: 18.6787, val_MinusLogProbMetric: 18.6787

Epoch 41: val_loss did not improve from 18.54097
196/196 - 77s - loss: 18.5310 - MinusLogProbMetric: 18.5310 - val_loss: 18.6787 - val_MinusLogProbMetric: 18.6787 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 42/1000
2023-09-27 10:21:58.729 
Epoch 42/1000 
	 loss: 18.3993, MinusLogProbMetric: 18.3993, val_loss: 18.8295, val_MinusLogProbMetric: 18.8295

Epoch 42: val_loss did not improve from 18.54097
196/196 - 76s - loss: 18.3993 - MinusLogProbMetric: 18.3993 - val_loss: 18.8295 - val_MinusLogProbMetric: 18.8295 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 43/1000
2023-09-27 10:23:15.776 
Epoch 43/1000 
	 loss: 18.4342, MinusLogProbMetric: 18.4342, val_loss: 19.0562, val_MinusLogProbMetric: 19.0562

Epoch 43: val_loss did not improve from 18.54097
196/196 - 77s - loss: 18.4342 - MinusLogProbMetric: 18.4342 - val_loss: 19.0562 - val_MinusLogProbMetric: 19.0562 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 44/1000
2023-09-27 10:24:32.743 
Epoch 44/1000 
	 loss: 18.3923, MinusLogProbMetric: 18.3923, val_loss: 18.8348, val_MinusLogProbMetric: 18.8348

Epoch 44: val_loss did not improve from 18.54097
196/196 - 77s - loss: 18.3923 - MinusLogProbMetric: 18.3923 - val_loss: 18.8348 - val_MinusLogProbMetric: 18.8348 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 45/1000
2023-09-27 10:25:49.568 
Epoch 45/1000 
	 loss: 18.3126, MinusLogProbMetric: 18.3126, val_loss: 19.3489, val_MinusLogProbMetric: 19.3489

Epoch 45: val_loss did not improve from 18.54097
196/196 - 77s - loss: 18.3126 - MinusLogProbMetric: 18.3126 - val_loss: 19.3489 - val_MinusLogProbMetric: 19.3489 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 46/1000
2023-09-27 10:27:06.635 
Epoch 46/1000 
	 loss: 18.3731, MinusLogProbMetric: 18.3731, val_loss: 18.7558, val_MinusLogProbMetric: 18.7558

Epoch 46: val_loss did not improve from 18.54097
196/196 - 77s - loss: 18.3731 - MinusLogProbMetric: 18.3731 - val_loss: 18.7558 - val_MinusLogProbMetric: 18.7558 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 47/1000
2023-09-27 10:28:23.156 
Epoch 47/1000 
	 loss: 18.3721, MinusLogProbMetric: 18.3721, val_loss: 18.7323, val_MinusLogProbMetric: 18.7323

Epoch 47: val_loss did not improve from 18.54097
196/196 - 77s - loss: 18.3721 - MinusLogProbMetric: 18.3721 - val_loss: 18.7323 - val_MinusLogProbMetric: 18.7323 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 48/1000
2023-09-27 10:29:39.927 
Epoch 48/1000 
	 loss: 18.2714, MinusLogProbMetric: 18.2714, val_loss: 18.3725, val_MinusLogProbMetric: 18.3725

Epoch 48: val_loss improved from 18.54097 to 18.37250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 18.2714 - MinusLogProbMetric: 18.2714 - val_loss: 18.3725 - val_MinusLogProbMetric: 18.3725 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 49/1000
2023-09-27 10:30:59.086 
Epoch 49/1000 
	 loss: 18.3156, MinusLogProbMetric: 18.3156, val_loss: 18.3261, val_MinusLogProbMetric: 18.3261

Epoch 49: val_loss improved from 18.37250 to 18.32614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 79s - loss: 18.3156 - MinusLogProbMetric: 18.3156 - val_loss: 18.3261 - val_MinusLogProbMetric: 18.3261 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 50/1000
2023-09-27 10:32:17.605 
Epoch 50/1000 
	 loss: 18.2511, MinusLogProbMetric: 18.2511, val_loss: 18.5767, val_MinusLogProbMetric: 18.5767

Epoch 50: val_loss did not improve from 18.32614
196/196 - 77s - loss: 18.2511 - MinusLogProbMetric: 18.2511 - val_loss: 18.5767 - val_MinusLogProbMetric: 18.5767 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 51/1000
2023-09-27 10:33:35.071 
Epoch 51/1000 
	 loss: 18.2097, MinusLogProbMetric: 18.2097, val_loss: 19.0370, val_MinusLogProbMetric: 19.0370

Epoch 51: val_loss did not improve from 18.32614
196/196 - 77s - loss: 18.2097 - MinusLogProbMetric: 18.2097 - val_loss: 19.0370 - val_MinusLogProbMetric: 19.0370 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 52/1000
2023-09-27 10:34:52.006 
Epoch 52/1000 
	 loss: 18.2368, MinusLogProbMetric: 18.2368, val_loss: 18.6072, val_MinusLogProbMetric: 18.6072

Epoch 52: val_loss did not improve from 18.32614
196/196 - 77s - loss: 18.2368 - MinusLogProbMetric: 18.2368 - val_loss: 18.6072 - val_MinusLogProbMetric: 18.6072 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 53/1000
2023-09-27 10:36:08.624 
Epoch 53/1000 
	 loss: 18.1862, MinusLogProbMetric: 18.1862, val_loss: 18.5029, val_MinusLogProbMetric: 18.5029

Epoch 53: val_loss did not improve from 18.32614
196/196 - 77s - loss: 18.1862 - MinusLogProbMetric: 18.1862 - val_loss: 18.5029 - val_MinusLogProbMetric: 18.5029 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 54/1000
2023-09-27 10:37:25.214 
Epoch 54/1000 
	 loss: 18.1606, MinusLogProbMetric: 18.1606, val_loss: 18.5924, val_MinusLogProbMetric: 18.5924

Epoch 54: val_loss did not improve from 18.32614
196/196 - 77s - loss: 18.1606 - MinusLogProbMetric: 18.1606 - val_loss: 18.5924 - val_MinusLogProbMetric: 18.5924 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 55/1000
2023-09-27 10:38:41.950 
Epoch 55/1000 
	 loss: 18.1527, MinusLogProbMetric: 18.1527, val_loss: 18.6700, val_MinusLogProbMetric: 18.6700

Epoch 55: val_loss did not improve from 18.32614
196/196 - 77s - loss: 18.1527 - MinusLogProbMetric: 18.1527 - val_loss: 18.6700 - val_MinusLogProbMetric: 18.6700 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 56/1000
2023-09-27 10:39:58.753 
Epoch 56/1000 
	 loss: 18.1380, MinusLogProbMetric: 18.1380, val_loss: 18.3888, val_MinusLogProbMetric: 18.3888

Epoch 56: val_loss did not improve from 18.32614
196/196 - 77s - loss: 18.1380 - MinusLogProbMetric: 18.1380 - val_loss: 18.3888 - val_MinusLogProbMetric: 18.3888 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 57/1000
2023-09-27 10:41:15.479 
Epoch 57/1000 
	 loss: 18.0894, MinusLogProbMetric: 18.0894, val_loss: 18.3181, val_MinusLogProbMetric: 18.3181

Epoch 57: val_loss improved from 18.32614 to 18.31810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 18.0894 - MinusLogProbMetric: 18.0894 - val_loss: 18.3181 - val_MinusLogProbMetric: 18.3181 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 58/1000
2023-09-27 10:42:33.875 
Epoch 58/1000 
	 loss: 18.1084, MinusLogProbMetric: 18.1084, val_loss: 18.7620, val_MinusLogProbMetric: 18.7620

Epoch 58: val_loss did not improve from 18.31810
196/196 - 77s - loss: 18.1084 - MinusLogProbMetric: 18.1084 - val_loss: 18.7620 - val_MinusLogProbMetric: 18.7620 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 59/1000
2023-09-27 10:43:51.465 
Epoch 59/1000 
	 loss: 18.1261, MinusLogProbMetric: 18.1261, val_loss: 18.3533, val_MinusLogProbMetric: 18.3533

Epoch 59: val_loss did not improve from 18.31810
196/196 - 78s - loss: 18.1261 - MinusLogProbMetric: 18.1261 - val_loss: 18.3533 - val_MinusLogProbMetric: 18.3533 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 60/1000
2023-09-27 10:45:08.588 
Epoch 60/1000 
	 loss: 18.0249, MinusLogProbMetric: 18.0249, val_loss: 18.2986, val_MinusLogProbMetric: 18.2986

Epoch 60: val_loss improved from 18.31810 to 18.29858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 79s - loss: 18.0249 - MinusLogProbMetric: 18.0249 - val_loss: 18.2986 - val_MinusLogProbMetric: 18.2986 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 61/1000
2023-09-27 10:46:27.372 
Epoch 61/1000 
	 loss: 18.0778, MinusLogProbMetric: 18.0778, val_loss: 18.3384, val_MinusLogProbMetric: 18.3384

Epoch 61: val_loss did not improve from 18.29858
196/196 - 77s - loss: 18.0778 - MinusLogProbMetric: 18.0778 - val_loss: 18.3384 - val_MinusLogProbMetric: 18.3384 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 62/1000
2023-09-27 10:47:43.961 
Epoch 62/1000 
	 loss: 18.0058, MinusLogProbMetric: 18.0058, val_loss: 18.9422, val_MinusLogProbMetric: 18.9422

Epoch 62: val_loss did not improve from 18.29858
196/196 - 77s - loss: 18.0058 - MinusLogProbMetric: 18.0058 - val_loss: 18.9422 - val_MinusLogProbMetric: 18.9422 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 63/1000
2023-09-27 10:49:00.104 
Epoch 63/1000 
	 loss: 18.0078, MinusLogProbMetric: 18.0078, val_loss: 18.2671, val_MinusLogProbMetric: 18.2671

Epoch 63: val_loss improved from 18.29858 to 18.26713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 18.0078 - MinusLogProbMetric: 18.0078 - val_loss: 18.2671 - val_MinusLogProbMetric: 18.2671 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 64/1000
2023-09-27 10:50:18.468 
Epoch 64/1000 
	 loss: 18.0641, MinusLogProbMetric: 18.0641, val_loss: 18.3483, val_MinusLogProbMetric: 18.3483

Epoch 64: val_loss did not improve from 18.26713
196/196 - 77s - loss: 18.0641 - MinusLogProbMetric: 18.0641 - val_loss: 18.3483 - val_MinusLogProbMetric: 18.3483 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 65/1000
2023-09-27 10:51:35.208 
Epoch 65/1000 
	 loss: 17.9426, MinusLogProbMetric: 17.9426, val_loss: 18.4285, val_MinusLogProbMetric: 18.4285

Epoch 65: val_loss did not improve from 18.26713
196/196 - 77s - loss: 17.9426 - MinusLogProbMetric: 17.9426 - val_loss: 18.4285 - val_MinusLogProbMetric: 18.4285 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 66/1000
2023-09-27 10:52:52.429 
Epoch 66/1000 
	 loss: 17.9775, MinusLogProbMetric: 17.9775, val_loss: 19.1533, val_MinusLogProbMetric: 19.1533

Epoch 66: val_loss did not improve from 18.26713
196/196 - 77s - loss: 17.9775 - MinusLogProbMetric: 17.9775 - val_loss: 19.1533 - val_MinusLogProbMetric: 19.1533 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 67/1000
2023-09-27 10:54:09.628 
Epoch 67/1000 
	 loss: 17.9963, MinusLogProbMetric: 17.9963, val_loss: 18.7200, val_MinusLogProbMetric: 18.7200

Epoch 67: val_loss did not improve from 18.26713
196/196 - 77s - loss: 17.9963 - MinusLogProbMetric: 17.9963 - val_loss: 18.7200 - val_MinusLogProbMetric: 18.7200 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 68/1000
2023-09-27 10:55:26.764 
Epoch 68/1000 
	 loss: 17.9480, MinusLogProbMetric: 17.9480, val_loss: 18.2215, val_MinusLogProbMetric: 18.2215

Epoch 68: val_loss improved from 18.26713 to 18.22145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 79s - loss: 17.9480 - MinusLogProbMetric: 17.9480 - val_loss: 18.2215 - val_MinusLogProbMetric: 18.2215 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 69/1000
2023-09-27 10:56:44.482 
Epoch 69/1000 
	 loss: 17.9657, MinusLogProbMetric: 17.9657, val_loss: 18.2401, val_MinusLogProbMetric: 18.2401

Epoch 69: val_loss did not improve from 18.22145
196/196 - 76s - loss: 17.9657 - MinusLogProbMetric: 17.9657 - val_loss: 18.2401 - val_MinusLogProbMetric: 18.2401 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 70/1000
2023-09-27 10:58:01.530 
Epoch 70/1000 
	 loss: 17.9519, MinusLogProbMetric: 17.9519, val_loss: 18.0692, val_MinusLogProbMetric: 18.0692

Epoch 70: val_loss improved from 18.22145 to 18.06922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.9519 - MinusLogProbMetric: 17.9519 - val_loss: 18.0692 - val_MinusLogProbMetric: 18.0692 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 71/1000
2023-09-27 10:59:19.751 
Epoch 71/1000 
	 loss: 17.8982, MinusLogProbMetric: 17.8982, val_loss: 18.2254, val_MinusLogProbMetric: 18.2254

Epoch 71: val_loss did not improve from 18.06922
196/196 - 77s - loss: 17.8982 - MinusLogProbMetric: 17.8982 - val_loss: 18.2254 - val_MinusLogProbMetric: 18.2254 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 72/1000
2023-09-27 11:00:36.110 
Epoch 72/1000 
	 loss: 17.8623, MinusLogProbMetric: 17.8623, val_loss: 18.2400, val_MinusLogProbMetric: 18.2400

Epoch 72: val_loss did not improve from 18.06922
196/196 - 76s - loss: 17.8623 - MinusLogProbMetric: 17.8623 - val_loss: 18.2400 - val_MinusLogProbMetric: 18.2400 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 73/1000
2023-09-27 11:01:53.136 
Epoch 73/1000 
	 loss: 17.9175, MinusLogProbMetric: 17.9175, val_loss: 18.0783, val_MinusLogProbMetric: 18.0783

Epoch 73: val_loss did not improve from 18.06922
196/196 - 77s - loss: 17.9175 - MinusLogProbMetric: 17.9175 - val_loss: 18.0783 - val_MinusLogProbMetric: 18.0783 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 74/1000
2023-09-27 11:03:09.219 
Epoch 74/1000 
	 loss: 17.8768, MinusLogProbMetric: 17.8768, val_loss: 19.0675, val_MinusLogProbMetric: 19.0675

Epoch 74: val_loss did not improve from 18.06922
196/196 - 76s - loss: 17.8768 - MinusLogProbMetric: 17.8768 - val_loss: 19.0675 - val_MinusLogProbMetric: 19.0675 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 75/1000
2023-09-27 11:04:25.527 
Epoch 75/1000 
	 loss: 17.8620, MinusLogProbMetric: 17.8620, val_loss: 18.1705, val_MinusLogProbMetric: 18.1705

Epoch 75: val_loss did not improve from 18.06922
196/196 - 76s - loss: 17.8620 - MinusLogProbMetric: 17.8620 - val_loss: 18.1705 - val_MinusLogProbMetric: 18.1705 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 76/1000
2023-09-27 11:05:42.946 
Epoch 76/1000 
	 loss: 17.8203, MinusLogProbMetric: 17.8203, val_loss: 18.0910, val_MinusLogProbMetric: 18.0910

Epoch 76: val_loss did not improve from 18.06922
196/196 - 77s - loss: 17.8203 - MinusLogProbMetric: 17.8203 - val_loss: 18.0910 - val_MinusLogProbMetric: 18.0910 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 77/1000
2023-09-27 11:06:59.274 
Epoch 77/1000 
	 loss: 17.8006, MinusLogProbMetric: 17.8006, val_loss: 18.6535, val_MinusLogProbMetric: 18.6535

Epoch 77: val_loss did not improve from 18.06922
196/196 - 76s - loss: 17.8006 - MinusLogProbMetric: 17.8006 - val_loss: 18.6535 - val_MinusLogProbMetric: 18.6535 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 78/1000
2023-09-27 11:08:15.651 
Epoch 78/1000 
	 loss: 17.8076, MinusLogProbMetric: 17.8076, val_loss: 18.1379, val_MinusLogProbMetric: 18.1379

Epoch 78: val_loss did not improve from 18.06922
196/196 - 76s - loss: 17.8076 - MinusLogProbMetric: 17.8076 - val_loss: 18.1379 - val_MinusLogProbMetric: 18.1379 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 79/1000
2023-09-27 11:09:32.234 
Epoch 79/1000 
	 loss: 17.8108, MinusLogProbMetric: 17.8108, val_loss: 18.0857, val_MinusLogProbMetric: 18.0857

Epoch 79: val_loss did not improve from 18.06922
196/196 - 77s - loss: 17.8108 - MinusLogProbMetric: 17.8108 - val_loss: 18.0857 - val_MinusLogProbMetric: 18.0857 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 80/1000
2023-09-27 11:10:48.693 
Epoch 80/1000 
	 loss: 17.7983, MinusLogProbMetric: 17.7983, val_loss: 18.2229, val_MinusLogProbMetric: 18.2229

Epoch 80: val_loss did not improve from 18.06922
196/196 - 76s - loss: 17.7983 - MinusLogProbMetric: 17.7983 - val_loss: 18.2229 - val_MinusLogProbMetric: 18.2229 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 81/1000
2023-09-27 11:12:00.024 
Epoch 81/1000 
	 loss: 17.7899, MinusLogProbMetric: 17.7899, val_loss: 18.3045, val_MinusLogProbMetric: 18.3045

Epoch 81: val_loss did not improve from 18.06922
196/196 - 71s - loss: 17.7899 - MinusLogProbMetric: 17.7899 - val_loss: 18.3045 - val_MinusLogProbMetric: 18.3045 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 82/1000
2023-09-27 11:13:16.986 
Epoch 82/1000 
	 loss: 17.8130, MinusLogProbMetric: 17.8130, val_loss: 18.0557, val_MinusLogProbMetric: 18.0557

Epoch 82: val_loss improved from 18.06922 to 18.05568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.8130 - MinusLogProbMetric: 17.8130 - val_loss: 18.0557 - val_MinusLogProbMetric: 18.0557 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 83/1000
2023-09-27 11:14:35.100 
Epoch 83/1000 
	 loss: 17.7663, MinusLogProbMetric: 17.7663, val_loss: 18.3242, val_MinusLogProbMetric: 18.3242

Epoch 83: val_loss did not improve from 18.05568
196/196 - 77s - loss: 17.7663 - MinusLogProbMetric: 17.7663 - val_loss: 18.3242 - val_MinusLogProbMetric: 18.3242 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 84/1000
2023-09-27 11:15:51.787 
Epoch 84/1000 
	 loss: 17.7752, MinusLogProbMetric: 17.7752, val_loss: 19.0143, val_MinusLogProbMetric: 19.0143

Epoch 84: val_loss did not improve from 18.05568
196/196 - 77s - loss: 17.7752 - MinusLogProbMetric: 17.7752 - val_loss: 19.0143 - val_MinusLogProbMetric: 19.0143 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 85/1000
2023-09-27 11:17:08.659 
Epoch 85/1000 
	 loss: 17.7675, MinusLogProbMetric: 17.7675, val_loss: 17.9912, val_MinusLogProbMetric: 17.9912

Epoch 85: val_loss improved from 18.05568 to 17.99125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.7675 - MinusLogProbMetric: 17.7675 - val_loss: 17.9912 - val_MinusLogProbMetric: 17.9912 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 86/1000
2023-09-27 11:18:26.824 
Epoch 86/1000 
	 loss: 17.7669, MinusLogProbMetric: 17.7669, val_loss: 18.0929, val_MinusLogProbMetric: 18.0929

Epoch 86: val_loss did not improve from 17.99125
196/196 - 77s - loss: 17.7669 - MinusLogProbMetric: 17.7669 - val_loss: 18.0929 - val_MinusLogProbMetric: 18.0929 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 87/1000
2023-09-27 11:19:43.587 
Epoch 87/1000 
	 loss: 17.7048, MinusLogProbMetric: 17.7048, val_loss: 17.9967, val_MinusLogProbMetric: 17.9967

Epoch 87: val_loss did not improve from 17.99125
196/196 - 77s - loss: 17.7048 - MinusLogProbMetric: 17.7048 - val_loss: 17.9967 - val_MinusLogProbMetric: 17.9967 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 88/1000
2023-09-27 11:21:00.154 
Epoch 88/1000 
	 loss: 17.7178, MinusLogProbMetric: 17.7178, val_loss: 17.8752, val_MinusLogProbMetric: 17.8752

Epoch 88: val_loss improved from 17.99125 to 17.87524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.7178 - MinusLogProbMetric: 17.7178 - val_loss: 17.8752 - val_MinusLogProbMetric: 17.8752 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 89/1000
2023-09-27 11:22:17.746 
Epoch 89/1000 
	 loss: 17.6642, MinusLogProbMetric: 17.6642, val_loss: 18.0105, val_MinusLogProbMetric: 18.0105

Epoch 89: val_loss did not improve from 17.87524
196/196 - 76s - loss: 17.6642 - MinusLogProbMetric: 17.6642 - val_loss: 18.0105 - val_MinusLogProbMetric: 18.0105 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 90/1000
2023-09-27 11:23:34.495 
Epoch 90/1000 
	 loss: 17.7290, MinusLogProbMetric: 17.7290, val_loss: 17.9881, val_MinusLogProbMetric: 17.9881

Epoch 90: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.7290 - MinusLogProbMetric: 17.7290 - val_loss: 17.9881 - val_MinusLogProbMetric: 17.9881 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 91/1000
2023-09-27 11:24:51.168 
Epoch 91/1000 
	 loss: 17.6869, MinusLogProbMetric: 17.6869, val_loss: 18.1369, val_MinusLogProbMetric: 18.1369

Epoch 91: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.6869 - MinusLogProbMetric: 17.6869 - val_loss: 18.1369 - val_MinusLogProbMetric: 18.1369 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 92/1000
2023-09-27 11:26:08.064 
Epoch 92/1000 
	 loss: 17.7063, MinusLogProbMetric: 17.7063, val_loss: 18.2102, val_MinusLogProbMetric: 18.2102

Epoch 92: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.7063 - MinusLogProbMetric: 17.7063 - val_loss: 18.2102 - val_MinusLogProbMetric: 18.2102 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 93/1000
2023-09-27 11:27:24.710 
Epoch 93/1000 
	 loss: 17.7197, MinusLogProbMetric: 17.7197, val_loss: 18.1654, val_MinusLogProbMetric: 18.1654

Epoch 93: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.7197 - MinusLogProbMetric: 17.7197 - val_loss: 18.1654 - val_MinusLogProbMetric: 18.1654 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 94/1000
2023-09-27 11:28:41.888 
Epoch 94/1000 
	 loss: 17.7146, MinusLogProbMetric: 17.7146, val_loss: 18.2264, val_MinusLogProbMetric: 18.2264

Epoch 94: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.7146 - MinusLogProbMetric: 17.7146 - val_loss: 18.2264 - val_MinusLogProbMetric: 18.2264 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 95/1000
2023-09-27 11:29:59.102 
Epoch 95/1000 
	 loss: 17.6353, MinusLogProbMetric: 17.6353, val_loss: 18.7134, val_MinusLogProbMetric: 18.7134

Epoch 95: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.6353 - MinusLogProbMetric: 17.6353 - val_loss: 18.7134 - val_MinusLogProbMetric: 18.7134 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 96/1000
2023-09-27 11:31:17.507 
Epoch 96/1000 
	 loss: 17.6480, MinusLogProbMetric: 17.6480, val_loss: 18.4520, val_MinusLogProbMetric: 18.4520

Epoch 96: val_loss did not improve from 17.87524
196/196 - 78s - loss: 17.6480 - MinusLogProbMetric: 17.6480 - val_loss: 18.4520 - val_MinusLogProbMetric: 18.4520 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 97/1000
2023-09-27 11:32:34.116 
Epoch 97/1000 
	 loss: 17.7068, MinusLogProbMetric: 17.7068, val_loss: 18.0225, val_MinusLogProbMetric: 18.0225

Epoch 97: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.7068 - MinusLogProbMetric: 17.7068 - val_loss: 18.0225 - val_MinusLogProbMetric: 18.0225 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 98/1000
2023-09-27 11:33:50.482 
Epoch 98/1000 
	 loss: 17.6246, MinusLogProbMetric: 17.6246, val_loss: 18.2513, val_MinusLogProbMetric: 18.2513

Epoch 98: val_loss did not improve from 17.87524
196/196 - 76s - loss: 17.6246 - MinusLogProbMetric: 17.6246 - val_loss: 18.2513 - val_MinusLogProbMetric: 18.2513 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 99/1000
2023-09-27 11:35:07.005 
Epoch 99/1000 
	 loss: 17.6125, MinusLogProbMetric: 17.6125, val_loss: 18.1843, val_MinusLogProbMetric: 18.1843

Epoch 99: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.6125 - MinusLogProbMetric: 17.6125 - val_loss: 18.1843 - val_MinusLogProbMetric: 18.1843 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 100/1000
2023-09-27 11:36:23.384 
Epoch 100/1000 
	 loss: 17.6685, MinusLogProbMetric: 17.6685, val_loss: 18.1300, val_MinusLogProbMetric: 18.1300

Epoch 100: val_loss did not improve from 17.87524
196/196 - 76s - loss: 17.6685 - MinusLogProbMetric: 17.6685 - val_loss: 18.1300 - val_MinusLogProbMetric: 18.1300 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 101/1000
2023-09-27 11:37:39.838 
Epoch 101/1000 
	 loss: 17.5734, MinusLogProbMetric: 17.5734, val_loss: 18.2537, val_MinusLogProbMetric: 18.2537

Epoch 101: val_loss did not improve from 17.87524
196/196 - 76s - loss: 17.5734 - MinusLogProbMetric: 17.5734 - val_loss: 18.2537 - val_MinusLogProbMetric: 18.2537 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 102/1000
2023-09-27 11:38:56.536 
Epoch 102/1000 
	 loss: 17.6997, MinusLogProbMetric: 17.6997, val_loss: 17.9198, val_MinusLogProbMetric: 17.9198

Epoch 102: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.6997 - MinusLogProbMetric: 17.6997 - val_loss: 17.9198 - val_MinusLogProbMetric: 17.9198 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 103/1000
2023-09-27 11:40:13.919 
Epoch 103/1000 
	 loss: 17.6079, MinusLogProbMetric: 17.6079, val_loss: 18.2846, val_MinusLogProbMetric: 18.2846

Epoch 103: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.6079 - MinusLogProbMetric: 17.6079 - val_loss: 18.2846 - val_MinusLogProbMetric: 18.2846 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 104/1000
2023-09-27 11:41:30.252 
Epoch 104/1000 
	 loss: 17.7002, MinusLogProbMetric: 17.7002, val_loss: 17.8846, val_MinusLogProbMetric: 17.8846

Epoch 104: val_loss did not improve from 17.87524
196/196 - 76s - loss: 17.7002 - MinusLogProbMetric: 17.7002 - val_loss: 17.8846 - val_MinusLogProbMetric: 17.8846 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 105/1000
2023-09-27 11:42:46.701 
Epoch 105/1000 
	 loss: 17.6089, MinusLogProbMetric: 17.6089, val_loss: 18.0423, val_MinusLogProbMetric: 18.0423

Epoch 105: val_loss did not improve from 17.87524
196/196 - 76s - loss: 17.6089 - MinusLogProbMetric: 17.6089 - val_loss: 18.0423 - val_MinusLogProbMetric: 18.0423 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 106/1000
2023-09-27 11:44:04.008 
Epoch 106/1000 
	 loss: 17.6282, MinusLogProbMetric: 17.6282, val_loss: 18.1635, val_MinusLogProbMetric: 18.1635

Epoch 106: val_loss did not improve from 17.87524
196/196 - 77s - loss: 17.6282 - MinusLogProbMetric: 17.6282 - val_loss: 18.1635 - val_MinusLogProbMetric: 18.1635 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 107/1000
2023-09-27 11:45:20.501 
Epoch 107/1000 
	 loss: 17.5791, MinusLogProbMetric: 17.5791, val_loss: 18.2547, val_MinusLogProbMetric: 18.2547

Epoch 107: val_loss did not improve from 17.87524
196/196 - 76s - loss: 17.5791 - MinusLogProbMetric: 17.5791 - val_loss: 18.2547 - val_MinusLogProbMetric: 18.2547 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 108/1000
2023-09-27 11:46:36.828 
Epoch 108/1000 
	 loss: 17.5505, MinusLogProbMetric: 17.5505, val_loss: 18.3905, val_MinusLogProbMetric: 18.3905

Epoch 108: val_loss did not improve from 17.87524
196/196 - 76s - loss: 17.5505 - MinusLogProbMetric: 17.5505 - val_loss: 18.3905 - val_MinusLogProbMetric: 18.3905 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 109/1000
2023-09-27 11:47:53.306 
Epoch 109/1000 
	 loss: 17.5654, MinusLogProbMetric: 17.5654, val_loss: 17.7948, val_MinusLogProbMetric: 17.7948

Epoch 109: val_loss improved from 17.87524 to 17.79477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.5654 - MinusLogProbMetric: 17.5654 - val_loss: 17.7948 - val_MinusLogProbMetric: 17.7948 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 110/1000
2023-09-27 11:49:11.658 
Epoch 110/1000 
	 loss: 17.5805, MinusLogProbMetric: 17.5805, val_loss: 18.2876, val_MinusLogProbMetric: 18.2876

Epoch 110: val_loss did not improve from 17.79477
196/196 - 77s - loss: 17.5805 - MinusLogProbMetric: 17.5805 - val_loss: 18.2876 - val_MinusLogProbMetric: 18.2876 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 111/1000
2023-09-27 11:50:28.991 
Epoch 111/1000 
	 loss: 17.5473, MinusLogProbMetric: 17.5473, val_loss: 18.1668, val_MinusLogProbMetric: 18.1668

Epoch 111: val_loss did not improve from 17.79477
196/196 - 77s - loss: 17.5473 - MinusLogProbMetric: 17.5473 - val_loss: 18.1668 - val_MinusLogProbMetric: 18.1668 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 112/1000
2023-09-27 11:51:46.388 
Epoch 112/1000 
	 loss: 17.6197, MinusLogProbMetric: 17.6197, val_loss: 17.8658, val_MinusLogProbMetric: 17.8658

Epoch 112: val_loss did not improve from 17.79477
196/196 - 77s - loss: 17.6197 - MinusLogProbMetric: 17.6197 - val_loss: 17.8658 - val_MinusLogProbMetric: 17.8658 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 113/1000
2023-09-27 11:53:03.760 
Epoch 113/1000 
	 loss: 17.5003, MinusLogProbMetric: 17.5003, val_loss: 17.7563, val_MinusLogProbMetric: 17.7563

Epoch 113: val_loss improved from 17.79477 to 17.75629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 79s - loss: 17.5003 - MinusLogProbMetric: 17.5003 - val_loss: 17.7563 - val_MinusLogProbMetric: 17.7563 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 114/1000
2023-09-27 11:54:22.099 
Epoch 114/1000 
	 loss: 17.5322, MinusLogProbMetric: 17.5322, val_loss: 18.2925, val_MinusLogProbMetric: 18.2925

Epoch 114: val_loss did not improve from 17.75629
196/196 - 77s - loss: 17.5322 - MinusLogProbMetric: 17.5322 - val_loss: 18.2925 - val_MinusLogProbMetric: 18.2925 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 115/1000
2023-09-27 11:55:38.595 
Epoch 115/1000 
	 loss: 17.4922, MinusLogProbMetric: 17.4922, val_loss: 18.2668, val_MinusLogProbMetric: 18.2668

Epoch 115: val_loss did not improve from 17.75629
196/196 - 76s - loss: 17.4922 - MinusLogProbMetric: 17.4922 - val_loss: 18.2668 - val_MinusLogProbMetric: 18.2668 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 116/1000
2023-09-27 11:56:55.635 
Epoch 116/1000 
	 loss: 17.5000, MinusLogProbMetric: 17.5000, val_loss: 17.8149, val_MinusLogProbMetric: 17.8149

Epoch 116: val_loss did not improve from 17.75629
196/196 - 77s - loss: 17.5000 - MinusLogProbMetric: 17.5000 - val_loss: 17.8149 - val_MinusLogProbMetric: 17.8149 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 117/1000
2023-09-27 11:58:12.515 
Epoch 117/1000 
	 loss: 17.5770, MinusLogProbMetric: 17.5770, val_loss: 18.2525, val_MinusLogProbMetric: 18.2525

Epoch 117: val_loss did not improve from 17.75629
196/196 - 77s - loss: 17.5770 - MinusLogProbMetric: 17.5770 - val_loss: 18.2525 - val_MinusLogProbMetric: 18.2525 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 118/1000
2023-09-27 11:59:29.401 
Epoch 118/1000 
	 loss: 17.5187, MinusLogProbMetric: 17.5187, val_loss: 17.8168, val_MinusLogProbMetric: 17.8168

Epoch 118: val_loss did not improve from 17.75629
196/196 - 77s - loss: 17.5187 - MinusLogProbMetric: 17.5187 - val_loss: 17.8168 - val_MinusLogProbMetric: 17.8168 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 119/1000
2023-09-27 12:00:45.961 
Epoch 119/1000 
	 loss: 17.4645, MinusLogProbMetric: 17.4645, val_loss: 17.9499, val_MinusLogProbMetric: 17.9499

Epoch 119: val_loss did not improve from 17.75629
196/196 - 77s - loss: 17.4645 - MinusLogProbMetric: 17.4645 - val_loss: 17.9499 - val_MinusLogProbMetric: 17.9499 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 120/1000
2023-09-27 12:02:03.145 
Epoch 120/1000 
	 loss: 17.4955, MinusLogProbMetric: 17.4955, val_loss: 17.8987, val_MinusLogProbMetric: 17.8987

Epoch 120: val_loss did not improve from 17.75629
196/196 - 77s - loss: 17.4955 - MinusLogProbMetric: 17.4955 - val_loss: 17.8987 - val_MinusLogProbMetric: 17.8987 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 121/1000
2023-09-27 12:03:19.525 
Epoch 121/1000 
	 loss: 17.4894, MinusLogProbMetric: 17.4894, val_loss: 17.7093, val_MinusLogProbMetric: 17.7093

Epoch 121: val_loss improved from 17.75629 to 17.70926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.4894 - MinusLogProbMetric: 17.4894 - val_loss: 17.7093 - val_MinusLogProbMetric: 17.7093 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 122/1000
2023-09-27 12:04:37.441 
Epoch 122/1000 
	 loss: 17.4550, MinusLogProbMetric: 17.4550, val_loss: 17.9137, val_MinusLogProbMetric: 17.9137

Epoch 122: val_loss did not improve from 17.70926
196/196 - 76s - loss: 17.4550 - MinusLogProbMetric: 17.4550 - val_loss: 17.9137 - val_MinusLogProbMetric: 17.9137 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 123/1000
2023-09-27 12:05:54.044 
Epoch 123/1000 
	 loss: 17.5268, MinusLogProbMetric: 17.5268, val_loss: 17.7518, val_MinusLogProbMetric: 17.7518

Epoch 123: val_loss did not improve from 17.70926
196/196 - 77s - loss: 17.5268 - MinusLogProbMetric: 17.5268 - val_loss: 17.7518 - val_MinusLogProbMetric: 17.7518 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 124/1000
2023-09-27 12:07:10.909 
Epoch 124/1000 
	 loss: 17.4953, MinusLogProbMetric: 17.4953, val_loss: 17.7398, val_MinusLogProbMetric: 17.7398

Epoch 124: val_loss did not improve from 17.70926
196/196 - 77s - loss: 17.4953 - MinusLogProbMetric: 17.4953 - val_loss: 17.7398 - val_MinusLogProbMetric: 17.7398 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 125/1000
2023-09-27 12:08:27.471 
Epoch 125/1000 
	 loss: 17.4253, MinusLogProbMetric: 17.4253, val_loss: 17.8266, val_MinusLogProbMetric: 17.8266

Epoch 125: val_loss did not improve from 17.70926
196/196 - 77s - loss: 17.4253 - MinusLogProbMetric: 17.4253 - val_loss: 17.8266 - val_MinusLogProbMetric: 17.8266 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 126/1000
2023-09-27 12:09:44.098 
Epoch 126/1000 
	 loss: 17.4414, MinusLogProbMetric: 17.4414, val_loss: 18.0925, val_MinusLogProbMetric: 18.0925

Epoch 126: val_loss did not improve from 17.70926
196/196 - 77s - loss: 17.4414 - MinusLogProbMetric: 17.4414 - val_loss: 18.0925 - val_MinusLogProbMetric: 18.0925 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 127/1000
2023-09-27 12:11:00.826 
Epoch 127/1000 
	 loss: 17.4445, MinusLogProbMetric: 17.4445, val_loss: 17.8837, val_MinusLogProbMetric: 17.8837

Epoch 127: val_loss did not improve from 17.70926
196/196 - 77s - loss: 17.4445 - MinusLogProbMetric: 17.4445 - val_loss: 17.8837 - val_MinusLogProbMetric: 17.8837 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 128/1000
2023-09-27 12:12:17.543 
Epoch 128/1000 
	 loss: 17.4319, MinusLogProbMetric: 17.4319, val_loss: 18.1424, val_MinusLogProbMetric: 18.1424

Epoch 128: val_loss did not improve from 17.70926
196/196 - 77s - loss: 17.4319 - MinusLogProbMetric: 17.4319 - val_loss: 18.1424 - val_MinusLogProbMetric: 18.1424 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 129/1000
2023-09-27 12:13:34.502 
Epoch 129/1000 
	 loss: 17.4773, MinusLogProbMetric: 17.4773, val_loss: 17.7176, val_MinusLogProbMetric: 17.7176

Epoch 129: val_loss did not improve from 17.70926
196/196 - 77s - loss: 17.4773 - MinusLogProbMetric: 17.4773 - val_loss: 17.7176 - val_MinusLogProbMetric: 17.7176 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 130/1000
2023-09-27 12:14:50.347 
Epoch 130/1000 
	 loss: 17.4257, MinusLogProbMetric: 17.4257, val_loss: 17.9697, val_MinusLogProbMetric: 17.9697

Epoch 130: val_loss did not improve from 17.70926
196/196 - 76s - loss: 17.4257 - MinusLogProbMetric: 17.4257 - val_loss: 17.9697 - val_MinusLogProbMetric: 17.9697 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 131/1000
2023-09-27 12:16:06.500 
Epoch 131/1000 
	 loss: 17.4323, MinusLogProbMetric: 17.4323, val_loss: 18.0849, val_MinusLogProbMetric: 18.0849

Epoch 131: val_loss did not improve from 17.70926
196/196 - 76s - loss: 17.4323 - MinusLogProbMetric: 17.4323 - val_loss: 18.0849 - val_MinusLogProbMetric: 18.0849 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 132/1000
2023-09-27 12:17:23.439 
Epoch 132/1000 
	 loss: 17.4362, MinusLogProbMetric: 17.4362, val_loss: 17.8969, val_MinusLogProbMetric: 17.8969

Epoch 132: val_loss did not improve from 17.70926
196/196 - 77s - loss: 17.4362 - MinusLogProbMetric: 17.4362 - val_loss: 17.8969 - val_MinusLogProbMetric: 17.8969 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 133/1000
2023-09-27 12:18:39.451 
Epoch 133/1000 
	 loss: 17.4240, MinusLogProbMetric: 17.4240, val_loss: 17.6526, val_MinusLogProbMetric: 17.6526

Epoch 133: val_loss improved from 17.70926 to 17.65262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.4240 - MinusLogProbMetric: 17.4240 - val_loss: 17.6526 - val_MinusLogProbMetric: 17.6526 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 134/1000
2023-09-27 12:19:57.708 
Epoch 134/1000 
	 loss: 17.4499, MinusLogProbMetric: 17.4499, val_loss: 17.9110, val_MinusLogProbMetric: 17.9110

Epoch 134: val_loss did not improve from 17.65262
196/196 - 77s - loss: 17.4499 - MinusLogProbMetric: 17.4499 - val_loss: 17.9110 - val_MinusLogProbMetric: 17.9110 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 135/1000
2023-09-27 12:21:14.139 
Epoch 135/1000 
	 loss: 17.4107, MinusLogProbMetric: 17.4107, val_loss: 17.7749, val_MinusLogProbMetric: 17.7749

Epoch 135: val_loss did not improve from 17.65262
196/196 - 76s - loss: 17.4107 - MinusLogProbMetric: 17.4107 - val_loss: 17.7749 - val_MinusLogProbMetric: 17.7749 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 136/1000
2023-09-27 12:22:30.616 
Epoch 136/1000 
	 loss: 17.4213, MinusLogProbMetric: 17.4213, val_loss: 17.6038, val_MinusLogProbMetric: 17.6038

Epoch 136: val_loss improved from 17.65262 to 17.60376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.4213 - MinusLogProbMetric: 17.4213 - val_loss: 17.6038 - val_MinusLogProbMetric: 17.6038 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 137/1000
2023-09-27 12:23:49.361 
Epoch 137/1000 
	 loss: 17.3744, MinusLogProbMetric: 17.3744, val_loss: 17.8415, val_MinusLogProbMetric: 17.8415

Epoch 137: val_loss did not improve from 17.60376
196/196 - 77s - loss: 17.3744 - MinusLogProbMetric: 17.3744 - val_loss: 17.8415 - val_MinusLogProbMetric: 17.8415 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 138/1000
2023-09-27 12:25:05.230 
Epoch 138/1000 
	 loss: 17.4398, MinusLogProbMetric: 17.4398, val_loss: 17.7413, val_MinusLogProbMetric: 17.7413

Epoch 138: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.4398 - MinusLogProbMetric: 17.4398 - val_loss: 17.7413 - val_MinusLogProbMetric: 17.7413 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 139/1000
2023-09-27 12:26:21.615 
Epoch 139/1000 
	 loss: 17.4409, MinusLogProbMetric: 17.4409, val_loss: 17.8011, val_MinusLogProbMetric: 17.8011

Epoch 139: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.4409 - MinusLogProbMetric: 17.4409 - val_loss: 17.8011 - val_MinusLogProbMetric: 17.8011 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 140/1000
2023-09-27 12:27:38.351 
Epoch 140/1000 
	 loss: 17.4412, MinusLogProbMetric: 17.4412, val_loss: 17.8653, val_MinusLogProbMetric: 17.8653

Epoch 140: val_loss did not improve from 17.60376
196/196 - 77s - loss: 17.4412 - MinusLogProbMetric: 17.4412 - val_loss: 17.8653 - val_MinusLogProbMetric: 17.8653 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 141/1000
2023-09-27 12:28:54.688 
Epoch 141/1000 
	 loss: 17.3581, MinusLogProbMetric: 17.3581, val_loss: 17.6637, val_MinusLogProbMetric: 17.6637

Epoch 141: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3581 - MinusLogProbMetric: 17.3581 - val_loss: 17.6637 - val_MinusLogProbMetric: 17.6637 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 142/1000
2023-09-27 12:30:10.772 
Epoch 142/1000 
	 loss: 17.3914, MinusLogProbMetric: 17.3914, val_loss: 18.0898, val_MinusLogProbMetric: 18.0898

Epoch 142: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3914 - MinusLogProbMetric: 17.3914 - val_loss: 18.0898 - val_MinusLogProbMetric: 18.0898 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 143/1000
2023-09-27 12:31:27.462 
Epoch 143/1000 
	 loss: 17.3660, MinusLogProbMetric: 17.3660, val_loss: 17.8100, val_MinusLogProbMetric: 17.8100

Epoch 143: val_loss did not improve from 17.60376
196/196 - 77s - loss: 17.3660 - MinusLogProbMetric: 17.3660 - val_loss: 17.8100 - val_MinusLogProbMetric: 17.8100 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 144/1000
2023-09-27 12:32:43.268 
Epoch 144/1000 
	 loss: 17.3528, MinusLogProbMetric: 17.3528, val_loss: 17.8184, val_MinusLogProbMetric: 17.8184

Epoch 144: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3528 - MinusLogProbMetric: 17.3528 - val_loss: 17.8184 - val_MinusLogProbMetric: 17.8184 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 145/1000
2023-09-27 12:33:59.485 
Epoch 145/1000 
	 loss: 17.3273, MinusLogProbMetric: 17.3273, val_loss: 18.0120, val_MinusLogProbMetric: 18.0120

Epoch 145: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3273 - MinusLogProbMetric: 17.3273 - val_loss: 18.0120 - val_MinusLogProbMetric: 18.0120 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 146/1000
2023-09-27 12:35:16.700 
Epoch 146/1000 
	 loss: 17.3905, MinusLogProbMetric: 17.3905, val_loss: 18.4174, val_MinusLogProbMetric: 18.4174

Epoch 146: val_loss did not improve from 17.60376
196/196 - 77s - loss: 17.3905 - MinusLogProbMetric: 17.3905 - val_loss: 18.4174 - val_MinusLogProbMetric: 18.4174 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 147/1000
2023-09-27 12:36:33.352 
Epoch 147/1000 
	 loss: 17.4130, MinusLogProbMetric: 17.4130, val_loss: 18.0318, val_MinusLogProbMetric: 18.0318

Epoch 147: val_loss did not improve from 17.60376
196/196 - 77s - loss: 17.4130 - MinusLogProbMetric: 17.4130 - val_loss: 18.0318 - val_MinusLogProbMetric: 18.0318 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 148/1000
2023-09-27 12:37:49.782 
Epoch 148/1000 
	 loss: 17.3590, MinusLogProbMetric: 17.3590, val_loss: 17.8401, val_MinusLogProbMetric: 17.8401

Epoch 148: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3590 - MinusLogProbMetric: 17.3590 - val_loss: 17.8401 - val_MinusLogProbMetric: 17.8401 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 149/1000
2023-09-27 12:39:06.675 
Epoch 149/1000 
	 loss: 17.3383, MinusLogProbMetric: 17.3383, val_loss: 17.6573, val_MinusLogProbMetric: 17.6573

Epoch 149: val_loss did not improve from 17.60376
196/196 - 77s - loss: 17.3383 - MinusLogProbMetric: 17.3383 - val_loss: 17.6573 - val_MinusLogProbMetric: 17.6573 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 150/1000
2023-09-27 12:40:23.070 
Epoch 150/1000 
	 loss: 17.3690, MinusLogProbMetric: 17.3690, val_loss: 17.7309, val_MinusLogProbMetric: 17.7309

Epoch 150: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3690 - MinusLogProbMetric: 17.3690 - val_loss: 17.7309 - val_MinusLogProbMetric: 17.7309 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 151/1000
2023-09-27 12:41:39.448 
Epoch 151/1000 
	 loss: 17.3498, MinusLogProbMetric: 17.3498, val_loss: 17.7887, val_MinusLogProbMetric: 17.7887

Epoch 151: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3498 - MinusLogProbMetric: 17.3498 - val_loss: 17.7887 - val_MinusLogProbMetric: 17.7887 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 152/1000
2023-09-27 12:42:55.723 
Epoch 152/1000 
	 loss: 17.3473, MinusLogProbMetric: 17.3473, val_loss: 17.8753, val_MinusLogProbMetric: 17.8753

Epoch 152: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3473 - MinusLogProbMetric: 17.3473 - val_loss: 17.8753 - val_MinusLogProbMetric: 17.8753 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 153/1000
2023-09-27 12:44:12.487 
Epoch 153/1000 
	 loss: 17.3060, MinusLogProbMetric: 17.3060, val_loss: 17.7155, val_MinusLogProbMetric: 17.7155

Epoch 153: val_loss did not improve from 17.60376
196/196 - 77s - loss: 17.3060 - MinusLogProbMetric: 17.3060 - val_loss: 17.7155 - val_MinusLogProbMetric: 17.7155 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 154/1000
2023-09-27 12:45:28.963 
Epoch 154/1000 
	 loss: 17.3203, MinusLogProbMetric: 17.3203, val_loss: 18.0842, val_MinusLogProbMetric: 18.0842

Epoch 154: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3203 - MinusLogProbMetric: 17.3203 - val_loss: 18.0842 - val_MinusLogProbMetric: 18.0842 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 155/1000
2023-09-27 12:46:45.583 
Epoch 155/1000 
	 loss: 17.3243, MinusLogProbMetric: 17.3243, val_loss: 18.1727, val_MinusLogProbMetric: 18.1727

Epoch 155: val_loss did not improve from 17.60376
196/196 - 77s - loss: 17.3243 - MinusLogProbMetric: 17.3243 - val_loss: 18.1727 - val_MinusLogProbMetric: 18.1727 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 156/1000
2023-09-27 12:48:01.941 
Epoch 156/1000 
	 loss: 17.3254, MinusLogProbMetric: 17.3254, val_loss: 17.7575, val_MinusLogProbMetric: 17.7575

Epoch 156: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3254 - MinusLogProbMetric: 17.3254 - val_loss: 17.7575 - val_MinusLogProbMetric: 17.7575 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 157/1000
2023-09-27 12:49:18.005 
Epoch 157/1000 
	 loss: 17.3011, MinusLogProbMetric: 17.3011, val_loss: 18.0203, val_MinusLogProbMetric: 18.0203

Epoch 157: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3011 - MinusLogProbMetric: 17.3011 - val_loss: 18.0203 - val_MinusLogProbMetric: 18.0203 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 158/1000
2023-09-27 12:50:33.674 
Epoch 158/1000 
	 loss: 17.3019, MinusLogProbMetric: 17.3019, val_loss: 17.9526, val_MinusLogProbMetric: 17.9526

Epoch 158: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3019 - MinusLogProbMetric: 17.3019 - val_loss: 17.9526 - val_MinusLogProbMetric: 17.9526 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 159/1000
2023-09-27 12:51:45.256 
Epoch 159/1000 
	 loss: 17.2596, MinusLogProbMetric: 17.2596, val_loss: 17.7941, val_MinusLogProbMetric: 17.7941

Epoch 159: val_loss did not improve from 17.60376
196/196 - 72s - loss: 17.2596 - MinusLogProbMetric: 17.2596 - val_loss: 17.7941 - val_MinusLogProbMetric: 17.7941 - lr: 3.3333e-04 - 72s/epoch - 365ms/step
Epoch 160/1000
2023-09-27 12:53:01.190 
Epoch 160/1000 
	 loss: 17.3717, MinusLogProbMetric: 17.3717, val_loss: 17.8636, val_MinusLogProbMetric: 17.8636

Epoch 160: val_loss did not improve from 17.60376
196/196 - 76s - loss: 17.3717 - MinusLogProbMetric: 17.3717 - val_loss: 17.8636 - val_MinusLogProbMetric: 17.8636 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 161/1000
2023-09-27 12:54:18.230 
Epoch 161/1000 
	 loss: 17.2942, MinusLogProbMetric: 17.2942, val_loss: 17.5866, val_MinusLogProbMetric: 17.5866

Epoch 161: val_loss improved from 17.60376 to 17.58660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 79s - loss: 17.2942 - MinusLogProbMetric: 17.2942 - val_loss: 17.5866 - val_MinusLogProbMetric: 17.5866 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 162/1000
2023-09-27 12:55:36.553 
Epoch 162/1000 
	 loss: 17.2972, MinusLogProbMetric: 17.2972, val_loss: 17.7279, val_MinusLogProbMetric: 17.7279

Epoch 162: val_loss did not improve from 17.58660
196/196 - 77s - loss: 17.2972 - MinusLogProbMetric: 17.2972 - val_loss: 17.7279 - val_MinusLogProbMetric: 17.7279 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 163/1000
2023-09-27 12:56:52.903 
Epoch 163/1000 
	 loss: 17.2430, MinusLogProbMetric: 17.2430, val_loss: 17.8967, val_MinusLogProbMetric: 17.8967

Epoch 163: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.2430 - MinusLogProbMetric: 17.2430 - val_loss: 17.8967 - val_MinusLogProbMetric: 17.8967 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 164/1000
2023-09-27 12:58:09.358 
Epoch 164/1000 
	 loss: 17.2788, MinusLogProbMetric: 17.2788, val_loss: 18.1828, val_MinusLogProbMetric: 18.1828

Epoch 164: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.2788 - MinusLogProbMetric: 17.2788 - val_loss: 18.1828 - val_MinusLogProbMetric: 18.1828 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 165/1000
2023-09-27 12:59:25.741 
Epoch 165/1000 
	 loss: 17.3316, MinusLogProbMetric: 17.3316, val_loss: 17.8945, val_MinusLogProbMetric: 17.8945

Epoch 165: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.3316 - MinusLogProbMetric: 17.3316 - val_loss: 17.8945 - val_MinusLogProbMetric: 17.8945 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 166/1000
2023-09-27 13:00:41.969 
Epoch 166/1000 
	 loss: 17.2555, MinusLogProbMetric: 17.2555, val_loss: 18.0900, val_MinusLogProbMetric: 18.0900

Epoch 166: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.2555 - MinusLogProbMetric: 17.2555 - val_loss: 18.0900 - val_MinusLogProbMetric: 18.0900 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 167/1000
2023-09-27 13:01:58.009 
Epoch 167/1000 
	 loss: 17.2821, MinusLogProbMetric: 17.2821, val_loss: 17.7626, val_MinusLogProbMetric: 17.7626

Epoch 167: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.2821 - MinusLogProbMetric: 17.2821 - val_loss: 17.7626 - val_MinusLogProbMetric: 17.7626 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 168/1000
2023-09-27 13:03:14.085 
Epoch 168/1000 
	 loss: 17.2971, MinusLogProbMetric: 17.2971, val_loss: 17.7878, val_MinusLogProbMetric: 17.7878

Epoch 168: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.2971 - MinusLogProbMetric: 17.2971 - val_loss: 17.7878 - val_MinusLogProbMetric: 17.7878 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 169/1000
2023-09-27 13:04:31.337 
Epoch 169/1000 
	 loss: 17.2229, MinusLogProbMetric: 17.2229, val_loss: 18.0297, val_MinusLogProbMetric: 18.0297

Epoch 169: val_loss did not improve from 17.58660
196/196 - 77s - loss: 17.2229 - MinusLogProbMetric: 17.2229 - val_loss: 18.0297 - val_MinusLogProbMetric: 18.0297 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 170/1000
2023-09-27 13:05:49.873 
Epoch 170/1000 
	 loss: 17.2877, MinusLogProbMetric: 17.2877, val_loss: 17.8323, val_MinusLogProbMetric: 17.8323

Epoch 170: val_loss did not improve from 17.58660
196/196 - 79s - loss: 17.2877 - MinusLogProbMetric: 17.2877 - val_loss: 17.8323 - val_MinusLogProbMetric: 17.8323 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 171/1000
2023-09-27 13:07:08.964 
Epoch 171/1000 
	 loss: 17.2342, MinusLogProbMetric: 17.2342, val_loss: 17.9056, val_MinusLogProbMetric: 17.9056

Epoch 171: val_loss did not improve from 17.58660
196/196 - 79s - loss: 17.2342 - MinusLogProbMetric: 17.2342 - val_loss: 17.9056 - val_MinusLogProbMetric: 17.9056 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 172/1000
2023-09-27 13:08:25.991 
Epoch 172/1000 
	 loss: 17.2357, MinusLogProbMetric: 17.2357, val_loss: 17.7904, val_MinusLogProbMetric: 17.7904

Epoch 172: val_loss did not improve from 17.58660
196/196 - 77s - loss: 17.2357 - MinusLogProbMetric: 17.2357 - val_loss: 17.7904 - val_MinusLogProbMetric: 17.7904 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 173/1000
2023-09-27 13:09:42.065 
Epoch 173/1000 
	 loss: 17.3061, MinusLogProbMetric: 17.3061, val_loss: 18.0427, val_MinusLogProbMetric: 18.0427

Epoch 173: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.3061 - MinusLogProbMetric: 17.3061 - val_loss: 18.0427 - val_MinusLogProbMetric: 18.0427 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 174/1000
2023-09-27 13:10:58.329 
Epoch 174/1000 
	 loss: 17.2363, MinusLogProbMetric: 17.2363, val_loss: 18.0176, val_MinusLogProbMetric: 18.0176

Epoch 174: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.2363 - MinusLogProbMetric: 17.2363 - val_loss: 18.0176 - val_MinusLogProbMetric: 18.0176 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 175/1000
2023-09-27 13:12:14.742 
Epoch 175/1000 
	 loss: 17.3083, MinusLogProbMetric: 17.3083, val_loss: 17.7479, val_MinusLogProbMetric: 17.7479

Epoch 175: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.3083 - MinusLogProbMetric: 17.3083 - val_loss: 17.7479 - val_MinusLogProbMetric: 17.7479 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 176/1000
2023-09-27 13:13:31.548 
Epoch 176/1000 
	 loss: 17.2674, MinusLogProbMetric: 17.2674, val_loss: 18.0428, val_MinusLogProbMetric: 18.0428

Epoch 176: val_loss did not improve from 17.58660
196/196 - 77s - loss: 17.2674 - MinusLogProbMetric: 17.2674 - val_loss: 18.0428 - val_MinusLogProbMetric: 18.0428 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 177/1000
2023-09-27 13:14:47.296 
Epoch 177/1000 
	 loss: 17.2503, MinusLogProbMetric: 17.2503, val_loss: 17.8784, val_MinusLogProbMetric: 17.8784

Epoch 177: val_loss did not improve from 17.58660
196/196 - 76s - loss: 17.2503 - MinusLogProbMetric: 17.2503 - val_loss: 17.8784 - val_MinusLogProbMetric: 17.8784 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 178/1000
2023-09-27 13:16:03.521 
Epoch 178/1000 
	 loss: 17.2214, MinusLogProbMetric: 17.2214, val_loss: 17.5538, val_MinusLogProbMetric: 17.5538

Epoch 178: val_loss improved from 17.58660 to 17.55378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.2214 - MinusLogProbMetric: 17.2214 - val_loss: 17.5538 - val_MinusLogProbMetric: 17.5538 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 179/1000
2023-09-27 13:17:21.274 
Epoch 179/1000 
	 loss: 17.3003, MinusLogProbMetric: 17.3003, val_loss: 17.8040, val_MinusLogProbMetric: 17.8040

Epoch 179: val_loss did not improve from 17.55378
196/196 - 76s - loss: 17.3003 - MinusLogProbMetric: 17.3003 - val_loss: 17.8040 - val_MinusLogProbMetric: 17.8040 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 180/1000
2023-09-27 13:18:36.792 
Epoch 180/1000 
	 loss: 17.2757, MinusLogProbMetric: 17.2757, val_loss: 17.6009, val_MinusLogProbMetric: 17.6009

Epoch 180: val_loss did not improve from 17.55378
196/196 - 76s - loss: 17.2757 - MinusLogProbMetric: 17.2757 - val_loss: 17.6009 - val_MinusLogProbMetric: 17.6009 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 181/1000
2023-09-27 13:19:52.703 
Epoch 181/1000 
	 loss: 17.1927, MinusLogProbMetric: 17.1927, val_loss: 17.8856, val_MinusLogProbMetric: 17.8856

Epoch 181: val_loss did not improve from 17.55378
196/196 - 76s - loss: 17.1927 - MinusLogProbMetric: 17.1927 - val_loss: 17.8856 - val_MinusLogProbMetric: 17.8856 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 182/1000
2023-09-27 13:21:08.974 
Epoch 182/1000 
	 loss: 17.2421, MinusLogProbMetric: 17.2421, val_loss: 17.6559, val_MinusLogProbMetric: 17.6559

Epoch 182: val_loss did not improve from 17.55378
196/196 - 76s - loss: 17.2421 - MinusLogProbMetric: 17.2421 - val_loss: 17.6559 - val_MinusLogProbMetric: 17.6559 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 183/1000
2023-09-27 13:22:25.300 
Epoch 183/1000 
	 loss: 17.2192, MinusLogProbMetric: 17.2192, val_loss: 17.8070, val_MinusLogProbMetric: 17.8070

Epoch 183: val_loss did not improve from 17.55378
196/196 - 76s - loss: 17.2192 - MinusLogProbMetric: 17.2192 - val_loss: 17.8070 - val_MinusLogProbMetric: 17.8070 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 184/1000
2023-09-27 13:23:41.998 
Epoch 184/1000 
	 loss: 17.2747, MinusLogProbMetric: 17.2747, val_loss: 17.7000, val_MinusLogProbMetric: 17.7000

Epoch 184: val_loss did not improve from 17.55378
196/196 - 77s - loss: 17.2747 - MinusLogProbMetric: 17.2747 - val_loss: 17.7000 - val_MinusLogProbMetric: 17.7000 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 185/1000
2023-09-27 13:24:58.236 
Epoch 185/1000 
	 loss: 17.1869, MinusLogProbMetric: 17.1869, val_loss: 17.6217, val_MinusLogProbMetric: 17.6217

Epoch 185: val_loss did not improve from 17.55378
196/196 - 76s - loss: 17.1869 - MinusLogProbMetric: 17.1869 - val_loss: 17.6217 - val_MinusLogProbMetric: 17.6217 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 186/1000
2023-09-27 13:26:14.312 
Epoch 186/1000 
	 loss: 17.2549, MinusLogProbMetric: 17.2549, val_loss: 17.8258, val_MinusLogProbMetric: 17.8258

Epoch 186: val_loss did not improve from 17.55378
196/196 - 76s - loss: 17.2549 - MinusLogProbMetric: 17.2549 - val_loss: 17.8258 - val_MinusLogProbMetric: 17.8258 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 187/1000
2023-09-27 13:27:30.739 
Epoch 187/1000 
	 loss: 17.1981, MinusLogProbMetric: 17.1981, val_loss: 17.5198, val_MinusLogProbMetric: 17.5198

Epoch 187: val_loss improved from 17.55378 to 17.51980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.1981 - MinusLogProbMetric: 17.1981 - val_loss: 17.5198 - val_MinusLogProbMetric: 17.5198 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 188/1000
2023-09-27 13:28:48.031 
Epoch 188/1000 
	 loss: 17.2426, MinusLogProbMetric: 17.2426, val_loss: 17.7356, val_MinusLogProbMetric: 17.7356

Epoch 188: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.2426 - MinusLogProbMetric: 17.2426 - val_loss: 17.7356 - val_MinusLogProbMetric: 17.7356 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 189/1000
2023-09-27 13:30:04.454 
Epoch 189/1000 
	 loss: 17.1612, MinusLogProbMetric: 17.1612, val_loss: 17.5719, val_MinusLogProbMetric: 17.5719

Epoch 189: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.1612 - MinusLogProbMetric: 17.1612 - val_loss: 17.5719 - val_MinusLogProbMetric: 17.5719 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 190/1000
2023-09-27 13:31:20.744 
Epoch 190/1000 
	 loss: 17.1981, MinusLogProbMetric: 17.1981, val_loss: 17.8179, val_MinusLogProbMetric: 17.8179

Epoch 190: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.1981 - MinusLogProbMetric: 17.1981 - val_loss: 17.8179 - val_MinusLogProbMetric: 17.8179 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 191/1000
2023-09-27 13:32:36.836 
Epoch 191/1000 
	 loss: 17.1691, MinusLogProbMetric: 17.1691, val_loss: 18.1861, val_MinusLogProbMetric: 18.1861

Epoch 191: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.1691 - MinusLogProbMetric: 17.1691 - val_loss: 18.1861 - val_MinusLogProbMetric: 18.1861 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 192/1000
2023-09-27 13:33:53.308 
Epoch 192/1000 
	 loss: 17.2390, MinusLogProbMetric: 17.2390, val_loss: 17.9309, val_MinusLogProbMetric: 17.9309

Epoch 192: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.2390 - MinusLogProbMetric: 17.2390 - val_loss: 17.9309 - val_MinusLogProbMetric: 17.9309 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 193/1000
2023-09-27 13:35:09.492 
Epoch 193/1000 
	 loss: 17.2121, MinusLogProbMetric: 17.2121, val_loss: 17.6630, val_MinusLogProbMetric: 17.6630

Epoch 193: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.2121 - MinusLogProbMetric: 17.2121 - val_loss: 17.6630 - val_MinusLogProbMetric: 17.6630 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 194/1000
2023-09-27 13:36:25.966 
Epoch 194/1000 
	 loss: 17.1864, MinusLogProbMetric: 17.1864, val_loss: 18.2845, val_MinusLogProbMetric: 18.2845

Epoch 194: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.1864 - MinusLogProbMetric: 17.1864 - val_loss: 18.2845 - val_MinusLogProbMetric: 18.2845 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 195/1000
2023-09-27 13:37:42.241 
Epoch 195/1000 
	 loss: 17.1770, MinusLogProbMetric: 17.1770, val_loss: 17.8792, val_MinusLogProbMetric: 17.8792

Epoch 195: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.1770 - MinusLogProbMetric: 17.1770 - val_loss: 17.8792 - val_MinusLogProbMetric: 17.8792 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 196/1000
2023-09-27 13:38:58.094 
Epoch 196/1000 
	 loss: 17.2013, MinusLogProbMetric: 17.2013, val_loss: 17.7141, val_MinusLogProbMetric: 17.7141

Epoch 196: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.2013 - MinusLogProbMetric: 17.2013 - val_loss: 17.7141 - val_MinusLogProbMetric: 17.7141 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 197/1000
2023-09-27 13:40:15.287 
Epoch 197/1000 
	 loss: 17.2050, MinusLogProbMetric: 17.2050, val_loss: 17.8329, val_MinusLogProbMetric: 17.8329

Epoch 197: val_loss did not improve from 17.51980
196/196 - 77s - loss: 17.2050 - MinusLogProbMetric: 17.2050 - val_loss: 17.8329 - val_MinusLogProbMetric: 17.8329 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 198/1000
2023-09-27 13:41:32.241 
Epoch 198/1000 
	 loss: 17.1757, MinusLogProbMetric: 17.1757, val_loss: 17.5240, val_MinusLogProbMetric: 17.5240

Epoch 198: val_loss did not improve from 17.51980
196/196 - 77s - loss: 17.1757 - MinusLogProbMetric: 17.1757 - val_loss: 17.5240 - val_MinusLogProbMetric: 17.5240 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 199/1000
2023-09-27 13:42:48.356 
Epoch 199/1000 
	 loss: 17.1754, MinusLogProbMetric: 17.1754, val_loss: 17.8047, val_MinusLogProbMetric: 17.8047

Epoch 199: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.1754 - MinusLogProbMetric: 17.1754 - val_loss: 17.8047 - val_MinusLogProbMetric: 17.8047 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 200/1000
2023-09-27 13:44:05.045 
Epoch 200/1000 
	 loss: 17.1618, MinusLogProbMetric: 17.1618, val_loss: 17.6505, val_MinusLogProbMetric: 17.6505

Epoch 200: val_loss did not improve from 17.51980
196/196 - 77s - loss: 17.1618 - MinusLogProbMetric: 17.1618 - val_loss: 17.6505 - val_MinusLogProbMetric: 17.6505 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 201/1000
2023-09-27 13:45:14.504 
Epoch 201/1000 
	 loss: 17.1486, MinusLogProbMetric: 17.1486, val_loss: 17.8701, val_MinusLogProbMetric: 17.8701

Epoch 201: val_loss did not improve from 17.51980
196/196 - 69s - loss: 17.1486 - MinusLogProbMetric: 17.1486 - val_loss: 17.8701 - val_MinusLogProbMetric: 17.8701 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 202/1000
2023-09-27 13:46:22.689 
Epoch 202/1000 
	 loss: 17.1757, MinusLogProbMetric: 17.1757, val_loss: 17.7564, val_MinusLogProbMetric: 17.7564

Epoch 202: val_loss did not improve from 17.51980
196/196 - 68s - loss: 17.1757 - MinusLogProbMetric: 17.1757 - val_loss: 17.7564 - val_MinusLogProbMetric: 17.7564 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 203/1000
2023-09-27 13:47:37.965 
Epoch 203/1000 
	 loss: 17.1440, MinusLogProbMetric: 17.1440, val_loss: 17.6118, val_MinusLogProbMetric: 17.6118

Epoch 203: val_loss did not improve from 17.51980
196/196 - 75s - loss: 17.1440 - MinusLogProbMetric: 17.1440 - val_loss: 17.6118 - val_MinusLogProbMetric: 17.6118 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 204/1000
2023-09-27 13:48:53.664 
Epoch 204/1000 
	 loss: 17.1592, MinusLogProbMetric: 17.1592, val_loss: 17.6697, val_MinusLogProbMetric: 17.6697

Epoch 204: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.1592 - MinusLogProbMetric: 17.1592 - val_loss: 17.6697 - val_MinusLogProbMetric: 17.6697 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 205/1000
2023-09-27 13:50:12.114 
Epoch 205/1000 
	 loss: 17.1026, MinusLogProbMetric: 17.1026, val_loss: 18.2455, val_MinusLogProbMetric: 18.2455

Epoch 205: val_loss did not improve from 17.51980
196/196 - 78s - loss: 17.1026 - MinusLogProbMetric: 17.1026 - val_loss: 18.2455 - val_MinusLogProbMetric: 18.2455 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 206/1000
2023-09-27 13:51:28.937 
Epoch 206/1000 
	 loss: 17.1639, MinusLogProbMetric: 17.1639, val_loss: 17.5232, val_MinusLogProbMetric: 17.5232

Epoch 206: val_loss did not improve from 17.51980
196/196 - 77s - loss: 17.1639 - MinusLogProbMetric: 17.1639 - val_loss: 17.5232 - val_MinusLogProbMetric: 17.5232 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 207/1000
2023-09-27 13:52:46.677 
Epoch 207/1000 
	 loss: 17.1783, MinusLogProbMetric: 17.1783, val_loss: 17.7599, val_MinusLogProbMetric: 17.7599

Epoch 207: val_loss did not improve from 17.51980
196/196 - 78s - loss: 17.1783 - MinusLogProbMetric: 17.1783 - val_loss: 17.7599 - val_MinusLogProbMetric: 17.7599 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 208/1000
2023-09-27 13:54:03.942 
Epoch 208/1000 
	 loss: 17.1326, MinusLogProbMetric: 17.1326, val_loss: 17.7042, val_MinusLogProbMetric: 17.7042

Epoch 208: val_loss did not improve from 17.51980
196/196 - 77s - loss: 17.1326 - MinusLogProbMetric: 17.1326 - val_loss: 17.7042 - val_MinusLogProbMetric: 17.7042 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 209/1000
2023-09-27 13:55:20.760 
Epoch 209/1000 
	 loss: 17.1702, MinusLogProbMetric: 17.1702, val_loss: 17.6078, val_MinusLogProbMetric: 17.6078

Epoch 209: val_loss did not improve from 17.51980
196/196 - 77s - loss: 17.1702 - MinusLogProbMetric: 17.1702 - val_loss: 17.6078 - val_MinusLogProbMetric: 17.6078 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 210/1000
2023-09-27 13:56:36.945 
Epoch 210/1000 
	 loss: 17.1253, MinusLogProbMetric: 17.1253, val_loss: 17.7635, val_MinusLogProbMetric: 17.7635

Epoch 210: val_loss did not improve from 17.51980
196/196 - 76s - loss: 17.1253 - MinusLogProbMetric: 17.1253 - val_loss: 17.7635 - val_MinusLogProbMetric: 17.7635 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 211/1000
2023-09-27 13:57:53.344 
Epoch 211/1000 
	 loss: 17.1327, MinusLogProbMetric: 17.1327, val_loss: 17.4795, val_MinusLogProbMetric: 17.4795

Epoch 211: val_loss improved from 17.51980 to 17.47947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 17.1327 - MinusLogProbMetric: 17.1327 - val_loss: 17.4795 - val_MinusLogProbMetric: 17.4795 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 212/1000
2023-09-27 13:59:11.117 
Epoch 212/1000 
	 loss: 17.1057, MinusLogProbMetric: 17.1057, val_loss: 17.9683, val_MinusLogProbMetric: 17.9683

Epoch 212: val_loss did not improve from 17.47947
196/196 - 76s - loss: 17.1057 - MinusLogProbMetric: 17.1057 - val_loss: 17.9683 - val_MinusLogProbMetric: 17.9683 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 213/1000
2023-09-27 14:00:27.661 
Epoch 213/1000 
	 loss: 17.1139, MinusLogProbMetric: 17.1139, val_loss: 17.7518, val_MinusLogProbMetric: 17.7518

Epoch 213: val_loss did not improve from 17.47947
196/196 - 77s - loss: 17.1139 - MinusLogProbMetric: 17.1139 - val_loss: 17.7518 - val_MinusLogProbMetric: 17.7518 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 214/1000
2023-09-27 14:01:44.065 
Epoch 214/1000 
	 loss: 17.1450, MinusLogProbMetric: 17.1450, val_loss: 17.6947, val_MinusLogProbMetric: 17.6947

Epoch 214: val_loss did not improve from 17.47947
196/196 - 76s - loss: 17.1450 - MinusLogProbMetric: 17.1450 - val_loss: 17.6947 - val_MinusLogProbMetric: 17.6947 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 215/1000
2023-09-27 14:03:00.626 
Epoch 215/1000 
	 loss: 17.1196, MinusLogProbMetric: 17.1196, val_loss: 18.2751, val_MinusLogProbMetric: 18.2751

Epoch 215: val_loss did not improve from 17.47947
196/196 - 77s - loss: 17.1196 - MinusLogProbMetric: 17.1196 - val_loss: 18.2751 - val_MinusLogProbMetric: 18.2751 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 216/1000
2023-09-27 14:04:17.022 
Epoch 216/1000 
	 loss: 17.1314, MinusLogProbMetric: 17.1314, val_loss: 17.6378, val_MinusLogProbMetric: 17.6378

Epoch 216: val_loss did not improve from 17.47947
196/196 - 76s - loss: 17.1314 - MinusLogProbMetric: 17.1314 - val_loss: 17.6378 - val_MinusLogProbMetric: 17.6378 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 217/1000
2023-09-27 14:05:32.707 
Epoch 217/1000 
	 loss: 17.1128, MinusLogProbMetric: 17.1128, val_loss: 17.5402, val_MinusLogProbMetric: 17.5402

Epoch 217: val_loss did not improve from 17.47947
196/196 - 76s - loss: 17.1128 - MinusLogProbMetric: 17.1128 - val_loss: 17.5402 - val_MinusLogProbMetric: 17.5402 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 218/1000
2023-09-27 14:06:49.735 
Epoch 218/1000 
	 loss: 17.0621, MinusLogProbMetric: 17.0621, val_loss: 17.6296, val_MinusLogProbMetric: 17.6296

Epoch 218: val_loss did not improve from 17.47947
196/196 - 77s - loss: 17.0621 - MinusLogProbMetric: 17.0621 - val_loss: 17.6296 - val_MinusLogProbMetric: 17.6296 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 219/1000
2023-09-27 14:08:06.373 
Epoch 219/1000 
	 loss: 17.1431, MinusLogProbMetric: 17.1431, val_loss: 17.5783, val_MinusLogProbMetric: 17.5783

Epoch 219: val_loss did not improve from 17.47947
196/196 - 77s - loss: 17.1431 - MinusLogProbMetric: 17.1431 - val_loss: 17.5783 - val_MinusLogProbMetric: 17.5783 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 220/1000
2023-09-27 14:09:23.548 
Epoch 220/1000 
	 loss: 17.1111, MinusLogProbMetric: 17.1111, val_loss: 17.7368, val_MinusLogProbMetric: 17.7368

Epoch 220: val_loss did not improve from 17.47947
196/196 - 77s - loss: 17.1111 - MinusLogProbMetric: 17.1111 - val_loss: 17.7368 - val_MinusLogProbMetric: 17.7368 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 221/1000
2023-09-27 14:10:40.651 
Epoch 221/1000 
	 loss: 17.0682, MinusLogProbMetric: 17.0682, val_loss: 17.7061, val_MinusLogProbMetric: 17.7061

Epoch 221: val_loss did not improve from 17.47947
196/196 - 77s - loss: 17.0682 - MinusLogProbMetric: 17.0682 - val_loss: 17.7061 - val_MinusLogProbMetric: 17.7061 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 222/1000
2023-09-27 14:11:57.244 
Epoch 222/1000 
	 loss: 17.1287, MinusLogProbMetric: 17.1287, val_loss: 17.5673, val_MinusLogProbMetric: 17.5673

Epoch 222: val_loss did not improve from 17.47947
196/196 - 77s - loss: 17.1287 - MinusLogProbMetric: 17.1287 - val_loss: 17.5673 - val_MinusLogProbMetric: 17.5673 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 223/1000
2023-09-27 14:13:14.533 
Epoch 223/1000 
	 loss: 17.1152, MinusLogProbMetric: 17.1152, val_loss: 17.9299, val_MinusLogProbMetric: 17.9299

Epoch 223: val_loss did not improve from 17.47947
196/196 - 77s - loss: 17.1152 - MinusLogProbMetric: 17.1152 - val_loss: 17.9299 - val_MinusLogProbMetric: 17.9299 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 224/1000
2023-09-27 14:14:35.491 
Epoch 224/1000 
	 loss: 17.1269, MinusLogProbMetric: 17.1269, val_loss: 17.8365, val_MinusLogProbMetric: 17.8365

Epoch 224: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.1269 - MinusLogProbMetric: 17.1269 - val_loss: 17.8365 - val_MinusLogProbMetric: 17.8365 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 225/1000
2023-09-27 14:15:55.972 
Epoch 225/1000 
	 loss: 17.0755, MinusLogProbMetric: 17.0755, val_loss: 18.3230, val_MinusLogProbMetric: 18.3230

Epoch 225: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0755 - MinusLogProbMetric: 17.0755 - val_loss: 18.3230 - val_MinusLogProbMetric: 18.3230 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 226/1000
2023-09-27 14:17:17.678 
Epoch 226/1000 
	 loss: 17.0933, MinusLogProbMetric: 17.0933, val_loss: 17.7094, val_MinusLogProbMetric: 17.7094

Epoch 226: val_loss did not improve from 17.47947
196/196 - 82s - loss: 17.0933 - MinusLogProbMetric: 17.0933 - val_loss: 17.7094 - val_MinusLogProbMetric: 17.7094 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 227/1000
2023-09-27 14:18:38.900 
Epoch 227/1000 
	 loss: 17.0706, MinusLogProbMetric: 17.0706, val_loss: 17.4981, val_MinusLogProbMetric: 17.4981

Epoch 227: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0706 - MinusLogProbMetric: 17.0706 - val_loss: 17.4981 - val_MinusLogProbMetric: 17.4981 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 228/1000
2023-09-27 14:20:00.476 
Epoch 228/1000 
	 loss: 17.0851, MinusLogProbMetric: 17.0851, val_loss: 17.6728, val_MinusLogProbMetric: 17.6728

Epoch 228: val_loss did not improve from 17.47947
196/196 - 82s - loss: 17.0851 - MinusLogProbMetric: 17.0851 - val_loss: 17.6728 - val_MinusLogProbMetric: 17.6728 - lr: 3.3333e-04 - 82s/epoch - 416ms/step
Epoch 229/1000
2023-09-27 14:21:20.239 
Epoch 229/1000 
	 loss: 17.1061, MinusLogProbMetric: 17.1061, val_loss: 17.5621, val_MinusLogProbMetric: 17.5621

Epoch 229: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.1061 - MinusLogProbMetric: 17.1061 - val_loss: 17.5621 - val_MinusLogProbMetric: 17.5621 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 230/1000
2023-09-27 14:22:40.465 
Epoch 230/1000 
	 loss: 17.0975, MinusLogProbMetric: 17.0975, val_loss: 17.6183, val_MinusLogProbMetric: 17.6183

Epoch 230: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0975 - MinusLogProbMetric: 17.0975 - val_loss: 17.6183 - val_MinusLogProbMetric: 17.6183 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 231/1000
2023-09-27 14:24:00.499 
Epoch 231/1000 
	 loss: 17.0894, MinusLogProbMetric: 17.0894, val_loss: 17.7580, val_MinusLogProbMetric: 17.7580

Epoch 231: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0894 - MinusLogProbMetric: 17.0894 - val_loss: 17.7580 - val_MinusLogProbMetric: 17.7580 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 232/1000
2023-09-27 14:25:20.463 
Epoch 232/1000 
	 loss: 17.1233, MinusLogProbMetric: 17.1233, val_loss: 17.6031, val_MinusLogProbMetric: 17.6031

Epoch 232: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.1233 - MinusLogProbMetric: 17.1233 - val_loss: 17.6031 - val_MinusLogProbMetric: 17.6031 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 233/1000
2023-09-27 14:26:39.335 
Epoch 233/1000 
	 loss: 17.1182, MinusLogProbMetric: 17.1182, val_loss: 17.4879, val_MinusLogProbMetric: 17.4879

Epoch 233: val_loss did not improve from 17.47947
196/196 - 79s - loss: 17.1182 - MinusLogProbMetric: 17.1182 - val_loss: 17.4879 - val_MinusLogProbMetric: 17.4879 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 234/1000
2023-09-27 14:27:59.289 
Epoch 234/1000 
	 loss: 17.0359, MinusLogProbMetric: 17.0359, val_loss: 17.8467, val_MinusLogProbMetric: 17.8467

Epoch 234: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0359 - MinusLogProbMetric: 17.0359 - val_loss: 17.8467 - val_MinusLogProbMetric: 17.8467 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 235/1000
2023-09-27 14:29:19.921 
Epoch 235/1000 
	 loss: 17.0586, MinusLogProbMetric: 17.0586, val_loss: 17.7095, val_MinusLogProbMetric: 17.7095

Epoch 235: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0586 - MinusLogProbMetric: 17.0586 - val_loss: 17.7095 - val_MinusLogProbMetric: 17.7095 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 236/1000
2023-09-27 14:30:41.519 
Epoch 236/1000 
	 loss: 17.1092, MinusLogProbMetric: 17.1092, val_loss: 17.6380, val_MinusLogProbMetric: 17.6380

Epoch 236: val_loss did not improve from 17.47947
196/196 - 82s - loss: 17.1092 - MinusLogProbMetric: 17.1092 - val_loss: 17.6380 - val_MinusLogProbMetric: 17.6380 - lr: 3.3333e-04 - 82s/epoch - 416ms/step
Epoch 237/1000
2023-09-27 14:32:01.311 
Epoch 237/1000 
	 loss: 17.0605, MinusLogProbMetric: 17.0605, val_loss: 17.6092, val_MinusLogProbMetric: 17.6092

Epoch 237: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0605 - MinusLogProbMetric: 17.0605 - val_loss: 17.6092 - val_MinusLogProbMetric: 17.6092 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 238/1000
2023-09-27 14:33:21.721 
Epoch 238/1000 
	 loss: 17.0908, MinusLogProbMetric: 17.0908, val_loss: 17.7596, val_MinusLogProbMetric: 17.7596

Epoch 238: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0908 - MinusLogProbMetric: 17.0908 - val_loss: 17.7596 - val_MinusLogProbMetric: 17.7596 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 239/1000
2023-09-27 14:34:42.800 
Epoch 239/1000 
	 loss: 17.0953, MinusLogProbMetric: 17.0953, val_loss: 17.8906, val_MinusLogProbMetric: 17.8906

Epoch 239: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0953 - MinusLogProbMetric: 17.0953 - val_loss: 17.8906 - val_MinusLogProbMetric: 17.8906 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 240/1000
2023-09-27 14:36:02.686 
Epoch 240/1000 
	 loss: 17.0500, MinusLogProbMetric: 17.0500, val_loss: 17.6840, val_MinusLogProbMetric: 17.6840

Epoch 240: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0500 - MinusLogProbMetric: 17.0500 - val_loss: 17.6840 - val_MinusLogProbMetric: 17.6840 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 241/1000
2023-09-27 14:37:24.122 
Epoch 241/1000 
	 loss: 17.0956, MinusLogProbMetric: 17.0956, val_loss: 17.8124, val_MinusLogProbMetric: 17.8124

Epoch 241: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0956 - MinusLogProbMetric: 17.0956 - val_loss: 17.8124 - val_MinusLogProbMetric: 17.8124 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 242/1000
2023-09-27 14:38:44.036 
Epoch 242/1000 
	 loss: 17.0442, MinusLogProbMetric: 17.0442, val_loss: 17.6799, val_MinusLogProbMetric: 17.6799

Epoch 242: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0442 - MinusLogProbMetric: 17.0442 - val_loss: 17.6799 - val_MinusLogProbMetric: 17.6799 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 243/1000
2023-09-27 14:40:04.707 
Epoch 243/1000 
	 loss: 17.0825, MinusLogProbMetric: 17.0825, val_loss: 17.8787, val_MinusLogProbMetric: 17.8787

Epoch 243: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0825 - MinusLogProbMetric: 17.0825 - val_loss: 17.8787 - val_MinusLogProbMetric: 17.8787 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 244/1000
2023-09-27 14:41:23.978 
Epoch 244/1000 
	 loss: 17.0135, MinusLogProbMetric: 17.0135, val_loss: 17.6652, val_MinusLogProbMetric: 17.6652

Epoch 244: val_loss did not improve from 17.47947
196/196 - 79s - loss: 17.0135 - MinusLogProbMetric: 17.0135 - val_loss: 17.6652 - val_MinusLogProbMetric: 17.6652 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 245/1000
2023-09-27 14:42:44.625 
Epoch 245/1000 
	 loss: 17.0724, MinusLogProbMetric: 17.0724, val_loss: 17.7407, val_MinusLogProbMetric: 17.7407

Epoch 245: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0724 - MinusLogProbMetric: 17.0724 - val_loss: 17.7407 - val_MinusLogProbMetric: 17.7407 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 246/1000
2023-09-27 14:44:06.122 
Epoch 246/1000 
	 loss: 17.0041, MinusLogProbMetric: 17.0041, val_loss: 17.7445, val_MinusLogProbMetric: 17.7445

Epoch 246: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0041 - MinusLogProbMetric: 17.0041 - val_loss: 17.7445 - val_MinusLogProbMetric: 17.7445 - lr: 3.3333e-04 - 81s/epoch - 416ms/step
Epoch 247/1000
2023-09-27 14:45:28.561 
Epoch 247/1000 
	 loss: 17.0530, MinusLogProbMetric: 17.0530, val_loss: 17.6211, val_MinusLogProbMetric: 17.6211

Epoch 247: val_loss did not improve from 17.47947
196/196 - 82s - loss: 17.0530 - MinusLogProbMetric: 17.0530 - val_loss: 17.6211 - val_MinusLogProbMetric: 17.6211 - lr: 3.3333e-04 - 82s/epoch - 421ms/step
Epoch 248/1000
2023-09-27 14:46:49.083 
Epoch 248/1000 
	 loss: 17.0673, MinusLogProbMetric: 17.0673, val_loss: 17.5956, val_MinusLogProbMetric: 17.5956

Epoch 248: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0673 - MinusLogProbMetric: 17.0673 - val_loss: 17.5956 - val_MinusLogProbMetric: 17.5956 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 249/1000
2023-09-27 14:48:09.662 
Epoch 249/1000 
	 loss: 17.0171, MinusLogProbMetric: 17.0171, val_loss: 17.6921, val_MinusLogProbMetric: 17.6921

Epoch 249: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0171 - MinusLogProbMetric: 17.0171 - val_loss: 17.6921 - val_MinusLogProbMetric: 17.6921 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 250/1000
2023-09-27 14:49:31.146 
Epoch 250/1000 
	 loss: 17.0445, MinusLogProbMetric: 17.0445, val_loss: 17.8944, val_MinusLogProbMetric: 17.8944

Epoch 250: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0445 - MinusLogProbMetric: 17.0445 - val_loss: 17.8944 - val_MinusLogProbMetric: 17.8944 - lr: 3.3333e-04 - 81s/epoch - 416ms/step
Epoch 251/1000
2023-09-27 14:50:50.853 
Epoch 251/1000 
	 loss: 17.0556, MinusLogProbMetric: 17.0556, val_loss: 17.5264, val_MinusLogProbMetric: 17.5264

Epoch 251: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0556 - MinusLogProbMetric: 17.0556 - val_loss: 17.5264 - val_MinusLogProbMetric: 17.5264 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 252/1000
2023-09-27 14:52:11.638 
Epoch 252/1000 
	 loss: 17.0454, MinusLogProbMetric: 17.0454, val_loss: 17.7361, val_MinusLogProbMetric: 17.7361

Epoch 252: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0454 - MinusLogProbMetric: 17.0454 - val_loss: 17.7361 - val_MinusLogProbMetric: 17.7361 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 253/1000
2023-09-27 14:53:32.127 
Epoch 253/1000 
	 loss: 17.0202, MinusLogProbMetric: 17.0202, val_loss: 17.8374, val_MinusLogProbMetric: 17.8374

Epoch 253: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0202 - MinusLogProbMetric: 17.0202 - val_loss: 17.8374 - val_MinusLogProbMetric: 17.8374 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 254/1000
2023-09-27 14:54:53.030 
Epoch 254/1000 
	 loss: 17.0113, MinusLogProbMetric: 17.0113, val_loss: 17.7947, val_MinusLogProbMetric: 17.7947

Epoch 254: val_loss did not improve from 17.47947
196/196 - 81s - loss: 17.0113 - MinusLogProbMetric: 17.0113 - val_loss: 17.7947 - val_MinusLogProbMetric: 17.7947 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 255/1000
2023-09-27 14:56:12.886 
Epoch 255/1000 
	 loss: 17.0709, MinusLogProbMetric: 17.0709, val_loss: 17.4834, val_MinusLogProbMetric: 17.4834

Epoch 255: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0709 - MinusLogProbMetric: 17.0709 - val_loss: 17.4834 - val_MinusLogProbMetric: 17.4834 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 256/1000
2023-09-27 14:57:33.307 
Epoch 256/1000 
	 loss: 16.9853, MinusLogProbMetric: 16.9853, val_loss: 17.5433, val_MinusLogProbMetric: 17.5433

Epoch 256: val_loss did not improve from 17.47947
196/196 - 80s - loss: 16.9853 - MinusLogProbMetric: 16.9853 - val_loss: 17.5433 - val_MinusLogProbMetric: 17.5433 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 257/1000
2023-09-27 14:58:54.784 
Epoch 257/1000 
	 loss: 16.9843, MinusLogProbMetric: 16.9843, val_loss: 17.5548, val_MinusLogProbMetric: 17.5548

Epoch 257: val_loss did not improve from 17.47947
196/196 - 81s - loss: 16.9843 - MinusLogProbMetric: 16.9843 - val_loss: 17.5548 - val_MinusLogProbMetric: 17.5548 - lr: 3.3333e-04 - 81s/epoch - 416ms/step
Epoch 258/1000
2023-09-27 15:00:14.430 
Epoch 258/1000 
	 loss: 16.9785, MinusLogProbMetric: 16.9785, val_loss: 17.7574, val_MinusLogProbMetric: 17.7574

Epoch 258: val_loss did not improve from 17.47947
196/196 - 80s - loss: 16.9785 - MinusLogProbMetric: 16.9785 - val_loss: 17.7574 - val_MinusLogProbMetric: 17.7574 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 259/1000
2023-09-27 15:01:33.819 
Epoch 259/1000 
	 loss: 17.0161, MinusLogProbMetric: 17.0161, val_loss: 17.6131, val_MinusLogProbMetric: 17.6131

Epoch 259: val_loss did not improve from 17.47947
196/196 - 79s - loss: 17.0161 - MinusLogProbMetric: 17.0161 - val_loss: 17.6131 - val_MinusLogProbMetric: 17.6131 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 260/1000
2023-09-27 15:02:53.419 
Epoch 260/1000 
	 loss: 17.0366, MinusLogProbMetric: 17.0366, val_loss: 17.7912, val_MinusLogProbMetric: 17.7912

Epoch 260: val_loss did not improve from 17.47947
196/196 - 80s - loss: 17.0366 - MinusLogProbMetric: 17.0366 - val_loss: 17.7912 - val_MinusLogProbMetric: 17.7912 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 261/1000
2023-09-27 15:04:12.785 
Epoch 261/1000 
	 loss: 17.0208, MinusLogProbMetric: 17.0208, val_loss: 17.6967, val_MinusLogProbMetric: 17.6967

Epoch 261: val_loss did not improve from 17.47947
196/196 - 79s - loss: 17.0208 - MinusLogProbMetric: 17.0208 - val_loss: 17.6967 - val_MinusLogProbMetric: 17.6967 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 262/1000
2023-09-27 15:05:31.474 
Epoch 262/1000 
	 loss: 16.7093, MinusLogProbMetric: 16.7093, val_loss: 17.4579, val_MinusLogProbMetric: 17.4579

Epoch 262: val_loss improved from 17.47947 to 17.45794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 80s - loss: 16.7093 - MinusLogProbMetric: 16.7093 - val_loss: 17.4579 - val_MinusLogProbMetric: 17.4579 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 263/1000
2023-09-27 15:06:50.725 
Epoch 263/1000 
	 loss: 16.6945, MinusLogProbMetric: 16.6945, val_loss: 17.4060, val_MinusLogProbMetric: 17.4060

Epoch 263: val_loss improved from 17.45794 to 17.40598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 79s - loss: 16.6945 - MinusLogProbMetric: 16.6945 - val_loss: 17.4060 - val_MinusLogProbMetric: 17.4060 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 264/1000
2023-09-27 15:08:11.725 
Epoch 264/1000 
	 loss: 16.6773, MinusLogProbMetric: 16.6773, val_loss: 17.2796, val_MinusLogProbMetric: 17.2796

Epoch 264: val_loss improved from 17.40598 to 17.27955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 81s - loss: 16.6773 - MinusLogProbMetric: 16.6773 - val_loss: 17.2796 - val_MinusLogProbMetric: 17.2796 - lr: 1.6667e-04 - 81s/epoch - 415ms/step
Epoch 265/1000
2023-09-27 15:09:33.516 
Epoch 265/1000 
	 loss: 16.6921, MinusLogProbMetric: 16.6921, val_loss: 17.3193, val_MinusLogProbMetric: 17.3193

Epoch 265: val_loss did not improve from 17.27955
196/196 - 80s - loss: 16.6921 - MinusLogProbMetric: 16.6921 - val_loss: 17.3193 - val_MinusLogProbMetric: 17.3193 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 266/1000
2023-09-27 15:10:54.884 
Epoch 266/1000 
	 loss: 16.7146, MinusLogProbMetric: 16.7146, val_loss: 17.3144, val_MinusLogProbMetric: 17.3144

Epoch 266: val_loss did not improve from 17.27955
196/196 - 81s - loss: 16.7146 - MinusLogProbMetric: 16.7146 - val_loss: 17.3144 - val_MinusLogProbMetric: 17.3144 - lr: 1.6667e-04 - 81s/epoch - 415ms/step
Epoch 267/1000
2023-09-27 15:12:13.563 
Epoch 267/1000 
	 loss: 16.6851, MinusLogProbMetric: 16.6851, val_loss: 17.3946, val_MinusLogProbMetric: 17.3946

Epoch 267: val_loss did not improve from 17.27955
196/196 - 79s - loss: 16.6851 - MinusLogProbMetric: 16.6851 - val_loss: 17.3946 - val_MinusLogProbMetric: 17.3946 - lr: 1.6667e-04 - 79s/epoch - 401ms/step
Epoch 268/1000
2023-09-27 15:13:33.619 
Epoch 268/1000 
	 loss: 16.7117, MinusLogProbMetric: 16.7117, val_loss: 17.5767, val_MinusLogProbMetric: 17.5767

Epoch 268: val_loss did not improve from 17.27955
196/196 - 80s - loss: 16.7117 - MinusLogProbMetric: 16.7117 - val_loss: 17.5767 - val_MinusLogProbMetric: 17.5767 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 269/1000
2023-09-27 15:14:52.660 
Epoch 269/1000 
	 loss: 16.6867, MinusLogProbMetric: 16.6867, val_loss: 17.3591, val_MinusLogProbMetric: 17.3591

Epoch 269: val_loss did not improve from 17.27955
196/196 - 79s - loss: 16.6867 - MinusLogProbMetric: 16.6867 - val_loss: 17.3591 - val_MinusLogProbMetric: 17.3591 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 270/1000
2023-09-27 15:16:12.873 
Epoch 270/1000 
	 loss: 16.7122, MinusLogProbMetric: 16.7122, val_loss: 17.6575, val_MinusLogProbMetric: 17.6575

Epoch 270: val_loss did not improve from 17.27955
196/196 - 80s - loss: 16.7122 - MinusLogProbMetric: 16.7122 - val_loss: 17.6575 - val_MinusLogProbMetric: 17.6575 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 271/1000
2023-09-27 15:17:32.923 
Epoch 271/1000 
	 loss: 16.6969, MinusLogProbMetric: 16.6969, val_loss: 17.5260, val_MinusLogProbMetric: 17.5260

Epoch 271: val_loss did not improve from 17.27955
196/196 - 80s - loss: 16.6969 - MinusLogProbMetric: 16.6969 - val_loss: 17.5260 - val_MinusLogProbMetric: 17.5260 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 272/1000
2023-09-27 15:18:54.587 
Epoch 272/1000 
	 loss: 16.6945, MinusLogProbMetric: 16.6945, val_loss: 17.5046, val_MinusLogProbMetric: 17.5046

Epoch 272: val_loss did not improve from 17.27955
196/196 - 82s - loss: 16.6945 - MinusLogProbMetric: 16.6945 - val_loss: 17.5046 - val_MinusLogProbMetric: 17.5046 - lr: 1.6667e-04 - 82s/epoch - 417ms/step
Epoch 273/1000
2023-09-27 15:20:15.590 
Epoch 273/1000 
	 loss: 16.7006, MinusLogProbMetric: 16.7006, val_loss: 17.3678, val_MinusLogProbMetric: 17.3678

Epoch 273: val_loss did not improve from 17.27955
196/196 - 81s - loss: 16.7006 - MinusLogProbMetric: 16.7006 - val_loss: 17.3678 - val_MinusLogProbMetric: 17.3678 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 274/1000
2023-09-27 15:21:41.380 
Epoch 274/1000 
	 loss: 16.6806, MinusLogProbMetric: 16.6806, val_loss: 17.2936, val_MinusLogProbMetric: 17.2936

Epoch 274: val_loss did not improve from 17.27955
196/196 - 86s - loss: 16.6806 - MinusLogProbMetric: 16.6806 - val_loss: 17.2936 - val_MinusLogProbMetric: 17.2936 - lr: 1.6667e-04 - 86s/epoch - 438ms/step
Epoch 275/1000
2023-09-27 15:23:03.444 
Epoch 275/1000 
	 loss: 16.6934, MinusLogProbMetric: 16.6934, val_loss: 17.7267, val_MinusLogProbMetric: 17.7267

Epoch 275: val_loss did not improve from 17.27955
196/196 - 82s - loss: 16.6934 - MinusLogProbMetric: 16.6934 - val_loss: 17.7267 - val_MinusLogProbMetric: 17.7267 - lr: 1.6667e-04 - 82s/epoch - 419ms/step
Epoch 276/1000
2023-09-27 15:24:23.830 
Epoch 276/1000 
	 loss: 16.6910, MinusLogProbMetric: 16.6910, val_loss: 17.3930, val_MinusLogProbMetric: 17.3930

Epoch 276: val_loss did not improve from 17.27955
196/196 - 80s - loss: 16.6910 - MinusLogProbMetric: 16.6910 - val_loss: 17.3930 - val_MinusLogProbMetric: 17.3930 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 277/1000
2023-09-27 15:25:44.860 
Epoch 277/1000 
	 loss: 16.6845, MinusLogProbMetric: 16.6845, val_loss: 17.3647, val_MinusLogProbMetric: 17.3647

Epoch 277: val_loss did not improve from 17.27955
196/196 - 81s - loss: 16.6845 - MinusLogProbMetric: 16.6845 - val_loss: 17.3647 - val_MinusLogProbMetric: 17.3647 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 278/1000
2023-09-27 15:27:04.736 
Epoch 278/1000 
	 loss: 16.6939, MinusLogProbMetric: 16.6939, val_loss: 17.3853, val_MinusLogProbMetric: 17.3853

Epoch 278: val_loss did not improve from 17.27955
196/196 - 80s - loss: 16.6939 - MinusLogProbMetric: 16.6939 - val_loss: 17.3853 - val_MinusLogProbMetric: 17.3853 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 279/1000
2023-09-27 15:28:26.133 
Epoch 279/1000 
	 loss: 16.6593, MinusLogProbMetric: 16.6593, val_loss: 17.3520, val_MinusLogProbMetric: 17.3520

Epoch 279: val_loss did not improve from 17.27955
196/196 - 81s - loss: 16.6593 - MinusLogProbMetric: 16.6593 - val_loss: 17.3520 - val_MinusLogProbMetric: 17.3520 - lr: 1.6667e-04 - 81s/epoch - 415ms/step
Epoch 280/1000
2023-09-27 15:29:47.096 
Epoch 280/1000 
	 loss: 16.6995, MinusLogProbMetric: 16.6995, val_loss: 17.5490, val_MinusLogProbMetric: 17.5490

Epoch 280: val_loss did not improve from 17.27955
196/196 - 81s - loss: 16.6995 - MinusLogProbMetric: 16.6995 - val_loss: 17.5490 - val_MinusLogProbMetric: 17.5490 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 281/1000
2023-09-27 15:31:08.210 
Epoch 281/1000 
	 loss: 16.6671, MinusLogProbMetric: 16.6671, val_loss: 17.3699, val_MinusLogProbMetric: 17.3699

Epoch 281: val_loss did not improve from 17.27955
196/196 - 81s - loss: 16.6671 - MinusLogProbMetric: 16.6671 - val_loss: 17.3699 - val_MinusLogProbMetric: 17.3699 - lr: 1.6667e-04 - 81s/epoch - 414ms/step
Epoch 282/1000
2023-09-27 15:32:30.436 
Epoch 282/1000 
	 loss: 16.6987, MinusLogProbMetric: 16.6987, val_loss: 17.3445, val_MinusLogProbMetric: 17.3445

Epoch 282: val_loss did not improve from 17.27955
196/196 - 82s - loss: 16.6987 - MinusLogProbMetric: 16.6987 - val_loss: 17.3445 - val_MinusLogProbMetric: 17.3445 - lr: 1.6667e-04 - 82s/epoch - 420ms/step
Epoch 283/1000
2023-09-27 15:33:49.169 
Epoch 283/1000 
	 loss: 16.7119, MinusLogProbMetric: 16.7119, val_loss: 17.4277, val_MinusLogProbMetric: 17.4277

Epoch 283: val_loss did not improve from 17.27955
196/196 - 79s - loss: 16.7119 - MinusLogProbMetric: 16.7119 - val_loss: 17.4277 - val_MinusLogProbMetric: 17.4277 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 284/1000
2023-09-27 15:34:59.346 
Epoch 284/1000 
	 loss: 16.6898, MinusLogProbMetric: 16.6898, val_loss: 17.2552, val_MinusLogProbMetric: 17.2552

Epoch 284: val_loss improved from 17.27955 to 17.25517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 71s - loss: 16.6898 - MinusLogProbMetric: 16.6898 - val_loss: 17.2552 - val_MinusLogProbMetric: 17.2552 - lr: 1.6667e-04 - 71s/epoch - 364ms/step
Epoch 285/1000
2023-09-27 15:36:06.035 
Epoch 285/1000 
	 loss: 16.7241, MinusLogProbMetric: 16.7241, val_loss: 17.3645, val_MinusLogProbMetric: 17.3645

Epoch 285: val_loss did not improve from 17.25517
196/196 - 65s - loss: 16.7241 - MinusLogProbMetric: 16.7241 - val_loss: 17.3645 - val_MinusLogProbMetric: 17.3645 - lr: 1.6667e-04 - 65s/epoch - 334ms/step
Epoch 286/1000
2023-09-27 15:37:17.489 
Epoch 286/1000 
	 loss: 16.6693, MinusLogProbMetric: 16.6693, val_loss: 17.3485, val_MinusLogProbMetric: 17.3485

Epoch 286: val_loss did not improve from 17.25517
196/196 - 71s - loss: 16.6693 - MinusLogProbMetric: 16.6693 - val_loss: 17.3485 - val_MinusLogProbMetric: 17.3485 - lr: 1.6667e-04 - 71s/epoch - 365ms/step
Epoch 287/1000
2023-09-27 15:38:22.006 
Epoch 287/1000 
	 loss: 16.6970, MinusLogProbMetric: 16.6970, val_loss: 17.6020, val_MinusLogProbMetric: 17.6020

Epoch 287: val_loss did not improve from 17.25517
196/196 - 65s - loss: 16.6970 - MinusLogProbMetric: 16.6970 - val_loss: 17.6020 - val_MinusLogProbMetric: 17.6020 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 288/1000
2023-09-27 15:39:39.817 
Epoch 288/1000 
	 loss: 16.7030, MinusLogProbMetric: 16.7030, val_loss: 17.2827, val_MinusLogProbMetric: 17.2827

Epoch 288: val_loss did not improve from 17.25517
196/196 - 78s - loss: 16.7030 - MinusLogProbMetric: 16.7030 - val_loss: 17.2827 - val_MinusLogProbMetric: 17.2827 - lr: 1.6667e-04 - 78s/epoch - 397ms/step
Epoch 289/1000
2023-09-27 15:40:59.393 
Epoch 289/1000 
	 loss: 16.6678, MinusLogProbMetric: 16.6678, val_loss: 17.4648, val_MinusLogProbMetric: 17.4648

Epoch 289: val_loss did not improve from 17.25517
196/196 - 80s - loss: 16.6678 - MinusLogProbMetric: 16.6678 - val_loss: 17.4648 - val_MinusLogProbMetric: 17.4648 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 290/1000
2023-09-27 15:42:20.230 
Epoch 290/1000 
	 loss: 16.6838, MinusLogProbMetric: 16.6838, val_loss: 17.3533, val_MinusLogProbMetric: 17.3533

Epoch 290: val_loss did not improve from 17.25517
196/196 - 81s - loss: 16.6838 - MinusLogProbMetric: 16.6838 - val_loss: 17.3533 - val_MinusLogProbMetric: 17.3533 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 291/1000
2023-09-27 15:43:41.537 
Epoch 291/1000 
	 loss: 16.7386, MinusLogProbMetric: 16.7386, val_loss: 17.3112, val_MinusLogProbMetric: 17.3112

Epoch 291: val_loss did not improve from 17.25517
196/196 - 81s - loss: 16.7386 - MinusLogProbMetric: 16.7386 - val_loss: 17.3112 - val_MinusLogProbMetric: 17.3112 - lr: 1.6667e-04 - 81s/epoch - 415ms/step
Epoch 292/1000
2023-09-27 15:45:01.493 
Epoch 292/1000 
	 loss: 16.6738, MinusLogProbMetric: 16.6738, val_loss: 17.3172, val_MinusLogProbMetric: 17.3172

Epoch 292: val_loss did not improve from 17.25517
196/196 - 80s - loss: 16.6738 - MinusLogProbMetric: 16.6738 - val_loss: 17.3172 - val_MinusLogProbMetric: 17.3172 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 293/1000
2023-09-27 15:46:21.329 
Epoch 293/1000 
	 loss: 16.7240, MinusLogProbMetric: 16.7240, val_loss: 17.2628, val_MinusLogProbMetric: 17.2628

Epoch 293: val_loss did not improve from 17.25517
196/196 - 80s - loss: 16.7240 - MinusLogProbMetric: 16.7240 - val_loss: 17.2628 - val_MinusLogProbMetric: 17.2628 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 294/1000
2023-09-27 15:47:42.285 
Epoch 294/1000 
	 loss: 16.6640, MinusLogProbMetric: 16.6640, val_loss: 17.2837, val_MinusLogProbMetric: 17.2837

Epoch 294: val_loss did not improve from 17.25517
196/196 - 81s - loss: 16.6640 - MinusLogProbMetric: 16.6640 - val_loss: 17.2837 - val_MinusLogProbMetric: 17.2837 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 295/1000
2023-09-27 15:48:50.285 
Epoch 295/1000 
	 loss: 16.6645, MinusLogProbMetric: 16.6645, val_loss: 17.3576, val_MinusLogProbMetric: 17.3576

Epoch 295: val_loss did not improve from 17.25517
196/196 - 68s - loss: 16.6645 - MinusLogProbMetric: 16.6645 - val_loss: 17.3576 - val_MinusLogProbMetric: 17.3576 - lr: 1.6667e-04 - 68s/epoch - 347ms/step
Epoch 296/1000
2023-09-27 15:49:58.514 
Epoch 296/1000 
	 loss: 16.7398, MinusLogProbMetric: 16.7398, val_loss: 17.7941, val_MinusLogProbMetric: 17.7941

Epoch 296: val_loss did not improve from 17.25517
196/196 - 68s - loss: 16.7398 - MinusLogProbMetric: 16.7398 - val_loss: 17.7941 - val_MinusLogProbMetric: 17.7941 - lr: 1.6667e-04 - 68s/epoch - 348ms/step
Epoch 297/1000
2023-09-27 15:51:07.563 
Epoch 297/1000 
	 loss: 16.6845, MinusLogProbMetric: 16.6845, val_loss: 17.2998, val_MinusLogProbMetric: 17.2998

Epoch 297: val_loss did not improve from 17.25517
196/196 - 69s - loss: 16.6845 - MinusLogProbMetric: 16.6845 - val_loss: 17.2998 - val_MinusLogProbMetric: 17.2998 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 298/1000
2023-09-27 15:52:12.925 
Epoch 298/1000 
	 loss: 16.6538, MinusLogProbMetric: 16.6538, val_loss: 17.2608, val_MinusLogProbMetric: 17.2608

Epoch 298: val_loss did not improve from 17.25517
196/196 - 65s - loss: 16.6538 - MinusLogProbMetric: 16.6538 - val_loss: 17.2608 - val_MinusLogProbMetric: 17.2608 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 299/1000
2023-09-27 15:53:30.028 
Epoch 299/1000 
	 loss: 16.7296, MinusLogProbMetric: 16.7296, val_loss: 17.2784, val_MinusLogProbMetric: 17.2784

Epoch 299: val_loss did not improve from 17.25517
196/196 - 77s - loss: 16.7296 - MinusLogProbMetric: 16.7296 - val_loss: 17.2784 - val_MinusLogProbMetric: 17.2784 - lr: 1.6667e-04 - 77s/epoch - 393ms/step
Epoch 300/1000
2023-09-27 15:54:43.314 
Epoch 300/1000 
	 loss: 16.6671, MinusLogProbMetric: 16.6671, val_loss: 17.2825, val_MinusLogProbMetric: 17.2825

Epoch 300: val_loss did not improve from 17.25517
196/196 - 73s - loss: 16.6671 - MinusLogProbMetric: 16.6671 - val_loss: 17.2825 - val_MinusLogProbMetric: 17.2825 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 301/1000
2023-09-27 15:55:52.884 
Epoch 301/1000 
	 loss: 16.6587, MinusLogProbMetric: 16.6587, val_loss: 17.2774, val_MinusLogProbMetric: 17.2774

Epoch 301: val_loss did not improve from 17.25517
196/196 - 70s - loss: 16.6587 - MinusLogProbMetric: 16.6587 - val_loss: 17.2774 - val_MinusLogProbMetric: 17.2774 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 302/1000
2023-09-27 15:57:06.810 
Epoch 302/1000 
	 loss: 16.6779, MinusLogProbMetric: 16.6779, val_loss: 17.2893, val_MinusLogProbMetric: 17.2893

Epoch 302: val_loss did not improve from 17.25517
196/196 - 74s - loss: 16.6779 - MinusLogProbMetric: 16.6779 - val_loss: 17.2893 - val_MinusLogProbMetric: 17.2893 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 303/1000
2023-09-27 15:58:22.752 
Epoch 303/1000 
	 loss: 16.6644, MinusLogProbMetric: 16.6644, val_loss: 17.3197, val_MinusLogProbMetric: 17.3197

Epoch 303: val_loss did not improve from 17.25517
196/196 - 76s - loss: 16.6644 - MinusLogProbMetric: 16.6644 - val_loss: 17.3197 - val_MinusLogProbMetric: 17.3197 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 304/1000
2023-09-27 15:59:38.305 
Epoch 304/1000 
	 loss: 16.6470, MinusLogProbMetric: 16.6470, val_loss: 17.3086, val_MinusLogProbMetric: 17.3086

Epoch 304: val_loss did not improve from 17.25517
196/196 - 76s - loss: 16.6470 - MinusLogProbMetric: 16.6470 - val_loss: 17.3086 - val_MinusLogProbMetric: 17.3086 - lr: 1.6667e-04 - 76s/epoch - 385ms/step
Epoch 305/1000
2023-09-27 16:00:48.793 
Epoch 305/1000 
	 loss: 16.6863, MinusLogProbMetric: 16.6863, val_loss: 17.4136, val_MinusLogProbMetric: 17.4136

Epoch 305: val_loss did not improve from 17.25517
196/196 - 70s - loss: 16.6863 - MinusLogProbMetric: 16.6863 - val_loss: 17.4136 - val_MinusLogProbMetric: 17.4136 - lr: 1.6667e-04 - 70s/epoch - 360ms/step
Epoch 306/1000
2023-09-27 16:02:01.862 
Epoch 306/1000 
	 loss: 16.6554, MinusLogProbMetric: 16.6554, val_loss: 17.2379, val_MinusLogProbMetric: 17.2379

Epoch 306: val_loss improved from 17.25517 to 17.23792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 74s - loss: 16.6554 - MinusLogProbMetric: 16.6554 - val_loss: 17.2379 - val_MinusLogProbMetric: 17.2379 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 307/1000
2023-09-27 16:03:19.407 
Epoch 307/1000 
	 loss: 16.6837, MinusLogProbMetric: 16.6837, val_loss: 17.3549, val_MinusLogProbMetric: 17.3549

Epoch 307: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6837 - MinusLogProbMetric: 16.6837 - val_loss: 17.3549 - val_MinusLogProbMetric: 17.3549 - lr: 1.6667e-04 - 76s/epoch - 390ms/step
Epoch 308/1000
2023-09-27 16:04:35.565 
Epoch 308/1000 
	 loss: 16.6537, MinusLogProbMetric: 16.6537, val_loss: 17.5097, val_MinusLogProbMetric: 17.5097

Epoch 308: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6537 - MinusLogProbMetric: 16.6537 - val_loss: 17.5097 - val_MinusLogProbMetric: 17.5097 - lr: 1.6667e-04 - 76s/epoch - 389ms/step
Epoch 309/1000
2023-09-27 16:05:52.229 
Epoch 309/1000 
	 loss: 16.6610, MinusLogProbMetric: 16.6610, val_loss: 17.3785, val_MinusLogProbMetric: 17.3785

Epoch 309: val_loss did not improve from 17.23792
196/196 - 77s - loss: 16.6610 - MinusLogProbMetric: 16.6610 - val_loss: 17.3785 - val_MinusLogProbMetric: 17.3785 - lr: 1.6667e-04 - 77s/epoch - 391ms/step
Epoch 310/1000
2023-09-27 16:07:07.338 
Epoch 310/1000 
	 loss: 16.6360, MinusLogProbMetric: 16.6360, val_loss: 17.3427, val_MinusLogProbMetric: 17.3427

Epoch 310: val_loss did not improve from 17.23792
196/196 - 75s - loss: 16.6360 - MinusLogProbMetric: 16.6360 - val_loss: 17.3427 - val_MinusLogProbMetric: 17.3427 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 311/1000
2023-09-27 16:08:18.377 
Epoch 311/1000 
	 loss: 16.6738, MinusLogProbMetric: 16.6738, val_loss: 17.4526, val_MinusLogProbMetric: 17.4526

Epoch 311: val_loss did not improve from 17.23792
196/196 - 71s - loss: 16.6738 - MinusLogProbMetric: 16.6738 - val_loss: 17.4526 - val_MinusLogProbMetric: 17.4526 - lr: 1.6667e-04 - 71s/epoch - 362ms/step
Epoch 312/1000
2023-09-27 16:09:29.770 
Epoch 312/1000 
	 loss: 16.6716, MinusLogProbMetric: 16.6716, val_loss: 17.3438, val_MinusLogProbMetric: 17.3438

Epoch 312: val_loss did not improve from 17.23792
196/196 - 71s - loss: 16.6716 - MinusLogProbMetric: 16.6716 - val_loss: 17.3438 - val_MinusLogProbMetric: 17.3438 - lr: 1.6667e-04 - 71s/epoch - 364ms/step
Epoch 313/1000
2023-09-27 16:10:41.962 
Epoch 313/1000 
	 loss: 16.6481, MinusLogProbMetric: 16.6481, val_loss: 17.3763, val_MinusLogProbMetric: 17.3763

Epoch 313: val_loss did not improve from 17.23792
196/196 - 72s - loss: 16.6481 - MinusLogProbMetric: 16.6481 - val_loss: 17.3763 - val_MinusLogProbMetric: 17.3763 - lr: 1.6667e-04 - 72s/epoch - 368ms/step
Epoch 314/1000
2023-09-27 16:11:53.956 
Epoch 314/1000 
	 loss: 16.6666, MinusLogProbMetric: 16.6666, val_loss: 17.5427, val_MinusLogProbMetric: 17.5427

Epoch 314: val_loss did not improve from 17.23792
196/196 - 72s - loss: 16.6666 - MinusLogProbMetric: 16.6666 - val_loss: 17.5427 - val_MinusLogProbMetric: 17.5427 - lr: 1.6667e-04 - 72s/epoch - 367ms/step
Epoch 315/1000
2023-09-27 16:13:09.524 
Epoch 315/1000 
	 loss: 16.6697, MinusLogProbMetric: 16.6697, val_loss: 17.3177, val_MinusLogProbMetric: 17.3177

Epoch 315: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6697 - MinusLogProbMetric: 16.6697 - val_loss: 17.3177 - val_MinusLogProbMetric: 17.3177 - lr: 1.6667e-04 - 76s/epoch - 386ms/step
Epoch 316/1000
2023-09-27 16:14:19.118 
Epoch 316/1000 
	 loss: 16.6216, MinusLogProbMetric: 16.6216, val_loss: 17.4017, val_MinusLogProbMetric: 17.4017

Epoch 316: val_loss did not improve from 17.23792
196/196 - 70s - loss: 16.6216 - MinusLogProbMetric: 16.6216 - val_loss: 17.4017 - val_MinusLogProbMetric: 17.4017 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 317/1000
2023-09-27 16:15:29.693 
Epoch 317/1000 
	 loss: 16.6646, MinusLogProbMetric: 16.6646, val_loss: 17.4712, val_MinusLogProbMetric: 17.4712

Epoch 317: val_loss did not improve from 17.23792
196/196 - 71s - loss: 16.6646 - MinusLogProbMetric: 16.6646 - val_loss: 17.4712 - val_MinusLogProbMetric: 17.4712 - lr: 1.6667e-04 - 71s/epoch - 360ms/step
Epoch 318/1000
2023-09-27 16:16:45.921 
Epoch 318/1000 
	 loss: 16.6990, MinusLogProbMetric: 16.6990, val_loss: 17.2798, val_MinusLogProbMetric: 17.2798

Epoch 318: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6990 - MinusLogProbMetric: 16.6990 - val_loss: 17.2798 - val_MinusLogProbMetric: 17.2798 - lr: 1.6667e-04 - 76s/epoch - 389ms/step
Epoch 319/1000
2023-09-27 16:18:01.088 
Epoch 319/1000 
	 loss: 16.6619, MinusLogProbMetric: 16.6619, val_loss: 17.3681, val_MinusLogProbMetric: 17.3681

Epoch 319: val_loss did not improve from 17.23792
196/196 - 75s - loss: 16.6619 - MinusLogProbMetric: 16.6619 - val_loss: 17.3681 - val_MinusLogProbMetric: 17.3681 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 320/1000
2023-09-27 16:19:09.224 
Epoch 320/1000 
	 loss: 16.6344, MinusLogProbMetric: 16.6344, val_loss: 17.3582, val_MinusLogProbMetric: 17.3582

Epoch 320: val_loss did not improve from 17.23792
196/196 - 68s - loss: 16.6344 - MinusLogProbMetric: 16.6344 - val_loss: 17.3582 - val_MinusLogProbMetric: 17.3582 - lr: 1.6667e-04 - 68s/epoch - 348ms/step
Epoch 321/1000
2023-09-27 16:20:22.510 
Epoch 321/1000 
	 loss: 16.6680, MinusLogProbMetric: 16.6680, val_loss: 17.3160, val_MinusLogProbMetric: 17.3160

Epoch 321: val_loss did not improve from 17.23792
196/196 - 73s - loss: 16.6680 - MinusLogProbMetric: 16.6680 - val_loss: 17.3160 - val_MinusLogProbMetric: 17.3160 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 322/1000
2023-09-27 16:21:37.082 
Epoch 322/1000 
	 loss: 16.6320, MinusLogProbMetric: 16.6320, val_loss: 17.4721, val_MinusLogProbMetric: 17.4721

Epoch 322: val_loss did not improve from 17.23792
196/196 - 75s - loss: 16.6320 - MinusLogProbMetric: 16.6320 - val_loss: 17.4721 - val_MinusLogProbMetric: 17.4721 - lr: 1.6667e-04 - 75s/epoch - 380ms/step
Epoch 323/1000
2023-09-27 16:22:45.560 
Epoch 323/1000 
	 loss: 16.6750, MinusLogProbMetric: 16.6750, val_loss: 17.3629, val_MinusLogProbMetric: 17.3629

Epoch 323: val_loss did not improve from 17.23792
196/196 - 68s - loss: 16.6750 - MinusLogProbMetric: 16.6750 - val_loss: 17.3629 - val_MinusLogProbMetric: 17.3629 - lr: 1.6667e-04 - 68s/epoch - 349ms/step
Epoch 324/1000
2023-09-27 16:23:59.161 
Epoch 324/1000 
	 loss: 16.6530, MinusLogProbMetric: 16.6530, val_loss: 17.2956, val_MinusLogProbMetric: 17.2956

Epoch 324: val_loss did not improve from 17.23792
196/196 - 74s - loss: 16.6530 - MinusLogProbMetric: 16.6530 - val_loss: 17.2956 - val_MinusLogProbMetric: 17.2956 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 325/1000
2023-09-27 16:25:14.576 
Epoch 325/1000 
	 loss: 16.6310, MinusLogProbMetric: 16.6310, val_loss: 17.3534, val_MinusLogProbMetric: 17.3534

Epoch 325: val_loss did not improve from 17.23792
196/196 - 75s - loss: 16.6310 - MinusLogProbMetric: 16.6310 - val_loss: 17.3534 - val_MinusLogProbMetric: 17.3534 - lr: 1.6667e-04 - 75s/epoch - 385ms/step
Epoch 326/1000
2023-09-27 16:26:31.477 
Epoch 326/1000 
	 loss: 16.6832, MinusLogProbMetric: 16.6832, val_loss: 17.3806, val_MinusLogProbMetric: 17.3806

Epoch 326: val_loss did not improve from 17.23792
196/196 - 77s - loss: 16.6832 - MinusLogProbMetric: 16.6832 - val_loss: 17.3806 - val_MinusLogProbMetric: 17.3806 - lr: 1.6667e-04 - 77s/epoch - 392ms/step
Epoch 327/1000
2023-09-27 16:27:48.979 
Epoch 327/1000 
	 loss: 16.6469, MinusLogProbMetric: 16.6469, val_loss: 17.3624, val_MinusLogProbMetric: 17.3624

Epoch 327: val_loss did not improve from 17.23792
196/196 - 77s - loss: 16.6469 - MinusLogProbMetric: 16.6469 - val_loss: 17.3624 - val_MinusLogProbMetric: 17.3624 - lr: 1.6667e-04 - 77s/epoch - 395ms/step
Epoch 328/1000
2023-09-27 16:29:05.067 
Epoch 328/1000 
	 loss: 16.6344, MinusLogProbMetric: 16.6344, val_loss: 17.3476, val_MinusLogProbMetric: 17.3476

Epoch 328: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6344 - MinusLogProbMetric: 16.6344 - val_loss: 17.3476 - val_MinusLogProbMetric: 17.3476 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 329/1000
2023-09-27 16:30:20.565 
Epoch 329/1000 
	 loss: 16.6810, MinusLogProbMetric: 16.6810, val_loss: 17.5215, val_MinusLogProbMetric: 17.5215

Epoch 329: val_loss did not improve from 17.23792
196/196 - 75s - loss: 16.6810 - MinusLogProbMetric: 16.6810 - val_loss: 17.5215 - val_MinusLogProbMetric: 17.5215 - lr: 1.6667e-04 - 75s/epoch - 385ms/step
Epoch 330/1000
2023-09-27 16:31:29.833 
Epoch 330/1000 
	 loss: 16.6448, MinusLogProbMetric: 16.6448, val_loss: 17.3202, val_MinusLogProbMetric: 17.3202

Epoch 330: val_loss did not improve from 17.23792
196/196 - 69s - loss: 16.6448 - MinusLogProbMetric: 16.6448 - val_loss: 17.3202 - val_MinusLogProbMetric: 17.3202 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 331/1000
2023-09-27 16:32:40.502 
Epoch 331/1000 
	 loss: 16.6480, MinusLogProbMetric: 16.6480, val_loss: 17.3750, val_MinusLogProbMetric: 17.3750

Epoch 331: val_loss did not improve from 17.23792
196/196 - 71s - loss: 16.6480 - MinusLogProbMetric: 16.6480 - val_loss: 17.3750 - val_MinusLogProbMetric: 17.3750 - lr: 1.6667e-04 - 71s/epoch - 361ms/step
Epoch 332/1000
2023-09-27 16:33:51.980 
Epoch 332/1000 
	 loss: 16.6290, MinusLogProbMetric: 16.6290, val_loss: 17.2497, val_MinusLogProbMetric: 17.2497

Epoch 332: val_loss did not improve from 17.23792
196/196 - 71s - loss: 16.6290 - MinusLogProbMetric: 16.6290 - val_loss: 17.2497 - val_MinusLogProbMetric: 17.2497 - lr: 1.6667e-04 - 71s/epoch - 365ms/step
Epoch 333/1000
2023-09-27 16:35:05.489 
Epoch 333/1000 
	 loss: 16.6443, MinusLogProbMetric: 16.6443, val_loss: 17.3105, val_MinusLogProbMetric: 17.3105

Epoch 333: val_loss did not improve from 17.23792
196/196 - 74s - loss: 16.6443 - MinusLogProbMetric: 16.6443 - val_loss: 17.3105 - val_MinusLogProbMetric: 17.3105 - lr: 1.6667e-04 - 74s/epoch - 375ms/step
Epoch 334/1000
2023-09-27 16:36:20.226 
Epoch 334/1000 
	 loss: 16.6492, MinusLogProbMetric: 16.6492, val_loss: 17.4925, val_MinusLogProbMetric: 17.4925

Epoch 334: val_loss did not improve from 17.23792
196/196 - 75s - loss: 16.6492 - MinusLogProbMetric: 16.6492 - val_loss: 17.4925 - val_MinusLogProbMetric: 17.4925 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 335/1000
2023-09-27 16:37:36.166 
Epoch 335/1000 
	 loss: 16.6749, MinusLogProbMetric: 16.6749, val_loss: 17.3792, val_MinusLogProbMetric: 17.3792

Epoch 335: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6749 - MinusLogProbMetric: 16.6749 - val_loss: 17.3792 - val_MinusLogProbMetric: 17.3792 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 336/1000
2023-09-27 16:38:52.301 
Epoch 336/1000 
	 loss: 16.6332, MinusLogProbMetric: 16.6332, val_loss: 17.3661, val_MinusLogProbMetric: 17.3661

Epoch 336: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6332 - MinusLogProbMetric: 16.6332 - val_loss: 17.3661 - val_MinusLogProbMetric: 17.3661 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 337/1000
2023-09-27 16:40:07.968 
Epoch 337/1000 
	 loss: 16.6302, MinusLogProbMetric: 16.6302, val_loss: 17.4725, val_MinusLogProbMetric: 17.4725

Epoch 337: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6302 - MinusLogProbMetric: 16.6302 - val_loss: 17.4725 - val_MinusLogProbMetric: 17.4725 - lr: 1.6667e-04 - 76s/epoch - 386ms/step
Epoch 338/1000
2023-09-27 16:41:22.966 
Epoch 338/1000 
	 loss: 16.6383, MinusLogProbMetric: 16.6383, val_loss: 17.6410, val_MinusLogProbMetric: 17.6410

Epoch 338: val_loss did not improve from 17.23792
196/196 - 75s - loss: 16.6383 - MinusLogProbMetric: 16.6383 - val_loss: 17.6410 - val_MinusLogProbMetric: 17.6410 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 339/1000
2023-09-27 16:42:39.260 
Epoch 339/1000 
	 loss: 16.6562, MinusLogProbMetric: 16.6562, val_loss: 17.3196, val_MinusLogProbMetric: 17.3196

Epoch 339: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6562 - MinusLogProbMetric: 16.6562 - val_loss: 17.3196 - val_MinusLogProbMetric: 17.3196 - lr: 1.6667e-04 - 76s/epoch - 389ms/step
Epoch 340/1000
2023-09-27 16:43:56.113 
Epoch 340/1000 
	 loss: 16.6158, MinusLogProbMetric: 16.6158, val_loss: 17.2858, val_MinusLogProbMetric: 17.2858

Epoch 340: val_loss did not improve from 17.23792
196/196 - 77s - loss: 16.6158 - MinusLogProbMetric: 16.6158 - val_loss: 17.2858 - val_MinusLogProbMetric: 17.2858 - lr: 1.6667e-04 - 77s/epoch - 392ms/step
Epoch 341/1000
2023-09-27 16:45:12.671 
Epoch 341/1000 
	 loss: 16.6212, MinusLogProbMetric: 16.6212, val_loss: 17.4960, val_MinusLogProbMetric: 17.4960

Epoch 341: val_loss did not improve from 17.23792
196/196 - 77s - loss: 16.6212 - MinusLogProbMetric: 16.6212 - val_loss: 17.4960 - val_MinusLogProbMetric: 17.4960 - lr: 1.6667e-04 - 77s/epoch - 391ms/step
Epoch 342/1000
2023-09-27 16:46:29.431 
Epoch 342/1000 
	 loss: 16.6440, MinusLogProbMetric: 16.6440, val_loss: 17.3181, val_MinusLogProbMetric: 17.3181

Epoch 342: val_loss did not improve from 17.23792
196/196 - 77s - loss: 16.6440 - MinusLogProbMetric: 16.6440 - val_loss: 17.3181 - val_MinusLogProbMetric: 17.3181 - lr: 1.6667e-04 - 77s/epoch - 392ms/step
Epoch 343/1000
2023-09-27 16:47:44.981 
Epoch 343/1000 
	 loss: 16.6437, MinusLogProbMetric: 16.6437, val_loss: 17.3333, val_MinusLogProbMetric: 17.3333

Epoch 343: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6437 - MinusLogProbMetric: 16.6437 - val_loss: 17.3333 - val_MinusLogProbMetric: 17.3333 - lr: 1.6667e-04 - 76s/epoch - 385ms/step
Epoch 344/1000
2023-09-27 16:49:00.943 
Epoch 344/1000 
	 loss: 16.6067, MinusLogProbMetric: 16.6067, val_loss: 17.3916, val_MinusLogProbMetric: 17.3916

Epoch 344: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6067 - MinusLogProbMetric: 16.6067 - val_loss: 17.3916 - val_MinusLogProbMetric: 17.3916 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 345/1000
2023-09-27 16:50:16.590 
Epoch 345/1000 
	 loss: 16.6234, MinusLogProbMetric: 16.6234, val_loss: 17.5403, val_MinusLogProbMetric: 17.5403

Epoch 345: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6234 - MinusLogProbMetric: 16.6234 - val_loss: 17.5403 - val_MinusLogProbMetric: 17.5403 - lr: 1.6667e-04 - 76s/epoch - 386ms/step
Epoch 346/1000
2023-09-27 16:51:32.330 
Epoch 346/1000 
	 loss: 16.6312, MinusLogProbMetric: 16.6312, val_loss: 17.3758, val_MinusLogProbMetric: 17.3758

Epoch 346: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6312 - MinusLogProbMetric: 16.6312 - val_loss: 17.3758 - val_MinusLogProbMetric: 17.3758 - lr: 1.6667e-04 - 76s/epoch - 386ms/step
Epoch 347/1000
2023-09-27 16:52:48.083 
Epoch 347/1000 
	 loss: 16.6328, MinusLogProbMetric: 16.6328, val_loss: 17.3952, val_MinusLogProbMetric: 17.3952

Epoch 347: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6328 - MinusLogProbMetric: 16.6328 - val_loss: 17.3952 - val_MinusLogProbMetric: 17.3952 - lr: 1.6667e-04 - 76s/epoch - 386ms/step
Epoch 348/1000
2023-09-27 16:54:03.819 
Epoch 348/1000 
	 loss: 16.6190, MinusLogProbMetric: 16.6190, val_loss: 17.7795, val_MinusLogProbMetric: 17.7795

Epoch 348: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6190 - MinusLogProbMetric: 16.6190 - val_loss: 17.7795 - val_MinusLogProbMetric: 17.7795 - lr: 1.6667e-04 - 76s/epoch - 386ms/step
Epoch 349/1000
2023-09-27 16:55:19.961 
Epoch 349/1000 
	 loss: 16.6574, MinusLogProbMetric: 16.6574, val_loss: 17.2928, val_MinusLogProbMetric: 17.2928

Epoch 349: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6574 - MinusLogProbMetric: 16.6574 - val_loss: 17.2928 - val_MinusLogProbMetric: 17.2928 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 350/1000
2023-09-27 16:56:36.652 
Epoch 350/1000 
	 loss: 16.6054, MinusLogProbMetric: 16.6054, val_loss: 17.2953, val_MinusLogProbMetric: 17.2953

Epoch 350: val_loss did not improve from 17.23792
196/196 - 77s - loss: 16.6054 - MinusLogProbMetric: 16.6054 - val_loss: 17.2953 - val_MinusLogProbMetric: 17.2953 - lr: 1.6667e-04 - 77s/epoch - 391ms/step
Epoch 351/1000
2023-09-27 16:57:52.560 
Epoch 351/1000 
	 loss: 16.6246, MinusLogProbMetric: 16.6246, val_loss: 17.3365, val_MinusLogProbMetric: 17.3365

Epoch 351: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6246 - MinusLogProbMetric: 16.6246 - val_loss: 17.3365 - val_MinusLogProbMetric: 17.3365 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 352/1000
2023-09-27 16:59:08.786 
Epoch 352/1000 
	 loss: 16.6121, MinusLogProbMetric: 16.6121, val_loss: 17.3270, val_MinusLogProbMetric: 17.3270

Epoch 352: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6121 - MinusLogProbMetric: 16.6121 - val_loss: 17.3270 - val_MinusLogProbMetric: 17.3270 - lr: 1.6667e-04 - 76s/epoch - 389ms/step
Epoch 353/1000
2023-09-27 17:00:24.816 
Epoch 353/1000 
	 loss: 16.6579, MinusLogProbMetric: 16.6579, val_loss: 17.3151, val_MinusLogProbMetric: 17.3151

Epoch 353: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6579 - MinusLogProbMetric: 16.6579 - val_loss: 17.3151 - val_MinusLogProbMetric: 17.3151 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 354/1000
2023-09-27 17:01:40.736 
Epoch 354/1000 
	 loss: 16.6283, MinusLogProbMetric: 16.6283, val_loss: 17.4432, val_MinusLogProbMetric: 17.4432

Epoch 354: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6283 - MinusLogProbMetric: 16.6283 - val_loss: 17.4432 - val_MinusLogProbMetric: 17.4432 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 355/1000
2023-09-27 17:02:57.138 
Epoch 355/1000 
	 loss: 16.6193, MinusLogProbMetric: 16.6193, val_loss: 17.3643, val_MinusLogProbMetric: 17.3643

Epoch 355: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.6193 - MinusLogProbMetric: 16.6193 - val_loss: 17.3643 - val_MinusLogProbMetric: 17.3643 - lr: 1.6667e-04 - 76s/epoch - 390ms/step
Epoch 356/1000
2023-09-27 17:04:12.182 
Epoch 356/1000 
	 loss: 16.6134, MinusLogProbMetric: 16.6134, val_loss: 17.3395, val_MinusLogProbMetric: 17.3395

Epoch 356: val_loss did not improve from 17.23792
196/196 - 75s - loss: 16.6134 - MinusLogProbMetric: 16.6134 - val_loss: 17.3395 - val_MinusLogProbMetric: 17.3395 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 357/1000
2023-09-27 17:05:27.829 
Epoch 357/1000 
	 loss: 16.4826, MinusLogProbMetric: 16.4826, val_loss: 17.2607, val_MinusLogProbMetric: 17.2607

Epoch 357: val_loss did not improve from 17.23792
196/196 - 76s - loss: 16.4826 - MinusLogProbMetric: 16.4826 - val_loss: 17.2607 - val_MinusLogProbMetric: 17.2607 - lr: 8.3333e-05 - 76s/epoch - 386ms/step
Epoch 358/1000
2023-09-27 17:06:37.207 
Epoch 358/1000 
	 loss: 16.4714, MinusLogProbMetric: 16.4714, val_loss: 17.2161, val_MinusLogProbMetric: 17.2161

Epoch 358: val_loss improved from 17.23792 to 17.21614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 70s - loss: 16.4714 - MinusLogProbMetric: 16.4714 - val_loss: 17.2161 - val_MinusLogProbMetric: 17.2161 - lr: 8.3333e-05 - 70s/epoch - 360ms/step
Epoch 359/1000
2023-09-27 17:07:43.011 
Epoch 359/1000 
	 loss: 16.4661, MinusLogProbMetric: 16.4661, val_loss: 17.2078, val_MinusLogProbMetric: 17.2078

Epoch 359: val_loss improved from 17.21614 to 17.20782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 66s - loss: 16.4661 - MinusLogProbMetric: 16.4661 - val_loss: 17.2078 - val_MinusLogProbMetric: 17.2078 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 360/1000
2023-09-27 17:08:48.553 
Epoch 360/1000 
	 loss: 16.4798, MinusLogProbMetric: 16.4798, val_loss: 17.2141, val_MinusLogProbMetric: 17.2141

Epoch 360: val_loss did not improve from 17.20782
196/196 - 65s - loss: 16.4798 - MinusLogProbMetric: 16.4798 - val_loss: 17.2141 - val_MinusLogProbMetric: 17.2141 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 361/1000
2023-09-27 17:10:00.735 
Epoch 361/1000 
	 loss: 16.4598, MinusLogProbMetric: 16.4598, val_loss: 17.2293, val_MinusLogProbMetric: 17.2293

Epoch 361: val_loss did not improve from 17.20782
196/196 - 72s - loss: 16.4598 - MinusLogProbMetric: 16.4598 - val_loss: 17.2293 - val_MinusLogProbMetric: 17.2293 - lr: 8.3333e-05 - 72s/epoch - 368ms/step
Epoch 362/1000
2023-09-27 17:11:04.405 
Epoch 362/1000 
	 loss: 16.4793, MinusLogProbMetric: 16.4793, val_loss: 17.2543, val_MinusLogProbMetric: 17.2543

Epoch 362: val_loss did not improve from 17.20782
196/196 - 64s - loss: 16.4793 - MinusLogProbMetric: 16.4793 - val_loss: 17.2543 - val_MinusLogProbMetric: 17.2543 - lr: 8.3333e-05 - 64s/epoch - 325ms/step
Epoch 363/1000
2023-09-27 17:12:12.685 
Epoch 363/1000 
	 loss: 16.4589, MinusLogProbMetric: 16.4589, val_loss: 17.2154, val_MinusLogProbMetric: 17.2154

Epoch 363: val_loss did not improve from 17.20782
196/196 - 68s - loss: 16.4589 - MinusLogProbMetric: 16.4589 - val_loss: 17.2154 - val_MinusLogProbMetric: 17.2154 - lr: 8.3333e-05 - 68s/epoch - 348ms/step
Epoch 364/1000
2023-09-27 17:13:28.069 
Epoch 364/1000 
	 loss: 16.4558, MinusLogProbMetric: 16.4558, val_loss: 17.2393, val_MinusLogProbMetric: 17.2393

Epoch 364: val_loss did not improve from 17.20782
196/196 - 75s - loss: 16.4558 - MinusLogProbMetric: 16.4558 - val_loss: 17.2393 - val_MinusLogProbMetric: 17.2393 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 365/1000
2023-09-27 17:14:42.140 
Epoch 365/1000 
	 loss: 16.4604, MinusLogProbMetric: 16.4604, val_loss: 17.1796, val_MinusLogProbMetric: 17.1796

Epoch 365: val_loss improved from 17.20782 to 17.17956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.4604 - MinusLogProbMetric: 16.4604 - val_loss: 17.1796 - val_MinusLogProbMetric: 17.1796 - lr: 8.3333e-05 - 76s/epoch - 386ms/step
Epoch 366/1000
2023-09-27 17:15:59.704 
Epoch 366/1000 
	 loss: 16.4748, MinusLogProbMetric: 16.4748, val_loss: 17.2910, val_MinusLogProbMetric: 17.2910

Epoch 366: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4748 - MinusLogProbMetric: 16.4748 - val_loss: 17.2910 - val_MinusLogProbMetric: 17.2910 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 367/1000
2023-09-27 17:17:16.197 
Epoch 367/1000 
	 loss: 16.4960, MinusLogProbMetric: 16.4960, val_loss: 17.3329, val_MinusLogProbMetric: 17.3329

Epoch 367: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4960 - MinusLogProbMetric: 16.4960 - val_loss: 17.3329 - val_MinusLogProbMetric: 17.3329 - lr: 8.3333e-05 - 76s/epoch - 390ms/step
Epoch 368/1000
2023-09-27 17:18:31.828 
Epoch 368/1000 
	 loss: 16.4586, MinusLogProbMetric: 16.4586, val_loss: 17.2600, val_MinusLogProbMetric: 17.2600

Epoch 368: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4586 - MinusLogProbMetric: 16.4586 - val_loss: 17.2600 - val_MinusLogProbMetric: 17.2600 - lr: 8.3333e-05 - 76s/epoch - 386ms/step
Epoch 369/1000
2023-09-27 17:19:47.939 
Epoch 369/1000 
	 loss: 16.4625, MinusLogProbMetric: 16.4625, val_loss: 17.2909, val_MinusLogProbMetric: 17.2909

Epoch 369: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4625 - MinusLogProbMetric: 16.4625 - val_loss: 17.2909 - val_MinusLogProbMetric: 17.2909 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 370/1000
2023-09-27 17:21:03.749 
Epoch 370/1000 
	 loss: 16.4666, MinusLogProbMetric: 16.4666, val_loss: 17.2530, val_MinusLogProbMetric: 17.2530

Epoch 370: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4666 - MinusLogProbMetric: 16.4666 - val_loss: 17.2530 - val_MinusLogProbMetric: 17.2530 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 371/1000
2023-09-27 17:22:19.297 
Epoch 371/1000 
	 loss: 16.4728, MinusLogProbMetric: 16.4728, val_loss: 17.2287, val_MinusLogProbMetric: 17.2287

Epoch 371: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4728 - MinusLogProbMetric: 16.4728 - val_loss: 17.2287 - val_MinusLogProbMetric: 17.2287 - lr: 8.3333e-05 - 76s/epoch - 385ms/step
Epoch 372/1000
2023-09-27 17:23:35.953 
Epoch 372/1000 
	 loss: 16.4724, MinusLogProbMetric: 16.4724, val_loss: 17.2551, val_MinusLogProbMetric: 17.2551

Epoch 372: val_loss did not improve from 17.17956
196/196 - 77s - loss: 16.4724 - MinusLogProbMetric: 16.4724 - val_loss: 17.2551 - val_MinusLogProbMetric: 17.2551 - lr: 8.3333e-05 - 77s/epoch - 391ms/step
Epoch 373/1000
2023-09-27 17:24:51.934 
Epoch 373/1000 
	 loss: 16.4646, MinusLogProbMetric: 16.4646, val_loss: 17.2311, val_MinusLogProbMetric: 17.2311

Epoch 373: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4646 - MinusLogProbMetric: 16.4646 - val_loss: 17.2311 - val_MinusLogProbMetric: 17.2311 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 374/1000
2023-09-27 17:26:07.638 
Epoch 374/1000 
	 loss: 16.4682, MinusLogProbMetric: 16.4682, val_loss: 17.3361, val_MinusLogProbMetric: 17.3361

Epoch 374: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4682 - MinusLogProbMetric: 16.4682 - val_loss: 17.3361 - val_MinusLogProbMetric: 17.3361 - lr: 8.3333e-05 - 76s/epoch - 386ms/step
Epoch 375/1000
2023-09-27 17:27:23.444 
Epoch 375/1000 
	 loss: 16.4772, MinusLogProbMetric: 16.4772, val_loss: 17.3939, val_MinusLogProbMetric: 17.3939

Epoch 375: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4772 - MinusLogProbMetric: 16.4772 - val_loss: 17.3939 - val_MinusLogProbMetric: 17.3939 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 376/1000
2023-09-27 17:28:38.976 
Epoch 376/1000 
	 loss: 16.4506, MinusLogProbMetric: 16.4506, val_loss: 17.2644, val_MinusLogProbMetric: 17.2644

Epoch 376: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4506 - MinusLogProbMetric: 16.4506 - val_loss: 17.2644 - val_MinusLogProbMetric: 17.2644 - lr: 8.3333e-05 - 76s/epoch - 385ms/step
Epoch 377/1000
2023-09-27 17:29:54.176 
Epoch 377/1000 
	 loss: 16.4588, MinusLogProbMetric: 16.4588, val_loss: 17.2827, val_MinusLogProbMetric: 17.2827

Epoch 377: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4588 - MinusLogProbMetric: 16.4588 - val_loss: 17.2827 - val_MinusLogProbMetric: 17.2827 - lr: 8.3333e-05 - 75s/epoch - 384ms/step
Epoch 378/1000
2023-09-27 17:31:09.864 
Epoch 378/1000 
	 loss: 16.5053, MinusLogProbMetric: 16.5053, val_loss: 17.1896, val_MinusLogProbMetric: 17.1896

Epoch 378: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.5053 - MinusLogProbMetric: 16.5053 - val_loss: 17.1896 - val_MinusLogProbMetric: 17.1896 - lr: 8.3333e-05 - 76s/epoch - 386ms/step
Epoch 379/1000
2023-09-27 17:32:25.316 
Epoch 379/1000 
	 loss: 16.4540, MinusLogProbMetric: 16.4540, val_loss: 17.2236, val_MinusLogProbMetric: 17.2236

Epoch 379: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4540 - MinusLogProbMetric: 16.4540 - val_loss: 17.2236 - val_MinusLogProbMetric: 17.2236 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 380/1000
2023-09-27 17:33:40.656 
Epoch 380/1000 
	 loss: 16.4555, MinusLogProbMetric: 16.4555, val_loss: 17.2060, val_MinusLogProbMetric: 17.2060

Epoch 380: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4555 - MinusLogProbMetric: 16.4555 - val_loss: 17.2060 - val_MinusLogProbMetric: 17.2060 - lr: 8.3333e-05 - 75s/epoch - 384ms/step
Epoch 381/1000
2023-09-27 17:34:57.117 
Epoch 381/1000 
	 loss: 16.4958, MinusLogProbMetric: 16.4958, val_loss: 17.2602, val_MinusLogProbMetric: 17.2602

Epoch 381: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4958 - MinusLogProbMetric: 16.4958 - val_loss: 17.2602 - val_MinusLogProbMetric: 17.2602 - lr: 8.3333e-05 - 76s/epoch - 390ms/step
Epoch 382/1000
2023-09-27 17:36:12.933 
Epoch 382/1000 
	 loss: 16.4634, MinusLogProbMetric: 16.4634, val_loss: 17.2368, val_MinusLogProbMetric: 17.2368

Epoch 382: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4634 - MinusLogProbMetric: 16.4634 - val_loss: 17.2368 - val_MinusLogProbMetric: 17.2368 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 383/1000
2023-09-27 17:37:28.722 
Epoch 383/1000 
	 loss: 16.4679, MinusLogProbMetric: 16.4679, val_loss: 17.2382, val_MinusLogProbMetric: 17.2382

Epoch 383: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4679 - MinusLogProbMetric: 16.4679 - val_loss: 17.2382 - val_MinusLogProbMetric: 17.2382 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 384/1000
2023-09-27 17:38:44.614 
Epoch 384/1000 
	 loss: 16.4580, MinusLogProbMetric: 16.4580, val_loss: 17.2075, val_MinusLogProbMetric: 17.2075

Epoch 384: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4580 - MinusLogProbMetric: 16.4580 - val_loss: 17.2075 - val_MinusLogProbMetric: 17.2075 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 385/1000
2023-09-27 17:40:00.516 
Epoch 385/1000 
	 loss: 16.4551, MinusLogProbMetric: 16.4551, val_loss: 17.3018, val_MinusLogProbMetric: 17.3018

Epoch 385: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4551 - MinusLogProbMetric: 16.4551 - val_loss: 17.3018 - val_MinusLogProbMetric: 17.3018 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 386/1000
2023-09-27 17:41:16.614 
Epoch 386/1000 
	 loss: 16.4692, MinusLogProbMetric: 16.4692, val_loss: 17.2494, val_MinusLogProbMetric: 17.2494

Epoch 386: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4692 - MinusLogProbMetric: 16.4692 - val_loss: 17.2494 - val_MinusLogProbMetric: 17.2494 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 387/1000
2023-09-27 17:42:31.708 
Epoch 387/1000 
	 loss: 16.4534, MinusLogProbMetric: 16.4534, val_loss: 17.2138, val_MinusLogProbMetric: 17.2138

Epoch 387: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4534 - MinusLogProbMetric: 16.4534 - val_loss: 17.2138 - val_MinusLogProbMetric: 17.2138 - lr: 8.3333e-05 - 75s/epoch - 383ms/step
Epoch 388/1000
2023-09-27 17:43:47.226 
Epoch 388/1000 
	 loss: 16.4579, MinusLogProbMetric: 16.4579, val_loss: 17.1997, val_MinusLogProbMetric: 17.1997

Epoch 388: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4579 - MinusLogProbMetric: 16.4579 - val_loss: 17.1997 - val_MinusLogProbMetric: 17.1997 - lr: 8.3333e-05 - 76s/epoch - 385ms/step
Epoch 389/1000
2023-09-27 17:45:03.107 
Epoch 389/1000 
	 loss: 16.4395, MinusLogProbMetric: 16.4395, val_loss: 17.2675, val_MinusLogProbMetric: 17.2675

Epoch 389: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4395 - MinusLogProbMetric: 16.4395 - val_loss: 17.2675 - val_MinusLogProbMetric: 17.2675 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 390/1000
2023-09-27 17:46:18.610 
Epoch 390/1000 
	 loss: 16.4717, MinusLogProbMetric: 16.4717, val_loss: 17.2050, val_MinusLogProbMetric: 17.2050

Epoch 390: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4717 - MinusLogProbMetric: 16.4717 - val_loss: 17.2050 - val_MinusLogProbMetric: 17.2050 - lr: 8.3333e-05 - 76s/epoch - 385ms/step
Epoch 391/1000
2023-09-27 17:47:34.066 
Epoch 391/1000 
	 loss: 16.4504, MinusLogProbMetric: 16.4504, val_loss: 17.2639, val_MinusLogProbMetric: 17.2639

Epoch 391: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4504 - MinusLogProbMetric: 16.4504 - val_loss: 17.2639 - val_MinusLogProbMetric: 17.2639 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 392/1000
2023-09-27 17:48:50.702 
Epoch 392/1000 
	 loss: 16.4498, MinusLogProbMetric: 16.4498, val_loss: 17.2605, val_MinusLogProbMetric: 17.2605

Epoch 392: val_loss did not improve from 17.17956
196/196 - 77s - loss: 16.4498 - MinusLogProbMetric: 16.4498 - val_loss: 17.2605 - val_MinusLogProbMetric: 17.2605 - lr: 8.3333e-05 - 77s/epoch - 391ms/step
Epoch 393/1000
2023-09-27 17:50:06.858 
Epoch 393/1000 
	 loss: 16.4475, MinusLogProbMetric: 16.4475, val_loss: 17.3124, val_MinusLogProbMetric: 17.3124

Epoch 393: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4475 - MinusLogProbMetric: 16.4475 - val_loss: 17.3124 - val_MinusLogProbMetric: 17.3124 - lr: 8.3333e-05 - 76s/epoch - 389ms/step
Epoch 394/1000
2023-09-27 17:51:22.324 
Epoch 394/1000 
	 loss: 16.4756, MinusLogProbMetric: 16.4756, val_loss: 17.2182, val_MinusLogProbMetric: 17.2182

Epoch 394: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4756 - MinusLogProbMetric: 16.4756 - val_loss: 17.2182 - val_MinusLogProbMetric: 17.2182 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 395/1000
2023-09-27 17:52:37.769 
Epoch 395/1000 
	 loss: 16.4637, MinusLogProbMetric: 16.4637, val_loss: 17.3942, val_MinusLogProbMetric: 17.3942

Epoch 395: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4637 - MinusLogProbMetric: 16.4637 - val_loss: 17.3942 - val_MinusLogProbMetric: 17.3942 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 396/1000
2023-09-27 17:53:52.678 
Epoch 396/1000 
	 loss: 16.4451, MinusLogProbMetric: 16.4451, val_loss: 17.2502, val_MinusLogProbMetric: 17.2502

Epoch 396: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4451 - MinusLogProbMetric: 16.4451 - val_loss: 17.2502 - val_MinusLogProbMetric: 17.2502 - lr: 8.3333e-05 - 75s/epoch - 382ms/step
Epoch 397/1000
2023-09-27 17:55:09.127 
Epoch 397/1000 
	 loss: 16.4520, MinusLogProbMetric: 16.4520, val_loss: 17.2319, val_MinusLogProbMetric: 17.2319

Epoch 397: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4520 - MinusLogProbMetric: 16.4520 - val_loss: 17.2319 - val_MinusLogProbMetric: 17.2319 - lr: 8.3333e-05 - 76s/epoch - 390ms/step
Epoch 398/1000
2023-09-27 17:56:24.953 
Epoch 398/1000 
	 loss: 16.4579, MinusLogProbMetric: 16.4579, val_loss: 17.3405, val_MinusLogProbMetric: 17.3405

Epoch 398: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4579 - MinusLogProbMetric: 16.4579 - val_loss: 17.3405 - val_MinusLogProbMetric: 17.3405 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 399/1000
2023-09-27 17:57:41.095 
Epoch 399/1000 
	 loss: 16.4470, MinusLogProbMetric: 16.4470, val_loss: 17.2093, val_MinusLogProbMetric: 17.2093

Epoch 399: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4470 - MinusLogProbMetric: 16.4470 - val_loss: 17.2093 - val_MinusLogProbMetric: 17.2093 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 400/1000
2023-09-27 17:58:56.910 
Epoch 400/1000 
	 loss: 16.4503, MinusLogProbMetric: 16.4503, val_loss: 17.2188, val_MinusLogProbMetric: 17.2188

Epoch 400: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4503 - MinusLogProbMetric: 16.4503 - val_loss: 17.2188 - val_MinusLogProbMetric: 17.2188 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 401/1000
2023-09-27 18:00:13.169 
Epoch 401/1000 
	 loss: 16.4417, MinusLogProbMetric: 16.4417, val_loss: 17.2219, val_MinusLogProbMetric: 17.2219

Epoch 401: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4417 - MinusLogProbMetric: 16.4417 - val_loss: 17.2219 - val_MinusLogProbMetric: 17.2219 - lr: 8.3333e-05 - 76s/epoch - 389ms/step
Epoch 402/1000
2023-09-27 18:01:29.204 
Epoch 402/1000 
	 loss: 16.4868, MinusLogProbMetric: 16.4868, val_loss: 17.5242, val_MinusLogProbMetric: 17.5242

Epoch 402: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4868 - MinusLogProbMetric: 16.4868 - val_loss: 17.5242 - val_MinusLogProbMetric: 17.5242 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 403/1000
2023-09-27 18:02:44.852 
Epoch 403/1000 
	 loss: 16.4745, MinusLogProbMetric: 16.4745, val_loss: 17.1936, val_MinusLogProbMetric: 17.1936

Epoch 403: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4745 - MinusLogProbMetric: 16.4745 - val_loss: 17.1936 - val_MinusLogProbMetric: 17.1936 - lr: 8.3333e-05 - 76s/epoch - 386ms/step
Epoch 404/1000
2023-09-27 18:04:00.284 
Epoch 404/1000 
	 loss: 16.4421, MinusLogProbMetric: 16.4421, val_loss: 17.2588, val_MinusLogProbMetric: 17.2588

Epoch 404: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4421 - MinusLogProbMetric: 16.4421 - val_loss: 17.2588 - val_MinusLogProbMetric: 17.2588 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 405/1000
2023-09-27 18:05:16.133 
Epoch 405/1000 
	 loss: 16.4294, MinusLogProbMetric: 16.4294, val_loss: 17.2333, val_MinusLogProbMetric: 17.2333

Epoch 405: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4294 - MinusLogProbMetric: 16.4294 - val_loss: 17.2333 - val_MinusLogProbMetric: 17.2333 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 406/1000
2023-09-27 18:06:32.341 
Epoch 406/1000 
	 loss: 16.4812, MinusLogProbMetric: 16.4812, val_loss: 17.4513, val_MinusLogProbMetric: 17.4513

Epoch 406: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4812 - MinusLogProbMetric: 16.4812 - val_loss: 17.4513 - val_MinusLogProbMetric: 17.4513 - lr: 8.3333e-05 - 76s/epoch - 389ms/step
Epoch 407/1000
2023-09-27 18:07:47.812 
Epoch 407/1000 
	 loss: 16.4662, MinusLogProbMetric: 16.4662, val_loss: 17.3450, val_MinusLogProbMetric: 17.3450

Epoch 407: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4662 - MinusLogProbMetric: 16.4662 - val_loss: 17.3450 - val_MinusLogProbMetric: 17.3450 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 408/1000
2023-09-27 18:09:04.058 
Epoch 408/1000 
	 loss: 16.4517, MinusLogProbMetric: 16.4517, val_loss: 17.2629, val_MinusLogProbMetric: 17.2629

Epoch 408: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4517 - MinusLogProbMetric: 16.4517 - val_loss: 17.2629 - val_MinusLogProbMetric: 17.2629 - lr: 8.3333e-05 - 76s/epoch - 389ms/step
Epoch 409/1000
2023-09-27 18:10:19.378 
Epoch 409/1000 
	 loss: 16.4824, MinusLogProbMetric: 16.4824, val_loss: 17.2559, val_MinusLogProbMetric: 17.2559

Epoch 409: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4824 - MinusLogProbMetric: 16.4824 - val_loss: 17.2559 - val_MinusLogProbMetric: 17.2559 - lr: 8.3333e-05 - 75s/epoch - 384ms/step
Epoch 410/1000
2023-09-27 18:11:35.194 
Epoch 410/1000 
	 loss: 16.4313, MinusLogProbMetric: 16.4313, val_loss: 17.2412, val_MinusLogProbMetric: 17.2412

Epoch 410: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4313 - MinusLogProbMetric: 16.4313 - val_loss: 17.2412 - val_MinusLogProbMetric: 17.2412 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 411/1000
2023-09-27 18:12:51.338 
Epoch 411/1000 
	 loss: 16.4544, MinusLogProbMetric: 16.4544, val_loss: 17.1924, val_MinusLogProbMetric: 17.1924

Epoch 411: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4544 - MinusLogProbMetric: 16.4544 - val_loss: 17.1924 - val_MinusLogProbMetric: 17.1924 - lr: 8.3333e-05 - 76s/epoch - 389ms/step
Epoch 412/1000
2023-09-27 18:14:06.721 
Epoch 412/1000 
	 loss: 16.4456, MinusLogProbMetric: 16.4456, val_loss: 17.2151, val_MinusLogProbMetric: 17.2151

Epoch 412: val_loss did not improve from 17.17956
196/196 - 75s - loss: 16.4456 - MinusLogProbMetric: 16.4456 - val_loss: 17.2151 - val_MinusLogProbMetric: 17.2151 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 413/1000
2023-09-27 18:15:22.530 
Epoch 413/1000 
	 loss: 16.4506, MinusLogProbMetric: 16.4506, val_loss: 17.3573, val_MinusLogProbMetric: 17.3573

Epoch 413: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4506 - MinusLogProbMetric: 16.4506 - val_loss: 17.3573 - val_MinusLogProbMetric: 17.3573 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 414/1000
2023-09-27 18:16:38.533 
Epoch 414/1000 
	 loss: 16.4496, MinusLogProbMetric: 16.4496, val_loss: 17.2579, val_MinusLogProbMetric: 17.2579

Epoch 414: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4496 - MinusLogProbMetric: 16.4496 - val_loss: 17.2579 - val_MinusLogProbMetric: 17.2579 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 415/1000
2023-09-27 18:17:54.916 
Epoch 415/1000 
	 loss: 16.4310, MinusLogProbMetric: 16.4310, val_loss: 17.2554, val_MinusLogProbMetric: 17.2554

Epoch 415: val_loss did not improve from 17.17956
196/196 - 76s - loss: 16.4310 - MinusLogProbMetric: 16.4310 - val_loss: 17.2554 - val_MinusLogProbMetric: 17.2554 - lr: 8.3333e-05 - 76s/epoch - 390ms/step
Epoch 416/1000
2023-09-27 18:19:10.810 
Epoch 416/1000 
	 loss: 16.3686, MinusLogProbMetric: 16.3686, val_loss: 17.1765, val_MinusLogProbMetric: 17.1765

Epoch 416: val_loss improved from 17.17956 to 17.17651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 16.3686 - MinusLogProbMetric: 16.3686 - val_loss: 17.1765 - val_MinusLogProbMetric: 17.1765 - lr: 4.1667e-05 - 77s/epoch - 394ms/step
Epoch 417/1000
2023-09-27 18:20:27.816 
Epoch 417/1000 
	 loss: 16.3701, MinusLogProbMetric: 16.3701, val_loss: 17.1637, val_MinusLogProbMetric: 17.1637

Epoch 417: val_loss improved from 17.17651 to 17.16374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 16.3701 - MinusLogProbMetric: 16.3701 - val_loss: 17.1637 - val_MinusLogProbMetric: 17.1637 - lr: 4.1667e-05 - 77s/epoch - 392ms/step
Epoch 418/1000
2023-09-27 18:21:44.599 
Epoch 418/1000 
	 loss: 16.3713, MinusLogProbMetric: 16.3713, val_loss: 17.2016, val_MinusLogProbMetric: 17.2016

Epoch 418: val_loss did not improve from 17.16374
196/196 - 76s - loss: 16.3713 - MinusLogProbMetric: 16.3713 - val_loss: 17.2016 - val_MinusLogProbMetric: 17.2016 - lr: 4.1667e-05 - 76s/epoch - 385ms/step
Epoch 419/1000
2023-09-27 18:23:00.503 
Epoch 419/1000 
	 loss: 16.3600, MinusLogProbMetric: 16.3600, val_loss: 17.1889, val_MinusLogProbMetric: 17.1889

Epoch 419: val_loss did not improve from 17.16374
196/196 - 76s - loss: 16.3600 - MinusLogProbMetric: 16.3600 - val_loss: 17.1889 - val_MinusLogProbMetric: 17.1889 - lr: 4.1667e-05 - 76s/epoch - 387ms/step
Epoch 420/1000
2023-09-27 18:24:17.063 
Epoch 420/1000 
	 loss: 16.3694, MinusLogProbMetric: 16.3694, val_loss: 17.2092, val_MinusLogProbMetric: 17.2092

Epoch 420: val_loss did not improve from 17.16374
196/196 - 77s - loss: 16.3694 - MinusLogProbMetric: 16.3694 - val_loss: 17.2092 - val_MinusLogProbMetric: 17.2092 - lr: 4.1667e-05 - 77s/epoch - 391ms/step
Epoch 421/1000
2023-09-27 18:25:32.824 
Epoch 421/1000 
	 loss: 16.3656, MinusLogProbMetric: 16.3656, val_loss: 17.1707, val_MinusLogProbMetric: 17.1707

Epoch 421: val_loss did not improve from 17.16374
196/196 - 76s - loss: 16.3656 - MinusLogProbMetric: 16.3656 - val_loss: 17.1707 - val_MinusLogProbMetric: 17.1707 - lr: 4.1667e-05 - 76s/epoch - 387ms/step
Epoch 422/1000
2023-09-27 18:26:48.364 
Epoch 422/1000 
	 loss: 16.3684, MinusLogProbMetric: 16.3684, val_loss: 17.2059, val_MinusLogProbMetric: 17.2059

Epoch 422: val_loss did not improve from 17.16374
196/196 - 76s - loss: 16.3684 - MinusLogProbMetric: 16.3684 - val_loss: 17.2059 - val_MinusLogProbMetric: 17.2059 - lr: 4.1667e-05 - 76s/epoch - 385ms/step
Epoch 423/1000
2023-09-27 18:28:04.307 
Epoch 423/1000 
	 loss: 16.3654, MinusLogProbMetric: 16.3654, val_loss: 17.1668, val_MinusLogProbMetric: 17.1668

Epoch 423: val_loss did not improve from 17.16374
196/196 - 76s - loss: 16.3654 - MinusLogProbMetric: 16.3654 - val_loss: 17.1668 - val_MinusLogProbMetric: 17.1668 - lr: 4.1667e-05 - 76s/epoch - 387ms/step
Epoch 424/1000
2023-09-27 18:29:20.008 
Epoch 424/1000 
	 loss: 16.3699, MinusLogProbMetric: 16.3699, val_loss: 17.1686, val_MinusLogProbMetric: 17.1686

Epoch 424: val_loss did not improve from 17.16374
196/196 - 76s - loss: 16.3699 - MinusLogProbMetric: 16.3699 - val_loss: 17.1686 - val_MinusLogProbMetric: 17.1686 - lr: 4.1667e-05 - 76s/epoch - 386ms/step
Epoch 425/1000
2023-09-27 18:30:36.229 
Epoch 425/1000 
	 loss: 16.3594, MinusLogProbMetric: 16.3594, val_loss: 17.1558, val_MinusLogProbMetric: 17.1558

Epoch 425: val_loss improved from 17.16374 to 17.15578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 78s - loss: 16.3594 - MinusLogProbMetric: 16.3594 - val_loss: 17.1558 - val_MinusLogProbMetric: 17.1558 - lr: 4.1667e-05 - 78s/epoch - 396ms/step
Epoch 426/1000
2023-09-27 18:31:53.276 
Epoch 426/1000 
	 loss: 16.3624, MinusLogProbMetric: 16.3624, val_loss: 17.1999, val_MinusLogProbMetric: 17.1999

Epoch 426: val_loss did not improve from 17.15578
196/196 - 76s - loss: 16.3624 - MinusLogProbMetric: 16.3624 - val_loss: 17.1999 - val_MinusLogProbMetric: 17.1999 - lr: 4.1667e-05 - 76s/epoch - 386ms/step
Epoch 427/1000
2023-09-27 18:33:09.142 
Epoch 427/1000 
	 loss: 16.3668, MinusLogProbMetric: 16.3668, val_loss: 17.2021, val_MinusLogProbMetric: 17.2021

Epoch 427: val_loss did not improve from 17.15578
196/196 - 76s - loss: 16.3668 - MinusLogProbMetric: 16.3668 - val_loss: 17.2021 - val_MinusLogProbMetric: 17.2021 - lr: 4.1667e-05 - 76s/epoch - 387ms/step
Epoch 428/1000
2023-09-27 18:34:20.898 
Epoch 428/1000 
	 loss: 16.3783, MinusLogProbMetric: 16.3783, val_loss: 17.3168, val_MinusLogProbMetric: 17.3168

Epoch 428: val_loss did not improve from 17.15578
196/196 - 72s - loss: 16.3783 - MinusLogProbMetric: 16.3783 - val_loss: 17.3168 - val_MinusLogProbMetric: 17.3168 - lr: 4.1667e-05 - 72s/epoch - 366ms/step
Epoch 429/1000
2023-09-27 18:35:28.704 
Epoch 429/1000 
	 loss: 16.3629, MinusLogProbMetric: 16.3629, val_loss: 17.2486, val_MinusLogProbMetric: 17.2486

Epoch 429: val_loss did not improve from 17.15578
196/196 - 68s - loss: 16.3629 - MinusLogProbMetric: 16.3629 - val_loss: 17.2486 - val_MinusLogProbMetric: 17.2486 - lr: 4.1667e-05 - 68s/epoch - 346ms/step
Epoch 430/1000
2023-09-27 18:36:39.982 
Epoch 430/1000 
	 loss: 16.3596, MinusLogProbMetric: 16.3596, val_loss: 17.1643, val_MinusLogProbMetric: 17.1643

Epoch 430: val_loss did not improve from 17.15578
196/196 - 71s - loss: 16.3596 - MinusLogProbMetric: 16.3596 - val_loss: 17.1643 - val_MinusLogProbMetric: 17.1643 - lr: 4.1667e-05 - 71s/epoch - 364ms/step
Epoch 431/1000
2023-09-27 18:37:51.620 
Epoch 431/1000 
	 loss: 16.3569, MinusLogProbMetric: 16.3569, val_loss: 17.1628, val_MinusLogProbMetric: 17.1628

Epoch 431: val_loss did not improve from 17.15578
196/196 - 72s - loss: 16.3569 - MinusLogProbMetric: 16.3569 - val_loss: 17.1628 - val_MinusLogProbMetric: 17.1628 - lr: 4.1667e-05 - 72s/epoch - 365ms/step
Epoch 432/1000
2023-09-27 18:38:58.579 
Epoch 432/1000 
	 loss: 16.3574, MinusLogProbMetric: 16.3574, val_loss: 17.1717, val_MinusLogProbMetric: 17.1717

Epoch 432: val_loss did not improve from 17.15578
196/196 - 67s - loss: 16.3574 - MinusLogProbMetric: 16.3574 - val_loss: 17.1717 - val_MinusLogProbMetric: 17.1717 - lr: 4.1667e-05 - 67s/epoch - 342ms/step
Epoch 433/1000
2023-09-27 18:40:11.606 
Epoch 433/1000 
	 loss: 16.3522, MinusLogProbMetric: 16.3522, val_loss: 17.1637, val_MinusLogProbMetric: 17.1637

Epoch 433: val_loss did not improve from 17.15578
196/196 - 73s - loss: 16.3522 - MinusLogProbMetric: 16.3522 - val_loss: 17.1637 - val_MinusLogProbMetric: 17.1637 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 434/1000
2023-09-27 18:41:26.131 
Epoch 434/1000 
	 loss: 16.3644, MinusLogProbMetric: 16.3644, val_loss: 17.1571, val_MinusLogProbMetric: 17.1571

Epoch 434: val_loss did not improve from 17.15578
196/196 - 75s - loss: 16.3644 - MinusLogProbMetric: 16.3644 - val_loss: 17.1571 - val_MinusLogProbMetric: 17.1571 - lr: 4.1667e-05 - 75s/epoch - 380ms/step
Epoch 435/1000
2023-09-27 18:42:40.841 
Epoch 435/1000 
	 loss: 16.3515, MinusLogProbMetric: 16.3515, val_loss: 17.1885, val_MinusLogProbMetric: 17.1885

Epoch 435: val_loss did not improve from 17.15578
196/196 - 75s - loss: 16.3515 - MinusLogProbMetric: 16.3515 - val_loss: 17.1885 - val_MinusLogProbMetric: 17.1885 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 436/1000
2023-09-27 18:43:55.345 
Epoch 436/1000 
	 loss: 16.3668, MinusLogProbMetric: 16.3668, val_loss: 17.1813, val_MinusLogProbMetric: 17.1813

Epoch 436: val_loss did not improve from 17.15578
196/196 - 75s - loss: 16.3668 - MinusLogProbMetric: 16.3668 - val_loss: 17.1813 - val_MinusLogProbMetric: 17.1813 - lr: 4.1667e-05 - 75s/epoch - 380ms/step
Epoch 437/1000
2023-09-27 18:45:10.024 
Epoch 437/1000 
	 loss: 16.3619, MinusLogProbMetric: 16.3619, val_loss: 17.1667, val_MinusLogProbMetric: 17.1667

Epoch 437: val_loss did not improve from 17.15578
196/196 - 75s - loss: 16.3619 - MinusLogProbMetric: 16.3619 - val_loss: 17.1667 - val_MinusLogProbMetric: 17.1667 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 438/1000
2023-09-27 18:46:24.257 
Epoch 438/1000 
	 loss: 16.3590, MinusLogProbMetric: 16.3590, val_loss: 17.1840, val_MinusLogProbMetric: 17.1840

Epoch 438: val_loss did not improve from 17.15578
196/196 - 74s - loss: 16.3590 - MinusLogProbMetric: 16.3590 - val_loss: 17.1840 - val_MinusLogProbMetric: 17.1840 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 439/1000
2023-09-27 18:47:38.718 
Epoch 439/1000 
	 loss: 16.3555, MinusLogProbMetric: 16.3555, val_loss: 17.1713, val_MinusLogProbMetric: 17.1713

Epoch 439: val_loss did not improve from 17.15578
196/196 - 74s - loss: 16.3555 - MinusLogProbMetric: 16.3555 - val_loss: 17.1713 - val_MinusLogProbMetric: 17.1713 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 440/1000
2023-09-27 18:48:53.582 
Epoch 440/1000 
	 loss: 16.3594, MinusLogProbMetric: 16.3594, val_loss: 17.1714, val_MinusLogProbMetric: 17.1714

Epoch 440: val_loss did not improve from 17.15578
196/196 - 75s - loss: 16.3594 - MinusLogProbMetric: 16.3594 - val_loss: 17.1714 - val_MinusLogProbMetric: 17.1714 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 441/1000
2023-09-27 18:50:08.266 
Epoch 441/1000 
	 loss: 16.3643, MinusLogProbMetric: 16.3643, val_loss: 17.1887, val_MinusLogProbMetric: 17.1887

Epoch 441: val_loss did not improve from 17.15578
196/196 - 75s - loss: 16.3643 - MinusLogProbMetric: 16.3643 - val_loss: 17.1887 - val_MinusLogProbMetric: 17.1887 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 442/1000
2023-09-27 18:51:23.204 
Epoch 442/1000 
	 loss: 16.3739, MinusLogProbMetric: 16.3739, val_loss: 17.1552, val_MinusLogProbMetric: 17.1552

Epoch 442: val_loss improved from 17.15578 to 17.15520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.3739 - MinusLogProbMetric: 16.3739 - val_loss: 17.1552 - val_MinusLogProbMetric: 17.1552 - lr: 4.1667e-05 - 76s/epoch - 388ms/step
Epoch 443/1000
2023-09-27 18:52:38.560 
Epoch 443/1000 
	 loss: 16.3775, MinusLogProbMetric: 16.3775, val_loss: 17.1717, val_MinusLogProbMetric: 17.1717

Epoch 443: val_loss did not improve from 17.15520
196/196 - 74s - loss: 16.3775 - MinusLogProbMetric: 16.3775 - val_loss: 17.1717 - val_MinusLogProbMetric: 17.1717 - lr: 4.1667e-05 - 74s/epoch - 378ms/step
Epoch 444/1000
2023-09-27 18:53:52.468 
Epoch 444/1000 
	 loss: 16.3538, MinusLogProbMetric: 16.3538, val_loss: 17.1741, val_MinusLogProbMetric: 17.1741

Epoch 444: val_loss did not improve from 17.15520
196/196 - 74s - loss: 16.3538 - MinusLogProbMetric: 16.3538 - val_loss: 17.1741 - val_MinusLogProbMetric: 17.1741 - lr: 4.1667e-05 - 74s/epoch - 377ms/step
Epoch 445/1000
2023-09-27 18:55:06.967 
Epoch 445/1000 
	 loss: 16.3634, MinusLogProbMetric: 16.3634, val_loss: 17.1549, val_MinusLogProbMetric: 17.1549

Epoch 445: val_loss improved from 17.15520 to 17.15487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.3634 - MinusLogProbMetric: 16.3634 - val_loss: 17.1549 - val_MinusLogProbMetric: 17.1549 - lr: 4.1667e-05 - 76s/epoch - 387ms/step
Epoch 446/1000
2023-09-27 18:56:22.723 
Epoch 446/1000 
	 loss: 16.3559, MinusLogProbMetric: 16.3559, val_loss: 17.1886, val_MinusLogProbMetric: 17.1886

Epoch 446: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3559 - MinusLogProbMetric: 16.3559 - val_loss: 17.1886 - val_MinusLogProbMetric: 17.1886 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 447/1000
2023-09-27 18:57:36.967 
Epoch 447/1000 
	 loss: 16.3683, MinusLogProbMetric: 16.3683, val_loss: 17.1648, val_MinusLogProbMetric: 17.1648

Epoch 447: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3683 - MinusLogProbMetric: 16.3683 - val_loss: 17.1648 - val_MinusLogProbMetric: 17.1648 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 448/1000
2023-09-27 18:58:51.391 
Epoch 448/1000 
	 loss: 16.3569, MinusLogProbMetric: 16.3569, val_loss: 17.1761, val_MinusLogProbMetric: 17.1761

Epoch 448: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3569 - MinusLogProbMetric: 16.3569 - val_loss: 17.1761 - val_MinusLogProbMetric: 17.1761 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 449/1000
2023-09-27 19:00:05.597 
Epoch 449/1000 
	 loss: 16.3606, MinusLogProbMetric: 16.3606, val_loss: 17.1857, val_MinusLogProbMetric: 17.1857

Epoch 449: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3606 - MinusLogProbMetric: 16.3606 - val_loss: 17.1857 - val_MinusLogProbMetric: 17.1857 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 450/1000
2023-09-27 19:01:20.563 
Epoch 450/1000 
	 loss: 16.3525, MinusLogProbMetric: 16.3525, val_loss: 17.1855, val_MinusLogProbMetric: 17.1855

Epoch 450: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3525 - MinusLogProbMetric: 16.3525 - val_loss: 17.1855 - val_MinusLogProbMetric: 17.1855 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 451/1000
2023-09-27 19:02:35.514 
Epoch 451/1000 
	 loss: 16.3593, MinusLogProbMetric: 16.3593, val_loss: 17.1824, val_MinusLogProbMetric: 17.1824

Epoch 451: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3593 - MinusLogProbMetric: 16.3593 - val_loss: 17.1824 - val_MinusLogProbMetric: 17.1824 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 452/1000
2023-09-27 19:03:50.394 
Epoch 452/1000 
	 loss: 16.3672, MinusLogProbMetric: 16.3672, val_loss: 17.1619, val_MinusLogProbMetric: 17.1619

Epoch 452: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3672 - MinusLogProbMetric: 16.3672 - val_loss: 17.1619 - val_MinusLogProbMetric: 17.1619 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 453/1000
2023-09-27 19:05:04.789 
Epoch 453/1000 
	 loss: 16.3544, MinusLogProbMetric: 16.3544, val_loss: 17.1626, val_MinusLogProbMetric: 17.1626

Epoch 453: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3544 - MinusLogProbMetric: 16.3544 - val_loss: 17.1626 - val_MinusLogProbMetric: 17.1626 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 454/1000
2023-09-27 19:06:18.792 
Epoch 454/1000 
	 loss: 16.3615, MinusLogProbMetric: 16.3615, val_loss: 17.2109, val_MinusLogProbMetric: 17.2109

Epoch 454: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3615 - MinusLogProbMetric: 16.3615 - val_loss: 17.2109 - val_MinusLogProbMetric: 17.2109 - lr: 4.1667e-05 - 74s/epoch - 378ms/step
Epoch 455/1000
2023-09-27 19:07:33.448 
Epoch 455/1000 
	 loss: 16.3543, MinusLogProbMetric: 16.3543, val_loss: 17.1682, val_MinusLogProbMetric: 17.1682

Epoch 455: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3543 - MinusLogProbMetric: 16.3543 - val_loss: 17.1682 - val_MinusLogProbMetric: 17.1682 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 456/1000
2023-09-27 19:08:48.794 
Epoch 456/1000 
	 loss: 16.3557, MinusLogProbMetric: 16.3557, val_loss: 17.2123, val_MinusLogProbMetric: 17.2123

Epoch 456: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3557 - MinusLogProbMetric: 16.3557 - val_loss: 17.2123 - val_MinusLogProbMetric: 17.2123 - lr: 4.1667e-05 - 75s/epoch - 384ms/step
Epoch 457/1000
2023-09-27 19:10:03.815 
Epoch 457/1000 
	 loss: 16.3544, MinusLogProbMetric: 16.3544, val_loss: 17.2085, val_MinusLogProbMetric: 17.2085

Epoch 457: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3544 - MinusLogProbMetric: 16.3544 - val_loss: 17.2085 - val_MinusLogProbMetric: 17.2085 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 458/1000
2023-09-27 19:11:18.700 
Epoch 458/1000 
	 loss: 16.3658, MinusLogProbMetric: 16.3658, val_loss: 17.1740, val_MinusLogProbMetric: 17.1740

Epoch 458: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3658 - MinusLogProbMetric: 16.3658 - val_loss: 17.1740 - val_MinusLogProbMetric: 17.1740 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 459/1000
2023-09-27 19:12:33.318 
Epoch 459/1000 
	 loss: 16.3563, MinusLogProbMetric: 16.3563, val_loss: 17.1662, val_MinusLogProbMetric: 17.1662

Epoch 459: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3563 - MinusLogProbMetric: 16.3563 - val_loss: 17.1662 - val_MinusLogProbMetric: 17.1662 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 460/1000
2023-09-27 19:13:48.203 
Epoch 460/1000 
	 loss: 16.3493, MinusLogProbMetric: 16.3493, val_loss: 17.1585, val_MinusLogProbMetric: 17.1585

Epoch 460: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3493 - MinusLogProbMetric: 16.3493 - val_loss: 17.1585 - val_MinusLogProbMetric: 17.1585 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 461/1000
2023-09-27 19:15:03.153 
Epoch 461/1000 
	 loss: 16.3587, MinusLogProbMetric: 16.3587, val_loss: 17.2120, val_MinusLogProbMetric: 17.2120

Epoch 461: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3587 - MinusLogProbMetric: 16.3587 - val_loss: 17.2120 - val_MinusLogProbMetric: 17.2120 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 462/1000
2023-09-27 19:16:17.622 
Epoch 462/1000 
	 loss: 16.3666, MinusLogProbMetric: 16.3666, val_loss: 17.1635, val_MinusLogProbMetric: 17.1635

Epoch 462: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3666 - MinusLogProbMetric: 16.3666 - val_loss: 17.1635 - val_MinusLogProbMetric: 17.1635 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 463/1000
2023-09-27 19:17:32.322 
Epoch 463/1000 
	 loss: 16.3488, MinusLogProbMetric: 16.3488, val_loss: 17.1648, val_MinusLogProbMetric: 17.1648

Epoch 463: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3488 - MinusLogProbMetric: 16.3488 - val_loss: 17.1648 - val_MinusLogProbMetric: 17.1648 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 464/1000
2023-09-27 19:18:47.881 
Epoch 464/1000 
	 loss: 16.3448, MinusLogProbMetric: 16.3448, val_loss: 17.1718, val_MinusLogProbMetric: 17.1718

Epoch 464: val_loss did not improve from 17.15487
196/196 - 76s - loss: 16.3448 - MinusLogProbMetric: 16.3448 - val_loss: 17.1718 - val_MinusLogProbMetric: 17.1718 - lr: 4.1667e-05 - 76s/epoch - 385ms/step
Epoch 465/1000
2023-09-27 19:20:02.063 
Epoch 465/1000 
	 loss: 16.3478, MinusLogProbMetric: 16.3478, val_loss: 17.1942, val_MinusLogProbMetric: 17.1942

Epoch 465: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3478 - MinusLogProbMetric: 16.3478 - val_loss: 17.1942 - val_MinusLogProbMetric: 17.1942 - lr: 4.1667e-05 - 74s/epoch - 378ms/step
Epoch 466/1000
2023-09-27 19:21:17.217 
Epoch 466/1000 
	 loss: 16.3451, MinusLogProbMetric: 16.3451, val_loss: 17.1640, val_MinusLogProbMetric: 17.1640

Epoch 466: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3451 - MinusLogProbMetric: 16.3451 - val_loss: 17.1640 - val_MinusLogProbMetric: 17.1640 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 467/1000
2023-09-27 19:22:31.882 
Epoch 467/1000 
	 loss: 16.3505, MinusLogProbMetric: 16.3505, val_loss: 17.1767, val_MinusLogProbMetric: 17.1767

Epoch 467: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3505 - MinusLogProbMetric: 16.3505 - val_loss: 17.1767 - val_MinusLogProbMetric: 17.1767 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 468/1000
2023-09-27 19:23:47.105 
Epoch 468/1000 
	 loss: 16.3589, MinusLogProbMetric: 16.3589, val_loss: 17.1885, val_MinusLogProbMetric: 17.1885

Epoch 468: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3589 - MinusLogProbMetric: 16.3589 - val_loss: 17.1885 - val_MinusLogProbMetric: 17.1885 - lr: 4.1667e-05 - 75s/epoch - 384ms/step
Epoch 469/1000
2023-09-27 19:25:01.810 
Epoch 469/1000 
	 loss: 16.3429, MinusLogProbMetric: 16.3429, val_loss: 17.1836, val_MinusLogProbMetric: 17.1836

Epoch 469: val_loss did not improve from 17.15487
196/196 - 75s - loss: 16.3429 - MinusLogProbMetric: 16.3429 - val_loss: 17.1836 - val_MinusLogProbMetric: 17.1836 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 470/1000
2023-09-27 19:26:14.430 
Epoch 470/1000 
	 loss: 16.3469, MinusLogProbMetric: 16.3469, val_loss: 17.1787, val_MinusLogProbMetric: 17.1787

Epoch 470: val_loss did not improve from 17.15487
196/196 - 73s - loss: 16.3469 - MinusLogProbMetric: 16.3469 - val_loss: 17.1787 - val_MinusLogProbMetric: 17.1787 - lr: 4.1667e-05 - 73s/epoch - 370ms/step
Epoch 471/1000
2023-09-27 19:27:25.082 
Epoch 471/1000 
	 loss: 16.3853, MinusLogProbMetric: 16.3853, val_loss: 17.1559, val_MinusLogProbMetric: 17.1559

Epoch 471: val_loss did not improve from 17.15487
196/196 - 71s - loss: 16.3853 - MinusLogProbMetric: 16.3853 - val_loss: 17.1559 - val_MinusLogProbMetric: 17.1559 - lr: 4.1667e-05 - 71s/epoch - 360ms/step
Epoch 472/1000
2023-09-27 19:28:37.676 
Epoch 472/1000 
	 loss: 16.3469, MinusLogProbMetric: 16.3469, val_loss: 17.1728, val_MinusLogProbMetric: 17.1728

Epoch 472: val_loss did not improve from 17.15487
196/196 - 73s - loss: 16.3469 - MinusLogProbMetric: 16.3469 - val_loss: 17.1728 - val_MinusLogProbMetric: 17.1728 - lr: 4.1667e-05 - 73s/epoch - 370ms/step
Epoch 473/1000
2023-09-27 19:29:52.149 
Epoch 473/1000 
	 loss: 16.3490, MinusLogProbMetric: 16.3490, val_loss: 17.1584, val_MinusLogProbMetric: 17.1584

Epoch 473: val_loss did not improve from 17.15487
196/196 - 74s - loss: 16.3490 - MinusLogProbMetric: 16.3490 - val_loss: 17.1584 - val_MinusLogProbMetric: 17.1584 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 474/1000
2023-09-27 19:31:06.789 
Epoch 474/1000 
	 loss: 16.3483, MinusLogProbMetric: 16.3483, val_loss: 17.1496, val_MinusLogProbMetric: 17.1496

Epoch 474: val_loss improved from 17.15487 to 17.14958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.3483 - MinusLogProbMetric: 16.3483 - val_loss: 17.1496 - val_MinusLogProbMetric: 17.1496 - lr: 4.1667e-05 - 76s/epoch - 388ms/step
Epoch 475/1000
2023-09-27 19:32:23.226 
Epoch 475/1000 
	 loss: 16.3544, MinusLogProbMetric: 16.3544, val_loss: 17.1685, val_MinusLogProbMetric: 17.1685

Epoch 475: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3544 - MinusLogProbMetric: 16.3544 - val_loss: 17.1685 - val_MinusLogProbMetric: 17.1685 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 476/1000
2023-09-27 19:33:37.872 
Epoch 476/1000 
	 loss: 16.3609, MinusLogProbMetric: 16.3609, val_loss: 17.1789, val_MinusLogProbMetric: 17.1789

Epoch 476: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3609 - MinusLogProbMetric: 16.3609 - val_loss: 17.1789 - val_MinusLogProbMetric: 17.1789 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 477/1000
2023-09-27 19:34:52.577 
Epoch 477/1000 
	 loss: 16.3418, MinusLogProbMetric: 16.3418, val_loss: 17.1633, val_MinusLogProbMetric: 17.1633

Epoch 477: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3418 - MinusLogProbMetric: 16.3418 - val_loss: 17.1633 - val_MinusLogProbMetric: 17.1633 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 478/1000
2023-09-27 19:36:07.619 
Epoch 478/1000 
	 loss: 16.3487, MinusLogProbMetric: 16.3487, val_loss: 17.1933, val_MinusLogProbMetric: 17.1933

Epoch 478: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3487 - MinusLogProbMetric: 16.3487 - val_loss: 17.1933 - val_MinusLogProbMetric: 17.1933 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 479/1000
2023-09-27 19:37:22.708 
Epoch 479/1000 
	 loss: 16.3614, MinusLogProbMetric: 16.3614, val_loss: 17.1697, val_MinusLogProbMetric: 17.1697

Epoch 479: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3614 - MinusLogProbMetric: 16.3614 - val_loss: 17.1697 - val_MinusLogProbMetric: 17.1697 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 480/1000
2023-09-27 19:38:37.149 
Epoch 480/1000 
	 loss: 16.3528, MinusLogProbMetric: 16.3528, val_loss: 17.1883, val_MinusLogProbMetric: 17.1883

Epoch 480: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3528 - MinusLogProbMetric: 16.3528 - val_loss: 17.1883 - val_MinusLogProbMetric: 17.1883 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 481/1000
2023-09-27 19:39:51.650 
Epoch 481/1000 
	 loss: 16.3475, MinusLogProbMetric: 16.3475, val_loss: 17.1884, val_MinusLogProbMetric: 17.1884

Epoch 481: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3475 - MinusLogProbMetric: 16.3475 - val_loss: 17.1884 - val_MinusLogProbMetric: 17.1884 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 482/1000
2023-09-27 19:41:06.067 
Epoch 482/1000 
	 loss: 16.3506, MinusLogProbMetric: 16.3506, val_loss: 17.1842, val_MinusLogProbMetric: 17.1842

Epoch 482: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3506 - MinusLogProbMetric: 16.3506 - val_loss: 17.1842 - val_MinusLogProbMetric: 17.1842 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 483/1000
2023-09-27 19:42:20.901 
Epoch 483/1000 
	 loss: 16.3448, MinusLogProbMetric: 16.3448, val_loss: 17.2234, val_MinusLogProbMetric: 17.2234

Epoch 483: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3448 - MinusLogProbMetric: 16.3448 - val_loss: 17.2234 - val_MinusLogProbMetric: 17.2234 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 484/1000
2023-09-27 19:43:35.811 
Epoch 484/1000 
	 loss: 16.3495, MinusLogProbMetric: 16.3495, val_loss: 17.2990, val_MinusLogProbMetric: 17.2990

Epoch 484: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3495 - MinusLogProbMetric: 16.3495 - val_loss: 17.2990 - val_MinusLogProbMetric: 17.2990 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 485/1000
2023-09-27 19:44:50.803 
Epoch 485/1000 
	 loss: 16.3533, MinusLogProbMetric: 16.3533, val_loss: 17.2546, val_MinusLogProbMetric: 17.2546

Epoch 485: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3533 - MinusLogProbMetric: 16.3533 - val_loss: 17.2546 - val_MinusLogProbMetric: 17.2546 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 486/1000
2023-09-27 19:46:05.691 
Epoch 486/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 17.2517, val_MinusLogProbMetric: 17.2517

Epoch 486: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 17.2517 - val_MinusLogProbMetric: 17.2517 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 487/1000
2023-09-27 19:47:20.206 
Epoch 487/1000 
	 loss: 16.3623, MinusLogProbMetric: 16.3623, val_loss: 17.1881, val_MinusLogProbMetric: 17.1881

Epoch 487: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3623 - MinusLogProbMetric: 16.3623 - val_loss: 17.1881 - val_MinusLogProbMetric: 17.1881 - lr: 4.1667e-05 - 75s/epoch - 380ms/step
Epoch 488/1000
2023-09-27 19:48:34.739 
Epoch 488/1000 
	 loss: 16.3603, MinusLogProbMetric: 16.3603, val_loss: 17.1980, val_MinusLogProbMetric: 17.1980

Epoch 488: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3603 - MinusLogProbMetric: 16.3603 - val_loss: 17.1980 - val_MinusLogProbMetric: 17.1980 - lr: 4.1667e-05 - 75s/epoch - 380ms/step
Epoch 489/1000
2023-09-27 19:49:49.435 
Epoch 489/1000 
	 loss: 16.3459, MinusLogProbMetric: 16.3459, val_loss: 17.1688, val_MinusLogProbMetric: 17.1688

Epoch 489: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3459 - MinusLogProbMetric: 16.3459 - val_loss: 17.1688 - val_MinusLogProbMetric: 17.1688 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 490/1000
2023-09-27 19:51:03.730 
Epoch 490/1000 
	 loss: 16.3380, MinusLogProbMetric: 16.3380, val_loss: 17.2079, val_MinusLogProbMetric: 17.2079

Epoch 490: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3380 - MinusLogProbMetric: 16.3380 - val_loss: 17.2079 - val_MinusLogProbMetric: 17.2079 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 491/1000
2023-09-27 19:52:18.648 
Epoch 491/1000 
	 loss: 16.3478, MinusLogProbMetric: 16.3478, val_loss: 17.1839, val_MinusLogProbMetric: 17.1839

Epoch 491: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3478 - MinusLogProbMetric: 16.3478 - val_loss: 17.1839 - val_MinusLogProbMetric: 17.1839 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 492/1000
2023-09-27 19:53:33.376 
Epoch 492/1000 
	 loss: 16.3541, MinusLogProbMetric: 16.3541, val_loss: 17.1553, val_MinusLogProbMetric: 17.1553

Epoch 492: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3541 - MinusLogProbMetric: 16.3541 - val_loss: 17.1553 - val_MinusLogProbMetric: 17.1553 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 493/1000
2023-09-27 19:54:47.572 
Epoch 493/1000 
	 loss: 16.3481, MinusLogProbMetric: 16.3481, val_loss: 17.1666, val_MinusLogProbMetric: 17.1666

Epoch 493: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3481 - MinusLogProbMetric: 16.3481 - val_loss: 17.1666 - val_MinusLogProbMetric: 17.1666 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 494/1000
2023-09-27 19:56:01.805 
Epoch 494/1000 
	 loss: 16.3519, MinusLogProbMetric: 16.3519, val_loss: 17.1875, val_MinusLogProbMetric: 17.1875

Epoch 494: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3519 - MinusLogProbMetric: 16.3519 - val_loss: 17.1875 - val_MinusLogProbMetric: 17.1875 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 495/1000
2023-09-27 19:57:16.774 
Epoch 495/1000 
	 loss: 16.3487, MinusLogProbMetric: 16.3487, val_loss: 17.1935, val_MinusLogProbMetric: 17.1935

Epoch 495: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3487 - MinusLogProbMetric: 16.3487 - val_loss: 17.1935 - val_MinusLogProbMetric: 17.1935 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 496/1000
2023-09-27 19:58:31.296 
Epoch 496/1000 
	 loss: 16.3481, MinusLogProbMetric: 16.3481, val_loss: 17.1668, val_MinusLogProbMetric: 17.1668

Epoch 496: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3481 - MinusLogProbMetric: 16.3481 - val_loss: 17.1668 - val_MinusLogProbMetric: 17.1668 - lr: 4.1667e-05 - 75s/epoch - 380ms/step
Epoch 497/1000
2023-09-27 19:59:46.305 
Epoch 497/1000 
	 loss: 16.3538, MinusLogProbMetric: 16.3538, val_loss: 17.1801, val_MinusLogProbMetric: 17.1801

Epoch 497: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3538 - MinusLogProbMetric: 16.3538 - val_loss: 17.1801 - val_MinusLogProbMetric: 17.1801 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 498/1000
2023-09-27 20:01:00.742 
Epoch 498/1000 
	 loss: 16.3515, MinusLogProbMetric: 16.3515, val_loss: 17.1760, val_MinusLogProbMetric: 17.1760

Epoch 498: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3515 - MinusLogProbMetric: 16.3515 - val_loss: 17.1760 - val_MinusLogProbMetric: 17.1760 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 499/1000
2023-09-27 20:02:15.293 
Epoch 499/1000 
	 loss: 16.3386, MinusLogProbMetric: 16.3386, val_loss: 17.2145, val_MinusLogProbMetric: 17.2145

Epoch 499: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3386 - MinusLogProbMetric: 16.3386 - val_loss: 17.2145 - val_MinusLogProbMetric: 17.2145 - lr: 4.1667e-05 - 75s/epoch - 380ms/step
Epoch 500/1000
2023-09-27 20:03:30.068 
Epoch 500/1000 
	 loss: 16.3392, MinusLogProbMetric: 16.3392, val_loss: 17.2263, val_MinusLogProbMetric: 17.2263

Epoch 500: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3392 - MinusLogProbMetric: 16.3392 - val_loss: 17.2263 - val_MinusLogProbMetric: 17.2263 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 501/1000
2023-09-27 20:04:44.462 
Epoch 501/1000 
	 loss: 16.3404, MinusLogProbMetric: 16.3404, val_loss: 17.1557, val_MinusLogProbMetric: 17.1557

Epoch 501: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3404 - MinusLogProbMetric: 16.3404 - val_loss: 17.1557 - val_MinusLogProbMetric: 17.1557 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 502/1000
2023-09-27 20:05:58.863 
Epoch 502/1000 
	 loss: 16.3519, MinusLogProbMetric: 16.3519, val_loss: 17.1567, val_MinusLogProbMetric: 17.1567

Epoch 502: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3519 - MinusLogProbMetric: 16.3519 - val_loss: 17.1567 - val_MinusLogProbMetric: 17.1567 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 503/1000
2023-09-27 20:07:14.221 
Epoch 503/1000 
	 loss: 16.3462, MinusLogProbMetric: 16.3462, val_loss: 17.3608, val_MinusLogProbMetric: 17.3608

Epoch 503: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3462 - MinusLogProbMetric: 16.3462 - val_loss: 17.3608 - val_MinusLogProbMetric: 17.3608 - lr: 4.1667e-05 - 75s/epoch - 384ms/step
Epoch 504/1000
2023-09-27 20:08:28.893 
Epoch 504/1000 
	 loss: 16.3345, MinusLogProbMetric: 16.3345, val_loss: 17.2070, val_MinusLogProbMetric: 17.2070

Epoch 504: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3345 - MinusLogProbMetric: 16.3345 - val_loss: 17.2070 - val_MinusLogProbMetric: 17.2070 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 505/1000
2023-09-27 20:09:43.236 
Epoch 505/1000 
	 loss: 16.3549, MinusLogProbMetric: 16.3549, val_loss: 17.1641, val_MinusLogProbMetric: 17.1641

Epoch 505: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3549 - MinusLogProbMetric: 16.3549 - val_loss: 17.1641 - val_MinusLogProbMetric: 17.1641 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 506/1000
2023-09-27 20:10:58.318 
Epoch 506/1000 
	 loss: 16.3537, MinusLogProbMetric: 16.3537, val_loss: 17.1894, val_MinusLogProbMetric: 17.1894

Epoch 506: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3537 - MinusLogProbMetric: 16.3537 - val_loss: 17.1894 - val_MinusLogProbMetric: 17.1894 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 507/1000
2023-09-27 20:12:12.771 
Epoch 507/1000 
	 loss: 16.3347, MinusLogProbMetric: 16.3347, val_loss: 17.1933, val_MinusLogProbMetric: 17.1933

Epoch 507: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3347 - MinusLogProbMetric: 16.3347 - val_loss: 17.1933 - val_MinusLogProbMetric: 17.1933 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 508/1000
2023-09-27 20:13:27.384 
Epoch 508/1000 
	 loss: 16.3446, MinusLogProbMetric: 16.3446, val_loss: 17.1641, val_MinusLogProbMetric: 17.1641

Epoch 508: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3446 - MinusLogProbMetric: 16.3446 - val_loss: 17.1641 - val_MinusLogProbMetric: 17.1641 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 509/1000
2023-09-27 20:14:41.867 
Epoch 509/1000 
	 loss: 16.3398, MinusLogProbMetric: 16.3398, val_loss: 17.1924, val_MinusLogProbMetric: 17.1924

Epoch 509: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3398 - MinusLogProbMetric: 16.3398 - val_loss: 17.1924 - val_MinusLogProbMetric: 17.1924 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 510/1000
2023-09-27 20:15:56.505 
Epoch 510/1000 
	 loss: 16.3424, MinusLogProbMetric: 16.3424, val_loss: 17.1729, val_MinusLogProbMetric: 17.1729

Epoch 510: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3424 - MinusLogProbMetric: 16.3424 - val_loss: 17.1729 - val_MinusLogProbMetric: 17.1729 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 511/1000
2023-09-27 20:17:11.812 
Epoch 511/1000 
	 loss: 16.3541, MinusLogProbMetric: 16.3541, val_loss: 17.2191, val_MinusLogProbMetric: 17.2191

Epoch 511: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3541 - MinusLogProbMetric: 16.3541 - val_loss: 17.2191 - val_MinusLogProbMetric: 17.2191 - lr: 4.1667e-05 - 75s/epoch - 384ms/step
Epoch 512/1000
2023-09-27 20:18:26.941 
Epoch 512/1000 
	 loss: 16.3419, MinusLogProbMetric: 16.3419, val_loss: 17.1746, val_MinusLogProbMetric: 17.1746

Epoch 512: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3419 - MinusLogProbMetric: 16.3419 - val_loss: 17.1746 - val_MinusLogProbMetric: 17.1746 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 513/1000
2023-09-27 20:19:41.608 
Epoch 513/1000 
	 loss: 16.3334, MinusLogProbMetric: 16.3334, val_loss: 17.1720, val_MinusLogProbMetric: 17.1720

Epoch 513: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3334 - MinusLogProbMetric: 16.3334 - val_loss: 17.1720 - val_MinusLogProbMetric: 17.1720 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 514/1000
2023-09-27 20:20:55.894 
Epoch 514/1000 
	 loss: 16.3419, MinusLogProbMetric: 16.3419, val_loss: 17.3524, val_MinusLogProbMetric: 17.3524

Epoch 514: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3419 - MinusLogProbMetric: 16.3419 - val_loss: 17.3524 - val_MinusLogProbMetric: 17.3524 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 515/1000
2023-09-27 20:22:10.878 
Epoch 515/1000 
	 loss: 16.3614, MinusLogProbMetric: 16.3614, val_loss: 17.1767, val_MinusLogProbMetric: 17.1767

Epoch 515: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3614 - MinusLogProbMetric: 16.3614 - val_loss: 17.1767 - val_MinusLogProbMetric: 17.1767 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 516/1000
2023-09-27 20:23:25.325 
Epoch 516/1000 
	 loss: 16.3327, MinusLogProbMetric: 16.3327, val_loss: 17.1640, val_MinusLogProbMetric: 17.1640

Epoch 516: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3327 - MinusLogProbMetric: 16.3327 - val_loss: 17.1640 - val_MinusLogProbMetric: 17.1640 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 517/1000
2023-09-27 20:24:40.457 
Epoch 517/1000 
	 loss: 16.3390, MinusLogProbMetric: 16.3390, val_loss: 17.2187, val_MinusLogProbMetric: 17.2187

Epoch 517: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3390 - MinusLogProbMetric: 16.3390 - val_loss: 17.2187 - val_MinusLogProbMetric: 17.2187 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 518/1000
2023-09-27 20:25:55.349 
Epoch 518/1000 
	 loss: 16.3596, MinusLogProbMetric: 16.3596, val_loss: 17.1640, val_MinusLogProbMetric: 17.1640

Epoch 518: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3596 - MinusLogProbMetric: 16.3596 - val_loss: 17.1640 - val_MinusLogProbMetric: 17.1640 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 519/1000
2023-09-27 20:27:09.648 
Epoch 519/1000 
	 loss: 16.3415, MinusLogProbMetric: 16.3415, val_loss: 17.1741, val_MinusLogProbMetric: 17.1741

Epoch 519: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3415 - MinusLogProbMetric: 16.3415 - val_loss: 17.1741 - val_MinusLogProbMetric: 17.1741 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 520/1000
2023-09-27 20:28:24.191 
Epoch 520/1000 
	 loss: 16.3297, MinusLogProbMetric: 16.3297, val_loss: 17.1654, val_MinusLogProbMetric: 17.1654

Epoch 520: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3297 - MinusLogProbMetric: 16.3297 - val_loss: 17.1654 - val_MinusLogProbMetric: 17.1654 - lr: 4.1667e-05 - 75s/epoch - 380ms/step
Epoch 521/1000
2023-09-27 20:29:38.544 
Epoch 521/1000 
	 loss: 16.3314, MinusLogProbMetric: 16.3314, val_loss: 17.1643, val_MinusLogProbMetric: 17.1643

Epoch 521: val_loss did not improve from 17.14958
196/196 - 74s - loss: 16.3314 - MinusLogProbMetric: 16.3314 - val_loss: 17.1643 - val_MinusLogProbMetric: 17.1643 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 522/1000
2023-09-27 20:30:53.645 
Epoch 522/1000 
	 loss: 16.3294, MinusLogProbMetric: 16.3294, val_loss: 17.1575, val_MinusLogProbMetric: 17.1575

Epoch 522: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3294 - MinusLogProbMetric: 16.3294 - val_loss: 17.1575 - val_MinusLogProbMetric: 17.1575 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 523/1000
2023-09-27 20:32:08.797 
Epoch 523/1000 
	 loss: 16.3354, MinusLogProbMetric: 16.3354, val_loss: 17.1949, val_MinusLogProbMetric: 17.1949

Epoch 523: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3354 - MinusLogProbMetric: 16.3354 - val_loss: 17.1949 - val_MinusLogProbMetric: 17.1949 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 524/1000
2023-09-27 20:33:23.945 
Epoch 524/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 17.1647, val_MinusLogProbMetric: 17.1647

Epoch 524: val_loss did not improve from 17.14958
196/196 - 75s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 17.1647 - val_MinusLogProbMetric: 17.1647 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 525/1000
2023-09-27 20:34:38.708 
Epoch 525/1000 
	 loss: 16.2959, MinusLogProbMetric: 16.2959, val_loss: 17.1463, val_MinusLogProbMetric: 17.1463

Epoch 525: val_loss improved from 17.14958 to 17.14630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2959 - MinusLogProbMetric: 16.2959 - val_loss: 17.1463 - val_MinusLogProbMetric: 17.1463 - lr: 2.0833e-05 - 76s/epoch - 389ms/step
Epoch 526/1000
2023-09-27 20:35:54.572 
Epoch 526/1000 
	 loss: 16.2956, MinusLogProbMetric: 16.2956, val_loss: 17.1438, val_MinusLogProbMetric: 17.1438

Epoch 526: val_loss improved from 17.14630 to 17.14382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2956 - MinusLogProbMetric: 16.2956 - val_loss: 17.1438 - val_MinusLogProbMetric: 17.1438 - lr: 2.0833e-05 - 76s/epoch - 387ms/step
Epoch 527/1000
2023-09-27 20:37:10.365 
Epoch 527/1000 
	 loss: 16.2984, MinusLogProbMetric: 16.2984, val_loss: 17.1554, val_MinusLogProbMetric: 17.1554

Epoch 527: val_loss did not improve from 17.14382
196/196 - 74s - loss: 16.2984 - MinusLogProbMetric: 16.2984 - val_loss: 17.1554 - val_MinusLogProbMetric: 17.1554 - lr: 2.0833e-05 - 74s/epoch - 379ms/step
Epoch 528/1000
2023-09-27 20:38:25.138 
Epoch 528/1000 
	 loss: 16.2973, MinusLogProbMetric: 16.2973, val_loss: 17.1364, val_MinusLogProbMetric: 17.1364

Epoch 528: val_loss improved from 17.14382 to 17.13640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2973 - MinusLogProbMetric: 16.2973 - val_loss: 17.1364 - val_MinusLogProbMetric: 17.1364 - lr: 2.0833e-05 - 76s/epoch - 387ms/step
Epoch 529/1000
2023-09-27 20:39:40.738 
Epoch 529/1000 
	 loss: 16.2921, MinusLogProbMetric: 16.2921, val_loss: 17.1375, val_MinusLogProbMetric: 17.1375

Epoch 529: val_loss did not improve from 17.13640
196/196 - 74s - loss: 16.2921 - MinusLogProbMetric: 16.2921 - val_loss: 17.1375 - val_MinusLogProbMetric: 17.1375 - lr: 2.0833e-05 - 74s/epoch - 380ms/step
Epoch 530/1000
2023-09-27 20:40:55.261 
Epoch 530/1000 
	 loss: 16.2921, MinusLogProbMetric: 16.2921, val_loss: 17.1601, val_MinusLogProbMetric: 17.1601

Epoch 530: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2921 - MinusLogProbMetric: 16.2921 - val_loss: 17.1601 - val_MinusLogProbMetric: 17.1601 - lr: 2.0833e-05 - 75s/epoch - 380ms/step
Epoch 531/1000
2023-09-27 20:42:09.744 
Epoch 531/1000 
	 loss: 16.2962, MinusLogProbMetric: 16.2962, val_loss: 17.1545, val_MinusLogProbMetric: 17.1545

Epoch 531: val_loss did not improve from 17.13640
196/196 - 74s - loss: 16.2962 - MinusLogProbMetric: 16.2962 - val_loss: 17.1545 - val_MinusLogProbMetric: 17.1545 - lr: 2.0833e-05 - 74s/epoch - 380ms/step
Epoch 532/1000
2023-09-27 20:43:20.159 
Epoch 532/1000 
	 loss: 16.3005, MinusLogProbMetric: 16.3005, val_loss: 17.1619, val_MinusLogProbMetric: 17.1619

Epoch 532: val_loss did not improve from 17.13640
196/196 - 70s - loss: 16.3005 - MinusLogProbMetric: 16.3005 - val_loss: 17.1619 - val_MinusLogProbMetric: 17.1619 - lr: 2.0833e-05 - 70s/epoch - 359ms/step
Epoch 533/1000
2023-09-27 20:44:31.488 
Epoch 533/1000 
	 loss: 16.3018, MinusLogProbMetric: 16.3018, val_loss: 17.1525, val_MinusLogProbMetric: 17.1525

Epoch 533: val_loss did not improve from 17.13640
196/196 - 71s - loss: 16.3018 - MinusLogProbMetric: 16.3018 - val_loss: 17.1525 - val_MinusLogProbMetric: 17.1525 - lr: 2.0833e-05 - 71s/epoch - 364ms/step
Epoch 534/1000
2023-09-27 20:45:46.479 
Epoch 534/1000 
	 loss: 16.2929, MinusLogProbMetric: 16.2929, val_loss: 17.1520, val_MinusLogProbMetric: 17.1520

Epoch 534: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2929 - MinusLogProbMetric: 16.2929 - val_loss: 17.1520 - val_MinusLogProbMetric: 17.1520 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 535/1000
2023-09-27 20:47:01.821 
Epoch 535/1000 
	 loss: 16.2921, MinusLogProbMetric: 16.2921, val_loss: 17.1481, val_MinusLogProbMetric: 17.1481

Epoch 535: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2921 - MinusLogProbMetric: 16.2921 - val_loss: 17.1481 - val_MinusLogProbMetric: 17.1481 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 536/1000
2023-09-27 20:48:17.256 
Epoch 536/1000 
	 loss: 16.3002, MinusLogProbMetric: 16.3002, val_loss: 17.1604, val_MinusLogProbMetric: 17.1604

Epoch 536: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.3002 - MinusLogProbMetric: 16.3002 - val_loss: 17.1604 - val_MinusLogProbMetric: 17.1604 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 537/1000
2023-09-27 20:49:32.565 
Epoch 537/1000 
	 loss: 16.2943, MinusLogProbMetric: 16.2943, val_loss: 17.1820, val_MinusLogProbMetric: 17.1820

Epoch 537: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2943 - MinusLogProbMetric: 16.2943 - val_loss: 17.1820 - val_MinusLogProbMetric: 17.1820 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 538/1000
2023-09-27 20:50:47.665 
Epoch 538/1000 
	 loss: 16.3022, MinusLogProbMetric: 16.3022, val_loss: 17.1416, val_MinusLogProbMetric: 17.1416

Epoch 538: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.3022 - MinusLogProbMetric: 16.3022 - val_loss: 17.1416 - val_MinusLogProbMetric: 17.1416 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 539/1000
2023-09-27 20:52:02.459 
Epoch 539/1000 
	 loss: 16.2939, MinusLogProbMetric: 16.2939, val_loss: 17.1583, val_MinusLogProbMetric: 17.1583

Epoch 539: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2939 - MinusLogProbMetric: 16.2939 - val_loss: 17.1583 - val_MinusLogProbMetric: 17.1583 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 540/1000
2023-09-27 20:53:17.827 
Epoch 540/1000 
	 loss: 16.3017, MinusLogProbMetric: 16.3017, val_loss: 17.1523, val_MinusLogProbMetric: 17.1523

Epoch 540: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.3017 - MinusLogProbMetric: 16.3017 - val_loss: 17.1523 - val_MinusLogProbMetric: 17.1523 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 541/1000
2023-09-27 20:54:32.958 
Epoch 541/1000 
	 loss: 16.2965, MinusLogProbMetric: 16.2965, val_loss: 17.1686, val_MinusLogProbMetric: 17.1686

Epoch 541: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2965 - MinusLogProbMetric: 16.2965 - val_loss: 17.1686 - val_MinusLogProbMetric: 17.1686 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 542/1000
2023-09-27 20:55:47.727 
Epoch 542/1000 
	 loss: 16.2988, MinusLogProbMetric: 16.2988, val_loss: 17.1563, val_MinusLogProbMetric: 17.1563

Epoch 542: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2988 - MinusLogProbMetric: 16.2988 - val_loss: 17.1563 - val_MinusLogProbMetric: 17.1563 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 543/1000
2023-09-27 20:57:02.826 
Epoch 543/1000 
	 loss: 16.2948, MinusLogProbMetric: 16.2948, val_loss: 17.1549, val_MinusLogProbMetric: 17.1549

Epoch 543: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2948 - MinusLogProbMetric: 16.2948 - val_loss: 17.1549 - val_MinusLogProbMetric: 17.1549 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 544/1000
2023-09-27 20:58:18.173 
Epoch 544/1000 
	 loss: 16.2900, MinusLogProbMetric: 16.2900, val_loss: 17.1487, val_MinusLogProbMetric: 17.1487

Epoch 544: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2900 - MinusLogProbMetric: 16.2900 - val_loss: 17.1487 - val_MinusLogProbMetric: 17.1487 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 545/1000
2023-09-27 20:59:32.537 
Epoch 545/1000 
	 loss: 16.2975, MinusLogProbMetric: 16.2975, val_loss: 17.1388, val_MinusLogProbMetric: 17.1388

Epoch 545: val_loss did not improve from 17.13640
196/196 - 74s - loss: 16.2975 - MinusLogProbMetric: 16.2975 - val_loss: 17.1388 - val_MinusLogProbMetric: 17.1388 - lr: 2.0833e-05 - 74s/epoch - 379ms/step
Epoch 546/1000
2023-09-27 21:00:47.903 
Epoch 546/1000 
	 loss: 16.2983, MinusLogProbMetric: 16.2983, val_loss: 17.1525, val_MinusLogProbMetric: 17.1525

Epoch 546: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2983 - MinusLogProbMetric: 16.2983 - val_loss: 17.1525 - val_MinusLogProbMetric: 17.1525 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 547/1000
2023-09-27 21:02:02.207 
Epoch 547/1000 
	 loss: 16.2937, MinusLogProbMetric: 16.2937, val_loss: 17.1633, val_MinusLogProbMetric: 17.1633

Epoch 547: val_loss did not improve from 17.13640
196/196 - 74s - loss: 16.2937 - MinusLogProbMetric: 16.2937 - val_loss: 17.1633 - val_MinusLogProbMetric: 17.1633 - lr: 2.0833e-05 - 74s/epoch - 379ms/step
Epoch 548/1000
2023-09-27 21:03:16.410 
Epoch 548/1000 
	 loss: 16.2968, MinusLogProbMetric: 16.2968, val_loss: 17.1861, val_MinusLogProbMetric: 17.1861

Epoch 548: val_loss did not improve from 17.13640
196/196 - 74s - loss: 16.2968 - MinusLogProbMetric: 16.2968 - val_loss: 17.1861 - val_MinusLogProbMetric: 17.1861 - lr: 2.0833e-05 - 74s/epoch - 379ms/step
Epoch 549/1000
2023-09-27 21:04:31.258 
Epoch 549/1000 
	 loss: 16.2970, MinusLogProbMetric: 16.2970, val_loss: 17.1409, val_MinusLogProbMetric: 17.1409

Epoch 549: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2970 - MinusLogProbMetric: 16.2970 - val_loss: 17.1409 - val_MinusLogProbMetric: 17.1409 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 550/1000
2023-09-27 21:05:46.342 
Epoch 550/1000 
	 loss: 16.2959, MinusLogProbMetric: 16.2959, val_loss: 17.1415, val_MinusLogProbMetric: 17.1415

Epoch 550: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2959 - MinusLogProbMetric: 16.2959 - val_loss: 17.1415 - val_MinusLogProbMetric: 17.1415 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 551/1000
2023-09-27 21:07:00.736 
Epoch 551/1000 
	 loss: 16.2912, MinusLogProbMetric: 16.2912, val_loss: 17.1740, val_MinusLogProbMetric: 17.1740

Epoch 551: val_loss did not improve from 17.13640
196/196 - 74s - loss: 16.2912 - MinusLogProbMetric: 16.2912 - val_loss: 17.1740 - val_MinusLogProbMetric: 17.1740 - lr: 2.0833e-05 - 74s/epoch - 380ms/step
Epoch 552/1000
2023-09-27 21:08:15.937 
Epoch 552/1000 
	 loss: 16.2975, MinusLogProbMetric: 16.2975, val_loss: 17.1551, val_MinusLogProbMetric: 17.1551

Epoch 552: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2975 - MinusLogProbMetric: 16.2975 - val_loss: 17.1551 - val_MinusLogProbMetric: 17.1551 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 553/1000
2023-09-27 21:09:30.980 
Epoch 553/1000 
	 loss: 16.2943, MinusLogProbMetric: 16.2943, val_loss: 17.1920, val_MinusLogProbMetric: 17.1920

Epoch 553: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2943 - MinusLogProbMetric: 16.2943 - val_loss: 17.1920 - val_MinusLogProbMetric: 17.1920 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 554/1000
2023-09-27 21:10:45.919 
Epoch 554/1000 
	 loss: 16.2942, MinusLogProbMetric: 16.2942, val_loss: 17.1528, val_MinusLogProbMetric: 17.1528

Epoch 554: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2942 - MinusLogProbMetric: 16.2942 - val_loss: 17.1528 - val_MinusLogProbMetric: 17.1528 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 555/1000
2023-09-27 21:12:00.527 
Epoch 555/1000 
	 loss: 16.2932, MinusLogProbMetric: 16.2932, val_loss: 17.1443, val_MinusLogProbMetric: 17.1443

Epoch 555: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2932 - MinusLogProbMetric: 16.2932 - val_loss: 17.1443 - val_MinusLogProbMetric: 17.1443 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 556/1000
2023-09-27 21:13:15.894 
Epoch 556/1000 
	 loss: 16.2925, MinusLogProbMetric: 16.2925, val_loss: 17.1415, val_MinusLogProbMetric: 17.1415

Epoch 556: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2925 - MinusLogProbMetric: 16.2925 - val_loss: 17.1415 - val_MinusLogProbMetric: 17.1415 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 557/1000
2023-09-27 21:14:30.924 
Epoch 557/1000 
	 loss: 16.2940, MinusLogProbMetric: 16.2940, val_loss: 17.1571, val_MinusLogProbMetric: 17.1571

Epoch 557: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2940 - MinusLogProbMetric: 16.2940 - val_loss: 17.1571 - val_MinusLogProbMetric: 17.1571 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 558/1000
2023-09-27 21:15:45.742 
Epoch 558/1000 
	 loss: 16.2925, MinusLogProbMetric: 16.2925, val_loss: 17.1471, val_MinusLogProbMetric: 17.1471

Epoch 558: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2925 - MinusLogProbMetric: 16.2925 - val_loss: 17.1471 - val_MinusLogProbMetric: 17.1471 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 559/1000
2023-09-27 21:17:00.672 
Epoch 559/1000 
	 loss: 16.2996, MinusLogProbMetric: 16.2996, val_loss: 17.1525, val_MinusLogProbMetric: 17.1525

Epoch 559: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2996 - MinusLogProbMetric: 16.2996 - val_loss: 17.1525 - val_MinusLogProbMetric: 17.1525 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 560/1000
2023-09-27 21:18:15.334 
Epoch 560/1000 
	 loss: 16.2946, MinusLogProbMetric: 16.2946, val_loss: 17.2025, val_MinusLogProbMetric: 17.2025

Epoch 560: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2946 - MinusLogProbMetric: 16.2946 - val_loss: 17.2025 - val_MinusLogProbMetric: 17.2025 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 561/1000
2023-09-27 21:19:30.591 
Epoch 561/1000 
	 loss: 16.2961, MinusLogProbMetric: 16.2961, val_loss: 17.1470, val_MinusLogProbMetric: 17.1470

Epoch 561: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2961 - MinusLogProbMetric: 16.2961 - val_loss: 17.1470 - val_MinusLogProbMetric: 17.1470 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 562/1000
2023-09-27 21:20:45.693 
Epoch 562/1000 
	 loss: 16.2939, MinusLogProbMetric: 16.2939, val_loss: 17.1510, val_MinusLogProbMetric: 17.1510

Epoch 562: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2939 - MinusLogProbMetric: 16.2939 - val_loss: 17.1510 - val_MinusLogProbMetric: 17.1510 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 563/1000
2023-09-27 21:22:01.080 
Epoch 563/1000 
	 loss: 16.3050, MinusLogProbMetric: 16.3050, val_loss: 17.1456, val_MinusLogProbMetric: 17.1456

Epoch 563: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.3050 - MinusLogProbMetric: 16.3050 - val_loss: 17.1456 - val_MinusLogProbMetric: 17.1456 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 564/1000
2023-09-27 21:23:16.631 
Epoch 564/1000 
	 loss: 16.2892, MinusLogProbMetric: 16.2892, val_loss: 17.1651, val_MinusLogProbMetric: 17.1651

Epoch 564: val_loss did not improve from 17.13640
196/196 - 76s - loss: 16.2892 - MinusLogProbMetric: 16.2892 - val_loss: 17.1651 - val_MinusLogProbMetric: 17.1651 - lr: 2.0833e-05 - 76s/epoch - 385ms/step
Epoch 565/1000
2023-09-27 21:24:31.908 
Epoch 565/1000 
	 loss: 16.2953, MinusLogProbMetric: 16.2953, val_loss: 17.1432, val_MinusLogProbMetric: 17.1432

Epoch 565: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2953 - MinusLogProbMetric: 16.2953 - val_loss: 17.1432 - val_MinusLogProbMetric: 17.1432 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 566/1000
2023-09-27 21:25:47.404 
Epoch 566/1000 
	 loss: 16.2872, MinusLogProbMetric: 16.2872, val_loss: 17.1606, val_MinusLogProbMetric: 17.1606

Epoch 566: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2872 - MinusLogProbMetric: 16.2872 - val_loss: 17.1606 - val_MinusLogProbMetric: 17.1606 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 567/1000
2023-09-27 21:27:02.571 
Epoch 567/1000 
	 loss: 16.2995, MinusLogProbMetric: 16.2995, val_loss: 17.1431, val_MinusLogProbMetric: 17.1431

Epoch 567: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2995 - MinusLogProbMetric: 16.2995 - val_loss: 17.1431 - val_MinusLogProbMetric: 17.1431 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 568/1000
2023-09-27 21:28:17.582 
Epoch 568/1000 
	 loss: 16.2868, MinusLogProbMetric: 16.2868, val_loss: 17.1680, val_MinusLogProbMetric: 17.1680

Epoch 568: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2868 - MinusLogProbMetric: 16.2868 - val_loss: 17.1680 - val_MinusLogProbMetric: 17.1680 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 569/1000
2023-09-27 21:29:32.233 
Epoch 569/1000 
	 loss: 16.2889, MinusLogProbMetric: 16.2889, val_loss: 17.1407, val_MinusLogProbMetric: 17.1407

Epoch 569: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2889 - MinusLogProbMetric: 16.2889 - val_loss: 17.1407 - val_MinusLogProbMetric: 17.1407 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 570/1000
2023-09-27 21:30:46.787 
Epoch 570/1000 
	 loss: 16.2839, MinusLogProbMetric: 16.2839, val_loss: 17.1562, val_MinusLogProbMetric: 17.1562

Epoch 570: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2839 - MinusLogProbMetric: 16.2839 - val_loss: 17.1562 - val_MinusLogProbMetric: 17.1562 - lr: 2.0833e-05 - 75s/epoch - 380ms/step
Epoch 571/1000
2023-09-27 21:32:01.739 
Epoch 571/1000 
	 loss: 16.2925, MinusLogProbMetric: 16.2925, val_loss: 17.1601, val_MinusLogProbMetric: 17.1601

Epoch 571: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2925 - MinusLogProbMetric: 16.2925 - val_loss: 17.1601 - val_MinusLogProbMetric: 17.1601 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 572/1000
2023-09-27 21:33:16.589 
Epoch 572/1000 
	 loss: 16.2945, MinusLogProbMetric: 16.2945, val_loss: 17.1593, val_MinusLogProbMetric: 17.1593

Epoch 572: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2945 - MinusLogProbMetric: 16.2945 - val_loss: 17.1593 - val_MinusLogProbMetric: 17.1593 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 573/1000
2023-09-27 21:34:32.027 
Epoch 573/1000 
	 loss: 16.2863, MinusLogProbMetric: 16.2863, val_loss: 17.1559, val_MinusLogProbMetric: 17.1559

Epoch 573: val_loss did not improve from 17.13640
196/196 - 75s - loss: 16.2863 - MinusLogProbMetric: 16.2863 - val_loss: 17.1559 - val_MinusLogProbMetric: 17.1559 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 574/1000
2023-09-27 21:35:47.548 
Epoch 574/1000 
	 loss: 16.2860, MinusLogProbMetric: 16.2860, val_loss: 17.2217, val_MinusLogProbMetric: 17.2217

Epoch 574: val_loss did not improve from 17.13640
196/196 - 76s - loss: 16.2860 - MinusLogProbMetric: 16.2860 - val_loss: 17.2217 - val_MinusLogProbMetric: 17.2217 - lr: 2.0833e-05 - 76s/epoch - 385ms/step
Epoch 575/1000
2023-09-27 21:37:03.371 
Epoch 575/1000 
	 loss: 16.2879, MinusLogProbMetric: 16.2879, val_loss: 17.1356, val_MinusLogProbMetric: 17.1356

Epoch 575: val_loss improved from 17.13640 to 17.13556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 16.2879 - MinusLogProbMetric: 16.2879 - val_loss: 17.1356 - val_MinusLogProbMetric: 17.1356 - lr: 2.0833e-05 - 77s/epoch - 394ms/step
Epoch 576/1000
2023-09-27 21:38:19.418 
Epoch 576/1000 
	 loss: 16.2850, MinusLogProbMetric: 16.2850, val_loss: 17.1411, val_MinusLogProbMetric: 17.1411

Epoch 576: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2850 - MinusLogProbMetric: 16.2850 - val_loss: 17.1411 - val_MinusLogProbMetric: 17.1411 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 577/1000
2023-09-27 21:39:34.477 
Epoch 577/1000 
	 loss: 16.2890, MinusLogProbMetric: 16.2890, val_loss: 17.1838, val_MinusLogProbMetric: 17.1838

Epoch 577: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2890 - MinusLogProbMetric: 16.2890 - val_loss: 17.1838 - val_MinusLogProbMetric: 17.1838 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 578/1000
2023-09-27 21:40:49.247 
Epoch 578/1000 
	 loss: 16.2948, MinusLogProbMetric: 16.2948, val_loss: 17.1478, val_MinusLogProbMetric: 17.1478

Epoch 578: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2948 - MinusLogProbMetric: 16.2948 - val_loss: 17.1478 - val_MinusLogProbMetric: 17.1478 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 579/1000
2023-09-27 21:42:04.104 
Epoch 579/1000 
	 loss: 16.2859, MinusLogProbMetric: 16.2859, val_loss: 17.1444, val_MinusLogProbMetric: 17.1444

Epoch 579: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2859 - MinusLogProbMetric: 16.2859 - val_loss: 17.1444 - val_MinusLogProbMetric: 17.1444 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 580/1000
2023-09-27 21:43:19.002 
Epoch 580/1000 
	 loss: 16.2837, MinusLogProbMetric: 16.2837, val_loss: 17.1429, val_MinusLogProbMetric: 17.1429

Epoch 580: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2837 - MinusLogProbMetric: 16.2837 - val_loss: 17.1429 - val_MinusLogProbMetric: 17.1429 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 581/1000
2023-09-27 21:44:34.014 
Epoch 581/1000 
	 loss: 16.2920, MinusLogProbMetric: 16.2920, val_loss: 17.1742, val_MinusLogProbMetric: 17.1742

Epoch 581: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2920 - MinusLogProbMetric: 16.2920 - val_loss: 17.1742 - val_MinusLogProbMetric: 17.1742 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 582/1000
2023-09-27 21:45:49.197 
Epoch 582/1000 
	 loss: 16.2976, MinusLogProbMetric: 16.2976, val_loss: 17.1605, val_MinusLogProbMetric: 17.1605

Epoch 582: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2976 - MinusLogProbMetric: 16.2976 - val_loss: 17.1605 - val_MinusLogProbMetric: 17.1605 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 583/1000
2023-09-27 21:47:04.588 
Epoch 583/1000 
	 loss: 16.2894, MinusLogProbMetric: 16.2894, val_loss: 17.1479, val_MinusLogProbMetric: 17.1479

Epoch 583: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2894 - MinusLogProbMetric: 16.2894 - val_loss: 17.1479 - val_MinusLogProbMetric: 17.1479 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 584/1000
2023-09-27 21:48:19.678 
Epoch 584/1000 
	 loss: 16.2917, MinusLogProbMetric: 16.2917, val_loss: 17.1457, val_MinusLogProbMetric: 17.1457

Epoch 584: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2917 - MinusLogProbMetric: 16.2917 - val_loss: 17.1457 - val_MinusLogProbMetric: 17.1457 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 585/1000
2023-09-27 21:49:35.167 
Epoch 585/1000 
	 loss: 16.2878, MinusLogProbMetric: 16.2878, val_loss: 17.1376, val_MinusLogProbMetric: 17.1376

Epoch 585: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2878 - MinusLogProbMetric: 16.2878 - val_loss: 17.1376 - val_MinusLogProbMetric: 17.1376 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 586/1000
2023-09-27 21:50:50.637 
Epoch 586/1000 
	 loss: 16.2868, MinusLogProbMetric: 16.2868, val_loss: 17.1594, val_MinusLogProbMetric: 17.1594

Epoch 586: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2868 - MinusLogProbMetric: 16.2868 - val_loss: 17.1594 - val_MinusLogProbMetric: 17.1594 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 587/1000
2023-09-27 21:52:05.068 
Epoch 587/1000 
	 loss: 16.2879, MinusLogProbMetric: 16.2879, val_loss: 17.1861, val_MinusLogProbMetric: 17.1861

Epoch 587: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2879 - MinusLogProbMetric: 16.2879 - val_loss: 17.1861 - val_MinusLogProbMetric: 17.1861 - lr: 2.0833e-05 - 74s/epoch - 380ms/step
Epoch 588/1000
2023-09-27 21:53:19.951 
Epoch 588/1000 
	 loss: 16.2924, MinusLogProbMetric: 16.2924, val_loss: 17.1622, val_MinusLogProbMetric: 17.1622

Epoch 588: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2924 - MinusLogProbMetric: 16.2924 - val_loss: 17.1622 - val_MinusLogProbMetric: 17.1622 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 589/1000
2023-09-27 21:54:34.669 
Epoch 589/1000 
	 loss: 16.2824, MinusLogProbMetric: 16.2824, val_loss: 17.1680, val_MinusLogProbMetric: 17.1680

Epoch 589: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2824 - MinusLogProbMetric: 16.2824 - val_loss: 17.1680 - val_MinusLogProbMetric: 17.1680 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 590/1000
2023-09-27 21:55:49.525 
Epoch 590/1000 
	 loss: 16.2922, MinusLogProbMetric: 16.2922, val_loss: 17.1516, val_MinusLogProbMetric: 17.1516

Epoch 590: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2922 - MinusLogProbMetric: 16.2922 - val_loss: 17.1516 - val_MinusLogProbMetric: 17.1516 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 591/1000
2023-09-27 21:57:04.911 
Epoch 591/1000 
	 loss: 16.2943, MinusLogProbMetric: 16.2943, val_loss: 17.1507, val_MinusLogProbMetric: 17.1507

Epoch 591: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2943 - MinusLogProbMetric: 16.2943 - val_loss: 17.1507 - val_MinusLogProbMetric: 17.1507 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 592/1000
2023-09-27 21:58:20.043 
Epoch 592/1000 
	 loss: 16.2892, MinusLogProbMetric: 16.2892, val_loss: 17.1412, val_MinusLogProbMetric: 17.1412

Epoch 592: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2892 - MinusLogProbMetric: 16.2892 - val_loss: 17.1412 - val_MinusLogProbMetric: 17.1412 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 593/1000
2023-09-27 21:59:35.234 
Epoch 593/1000 
	 loss: 16.2888, MinusLogProbMetric: 16.2888, val_loss: 17.1497, val_MinusLogProbMetric: 17.1497

Epoch 593: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2888 - MinusLogProbMetric: 16.2888 - val_loss: 17.1497 - val_MinusLogProbMetric: 17.1497 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 594/1000
2023-09-27 22:00:49.973 
Epoch 594/1000 
	 loss: 16.2859, MinusLogProbMetric: 16.2859, val_loss: 17.1616, val_MinusLogProbMetric: 17.1616

Epoch 594: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2859 - MinusLogProbMetric: 16.2859 - val_loss: 17.1616 - val_MinusLogProbMetric: 17.1616 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 595/1000
2023-09-27 22:02:05.377 
Epoch 595/1000 
	 loss: 16.2866, MinusLogProbMetric: 16.2866, val_loss: 17.1571, val_MinusLogProbMetric: 17.1571

Epoch 595: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2866 - MinusLogProbMetric: 16.2866 - val_loss: 17.1571 - val_MinusLogProbMetric: 17.1571 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 596/1000
2023-09-27 22:03:19.937 
Epoch 596/1000 
	 loss: 16.2901, MinusLogProbMetric: 16.2901, val_loss: 17.1984, val_MinusLogProbMetric: 17.1984

Epoch 596: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2901 - MinusLogProbMetric: 16.2901 - val_loss: 17.1984 - val_MinusLogProbMetric: 17.1984 - lr: 2.0833e-05 - 75s/epoch - 380ms/step
Epoch 597/1000
2023-09-27 22:04:35.066 
Epoch 597/1000 
	 loss: 16.2888, MinusLogProbMetric: 16.2888, val_loss: 17.1449, val_MinusLogProbMetric: 17.1449

Epoch 597: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2888 - MinusLogProbMetric: 16.2888 - val_loss: 17.1449 - val_MinusLogProbMetric: 17.1449 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 598/1000
2023-09-27 22:05:49.874 
Epoch 598/1000 
	 loss: 16.2836, MinusLogProbMetric: 16.2836, val_loss: 17.1548, val_MinusLogProbMetric: 17.1548

Epoch 598: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2836 - MinusLogProbMetric: 16.2836 - val_loss: 17.1548 - val_MinusLogProbMetric: 17.1548 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 599/1000
2023-09-27 22:07:05.352 
Epoch 599/1000 
	 loss: 16.2881, MinusLogProbMetric: 16.2881, val_loss: 17.1503, val_MinusLogProbMetric: 17.1503

Epoch 599: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2881 - MinusLogProbMetric: 16.2881 - val_loss: 17.1503 - val_MinusLogProbMetric: 17.1503 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 600/1000
2023-09-27 22:08:19.483 
Epoch 600/1000 
	 loss: 16.2929, MinusLogProbMetric: 16.2929, val_loss: 17.2120, val_MinusLogProbMetric: 17.2120

Epoch 600: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2929 - MinusLogProbMetric: 16.2929 - val_loss: 17.2120 - val_MinusLogProbMetric: 17.2120 - lr: 2.0833e-05 - 74s/epoch - 378ms/step
Epoch 601/1000
2023-09-27 22:09:29.698 
Epoch 601/1000 
	 loss: 16.2927, MinusLogProbMetric: 16.2927, val_loss: 17.1476, val_MinusLogProbMetric: 17.1476

Epoch 601: val_loss did not improve from 17.13556
196/196 - 70s - loss: 16.2927 - MinusLogProbMetric: 16.2927 - val_loss: 17.1476 - val_MinusLogProbMetric: 17.1476 - lr: 2.0833e-05 - 70s/epoch - 358ms/step
Epoch 602/1000
2023-09-27 22:10:35.543 
Epoch 602/1000 
	 loss: 16.2809, MinusLogProbMetric: 16.2809, val_loss: 17.1385, val_MinusLogProbMetric: 17.1385

Epoch 602: val_loss did not improve from 17.13556
196/196 - 66s - loss: 16.2809 - MinusLogProbMetric: 16.2809 - val_loss: 17.1385 - val_MinusLogProbMetric: 17.1385 - lr: 2.0833e-05 - 66s/epoch - 336ms/step
Epoch 603/1000
2023-09-27 22:11:49.999 
Epoch 603/1000 
	 loss: 16.2855, MinusLogProbMetric: 16.2855, val_loss: 17.1602, val_MinusLogProbMetric: 17.1602

Epoch 603: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2855 - MinusLogProbMetric: 16.2855 - val_loss: 17.1602 - val_MinusLogProbMetric: 17.1602 - lr: 2.0833e-05 - 74s/epoch - 380ms/step
Epoch 604/1000
2023-09-27 22:13:04.549 
Epoch 604/1000 
	 loss: 16.2863, MinusLogProbMetric: 16.2863, val_loss: 17.1612, val_MinusLogProbMetric: 17.1612

Epoch 604: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2863 - MinusLogProbMetric: 16.2863 - val_loss: 17.1612 - val_MinusLogProbMetric: 17.1612 - lr: 2.0833e-05 - 75s/epoch - 380ms/step
Epoch 605/1000
2023-09-27 22:14:18.686 
Epoch 605/1000 
	 loss: 16.2854, MinusLogProbMetric: 16.2854, val_loss: 17.1439, val_MinusLogProbMetric: 17.1439

Epoch 605: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2854 - MinusLogProbMetric: 16.2854 - val_loss: 17.1439 - val_MinusLogProbMetric: 17.1439 - lr: 2.0833e-05 - 74s/epoch - 378ms/step
Epoch 606/1000
2023-09-27 22:15:33.185 
Epoch 606/1000 
	 loss: 16.2881, MinusLogProbMetric: 16.2881, val_loss: 17.1538, val_MinusLogProbMetric: 17.1538

Epoch 606: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2881 - MinusLogProbMetric: 16.2881 - val_loss: 17.1538 - val_MinusLogProbMetric: 17.1538 - lr: 2.0833e-05 - 74s/epoch - 380ms/step
Epoch 607/1000
2023-09-27 22:16:47.223 
Epoch 607/1000 
	 loss: 16.2851, MinusLogProbMetric: 16.2851, val_loss: 17.1540, val_MinusLogProbMetric: 17.1540

Epoch 607: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2851 - MinusLogProbMetric: 16.2851 - val_loss: 17.1540 - val_MinusLogProbMetric: 17.1540 - lr: 2.0833e-05 - 74s/epoch - 378ms/step
Epoch 608/1000
2023-09-27 22:18:03.233 
Epoch 608/1000 
	 loss: 16.2819, MinusLogProbMetric: 16.2819, val_loss: 17.1543, val_MinusLogProbMetric: 17.1543

Epoch 608: val_loss did not improve from 17.13556
196/196 - 76s - loss: 16.2819 - MinusLogProbMetric: 16.2819 - val_loss: 17.1543 - val_MinusLogProbMetric: 17.1543 - lr: 2.0833e-05 - 76s/epoch - 388ms/step
Epoch 609/1000
2023-09-27 22:19:20.385 
Epoch 609/1000 
	 loss: 16.2824, MinusLogProbMetric: 16.2824, val_loss: 17.1412, val_MinusLogProbMetric: 17.1412

Epoch 609: val_loss did not improve from 17.13556
196/196 - 77s - loss: 16.2824 - MinusLogProbMetric: 16.2824 - val_loss: 17.1412 - val_MinusLogProbMetric: 17.1412 - lr: 2.0833e-05 - 77s/epoch - 394ms/step
Epoch 610/1000
2023-09-27 22:20:35.885 
Epoch 610/1000 
	 loss: 16.2812, MinusLogProbMetric: 16.2812, val_loss: 17.2110, val_MinusLogProbMetric: 17.2110

Epoch 610: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2812 - MinusLogProbMetric: 16.2812 - val_loss: 17.2110 - val_MinusLogProbMetric: 17.2110 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 611/1000
2023-09-27 22:21:51.411 
Epoch 611/1000 
	 loss: 16.2892, MinusLogProbMetric: 16.2892, val_loss: 17.1537, val_MinusLogProbMetric: 17.1537

Epoch 611: val_loss did not improve from 17.13556
196/196 - 76s - loss: 16.2892 - MinusLogProbMetric: 16.2892 - val_loss: 17.1537 - val_MinusLogProbMetric: 17.1537 - lr: 2.0833e-05 - 76s/epoch - 385ms/step
Epoch 612/1000
2023-09-27 22:23:06.646 
Epoch 612/1000 
	 loss: 16.2964, MinusLogProbMetric: 16.2964, val_loss: 17.1474, val_MinusLogProbMetric: 17.1474

Epoch 612: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2964 - MinusLogProbMetric: 16.2964 - val_loss: 17.1474 - val_MinusLogProbMetric: 17.1474 - lr: 2.0833e-05 - 75s/epoch - 384ms/step
Epoch 613/1000
2023-09-27 22:24:21.796 
Epoch 613/1000 
	 loss: 16.2849, MinusLogProbMetric: 16.2849, val_loss: 17.1606, val_MinusLogProbMetric: 17.1606

Epoch 613: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2849 - MinusLogProbMetric: 16.2849 - val_loss: 17.1606 - val_MinusLogProbMetric: 17.1606 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 614/1000
2023-09-27 22:25:36.821 
Epoch 614/1000 
	 loss: 16.2841, MinusLogProbMetric: 16.2841, val_loss: 17.1701, val_MinusLogProbMetric: 17.1701

Epoch 614: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2841 - MinusLogProbMetric: 16.2841 - val_loss: 17.1701 - val_MinusLogProbMetric: 17.1701 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 615/1000
2023-09-27 22:26:51.221 
Epoch 615/1000 
	 loss: 16.2802, MinusLogProbMetric: 16.2802, val_loss: 17.1401, val_MinusLogProbMetric: 17.1401

Epoch 615: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2802 - MinusLogProbMetric: 16.2802 - val_loss: 17.1401 - val_MinusLogProbMetric: 17.1401 - lr: 2.0833e-05 - 74s/epoch - 380ms/step
Epoch 616/1000
2023-09-27 22:28:05.979 
Epoch 616/1000 
	 loss: 16.2807, MinusLogProbMetric: 16.2807, val_loss: 17.1643, val_MinusLogProbMetric: 17.1643

Epoch 616: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2807 - MinusLogProbMetric: 16.2807 - val_loss: 17.1643 - val_MinusLogProbMetric: 17.1643 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 617/1000
2023-09-27 22:29:20.698 
Epoch 617/1000 
	 loss: 16.2825, MinusLogProbMetric: 16.2825, val_loss: 17.1550, val_MinusLogProbMetric: 17.1550

Epoch 617: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2825 - MinusLogProbMetric: 16.2825 - val_loss: 17.1550 - val_MinusLogProbMetric: 17.1550 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 618/1000
2023-09-27 22:30:35.415 
Epoch 618/1000 
	 loss: 16.2757, MinusLogProbMetric: 16.2757, val_loss: 17.1678, val_MinusLogProbMetric: 17.1678

Epoch 618: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2757 - MinusLogProbMetric: 16.2757 - val_loss: 17.1678 - val_MinusLogProbMetric: 17.1678 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 619/1000
2023-09-27 22:31:49.976 
Epoch 619/1000 
	 loss: 16.2897, MinusLogProbMetric: 16.2897, val_loss: 17.1999, val_MinusLogProbMetric: 17.1999

Epoch 619: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2897 - MinusLogProbMetric: 16.2897 - val_loss: 17.1999 - val_MinusLogProbMetric: 17.1999 - lr: 2.0833e-05 - 75s/epoch - 380ms/step
Epoch 620/1000
2023-09-27 22:33:04.608 
Epoch 620/1000 
	 loss: 16.2820, MinusLogProbMetric: 16.2820, val_loss: 17.1508, val_MinusLogProbMetric: 17.1508

Epoch 620: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2820 - MinusLogProbMetric: 16.2820 - val_loss: 17.1508 - val_MinusLogProbMetric: 17.1508 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 621/1000
2023-09-27 22:34:19.763 
Epoch 621/1000 
	 loss: 16.2805, MinusLogProbMetric: 16.2805, val_loss: 17.2034, val_MinusLogProbMetric: 17.2034

Epoch 621: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2805 - MinusLogProbMetric: 16.2805 - val_loss: 17.2034 - val_MinusLogProbMetric: 17.2034 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 622/1000
2023-09-27 22:35:34.764 
Epoch 622/1000 
	 loss: 16.2848, MinusLogProbMetric: 16.2848, val_loss: 17.1419, val_MinusLogProbMetric: 17.1419

Epoch 622: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2848 - MinusLogProbMetric: 16.2848 - val_loss: 17.1419 - val_MinusLogProbMetric: 17.1419 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 623/1000
2023-09-27 22:36:49.404 
Epoch 623/1000 
	 loss: 16.2804, MinusLogProbMetric: 16.2804, val_loss: 17.1607, val_MinusLogProbMetric: 17.1607

Epoch 623: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2804 - MinusLogProbMetric: 16.2804 - val_loss: 17.1607 - val_MinusLogProbMetric: 17.1607 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 624/1000
2023-09-27 22:38:04.553 
Epoch 624/1000 
	 loss: 16.2916, MinusLogProbMetric: 16.2916, val_loss: 17.1837, val_MinusLogProbMetric: 17.1837

Epoch 624: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2916 - MinusLogProbMetric: 16.2916 - val_loss: 17.1837 - val_MinusLogProbMetric: 17.1837 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 625/1000
2023-09-27 22:39:19.365 
Epoch 625/1000 
	 loss: 16.2816, MinusLogProbMetric: 16.2816, val_loss: 17.1557, val_MinusLogProbMetric: 17.1557

Epoch 625: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2816 - MinusLogProbMetric: 16.2816 - val_loss: 17.1557 - val_MinusLogProbMetric: 17.1557 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 626/1000
2023-09-27 22:40:33.706 
Epoch 626/1000 
	 loss: 16.2639, MinusLogProbMetric: 16.2639, val_loss: 17.1413, val_MinusLogProbMetric: 17.1413

Epoch 626: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2639 - MinusLogProbMetric: 16.2639 - val_loss: 17.1413 - val_MinusLogProbMetric: 17.1413 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 627/1000
2023-09-27 22:41:47.826 
Epoch 627/1000 
	 loss: 16.2621, MinusLogProbMetric: 16.2621, val_loss: 17.1374, val_MinusLogProbMetric: 17.1374

Epoch 627: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2621 - MinusLogProbMetric: 16.2621 - val_loss: 17.1374 - val_MinusLogProbMetric: 17.1374 - lr: 1.0417e-05 - 74s/epoch - 378ms/step
Epoch 628/1000
2023-09-27 22:43:02.327 
Epoch 628/1000 
	 loss: 16.2620, MinusLogProbMetric: 16.2620, val_loss: 17.1509, val_MinusLogProbMetric: 17.1509

Epoch 628: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2620 - MinusLogProbMetric: 16.2620 - val_loss: 17.1509 - val_MinusLogProbMetric: 17.1509 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 629/1000
2023-09-27 22:44:16.939 
Epoch 629/1000 
	 loss: 16.2644, MinusLogProbMetric: 16.2644, val_loss: 17.1400, val_MinusLogProbMetric: 17.1400

Epoch 629: val_loss did not improve from 17.13556
196/196 - 75s - loss: 16.2644 - MinusLogProbMetric: 16.2644 - val_loss: 17.1400 - val_MinusLogProbMetric: 17.1400 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 630/1000
2023-09-27 22:45:30.964 
Epoch 630/1000 
	 loss: 16.2618, MinusLogProbMetric: 16.2618, val_loss: 17.1365, val_MinusLogProbMetric: 17.1365

Epoch 630: val_loss did not improve from 17.13556
196/196 - 74s - loss: 16.2618 - MinusLogProbMetric: 16.2618 - val_loss: 17.1365 - val_MinusLogProbMetric: 17.1365 - lr: 1.0417e-05 - 74s/epoch - 378ms/step
Epoch 631/1000
2023-09-27 22:46:45.747 
Epoch 631/1000 
	 loss: 16.2623, MinusLogProbMetric: 16.2623, val_loss: 17.1334, val_MinusLogProbMetric: 17.1334

Epoch 631: val_loss improved from 17.13556 to 17.13339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2623 - MinusLogProbMetric: 16.2623 - val_loss: 17.1334 - val_MinusLogProbMetric: 17.1334 - lr: 1.0417e-05 - 76s/epoch - 389ms/step
Epoch 632/1000
2023-09-27 22:48:02.194 
Epoch 632/1000 
	 loss: 16.2646, MinusLogProbMetric: 16.2646, val_loss: 17.1642, val_MinusLogProbMetric: 17.1642

Epoch 632: val_loss did not improve from 17.13339
196/196 - 75s - loss: 16.2646 - MinusLogProbMetric: 16.2646 - val_loss: 17.1642 - val_MinusLogProbMetric: 17.1642 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 633/1000
2023-09-27 22:49:17.097 
Epoch 633/1000 
	 loss: 16.2634, MinusLogProbMetric: 16.2634, val_loss: 17.1359, val_MinusLogProbMetric: 17.1359

Epoch 633: val_loss did not improve from 17.13339
196/196 - 75s - loss: 16.2634 - MinusLogProbMetric: 16.2634 - val_loss: 17.1359 - val_MinusLogProbMetric: 17.1359 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 634/1000
2023-09-27 22:50:31.963 
Epoch 634/1000 
	 loss: 16.2606, MinusLogProbMetric: 16.2606, val_loss: 17.1329, val_MinusLogProbMetric: 17.1329

Epoch 634: val_loss improved from 17.13339 to 17.13294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2606 - MinusLogProbMetric: 16.2606 - val_loss: 17.1329 - val_MinusLogProbMetric: 17.1329 - lr: 1.0417e-05 - 76s/epoch - 389ms/step
Epoch 635/1000
2023-09-27 22:51:48.153 
Epoch 635/1000 
	 loss: 16.2596, MinusLogProbMetric: 16.2596, val_loss: 17.1336, val_MinusLogProbMetric: 17.1336

Epoch 635: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2596 - MinusLogProbMetric: 16.2596 - val_loss: 17.1336 - val_MinusLogProbMetric: 17.1336 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 636/1000
2023-09-27 22:53:02.572 
Epoch 636/1000 
	 loss: 16.2617, MinusLogProbMetric: 16.2617, val_loss: 17.1459, val_MinusLogProbMetric: 17.1459

Epoch 636: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2617 - MinusLogProbMetric: 16.2617 - val_loss: 17.1459 - val_MinusLogProbMetric: 17.1459 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 637/1000
2023-09-27 22:54:17.583 
Epoch 637/1000 
	 loss: 16.2597, MinusLogProbMetric: 16.2597, val_loss: 17.1366, val_MinusLogProbMetric: 17.1366

Epoch 637: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2597 - MinusLogProbMetric: 16.2597 - val_loss: 17.1366 - val_MinusLogProbMetric: 17.1366 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 638/1000
2023-09-27 22:55:31.550 
Epoch 638/1000 
	 loss: 16.2615, MinusLogProbMetric: 16.2615, val_loss: 17.1455, val_MinusLogProbMetric: 17.1455

Epoch 638: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2615 - MinusLogProbMetric: 16.2615 - val_loss: 17.1455 - val_MinusLogProbMetric: 17.1455 - lr: 1.0417e-05 - 74s/epoch - 377ms/step
Epoch 639/1000
2023-09-27 22:56:45.908 
Epoch 639/1000 
	 loss: 16.2591, MinusLogProbMetric: 16.2591, val_loss: 17.1405, val_MinusLogProbMetric: 17.1405

Epoch 639: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2591 - MinusLogProbMetric: 16.2591 - val_loss: 17.1405 - val_MinusLogProbMetric: 17.1405 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 640/1000
2023-09-27 22:58:00.689 
Epoch 640/1000 
	 loss: 16.2597, MinusLogProbMetric: 16.2597, val_loss: 17.1442, val_MinusLogProbMetric: 17.1442

Epoch 640: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2597 - MinusLogProbMetric: 16.2597 - val_loss: 17.1442 - val_MinusLogProbMetric: 17.1442 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 641/1000
2023-09-27 22:59:15.159 
Epoch 641/1000 
	 loss: 16.2614, MinusLogProbMetric: 16.2614, val_loss: 17.1423, val_MinusLogProbMetric: 17.1423

Epoch 641: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2614 - MinusLogProbMetric: 16.2614 - val_loss: 17.1423 - val_MinusLogProbMetric: 17.1423 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 642/1000
2023-09-27 23:00:30.060 
Epoch 642/1000 
	 loss: 16.2592, MinusLogProbMetric: 16.2592, val_loss: 17.1351, val_MinusLogProbMetric: 17.1351

Epoch 642: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2592 - MinusLogProbMetric: 16.2592 - val_loss: 17.1351 - val_MinusLogProbMetric: 17.1351 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 643/1000
2023-09-27 23:01:44.849 
Epoch 643/1000 
	 loss: 16.2584, MinusLogProbMetric: 16.2584, val_loss: 17.1356, val_MinusLogProbMetric: 17.1356

Epoch 643: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2584 - MinusLogProbMetric: 16.2584 - val_loss: 17.1356 - val_MinusLogProbMetric: 17.1356 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 644/1000
2023-09-27 23:02:59.807 
Epoch 644/1000 
	 loss: 16.2599, MinusLogProbMetric: 16.2599, val_loss: 17.1432, val_MinusLogProbMetric: 17.1432

Epoch 644: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2599 - MinusLogProbMetric: 16.2599 - val_loss: 17.1432 - val_MinusLogProbMetric: 17.1432 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 645/1000
2023-09-27 23:04:14.693 
Epoch 645/1000 
	 loss: 16.2624, MinusLogProbMetric: 16.2624, val_loss: 17.1364, val_MinusLogProbMetric: 17.1364

Epoch 645: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2624 - MinusLogProbMetric: 16.2624 - val_loss: 17.1364 - val_MinusLogProbMetric: 17.1364 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 646/1000
2023-09-27 23:05:29.353 
Epoch 646/1000 
	 loss: 16.2645, MinusLogProbMetric: 16.2645, val_loss: 17.1721, val_MinusLogProbMetric: 17.1721

Epoch 646: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2645 - MinusLogProbMetric: 16.2645 - val_loss: 17.1721 - val_MinusLogProbMetric: 17.1721 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 647/1000
2023-09-27 23:06:44.195 
Epoch 647/1000 
	 loss: 16.2614, MinusLogProbMetric: 16.2614, val_loss: 17.1361, val_MinusLogProbMetric: 17.1361

Epoch 647: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2614 - MinusLogProbMetric: 16.2614 - val_loss: 17.1361 - val_MinusLogProbMetric: 17.1361 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 648/1000
2023-09-27 23:08:00.176 
Epoch 648/1000 
	 loss: 16.2595, MinusLogProbMetric: 16.2595, val_loss: 17.1366, val_MinusLogProbMetric: 17.1366

Epoch 648: val_loss did not improve from 17.13294
196/196 - 76s - loss: 16.2595 - MinusLogProbMetric: 16.2595 - val_loss: 17.1366 - val_MinusLogProbMetric: 17.1366 - lr: 1.0417e-05 - 76s/epoch - 388ms/step
Epoch 649/1000
2023-09-27 23:09:14.935 
Epoch 649/1000 
	 loss: 16.2588, MinusLogProbMetric: 16.2588, val_loss: 17.1382, val_MinusLogProbMetric: 17.1382

Epoch 649: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2588 - MinusLogProbMetric: 16.2588 - val_loss: 17.1382 - val_MinusLogProbMetric: 17.1382 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 650/1000
2023-09-27 23:10:30.358 
Epoch 650/1000 
	 loss: 16.2582, MinusLogProbMetric: 16.2582, val_loss: 17.1466, val_MinusLogProbMetric: 17.1466

Epoch 650: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2582 - MinusLogProbMetric: 16.2582 - val_loss: 17.1466 - val_MinusLogProbMetric: 17.1466 - lr: 1.0417e-05 - 75s/epoch - 385ms/step
Epoch 651/1000
2023-09-27 23:11:44.427 
Epoch 651/1000 
	 loss: 16.2587, MinusLogProbMetric: 16.2587, val_loss: 17.1488, val_MinusLogProbMetric: 17.1488

Epoch 651: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2587 - MinusLogProbMetric: 16.2587 - val_loss: 17.1488 - val_MinusLogProbMetric: 17.1488 - lr: 1.0417e-05 - 74s/epoch - 378ms/step
Epoch 652/1000
2023-09-27 23:12:59.490 
Epoch 652/1000 
	 loss: 16.2608, MinusLogProbMetric: 16.2608, val_loss: 17.1360, val_MinusLogProbMetric: 17.1360

Epoch 652: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2608 - MinusLogProbMetric: 16.2608 - val_loss: 17.1360 - val_MinusLogProbMetric: 17.1360 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 653/1000
2023-09-27 23:14:14.829 
Epoch 653/1000 
	 loss: 16.2581, MinusLogProbMetric: 16.2581, val_loss: 17.1430, val_MinusLogProbMetric: 17.1430

Epoch 653: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2581 - MinusLogProbMetric: 16.2581 - val_loss: 17.1430 - val_MinusLogProbMetric: 17.1430 - lr: 1.0417e-05 - 75s/epoch - 384ms/step
Epoch 654/1000
2023-09-27 23:15:29.545 
Epoch 654/1000 
	 loss: 16.2631, MinusLogProbMetric: 16.2631, val_loss: 17.1346, val_MinusLogProbMetric: 17.1346

Epoch 654: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2631 - MinusLogProbMetric: 16.2631 - val_loss: 17.1346 - val_MinusLogProbMetric: 17.1346 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 655/1000
2023-09-27 23:16:43.861 
Epoch 655/1000 
	 loss: 16.2581, MinusLogProbMetric: 16.2581, val_loss: 17.1338, val_MinusLogProbMetric: 17.1338

Epoch 655: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2581 - MinusLogProbMetric: 16.2581 - val_loss: 17.1338 - val_MinusLogProbMetric: 17.1338 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 656/1000
2023-09-27 23:17:58.807 
Epoch 656/1000 
	 loss: 16.2581, MinusLogProbMetric: 16.2581, val_loss: 17.1346, val_MinusLogProbMetric: 17.1346

Epoch 656: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2581 - MinusLogProbMetric: 16.2581 - val_loss: 17.1346 - val_MinusLogProbMetric: 17.1346 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 657/1000
2023-09-27 23:19:13.959 
Epoch 657/1000 
	 loss: 16.2609, MinusLogProbMetric: 16.2609, val_loss: 17.1345, val_MinusLogProbMetric: 17.1345

Epoch 657: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2609 - MinusLogProbMetric: 16.2609 - val_loss: 17.1345 - val_MinusLogProbMetric: 17.1345 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 658/1000
2023-09-27 23:20:28.475 
Epoch 658/1000 
	 loss: 16.2575, MinusLogProbMetric: 16.2575, val_loss: 17.1370, val_MinusLogProbMetric: 17.1370

Epoch 658: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2575 - MinusLogProbMetric: 16.2575 - val_loss: 17.1370 - val_MinusLogProbMetric: 17.1370 - lr: 1.0417e-05 - 75s/epoch - 380ms/step
Epoch 659/1000
2023-09-27 23:21:43.631 
Epoch 659/1000 
	 loss: 16.2570, MinusLogProbMetric: 16.2570, val_loss: 17.1412, val_MinusLogProbMetric: 17.1412

Epoch 659: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2570 - MinusLogProbMetric: 16.2570 - val_loss: 17.1412 - val_MinusLogProbMetric: 17.1412 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 660/1000
2023-09-27 23:22:58.036 
Epoch 660/1000 
	 loss: 16.2572, MinusLogProbMetric: 16.2572, val_loss: 17.1422, val_MinusLogProbMetric: 17.1422

Epoch 660: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2572 - MinusLogProbMetric: 16.2572 - val_loss: 17.1422 - val_MinusLogProbMetric: 17.1422 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 661/1000
2023-09-27 23:24:12.516 
Epoch 661/1000 
	 loss: 16.2617, MinusLogProbMetric: 16.2617, val_loss: 17.1414, val_MinusLogProbMetric: 17.1414

Epoch 661: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2617 - MinusLogProbMetric: 16.2617 - val_loss: 17.1414 - val_MinusLogProbMetric: 17.1414 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 662/1000
2023-09-27 23:25:25.132 
Epoch 662/1000 
	 loss: 16.2580, MinusLogProbMetric: 16.2580, val_loss: 17.1357, val_MinusLogProbMetric: 17.1357

Epoch 662: val_loss did not improve from 17.13294
196/196 - 73s - loss: 16.2580 - MinusLogProbMetric: 16.2580 - val_loss: 17.1357 - val_MinusLogProbMetric: 17.1357 - lr: 1.0417e-05 - 73s/epoch - 370ms/step
Epoch 663/1000
2023-09-27 23:26:34.983 
Epoch 663/1000 
	 loss: 16.2577, MinusLogProbMetric: 16.2577, val_loss: 17.1366, val_MinusLogProbMetric: 17.1366

Epoch 663: val_loss did not improve from 17.13294
196/196 - 70s - loss: 16.2577 - MinusLogProbMetric: 16.2577 - val_loss: 17.1366 - val_MinusLogProbMetric: 17.1366 - lr: 1.0417e-05 - 70s/epoch - 356ms/step
Epoch 664/1000
2023-09-27 23:27:48.864 
Epoch 664/1000 
	 loss: 16.2559, MinusLogProbMetric: 16.2559, val_loss: 17.1482, val_MinusLogProbMetric: 17.1482

Epoch 664: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2559 - MinusLogProbMetric: 16.2559 - val_loss: 17.1482 - val_MinusLogProbMetric: 17.1482 - lr: 1.0417e-05 - 74s/epoch - 377ms/step
Epoch 665/1000
2023-09-27 23:29:03.306 
Epoch 665/1000 
	 loss: 16.2567, MinusLogProbMetric: 16.2567, val_loss: 17.1366, val_MinusLogProbMetric: 17.1366

Epoch 665: val_loss did not improve from 17.13294
196/196 - 74s - loss: 16.2567 - MinusLogProbMetric: 16.2567 - val_loss: 17.1366 - val_MinusLogProbMetric: 17.1366 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 666/1000
2023-09-27 23:30:18.099 
Epoch 666/1000 
	 loss: 16.2564, MinusLogProbMetric: 16.2564, val_loss: 17.1338, val_MinusLogProbMetric: 17.1338

Epoch 666: val_loss did not improve from 17.13294
196/196 - 75s - loss: 16.2564 - MinusLogProbMetric: 16.2564 - val_loss: 17.1338 - val_MinusLogProbMetric: 17.1338 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 667/1000
2023-09-27 23:31:33.125 
Epoch 667/1000 
	 loss: 16.2574, MinusLogProbMetric: 16.2574, val_loss: 17.1316, val_MinusLogProbMetric: 17.1316

Epoch 667: val_loss improved from 17.13294 to 17.13156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 77s - loss: 16.2574 - MinusLogProbMetric: 16.2574 - val_loss: 17.1316 - val_MinusLogProbMetric: 17.1316 - lr: 1.0417e-05 - 77s/epoch - 391ms/step
Epoch 668/1000
2023-09-27 23:32:49.436 
Epoch 668/1000 
	 loss: 16.2582, MinusLogProbMetric: 16.2582, val_loss: 17.1408, val_MinusLogProbMetric: 17.1408

Epoch 668: val_loss did not improve from 17.13156
196/196 - 75s - loss: 16.2582 - MinusLogProbMetric: 16.2582 - val_loss: 17.1408 - val_MinusLogProbMetric: 17.1408 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 669/1000
2023-09-27 23:34:04.296 
Epoch 669/1000 
	 loss: 16.2550, MinusLogProbMetric: 16.2550, val_loss: 17.1371, val_MinusLogProbMetric: 17.1371

Epoch 669: val_loss did not improve from 17.13156
196/196 - 75s - loss: 16.2550 - MinusLogProbMetric: 16.2550 - val_loss: 17.1371 - val_MinusLogProbMetric: 17.1371 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 670/1000
2023-09-27 23:35:18.846 
Epoch 670/1000 
	 loss: 16.2585, MinusLogProbMetric: 16.2585, val_loss: 17.1388, val_MinusLogProbMetric: 17.1388

Epoch 670: val_loss did not improve from 17.13156
196/196 - 75s - loss: 16.2585 - MinusLogProbMetric: 16.2585 - val_loss: 17.1388 - val_MinusLogProbMetric: 17.1388 - lr: 1.0417e-05 - 75s/epoch - 380ms/step
Epoch 671/1000
2023-09-27 23:36:33.360 
Epoch 671/1000 
	 loss: 16.2589, MinusLogProbMetric: 16.2589, val_loss: 17.1589, val_MinusLogProbMetric: 17.1589

Epoch 671: val_loss did not improve from 17.13156
196/196 - 75s - loss: 16.2589 - MinusLogProbMetric: 16.2589 - val_loss: 17.1589 - val_MinusLogProbMetric: 17.1589 - lr: 1.0417e-05 - 75s/epoch - 380ms/step
Epoch 672/1000
2023-09-27 23:37:48.105 
Epoch 672/1000 
	 loss: 16.2593, MinusLogProbMetric: 16.2593, val_loss: 17.1389, val_MinusLogProbMetric: 17.1389

Epoch 672: val_loss did not improve from 17.13156
196/196 - 75s - loss: 16.2593 - MinusLogProbMetric: 16.2593 - val_loss: 17.1389 - val_MinusLogProbMetric: 17.1389 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 673/1000
2023-09-27 23:39:02.482 
Epoch 673/1000 
	 loss: 16.2575, MinusLogProbMetric: 16.2575, val_loss: 17.1345, val_MinusLogProbMetric: 17.1345

Epoch 673: val_loss did not improve from 17.13156
196/196 - 74s - loss: 16.2575 - MinusLogProbMetric: 16.2575 - val_loss: 17.1345 - val_MinusLogProbMetric: 17.1345 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 674/1000
2023-09-27 23:40:17.041 
Epoch 674/1000 
	 loss: 16.2624, MinusLogProbMetric: 16.2624, val_loss: 17.1577, val_MinusLogProbMetric: 17.1577

Epoch 674: val_loss did not improve from 17.13156
196/196 - 75s - loss: 16.2624 - MinusLogProbMetric: 16.2624 - val_loss: 17.1577 - val_MinusLogProbMetric: 17.1577 - lr: 1.0417e-05 - 75s/epoch - 380ms/step
Epoch 675/1000
2023-09-27 23:41:31.513 
Epoch 675/1000 
	 loss: 16.2579, MinusLogProbMetric: 16.2579, val_loss: 17.1467, val_MinusLogProbMetric: 17.1467

Epoch 675: val_loss did not improve from 17.13156
196/196 - 74s - loss: 16.2579 - MinusLogProbMetric: 16.2579 - val_loss: 17.1467 - val_MinusLogProbMetric: 17.1467 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 676/1000
2023-09-27 23:42:47.063 
Epoch 676/1000 
	 loss: 16.2594, MinusLogProbMetric: 16.2594, val_loss: 17.1374, val_MinusLogProbMetric: 17.1374

Epoch 676: val_loss did not improve from 17.13156
196/196 - 76s - loss: 16.2594 - MinusLogProbMetric: 16.2594 - val_loss: 17.1374 - val_MinusLogProbMetric: 17.1374 - lr: 1.0417e-05 - 76s/epoch - 385ms/step
Epoch 677/1000
2023-09-27 23:44:01.426 
Epoch 677/1000 
	 loss: 16.2576, MinusLogProbMetric: 16.2576, val_loss: 17.1369, val_MinusLogProbMetric: 17.1369

Epoch 677: val_loss did not improve from 17.13156
196/196 - 74s - loss: 16.2576 - MinusLogProbMetric: 16.2576 - val_loss: 17.1369 - val_MinusLogProbMetric: 17.1369 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 678/1000
2023-09-27 23:45:16.469 
Epoch 678/1000 
	 loss: 16.2592, MinusLogProbMetric: 16.2592, val_loss: 17.1318, val_MinusLogProbMetric: 17.1318

Epoch 678: val_loss did not improve from 17.13156
196/196 - 75s - loss: 16.2592 - MinusLogProbMetric: 16.2592 - val_loss: 17.1318 - val_MinusLogProbMetric: 17.1318 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 679/1000
2023-09-27 23:46:31.301 
Epoch 679/1000 
	 loss: 16.2571, MinusLogProbMetric: 16.2571, val_loss: 17.1434, val_MinusLogProbMetric: 17.1434

Epoch 679: val_loss did not improve from 17.13156
196/196 - 75s - loss: 16.2571 - MinusLogProbMetric: 16.2571 - val_loss: 17.1434 - val_MinusLogProbMetric: 17.1434 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 680/1000
2023-09-27 23:47:46.305 
Epoch 680/1000 
	 loss: 16.2633, MinusLogProbMetric: 16.2633, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 680: val_loss improved from 17.13156 to 17.13104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2633 - MinusLogProbMetric: 16.2633 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 1.0417e-05 - 76s/epoch - 389ms/step
Epoch 681/1000
2023-09-27 23:49:01.637 
Epoch 681/1000 
	 loss: 16.2566, MinusLogProbMetric: 16.2566, val_loss: 17.1767, val_MinusLogProbMetric: 17.1767

Epoch 681: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2566 - MinusLogProbMetric: 16.2566 - val_loss: 17.1767 - val_MinusLogProbMetric: 17.1767 - lr: 1.0417e-05 - 74s/epoch - 378ms/step
Epoch 682/1000
2023-09-27 23:50:15.978 
Epoch 682/1000 
	 loss: 16.2579, MinusLogProbMetric: 16.2579, val_loss: 17.1402, val_MinusLogProbMetric: 17.1402

Epoch 682: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2579 - MinusLogProbMetric: 16.2579 - val_loss: 17.1402 - val_MinusLogProbMetric: 17.1402 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 683/1000
2023-09-27 23:51:30.575 
Epoch 683/1000 
	 loss: 16.2582, MinusLogProbMetric: 16.2582, val_loss: 17.1462, val_MinusLogProbMetric: 17.1462

Epoch 683: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2582 - MinusLogProbMetric: 16.2582 - val_loss: 17.1462 - val_MinusLogProbMetric: 17.1462 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 684/1000
2023-09-27 23:52:45.319 
Epoch 684/1000 
	 loss: 16.2580, MinusLogProbMetric: 16.2580, val_loss: 17.1464, val_MinusLogProbMetric: 17.1464

Epoch 684: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2580 - MinusLogProbMetric: 16.2580 - val_loss: 17.1464 - val_MinusLogProbMetric: 17.1464 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 685/1000
2023-09-27 23:53:59.720 
Epoch 685/1000 
	 loss: 16.2554, MinusLogProbMetric: 16.2554, val_loss: 17.1411, val_MinusLogProbMetric: 17.1411

Epoch 685: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2554 - MinusLogProbMetric: 16.2554 - val_loss: 17.1411 - val_MinusLogProbMetric: 17.1411 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 686/1000
2023-09-27 23:55:14.995 
Epoch 686/1000 
	 loss: 16.2570, MinusLogProbMetric: 16.2570, val_loss: 17.1420, val_MinusLogProbMetric: 17.1420

Epoch 686: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2570 - MinusLogProbMetric: 16.2570 - val_loss: 17.1420 - val_MinusLogProbMetric: 17.1420 - lr: 1.0417e-05 - 75s/epoch - 384ms/step
Epoch 687/1000
2023-09-27 23:56:29.610 
Epoch 687/1000 
	 loss: 16.2540, MinusLogProbMetric: 16.2540, val_loss: 17.1387, val_MinusLogProbMetric: 17.1387

Epoch 687: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2540 - MinusLogProbMetric: 16.2540 - val_loss: 17.1387 - val_MinusLogProbMetric: 17.1387 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 688/1000
2023-09-27 23:57:44.307 
Epoch 688/1000 
	 loss: 16.2580, MinusLogProbMetric: 16.2580, val_loss: 17.1359, val_MinusLogProbMetric: 17.1359

Epoch 688: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2580 - MinusLogProbMetric: 16.2580 - val_loss: 17.1359 - val_MinusLogProbMetric: 17.1359 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 689/1000
2023-09-27 23:58:59.459 
Epoch 689/1000 
	 loss: 16.2555, MinusLogProbMetric: 16.2555, val_loss: 17.1388, val_MinusLogProbMetric: 17.1388

Epoch 689: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2555 - MinusLogProbMetric: 16.2555 - val_loss: 17.1388 - val_MinusLogProbMetric: 17.1388 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 690/1000
2023-09-28 00:00:14.164 
Epoch 690/1000 
	 loss: 16.2591, MinusLogProbMetric: 16.2591, val_loss: 17.1631, val_MinusLogProbMetric: 17.1631

Epoch 690: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2591 - MinusLogProbMetric: 16.2591 - val_loss: 17.1631 - val_MinusLogProbMetric: 17.1631 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 691/1000
2023-09-28 00:01:28.933 
Epoch 691/1000 
	 loss: 16.2580, MinusLogProbMetric: 16.2580, val_loss: 17.1420, val_MinusLogProbMetric: 17.1420

Epoch 691: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2580 - MinusLogProbMetric: 16.2580 - val_loss: 17.1420 - val_MinusLogProbMetric: 17.1420 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 692/1000
2023-09-28 00:02:44.547 
Epoch 692/1000 
	 loss: 16.2534, MinusLogProbMetric: 16.2534, val_loss: 17.1327, val_MinusLogProbMetric: 17.1327

Epoch 692: val_loss did not improve from 17.13104
196/196 - 76s - loss: 16.2534 - MinusLogProbMetric: 16.2534 - val_loss: 17.1327 - val_MinusLogProbMetric: 17.1327 - lr: 1.0417e-05 - 76s/epoch - 386ms/step
Epoch 693/1000
2023-09-28 00:03:59.070 
Epoch 693/1000 
	 loss: 16.2548, MinusLogProbMetric: 16.2548, val_loss: 17.1633, val_MinusLogProbMetric: 17.1633

Epoch 693: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2548 - MinusLogProbMetric: 16.2548 - val_loss: 17.1633 - val_MinusLogProbMetric: 17.1633 - lr: 1.0417e-05 - 75s/epoch - 380ms/step
Epoch 694/1000
2023-09-28 00:05:13.804 
Epoch 694/1000 
	 loss: 16.2585, MinusLogProbMetric: 16.2585, val_loss: 17.1572, val_MinusLogProbMetric: 17.1572

Epoch 694: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2585 - MinusLogProbMetric: 16.2585 - val_loss: 17.1572 - val_MinusLogProbMetric: 17.1572 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 695/1000
2023-09-28 00:06:28.537 
Epoch 695/1000 
	 loss: 16.2551, MinusLogProbMetric: 16.2551, val_loss: 17.1366, val_MinusLogProbMetric: 17.1366

Epoch 695: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2551 - MinusLogProbMetric: 16.2551 - val_loss: 17.1366 - val_MinusLogProbMetric: 17.1366 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 696/1000
2023-09-28 00:07:43.699 
Epoch 696/1000 
	 loss: 16.2598, MinusLogProbMetric: 16.2598, val_loss: 17.1339, val_MinusLogProbMetric: 17.1339

Epoch 696: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2598 - MinusLogProbMetric: 16.2598 - val_loss: 17.1339 - val_MinusLogProbMetric: 17.1339 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 697/1000
2023-09-28 00:08:58.398 
Epoch 697/1000 
	 loss: 16.2553, MinusLogProbMetric: 16.2553, val_loss: 17.1807, val_MinusLogProbMetric: 17.1807

Epoch 697: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2553 - MinusLogProbMetric: 16.2553 - val_loss: 17.1807 - val_MinusLogProbMetric: 17.1807 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 698/1000
2023-09-28 00:10:12.621 
Epoch 698/1000 
	 loss: 16.2559, MinusLogProbMetric: 16.2559, val_loss: 17.1411, val_MinusLogProbMetric: 17.1411

Epoch 698: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2559 - MinusLogProbMetric: 16.2559 - val_loss: 17.1411 - val_MinusLogProbMetric: 17.1411 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 699/1000
2023-09-28 00:11:27.629 
Epoch 699/1000 
	 loss: 16.2556, MinusLogProbMetric: 16.2556, val_loss: 17.1457, val_MinusLogProbMetric: 17.1457

Epoch 699: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2556 - MinusLogProbMetric: 16.2556 - val_loss: 17.1457 - val_MinusLogProbMetric: 17.1457 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 700/1000
2023-09-28 00:12:42.181 
Epoch 700/1000 
	 loss: 16.2555, MinusLogProbMetric: 16.2555, val_loss: 17.1359, val_MinusLogProbMetric: 17.1359

Epoch 700: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2555 - MinusLogProbMetric: 16.2555 - val_loss: 17.1359 - val_MinusLogProbMetric: 17.1359 - lr: 1.0417e-05 - 75s/epoch - 380ms/step
Epoch 701/1000
2023-09-28 00:13:56.447 
Epoch 701/1000 
	 loss: 16.2547, MinusLogProbMetric: 16.2547, val_loss: 17.1380, val_MinusLogProbMetric: 17.1380

Epoch 701: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2547 - MinusLogProbMetric: 16.2547 - val_loss: 17.1380 - val_MinusLogProbMetric: 17.1380 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 702/1000
2023-09-28 00:15:11.497 
Epoch 702/1000 
	 loss: 16.2555, MinusLogProbMetric: 16.2555, val_loss: 17.1552, val_MinusLogProbMetric: 17.1552

Epoch 702: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2555 - MinusLogProbMetric: 16.2555 - val_loss: 17.1552 - val_MinusLogProbMetric: 17.1552 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 703/1000
2023-09-28 00:16:26.767 
Epoch 703/1000 
	 loss: 16.2567, MinusLogProbMetric: 16.2567, val_loss: 17.1422, val_MinusLogProbMetric: 17.1422

Epoch 703: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2567 - MinusLogProbMetric: 16.2567 - val_loss: 17.1422 - val_MinusLogProbMetric: 17.1422 - lr: 1.0417e-05 - 75s/epoch - 384ms/step
Epoch 704/1000
2023-09-28 00:17:42.103 
Epoch 704/1000 
	 loss: 16.2560, MinusLogProbMetric: 16.2560, val_loss: 17.1374, val_MinusLogProbMetric: 17.1374

Epoch 704: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2560 - MinusLogProbMetric: 16.2560 - val_loss: 17.1374 - val_MinusLogProbMetric: 17.1374 - lr: 1.0417e-05 - 75s/epoch - 384ms/step
Epoch 705/1000
2023-09-28 00:18:56.817 
Epoch 705/1000 
	 loss: 16.2552, MinusLogProbMetric: 16.2552, val_loss: 17.1366, val_MinusLogProbMetric: 17.1366

Epoch 705: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2552 - MinusLogProbMetric: 16.2552 - val_loss: 17.1366 - val_MinusLogProbMetric: 17.1366 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 706/1000
2023-09-28 00:20:11.590 
Epoch 706/1000 
	 loss: 16.2541, MinusLogProbMetric: 16.2541, val_loss: 17.1364, val_MinusLogProbMetric: 17.1364

Epoch 706: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2541 - MinusLogProbMetric: 16.2541 - val_loss: 17.1364 - val_MinusLogProbMetric: 17.1364 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 707/1000
2023-09-28 00:21:25.803 
Epoch 707/1000 
	 loss: 16.2549, MinusLogProbMetric: 16.2549, val_loss: 17.1382, val_MinusLogProbMetric: 17.1382

Epoch 707: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2549 - MinusLogProbMetric: 16.2549 - val_loss: 17.1382 - val_MinusLogProbMetric: 17.1382 - lr: 1.0417e-05 - 74s/epoch - 379ms/step
Epoch 708/1000
2023-09-28 00:22:40.230 
Epoch 708/1000 
	 loss: 16.2553, MinusLogProbMetric: 16.2553, val_loss: 17.1366, val_MinusLogProbMetric: 17.1366

Epoch 708: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2553 - MinusLogProbMetric: 16.2553 - val_loss: 17.1366 - val_MinusLogProbMetric: 17.1366 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 709/1000
2023-09-28 00:23:55.334 
Epoch 709/1000 
	 loss: 16.2530, MinusLogProbMetric: 16.2530, val_loss: 17.1474, val_MinusLogProbMetric: 17.1474

Epoch 709: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2530 - MinusLogProbMetric: 16.2530 - val_loss: 17.1474 - val_MinusLogProbMetric: 17.1474 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 710/1000
2023-09-28 00:25:10.390 
Epoch 710/1000 
	 loss: 16.2555, MinusLogProbMetric: 16.2555, val_loss: 17.1419, val_MinusLogProbMetric: 17.1419

Epoch 710: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2555 - MinusLogProbMetric: 16.2555 - val_loss: 17.1419 - val_MinusLogProbMetric: 17.1419 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 711/1000
2023-09-28 00:26:25.295 
Epoch 711/1000 
	 loss: 16.2569, MinusLogProbMetric: 16.2569, val_loss: 17.1499, val_MinusLogProbMetric: 17.1499

Epoch 711: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2569 - MinusLogProbMetric: 16.2569 - val_loss: 17.1499 - val_MinusLogProbMetric: 17.1499 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 712/1000
2023-09-28 00:27:40.119 
Epoch 712/1000 
	 loss: 16.2553, MinusLogProbMetric: 16.2553, val_loss: 17.1381, val_MinusLogProbMetric: 17.1381

Epoch 712: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2553 - MinusLogProbMetric: 16.2553 - val_loss: 17.1381 - val_MinusLogProbMetric: 17.1381 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 713/1000
2023-09-28 00:28:55.167 
Epoch 713/1000 
	 loss: 16.2528, MinusLogProbMetric: 16.2528, val_loss: 17.1383, val_MinusLogProbMetric: 17.1383

Epoch 713: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2528 - MinusLogProbMetric: 16.2528 - val_loss: 17.1383 - val_MinusLogProbMetric: 17.1383 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 714/1000
2023-09-28 00:30:10.146 
Epoch 714/1000 
	 loss: 16.2545, MinusLogProbMetric: 16.2545, val_loss: 17.1431, val_MinusLogProbMetric: 17.1431

Epoch 714: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2545 - MinusLogProbMetric: 16.2545 - val_loss: 17.1431 - val_MinusLogProbMetric: 17.1431 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 715/1000
2023-09-28 00:31:24.274 
Epoch 715/1000 
	 loss: 16.2582, MinusLogProbMetric: 16.2582, val_loss: 17.1443, val_MinusLogProbMetric: 17.1443

Epoch 715: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2582 - MinusLogProbMetric: 16.2582 - val_loss: 17.1443 - val_MinusLogProbMetric: 17.1443 - lr: 1.0417e-05 - 74s/epoch - 378ms/step
Epoch 716/1000
2023-09-28 00:32:39.199 
Epoch 716/1000 
	 loss: 16.2538, MinusLogProbMetric: 16.2538, val_loss: 17.1395, val_MinusLogProbMetric: 17.1395

Epoch 716: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2538 - MinusLogProbMetric: 16.2538 - val_loss: 17.1395 - val_MinusLogProbMetric: 17.1395 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 717/1000
2023-09-28 00:33:53.833 
Epoch 717/1000 
	 loss: 16.2590, MinusLogProbMetric: 16.2590, val_loss: 17.1470, val_MinusLogProbMetric: 17.1470

Epoch 717: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2590 - MinusLogProbMetric: 16.2590 - val_loss: 17.1470 - val_MinusLogProbMetric: 17.1470 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 718/1000
2023-09-28 00:35:09.096 
Epoch 718/1000 
	 loss: 16.2543, MinusLogProbMetric: 16.2543, val_loss: 17.1393, val_MinusLogProbMetric: 17.1393

Epoch 718: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2543 - MinusLogProbMetric: 16.2543 - val_loss: 17.1393 - val_MinusLogProbMetric: 17.1393 - lr: 1.0417e-05 - 75s/epoch - 384ms/step
Epoch 719/1000
2023-09-28 00:36:24.418 
Epoch 719/1000 
	 loss: 16.2549, MinusLogProbMetric: 16.2549, val_loss: 17.1440, val_MinusLogProbMetric: 17.1440

Epoch 719: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2549 - MinusLogProbMetric: 16.2549 - val_loss: 17.1440 - val_MinusLogProbMetric: 17.1440 - lr: 1.0417e-05 - 75s/epoch - 384ms/step
Epoch 720/1000
2023-09-28 00:37:35.974 
Epoch 720/1000 
	 loss: 16.2526, MinusLogProbMetric: 16.2526, val_loss: 17.1388, val_MinusLogProbMetric: 17.1388

Epoch 720: val_loss did not improve from 17.13104
196/196 - 72s - loss: 16.2526 - MinusLogProbMetric: 16.2526 - val_loss: 17.1388 - val_MinusLogProbMetric: 17.1388 - lr: 1.0417e-05 - 72s/epoch - 365ms/step
Epoch 721/1000
2023-09-28 00:38:44.946 
Epoch 721/1000 
	 loss: 16.2597, MinusLogProbMetric: 16.2597, val_loss: 17.1333, val_MinusLogProbMetric: 17.1333

Epoch 721: val_loss did not improve from 17.13104
196/196 - 69s - loss: 16.2597 - MinusLogProbMetric: 16.2597 - val_loss: 17.1333 - val_MinusLogProbMetric: 17.1333 - lr: 1.0417e-05 - 69s/epoch - 352ms/step
Epoch 722/1000
2023-09-28 00:39:55.814 
Epoch 722/1000 
	 loss: 16.2540, MinusLogProbMetric: 16.2540, val_loss: 17.1342, val_MinusLogProbMetric: 17.1342

Epoch 722: val_loss did not improve from 17.13104
196/196 - 71s - loss: 16.2540 - MinusLogProbMetric: 16.2540 - val_loss: 17.1342 - val_MinusLogProbMetric: 17.1342 - lr: 1.0417e-05 - 71s/epoch - 362ms/step
Epoch 723/1000
2023-09-28 00:41:03.316 
Epoch 723/1000 
	 loss: 16.2550, MinusLogProbMetric: 16.2550, val_loss: 17.1379, val_MinusLogProbMetric: 17.1379

Epoch 723: val_loss did not improve from 17.13104
196/196 - 67s - loss: 16.2550 - MinusLogProbMetric: 16.2550 - val_loss: 17.1379 - val_MinusLogProbMetric: 17.1379 - lr: 1.0417e-05 - 67s/epoch - 344ms/step
Epoch 724/1000
2023-09-28 00:42:18.158 
Epoch 724/1000 
	 loss: 16.2543, MinusLogProbMetric: 16.2543, val_loss: 17.1398, val_MinusLogProbMetric: 17.1398

Epoch 724: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2543 - MinusLogProbMetric: 16.2543 - val_loss: 17.1398 - val_MinusLogProbMetric: 17.1398 - lr: 1.0417e-05 - 75s/epoch - 382ms/step
Epoch 725/1000
2023-09-28 00:43:33.335 
Epoch 725/1000 
	 loss: 16.2552, MinusLogProbMetric: 16.2552, val_loss: 17.1376, val_MinusLogProbMetric: 17.1376

Epoch 725: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2552 - MinusLogProbMetric: 16.2552 - val_loss: 17.1376 - val_MinusLogProbMetric: 17.1376 - lr: 1.0417e-05 - 75s/epoch - 384ms/step
Epoch 726/1000
2023-09-28 00:44:47.723 
Epoch 726/1000 
	 loss: 16.2519, MinusLogProbMetric: 16.2519, val_loss: 17.1435, val_MinusLogProbMetric: 17.1435

Epoch 726: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2519 - MinusLogProbMetric: 16.2519 - val_loss: 17.1435 - val_MinusLogProbMetric: 17.1435 - lr: 1.0417e-05 - 74s/epoch - 380ms/step
Epoch 727/1000
2023-09-28 00:46:02.500 
Epoch 727/1000 
	 loss: 16.2518, MinusLogProbMetric: 16.2518, val_loss: 17.1312, val_MinusLogProbMetric: 17.1312

Epoch 727: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2518 - MinusLogProbMetric: 16.2518 - val_loss: 17.1312 - val_MinusLogProbMetric: 17.1312 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 728/1000
2023-09-28 00:47:17.543 
Epoch 728/1000 
	 loss: 16.2521, MinusLogProbMetric: 16.2521, val_loss: 17.1391, val_MinusLogProbMetric: 17.1391

Epoch 728: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2521 - MinusLogProbMetric: 16.2521 - val_loss: 17.1391 - val_MinusLogProbMetric: 17.1391 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 729/1000
2023-09-28 00:48:32.146 
Epoch 729/1000 
	 loss: 16.2509, MinusLogProbMetric: 16.2509, val_loss: 17.1498, val_MinusLogProbMetric: 17.1498

Epoch 729: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2509 - MinusLogProbMetric: 16.2509 - val_loss: 17.1498 - val_MinusLogProbMetric: 17.1498 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 730/1000
2023-09-28 00:49:47.258 
Epoch 730/1000 
	 loss: 16.2533, MinusLogProbMetric: 16.2533, val_loss: 17.1329, val_MinusLogProbMetric: 17.1329

Epoch 730: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2533 - MinusLogProbMetric: 16.2533 - val_loss: 17.1329 - val_MinusLogProbMetric: 17.1329 - lr: 1.0417e-05 - 75s/epoch - 383ms/step
Epoch 731/1000
2023-09-28 00:51:01.961 
Epoch 731/1000 
	 loss: 16.2427, MinusLogProbMetric: 16.2427, val_loss: 17.1316, val_MinusLogProbMetric: 17.1316

Epoch 731: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2427 - MinusLogProbMetric: 16.2427 - val_loss: 17.1316 - val_MinusLogProbMetric: 17.1316 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 732/1000
2023-09-28 00:52:16.797 
Epoch 732/1000 
	 loss: 16.2432, MinusLogProbMetric: 16.2432, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 732: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2432 - MinusLogProbMetric: 16.2432 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 733/1000
2023-09-28 00:53:31.057 
Epoch 733/1000 
	 loss: 16.2418, MinusLogProbMetric: 16.2418, val_loss: 17.1439, val_MinusLogProbMetric: 17.1439

Epoch 733: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2418 - MinusLogProbMetric: 16.2418 - val_loss: 17.1439 - val_MinusLogProbMetric: 17.1439 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 734/1000
2023-09-28 00:54:46.154 
Epoch 734/1000 
	 loss: 16.2433, MinusLogProbMetric: 16.2433, val_loss: 17.1341, val_MinusLogProbMetric: 17.1341

Epoch 734: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2433 - MinusLogProbMetric: 16.2433 - val_loss: 17.1341 - val_MinusLogProbMetric: 17.1341 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 735/1000
2023-09-28 00:56:00.758 
Epoch 735/1000 
	 loss: 16.2430, MinusLogProbMetric: 16.2430, val_loss: 17.1457, val_MinusLogProbMetric: 17.1457

Epoch 735: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2430 - MinusLogProbMetric: 16.2430 - val_loss: 17.1457 - val_MinusLogProbMetric: 17.1457 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 736/1000
2023-09-28 00:57:15.904 
Epoch 736/1000 
	 loss: 16.2431, MinusLogProbMetric: 16.2431, val_loss: 17.1332, val_MinusLogProbMetric: 17.1332

Epoch 736: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2431 - MinusLogProbMetric: 16.2431 - val_loss: 17.1332 - val_MinusLogProbMetric: 17.1332 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 737/1000
2023-09-28 00:58:30.486 
Epoch 737/1000 
	 loss: 16.2430, MinusLogProbMetric: 16.2430, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 737: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2430 - MinusLogProbMetric: 16.2430 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 5.2083e-06 - 75s/epoch - 380ms/step
Epoch 738/1000
2023-09-28 00:59:45.540 
Epoch 738/1000 
	 loss: 16.2433, MinusLogProbMetric: 16.2433, val_loss: 17.1321, val_MinusLogProbMetric: 17.1321

Epoch 738: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2433 - MinusLogProbMetric: 16.2433 - val_loss: 17.1321 - val_MinusLogProbMetric: 17.1321 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 739/1000
2023-09-28 01:01:00.903 
Epoch 739/1000 
	 loss: 16.2425, MinusLogProbMetric: 16.2425, val_loss: 17.1330, val_MinusLogProbMetric: 17.1330

Epoch 739: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2425 - MinusLogProbMetric: 16.2425 - val_loss: 17.1330 - val_MinusLogProbMetric: 17.1330 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 740/1000
2023-09-28 01:02:15.603 
Epoch 740/1000 
	 loss: 16.2433, MinusLogProbMetric: 16.2433, val_loss: 17.1329, val_MinusLogProbMetric: 17.1329

Epoch 740: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2433 - MinusLogProbMetric: 16.2433 - val_loss: 17.1329 - val_MinusLogProbMetric: 17.1329 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 741/1000
2023-09-28 01:03:30.452 
Epoch 741/1000 
	 loss: 16.2423, MinusLogProbMetric: 16.2423, val_loss: 17.1321, val_MinusLogProbMetric: 17.1321

Epoch 741: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2423 - MinusLogProbMetric: 16.2423 - val_loss: 17.1321 - val_MinusLogProbMetric: 17.1321 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 742/1000
2023-09-28 01:04:45.348 
Epoch 742/1000 
	 loss: 16.2416, MinusLogProbMetric: 16.2416, val_loss: 17.1332, val_MinusLogProbMetric: 17.1332

Epoch 742: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2416 - MinusLogProbMetric: 16.2416 - val_loss: 17.1332 - val_MinusLogProbMetric: 17.1332 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 743/1000
2023-09-28 01:06:00.359 
Epoch 743/1000 
	 loss: 16.2430, MinusLogProbMetric: 16.2430, val_loss: 17.1314, val_MinusLogProbMetric: 17.1314

Epoch 743: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2430 - MinusLogProbMetric: 16.2430 - val_loss: 17.1314 - val_MinusLogProbMetric: 17.1314 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 744/1000
2023-09-28 01:07:14.595 
Epoch 744/1000 
	 loss: 16.2411, MinusLogProbMetric: 16.2411, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 744: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2411 - MinusLogProbMetric: 16.2411 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 745/1000
2023-09-28 01:08:29.936 
Epoch 745/1000 
	 loss: 16.2431, MinusLogProbMetric: 16.2431, val_loss: 17.1369, val_MinusLogProbMetric: 17.1369

Epoch 745: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2431 - MinusLogProbMetric: 16.2431 - val_loss: 17.1369 - val_MinusLogProbMetric: 17.1369 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 746/1000
2023-09-28 01:09:45.026 
Epoch 746/1000 
	 loss: 16.2434, MinusLogProbMetric: 16.2434, val_loss: 17.1317, val_MinusLogProbMetric: 17.1317

Epoch 746: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2434 - MinusLogProbMetric: 16.2434 - val_loss: 17.1317 - val_MinusLogProbMetric: 17.1317 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 747/1000
2023-09-28 01:11:00.144 
Epoch 747/1000 
	 loss: 16.2421, MinusLogProbMetric: 16.2421, val_loss: 17.1325, val_MinusLogProbMetric: 17.1325

Epoch 747: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2421 - MinusLogProbMetric: 16.2421 - val_loss: 17.1325 - val_MinusLogProbMetric: 17.1325 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 748/1000
2023-09-28 01:12:14.307 
Epoch 748/1000 
	 loss: 16.2420, MinusLogProbMetric: 16.2420, val_loss: 17.1363, val_MinusLogProbMetric: 17.1363

Epoch 748: val_loss did not improve from 17.13104
196/196 - 74s - loss: 16.2420 - MinusLogProbMetric: 16.2420 - val_loss: 17.1363 - val_MinusLogProbMetric: 17.1363 - lr: 5.2083e-06 - 74s/epoch - 378ms/step
Epoch 749/1000
2023-09-28 01:13:29.240 
Epoch 749/1000 
	 loss: 16.2423, MinusLogProbMetric: 16.2423, val_loss: 17.1314, val_MinusLogProbMetric: 17.1314

Epoch 749: val_loss did not improve from 17.13104
196/196 - 75s - loss: 16.2423 - MinusLogProbMetric: 16.2423 - val_loss: 17.1314 - val_MinusLogProbMetric: 17.1314 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 750/1000
2023-09-28 01:14:43.991 
Epoch 750/1000 
	 loss: 16.2418, MinusLogProbMetric: 16.2418, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 750: val_loss improved from 17.13104 to 17.13075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2418 - MinusLogProbMetric: 16.2418 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 5.2083e-06 - 76s/epoch - 388ms/step
Epoch 751/1000
2023-09-28 01:16:00.251 
Epoch 751/1000 
	 loss: 16.2425, MinusLogProbMetric: 16.2425, val_loss: 17.1356, val_MinusLogProbMetric: 17.1356

Epoch 751: val_loss did not improve from 17.13075
196/196 - 75s - loss: 16.2425 - MinusLogProbMetric: 16.2425 - val_loss: 17.1356 - val_MinusLogProbMetric: 17.1356 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 752/1000
2023-09-28 01:17:15.706 
Epoch 752/1000 
	 loss: 16.2420, MinusLogProbMetric: 16.2420, val_loss: 17.1338, val_MinusLogProbMetric: 17.1338

Epoch 752: val_loss did not improve from 17.13075
196/196 - 75s - loss: 16.2420 - MinusLogProbMetric: 16.2420 - val_loss: 17.1338 - val_MinusLogProbMetric: 17.1338 - lr: 5.2083e-06 - 75s/epoch - 385ms/step
Epoch 753/1000
2023-09-28 01:18:31.504 
Epoch 753/1000 
	 loss: 16.2417, MinusLogProbMetric: 16.2417, val_loss: 17.1329, val_MinusLogProbMetric: 17.1329

Epoch 753: val_loss did not improve from 17.13075
196/196 - 76s - loss: 16.2417 - MinusLogProbMetric: 16.2417 - val_loss: 17.1329 - val_MinusLogProbMetric: 17.1329 - lr: 5.2083e-06 - 76s/epoch - 387ms/step
Epoch 754/1000
2023-09-28 01:19:46.695 
Epoch 754/1000 
	 loss: 16.2414, MinusLogProbMetric: 16.2414, val_loss: 17.1365, val_MinusLogProbMetric: 17.1365

Epoch 754: val_loss did not improve from 17.13075
196/196 - 75s - loss: 16.2414 - MinusLogProbMetric: 16.2414 - val_loss: 17.1365 - val_MinusLogProbMetric: 17.1365 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 755/1000
2023-09-28 01:21:01.235 
Epoch 755/1000 
	 loss: 16.2415, MinusLogProbMetric: 16.2415, val_loss: 17.1315, val_MinusLogProbMetric: 17.1315

Epoch 755: val_loss did not improve from 17.13075
196/196 - 75s - loss: 16.2415 - MinusLogProbMetric: 16.2415 - val_loss: 17.1315 - val_MinusLogProbMetric: 17.1315 - lr: 5.2083e-06 - 75s/epoch - 380ms/step
Epoch 756/1000
2023-09-28 01:22:15.499 
Epoch 756/1000 
	 loss: 16.2430, MinusLogProbMetric: 16.2430, val_loss: 17.1336, val_MinusLogProbMetric: 17.1336

Epoch 756: val_loss did not improve from 17.13075
196/196 - 74s - loss: 16.2430 - MinusLogProbMetric: 16.2430 - val_loss: 17.1336 - val_MinusLogProbMetric: 17.1336 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 757/1000
2023-09-28 01:23:30.007 
Epoch 757/1000 
	 loss: 16.2403, MinusLogProbMetric: 16.2403, val_loss: 17.1305, val_MinusLogProbMetric: 17.1305

Epoch 757: val_loss improved from 17.13075 to 17.13049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2403 - MinusLogProbMetric: 16.2403 - val_loss: 17.1305 - val_MinusLogProbMetric: 17.1305 - lr: 5.2083e-06 - 76s/epoch - 386ms/step
Epoch 758/1000
2023-09-28 01:24:45.694 
Epoch 758/1000 
	 loss: 16.2411, MinusLogProbMetric: 16.2411, val_loss: 17.1339, val_MinusLogProbMetric: 17.1339

Epoch 758: val_loss did not improve from 17.13049
196/196 - 74s - loss: 16.2411 - MinusLogProbMetric: 16.2411 - val_loss: 17.1339 - val_MinusLogProbMetric: 17.1339 - lr: 5.2083e-06 - 74s/epoch - 380ms/step
Epoch 759/1000
2023-09-28 01:26:00.221 
Epoch 759/1000 
	 loss: 16.2430, MinusLogProbMetric: 16.2430, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 759: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2430 - MinusLogProbMetric: 16.2430 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 5.2083e-06 - 75s/epoch - 380ms/step
Epoch 760/1000
2023-09-28 01:27:15.262 
Epoch 760/1000 
	 loss: 16.2424, MinusLogProbMetric: 16.2424, val_loss: 17.1343, val_MinusLogProbMetric: 17.1343

Epoch 760: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2424 - MinusLogProbMetric: 16.2424 - val_loss: 17.1343 - val_MinusLogProbMetric: 17.1343 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 761/1000
2023-09-28 01:28:30.152 
Epoch 761/1000 
	 loss: 16.2422, MinusLogProbMetric: 16.2422, val_loss: 17.1316, val_MinusLogProbMetric: 17.1316

Epoch 761: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2422 - MinusLogProbMetric: 16.2422 - val_loss: 17.1316 - val_MinusLogProbMetric: 17.1316 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 762/1000
2023-09-28 01:29:44.650 
Epoch 762/1000 
	 loss: 16.2410, MinusLogProbMetric: 16.2410, val_loss: 17.1353, val_MinusLogProbMetric: 17.1353

Epoch 762: val_loss did not improve from 17.13049
196/196 - 74s - loss: 16.2410 - MinusLogProbMetric: 16.2410 - val_loss: 17.1353 - val_MinusLogProbMetric: 17.1353 - lr: 5.2083e-06 - 74s/epoch - 380ms/step
Epoch 763/1000
2023-09-28 01:30:58.911 
Epoch 763/1000 
	 loss: 16.2421, MinusLogProbMetric: 16.2421, val_loss: 17.1312, val_MinusLogProbMetric: 17.1312

Epoch 763: val_loss did not improve from 17.13049
196/196 - 74s - loss: 16.2421 - MinusLogProbMetric: 16.2421 - val_loss: 17.1312 - val_MinusLogProbMetric: 17.1312 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 764/1000
2023-09-28 01:32:13.983 
Epoch 764/1000 
	 loss: 16.2414, MinusLogProbMetric: 16.2414, val_loss: 17.1344, val_MinusLogProbMetric: 17.1344

Epoch 764: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2414 - MinusLogProbMetric: 16.2414 - val_loss: 17.1344 - val_MinusLogProbMetric: 17.1344 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 765/1000
2023-09-28 01:33:28.899 
Epoch 765/1000 
	 loss: 16.2419, MinusLogProbMetric: 16.2419, val_loss: 17.1358, val_MinusLogProbMetric: 17.1358

Epoch 765: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2419 - MinusLogProbMetric: 16.2419 - val_loss: 17.1358 - val_MinusLogProbMetric: 17.1358 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 766/1000
2023-09-28 01:34:43.608 
Epoch 766/1000 
	 loss: 16.2415, MinusLogProbMetric: 16.2415, val_loss: 17.1362, val_MinusLogProbMetric: 17.1362

Epoch 766: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2415 - MinusLogProbMetric: 16.2415 - val_loss: 17.1362 - val_MinusLogProbMetric: 17.1362 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 767/1000
2023-09-28 01:35:58.137 
Epoch 767/1000 
	 loss: 16.2408, MinusLogProbMetric: 16.2408, val_loss: 17.1311, val_MinusLogProbMetric: 17.1311

Epoch 767: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2408 - MinusLogProbMetric: 16.2408 - val_loss: 17.1311 - val_MinusLogProbMetric: 17.1311 - lr: 5.2083e-06 - 75s/epoch - 380ms/step
Epoch 768/1000
2023-09-28 01:37:12.942 
Epoch 768/1000 
	 loss: 16.2416, MinusLogProbMetric: 16.2416, val_loss: 17.1356, val_MinusLogProbMetric: 17.1356

Epoch 768: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2416 - MinusLogProbMetric: 16.2416 - val_loss: 17.1356 - val_MinusLogProbMetric: 17.1356 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 769/1000
2023-09-28 01:38:28.261 
Epoch 769/1000 
	 loss: 16.2404, MinusLogProbMetric: 16.2404, val_loss: 17.1325, val_MinusLogProbMetric: 17.1325

Epoch 769: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2404 - MinusLogProbMetric: 16.2404 - val_loss: 17.1325 - val_MinusLogProbMetric: 17.1325 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 770/1000
2023-09-28 01:39:42.443 
Epoch 770/1000 
	 loss: 16.2427, MinusLogProbMetric: 16.2427, val_loss: 17.1390, val_MinusLogProbMetric: 17.1390

Epoch 770: val_loss did not improve from 17.13049
196/196 - 74s - loss: 16.2427 - MinusLogProbMetric: 16.2427 - val_loss: 17.1390 - val_MinusLogProbMetric: 17.1390 - lr: 5.2083e-06 - 74s/epoch - 378ms/step
Epoch 771/1000
2023-09-28 01:40:51.956 
Epoch 771/1000 
	 loss: 16.2421, MinusLogProbMetric: 16.2421, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 771: val_loss did not improve from 17.13049
196/196 - 70s - loss: 16.2421 - MinusLogProbMetric: 16.2421 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 5.2083e-06 - 70s/epoch - 355ms/step
Epoch 772/1000
2023-09-28 01:41:59.913 
Epoch 772/1000 
	 loss: 16.2409, MinusLogProbMetric: 16.2409, val_loss: 17.1365, val_MinusLogProbMetric: 17.1365

Epoch 772: val_loss did not improve from 17.13049
196/196 - 68s - loss: 16.2409 - MinusLogProbMetric: 16.2409 - val_loss: 17.1365 - val_MinusLogProbMetric: 17.1365 - lr: 5.2083e-06 - 68s/epoch - 347ms/step
Epoch 773/1000
2023-09-28 01:43:10.199 
Epoch 773/1000 
	 loss: 16.2410, MinusLogProbMetric: 16.2410, val_loss: 17.1379, val_MinusLogProbMetric: 17.1379

Epoch 773: val_loss did not improve from 17.13049
196/196 - 70s - loss: 16.2410 - MinusLogProbMetric: 16.2410 - val_loss: 17.1379 - val_MinusLogProbMetric: 17.1379 - lr: 5.2083e-06 - 70s/epoch - 359ms/step
Epoch 774/1000
2023-09-28 01:44:17.103 
Epoch 774/1000 
	 loss: 16.2406, MinusLogProbMetric: 16.2406, val_loss: 17.1321, val_MinusLogProbMetric: 17.1321

Epoch 774: val_loss did not improve from 17.13049
196/196 - 67s - loss: 16.2406 - MinusLogProbMetric: 16.2406 - val_loss: 17.1321 - val_MinusLogProbMetric: 17.1321 - lr: 5.2083e-06 - 67s/epoch - 341ms/step
Epoch 775/1000
2023-09-28 01:45:32.147 
Epoch 775/1000 
	 loss: 16.2432, MinusLogProbMetric: 16.2432, val_loss: 17.1347, val_MinusLogProbMetric: 17.1347

Epoch 775: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2432 - MinusLogProbMetric: 16.2432 - val_loss: 17.1347 - val_MinusLogProbMetric: 17.1347 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 776/1000
2023-09-28 01:46:46.450 
Epoch 776/1000 
	 loss: 16.2412, MinusLogProbMetric: 16.2412, val_loss: 17.1389, val_MinusLogProbMetric: 17.1389

Epoch 776: val_loss did not improve from 17.13049
196/196 - 74s - loss: 16.2412 - MinusLogProbMetric: 16.2412 - val_loss: 17.1389 - val_MinusLogProbMetric: 17.1389 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 777/1000
2023-09-28 01:48:01.082 
Epoch 777/1000 
	 loss: 16.2403, MinusLogProbMetric: 16.2403, val_loss: 17.1344, val_MinusLogProbMetric: 17.1344

Epoch 777: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2403 - MinusLogProbMetric: 16.2403 - val_loss: 17.1344 - val_MinusLogProbMetric: 17.1344 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 778/1000
2023-09-28 01:49:15.522 
Epoch 778/1000 
	 loss: 16.2417, MinusLogProbMetric: 16.2417, val_loss: 17.1349, val_MinusLogProbMetric: 17.1349

Epoch 778: val_loss did not improve from 17.13049
196/196 - 74s - loss: 16.2417 - MinusLogProbMetric: 16.2417 - val_loss: 17.1349 - val_MinusLogProbMetric: 17.1349 - lr: 5.2083e-06 - 74s/epoch - 380ms/step
Epoch 779/1000
2023-09-28 01:50:30.558 
Epoch 779/1000 
	 loss: 16.2401, MinusLogProbMetric: 16.2401, val_loss: 17.1524, val_MinusLogProbMetric: 17.1524

Epoch 779: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2401 - MinusLogProbMetric: 16.2401 - val_loss: 17.1524 - val_MinusLogProbMetric: 17.1524 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 780/1000
2023-09-28 01:51:45.867 
Epoch 780/1000 
	 loss: 16.2426, MinusLogProbMetric: 16.2426, val_loss: 17.1402, val_MinusLogProbMetric: 17.1402

Epoch 780: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2426 - MinusLogProbMetric: 16.2426 - val_loss: 17.1402 - val_MinusLogProbMetric: 17.1402 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 781/1000
2023-09-28 01:53:01.196 
Epoch 781/1000 
	 loss: 16.2407, MinusLogProbMetric: 16.2407, val_loss: 17.1388, val_MinusLogProbMetric: 17.1388

Epoch 781: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2407 - MinusLogProbMetric: 16.2407 - val_loss: 17.1388 - val_MinusLogProbMetric: 17.1388 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 782/1000
2023-09-28 01:54:16.227 
Epoch 782/1000 
	 loss: 16.2428, MinusLogProbMetric: 16.2428, val_loss: 17.1313, val_MinusLogProbMetric: 17.1313

Epoch 782: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2428 - MinusLogProbMetric: 16.2428 - val_loss: 17.1313 - val_MinusLogProbMetric: 17.1313 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 783/1000
2023-09-28 01:55:31.503 
Epoch 783/1000 
	 loss: 16.2411, MinusLogProbMetric: 16.2411, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 783: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2411 - MinusLogProbMetric: 16.2411 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 784/1000
2023-09-28 01:56:46.099 
Epoch 784/1000 
	 loss: 16.2400, MinusLogProbMetric: 16.2400, val_loss: 17.1327, val_MinusLogProbMetric: 17.1327

Epoch 784: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2400 - MinusLogProbMetric: 16.2400 - val_loss: 17.1327 - val_MinusLogProbMetric: 17.1327 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 785/1000
2023-09-28 01:58:01.385 
Epoch 785/1000 
	 loss: 16.2408, MinusLogProbMetric: 16.2408, val_loss: 17.1377, val_MinusLogProbMetric: 17.1377

Epoch 785: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2408 - MinusLogProbMetric: 16.2408 - val_loss: 17.1377 - val_MinusLogProbMetric: 17.1377 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 786/1000
2023-09-28 01:59:16.571 
Epoch 786/1000 
	 loss: 16.2406, MinusLogProbMetric: 16.2406, val_loss: 17.1332, val_MinusLogProbMetric: 17.1332

Epoch 786: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2406 - MinusLogProbMetric: 16.2406 - val_loss: 17.1332 - val_MinusLogProbMetric: 17.1332 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 787/1000
2023-09-28 02:00:31.524 
Epoch 787/1000 
	 loss: 16.2404, MinusLogProbMetric: 16.2404, val_loss: 17.1432, val_MinusLogProbMetric: 17.1432

Epoch 787: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2404 - MinusLogProbMetric: 16.2404 - val_loss: 17.1432 - val_MinusLogProbMetric: 17.1432 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 788/1000
2023-09-28 02:01:46.824 
Epoch 788/1000 
	 loss: 16.2407, MinusLogProbMetric: 16.2407, val_loss: 17.1341, val_MinusLogProbMetric: 17.1341

Epoch 788: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2407 - MinusLogProbMetric: 16.2407 - val_loss: 17.1341 - val_MinusLogProbMetric: 17.1341 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 789/1000
2023-09-28 02:03:01.831 
Epoch 789/1000 
	 loss: 16.2392, MinusLogProbMetric: 16.2392, val_loss: 17.1429, val_MinusLogProbMetric: 17.1429

Epoch 789: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2392 - MinusLogProbMetric: 16.2392 - val_loss: 17.1429 - val_MinusLogProbMetric: 17.1429 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 790/1000
2023-09-28 02:04:16.864 
Epoch 790/1000 
	 loss: 16.2405, MinusLogProbMetric: 16.2405, val_loss: 17.1343, val_MinusLogProbMetric: 17.1343

Epoch 790: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2405 - MinusLogProbMetric: 16.2405 - val_loss: 17.1343 - val_MinusLogProbMetric: 17.1343 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 791/1000
2023-09-28 02:05:32.436 
Epoch 791/1000 
	 loss: 16.2402, MinusLogProbMetric: 16.2402, val_loss: 17.1329, val_MinusLogProbMetric: 17.1329

Epoch 791: val_loss did not improve from 17.13049
196/196 - 76s - loss: 16.2402 - MinusLogProbMetric: 16.2402 - val_loss: 17.1329 - val_MinusLogProbMetric: 17.1329 - lr: 5.2083e-06 - 76s/epoch - 386ms/step
Epoch 792/1000
2023-09-28 02:06:47.304 
Epoch 792/1000 
	 loss: 16.2401, MinusLogProbMetric: 16.2401, val_loss: 17.1497, val_MinusLogProbMetric: 17.1497

Epoch 792: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2401 - MinusLogProbMetric: 16.2401 - val_loss: 17.1497 - val_MinusLogProbMetric: 17.1497 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 793/1000
2023-09-28 02:08:01.692 
Epoch 793/1000 
	 loss: 16.2404, MinusLogProbMetric: 16.2404, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 793: val_loss did not improve from 17.13049
196/196 - 74s - loss: 16.2404 - MinusLogProbMetric: 16.2404 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 5.2083e-06 - 74s/epoch - 380ms/step
Epoch 794/1000
2023-09-28 02:09:16.674 
Epoch 794/1000 
	 loss: 16.2415, MinusLogProbMetric: 16.2415, val_loss: 17.1348, val_MinusLogProbMetric: 17.1348

Epoch 794: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2415 - MinusLogProbMetric: 16.2415 - val_loss: 17.1348 - val_MinusLogProbMetric: 17.1348 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 795/1000
2023-09-28 02:10:31.363 
Epoch 795/1000 
	 loss: 16.2393, MinusLogProbMetric: 16.2393, val_loss: 17.1322, val_MinusLogProbMetric: 17.1322

Epoch 795: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2393 - MinusLogProbMetric: 16.2393 - val_loss: 17.1322 - val_MinusLogProbMetric: 17.1322 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 796/1000
2023-09-28 02:11:46.320 
Epoch 796/1000 
	 loss: 16.2390, MinusLogProbMetric: 16.2390, val_loss: 17.1364, val_MinusLogProbMetric: 17.1364

Epoch 796: val_loss did not improve from 17.13049
196/196 - 75s - loss: 16.2390 - MinusLogProbMetric: 16.2390 - val_loss: 17.1364 - val_MinusLogProbMetric: 17.1364 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 797/1000
2023-09-28 02:12:59.839 
Epoch 797/1000 
	 loss: 16.2393, MinusLogProbMetric: 16.2393, val_loss: 17.1299, val_MinusLogProbMetric: 17.1299

Epoch 797: val_loss improved from 17.13049 to 17.12992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 75s - loss: 16.2393 - MinusLogProbMetric: 16.2393 - val_loss: 17.1299 - val_MinusLogProbMetric: 17.1299 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 798/1000
2023-09-28 02:14:15.982 
Epoch 798/1000 
	 loss: 16.2403, MinusLogProbMetric: 16.2403, val_loss: 17.1352, val_MinusLogProbMetric: 17.1352

Epoch 798: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2403 - MinusLogProbMetric: 16.2403 - val_loss: 17.1352 - val_MinusLogProbMetric: 17.1352 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 799/1000
2023-09-28 02:15:30.651 
Epoch 799/1000 
	 loss: 16.2405, MinusLogProbMetric: 16.2405, val_loss: 17.1322, val_MinusLogProbMetric: 17.1322

Epoch 799: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2405 - MinusLogProbMetric: 16.2405 - val_loss: 17.1322 - val_MinusLogProbMetric: 17.1322 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 800/1000
2023-09-28 02:16:45.311 
Epoch 800/1000 
	 loss: 16.2415, MinusLogProbMetric: 16.2415, val_loss: 17.1372, val_MinusLogProbMetric: 17.1372

Epoch 800: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2415 - MinusLogProbMetric: 16.2415 - val_loss: 17.1372 - val_MinusLogProbMetric: 17.1372 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 801/1000
2023-09-28 02:18:00.055 
Epoch 801/1000 
	 loss: 16.2393, MinusLogProbMetric: 16.2393, val_loss: 17.1322, val_MinusLogProbMetric: 17.1322

Epoch 801: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2393 - MinusLogProbMetric: 16.2393 - val_loss: 17.1322 - val_MinusLogProbMetric: 17.1322 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 802/1000
2023-09-28 02:19:15.205 
Epoch 802/1000 
	 loss: 16.2392, MinusLogProbMetric: 16.2392, val_loss: 17.1352, val_MinusLogProbMetric: 17.1352

Epoch 802: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2392 - MinusLogProbMetric: 16.2392 - val_loss: 17.1352 - val_MinusLogProbMetric: 17.1352 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 803/1000
2023-09-28 02:20:29.852 
Epoch 803/1000 
	 loss: 16.2395, MinusLogProbMetric: 16.2395, val_loss: 17.1351, val_MinusLogProbMetric: 17.1351

Epoch 803: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2395 - MinusLogProbMetric: 16.2395 - val_loss: 17.1351 - val_MinusLogProbMetric: 17.1351 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 804/1000
2023-09-28 02:21:44.634 
Epoch 804/1000 
	 loss: 16.2389, MinusLogProbMetric: 16.2389, val_loss: 17.1485, val_MinusLogProbMetric: 17.1485

Epoch 804: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2389 - MinusLogProbMetric: 16.2389 - val_loss: 17.1485 - val_MinusLogProbMetric: 17.1485 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 805/1000
2023-09-28 02:22:59.255 
Epoch 805/1000 
	 loss: 16.2399, MinusLogProbMetric: 16.2399, val_loss: 17.1330, val_MinusLogProbMetric: 17.1330

Epoch 805: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2399 - MinusLogProbMetric: 16.2399 - val_loss: 17.1330 - val_MinusLogProbMetric: 17.1330 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 806/1000
2023-09-28 02:24:14.009 
Epoch 806/1000 
	 loss: 16.2428, MinusLogProbMetric: 16.2428, val_loss: 17.1334, val_MinusLogProbMetric: 17.1334

Epoch 806: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2428 - MinusLogProbMetric: 16.2428 - val_loss: 17.1334 - val_MinusLogProbMetric: 17.1334 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 807/1000
2023-09-28 02:25:28.979 
Epoch 807/1000 
	 loss: 16.2406, MinusLogProbMetric: 16.2406, val_loss: 17.1343, val_MinusLogProbMetric: 17.1343

Epoch 807: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2406 - MinusLogProbMetric: 16.2406 - val_loss: 17.1343 - val_MinusLogProbMetric: 17.1343 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 808/1000
2023-09-28 02:26:44.121 
Epoch 808/1000 
	 loss: 16.2381, MinusLogProbMetric: 16.2381, val_loss: 17.1399, val_MinusLogProbMetric: 17.1399

Epoch 808: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2381 - MinusLogProbMetric: 16.2381 - val_loss: 17.1399 - val_MinusLogProbMetric: 17.1399 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 809/1000
2023-09-28 02:27:58.874 
Epoch 809/1000 
	 loss: 16.2400, MinusLogProbMetric: 16.2400, val_loss: 17.1313, val_MinusLogProbMetric: 17.1313

Epoch 809: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2400 - MinusLogProbMetric: 16.2400 - val_loss: 17.1313 - val_MinusLogProbMetric: 17.1313 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 810/1000
2023-09-28 02:29:14.046 
Epoch 810/1000 
	 loss: 16.2394, MinusLogProbMetric: 16.2394, val_loss: 17.1331, val_MinusLogProbMetric: 17.1331

Epoch 810: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2394 - MinusLogProbMetric: 16.2394 - val_loss: 17.1331 - val_MinusLogProbMetric: 17.1331 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 811/1000
2023-09-28 02:30:29.482 
Epoch 811/1000 
	 loss: 16.2393, MinusLogProbMetric: 16.2393, val_loss: 17.1445, val_MinusLogProbMetric: 17.1445

Epoch 811: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2393 - MinusLogProbMetric: 16.2393 - val_loss: 17.1445 - val_MinusLogProbMetric: 17.1445 - lr: 5.2083e-06 - 75s/epoch - 385ms/step
Epoch 812/1000
2023-09-28 02:31:44.729 
Epoch 812/1000 
	 loss: 16.2405, MinusLogProbMetric: 16.2405, val_loss: 17.1373, val_MinusLogProbMetric: 17.1373

Epoch 812: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2405 - MinusLogProbMetric: 16.2405 - val_loss: 17.1373 - val_MinusLogProbMetric: 17.1373 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 813/1000
2023-09-28 02:32:59.433 
Epoch 813/1000 
	 loss: 16.2406, MinusLogProbMetric: 16.2406, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 813: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2406 - MinusLogProbMetric: 16.2406 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 814/1000
2023-09-28 02:34:14.551 
Epoch 814/1000 
	 loss: 16.2390, MinusLogProbMetric: 16.2390, val_loss: 17.1317, val_MinusLogProbMetric: 17.1317

Epoch 814: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2390 - MinusLogProbMetric: 16.2390 - val_loss: 17.1317 - val_MinusLogProbMetric: 17.1317 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 815/1000
2023-09-28 02:35:29.646 
Epoch 815/1000 
	 loss: 16.2392, MinusLogProbMetric: 16.2392, val_loss: 17.1339, val_MinusLogProbMetric: 17.1339

Epoch 815: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2392 - MinusLogProbMetric: 16.2392 - val_loss: 17.1339 - val_MinusLogProbMetric: 17.1339 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 816/1000
2023-09-28 02:36:44.105 
Epoch 816/1000 
	 loss: 16.2388, MinusLogProbMetric: 16.2388, val_loss: 17.1375, val_MinusLogProbMetric: 17.1375

Epoch 816: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2388 - MinusLogProbMetric: 16.2388 - val_loss: 17.1375 - val_MinusLogProbMetric: 17.1375 - lr: 5.2083e-06 - 74s/epoch - 380ms/step
Epoch 817/1000
2023-09-28 02:37:58.797 
Epoch 817/1000 
	 loss: 16.2391, MinusLogProbMetric: 16.2391, val_loss: 17.1333, val_MinusLogProbMetric: 17.1333

Epoch 817: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2391 - MinusLogProbMetric: 16.2391 - val_loss: 17.1333 - val_MinusLogProbMetric: 17.1333 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 818/1000
2023-09-28 02:39:13.905 
Epoch 818/1000 
	 loss: 16.2382, MinusLogProbMetric: 16.2382, val_loss: 17.1337, val_MinusLogProbMetric: 17.1337

Epoch 818: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2382 - MinusLogProbMetric: 16.2382 - val_loss: 17.1337 - val_MinusLogProbMetric: 17.1337 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 819/1000
2023-09-28 02:40:28.551 
Epoch 819/1000 
	 loss: 16.2402, MinusLogProbMetric: 16.2402, val_loss: 17.1348, val_MinusLogProbMetric: 17.1348

Epoch 819: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2402 - MinusLogProbMetric: 16.2402 - val_loss: 17.1348 - val_MinusLogProbMetric: 17.1348 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 820/1000
2023-09-28 02:41:42.826 
Epoch 820/1000 
	 loss: 16.2416, MinusLogProbMetric: 16.2416, val_loss: 17.1345, val_MinusLogProbMetric: 17.1345

Epoch 820: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2416 - MinusLogProbMetric: 16.2416 - val_loss: 17.1345 - val_MinusLogProbMetric: 17.1345 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 821/1000
2023-09-28 02:42:57.296 
Epoch 821/1000 
	 loss: 16.2385, MinusLogProbMetric: 16.2385, val_loss: 17.1319, val_MinusLogProbMetric: 17.1319

Epoch 821: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2385 - MinusLogProbMetric: 16.2385 - val_loss: 17.1319 - val_MinusLogProbMetric: 17.1319 - lr: 5.2083e-06 - 74s/epoch - 380ms/step
Epoch 822/1000
2023-09-28 02:44:12.364 
Epoch 822/1000 
	 loss: 16.2386, MinusLogProbMetric: 16.2386, val_loss: 17.1321, val_MinusLogProbMetric: 17.1321

Epoch 822: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2386 - MinusLogProbMetric: 16.2386 - val_loss: 17.1321 - val_MinusLogProbMetric: 17.1321 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 823/1000
2023-09-28 02:45:27.535 
Epoch 823/1000 
	 loss: 16.2390, MinusLogProbMetric: 16.2390, val_loss: 17.1314, val_MinusLogProbMetric: 17.1314

Epoch 823: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2390 - MinusLogProbMetric: 16.2390 - val_loss: 17.1314 - val_MinusLogProbMetric: 17.1314 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 824/1000
2023-09-28 02:46:42.680 
Epoch 824/1000 
	 loss: 16.2382, MinusLogProbMetric: 16.2382, val_loss: 17.1370, val_MinusLogProbMetric: 17.1370

Epoch 824: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2382 - MinusLogProbMetric: 16.2382 - val_loss: 17.1370 - val_MinusLogProbMetric: 17.1370 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 825/1000
2023-09-28 02:47:57.236 
Epoch 825/1000 
	 loss: 16.2396, MinusLogProbMetric: 16.2396, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 825: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2396 - MinusLogProbMetric: 16.2396 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 5.2083e-06 - 75s/epoch - 380ms/step
Epoch 826/1000
2023-09-28 02:49:12.052 
Epoch 826/1000 
	 loss: 16.2391, MinusLogProbMetric: 16.2391, val_loss: 17.1353, val_MinusLogProbMetric: 17.1353

Epoch 826: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2391 - MinusLogProbMetric: 16.2391 - val_loss: 17.1353 - val_MinusLogProbMetric: 17.1353 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 827/1000
2023-09-28 02:50:26.702 
Epoch 827/1000 
	 loss: 16.2404, MinusLogProbMetric: 16.2404, val_loss: 17.1383, val_MinusLogProbMetric: 17.1383

Epoch 827: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2404 - MinusLogProbMetric: 16.2404 - val_loss: 17.1383 - val_MinusLogProbMetric: 17.1383 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 828/1000
2023-09-28 02:51:41.388 
Epoch 828/1000 
	 loss: 16.2397, MinusLogProbMetric: 16.2397, val_loss: 17.1325, val_MinusLogProbMetric: 17.1325

Epoch 828: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2397 - MinusLogProbMetric: 16.2397 - val_loss: 17.1325 - val_MinusLogProbMetric: 17.1325 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 829/1000
2023-09-28 02:52:56.003 
Epoch 829/1000 
	 loss: 16.2390, MinusLogProbMetric: 16.2390, val_loss: 17.1339, val_MinusLogProbMetric: 17.1339

Epoch 829: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2390 - MinusLogProbMetric: 16.2390 - val_loss: 17.1339 - val_MinusLogProbMetric: 17.1339 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 830/1000
2023-09-28 02:54:11.730 
Epoch 830/1000 
	 loss: 16.2390, MinusLogProbMetric: 16.2390, val_loss: 17.1332, val_MinusLogProbMetric: 17.1332

Epoch 830: val_loss did not improve from 17.12992
196/196 - 76s - loss: 16.2390 - MinusLogProbMetric: 16.2390 - val_loss: 17.1332 - val_MinusLogProbMetric: 17.1332 - lr: 5.2083e-06 - 76s/epoch - 386ms/step
Epoch 831/1000
2023-09-28 02:55:26.580 
Epoch 831/1000 
	 loss: 16.2396, MinusLogProbMetric: 16.2396, val_loss: 17.1403, val_MinusLogProbMetric: 17.1403

Epoch 831: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2396 - MinusLogProbMetric: 16.2396 - val_loss: 17.1403 - val_MinusLogProbMetric: 17.1403 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 832/1000
2023-09-28 02:56:41.624 
Epoch 832/1000 
	 loss: 16.2392, MinusLogProbMetric: 16.2392, val_loss: 17.1374, val_MinusLogProbMetric: 17.1374

Epoch 832: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2392 - MinusLogProbMetric: 16.2392 - val_loss: 17.1374 - val_MinusLogProbMetric: 17.1374 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 833/1000
2023-09-28 02:57:56.273 
Epoch 833/1000 
	 loss: 16.2390, MinusLogProbMetric: 16.2390, val_loss: 17.1344, val_MinusLogProbMetric: 17.1344

Epoch 833: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2390 - MinusLogProbMetric: 16.2390 - val_loss: 17.1344 - val_MinusLogProbMetric: 17.1344 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 834/1000
2023-09-28 02:59:10.645 
Epoch 834/1000 
	 loss: 16.2383, MinusLogProbMetric: 16.2383, val_loss: 17.1393, val_MinusLogProbMetric: 17.1393

Epoch 834: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2383 - MinusLogProbMetric: 16.2383 - val_loss: 17.1393 - val_MinusLogProbMetric: 17.1393 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 835/1000
2023-09-28 03:00:25.399 
Epoch 835/1000 
	 loss: 16.2400, MinusLogProbMetric: 16.2400, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 835: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2400 - MinusLogProbMetric: 16.2400 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 5.2083e-06 - 75s/epoch - 381ms/step
Epoch 836/1000
2023-09-28 03:01:40.547 
Epoch 836/1000 
	 loss: 16.2389, MinusLogProbMetric: 16.2389, val_loss: 17.1368, val_MinusLogProbMetric: 17.1368

Epoch 836: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2389 - MinusLogProbMetric: 16.2389 - val_loss: 17.1368 - val_MinusLogProbMetric: 17.1368 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 837/1000
2023-09-28 03:02:55.404 
Epoch 837/1000 
	 loss: 16.2387, MinusLogProbMetric: 16.2387, val_loss: 17.1362, val_MinusLogProbMetric: 17.1362

Epoch 837: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2387 - MinusLogProbMetric: 16.2387 - val_loss: 17.1362 - val_MinusLogProbMetric: 17.1362 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 838/1000
2023-09-28 03:04:09.858 
Epoch 838/1000 
	 loss: 16.2408, MinusLogProbMetric: 16.2408, val_loss: 17.1337, val_MinusLogProbMetric: 17.1337

Epoch 838: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2408 - MinusLogProbMetric: 16.2408 - val_loss: 17.1337 - val_MinusLogProbMetric: 17.1337 - lr: 5.2083e-06 - 74s/epoch - 380ms/step
Epoch 839/1000
2023-09-28 03:05:24.709 
Epoch 839/1000 
	 loss: 16.2381, MinusLogProbMetric: 16.2381, val_loss: 17.1319, val_MinusLogProbMetric: 17.1319

Epoch 839: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2381 - MinusLogProbMetric: 16.2381 - val_loss: 17.1319 - val_MinusLogProbMetric: 17.1319 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 840/1000
2023-09-28 03:06:39.649 
Epoch 840/1000 
	 loss: 16.2402, MinusLogProbMetric: 16.2402, val_loss: 17.1344, val_MinusLogProbMetric: 17.1344

Epoch 840: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2402 - MinusLogProbMetric: 16.2402 - val_loss: 17.1344 - val_MinusLogProbMetric: 17.1344 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 841/1000
2023-09-28 03:07:54.882 
Epoch 841/1000 
	 loss: 16.2382, MinusLogProbMetric: 16.2382, val_loss: 17.1338, val_MinusLogProbMetric: 17.1338

Epoch 841: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2382 - MinusLogProbMetric: 16.2382 - val_loss: 17.1338 - val_MinusLogProbMetric: 17.1338 - lr: 5.2083e-06 - 75s/epoch - 384ms/step
Epoch 842/1000
2023-09-28 03:09:08.913 
Epoch 842/1000 
	 loss: 16.2375, MinusLogProbMetric: 16.2375, val_loss: 17.1338, val_MinusLogProbMetric: 17.1338

Epoch 842: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2375 - MinusLogProbMetric: 16.2375 - val_loss: 17.1338 - val_MinusLogProbMetric: 17.1338 - lr: 5.2083e-06 - 74s/epoch - 378ms/step
Epoch 843/1000
2023-09-28 03:10:23.931 
Epoch 843/1000 
	 loss: 16.2373, MinusLogProbMetric: 16.2373, val_loss: 17.1314, val_MinusLogProbMetric: 17.1314

Epoch 843: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2373 - MinusLogProbMetric: 16.2373 - val_loss: 17.1314 - val_MinusLogProbMetric: 17.1314 - lr: 5.2083e-06 - 75s/epoch - 383ms/step
Epoch 844/1000
2023-09-28 03:11:37.924 
Epoch 844/1000 
	 loss: 16.2366, MinusLogProbMetric: 16.2366, val_loss: 17.1311, val_MinusLogProbMetric: 17.1311

Epoch 844: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2366 - MinusLogProbMetric: 16.2366 - val_loss: 17.1311 - val_MinusLogProbMetric: 17.1311 - lr: 5.2083e-06 - 74s/epoch - 377ms/step
Epoch 845/1000
2023-09-28 03:12:52.235 
Epoch 845/1000 
	 loss: 16.2395, MinusLogProbMetric: 16.2395, val_loss: 17.1321, val_MinusLogProbMetric: 17.1321

Epoch 845: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2395 - MinusLogProbMetric: 16.2395 - val_loss: 17.1321 - val_MinusLogProbMetric: 17.1321 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 846/1000
2023-09-28 03:14:07.161 
Epoch 846/1000 
	 loss: 16.2371, MinusLogProbMetric: 16.2371, val_loss: 17.1346, val_MinusLogProbMetric: 17.1346

Epoch 846: val_loss did not improve from 17.12992
196/196 - 75s - loss: 16.2371 - MinusLogProbMetric: 16.2371 - val_loss: 17.1346 - val_MinusLogProbMetric: 17.1346 - lr: 5.2083e-06 - 75s/epoch - 382ms/step
Epoch 847/1000
2023-09-28 03:15:21.530 
Epoch 847/1000 
	 loss: 16.2381, MinusLogProbMetric: 16.2381, val_loss: 17.1397, val_MinusLogProbMetric: 17.1397

Epoch 847: val_loss did not improve from 17.12992
196/196 - 74s - loss: 16.2381 - MinusLogProbMetric: 16.2381 - val_loss: 17.1397 - val_MinusLogProbMetric: 17.1397 - lr: 5.2083e-06 - 74s/epoch - 379ms/step
Epoch 848/1000
2023-09-28 03:16:36.530 
Epoch 848/1000 
	 loss: 16.2328, MinusLogProbMetric: 16.2328, val_loss: 17.1293, val_MinusLogProbMetric: 17.1293

Epoch 848: val_loss improved from 17.12992 to 17.12928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2328 - MinusLogProbMetric: 16.2328 - val_loss: 17.1293 - val_MinusLogProbMetric: 17.1293 - lr: 2.6042e-06 - 76s/epoch - 390ms/step
Epoch 849/1000
2023-09-28 03:17:52.843 
Epoch 849/1000 
	 loss: 16.2341, MinusLogProbMetric: 16.2341, val_loss: 17.1334, val_MinusLogProbMetric: 17.1334

Epoch 849: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2341 - MinusLogProbMetric: 16.2341 - val_loss: 17.1334 - val_MinusLogProbMetric: 17.1334 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 850/1000
2023-09-28 03:19:07.563 
Epoch 850/1000 
	 loss: 16.2327, MinusLogProbMetric: 16.2327, val_loss: 17.1316, val_MinusLogProbMetric: 17.1316

Epoch 850: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2327 - MinusLogProbMetric: 16.2327 - val_loss: 17.1316 - val_MinusLogProbMetric: 17.1316 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 851/1000
2023-09-28 03:20:22.755 
Epoch 851/1000 
	 loss: 16.2332, MinusLogProbMetric: 16.2332, val_loss: 17.1300, val_MinusLogProbMetric: 17.1300

Epoch 851: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2332 - MinusLogProbMetric: 16.2332 - val_loss: 17.1300 - val_MinusLogProbMetric: 17.1300 - lr: 2.6042e-06 - 75s/epoch - 384ms/step
Epoch 852/1000
2023-09-28 03:21:37.168 
Epoch 852/1000 
	 loss: 16.2324, MinusLogProbMetric: 16.2324, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 852: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2324 - MinusLogProbMetric: 16.2324 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 2.6042e-06 - 74s/epoch - 380ms/step
Epoch 853/1000
2023-09-28 03:22:51.838 
Epoch 853/1000 
	 loss: 16.2320, MinusLogProbMetric: 16.2320, val_loss: 17.1299, val_MinusLogProbMetric: 17.1299

Epoch 853: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2320 - MinusLogProbMetric: 16.2320 - val_loss: 17.1299 - val_MinusLogProbMetric: 17.1299 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 854/1000
2023-09-28 03:24:06.655 
Epoch 854/1000 
	 loss: 16.2331, MinusLogProbMetric: 16.2331, val_loss: 17.1300, val_MinusLogProbMetric: 17.1300

Epoch 854: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2331 - MinusLogProbMetric: 16.2331 - val_loss: 17.1300 - val_MinusLogProbMetric: 17.1300 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 855/1000
2023-09-28 03:25:21.823 
Epoch 855/1000 
	 loss: 16.2324, MinusLogProbMetric: 16.2324, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 855: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2324 - MinusLogProbMetric: 16.2324 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 2.6042e-06 - 75s/epoch - 383ms/step
Epoch 856/1000
2023-09-28 03:26:36.300 
Epoch 856/1000 
	 loss: 16.2329, MinusLogProbMetric: 16.2329, val_loss: 17.1324, val_MinusLogProbMetric: 17.1324

Epoch 856: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2329 - MinusLogProbMetric: 16.2329 - val_loss: 17.1324 - val_MinusLogProbMetric: 17.1324 - lr: 2.6042e-06 - 74s/epoch - 380ms/step
Epoch 857/1000
2023-09-28 03:27:52.005 
Epoch 857/1000 
	 loss: 16.2327, MinusLogProbMetric: 16.2327, val_loss: 17.1334, val_MinusLogProbMetric: 17.1334

Epoch 857: val_loss did not improve from 17.12928
196/196 - 76s - loss: 16.2327 - MinusLogProbMetric: 16.2327 - val_loss: 17.1334 - val_MinusLogProbMetric: 17.1334 - lr: 2.6042e-06 - 76s/epoch - 386ms/step
Epoch 858/1000
2023-09-28 03:29:06.148 
Epoch 858/1000 
	 loss: 16.2318, MinusLogProbMetric: 16.2318, val_loss: 17.1296, val_MinusLogProbMetric: 17.1296

Epoch 858: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2318 - MinusLogProbMetric: 16.2318 - val_loss: 17.1296 - val_MinusLogProbMetric: 17.1296 - lr: 2.6042e-06 - 74s/epoch - 378ms/step
Epoch 859/1000
2023-09-28 03:30:21.061 
Epoch 859/1000 
	 loss: 16.2322, MinusLogProbMetric: 16.2322, val_loss: 17.1300, val_MinusLogProbMetric: 17.1300

Epoch 859: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2322 - MinusLogProbMetric: 16.2322 - val_loss: 17.1300 - val_MinusLogProbMetric: 17.1300 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 860/1000
2023-09-28 03:31:35.253 
Epoch 860/1000 
	 loss: 16.2319, MinusLogProbMetric: 16.2319, val_loss: 17.1306, val_MinusLogProbMetric: 17.1306

Epoch 860: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2319 - MinusLogProbMetric: 16.2319 - val_loss: 17.1306 - val_MinusLogProbMetric: 17.1306 - lr: 2.6042e-06 - 74s/epoch - 379ms/step
Epoch 861/1000
2023-09-28 03:32:48.937 
Epoch 861/1000 
	 loss: 16.2328, MinusLogProbMetric: 16.2328, val_loss: 17.1301, val_MinusLogProbMetric: 17.1301

Epoch 861: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2328 - MinusLogProbMetric: 16.2328 - val_loss: 17.1301 - val_MinusLogProbMetric: 17.1301 - lr: 2.6042e-06 - 74s/epoch - 376ms/step
Epoch 862/1000
2023-09-28 03:34:03.709 
Epoch 862/1000 
	 loss: 16.2323, MinusLogProbMetric: 16.2323, val_loss: 17.1313, val_MinusLogProbMetric: 17.1313

Epoch 862: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2323 - MinusLogProbMetric: 16.2323 - val_loss: 17.1313 - val_MinusLogProbMetric: 17.1313 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 863/1000
2023-09-28 03:35:18.094 
Epoch 863/1000 
	 loss: 16.2320, MinusLogProbMetric: 16.2320, val_loss: 17.1360, val_MinusLogProbMetric: 17.1360

Epoch 863: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2320 - MinusLogProbMetric: 16.2320 - val_loss: 17.1360 - val_MinusLogProbMetric: 17.1360 - lr: 2.6042e-06 - 74s/epoch - 379ms/step
Epoch 864/1000
2023-09-28 03:36:32.644 
Epoch 864/1000 
	 loss: 16.2325, MinusLogProbMetric: 16.2325, val_loss: 17.1403, val_MinusLogProbMetric: 17.1403

Epoch 864: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2325 - MinusLogProbMetric: 16.2325 - val_loss: 17.1403 - val_MinusLogProbMetric: 17.1403 - lr: 2.6042e-06 - 75s/epoch - 380ms/step
Epoch 865/1000
2023-09-28 03:37:47.680 
Epoch 865/1000 
	 loss: 16.2321, MinusLogProbMetric: 16.2321, val_loss: 17.1302, val_MinusLogProbMetric: 17.1302

Epoch 865: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2321 - MinusLogProbMetric: 16.2321 - val_loss: 17.1302 - val_MinusLogProbMetric: 17.1302 - lr: 2.6042e-06 - 75s/epoch - 383ms/step
Epoch 866/1000
2023-09-28 03:39:02.649 
Epoch 866/1000 
	 loss: 16.2328, MinusLogProbMetric: 16.2328, val_loss: 17.1318, val_MinusLogProbMetric: 17.1318

Epoch 866: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2328 - MinusLogProbMetric: 16.2328 - val_loss: 17.1318 - val_MinusLogProbMetric: 17.1318 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 867/1000
2023-09-28 03:40:17.179 
Epoch 867/1000 
	 loss: 16.2338, MinusLogProbMetric: 16.2338, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 867: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2338 - MinusLogProbMetric: 16.2338 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 2.6042e-06 - 75s/epoch - 380ms/step
Epoch 868/1000
2023-09-28 03:41:31.718 
Epoch 868/1000 
	 loss: 16.2331, MinusLogProbMetric: 16.2331, val_loss: 17.1302, val_MinusLogProbMetric: 17.1302

Epoch 868: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2331 - MinusLogProbMetric: 16.2331 - val_loss: 17.1302 - val_MinusLogProbMetric: 17.1302 - lr: 2.6042e-06 - 75s/epoch - 380ms/step
Epoch 869/1000
2023-09-28 03:42:46.364 
Epoch 869/1000 
	 loss: 16.2321, MinusLogProbMetric: 16.2321, val_loss: 17.1297, val_MinusLogProbMetric: 17.1297

Epoch 869: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2321 - MinusLogProbMetric: 16.2321 - val_loss: 17.1297 - val_MinusLogProbMetric: 17.1297 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 870/1000
2023-09-28 03:44:00.598 
Epoch 870/1000 
	 loss: 16.2329, MinusLogProbMetric: 16.2329, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 870: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2329 - MinusLogProbMetric: 16.2329 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 2.6042e-06 - 74s/epoch - 379ms/step
Epoch 871/1000
2023-09-28 03:45:15.202 
Epoch 871/1000 
	 loss: 16.2322, MinusLogProbMetric: 16.2322, val_loss: 17.1337, val_MinusLogProbMetric: 17.1337

Epoch 871: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2322 - MinusLogProbMetric: 16.2322 - val_loss: 17.1337 - val_MinusLogProbMetric: 17.1337 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 872/1000
2023-09-28 03:46:29.764 
Epoch 872/1000 
	 loss: 16.2321, MinusLogProbMetric: 16.2321, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 872: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2321 - MinusLogProbMetric: 16.2321 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 2.6042e-06 - 75s/epoch - 380ms/step
Epoch 873/1000
2023-09-28 03:47:44.561 
Epoch 873/1000 
	 loss: 16.2322, MinusLogProbMetric: 16.2322, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 873: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2322 - MinusLogProbMetric: 16.2322 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 874/1000
2023-09-28 03:48:59.158 
Epoch 874/1000 
	 loss: 16.2316, MinusLogProbMetric: 16.2316, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 874: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2316 - MinusLogProbMetric: 16.2316 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 875/1000
2023-09-28 03:50:14.049 
Epoch 875/1000 
	 loss: 16.2326, MinusLogProbMetric: 16.2326, val_loss: 17.1321, val_MinusLogProbMetric: 17.1321

Epoch 875: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2326 - MinusLogProbMetric: 16.2326 - val_loss: 17.1321 - val_MinusLogProbMetric: 17.1321 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 876/1000
2023-09-28 03:51:28.370 
Epoch 876/1000 
	 loss: 16.2325, MinusLogProbMetric: 16.2325, val_loss: 17.1328, val_MinusLogProbMetric: 17.1328

Epoch 876: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2325 - MinusLogProbMetric: 16.2325 - val_loss: 17.1328 - val_MinusLogProbMetric: 17.1328 - lr: 2.6042e-06 - 74s/epoch - 379ms/step
Epoch 877/1000
2023-09-28 03:52:42.879 
Epoch 877/1000 
	 loss: 16.2332, MinusLogProbMetric: 16.2332, val_loss: 17.1327, val_MinusLogProbMetric: 17.1327

Epoch 877: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2332 - MinusLogProbMetric: 16.2332 - val_loss: 17.1327 - val_MinusLogProbMetric: 17.1327 - lr: 2.6042e-06 - 75s/epoch - 380ms/step
Epoch 878/1000
2023-09-28 03:53:57.539 
Epoch 878/1000 
	 loss: 16.2322, MinusLogProbMetric: 16.2322, val_loss: 17.1347, val_MinusLogProbMetric: 17.1347

Epoch 878: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2322 - MinusLogProbMetric: 16.2322 - val_loss: 17.1347 - val_MinusLogProbMetric: 17.1347 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 879/1000
2023-09-28 03:55:12.130 
Epoch 879/1000 
	 loss: 16.2319, MinusLogProbMetric: 16.2319, val_loss: 17.1303, val_MinusLogProbMetric: 17.1303

Epoch 879: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2319 - MinusLogProbMetric: 16.2319 - val_loss: 17.1303 - val_MinusLogProbMetric: 17.1303 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 880/1000
2023-09-28 03:56:26.758 
Epoch 880/1000 
	 loss: 16.2319, MinusLogProbMetric: 16.2319, val_loss: 17.1321, val_MinusLogProbMetric: 17.1321

Epoch 880: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2319 - MinusLogProbMetric: 16.2319 - val_loss: 17.1321 - val_MinusLogProbMetric: 17.1321 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 881/1000
2023-09-28 03:57:41.701 
Epoch 881/1000 
	 loss: 16.2322, MinusLogProbMetric: 16.2322, val_loss: 17.1341, val_MinusLogProbMetric: 17.1341

Epoch 881: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2322 - MinusLogProbMetric: 16.2322 - val_loss: 17.1341 - val_MinusLogProbMetric: 17.1341 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 882/1000
2023-09-28 03:58:57.033 
Epoch 882/1000 
	 loss: 16.2328, MinusLogProbMetric: 16.2328, val_loss: 17.1330, val_MinusLogProbMetric: 17.1330

Epoch 882: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2328 - MinusLogProbMetric: 16.2328 - val_loss: 17.1330 - val_MinusLogProbMetric: 17.1330 - lr: 2.6042e-06 - 75s/epoch - 384ms/step
Epoch 883/1000
2023-09-28 04:00:12.204 
Epoch 883/1000 
	 loss: 16.2338, MinusLogProbMetric: 16.2338, val_loss: 17.1358, val_MinusLogProbMetric: 17.1358

Epoch 883: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2338 - MinusLogProbMetric: 16.2338 - val_loss: 17.1358 - val_MinusLogProbMetric: 17.1358 - lr: 2.6042e-06 - 75s/epoch - 384ms/step
Epoch 884/1000
2023-09-28 04:01:27.465 
Epoch 884/1000 
	 loss: 16.2320, MinusLogProbMetric: 16.2320, val_loss: 17.1296, val_MinusLogProbMetric: 17.1296

Epoch 884: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2320 - MinusLogProbMetric: 16.2320 - val_loss: 17.1296 - val_MinusLogProbMetric: 17.1296 - lr: 2.6042e-06 - 75s/epoch - 384ms/step
Epoch 885/1000
2023-09-28 04:02:42.304 
Epoch 885/1000 
	 loss: 16.2323, MinusLogProbMetric: 16.2323, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 885: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2323 - MinusLogProbMetric: 16.2323 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 886/1000
2023-09-28 04:03:56.868 
Epoch 886/1000 
	 loss: 16.2318, MinusLogProbMetric: 16.2318, val_loss: 17.1306, val_MinusLogProbMetric: 17.1306

Epoch 886: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2318 - MinusLogProbMetric: 16.2318 - val_loss: 17.1306 - val_MinusLogProbMetric: 17.1306 - lr: 2.6042e-06 - 75s/epoch - 380ms/step
Epoch 887/1000
2023-09-28 04:05:11.880 
Epoch 887/1000 
	 loss: 16.2326, MinusLogProbMetric: 16.2326, val_loss: 17.1312, val_MinusLogProbMetric: 17.1312

Epoch 887: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2326 - MinusLogProbMetric: 16.2326 - val_loss: 17.1312 - val_MinusLogProbMetric: 17.1312 - lr: 2.6042e-06 - 75s/epoch - 383ms/step
Epoch 888/1000
2023-09-28 04:06:27.121 
Epoch 888/1000 
	 loss: 16.2316, MinusLogProbMetric: 16.2316, val_loss: 17.1303, val_MinusLogProbMetric: 17.1303

Epoch 888: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2316 - MinusLogProbMetric: 16.2316 - val_loss: 17.1303 - val_MinusLogProbMetric: 17.1303 - lr: 2.6042e-06 - 75s/epoch - 384ms/step
Epoch 889/1000
2023-09-28 04:07:42.058 
Epoch 889/1000 
	 loss: 16.2316, MinusLogProbMetric: 16.2316, val_loss: 17.1350, val_MinusLogProbMetric: 17.1350

Epoch 889: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2316 - MinusLogProbMetric: 16.2316 - val_loss: 17.1350 - val_MinusLogProbMetric: 17.1350 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 890/1000
2023-09-28 04:08:56.870 
Epoch 890/1000 
	 loss: 16.2312, MinusLogProbMetric: 16.2312, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 890: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2312 - MinusLogProbMetric: 16.2312 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 891/1000
2023-09-28 04:10:10.675 
Epoch 891/1000 
	 loss: 16.2319, MinusLogProbMetric: 16.2319, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 891: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2319 - MinusLogProbMetric: 16.2319 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 2.6042e-06 - 74s/epoch - 377ms/step
Epoch 892/1000
2023-09-28 04:11:25.084 
Epoch 892/1000 
	 loss: 16.2322, MinusLogProbMetric: 16.2322, val_loss: 17.1393, val_MinusLogProbMetric: 17.1393

Epoch 892: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2322 - MinusLogProbMetric: 16.2322 - val_loss: 17.1393 - val_MinusLogProbMetric: 17.1393 - lr: 2.6042e-06 - 74s/epoch - 380ms/step
Epoch 893/1000
2023-09-28 04:12:39.806 
Epoch 893/1000 
	 loss: 16.2327, MinusLogProbMetric: 16.2327, val_loss: 17.1370, val_MinusLogProbMetric: 17.1370

Epoch 893: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2327 - MinusLogProbMetric: 16.2327 - val_loss: 17.1370 - val_MinusLogProbMetric: 17.1370 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 894/1000
2023-09-28 04:13:54.877 
Epoch 894/1000 
	 loss: 16.2314, MinusLogProbMetric: 16.2314, val_loss: 17.1332, val_MinusLogProbMetric: 17.1332

Epoch 894: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2314 - MinusLogProbMetric: 16.2314 - val_loss: 17.1332 - val_MinusLogProbMetric: 17.1332 - lr: 2.6042e-06 - 75s/epoch - 383ms/step
Epoch 895/1000
2023-09-28 04:15:09.421 
Epoch 895/1000 
	 loss: 16.2314, MinusLogProbMetric: 16.2314, val_loss: 17.1320, val_MinusLogProbMetric: 17.1320

Epoch 895: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2314 - MinusLogProbMetric: 16.2314 - val_loss: 17.1320 - val_MinusLogProbMetric: 17.1320 - lr: 2.6042e-06 - 75s/epoch - 380ms/step
Epoch 896/1000
2023-09-28 04:16:23.808 
Epoch 896/1000 
	 loss: 16.2322, MinusLogProbMetric: 16.2322, val_loss: 17.1322, val_MinusLogProbMetric: 17.1322

Epoch 896: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2322 - MinusLogProbMetric: 16.2322 - val_loss: 17.1322 - val_MinusLogProbMetric: 17.1322 - lr: 2.6042e-06 - 74s/epoch - 380ms/step
Epoch 897/1000
2023-09-28 04:17:38.378 
Epoch 897/1000 
	 loss: 16.2311, MinusLogProbMetric: 16.2311, val_loss: 17.1379, val_MinusLogProbMetric: 17.1379

Epoch 897: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2311 - MinusLogProbMetric: 16.2311 - val_loss: 17.1379 - val_MinusLogProbMetric: 17.1379 - lr: 2.6042e-06 - 75s/epoch - 380ms/step
Epoch 898/1000
2023-09-28 04:18:52.658 
Epoch 898/1000 
	 loss: 16.2314, MinusLogProbMetric: 16.2314, val_loss: 17.1340, val_MinusLogProbMetric: 17.1340

Epoch 898: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2314 - MinusLogProbMetric: 16.2314 - val_loss: 17.1340 - val_MinusLogProbMetric: 17.1340 - lr: 2.6042e-06 - 74s/epoch - 379ms/step
Epoch 899/1000
2023-09-28 04:20:07.177 
Epoch 899/1000 
	 loss: 16.2291, MinusLogProbMetric: 16.2291, val_loss: 17.1302, val_MinusLogProbMetric: 17.1302

Epoch 899: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2291 - MinusLogProbMetric: 16.2291 - val_loss: 17.1302 - val_MinusLogProbMetric: 17.1302 - lr: 1.3021e-06 - 75s/epoch - 380ms/step
Epoch 900/1000
2023-09-28 04:21:21.825 
Epoch 900/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1335, val_MinusLogProbMetric: 17.1335

Epoch 900: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1335 - val_MinusLogProbMetric: 17.1335 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 901/1000
2023-09-28 04:22:36.489 
Epoch 901/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1305, val_MinusLogProbMetric: 17.1305

Epoch 901: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1305 - val_MinusLogProbMetric: 17.1305 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 902/1000
2023-09-28 04:23:50.666 
Epoch 902/1000 
	 loss: 16.2294, MinusLogProbMetric: 16.2294, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 902: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2294 - MinusLogProbMetric: 16.2294 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 1.3021e-06 - 74s/epoch - 378ms/step
Epoch 903/1000
2023-09-28 04:25:05.308 
Epoch 903/1000 
	 loss: 16.2293, MinusLogProbMetric: 16.2293, val_loss: 17.1296, val_MinusLogProbMetric: 17.1296

Epoch 903: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2293 - MinusLogProbMetric: 16.2293 - val_loss: 17.1296 - val_MinusLogProbMetric: 17.1296 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 904/1000
2023-09-28 04:26:19.883 
Epoch 904/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 904: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 1.3021e-06 - 75s/epoch - 380ms/step
Epoch 905/1000
2023-09-28 04:27:35.308 
Epoch 905/1000 
	 loss: 16.2292, MinusLogProbMetric: 16.2292, val_loss: 17.1302, val_MinusLogProbMetric: 17.1302

Epoch 905: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2292 - MinusLogProbMetric: 16.2292 - val_loss: 17.1302 - val_MinusLogProbMetric: 17.1302 - lr: 1.3021e-06 - 75s/epoch - 385ms/step
Epoch 906/1000
2023-09-28 04:28:49.285 
Epoch 906/1000 
	 loss: 16.2293, MinusLogProbMetric: 16.2293, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 906: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2293 - MinusLogProbMetric: 16.2293 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.3021e-06 - 74s/epoch - 377ms/step
Epoch 907/1000
2023-09-28 04:30:04.284 
Epoch 907/1000 
	 loss: 16.2295, MinusLogProbMetric: 16.2295, val_loss: 17.1301, val_MinusLogProbMetric: 17.1301

Epoch 907: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2295 - MinusLogProbMetric: 16.2295 - val_loss: 17.1301 - val_MinusLogProbMetric: 17.1301 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 908/1000
2023-09-28 04:31:19.124 
Epoch 908/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 908: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 909/1000
2023-09-28 04:32:33.742 
Epoch 909/1000 
	 loss: 16.2292, MinusLogProbMetric: 16.2292, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 909: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2292 - MinusLogProbMetric: 16.2292 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 910/1000
2023-09-28 04:33:47.628 
Epoch 910/1000 
	 loss: 16.2292, MinusLogProbMetric: 16.2292, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 910: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2292 - MinusLogProbMetric: 16.2292 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 1.3021e-06 - 74s/epoch - 377ms/step
Epoch 911/1000
2023-09-28 04:35:01.728 
Epoch 911/1000 
	 loss: 16.2292, MinusLogProbMetric: 16.2292, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 911: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2292 - MinusLogProbMetric: 16.2292 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.3021e-06 - 74s/epoch - 378ms/step
Epoch 912/1000
2023-09-28 04:36:16.064 
Epoch 912/1000 
	 loss: 16.2292, MinusLogProbMetric: 16.2292, val_loss: 17.1297, val_MinusLogProbMetric: 17.1297

Epoch 912: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2292 - MinusLogProbMetric: 16.2292 - val_loss: 17.1297 - val_MinusLogProbMetric: 17.1297 - lr: 1.3021e-06 - 74s/epoch - 379ms/step
Epoch 913/1000
2023-09-28 04:37:30.936 
Epoch 913/1000 
	 loss: 16.2286, MinusLogProbMetric: 16.2286, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 913: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2286 - MinusLogProbMetric: 16.2286 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 914/1000
2023-09-28 04:38:45.452 
Epoch 914/1000 
	 loss: 16.2287, MinusLogProbMetric: 16.2287, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 914: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2287 - MinusLogProbMetric: 16.2287 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.3021e-06 - 75s/epoch - 380ms/step
Epoch 915/1000
2023-09-28 04:39:59.464 
Epoch 915/1000 
	 loss: 16.2288, MinusLogProbMetric: 16.2288, val_loss: 17.1297, val_MinusLogProbMetric: 17.1297

Epoch 915: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2288 - MinusLogProbMetric: 16.2288 - val_loss: 17.1297 - val_MinusLogProbMetric: 17.1297 - lr: 1.3021e-06 - 74s/epoch - 378ms/step
Epoch 916/1000
2023-09-28 04:41:14.451 
Epoch 916/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1340, val_MinusLogProbMetric: 17.1340

Epoch 916: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1340 - val_MinusLogProbMetric: 17.1340 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 917/1000
2023-09-28 04:42:29.086 
Epoch 917/1000 
	 loss: 16.2287, MinusLogProbMetric: 16.2287, val_loss: 17.1322, val_MinusLogProbMetric: 17.1322

Epoch 917: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2287 - MinusLogProbMetric: 16.2287 - val_loss: 17.1322 - val_MinusLogProbMetric: 17.1322 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 918/1000
2023-09-28 04:43:43.543 
Epoch 918/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1298, val_MinusLogProbMetric: 17.1298

Epoch 918: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1298 - val_MinusLogProbMetric: 17.1298 - lr: 1.3021e-06 - 74s/epoch - 380ms/step
Epoch 919/1000
2023-09-28 04:44:58.369 
Epoch 919/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1305, val_MinusLogProbMetric: 17.1305

Epoch 919: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1305 - val_MinusLogProbMetric: 17.1305 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 920/1000
2023-09-28 04:46:13.079 
Epoch 920/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1306, val_MinusLogProbMetric: 17.1306

Epoch 920: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1306 - val_MinusLogProbMetric: 17.1306 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 921/1000
2023-09-28 04:47:27.487 
Epoch 921/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1316, val_MinusLogProbMetric: 17.1316

Epoch 921: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1316 - val_MinusLogProbMetric: 17.1316 - lr: 1.3021e-06 - 74s/epoch - 380ms/step
Epoch 922/1000
2023-09-28 04:48:42.445 
Epoch 922/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 922: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 923/1000
2023-09-28 04:49:57.418 
Epoch 923/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1332, val_MinusLogProbMetric: 17.1332

Epoch 923: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1332 - val_MinusLogProbMetric: 17.1332 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 924/1000
2023-09-28 04:51:11.612 
Epoch 924/1000 
	 loss: 16.2291, MinusLogProbMetric: 16.2291, val_loss: 17.1303, val_MinusLogProbMetric: 17.1303

Epoch 924: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2291 - MinusLogProbMetric: 16.2291 - val_loss: 17.1303 - val_MinusLogProbMetric: 17.1303 - lr: 1.3021e-06 - 74s/epoch - 379ms/step
Epoch 925/1000
2023-09-28 04:52:26.248 
Epoch 925/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1305, val_MinusLogProbMetric: 17.1305

Epoch 925: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1305 - val_MinusLogProbMetric: 17.1305 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 926/1000
2023-09-28 04:53:41.041 
Epoch 926/1000 
	 loss: 16.2292, MinusLogProbMetric: 16.2292, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 926: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2292 - MinusLogProbMetric: 16.2292 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 927/1000
2023-09-28 04:54:56.032 
Epoch 927/1000 
	 loss: 16.2294, MinusLogProbMetric: 16.2294, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 927: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2294 - MinusLogProbMetric: 16.2294 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 928/1000
2023-09-28 04:56:10.647 
Epoch 928/1000 
	 loss: 16.2288, MinusLogProbMetric: 16.2288, val_loss: 17.1299, val_MinusLogProbMetric: 17.1299

Epoch 928: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2288 - MinusLogProbMetric: 16.2288 - val_loss: 17.1299 - val_MinusLogProbMetric: 17.1299 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 929/1000
2023-09-28 04:57:24.994 
Epoch 929/1000 
	 loss: 16.2286, MinusLogProbMetric: 16.2286, val_loss: 17.1322, val_MinusLogProbMetric: 17.1322

Epoch 929: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2286 - MinusLogProbMetric: 16.2286 - val_loss: 17.1322 - val_MinusLogProbMetric: 17.1322 - lr: 1.3021e-06 - 74s/epoch - 379ms/step
Epoch 930/1000
2023-09-28 04:58:39.271 
Epoch 930/1000 
	 loss: 16.2291, MinusLogProbMetric: 16.2291, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 930: val_loss did not improve from 17.12928
196/196 - 74s - loss: 16.2291 - MinusLogProbMetric: 16.2291 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 1.3021e-06 - 74s/epoch - 379ms/step
Epoch 931/1000
2023-09-28 04:59:53.873 
Epoch 931/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1299, val_MinusLogProbMetric: 17.1299

Epoch 931: val_loss did not improve from 17.12928
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1299 - val_MinusLogProbMetric: 17.1299 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 932/1000
2023-09-28 05:01:08.816 
Epoch 932/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1290, val_MinusLogProbMetric: 17.1290

Epoch 932: val_loss improved from 17.12928 to 17.12900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_304/weights/best_weights.h5
196/196 - 76s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1290 - val_MinusLogProbMetric: 17.1290 - lr: 1.3021e-06 - 76s/epoch - 389ms/step
Epoch 933/1000
2023-09-28 05:02:25.007 
Epoch 933/1000 
	 loss: 16.2292, MinusLogProbMetric: 16.2292, val_loss: 17.1306, val_MinusLogProbMetric: 17.1306

Epoch 933: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2292 - MinusLogProbMetric: 16.2292 - val_loss: 17.1306 - val_MinusLogProbMetric: 17.1306 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 934/1000
2023-09-28 05:03:39.500 
Epoch 934/1000 
	 loss: 16.2291, MinusLogProbMetric: 16.2291, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 934: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2291 - MinusLogProbMetric: 16.2291 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 1.3021e-06 - 74s/epoch - 380ms/step
Epoch 935/1000
2023-09-28 05:04:54.048 
Epoch 935/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 935: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 1.3021e-06 - 75s/epoch - 380ms/step
Epoch 936/1000
2023-09-28 05:06:09.027 
Epoch 936/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 936: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 937/1000
2023-09-28 05:07:23.927 
Epoch 937/1000 
	 loss: 16.2292, MinusLogProbMetric: 16.2292, val_loss: 17.1315, val_MinusLogProbMetric: 17.1315

Epoch 937: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2292 - MinusLogProbMetric: 16.2292 - val_loss: 17.1315 - val_MinusLogProbMetric: 17.1315 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 938/1000
2023-09-28 05:08:38.297 
Epoch 938/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1296, val_MinusLogProbMetric: 17.1296

Epoch 938: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1296 - val_MinusLogProbMetric: 17.1296 - lr: 1.3021e-06 - 74s/epoch - 379ms/step
Epoch 939/1000
2023-09-28 05:09:52.218 
Epoch 939/1000 
	 loss: 16.2288, MinusLogProbMetric: 16.2288, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 939: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2288 - MinusLogProbMetric: 16.2288 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 1.3021e-06 - 74s/epoch - 377ms/step
Epoch 940/1000
2023-09-28 05:11:07.131 
Epoch 940/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1314, val_MinusLogProbMetric: 17.1314

Epoch 940: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1314 - val_MinusLogProbMetric: 17.1314 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 941/1000
2023-09-28 05:12:21.812 
Epoch 941/1000 
	 loss: 16.2287, MinusLogProbMetric: 16.2287, val_loss: 17.1300, val_MinusLogProbMetric: 17.1300

Epoch 941: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2287 - MinusLogProbMetric: 16.2287 - val_loss: 17.1300 - val_MinusLogProbMetric: 17.1300 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 942/1000
2023-09-28 05:13:36.610 
Epoch 942/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1313, val_MinusLogProbMetric: 17.1313

Epoch 942: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1313 - val_MinusLogProbMetric: 17.1313 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 943/1000
2023-09-28 05:14:51.425 
Epoch 943/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1302, val_MinusLogProbMetric: 17.1302

Epoch 943: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1302 - val_MinusLogProbMetric: 17.1302 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 944/1000
2023-09-28 05:16:06.620 
Epoch 944/1000 
	 loss: 16.2286, MinusLogProbMetric: 16.2286, val_loss: 17.1315, val_MinusLogProbMetric: 17.1315

Epoch 944: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2286 - MinusLogProbMetric: 16.2286 - val_loss: 17.1315 - val_MinusLogProbMetric: 17.1315 - lr: 1.3021e-06 - 75s/epoch - 384ms/step
Epoch 945/1000
2023-09-28 05:17:21.447 
Epoch 945/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 945: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 946/1000
2023-09-28 05:18:35.862 
Epoch 946/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1322, val_MinusLogProbMetric: 17.1322

Epoch 946: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1322 - val_MinusLogProbMetric: 17.1322 - lr: 1.3021e-06 - 74s/epoch - 380ms/step
Epoch 947/1000
2023-09-28 05:19:50.478 
Epoch 947/1000 
	 loss: 16.2288, MinusLogProbMetric: 16.2288, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 947: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2288 - MinusLogProbMetric: 16.2288 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 948/1000
2023-09-28 05:21:04.988 
Epoch 948/1000 
	 loss: 16.2284, MinusLogProbMetric: 16.2284, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 948: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2284 - MinusLogProbMetric: 16.2284 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.3021e-06 - 75s/epoch - 380ms/step
Epoch 949/1000
2023-09-28 05:22:19.534 
Epoch 949/1000 
	 loss: 16.2283, MinusLogProbMetric: 16.2283, val_loss: 17.1302, val_MinusLogProbMetric: 17.1302

Epoch 949: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2283 - MinusLogProbMetric: 16.2283 - val_loss: 17.1302 - val_MinusLogProbMetric: 17.1302 - lr: 1.3021e-06 - 75s/epoch - 380ms/step
Epoch 950/1000
2023-09-28 05:23:34.370 
Epoch 950/1000 
	 loss: 16.2287, MinusLogProbMetric: 16.2287, val_loss: 17.1313, val_MinusLogProbMetric: 17.1313

Epoch 950: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2287 - MinusLogProbMetric: 16.2287 - val_loss: 17.1313 - val_MinusLogProbMetric: 17.1313 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 951/1000
2023-09-28 05:24:49.016 
Epoch 951/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1315, val_MinusLogProbMetric: 17.1315

Epoch 951: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1315 - val_MinusLogProbMetric: 17.1315 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 952/1000
2023-09-28 05:26:03.406 
Epoch 952/1000 
	 loss: 16.2284, MinusLogProbMetric: 16.2284, val_loss: 17.1326, val_MinusLogProbMetric: 17.1326

Epoch 952: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2284 - MinusLogProbMetric: 16.2284 - val_loss: 17.1326 - val_MinusLogProbMetric: 17.1326 - lr: 1.3021e-06 - 74s/epoch - 380ms/step
Epoch 953/1000
2023-09-28 05:27:18.504 
Epoch 953/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1300, val_MinusLogProbMetric: 17.1300

Epoch 953: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1300 - val_MinusLogProbMetric: 17.1300 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 954/1000
2023-09-28 05:28:33.622 
Epoch 954/1000 
	 loss: 16.2287, MinusLogProbMetric: 16.2287, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 954: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2287 - MinusLogProbMetric: 16.2287 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 955/1000
2023-09-28 05:29:48.496 
Epoch 955/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1319, val_MinusLogProbMetric: 17.1319

Epoch 955: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1319 - val_MinusLogProbMetric: 17.1319 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 956/1000
2023-09-28 05:31:03.902 
Epoch 956/1000 
	 loss: 16.2286, MinusLogProbMetric: 16.2286, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 956: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2286 - MinusLogProbMetric: 16.2286 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 1.3021e-06 - 75s/epoch - 385ms/step
Epoch 957/1000
2023-09-28 05:32:18.813 
Epoch 957/1000 
	 loss: 16.2287, MinusLogProbMetric: 16.2287, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 957: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2287 - MinusLogProbMetric: 16.2287 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 958/1000
2023-09-28 05:33:34.055 
Epoch 958/1000 
	 loss: 16.2288, MinusLogProbMetric: 16.2288, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 958: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2288 - MinusLogProbMetric: 16.2288 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 1.3021e-06 - 75s/epoch - 384ms/step
Epoch 959/1000
2023-09-28 05:34:48.703 
Epoch 959/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1299, val_MinusLogProbMetric: 17.1299

Epoch 959: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1299 - val_MinusLogProbMetric: 17.1299 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 960/1000
2023-09-28 05:36:03.382 
Epoch 960/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 960: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 961/1000
2023-09-28 05:37:18.917 
Epoch 961/1000 
	 loss: 16.2283, MinusLogProbMetric: 16.2283, val_loss: 17.1311, val_MinusLogProbMetric: 17.1311

Epoch 961: val_loss did not improve from 17.12900
196/196 - 76s - loss: 16.2283 - MinusLogProbMetric: 16.2283 - val_loss: 17.1311 - val_MinusLogProbMetric: 17.1311 - lr: 1.3021e-06 - 76s/epoch - 385ms/step
Epoch 962/1000
2023-09-28 05:38:33.727 
Epoch 962/1000 
	 loss: 16.2283, MinusLogProbMetric: 16.2283, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 962: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2283 - MinusLogProbMetric: 16.2283 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 963/1000
2023-09-28 05:39:48.215 
Epoch 963/1000 
	 loss: 16.2284, MinusLogProbMetric: 16.2284, val_loss: 17.1296, val_MinusLogProbMetric: 17.1296

Epoch 963: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2284 - MinusLogProbMetric: 16.2284 - val_loss: 17.1296 - val_MinusLogProbMetric: 17.1296 - lr: 1.3021e-06 - 74s/epoch - 380ms/step
Epoch 964/1000
2023-09-28 05:41:03.066 
Epoch 964/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1313, val_MinusLogProbMetric: 17.1313

Epoch 964: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1313 - val_MinusLogProbMetric: 17.1313 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 965/1000
2023-09-28 05:42:17.688 
Epoch 965/1000 
	 loss: 16.2283, MinusLogProbMetric: 16.2283, val_loss: 17.1328, val_MinusLogProbMetric: 17.1328

Epoch 965: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2283 - MinusLogProbMetric: 16.2283 - val_loss: 17.1328 - val_MinusLogProbMetric: 17.1328 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 966/1000
2023-09-28 05:43:32.684 
Epoch 966/1000 
	 loss: 16.2282, MinusLogProbMetric: 16.2282, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 966: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2282 - MinusLogProbMetric: 16.2282 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 967/1000
2023-09-28 05:44:47.717 
Epoch 967/1000 
	 loss: 16.2283, MinusLogProbMetric: 16.2283, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 967: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2283 - MinusLogProbMetric: 16.2283 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 968/1000
2023-09-28 05:46:02.745 
Epoch 968/1000 
	 loss: 16.2289, MinusLogProbMetric: 16.2289, val_loss: 17.1305, val_MinusLogProbMetric: 17.1305

Epoch 968: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2289 - MinusLogProbMetric: 16.2289 - val_loss: 17.1305 - val_MinusLogProbMetric: 17.1305 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 969/1000
2023-09-28 05:47:17.427 
Epoch 969/1000 
	 loss: 16.2287, MinusLogProbMetric: 16.2287, val_loss: 17.1324, val_MinusLogProbMetric: 17.1324

Epoch 969: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2287 - MinusLogProbMetric: 16.2287 - val_loss: 17.1324 - val_MinusLogProbMetric: 17.1324 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 970/1000
2023-09-28 05:48:32.952 
Epoch 970/1000 
	 loss: 16.2287, MinusLogProbMetric: 16.2287, val_loss: 17.1293, val_MinusLogProbMetric: 17.1293

Epoch 970: val_loss did not improve from 17.12900
196/196 - 76s - loss: 16.2287 - MinusLogProbMetric: 16.2287 - val_loss: 17.1293 - val_MinusLogProbMetric: 17.1293 - lr: 1.3021e-06 - 76s/epoch - 385ms/step
Epoch 971/1000
2023-09-28 05:49:47.626 
Epoch 971/1000 
	 loss: 16.2283, MinusLogProbMetric: 16.2283, val_loss: 17.1313, val_MinusLogProbMetric: 17.1313

Epoch 971: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2283 - MinusLogProbMetric: 16.2283 - val_loss: 17.1313 - val_MinusLogProbMetric: 17.1313 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 972/1000
2023-09-28 05:51:02.523 
Epoch 972/1000 
	 loss: 16.2288, MinusLogProbMetric: 16.2288, val_loss: 17.1303, val_MinusLogProbMetric: 17.1303

Epoch 972: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2288 - MinusLogProbMetric: 16.2288 - val_loss: 17.1303 - val_MinusLogProbMetric: 17.1303 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 973/1000
2023-09-28 05:52:17.556 
Epoch 973/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1294, val_MinusLogProbMetric: 17.1294

Epoch 973: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1294 - val_MinusLogProbMetric: 17.1294 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 974/1000
2023-09-28 05:53:32.306 
Epoch 974/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1312, val_MinusLogProbMetric: 17.1312

Epoch 974: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1312 - val_MinusLogProbMetric: 17.1312 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 975/1000
2023-09-28 05:54:46.965 
Epoch 975/1000 
	 loss: 16.2288, MinusLogProbMetric: 16.2288, val_loss: 17.1295, val_MinusLogProbMetric: 17.1295

Epoch 975: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2288 - MinusLogProbMetric: 16.2288 - val_loss: 17.1295 - val_MinusLogProbMetric: 17.1295 - lr: 1.3021e-06 - 75s/epoch - 381ms/step
Epoch 976/1000
2023-09-28 05:56:01.526 
Epoch 976/1000 
	 loss: 16.2278, MinusLogProbMetric: 16.2278, val_loss: 17.1315, val_MinusLogProbMetric: 17.1315

Epoch 976: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2278 - MinusLogProbMetric: 16.2278 - val_loss: 17.1315 - val_MinusLogProbMetric: 17.1315 - lr: 1.3021e-06 - 75s/epoch - 380ms/step
Epoch 977/1000
2023-09-28 05:57:16.385 
Epoch 977/1000 
	 loss: 16.2281, MinusLogProbMetric: 16.2281, val_loss: 17.1355, val_MinusLogProbMetric: 17.1355

Epoch 977: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2281 - MinusLogProbMetric: 16.2281 - val_loss: 17.1355 - val_MinusLogProbMetric: 17.1355 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 978/1000
2023-09-28 05:58:31.353 
Epoch 978/1000 
	 loss: 16.2290, MinusLogProbMetric: 16.2290, val_loss: 17.1315, val_MinusLogProbMetric: 17.1315

Epoch 978: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2290 - MinusLogProbMetric: 16.2290 - val_loss: 17.1315 - val_MinusLogProbMetric: 17.1315 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 979/1000
2023-09-28 05:59:46.379 
Epoch 979/1000 
	 loss: 16.2283, MinusLogProbMetric: 16.2283, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 979: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2283 - MinusLogProbMetric: 16.2283 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 1.3021e-06 - 75s/epoch - 383ms/step
Epoch 980/1000
2023-09-28 06:01:00.434 
Epoch 980/1000 
	 loss: 16.2285, MinusLogProbMetric: 16.2285, val_loss: 17.1323, val_MinusLogProbMetric: 17.1323

Epoch 980: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2285 - MinusLogProbMetric: 16.2285 - val_loss: 17.1323 - val_MinusLogProbMetric: 17.1323 - lr: 1.3021e-06 - 74s/epoch - 378ms/step
Epoch 981/1000
2023-09-28 06:02:15.224 
Epoch 981/1000 
	 loss: 16.2284, MinusLogProbMetric: 16.2284, val_loss: 17.1318, val_MinusLogProbMetric: 17.1318

Epoch 981: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2284 - MinusLogProbMetric: 16.2284 - val_loss: 17.1318 - val_MinusLogProbMetric: 17.1318 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 982/1000
2023-09-28 06:03:30.108 
Epoch 982/1000 
	 loss: 16.2281, MinusLogProbMetric: 16.2281, val_loss: 17.1328, val_MinusLogProbMetric: 17.1328

Epoch 982: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2281 - MinusLogProbMetric: 16.2281 - val_loss: 17.1328 - val_MinusLogProbMetric: 17.1328 - lr: 1.3021e-06 - 75s/epoch - 382ms/step
Epoch 983/1000
2023-09-28 06:04:45.240 
Epoch 983/1000 
	 loss: 16.2275, MinusLogProbMetric: 16.2275, val_loss: 17.1295, val_MinusLogProbMetric: 17.1295

Epoch 983: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2275 - MinusLogProbMetric: 16.2275 - val_loss: 17.1295 - val_MinusLogProbMetric: 17.1295 - lr: 1.0000e-06 - 75s/epoch - 383ms/step
Epoch 984/1000
2023-09-28 06:05:59.482 
Epoch 984/1000 
	 loss: 16.2278, MinusLogProbMetric: 16.2278, val_loss: 17.1305, val_MinusLogProbMetric: 17.1305

Epoch 984: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2278 - MinusLogProbMetric: 16.2278 - val_loss: 17.1305 - val_MinusLogProbMetric: 17.1305 - lr: 1.0000e-06 - 74s/epoch - 379ms/step
Epoch 985/1000
2023-09-28 06:07:14.351 
Epoch 985/1000 
	 loss: 16.2277, MinusLogProbMetric: 16.2277, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 985: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2277 - MinusLogProbMetric: 16.2277 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 1.0000e-06 - 75s/epoch - 382ms/step
Epoch 986/1000
2023-09-28 06:08:29.148 
Epoch 986/1000 
	 loss: 16.2275, MinusLogProbMetric: 16.2275, val_loss: 17.1301, val_MinusLogProbMetric: 17.1301

Epoch 986: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2275 - MinusLogProbMetric: 16.2275 - val_loss: 17.1301 - val_MinusLogProbMetric: 17.1301 - lr: 1.0000e-06 - 75s/epoch - 382ms/step
Epoch 987/1000
2023-09-28 06:09:43.466 
Epoch 987/1000 
	 loss: 16.2274, MinusLogProbMetric: 16.2274, val_loss: 17.1316, val_MinusLogProbMetric: 17.1316

Epoch 987: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2274 - MinusLogProbMetric: 16.2274 - val_loss: 17.1316 - val_MinusLogProbMetric: 17.1316 - lr: 1.0000e-06 - 74s/epoch - 379ms/step
Epoch 988/1000
2023-09-28 06:10:57.982 
Epoch 988/1000 
	 loss: 16.2278, MinusLogProbMetric: 16.2278, val_loss: 17.1299, val_MinusLogProbMetric: 17.1299

Epoch 988: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2278 - MinusLogProbMetric: 16.2278 - val_loss: 17.1299 - val_MinusLogProbMetric: 17.1299 - lr: 1.0000e-06 - 75s/epoch - 380ms/step
Epoch 989/1000
2023-09-28 06:12:12.525 
Epoch 989/1000 
	 loss: 16.2279, MinusLogProbMetric: 16.2279, val_loss: 17.1299, val_MinusLogProbMetric: 17.1299

Epoch 989: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2279 - MinusLogProbMetric: 16.2279 - val_loss: 17.1299 - val_MinusLogProbMetric: 17.1299 - lr: 1.0000e-06 - 75s/epoch - 380ms/step
Epoch 990/1000
2023-09-28 06:13:27.194 
Epoch 990/1000 
	 loss: 16.2278, MinusLogProbMetric: 16.2278, val_loss: 17.1314, val_MinusLogProbMetric: 17.1314

Epoch 990: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2278 - MinusLogProbMetric: 16.2278 - val_loss: 17.1314 - val_MinusLogProbMetric: 17.1314 - lr: 1.0000e-06 - 75s/epoch - 381ms/step
Epoch 991/1000
2023-09-28 06:14:42.325 
Epoch 991/1000 
	 loss: 16.2280, MinusLogProbMetric: 16.2280, val_loss: 17.1329, val_MinusLogProbMetric: 17.1329

Epoch 991: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2280 - MinusLogProbMetric: 16.2280 - val_loss: 17.1329 - val_MinusLogProbMetric: 17.1329 - lr: 1.0000e-06 - 75s/epoch - 383ms/step
Epoch 992/1000
2023-09-28 06:15:57.575 
Epoch 992/1000 
	 loss: 16.2272, MinusLogProbMetric: 16.2272, val_loss: 17.1303, val_MinusLogProbMetric: 17.1303

Epoch 992: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2272 - MinusLogProbMetric: 16.2272 - val_loss: 17.1303 - val_MinusLogProbMetric: 17.1303 - lr: 1.0000e-06 - 75s/epoch - 384ms/step
Epoch 993/1000
2023-09-28 06:17:12.586 
Epoch 993/1000 
	 loss: 16.2274, MinusLogProbMetric: 16.2274, val_loss: 17.1302, val_MinusLogProbMetric: 17.1302

Epoch 993: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2274 - MinusLogProbMetric: 16.2274 - val_loss: 17.1302 - val_MinusLogProbMetric: 17.1302 - lr: 1.0000e-06 - 75s/epoch - 383ms/step
Epoch 994/1000
2023-09-28 06:18:27.086 
Epoch 994/1000 
	 loss: 16.2276, MinusLogProbMetric: 16.2276, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 994: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2276 - MinusLogProbMetric: 16.2276 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.0000e-06 - 74s/epoch - 380ms/step
Epoch 995/1000
2023-09-28 06:19:41.951 
Epoch 995/1000 
	 loss: 16.2273, MinusLogProbMetric: 16.2273, val_loss: 17.1299, val_MinusLogProbMetric: 17.1299

Epoch 995: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2273 - MinusLogProbMetric: 16.2273 - val_loss: 17.1299 - val_MinusLogProbMetric: 17.1299 - lr: 1.0000e-06 - 75s/epoch - 382ms/step
Epoch 996/1000
2023-09-28 06:20:56.406 
Epoch 996/1000 
	 loss: 16.2273, MinusLogProbMetric: 16.2273, val_loss: 17.1306, val_MinusLogProbMetric: 17.1306

Epoch 996: val_loss did not improve from 17.12900
196/196 - 74s - loss: 16.2273 - MinusLogProbMetric: 16.2273 - val_loss: 17.1306 - val_MinusLogProbMetric: 17.1306 - lr: 1.0000e-06 - 74s/epoch - 380ms/step
Epoch 997/1000
2023-09-28 06:22:11.328 
Epoch 997/1000 
	 loss: 16.2274, MinusLogProbMetric: 16.2274, val_loss: 17.1308, val_MinusLogProbMetric: 17.1308

Epoch 997: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2274 - MinusLogProbMetric: 16.2274 - val_loss: 17.1308 - val_MinusLogProbMetric: 17.1308 - lr: 1.0000e-06 - 75s/epoch - 382ms/step
Epoch 998/1000
2023-09-28 06:23:25.856 
Epoch 998/1000 
	 loss: 16.2278, MinusLogProbMetric: 16.2278, val_loss: 17.1296, val_MinusLogProbMetric: 17.1296

Epoch 998: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2278 - MinusLogProbMetric: 16.2278 - val_loss: 17.1296 - val_MinusLogProbMetric: 17.1296 - lr: 1.0000e-06 - 75s/epoch - 380ms/step
Epoch 999/1000
2023-09-28 06:24:40.943 
Epoch 999/1000 
	 loss: 16.2275, MinusLogProbMetric: 16.2275, val_loss: 17.1300, val_MinusLogProbMetric: 17.1300

Epoch 999: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2275 - MinusLogProbMetric: 16.2275 - val_loss: 17.1300 - val_MinusLogProbMetric: 17.1300 - lr: 1.0000e-06 - 75s/epoch - 383ms/step
Epoch 1000/1000
2023-09-28 06:25:55.622 
Epoch 1000/1000 
	 loss: 16.2270, MinusLogProbMetric: 16.2270, val_loss: 17.1332, val_MinusLogProbMetric: 17.1332

Epoch 1000: val_loss did not improve from 17.12900
196/196 - 75s - loss: 16.2270 - MinusLogProbMetric: 16.2270 - val_loss: 17.1332 - val_MinusLogProbMetric: 17.1332 - lr: 1.0000e-06 - 75s/epoch - 381ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 29.38727733801352 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 14.628855504037347 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 12.58211304602446 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 13.664580196025781 seconds.
Training succeeded with seed 869.
Model trained in 75606.51 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 72.41 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 72.62 s.
===========
Run 304/720 done in 75871.95 s.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

===========
Generating train data for run 319.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7fea6819aef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea68627040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea68627040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea682783a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea6823d360>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea6823cdf0>, <keras.callbacks.ModelCheckpoint object at 0x7fea6823cd60>, <keras.callbacks.EarlyStopping object at 0x7fea6823ca60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea6823cac0>, <keras.callbacks.TerminateOnNaN object at 0x7fea6823ceb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:27:20.482825
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 35: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:30:13.515 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2756.5791, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 173s - loss: nan - MinusLogProbMetric: 2756.5791 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 173s/epoch - 882ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 319.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_71"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_72 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7fea35a26140>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea356face0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea356face0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe9947d7790>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea16e4ae90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea16e4b400>, <keras.callbacks.ModelCheckpoint object at 0x7fea16e4b4c0>, <keras.callbacks.EarlyStopping object at 0x7fea16e4b730>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea16e4b760>, <keras.callbacks.TerminateOnNaN object at 0x7fea16e4b3a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:30:24.857650
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 45: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:33:26.042 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2505.1880, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 181s - loss: nan - MinusLogProbMetric: 2505.1880 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 181s/epoch - 924ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0001111111111111111.
===========
Generating train data for run 319.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_82"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_83 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7fe93466e740>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff09717f190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff09717f190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe954602290>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe9145ae260>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe9145ae7d0>, <keras.callbacks.ModelCheckpoint object at 0x7fe9145ae890>, <keras.callbacks.EarlyStopping object at 0x7fe9145aeb00>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe9145aeb30>, <keras.callbacks.TerminateOnNaN object at 0x7fe9145ae770>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:33:38.168781
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:36:26.915 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3304.0398, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 169s - loss: nan - MinusLogProbMetric: 3304.0398 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 169s/epoch - 861ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 3.703703703703703e-05.
===========
Generating train data for run 319.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_93"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_94 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7fea354a6c80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe97452b370>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe97452b370>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff097032e60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea354193c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea35419930>, <keras.callbacks.ModelCheckpoint object at 0x7fea354199f0>, <keras.callbacks.EarlyStopping object at 0x7fea35419c60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea35419c90>, <keras.callbacks.TerminateOnNaN object at 0x7fea354198d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:36:43.056829
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 11: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:39:39.241 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3362.8752, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 3362.8752 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 176s/epoch - 897ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.2345679012345677e-05.
===========
Generating train data for run 319.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_104"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_105 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7ff096ca9ae0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea3655ac80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea3655ac80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe819038f40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff0eb6fbbb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff0eb6f4160>, <keras.callbacks.ModelCheckpoint object at 0x7ff0eb6f4220>, <keras.callbacks.EarlyStopping object at 0x7ff0eb6f4490>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff0eb6f44c0>, <keras.callbacks.TerminateOnNaN object at 0x7ff0eb6f4100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:39:48.414533
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 10: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:42:40.278 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3359.0571, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 3359.0571 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 172s/epoch - 876ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 4.115226337448558e-06.
===========
Generating train data for run 319.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_115"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_116 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7fe974505090>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea367d5f60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea367d5f60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff105d62200>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea3673f280>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea3673f7f0>, <keras.callbacks.ModelCheckpoint object at 0x7fea3673f8b0>, <keras.callbacks.EarlyStopping object at 0x7fea3673fb20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea3673fb50>, <keras.callbacks.TerminateOnNaN object at 0x7fea3673f790>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:42:52.575146
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 46: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:46:01.191 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3318.7744, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 189s - loss: nan - MinusLogProbMetric: 3318.7744 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 189s/epoch - 962ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.3717421124828526e-06.
===========
Generating train data for run 319.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7ff0eb81d000>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea346728c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea346728c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe954225c00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe9542bfd30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe9542e02e0>, <keras.callbacks.ModelCheckpoint object at 0x7fe9542e03a0>, <keras.callbacks.EarlyStopping object at 0x7fe9542e0610>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe9542e0640>, <keras.callbacks.TerminateOnNaN object at 0x7fe9542e0280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:46:10.548975
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 27: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:49:08.776 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3377.4182, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 178s - loss: nan - MinusLogProbMetric: 3377.4182 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 178s/epoch - 908ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 4.572473708276175e-07.
===========
Generating train data for run 319.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_137"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_138 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7fe72f6c6fe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea351389a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea351389a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe8b0304790>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe72f6031c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe72f603730>, <keras.callbacks.ModelCheckpoint object at 0x7fe72f6037f0>, <keras.callbacks.EarlyStopping object at 0x7fe72f603a60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe72f603a90>, <keras.callbacks.TerminateOnNaN object at 0x7fe72f6036d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:49:20.460860
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 68: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:52:34.248 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3382.5496, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 194s - loss: nan - MinusLogProbMetric: 3382.5496 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 194s/epoch - 988ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.524157902758725e-07.
===========
Generating train data for run 319.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_148"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_149 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7fea7427b970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe72e5d2530>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe72e5d2530>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7febd87059f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7febd82bd5a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7febd82bdb10>, <keras.callbacks.ModelCheckpoint object at 0x7febd82bdbd0>, <keras.callbacks.EarlyStopping object at 0x7febd82bde40>, <keras.callbacks.ReduceLROnPlateau object at 0x7febd82bde70>, <keras.callbacks.TerminateOnNaN object at 0x7febd82bdab0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:52:45.633623
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 15: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:55:40.198 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3387.8140, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 174s - loss: nan - MinusLogProbMetric: 3387.8140 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 174s/epoch - 890ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 5.0805263425290834e-08.
===========
Generating train data for run 319.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_159"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_160 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7fe7442a2620>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe818f739d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe818f739d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe74425ee00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe74423cd60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe74423d2d0>, <keras.callbacks.ModelCheckpoint object at 0x7fe74423d390>, <keras.callbacks.EarlyStopping object at 0x7fe74423d600>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe74423d630>, <keras.callbacks.TerminateOnNaN object at 0x7fe74423d270>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:55:51.951402
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 40: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 06:58:57.719 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3389.1753, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 185s - loss: nan - MinusLogProbMetric: 3389.1753 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 185s/epoch - 945ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.6935087808430278e-08.
===========
Generating train data for run 319.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_319/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_319
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_170"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_171 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7febd8211420>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe8fc452080>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe8fc452080>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea542bb700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe8fc3d8100>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_319/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe8fc3d8670>, <keras.callbacks.ModelCheckpoint object at 0x7fe8fc3d8730>, <keras.callbacks.EarlyStopping object at 0x7fe8fc3d89a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe8fc3d89d0>, <keras.callbacks.TerminateOnNaN object at 0x7fe8fc3d8610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_319/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 319/720 with hyperparameters:
timestamp = 2023-09-28 06:59:10.809438
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 137: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 07:02:54.086 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3389.1721, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 223s - loss: nan - MinusLogProbMetric: 3389.1721 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 223s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 5.645029269476759e-09.
===========
Run 319/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

===========
Generating train data for run 321.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_321/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_321/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_321/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_321
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_176"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_177 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7fe8c05c6290>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0fd7b6830>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0fd7b6830>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe95428f910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe744f2c550>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe744f2d7e0>, <keras.callbacks.ModelCheckpoint object at 0x7fe744f2c1f0>, <keras.callbacks.EarlyStopping object at 0x7fe744f2cee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe744f2cb20>, <keras.callbacks.TerminateOnNaN object at 0x7fe744f246d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_321/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 321/720 with hyperparameters:
timestamp = 2023-09-28 07:03:00.143735
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 07:04:31.044 
Epoch 1/1000 
	 loss: 838.9990, MinusLogProbMetric: 838.9990, val_loss: 180.1802, val_MinusLogProbMetric: 180.1802

Epoch 1: val_loss improved from inf to 180.18024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 91s - loss: 838.9990 - MinusLogProbMetric: 838.9990 - val_loss: 180.1802 - val_MinusLogProbMetric: 180.1802 - lr: 0.0010 - 91s/epoch - 465ms/step
Epoch 2/1000
2023-09-28 07:05:03.966 
Epoch 2/1000 
	 loss: 164.6567, MinusLogProbMetric: 164.6567, val_loss: 122.4306, val_MinusLogProbMetric: 122.4306

Epoch 2: val_loss improved from 180.18024 to 122.43064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 164.6567 - MinusLogProbMetric: 164.6567 - val_loss: 122.4306 - val_MinusLogProbMetric: 122.4306 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 3/1000
2023-09-28 07:05:37.057 
Epoch 3/1000 
	 loss: 103.6997, MinusLogProbMetric: 103.6997, val_loss: 91.8291, val_MinusLogProbMetric: 91.8291

Epoch 3: val_loss improved from 122.43064 to 91.82912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 103.6997 - MinusLogProbMetric: 103.6997 - val_loss: 91.8291 - val_MinusLogProbMetric: 91.8291 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 4/1000
2023-09-28 07:06:10.408 
Epoch 4/1000 
	 loss: 84.1174, MinusLogProbMetric: 84.1174, val_loss: 76.6194, val_MinusLogProbMetric: 76.6194

Epoch 4: val_loss improved from 91.82912 to 76.61942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 84.1174 - MinusLogProbMetric: 84.1174 - val_loss: 76.6194 - val_MinusLogProbMetric: 76.6194 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 5/1000
2023-09-28 07:06:43.464 
Epoch 5/1000 
	 loss: 72.0766, MinusLogProbMetric: 72.0766, val_loss: 67.9632, val_MinusLogProbMetric: 67.9632

Epoch 5: val_loss improved from 76.61942 to 67.96316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 72.0766 - MinusLogProbMetric: 72.0766 - val_loss: 67.9632 - val_MinusLogProbMetric: 67.9632 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 6/1000
2023-09-28 07:07:16.362 
Epoch 6/1000 
	 loss: 63.8214, MinusLogProbMetric: 63.8214, val_loss: 61.2963, val_MinusLogProbMetric: 61.2963

Epoch 6: val_loss improved from 67.96316 to 61.29625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 63.8214 - MinusLogProbMetric: 63.8214 - val_loss: 61.2963 - val_MinusLogProbMetric: 61.2963 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 7/1000
2023-09-28 07:07:49.589 
Epoch 7/1000 
	 loss: 57.5984, MinusLogProbMetric: 57.5984, val_loss: 56.4751, val_MinusLogProbMetric: 56.4751

Epoch 7: val_loss improved from 61.29625 to 56.47508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 57.5984 - MinusLogProbMetric: 57.5984 - val_loss: 56.4751 - val_MinusLogProbMetric: 56.4751 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 8/1000
2023-09-28 07:08:22.675 
Epoch 8/1000 
	 loss: 52.6436, MinusLogProbMetric: 52.6436, val_loss: 50.6571, val_MinusLogProbMetric: 50.6571

Epoch 8: val_loss improved from 56.47508 to 50.65708, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 52.6436 - MinusLogProbMetric: 52.6436 - val_loss: 50.6571 - val_MinusLogProbMetric: 50.6571 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 9/1000
2023-09-28 07:08:54.556 
Epoch 9/1000 
	 loss: 49.4835, MinusLogProbMetric: 49.4835, val_loss: 48.5928, val_MinusLogProbMetric: 48.5928

Epoch 9: val_loss improved from 50.65708 to 48.59275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 32s - loss: 49.4835 - MinusLogProbMetric: 49.4835 - val_loss: 48.5928 - val_MinusLogProbMetric: 48.5928 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 10/1000
2023-09-28 07:09:26.767 
Epoch 10/1000 
	 loss: 47.2853, MinusLogProbMetric: 47.2853, val_loss: 48.6175, val_MinusLogProbMetric: 48.6175

Epoch 10: val_loss did not improve from 48.59275
196/196 - 32s - loss: 47.2853 - MinusLogProbMetric: 47.2853 - val_loss: 48.6175 - val_MinusLogProbMetric: 48.6175 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 11/1000
2023-09-28 07:09:54.336 
Epoch 11/1000 
	 loss: 45.6606, MinusLogProbMetric: 45.6606, val_loss: 45.5939, val_MinusLogProbMetric: 45.5939

Epoch 11: val_loss improved from 48.59275 to 45.59394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 28s - loss: 45.6606 - MinusLogProbMetric: 45.6606 - val_loss: 45.5939 - val_MinusLogProbMetric: 45.5939 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 12/1000
2023-09-28 07:10:24.246 
Epoch 12/1000 
	 loss: 44.1220, MinusLogProbMetric: 44.1220, val_loss: 45.1265, val_MinusLogProbMetric: 45.1265

Epoch 12: val_loss improved from 45.59394 to 45.12654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 30s - loss: 44.1220 - MinusLogProbMetric: 44.1220 - val_loss: 45.1265 - val_MinusLogProbMetric: 45.1265 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 13/1000
2023-09-28 07:10:57.119 
Epoch 13/1000 
	 loss: 43.3705, MinusLogProbMetric: 43.3705, val_loss: 43.4093, val_MinusLogProbMetric: 43.4093

Epoch 13: val_loss improved from 45.12654 to 43.40930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 43.3705 - MinusLogProbMetric: 43.3705 - val_loss: 43.4093 - val_MinusLogProbMetric: 43.4093 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 14/1000
2023-09-28 07:11:30.139 
Epoch 14/1000 
	 loss: 42.2223, MinusLogProbMetric: 42.2223, val_loss: 43.3427, val_MinusLogProbMetric: 43.3427

Epoch 14: val_loss improved from 43.40930 to 43.34272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 42.2223 - MinusLogProbMetric: 42.2223 - val_loss: 43.3427 - val_MinusLogProbMetric: 43.3427 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 15/1000
2023-09-28 07:12:03.564 
Epoch 15/1000 
	 loss: 41.7243, MinusLogProbMetric: 41.7243, val_loss: 41.3287, val_MinusLogProbMetric: 41.3287

Epoch 15: val_loss improved from 43.34272 to 41.32868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 41.7243 - MinusLogProbMetric: 41.7243 - val_loss: 41.3287 - val_MinusLogProbMetric: 41.3287 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 16/1000
2023-09-28 07:12:36.664 
Epoch 16/1000 
	 loss: 41.2138, MinusLogProbMetric: 41.2138, val_loss: 44.2538, val_MinusLogProbMetric: 44.2538

Epoch 16: val_loss did not improve from 41.32868
196/196 - 33s - loss: 41.2138 - MinusLogProbMetric: 41.2138 - val_loss: 44.2538 - val_MinusLogProbMetric: 44.2538 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 17/1000
2023-09-28 07:13:09.365 
Epoch 17/1000 
	 loss: 40.4660, MinusLogProbMetric: 40.4660, val_loss: 41.0527, val_MinusLogProbMetric: 41.0527

Epoch 17: val_loss improved from 41.32868 to 41.05273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 40.4660 - MinusLogProbMetric: 40.4660 - val_loss: 41.0527 - val_MinusLogProbMetric: 41.0527 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 18/1000
2023-09-28 07:13:42.368 
Epoch 18/1000 
	 loss: 40.2029, MinusLogProbMetric: 40.2029, val_loss: 39.4294, val_MinusLogProbMetric: 39.4294

Epoch 18: val_loss improved from 41.05273 to 39.42944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 40.2029 - MinusLogProbMetric: 40.2029 - val_loss: 39.4294 - val_MinusLogProbMetric: 39.4294 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 19/1000
2023-09-28 07:14:15.578 
Epoch 19/1000 
	 loss: 39.8312, MinusLogProbMetric: 39.8312, val_loss: 39.3333, val_MinusLogProbMetric: 39.3333

Epoch 19: val_loss improved from 39.42944 to 39.33327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 39.8312 - MinusLogProbMetric: 39.8312 - val_loss: 39.3333 - val_MinusLogProbMetric: 39.3333 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 20/1000
2023-09-28 07:14:48.753 
Epoch 20/1000 
	 loss: 39.0261, MinusLogProbMetric: 39.0261, val_loss: 40.1460, val_MinusLogProbMetric: 40.1460

Epoch 20: val_loss did not improve from 39.33327
196/196 - 33s - loss: 39.0261 - MinusLogProbMetric: 39.0261 - val_loss: 40.1460 - val_MinusLogProbMetric: 40.1460 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 21/1000
2023-09-28 07:15:21.117 
Epoch 21/1000 
	 loss: 38.6871, MinusLogProbMetric: 38.6871, val_loss: 39.4962, val_MinusLogProbMetric: 39.4962

Epoch 21: val_loss did not improve from 39.33327
196/196 - 32s - loss: 38.6871 - MinusLogProbMetric: 38.6871 - val_loss: 39.4962 - val_MinusLogProbMetric: 39.4962 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 22/1000
2023-09-28 07:15:53.465 
Epoch 22/1000 
	 loss: 38.4534, MinusLogProbMetric: 38.4534, val_loss: 40.2126, val_MinusLogProbMetric: 40.2126

Epoch 22: val_loss did not improve from 39.33327
196/196 - 32s - loss: 38.4534 - MinusLogProbMetric: 38.4534 - val_loss: 40.2126 - val_MinusLogProbMetric: 40.2126 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 23/1000
2023-09-28 07:16:26.118 
Epoch 23/1000 
	 loss: 38.2255, MinusLogProbMetric: 38.2255, val_loss: 37.5855, val_MinusLogProbMetric: 37.5855

Epoch 23: val_loss improved from 39.33327 to 37.58549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 38.2255 - MinusLogProbMetric: 38.2255 - val_loss: 37.5855 - val_MinusLogProbMetric: 37.5855 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 24/1000
2023-09-28 07:16:59.653 
Epoch 24/1000 
	 loss: 37.9927, MinusLogProbMetric: 37.9927, val_loss: 40.2927, val_MinusLogProbMetric: 40.2927

Epoch 24: val_loss did not improve from 37.58549
196/196 - 33s - loss: 37.9927 - MinusLogProbMetric: 37.9927 - val_loss: 40.2927 - val_MinusLogProbMetric: 40.2927 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 25/1000
2023-09-28 07:17:32.336 
Epoch 25/1000 
	 loss: 37.2927, MinusLogProbMetric: 37.2927, val_loss: 39.1153, val_MinusLogProbMetric: 39.1153

Epoch 25: val_loss did not improve from 37.58549
196/196 - 33s - loss: 37.2927 - MinusLogProbMetric: 37.2927 - val_loss: 39.1153 - val_MinusLogProbMetric: 39.1153 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 26/1000
2023-09-28 07:18:04.799 
Epoch 26/1000 
	 loss: 37.3308, MinusLogProbMetric: 37.3308, val_loss: 36.4463, val_MinusLogProbMetric: 36.4463

Epoch 26: val_loss improved from 37.58549 to 36.44625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 37.3308 - MinusLogProbMetric: 37.3308 - val_loss: 36.4463 - val_MinusLogProbMetric: 36.4463 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 27/1000
2023-09-28 07:18:37.711 
Epoch 27/1000 
	 loss: 37.7347, MinusLogProbMetric: 37.7347, val_loss: 36.1304, val_MinusLogProbMetric: 36.1304

Epoch 27: val_loss improved from 36.44625 to 36.13041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 37.7347 - MinusLogProbMetric: 37.7347 - val_loss: 36.1304 - val_MinusLogProbMetric: 36.1304 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 28/1000
2023-09-28 07:19:10.667 
Epoch 28/1000 
	 loss: 36.6423, MinusLogProbMetric: 36.6423, val_loss: 35.7474, val_MinusLogProbMetric: 35.7474

Epoch 28: val_loss improved from 36.13041 to 35.74744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 36.6423 - MinusLogProbMetric: 36.6423 - val_loss: 35.7474 - val_MinusLogProbMetric: 35.7474 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 29/1000
2023-09-28 07:19:44.122 
Epoch 29/1000 
	 loss: 37.0373, MinusLogProbMetric: 37.0373, val_loss: 37.2431, val_MinusLogProbMetric: 37.2431

Epoch 29: val_loss did not improve from 35.74744
196/196 - 33s - loss: 37.0373 - MinusLogProbMetric: 37.0373 - val_loss: 37.2431 - val_MinusLogProbMetric: 37.2431 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 30/1000
2023-09-28 07:20:16.536 
Epoch 30/1000 
	 loss: 36.5401, MinusLogProbMetric: 36.5401, val_loss: 35.5957, val_MinusLogProbMetric: 35.5957

Epoch 30: val_loss improved from 35.74744 to 35.59570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 36.5401 - MinusLogProbMetric: 36.5401 - val_loss: 35.5957 - val_MinusLogProbMetric: 35.5957 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 31/1000
2023-09-28 07:20:49.558 
Epoch 31/1000 
	 loss: 36.5191, MinusLogProbMetric: 36.5191, val_loss: 36.2887, val_MinusLogProbMetric: 36.2887

Epoch 31: val_loss did not improve from 35.59570
196/196 - 33s - loss: 36.5191 - MinusLogProbMetric: 36.5191 - val_loss: 36.2887 - val_MinusLogProbMetric: 36.2887 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 32/1000
2023-09-28 07:21:22.002 
Epoch 32/1000 
	 loss: 36.2821, MinusLogProbMetric: 36.2821, val_loss: 37.1767, val_MinusLogProbMetric: 37.1767

Epoch 32: val_loss did not improve from 35.59570
196/196 - 32s - loss: 36.2821 - MinusLogProbMetric: 36.2821 - val_loss: 37.1767 - val_MinusLogProbMetric: 37.1767 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 33/1000
2023-09-28 07:21:54.456 
Epoch 33/1000 
	 loss: 36.0745, MinusLogProbMetric: 36.0745, val_loss: 36.2092, val_MinusLogProbMetric: 36.2092

Epoch 33: val_loss did not improve from 35.59570
196/196 - 32s - loss: 36.0745 - MinusLogProbMetric: 36.0745 - val_loss: 36.2092 - val_MinusLogProbMetric: 36.2092 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 34/1000
2023-09-28 07:22:27.014 
Epoch 34/1000 
	 loss: 36.0670, MinusLogProbMetric: 36.0670, val_loss: 36.0176, val_MinusLogProbMetric: 36.0176

Epoch 34: val_loss did not improve from 35.59570
196/196 - 33s - loss: 36.0670 - MinusLogProbMetric: 36.0670 - val_loss: 36.0176 - val_MinusLogProbMetric: 36.0176 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 35/1000
2023-09-28 07:22:59.478 
Epoch 35/1000 
	 loss: 35.7066, MinusLogProbMetric: 35.7066, val_loss: 37.9946, val_MinusLogProbMetric: 37.9946

Epoch 35: val_loss did not improve from 35.59570
196/196 - 32s - loss: 35.7066 - MinusLogProbMetric: 35.7066 - val_loss: 37.9946 - val_MinusLogProbMetric: 37.9946 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 36/1000
2023-09-28 07:23:31.809 
Epoch 36/1000 
	 loss: 35.8318, MinusLogProbMetric: 35.8318, val_loss: 36.2247, val_MinusLogProbMetric: 36.2247

Epoch 36: val_loss did not improve from 35.59570
196/196 - 32s - loss: 35.8318 - MinusLogProbMetric: 35.8318 - val_loss: 36.2247 - val_MinusLogProbMetric: 36.2247 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 37/1000
2023-09-28 07:24:04.542 
Epoch 37/1000 
	 loss: 35.5095, MinusLogProbMetric: 35.5095, val_loss: 35.2254, val_MinusLogProbMetric: 35.2254

Epoch 37: val_loss improved from 35.59570 to 35.22542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 35.5095 - MinusLogProbMetric: 35.5095 - val_loss: 35.2254 - val_MinusLogProbMetric: 35.2254 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 38/1000
2023-09-28 07:24:37.599 
Epoch 38/1000 
	 loss: 35.4827, MinusLogProbMetric: 35.4827, val_loss: 36.4302, val_MinusLogProbMetric: 36.4302

Epoch 38: val_loss did not improve from 35.22542
196/196 - 33s - loss: 35.4827 - MinusLogProbMetric: 35.4827 - val_loss: 36.4302 - val_MinusLogProbMetric: 36.4302 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 39/1000
2023-09-28 07:25:09.988 
Epoch 39/1000 
	 loss: 35.1339, MinusLogProbMetric: 35.1339, val_loss: 34.9979, val_MinusLogProbMetric: 34.9979

Epoch 39: val_loss improved from 35.22542 to 34.99794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 35.1339 - MinusLogProbMetric: 35.1339 - val_loss: 34.9979 - val_MinusLogProbMetric: 34.9979 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 40/1000
2023-09-28 07:25:43.034 
Epoch 40/1000 
	 loss: 35.3039, MinusLogProbMetric: 35.3039, val_loss: 35.3875, val_MinusLogProbMetric: 35.3875

Epoch 40: val_loss did not improve from 34.99794
196/196 - 32s - loss: 35.3039 - MinusLogProbMetric: 35.3039 - val_loss: 35.3875 - val_MinusLogProbMetric: 35.3875 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 41/1000
2023-09-28 07:26:15.473 
Epoch 41/1000 
	 loss: 35.1675, MinusLogProbMetric: 35.1675, val_loss: 35.2509, val_MinusLogProbMetric: 35.2509

Epoch 41: val_loss did not improve from 34.99794
196/196 - 32s - loss: 35.1675 - MinusLogProbMetric: 35.1675 - val_loss: 35.2509 - val_MinusLogProbMetric: 35.2509 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 42/1000
2023-09-28 07:26:48.082 
Epoch 42/1000 
	 loss: 34.8130, MinusLogProbMetric: 34.8130, val_loss: 34.4567, val_MinusLogProbMetric: 34.4567

Epoch 42: val_loss improved from 34.99794 to 34.45670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 34.8130 - MinusLogProbMetric: 34.8130 - val_loss: 34.4567 - val_MinusLogProbMetric: 34.4567 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 43/1000
2023-09-28 07:27:21.136 
Epoch 43/1000 
	 loss: 34.7995, MinusLogProbMetric: 34.7995, val_loss: 34.9720, val_MinusLogProbMetric: 34.9720

Epoch 43: val_loss did not improve from 34.45670
196/196 - 33s - loss: 34.7995 - MinusLogProbMetric: 34.7995 - val_loss: 34.9720 - val_MinusLogProbMetric: 34.9720 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 44/1000
2023-09-28 07:27:53.600 
Epoch 44/1000 
	 loss: 34.2589, MinusLogProbMetric: 34.2589, val_loss: 36.3287, val_MinusLogProbMetric: 36.3287

Epoch 44: val_loss did not improve from 34.45670
196/196 - 32s - loss: 34.2589 - MinusLogProbMetric: 34.2589 - val_loss: 36.3287 - val_MinusLogProbMetric: 36.3287 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 45/1000
2023-09-28 07:28:26.068 
Epoch 45/1000 
	 loss: 34.4729, MinusLogProbMetric: 34.4729, val_loss: 33.2944, val_MinusLogProbMetric: 33.2944

Epoch 45: val_loss improved from 34.45670 to 33.29436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 34.4729 - MinusLogProbMetric: 34.4729 - val_loss: 33.2944 - val_MinusLogProbMetric: 33.2944 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 46/1000
2023-09-28 07:28:59.030 
Epoch 46/1000 
	 loss: 34.5549, MinusLogProbMetric: 34.5549, val_loss: 34.3263, val_MinusLogProbMetric: 34.3263

Epoch 46: val_loss did not improve from 33.29436
196/196 - 32s - loss: 34.5549 - MinusLogProbMetric: 34.5549 - val_loss: 34.3263 - val_MinusLogProbMetric: 34.3263 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 47/1000
2023-09-28 07:29:31.539 
Epoch 47/1000 
	 loss: 34.2790, MinusLogProbMetric: 34.2790, val_loss: 34.6270, val_MinusLogProbMetric: 34.6270

Epoch 47: val_loss did not improve from 33.29436
196/196 - 33s - loss: 34.2790 - MinusLogProbMetric: 34.2790 - val_loss: 34.6270 - val_MinusLogProbMetric: 34.6270 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 48/1000
2023-09-28 07:30:04.034 
Epoch 48/1000 
	 loss: 34.4210, MinusLogProbMetric: 34.4210, val_loss: 35.4072, val_MinusLogProbMetric: 35.4072

Epoch 48: val_loss did not improve from 33.29436
196/196 - 32s - loss: 34.4210 - MinusLogProbMetric: 34.4210 - val_loss: 35.4072 - val_MinusLogProbMetric: 35.4072 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 49/1000
2023-09-28 07:30:36.505 
Epoch 49/1000 
	 loss: 34.1601, MinusLogProbMetric: 34.1601, val_loss: 34.0285, val_MinusLogProbMetric: 34.0285

Epoch 49: val_loss did not improve from 33.29436
196/196 - 32s - loss: 34.1601 - MinusLogProbMetric: 34.1601 - val_loss: 34.0285 - val_MinusLogProbMetric: 34.0285 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 50/1000
2023-09-28 07:31:09.078 
Epoch 50/1000 
	 loss: 33.9219, MinusLogProbMetric: 33.9219, val_loss: 33.1938, val_MinusLogProbMetric: 33.1938

Epoch 50: val_loss improved from 33.29436 to 33.19378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 33.9219 - MinusLogProbMetric: 33.9219 - val_loss: 33.1938 - val_MinusLogProbMetric: 33.1938 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 51/1000
2023-09-28 07:31:42.181 
Epoch 51/1000 
	 loss: 33.7496, MinusLogProbMetric: 33.7496, val_loss: 33.3300, val_MinusLogProbMetric: 33.3300

Epoch 51: val_loss did not improve from 33.19378
196/196 - 33s - loss: 33.7496 - MinusLogProbMetric: 33.7496 - val_loss: 33.3300 - val_MinusLogProbMetric: 33.3300 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 52/1000
2023-09-28 07:32:14.693 
Epoch 52/1000 
	 loss: 33.7830, MinusLogProbMetric: 33.7830, val_loss: 33.3771, val_MinusLogProbMetric: 33.3771

Epoch 52: val_loss did not improve from 33.19378
196/196 - 33s - loss: 33.7830 - MinusLogProbMetric: 33.7830 - val_loss: 33.3771 - val_MinusLogProbMetric: 33.3771 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 53/1000
2023-09-28 07:32:47.170 
Epoch 53/1000 
	 loss: 34.0250, MinusLogProbMetric: 34.0250, val_loss: 34.4517, val_MinusLogProbMetric: 34.4517

Epoch 53: val_loss did not improve from 33.19378
196/196 - 32s - loss: 34.0250 - MinusLogProbMetric: 34.0250 - val_loss: 34.4517 - val_MinusLogProbMetric: 34.4517 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 54/1000
2023-09-28 07:33:19.825 
Epoch 54/1000 
	 loss: 33.8003, MinusLogProbMetric: 33.8003, val_loss: 33.5472, val_MinusLogProbMetric: 33.5472

Epoch 54: val_loss did not improve from 33.19378
196/196 - 33s - loss: 33.8003 - MinusLogProbMetric: 33.8003 - val_loss: 33.5472 - val_MinusLogProbMetric: 33.5472 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 55/1000
2023-09-28 07:33:52.298 
Epoch 55/1000 
	 loss: 33.8300, MinusLogProbMetric: 33.8300, val_loss: 33.5721, val_MinusLogProbMetric: 33.5721

Epoch 55: val_loss did not improve from 33.19378
196/196 - 32s - loss: 33.8300 - MinusLogProbMetric: 33.8300 - val_loss: 33.5721 - val_MinusLogProbMetric: 33.5721 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 56/1000
2023-09-28 07:34:24.703 
Epoch 56/1000 
	 loss: 33.4164, MinusLogProbMetric: 33.4164, val_loss: 34.2112, val_MinusLogProbMetric: 34.2112

Epoch 56: val_loss did not improve from 33.19378
196/196 - 32s - loss: 33.4164 - MinusLogProbMetric: 33.4164 - val_loss: 34.2112 - val_MinusLogProbMetric: 34.2112 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 57/1000
2023-09-28 07:34:57.229 
Epoch 57/1000 
	 loss: 33.5449, MinusLogProbMetric: 33.5449, val_loss: 33.0550, val_MinusLogProbMetric: 33.0550

Epoch 57: val_loss improved from 33.19378 to 33.05500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 33.5449 - MinusLogProbMetric: 33.5449 - val_loss: 33.0550 - val_MinusLogProbMetric: 33.0550 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 58/1000
2023-09-28 07:35:30.347 
Epoch 58/1000 
	 loss: 33.5475, MinusLogProbMetric: 33.5475, val_loss: 33.6812, val_MinusLogProbMetric: 33.6812

Epoch 58: val_loss did not improve from 33.05500
196/196 - 33s - loss: 33.5475 - MinusLogProbMetric: 33.5475 - val_loss: 33.6812 - val_MinusLogProbMetric: 33.6812 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 59/1000
2023-09-28 07:36:02.981 
Epoch 59/1000 
	 loss: 33.7220, MinusLogProbMetric: 33.7220, val_loss: 32.9824, val_MinusLogProbMetric: 32.9824

Epoch 59: val_loss improved from 33.05500 to 32.98241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 33.7220 - MinusLogProbMetric: 33.7220 - val_loss: 32.9824 - val_MinusLogProbMetric: 32.9824 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 60/1000
2023-09-28 07:36:35.965 
Epoch 60/1000 
	 loss: 33.3566, MinusLogProbMetric: 33.3566, val_loss: 34.2560, val_MinusLogProbMetric: 34.2560

Epoch 60: val_loss did not improve from 32.98241
196/196 - 32s - loss: 33.3566 - MinusLogProbMetric: 33.3566 - val_loss: 34.2560 - val_MinusLogProbMetric: 34.2560 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 61/1000
2023-09-28 07:37:08.727 
Epoch 61/1000 
	 loss: 33.2658, MinusLogProbMetric: 33.2658, val_loss: 34.9914, val_MinusLogProbMetric: 34.9914

Epoch 61: val_loss did not improve from 32.98241
196/196 - 33s - loss: 33.2658 - MinusLogProbMetric: 33.2658 - val_loss: 34.9914 - val_MinusLogProbMetric: 34.9914 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 62/1000
2023-09-28 07:37:41.355 
Epoch 62/1000 
	 loss: 33.3480, MinusLogProbMetric: 33.3480, val_loss: 32.5951, val_MinusLogProbMetric: 32.5951

Epoch 62: val_loss improved from 32.98241 to 32.59507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 33.3480 - MinusLogProbMetric: 33.3480 - val_loss: 32.5951 - val_MinusLogProbMetric: 32.5951 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 63/1000
2023-09-28 07:38:14.513 
Epoch 63/1000 
	 loss: 33.3261, MinusLogProbMetric: 33.3261, val_loss: 33.1053, val_MinusLogProbMetric: 33.1053

Epoch 63: val_loss did not improve from 32.59507
196/196 - 33s - loss: 33.3261 - MinusLogProbMetric: 33.3261 - val_loss: 33.1053 - val_MinusLogProbMetric: 33.1053 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 64/1000
2023-09-28 07:38:47.203 
Epoch 64/1000 
	 loss: 33.0804, MinusLogProbMetric: 33.0804, val_loss: 32.2686, val_MinusLogProbMetric: 32.2686

Epoch 64: val_loss improved from 32.59507 to 32.26861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 33.0804 - MinusLogProbMetric: 33.0804 - val_loss: 32.2686 - val_MinusLogProbMetric: 32.2686 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 65/1000
2023-09-28 07:39:20.281 
Epoch 65/1000 
	 loss: 32.9648, MinusLogProbMetric: 32.9648, val_loss: 32.7180, val_MinusLogProbMetric: 32.7180

Epoch 65: val_loss did not improve from 32.26861
196/196 - 33s - loss: 32.9648 - MinusLogProbMetric: 32.9648 - val_loss: 32.7180 - val_MinusLogProbMetric: 32.7180 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 66/1000
2023-09-28 07:39:52.678 
Epoch 66/1000 
	 loss: 33.0126, MinusLogProbMetric: 33.0126, val_loss: 34.2871, val_MinusLogProbMetric: 34.2871

Epoch 66: val_loss did not improve from 32.26861
196/196 - 32s - loss: 33.0126 - MinusLogProbMetric: 33.0126 - val_loss: 34.2871 - val_MinusLogProbMetric: 34.2871 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 67/1000
2023-09-28 07:40:25.207 
Epoch 67/1000 
	 loss: 33.0963, MinusLogProbMetric: 33.0963, val_loss: 32.9646, val_MinusLogProbMetric: 32.9646

Epoch 67: val_loss did not improve from 32.26861
196/196 - 33s - loss: 33.0963 - MinusLogProbMetric: 33.0963 - val_loss: 32.9646 - val_MinusLogProbMetric: 32.9646 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 68/1000
2023-09-28 07:40:57.639 
Epoch 68/1000 
	 loss: 32.8568, MinusLogProbMetric: 32.8568, val_loss: 33.0845, val_MinusLogProbMetric: 33.0845

Epoch 68: val_loss did not improve from 32.26861
196/196 - 32s - loss: 32.8568 - MinusLogProbMetric: 32.8568 - val_loss: 33.0845 - val_MinusLogProbMetric: 33.0845 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 69/1000
2023-09-28 07:41:30.192 
Epoch 69/1000 
	 loss: 32.8475, MinusLogProbMetric: 32.8475, val_loss: 34.8370, val_MinusLogProbMetric: 34.8370

Epoch 69: val_loss did not improve from 32.26861
196/196 - 33s - loss: 32.8475 - MinusLogProbMetric: 32.8475 - val_loss: 34.8370 - val_MinusLogProbMetric: 34.8370 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 70/1000
2023-09-28 07:42:02.428 
Epoch 70/1000 
	 loss: 32.6064, MinusLogProbMetric: 32.6064, val_loss: 32.1221, val_MinusLogProbMetric: 32.1221

Epoch 70: val_loss improved from 32.26861 to 32.12213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 32.6064 - MinusLogProbMetric: 32.6064 - val_loss: 32.1221 - val_MinusLogProbMetric: 32.1221 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 71/1000
2023-09-28 07:42:35.670 
Epoch 71/1000 
	 loss: 32.7678, MinusLogProbMetric: 32.7678, val_loss: 33.6246, val_MinusLogProbMetric: 33.6246

Epoch 71: val_loss did not improve from 32.12213
196/196 - 33s - loss: 32.7678 - MinusLogProbMetric: 32.7678 - val_loss: 33.6246 - val_MinusLogProbMetric: 33.6246 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 72/1000
2023-09-28 07:43:08.046 
Epoch 72/1000 
	 loss: 32.8779, MinusLogProbMetric: 32.8779, val_loss: 31.7016, val_MinusLogProbMetric: 31.7016

Epoch 72: val_loss improved from 32.12213 to 31.70159, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 32.8779 - MinusLogProbMetric: 32.8779 - val_loss: 31.7016 - val_MinusLogProbMetric: 31.7016 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 73/1000
2023-09-28 07:43:41.205 
Epoch 73/1000 
	 loss: 32.6223, MinusLogProbMetric: 32.6223, val_loss: 31.9243, val_MinusLogProbMetric: 31.9243

Epoch 73: val_loss did not improve from 31.70159
196/196 - 33s - loss: 32.6223 - MinusLogProbMetric: 32.6223 - val_loss: 31.9243 - val_MinusLogProbMetric: 31.9243 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 74/1000
2023-09-28 07:44:13.713 
Epoch 74/1000 
	 loss: 32.6862, MinusLogProbMetric: 32.6862, val_loss: 32.3284, val_MinusLogProbMetric: 32.3284

Epoch 74: val_loss did not improve from 31.70159
196/196 - 33s - loss: 32.6862 - MinusLogProbMetric: 32.6862 - val_loss: 32.3284 - val_MinusLogProbMetric: 32.3284 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 75/1000
2023-09-28 07:44:46.284 
Epoch 75/1000 
	 loss: 32.5164, MinusLogProbMetric: 32.5164, val_loss: 31.9223, val_MinusLogProbMetric: 31.9223

Epoch 75: val_loss did not improve from 31.70159
196/196 - 33s - loss: 32.5164 - MinusLogProbMetric: 32.5164 - val_loss: 31.9223 - val_MinusLogProbMetric: 31.9223 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 76/1000
2023-09-28 07:45:18.937 
Epoch 76/1000 
	 loss: 32.6066, MinusLogProbMetric: 32.6066, val_loss: 33.5626, val_MinusLogProbMetric: 33.5626

Epoch 76: val_loss did not improve from 31.70159
196/196 - 33s - loss: 32.6066 - MinusLogProbMetric: 32.6066 - val_loss: 33.5626 - val_MinusLogProbMetric: 33.5626 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 77/1000
2023-09-28 07:45:51.471 
Epoch 77/1000 
	 loss: 32.6363, MinusLogProbMetric: 32.6363, val_loss: 32.6539, val_MinusLogProbMetric: 32.6539

Epoch 77: val_loss did not improve from 31.70159
196/196 - 33s - loss: 32.6363 - MinusLogProbMetric: 32.6363 - val_loss: 32.6539 - val_MinusLogProbMetric: 32.6539 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 78/1000
2023-09-28 07:46:23.948 
Epoch 78/1000 
	 loss: 32.7231, MinusLogProbMetric: 32.7231, val_loss: 31.5738, val_MinusLogProbMetric: 31.5738

Epoch 78: val_loss improved from 31.70159 to 31.57381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 32.7231 - MinusLogProbMetric: 32.7231 - val_loss: 31.5738 - val_MinusLogProbMetric: 31.5738 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 79/1000
2023-09-28 07:46:57.173 
Epoch 79/1000 
	 loss: 32.3836, MinusLogProbMetric: 32.3836, val_loss: 31.6239, val_MinusLogProbMetric: 31.6239

Epoch 79: val_loss did not improve from 31.57381
196/196 - 33s - loss: 32.3836 - MinusLogProbMetric: 32.3836 - val_loss: 31.6239 - val_MinusLogProbMetric: 31.6239 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 80/1000
2023-09-28 07:47:29.591 
Epoch 80/1000 
	 loss: 32.6614, MinusLogProbMetric: 32.6614, val_loss: 32.4705, val_MinusLogProbMetric: 32.4705

Epoch 80: val_loss did not improve from 31.57381
196/196 - 32s - loss: 32.6614 - MinusLogProbMetric: 32.6614 - val_loss: 32.4705 - val_MinusLogProbMetric: 32.4705 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 81/1000
2023-09-28 07:48:01.974 
Epoch 81/1000 
	 loss: 32.3151, MinusLogProbMetric: 32.3151, val_loss: 32.1560, val_MinusLogProbMetric: 32.1560

Epoch 81: val_loss did not improve from 31.57381
196/196 - 32s - loss: 32.3151 - MinusLogProbMetric: 32.3151 - val_loss: 32.1560 - val_MinusLogProbMetric: 32.1560 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 82/1000
2023-09-28 07:48:34.293 
Epoch 82/1000 
	 loss: 32.4380, MinusLogProbMetric: 32.4380, val_loss: 31.8736, val_MinusLogProbMetric: 31.8736

Epoch 82: val_loss did not improve from 31.57381
196/196 - 32s - loss: 32.4380 - MinusLogProbMetric: 32.4380 - val_loss: 31.8736 - val_MinusLogProbMetric: 31.8736 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 83/1000
2023-09-28 07:49:06.687 
Epoch 83/1000 
	 loss: 32.1355, MinusLogProbMetric: 32.1355, val_loss: 32.2511, val_MinusLogProbMetric: 32.2511

Epoch 83: val_loss did not improve from 31.57381
196/196 - 32s - loss: 32.1355 - MinusLogProbMetric: 32.1355 - val_loss: 32.2511 - val_MinusLogProbMetric: 32.2511 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 84/1000
2023-09-28 07:49:39.318 
Epoch 84/1000 
	 loss: 32.3137, MinusLogProbMetric: 32.3137, val_loss: 32.6569, val_MinusLogProbMetric: 32.6569

Epoch 84: val_loss did not improve from 31.57381
196/196 - 33s - loss: 32.3137 - MinusLogProbMetric: 32.3137 - val_loss: 32.6569 - val_MinusLogProbMetric: 32.6569 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 85/1000
2023-09-28 07:50:11.778 
Epoch 85/1000 
	 loss: 32.1511, MinusLogProbMetric: 32.1511, val_loss: 32.1281, val_MinusLogProbMetric: 32.1281

Epoch 85: val_loss did not improve from 31.57381
196/196 - 32s - loss: 32.1511 - MinusLogProbMetric: 32.1511 - val_loss: 32.1281 - val_MinusLogProbMetric: 32.1281 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 86/1000
2023-09-28 07:50:44.162 
Epoch 86/1000 
	 loss: 32.2528, MinusLogProbMetric: 32.2528, val_loss: 32.6926, val_MinusLogProbMetric: 32.6926

Epoch 86: val_loss did not improve from 31.57381
196/196 - 32s - loss: 32.2528 - MinusLogProbMetric: 32.2528 - val_loss: 32.6926 - val_MinusLogProbMetric: 32.6926 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 87/1000
2023-09-28 07:51:16.474 
Epoch 87/1000 
	 loss: 32.4841, MinusLogProbMetric: 32.4841, val_loss: 31.8516, val_MinusLogProbMetric: 31.8516

Epoch 87: val_loss did not improve from 31.57381
196/196 - 32s - loss: 32.4841 - MinusLogProbMetric: 32.4841 - val_loss: 31.8516 - val_MinusLogProbMetric: 31.8516 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 88/1000
2023-09-28 07:51:49.251 
Epoch 88/1000 
	 loss: 32.2002, MinusLogProbMetric: 32.2002, val_loss: 32.8057, val_MinusLogProbMetric: 32.8057

Epoch 88: val_loss did not improve from 31.57381
196/196 - 33s - loss: 32.2002 - MinusLogProbMetric: 32.2002 - val_loss: 32.8057 - val_MinusLogProbMetric: 32.8057 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 89/1000
2023-09-28 07:52:21.734 
Epoch 89/1000 
	 loss: 32.2053, MinusLogProbMetric: 32.2053, val_loss: 31.8502, val_MinusLogProbMetric: 31.8502

Epoch 89: val_loss did not improve from 31.57381
196/196 - 32s - loss: 32.2053 - MinusLogProbMetric: 32.2053 - val_loss: 31.8502 - val_MinusLogProbMetric: 31.8502 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 90/1000
2023-09-28 07:52:54.289 
Epoch 90/1000 
	 loss: 32.0499, MinusLogProbMetric: 32.0499, val_loss: 32.5268, val_MinusLogProbMetric: 32.5268

Epoch 90: val_loss did not improve from 31.57381
196/196 - 33s - loss: 32.0499 - MinusLogProbMetric: 32.0499 - val_loss: 32.5268 - val_MinusLogProbMetric: 32.5268 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 91/1000
2023-09-28 07:53:26.929 
Epoch 91/1000 
	 loss: 32.1286, MinusLogProbMetric: 32.1286, val_loss: 33.3379, val_MinusLogProbMetric: 33.3379

Epoch 91: val_loss did not improve from 31.57381
196/196 - 33s - loss: 32.1286 - MinusLogProbMetric: 32.1286 - val_loss: 33.3379 - val_MinusLogProbMetric: 33.3379 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 92/1000
2023-09-28 07:53:59.318 
Epoch 92/1000 
	 loss: 31.9811, MinusLogProbMetric: 31.9811, val_loss: 32.0032, val_MinusLogProbMetric: 32.0032

Epoch 92: val_loss did not improve from 31.57381
196/196 - 32s - loss: 31.9811 - MinusLogProbMetric: 31.9811 - val_loss: 32.0032 - val_MinusLogProbMetric: 32.0032 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 93/1000
2023-09-28 07:54:32.072 
Epoch 93/1000 
	 loss: 32.0439, MinusLogProbMetric: 32.0439, val_loss: 32.7994, val_MinusLogProbMetric: 32.7994

Epoch 93: val_loss did not improve from 31.57381
196/196 - 33s - loss: 32.0439 - MinusLogProbMetric: 32.0439 - val_loss: 32.7994 - val_MinusLogProbMetric: 32.7994 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 94/1000
2023-09-28 07:55:04.626 
Epoch 94/1000 
	 loss: 31.8643, MinusLogProbMetric: 31.8643, val_loss: 31.8932, val_MinusLogProbMetric: 31.8932

Epoch 94: val_loss did not improve from 31.57381
196/196 - 33s - loss: 31.8643 - MinusLogProbMetric: 31.8643 - val_loss: 31.8932 - val_MinusLogProbMetric: 31.8932 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 95/1000
2023-09-28 07:55:37.111 
Epoch 95/1000 
	 loss: 31.7967, MinusLogProbMetric: 31.7967, val_loss: 31.3001, val_MinusLogProbMetric: 31.3001

Epoch 95: val_loss improved from 31.57381 to 31.30006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 31.7967 - MinusLogProbMetric: 31.7967 - val_loss: 31.3001 - val_MinusLogProbMetric: 31.3001 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 96/1000
2023-09-28 07:56:10.134 
Epoch 96/1000 
	 loss: 32.0941, MinusLogProbMetric: 32.0941, val_loss: 31.3128, val_MinusLogProbMetric: 31.3128

Epoch 96: val_loss did not improve from 31.30006
196/196 - 33s - loss: 32.0941 - MinusLogProbMetric: 32.0941 - val_loss: 31.3128 - val_MinusLogProbMetric: 31.3128 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 97/1000
2023-09-28 07:56:42.624 
Epoch 97/1000 
	 loss: 31.8567, MinusLogProbMetric: 31.8567, val_loss: 32.4240, val_MinusLogProbMetric: 32.4240

Epoch 97: val_loss did not improve from 31.30006
196/196 - 32s - loss: 31.8567 - MinusLogProbMetric: 31.8567 - val_loss: 32.4240 - val_MinusLogProbMetric: 32.4240 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 98/1000
2023-09-28 07:57:15.081 
Epoch 98/1000 
	 loss: 31.9236, MinusLogProbMetric: 31.9236, val_loss: 32.3845, val_MinusLogProbMetric: 32.3845

Epoch 98: val_loss did not improve from 31.30006
196/196 - 32s - loss: 31.9236 - MinusLogProbMetric: 31.9236 - val_loss: 32.3845 - val_MinusLogProbMetric: 32.3845 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 99/1000
2023-09-28 07:57:47.723 
Epoch 99/1000 
	 loss: 32.0573, MinusLogProbMetric: 32.0573, val_loss: 34.6566, val_MinusLogProbMetric: 34.6566

Epoch 99: val_loss did not improve from 31.30006
196/196 - 33s - loss: 32.0573 - MinusLogProbMetric: 32.0573 - val_loss: 34.6566 - val_MinusLogProbMetric: 34.6566 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 100/1000
2023-09-28 07:58:19.994 
Epoch 100/1000 
	 loss: 31.6519, MinusLogProbMetric: 31.6519, val_loss: 34.4028, val_MinusLogProbMetric: 34.4028

Epoch 100: val_loss did not improve from 31.30006
196/196 - 32s - loss: 31.6519 - MinusLogProbMetric: 31.6519 - val_loss: 34.4028 - val_MinusLogProbMetric: 34.4028 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 101/1000
2023-09-28 07:58:52.622 
Epoch 101/1000 
	 loss: 31.7557, MinusLogProbMetric: 31.7557, val_loss: 31.3603, val_MinusLogProbMetric: 31.3603

Epoch 101: val_loss did not improve from 31.30006
196/196 - 33s - loss: 31.7557 - MinusLogProbMetric: 31.7557 - val_loss: 31.3603 - val_MinusLogProbMetric: 31.3603 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 102/1000
2023-09-28 07:59:25.069 
Epoch 102/1000 
	 loss: 31.7884, MinusLogProbMetric: 31.7884, val_loss: 30.9927, val_MinusLogProbMetric: 30.9927

Epoch 102: val_loss improved from 31.30006 to 30.99268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 31.7884 - MinusLogProbMetric: 31.7884 - val_loss: 30.9927 - val_MinusLogProbMetric: 30.9927 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 103/1000
2023-09-28 07:59:58.110 
Epoch 103/1000 
	 loss: 31.8566, MinusLogProbMetric: 31.8566, val_loss: 32.4059, val_MinusLogProbMetric: 32.4059

Epoch 103: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.8566 - MinusLogProbMetric: 31.8566 - val_loss: 32.4059 - val_MinusLogProbMetric: 32.4059 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 104/1000
2023-09-28 08:00:30.474 
Epoch 104/1000 
	 loss: 31.5916, MinusLogProbMetric: 31.5916, val_loss: 31.0293, val_MinusLogProbMetric: 31.0293

Epoch 104: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.5916 - MinusLogProbMetric: 31.5916 - val_loss: 31.0293 - val_MinusLogProbMetric: 31.0293 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 105/1000
2023-09-28 08:01:03.012 
Epoch 105/1000 
	 loss: 31.5635, MinusLogProbMetric: 31.5635, val_loss: 31.4270, val_MinusLogProbMetric: 31.4270

Epoch 105: val_loss did not improve from 30.99268
196/196 - 33s - loss: 31.5635 - MinusLogProbMetric: 31.5635 - val_loss: 31.4270 - val_MinusLogProbMetric: 31.4270 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 106/1000
2023-09-28 08:01:35.464 
Epoch 106/1000 
	 loss: 31.7127, MinusLogProbMetric: 31.7127, val_loss: 31.3069, val_MinusLogProbMetric: 31.3069

Epoch 106: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.7127 - MinusLogProbMetric: 31.7127 - val_loss: 31.3069 - val_MinusLogProbMetric: 31.3069 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 107/1000
2023-09-28 08:02:07.871 
Epoch 107/1000 
	 loss: 31.5097, MinusLogProbMetric: 31.5097, val_loss: 33.1435, val_MinusLogProbMetric: 33.1435

Epoch 107: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.5097 - MinusLogProbMetric: 31.5097 - val_loss: 33.1435 - val_MinusLogProbMetric: 33.1435 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 108/1000
2023-09-28 08:02:40.676 
Epoch 108/1000 
	 loss: 31.6352, MinusLogProbMetric: 31.6352, val_loss: 31.3347, val_MinusLogProbMetric: 31.3347

Epoch 108: val_loss did not improve from 30.99268
196/196 - 33s - loss: 31.6352 - MinusLogProbMetric: 31.6352 - val_loss: 31.3347 - val_MinusLogProbMetric: 31.3347 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 109/1000
2023-09-28 08:03:13.102 
Epoch 109/1000 
	 loss: 31.7880, MinusLogProbMetric: 31.7880, val_loss: 31.7575, val_MinusLogProbMetric: 31.7575

Epoch 109: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.7880 - MinusLogProbMetric: 31.7880 - val_loss: 31.7575 - val_MinusLogProbMetric: 31.7575 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 110/1000
2023-09-28 08:03:45.457 
Epoch 110/1000 
	 loss: 31.6145, MinusLogProbMetric: 31.6145, val_loss: 33.2921, val_MinusLogProbMetric: 33.2921

Epoch 110: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.6145 - MinusLogProbMetric: 31.6145 - val_loss: 33.2921 - val_MinusLogProbMetric: 33.2921 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 111/1000
2023-09-28 08:04:17.712 
Epoch 111/1000 
	 loss: 31.6441, MinusLogProbMetric: 31.6441, val_loss: 32.3927, val_MinusLogProbMetric: 32.3927

Epoch 111: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.6441 - MinusLogProbMetric: 31.6441 - val_loss: 32.3927 - val_MinusLogProbMetric: 32.3927 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 112/1000
2023-09-28 08:04:50.155 
Epoch 112/1000 
	 loss: 31.7290, MinusLogProbMetric: 31.7290, val_loss: 31.5063, val_MinusLogProbMetric: 31.5063

Epoch 112: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.7290 - MinusLogProbMetric: 31.7290 - val_loss: 31.5063 - val_MinusLogProbMetric: 31.5063 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 113/1000
2023-09-28 08:05:22.576 
Epoch 113/1000 
	 loss: 31.7098, MinusLogProbMetric: 31.7098, val_loss: 31.4346, val_MinusLogProbMetric: 31.4346

Epoch 113: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.7098 - MinusLogProbMetric: 31.7098 - val_loss: 31.4346 - val_MinusLogProbMetric: 31.4346 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 114/1000
2023-09-28 08:05:54.959 
Epoch 114/1000 
	 loss: 31.4477, MinusLogProbMetric: 31.4477, val_loss: 32.8399, val_MinusLogProbMetric: 32.8399

Epoch 114: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.4477 - MinusLogProbMetric: 31.4477 - val_loss: 32.8399 - val_MinusLogProbMetric: 32.8399 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 115/1000
2023-09-28 08:06:27.169 
Epoch 115/1000 
	 loss: 31.5163, MinusLogProbMetric: 31.5163, val_loss: 31.8519, val_MinusLogProbMetric: 31.8519

Epoch 115: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.5163 - MinusLogProbMetric: 31.5163 - val_loss: 31.8519 - val_MinusLogProbMetric: 31.8519 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 116/1000
2023-09-28 08:06:59.666 
Epoch 116/1000 
	 loss: 31.4897, MinusLogProbMetric: 31.4897, val_loss: 31.5290, val_MinusLogProbMetric: 31.5290

Epoch 116: val_loss did not improve from 30.99268
196/196 - 32s - loss: 31.4897 - MinusLogProbMetric: 31.4897 - val_loss: 31.5290 - val_MinusLogProbMetric: 31.5290 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 117/1000
2023-09-28 08:07:32.301 
Epoch 117/1000 
	 loss: 31.6991, MinusLogProbMetric: 31.6991, val_loss: 31.9398, val_MinusLogProbMetric: 31.9398

Epoch 117: val_loss did not improve from 30.99268
196/196 - 33s - loss: 31.6991 - MinusLogProbMetric: 31.6991 - val_loss: 31.9398 - val_MinusLogProbMetric: 31.9398 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 118/1000
2023-09-28 08:08:04.983 
Epoch 118/1000 
	 loss: 31.4641, MinusLogProbMetric: 31.4641, val_loss: 30.7900, val_MinusLogProbMetric: 30.7900

Epoch 118: val_loss improved from 30.99268 to 30.78999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 31.4641 - MinusLogProbMetric: 31.4641 - val_loss: 30.7900 - val_MinusLogProbMetric: 30.7900 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 119/1000
2023-09-28 08:08:38.425 
Epoch 119/1000 
	 loss: 31.4464, MinusLogProbMetric: 31.4464, val_loss: 30.7730, val_MinusLogProbMetric: 30.7730

Epoch 119: val_loss improved from 30.78999 to 30.77303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 31.4464 - MinusLogProbMetric: 31.4464 - val_loss: 30.7730 - val_MinusLogProbMetric: 30.7730 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 120/1000
2023-09-28 08:09:11.788 
Epoch 120/1000 
	 loss: 31.3028, MinusLogProbMetric: 31.3028, val_loss: 31.6453, val_MinusLogProbMetric: 31.6453

Epoch 120: val_loss did not improve from 30.77303
196/196 - 33s - loss: 31.3028 - MinusLogProbMetric: 31.3028 - val_loss: 31.6453 - val_MinusLogProbMetric: 31.6453 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 121/1000
2023-09-28 08:09:44.440 
Epoch 121/1000 
	 loss: 31.5269, MinusLogProbMetric: 31.5269, val_loss: 31.0431, val_MinusLogProbMetric: 31.0431

Epoch 121: val_loss did not improve from 30.77303
196/196 - 33s - loss: 31.5269 - MinusLogProbMetric: 31.5269 - val_loss: 31.0431 - val_MinusLogProbMetric: 31.0431 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 122/1000
2023-09-28 08:10:16.943 
Epoch 122/1000 
	 loss: 31.3331, MinusLogProbMetric: 31.3331, val_loss: 30.7015, val_MinusLogProbMetric: 30.7015

Epoch 122: val_loss improved from 30.77303 to 30.70145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 31.3331 - MinusLogProbMetric: 31.3331 - val_loss: 30.7015 - val_MinusLogProbMetric: 30.7015 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 123/1000
2023-09-28 08:10:49.931 
Epoch 123/1000 
	 loss: 31.2472, MinusLogProbMetric: 31.2472, val_loss: 32.9904, val_MinusLogProbMetric: 32.9904

Epoch 123: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.2472 - MinusLogProbMetric: 31.2472 - val_loss: 32.9904 - val_MinusLogProbMetric: 32.9904 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 124/1000
2023-09-28 08:11:22.035 
Epoch 124/1000 
	 loss: 31.2974, MinusLogProbMetric: 31.2974, val_loss: 33.5905, val_MinusLogProbMetric: 33.5905

Epoch 124: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.2974 - MinusLogProbMetric: 31.2974 - val_loss: 33.5905 - val_MinusLogProbMetric: 33.5905 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 125/1000
2023-09-28 08:11:54.055 
Epoch 125/1000 
	 loss: 31.2760, MinusLogProbMetric: 31.2760, val_loss: 31.9619, val_MinusLogProbMetric: 31.9619

Epoch 125: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.2760 - MinusLogProbMetric: 31.2760 - val_loss: 31.9619 - val_MinusLogProbMetric: 31.9619 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 126/1000
2023-09-28 08:12:26.638 
Epoch 126/1000 
	 loss: 31.3575, MinusLogProbMetric: 31.3575, val_loss: 31.0217, val_MinusLogProbMetric: 31.0217

Epoch 126: val_loss did not improve from 30.70145
196/196 - 33s - loss: 31.3575 - MinusLogProbMetric: 31.3575 - val_loss: 31.0217 - val_MinusLogProbMetric: 31.0217 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 127/1000
2023-09-28 08:12:58.958 
Epoch 127/1000 
	 loss: 31.1567, MinusLogProbMetric: 31.1567, val_loss: 31.4666, val_MinusLogProbMetric: 31.4666

Epoch 127: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.1567 - MinusLogProbMetric: 31.1567 - val_loss: 31.4666 - val_MinusLogProbMetric: 31.4666 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 128/1000
2023-09-28 08:13:31.310 
Epoch 128/1000 
	 loss: 31.2159, MinusLogProbMetric: 31.2159, val_loss: 31.1971, val_MinusLogProbMetric: 31.1971

Epoch 128: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.2159 - MinusLogProbMetric: 31.2159 - val_loss: 31.1971 - val_MinusLogProbMetric: 31.1971 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 129/1000
2023-09-28 08:14:03.789 
Epoch 129/1000 
	 loss: 31.1082, MinusLogProbMetric: 31.1082, val_loss: 33.6339, val_MinusLogProbMetric: 33.6339

Epoch 129: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.1082 - MinusLogProbMetric: 31.1082 - val_loss: 33.6339 - val_MinusLogProbMetric: 33.6339 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 130/1000
2023-09-28 08:14:36.464 
Epoch 130/1000 
	 loss: 31.0683, MinusLogProbMetric: 31.0683, val_loss: 31.2356, val_MinusLogProbMetric: 31.2356

Epoch 130: val_loss did not improve from 30.70145
196/196 - 33s - loss: 31.0683 - MinusLogProbMetric: 31.0683 - val_loss: 31.2356 - val_MinusLogProbMetric: 31.2356 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 131/1000
2023-09-28 08:15:09.109 
Epoch 131/1000 
	 loss: 31.1645, MinusLogProbMetric: 31.1645, val_loss: 31.8596, val_MinusLogProbMetric: 31.8596

Epoch 131: val_loss did not improve from 30.70145
196/196 - 33s - loss: 31.1645 - MinusLogProbMetric: 31.1645 - val_loss: 31.8596 - val_MinusLogProbMetric: 31.8596 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 132/1000
2023-09-28 08:15:41.593 
Epoch 132/1000 
	 loss: 31.2449, MinusLogProbMetric: 31.2449, val_loss: 31.2512, val_MinusLogProbMetric: 31.2512

Epoch 132: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.2449 - MinusLogProbMetric: 31.2449 - val_loss: 31.2512 - val_MinusLogProbMetric: 31.2512 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 133/1000
2023-09-28 08:16:13.749 
Epoch 133/1000 
	 loss: 31.0922, MinusLogProbMetric: 31.0922, val_loss: 30.7423, val_MinusLogProbMetric: 30.7423

Epoch 133: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.0922 - MinusLogProbMetric: 31.0922 - val_loss: 30.7423 - val_MinusLogProbMetric: 30.7423 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 134/1000
2023-09-28 08:16:46.041 
Epoch 134/1000 
	 loss: 30.9701, MinusLogProbMetric: 30.9701, val_loss: 32.0477, val_MinusLogProbMetric: 32.0477

Epoch 134: val_loss did not improve from 30.70145
196/196 - 32s - loss: 30.9701 - MinusLogProbMetric: 30.9701 - val_loss: 32.0477 - val_MinusLogProbMetric: 32.0477 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 135/1000
2023-09-28 08:17:18.652 
Epoch 135/1000 
	 loss: 31.1196, MinusLogProbMetric: 31.1196, val_loss: 32.2933, val_MinusLogProbMetric: 32.2933

Epoch 135: val_loss did not improve from 30.70145
196/196 - 33s - loss: 31.1196 - MinusLogProbMetric: 31.1196 - val_loss: 32.2933 - val_MinusLogProbMetric: 32.2933 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 136/1000
2023-09-28 08:17:51.322 
Epoch 136/1000 
	 loss: 31.1408, MinusLogProbMetric: 31.1408, val_loss: 30.8743, val_MinusLogProbMetric: 30.8743

Epoch 136: val_loss did not improve from 30.70145
196/196 - 33s - loss: 31.1408 - MinusLogProbMetric: 31.1408 - val_loss: 30.8743 - val_MinusLogProbMetric: 30.8743 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 137/1000
2023-09-28 08:18:23.597 
Epoch 137/1000 
	 loss: 31.0164, MinusLogProbMetric: 31.0164, val_loss: 31.3997, val_MinusLogProbMetric: 31.3997

Epoch 137: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.0164 - MinusLogProbMetric: 31.0164 - val_loss: 31.3997 - val_MinusLogProbMetric: 31.3997 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 138/1000
2023-09-28 08:18:56.092 
Epoch 138/1000 
	 loss: 31.2413, MinusLogProbMetric: 31.2413, val_loss: 32.2575, val_MinusLogProbMetric: 32.2575

Epoch 138: val_loss did not improve from 30.70145
196/196 - 32s - loss: 31.2413 - MinusLogProbMetric: 31.2413 - val_loss: 32.2575 - val_MinusLogProbMetric: 32.2575 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 139/1000
2023-09-28 08:19:28.585 
Epoch 139/1000 
	 loss: 30.9781, MinusLogProbMetric: 30.9781, val_loss: 30.8617, val_MinusLogProbMetric: 30.8617

Epoch 139: val_loss did not improve from 30.70145
196/196 - 32s - loss: 30.9781 - MinusLogProbMetric: 30.9781 - val_loss: 30.8617 - val_MinusLogProbMetric: 30.8617 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 140/1000
2023-09-28 08:20:01.197 
Epoch 140/1000 
	 loss: 31.0829, MinusLogProbMetric: 31.0829, val_loss: 30.9900, val_MinusLogProbMetric: 30.9900

Epoch 140: val_loss did not improve from 30.70145
196/196 - 33s - loss: 31.0829 - MinusLogProbMetric: 31.0829 - val_loss: 30.9900 - val_MinusLogProbMetric: 30.9900 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 141/1000
2023-09-28 08:20:33.652 
Epoch 141/1000 
	 loss: 30.9744, MinusLogProbMetric: 30.9744, val_loss: 31.7800, val_MinusLogProbMetric: 31.7800

Epoch 141: val_loss did not improve from 30.70145
196/196 - 32s - loss: 30.9744 - MinusLogProbMetric: 30.9744 - val_loss: 31.7800 - val_MinusLogProbMetric: 31.7800 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 142/1000
2023-09-28 08:21:06.007 
Epoch 142/1000 
	 loss: 31.0036, MinusLogProbMetric: 31.0036, val_loss: 30.4507, val_MinusLogProbMetric: 30.4507

Epoch 142: val_loss improved from 30.70145 to 30.45073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 31.0036 - MinusLogProbMetric: 31.0036 - val_loss: 30.4507 - val_MinusLogProbMetric: 30.4507 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 143/1000
2023-09-28 08:21:39.285 
Epoch 143/1000 
	 loss: 31.1635, MinusLogProbMetric: 31.1635, val_loss: 30.6689, val_MinusLogProbMetric: 30.6689

Epoch 143: val_loss did not improve from 30.45073
196/196 - 33s - loss: 31.1635 - MinusLogProbMetric: 31.1635 - val_loss: 30.6689 - val_MinusLogProbMetric: 30.6689 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 144/1000
2023-09-28 08:22:11.417 
Epoch 144/1000 
	 loss: 30.8859, MinusLogProbMetric: 30.8859, val_loss: 30.9635, val_MinusLogProbMetric: 30.9635

Epoch 144: val_loss did not improve from 30.45073
196/196 - 32s - loss: 30.8859 - MinusLogProbMetric: 30.8859 - val_loss: 30.9635 - val_MinusLogProbMetric: 30.9635 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 145/1000
2023-09-28 08:22:43.848 
Epoch 145/1000 
	 loss: 31.0798, MinusLogProbMetric: 31.0798, val_loss: 30.7598, val_MinusLogProbMetric: 30.7598

Epoch 145: val_loss did not improve from 30.45073
196/196 - 32s - loss: 31.0798 - MinusLogProbMetric: 31.0798 - val_loss: 30.7598 - val_MinusLogProbMetric: 30.7598 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 146/1000
2023-09-28 08:23:16.284 
Epoch 146/1000 
	 loss: 30.7617, MinusLogProbMetric: 30.7617, val_loss: 31.8820, val_MinusLogProbMetric: 31.8820

Epoch 146: val_loss did not improve from 30.45073
196/196 - 32s - loss: 30.7617 - MinusLogProbMetric: 30.7617 - val_loss: 31.8820 - val_MinusLogProbMetric: 31.8820 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 147/1000
2023-09-28 08:23:49.019 
Epoch 147/1000 
	 loss: 30.9783, MinusLogProbMetric: 30.9783, val_loss: 31.0610, val_MinusLogProbMetric: 31.0610

Epoch 147: val_loss did not improve from 30.45073
196/196 - 33s - loss: 30.9783 - MinusLogProbMetric: 30.9783 - val_loss: 31.0610 - val_MinusLogProbMetric: 31.0610 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 148/1000
2023-09-28 08:24:21.816 
Epoch 148/1000 
	 loss: 30.6439, MinusLogProbMetric: 30.6439, val_loss: 31.8116, val_MinusLogProbMetric: 31.8116

Epoch 148: val_loss did not improve from 30.45073
196/196 - 33s - loss: 30.6439 - MinusLogProbMetric: 30.6439 - val_loss: 31.8116 - val_MinusLogProbMetric: 31.8116 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 149/1000
2023-09-28 08:24:54.251 
Epoch 149/1000 
	 loss: 30.9407, MinusLogProbMetric: 30.9407, val_loss: 30.7020, val_MinusLogProbMetric: 30.7020

Epoch 149: val_loss did not improve from 30.45073
196/196 - 32s - loss: 30.9407 - MinusLogProbMetric: 30.9407 - val_loss: 30.7020 - val_MinusLogProbMetric: 30.7020 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 150/1000
2023-09-28 08:25:26.826 
Epoch 150/1000 
	 loss: 30.6966, MinusLogProbMetric: 30.6966, val_loss: 30.9798, val_MinusLogProbMetric: 30.9798

Epoch 150: val_loss did not improve from 30.45073
196/196 - 33s - loss: 30.6966 - MinusLogProbMetric: 30.6966 - val_loss: 30.9798 - val_MinusLogProbMetric: 30.9798 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 151/1000
2023-09-28 08:25:59.179 
Epoch 151/1000 
	 loss: 30.9819, MinusLogProbMetric: 30.9819, val_loss: 31.3230, val_MinusLogProbMetric: 31.3230

Epoch 151: val_loss did not improve from 30.45073
196/196 - 32s - loss: 30.9819 - MinusLogProbMetric: 30.9819 - val_loss: 31.3230 - val_MinusLogProbMetric: 31.3230 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 152/1000
2023-09-28 08:26:31.682 
Epoch 152/1000 
	 loss: 30.7173, MinusLogProbMetric: 30.7173, val_loss: 32.7247, val_MinusLogProbMetric: 32.7247

Epoch 152: val_loss did not improve from 30.45073
196/196 - 32s - loss: 30.7173 - MinusLogProbMetric: 30.7173 - val_loss: 32.7247 - val_MinusLogProbMetric: 32.7247 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 153/1000
2023-09-28 08:27:04.343 
Epoch 153/1000 
	 loss: 30.9757, MinusLogProbMetric: 30.9757, val_loss: 30.7501, val_MinusLogProbMetric: 30.7501

Epoch 153: val_loss did not improve from 30.45073
196/196 - 33s - loss: 30.9757 - MinusLogProbMetric: 30.9757 - val_loss: 30.7501 - val_MinusLogProbMetric: 30.7501 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 154/1000
2023-09-28 08:27:36.799 
Epoch 154/1000 
	 loss: 30.6440, MinusLogProbMetric: 30.6440, val_loss: 30.7709, val_MinusLogProbMetric: 30.7709

Epoch 154: val_loss did not improve from 30.45073
196/196 - 32s - loss: 30.6440 - MinusLogProbMetric: 30.6440 - val_loss: 30.7709 - val_MinusLogProbMetric: 30.7709 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 155/1000
2023-09-28 08:28:09.333 
Epoch 155/1000 
	 loss: 31.1401, MinusLogProbMetric: 31.1401, val_loss: 31.1286, val_MinusLogProbMetric: 31.1286

Epoch 155: val_loss did not improve from 30.45073
196/196 - 33s - loss: 31.1401 - MinusLogProbMetric: 31.1401 - val_loss: 31.1286 - val_MinusLogProbMetric: 31.1286 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 156/1000
2023-09-28 08:28:41.800 
Epoch 156/1000 
	 loss: 30.7048, MinusLogProbMetric: 30.7048, val_loss: 31.9446, val_MinusLogProbMetric: 31.9446

Epoch 156: val_loss did not improve from 30.45073
196/196 - 32s - loss: 30.7048 - MinusLogProbMetric: 30.7048 - val_loss: 31.9446 - val_MinusLogProbMetric: 31.9446 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 157/1000
2023-09-28 08:29:13.770 
Epoch 157/1000 
	 loss: 30.7583, MinusLogProbMetric: 30.7583, val_loss: 30.8298, val_MinusLogProbMetric: 30.8298

Epoch 157: val_loss did not improve from 30.45073
196/196 - 32s - loss: 30.7583 - MinusLogProbMetric: 30.7583 - val_loss: 30.8298 - val_MinusLogProbMetric: 30.8298 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 158/1000
2023-09-28 08:29:46.029 
Epoch 158/1000 
	 loss: 30.6956, MinusLogProbMetric: 30.6956, val_loss: 30.1483, val_MinusLogProbMetric: 30.1483

Epoch 158: val_loss improved from 30.45073 to 30.14830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 30.6956 - MinusLogProbMetric: 30.6956 - val_loss: 30.1483 - val_MinusLogProbMetric: 30.1483 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 159/1000
2023-09-28 08:30:19.009 
Epoch 159/1000 
	 loss: 30.7408, MinusLogProbMetric: 30.7408, val_loss: 32.2589, val_MinusLogProbMetric: 32.2589

Epoch 159: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.7408 - MinusLogProbMetric: 30.7408 - val_loss: 32.2589 - val_MinusLogProbMetric: 32.2589 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 160/1000
2023-09-28 08:30:51.633 
Epoch 160/1000 
	 loss: 30.7349, MinusLogProbMetric: 30.7349, val_loss: 31.5477, val_MinusLogProbMetric: 31.5477

Epoch 160: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.7349 - MinusLogProbMetric: 30.7349 - val_loss: 31.5477 - val_MinusLogProbMetric: 31.5477 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 161/1000
2023-09-28 08:31:23.834 
Epoch 161/1000 
	 loss: 30.8838, MinusLogProbMetric: 30.8838, val_loss: 31.2094, val_MinusLogProbMetric: 31.2094

Epoch 161: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.8838 - MinusLogProbMetric: 30.8838 - val_loss: 31.2094 - val_MinusLogProbMetric: 31.2094 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 162/1000
2023-09-28 08:31:56.445 
Epoch 162/1000 
	 loss: 30.7994, MinusLogProbMetric: 30.7994, val_loss: 30.6223, val_MinusLogProbMetric: 30.6223

Epoch 162: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.7994 - MinusLogProbMetric: 30.7994 - val_loss: 30.6223 - val_MinusLogProbMetric: 30.6223 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 163/1000
2023-09-28 08:32:28.981 
Epoch 163/1000 
	 loss: 30.6737, MinusLogProbMetric: 30.6737, val_loss: 31.0337, val_MinusLogProbMetric: 31.0337

Epoch 163: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.6737 - MinusLogProbMetric: 30.6737 - val_loss: 31.0337 - val_MinusLogProbMetric: 31.0337 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 164/1000
2023-09-28 08:33:01.634 
Epoch 164/1000 
	 loss: 30.6524, MinusLogProbMetric: 30.6524, val_loss: 32.1077, val_MinusLogProbMetric: 32.1077

Epoch 164: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.6524 - MinusLogProbMetric: 30.6524 - val_loss: 32.1077 - val_MinusLogProbMetric: 32.1077 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 165/1000
2023-09-28 08:33:34.034 
Epoch 165/1000 
	 loss: 30.7690, MinusLogProbMetric: 30.7690, val_loss: 30.3363, val_MinusLogProbMetric: 30.3363

Epoch 165: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.7690 - MinusLogProbMetric: 30.7690 - val_loss: 30.3363 - val_MinusLogProbMetric: 30.3363 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 166/1000
2023-09-28 08:34:06.442 
Epoch 166/1000 
	 loss: 30.6349, MinusLogProbMetric: 30.6349, val_loss: 31.6313, val_MinusLogProbMetric: 31.6313

Epoch 166: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.6349 - MinusLogProbMetric: 30.6349 - val_loss: 31.6313 - val_MinusLogProbMetric: 31.6313 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 167/1000
2023-09-28 08:34:38.795 
Epoch 167/1000 
	 loss: 30.7962, MinusLogProbMetric: 30.7962, val_loss: 31.2818, val_MinusLogProbMetric: 31.2818

Epoch 167: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.7962 - MinusLogProbMetric: 30.7962 - val_loss: 31.2818 - val_MinusLogProbMetric: 31.2818 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 168/1000
2023-09-28 08:35:11.433 
Epoch 168/1000 
	 loss: 30.7234, MinusLogProbMetric: 30.7234, val_loss: 30.6475, val_MinusLogProbMetric: 30.6475

Epoch 168: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.7234 - MinusLogProbMetric: 30.7234 - val_loss: 30.6475 - val_MinusLogProbMetric: 30.6475 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 169/1000
2023-09-28 08:35:44.015 
Epoch 169/1000 
	 loss: 30.5711, MinusLogProbMetric: 30.5711, val_loss: 30.7216, val_MinusLogProbMetric: 30.7216

Epoch 169: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.5711 - MinusLogProbMetric: 30.5711 - val_loss: 30.7216 - val_MinusLogProbMetric: 30.7216 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 170/1000
2023-09-28 08:36:16.752 
Epoch 170/1000 
	 loss: 30.5693, MinusLogProbMetric: 30.5693, val_loss: 32.3484, val_MinusLogProbMetric: 32.3484

Epoch 170: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.5693 - MinusLogProbMetric: 30.5693 - val_loss: 32.3484 - val_MinusLogProbMetric: 32.3484 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 171/1000
2023-09-28 08:36:48.868 
Epoch 171/1000 
	 loss: 30.8115, MinusLogProbMetric: 30.8115, val_loss: 30.6109, val_MinusLogProbMetric: 30.6109

Epoch 171: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.8115 - MinusLogProbMetric: 30.8115 - val_loss: 30.6109 - val_MinusLogProbMetric: 30.6109 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 172/1000
2023-09-28 08:37:21.155 
Epoch 172/1000 
	 loss: 30.4260, MinusLogProbMetric: 30.4260, val_loss: 31.8305, val_MinusLogProbMetric: 31.8305

Epoch 172: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.4260 - MinusLogProbMetric: 30.4260 - val_loss: 31.8305 - val_MinusLogProbMetric: 31.8305 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 173/1000
2023-09-28 08:37:53.493 
Epoch 173/1000 
	 loss: 30.6960, MinusLogProbMetric: 30.6960, val_loss: 30.9271, val_MinusLogProbMetric: 30.9271

Epoch 173: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.6960 - MinusLogProbMetric: 30.6960 - val_loss: 30.9271 - val_MinusLogProbMetric: 30.9271 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 174/1000
2023-09-28 08:38:26.073 
Epoch 174/1000 
	 loss: 30.7254, MinusLogProbMetric: 30.7254, val_loss: 30.8903, val_MinusLogProbMetric: 30.8903

Epoch 174: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.7254 - MinusLogProbMetric: 30.7254 - val_loss: 30.8903 - val_MinusLogProbMetric: 30.8903 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 175/1000
2023-09-28 08:38:58.618 
Epoch 175/1000 
	 loss: 30.5913, MinusLogProbMetric: 30.5913, val_loss: 30.5260, val_MinusLogProbMetric: 30.5260

Epoch 175: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.5913 - MinusLogProbMetric: 30.5913 - val_loss: 30.5260 - val_MinusLogProbMetric: 30.5260 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 176/1000
2023-09-28 08:39:31.039 
Epoch 176/1000 
	 loss: 30.3288, MinusLogProbMetric: 30.3288, val_loss: 31.7876, val_MinusLogProbMetric: 31.7876

Epoch 176: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.3288 - MinusLogProbMetric: 30.3288 - val_loss: 31.7876 - val_MinusLogProbMetric: 31.7876 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 177/1000
2023-09-28 08:40:03.394 
Epoch 177/1000 
	 loss: 30.5458, MinusLogProbMetric: 30.5458, val_loss: 30.9393, val_MinusLogProbMetric: 30.9393

Epoch 177: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.5458 - MinusLogProbMetric: 30.5458 - val_loss: 30.9393 - val_MinusLogProbMetric: 30.9393 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 178/1000
2023-09-28 08:40:35.878 
Epoch 178/1000 
	 loss: 30.4307, MinusLogProbMetric: 30.4307, val_loss: 30.7716, val_MinusLogProbMetric: 30.7716

Epoch 178: val_loss did not improve from 30.14830
196/196 - 32s - loss: 30.4307 - MinusLogProbMetric: 30.4307 - val_loss: 30.7716 - val_MinusLogProbMetric: 30.7716 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 179/1000
2023-09-28 08:41:08.445 
Epoch 179/1000 
	 loss: 30.5722, MinusLogProbMetric: 30.5722, val_loss: 30.7245, val_MinusLogProbMetric: 30.7245

Epoch 179: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.5722 - MinusLogProbMetric: 30.5722 - val_loss: 30.7245 - val_MinusLogProbMetric: 30.7245 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 180/1000
2023-09-28 08:41:41.176 
Epoch 180/1000 
	 loss: 30.6516, MinusLogProbMetric: 30.6516, val_loss: 31.6316, val_MinusLogProbMetric: 31.6316

Epoch 180: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.6516 - MinusLogProbMetric: 30.6516 - val_loss: 31.6316 - val_MinusLogProbMetric: 31.6316 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 181/1000
2023-09-28 08:42:13.807 
Epoch 181/1000 
	 loss: 30.6106, MinusLogProbMetric: 30.6106, val_loss: 31.1088, val_MinusLogProbMetric: 31.1088

Epoch 181: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.6106 - MinusLogProbMetric: 30.6106 - val_loss: 31.1088 - val_MinusLogProbMetric: 31.1088 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 182/1000
2023-09-28 08:42:46.676 
Epoch 182/1000 
	 loss: 30.5515, MinusLogProbMetric: 30.5515, val_loss: 30.4050, val_MinusLogProbMetric: 30.4050

Epoch 182: val_loss did not improve from 30.14830
196/196 - 33s - loss: 30.5515 - MinusLogProbMetric: 30.5515 - val_loss: 30.4050 - val_MinusLogProbMetric: 30.4050 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 183/1000
2023-09-28 08:43:19.013 
Epoch 183/1000 
	 loss: 30.5065, MinusLogProbMetric: 30.5065, val_loss: 29.9529, val_MinusLogProbMetric: 29.9529

Epoch 183: val_loss improved from 30.14830 to 29.95295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_321/weights/best_weights.h5
196/196 - 33s - loss: 30.5065 - MinusLogProbMetric: 30.5065 - val_loss: 29.9529 - val_MinusLogProbMetric: 29.9529 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 184/1000
2023-09-28 08:43:53.387 
Epoch 184/1000 
	 loss: 30.4080, MinusLogProbMetric: 30.4080, val_loss: 30.5741, val_MinusLogProbMetric: 30.5741

Epoch 184: val_loss did not improve from 29.95295
196/196 - 34s - loss: 30.4080 - MinusLogProbMetric: 30.4080 - val_loss: 30.5741 - val_MinusLogProbMetric: 30.5741 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 185/1000
2023-09-28 08:44:28.119 
Epoch 185/1000 
	 loss: 30.4390, MinusLogProbMetric: 30.4390, val_loss: 33.3085, val_MinusLogProbMetric: 33.3085

Epoch 185: val_loss did not improve from 29.95295
196/196 - 35s - loss: 30.4390 - MinusLogProbMetric: 30.4390 - val_loss: 33.3085 - val_MinusLogProbMetric: 33.3085 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 186/1000
2023-09-28 08:45:02.413 
Epoch 186/1000 
	 loss: 30.4626, MinusLogProbMetric: 30.4626, val_loss: 30.6823, val_MinusLogProbMetric: 30.6823

Epoch 186: val_loss did not improve from 29.95295
196/196 - 34s - loss: 30.4626 - MinusLogProbMetric: 30.4626 - val_loss: 30.6823 - val_MinusLogProbMetric: 30.6823 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 187/1000
2023-09-28 08:45:38.011 
Epoch 187/1000 
	 loss: 30.4097, MinusLogProbMetric: 30.4097, val_loss: 32.4115, val_MinusLogProbMetric: 32.4115

Epoch 187: val_loss did not improve from 29.95295
196/196 - 36s - loss: 30.4097 - MinusLogProbMetric: 30.4097 - val_loss: 32.4115 - val_MinusLogProbMetric: 32.4115 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 188/1000
2023-09-28 08:46:13.124 
Epoch 188/1000 
	 loss: 30.4066, MinusLogProbMetric: 30.4066, val_loss: 31.9427, val_MinusLogProbMetric: 31.9427

Epoch 188: val_loss did not improve from 29.95295
196/196 - 35s - loss: 30.4066 - MinusLogProbMetric: 30.4066 - val_loss: 31.9427 - val_MinusLogProbMetric: 31.9427 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 189/1000
2023-09-28 08:46:48.232 
Epoch 189/1000 
	 loss: 30.3377, MinusLogProbMetric: 30.3377, val_loss: 30.6262, val_MinusLogProbMetric: 30.6262

Epoch 189: val_loss did not improve from 29.95295
196/196 - 35s - loss: 30.3377 - MinusLogProbMetric: 30.3377 - val_loss: 30.6262 - val_MinusLogProbMetric: 30.6262 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 190/1000
